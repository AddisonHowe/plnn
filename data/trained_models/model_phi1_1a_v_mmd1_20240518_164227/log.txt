Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 247336332

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.732777681390829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.732777681390829 | validation: 7.088548069149508]
	TIME [epoch: 114 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.503829627046301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.503829627046301 | validation: 6.035847695439442]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.365555362181014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.365555362181014 | validation: 5.9357960576288935]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.91114628464287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.91114628464287 | validation: 5.855017838956646]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.815483062805829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.815483062805829 | validation: 5.771875557908201]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.736544443003584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.736544443003584 | validation: 5.658838968742305]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.570626957458917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.570626957458917 | validation: 5.39234165305445]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.506565934692338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.506565934692338 | validation: 5.270289449655813]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2978352783663665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2978352783663665 | validation: 5.162971723379179]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.05338219196787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.05338219196787 | validation: 4.928988960385836]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908493527111544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.908493527111544 | validation: 4.855403758547486]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.791781724156928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.791781724156928 | validation: 4.750332671339807]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.645389024618878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.645389024618878 | validation: 4.555710997677935]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.656509682640113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.656509682640113 | validation: 4.639284960573046]
	TIME [epoch: 7.74 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.46408387917164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.46408387917164 | validation: 4.367354130249238]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.308749353255536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.308749353255536 | validation: 4.4430759070808765]
	TIME [epoch: 7.73 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.264636657155493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.264636657155493 | validation: 4.165011679652778]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.189552621646367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.189552621646367 | validation: 4.110772517709664]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.032151424323273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032151424323273 | validation: 3.9032402239150557]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.01812331727604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01812331727604 | validation: 3.8355693303605403]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.922616401448889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.922616401448889 | validation: 3.7545677879841737]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7674040508088735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7674040508088735 | validation: 3.6606299408362015]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767432944746326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.767432944746326 | validation: 3.7120533286581185]
	TIME [epoch: 7.71 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7529241528542503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7529241528542503 | validation: 3.60836962437911]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6764315073413973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6764315073413973 | validation: 3.6361402507190936]
	TIME [epoch: 7.72 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.629794616671811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.629794616671811 | validation: 3.7402634689800025]
	TIME [epoch: 7.71 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.607064826590404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.607064826590404 | validation: 3.526658199071811]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4889095592210406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4889095592210406 | validation: 3.4556945201898017]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6341008747680306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6341008747680306 | validation: 3.663657563641676]
	TIME [epoch: 7.76 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5059377070126994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5059377070126994 | validation: 3.4391719239394343]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.431332418527583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.431332418527583 | validation: 3.5202140304591722]
	TIME [epoch: 7.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5919801866442125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5919801866442125 | validation: 3.451205950840305]
	TIME [epoch: 7.71 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.452462249617862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.452462249617862 | validation: 3.4294587749369763]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4136316604676935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4136316604676935 | validation: 3.437580878276989]
	TIME [epoch: 7.74 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4427324031788475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4427324031788475 | validation: 3.414314964350874]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3979324962560957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3979324962560957 | validation: 3.4129535985657435]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4471778284772636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4471778284772636 | validation: 3.4298331345827138]
	TIME [epoch: 7.72 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4602096813405585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4602096813405585 | validation: 3.41130162838055]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.407833056817891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.407833056817891 | validation: 3.4345541858095276]
	TIME [epoch: 7.72 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4132971289738054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4132971289738054 | validation: 3.4208249960848343]
	TIME [epoch: 7.72 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.409052443390165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.409052443390165 | validation: 3.3882540909992933]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3848607723636883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3848607723636883 | validation: 3.4127840802973277]
	TIME [epoch: 7.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4414381445168067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4414381445168067 | validation: 3.4851388940320303]
	TIME [epoch: 7.76 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.459581370114927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.459581370114927 | validation: 3.463017404552511]
	TIME [epoch: 7.72 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402342672614787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.402342672614787 | validation: 3.3901581639556753]
	TIME [epoch: 7.72 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.370362691719703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.370362691719703 | validation: 3.3826484305566487]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3936157346402753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3936157346402753 | validation: 3.385374125811669]
	TIME [epoch: 7.78 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.382811759516798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.382811759516798 | validation: 3.4112395157789708]
	TIME [epoch: 7.73 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3763224160304595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3763224160304595 | validation: 3.3662894385003]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373883148631106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.373883148631106 | validation: 3.3789157970179913]
	TIME [epoch: 7.72 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3868095969734795		[learning rate: 0.0099823]
	Learning Rate: 0.0099823
	LOSS [training: 3.3868095969734795 | validation: 3.434335298918727]
	TIME [epoch: 7.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.374579548657549		[learning rate: 0.0099588]
	Learning Rate: 0.00995876
	LOSS [training: 3.374579548657549 | validation: 3.3630582591033953]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376377021088621		[learning rate: 0.0099353]
	Learning Rate: 0.00993527
	LOSS [training: 3.376377021088621 | validation: 3.3517819695213555]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.396749699903275		[learning rate: 0.0099118]
	Learning Rate: 0.00991183
	LOSS [training: 3.396749699903275 | validation: 3.3904613786251456]
	TIME [epoch: 7.74 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3539016030728983		[learning rate: 0.0098884]
	Learning Rate: 0.00988845
	LOSS [training: 3.3539016030728983 | validation: 3.3641434531840364]
	TIME [epoch: 7.73 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3603715166672283		[learning rate: 0.0098651]
	Learning Rate: 0.00986512
	LOSS [training: 3.3603715166672283 | validation: 3.4173854444279677]
	TIME [epoch: 7.76 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.38010492884976		[learning rate: 0.0098419]
	Learning Rate: 0.00984185
	LOSS [training: 3.38010492884976 | validation: 3.3842705822641768]
	TIME [epoch: 7.79 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.372666251918626		[learning rate: 0.0098186]
	Learning Rate: 0.00981864
	LOSS [training: 3.372666251918626 | validation: 3.3717993621695923]
	TIME [epoch: 7.76 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3367851741483427		[learning rate: 0.0097955]
	Learning Rate: 0.00979548
	LOSS [training: 3.3367851741483427 | validation: 3.351849205148674]
	TIME [epoch: 7.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3837505608637692		[learning rate: 0.0097724]
	Learning Rate: 0.00977237
	LOSS [training: 3.3837505608637692 | validation: 3.365742749281474]
	TIME [epoch: 7.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3342235404273906		[learning rate: 0.0097493]
	Learning Rate: 0.00974932
	LOSS [training: 3.3342235404273906 | validation: 3.3331452368281465]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3531550694207324		[learning rate: 0.0097263]
	Learning Rate: 0.00972632
	LOSS [training: 3.3531550694207324 | validation: 3.4185314713658057]
	TIME [epoch: 7.77 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3474013494318915		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 3.3474013494318915 | validation: 3.3256788232506116]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.309144934581444		[learning rate: 0.0096805]
	Learning Rate: 0.00968049
	LOSS [training: 3.309144934581444 | validation: 3.3390414909621544]
	TIME [epoch: 7.72 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3828243733449583		[learning rate: 0.0096577]
	Learning Rate: 0.00965766
	LOSS [training: 3.3828243733449583 | validation: 3.374880964556059]
	TIME [epoch: 7.72 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333391152153195		[learning rate: 0.0096349]
	Learning Rate: 0.00963488
	LOSS [training: 3.333391152153195 | validation: 3.3240567458073143]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3060011017023156		[learning rate: 0.0096121]
	Learning Rate: 0.00961215
	LOSS [training: 3.3060011017023156 | validation: 3.3784796626429863]
	TIME [epoch: 7.74 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352494265723709		[learning rate: 0.0095895]
	Learning Rate: 0.00958948
	LOSS [training: 3.352494265723709 | validation: 3.309292271416313]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3161268183126924		[learning rate: 0.0095669]
	Learning Rate: 0.00956686
	LOSS [training: 3.3161268183126924 | validation: 3.322935505801659]
	TIME [epoch: 7.75 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.312950935977035		[learning rate: 0.0095443]
	Learning Rate: 0.00954429
	LOSS [training: 3.312950935977035 | validation: 3.3667087247020895]
	TIME [epoch: 7.73 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.384955115629528		[learning rate: 0.0095218]
	Learning Rate: 0.00952177
	LOSS [training: 3.384955115629528 | validation: 3.342322078554028]
	TIME [epoch: 7.78 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3100340625742235		[learning rate: 0.0094993]
	Learning Rate: 0.00949931
	LOSS [training: 3.3100340625742235 | validation: 3.301207850823725]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3261962264819953		[learning rate: 0.0094769]
	Learning Rate: 0.00947691
	LOSS [training: 3.3261962264819953 | validation: 3.3392699151339214]
	TIME [epoch: 7.74 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2998664499306534		[learning rate: 0.0094546]
	Learning Rate: 0.00945455
	LOSS [training: 3.2998664499306534 | validation: 3.296875758757505]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282750152671012		[learning rate: 0.0094323]
	Learning Rate: 0.00943225
	LOSS [training: 3.282750152671012 | validation: 3.298657512177587]
	TIME [epoch: 7.75 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.418095413942332		[learning rate: 0.00941]
	Learning Rate: 0.00941
	LOSS [training: 3.418095413942332 | validation: 3.3703040572773837]
	TIME [epoch: 7.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3418124036903447		[learning rate: 0.0093878]
	Learning Rate: 0.00938781
	LOSS [training: 3.3418124036903447 | validation: 3.288524791902817]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2812134914965907		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 3.2812134914965907 | validation: 3.3074662492761746]
	TIME [epoch: 7.74 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2882253517019957		[learning rate: 0.0093436]
	Learning Rate: 0.00934357
	LOSS [training: 3.2882253517019957 | validation: 3.3107999849548104]
	TIME [epoch: 7.74 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.301767425566619		[learning rate: 0.0093215]
	Learning Rate: 0.00932153
	LOSS [training: 3.301767425566619 | validation: 3.305772383232776]
	TIME [epoch: 7.78 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.305282047817573		[learning rate: 0.0092995]
	Learning Rate: 0.00929954
	LOSS [training: 3.305282047817573 | validation: 3.3624980223939867]
	TIME [epoch: 7.75 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378520445694777		[learning rate: 0.0092776]
	Learning Rate: 0.00927761
	LOSS [training: 3.378520445694777 | validation: 3.303988475605486]
	TIME [epoch: 7.75 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278553925189613		[learning rate: 0.0092557]
	Learning Rate: 0.00925572
	LOSS [training: 3.278553925189613 | validation: 3.3971338514154477]
	TIME [epoch: 7.74 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321126452675659		[learning rate: 0.0092339]
	Learning Rate: 0.00923389
	LOSS [training: 3.321126452675659 | validation: 3.30408908676373]
	TIME [epoch: 7.74 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2864232659843067		[learning rate: 0.0092121]
	Learning Rate: 0.00921211
	LOSS [training: 3.2864232659843067 | validation: 3.506208808881344]
	TIME [epoch: 7.78 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3886644400636174		[learning rate: 0.0091904]
	Learning Rate: 0.00919038
	LOSS [training: 3.3886644400636174 | validation: 3.293717364608363]
	TIME [epoch: 7.75 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282708080989919		[learning rate: 0.0091687]
	Learning Rate: 0.0091687
	LOSS [training: 3.282708080989919 | validation: 3.2997891076841466]
	TIME [epoch: 7.74 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302385937602898		[learning rate: 0.0091471]
	Learning Rate: 0.00914707
	LOSS [training: 3.302385937602898 | validation: 3.34029517406283]
	TIME [epoch: 7.74 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3147032680570736		[learning rate: 0.0091255]
	Learning Rate: 0.00912549
	LOSS [training: 3.3147032680570736 | validation: 3.3377350273061044]
	TIME [epoch: 7.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2970865169530477		[learning rate: 0.009104]
	Learning Rate: 0.00910397
	LOSS [training: 3.2970865169530477 | validation: 3.3037953171139574]
	TIME [epoch: 7.8 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2864263980620683		[learning rate: 0.0090825]
	Learning Rate: 0.00908249
	LOSS [training: 3.2864263980620683 | validation: 3.3156661315718026]
	TIME [epoch: 7.75 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5975313249810013		[learning rate: 0.0090611]
	Learning Rate: 0.00906107
	LOSS [training: 3.5975313249810013 | validation: 3.4698981707282326]
	TIME [epoch: 7.75 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.371446536254344		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 3.371446536254344 | validation: 3.429049601295292]
	TIME [epoch: 7.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.430972952219262		[learning rate: 0.0090184]
	Learning Rate: 0.00901837
	LOSS [training: 3.430972952219262 | validation: 3.544276893855173]
	TIME [epoch: 7.75 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347048333180794		[learning rate: 0.0089971]
	Learning Rate: 0.0089971
	LOSS [training: 3.347048333180794 | validation: 3.3370903653938857]
	TIME [epoch: 7.79 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2856071940486027		[learning rate: 0.0089759]
	Learning Rate: 0.00897588
	LOSS [training: 3.2856071940486027 | validation: 3.2877284054968072]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3277983416039696		[learning rate: 0.0089547]
	Learning Rate: 0.0089547
	LOSS [training: 3.3277983416039696 | validation: 3.502508079452449]
	TIME [epoch: 7.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378527071609228		[learning rate: 0.0089336]
	Learning Rate: 0.00893358
	LOSS [training: 3.378527071609228 | validation: 3.411070082023905]
	TIME [epoch: 7.75 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6865315966404104		[learning rate: 0.0089125]
	Learning Rate: 0.00891251
	LOSS [training: 3.6865315966404104 | validation: 3.8228586345979814]
	TIME [epoch: 7.74 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.622821577426744		[learning rate: 0.0088915]
	Learning Rate: 0.00889149
	LOSS [training: 3.622821577426744 | validation: 3.4791598847237006]
	TIME [epoch: 7.78 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426743151232226		[learning rate: 0.0088705]
	Learning Rate: 0.00887051
	LOSS [training: 3.426743151232226 | validation: 3.377934641560956]
	TIME [epoch: 7.74 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329435127423242		[learning rate: 0.0088496]
	Learning Rate: 0.00884959
	LOSS [training: 3.329435127423242 | validation: 3.3347474226864042]
	TIME [epoch: 7.74 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.312213241194659		[learning rate: 0.0088287]
	Learning Rate: 0.00882871
	LOSS [training: 3.312213241194659 | validation: 3.334391681217378]
	TIME [epoch: 7.74 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.314273344812889		[learning rate: 0.0088079]
	Learning Rate: 0.00880789
	LOSS [training: 3.314273344812889 | validation: 3.298613021888697]
	TIME [epoch: 7.78 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2958234599720306		[learning rate: 0.0087871]
	Learning Rate: 0.00878711
	LOSS [training: 3.2958234599720306 | validation: 3.3639791953630978]
	TIME [epoch: 7.76 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329160832419131		[learning rate: 0.0087664]
	Learning Rate: 0.00876638
	LOSS [training: 3.329160832419131 | validation: 3.3025309116400208]
	TIME [epoch: 7.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.303738142346455		[learning rate: 0.0087457]
	Learning Rate: 0.00874571
	LOSS [training: 3.303738142346455 | validation: 3.3033700943103863]
	TIME [epoch: 7.75 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2842608608035557		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 3.2842608608035557 | validation: 3.298959592718297]
	TIME [epoch: 7.75 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31121930215428		[learning rate: 0.0087045]
	Learning Rate: 0.00870449
	LOSS [training: 3.31121930215428 | validation: 3.3396476640731465]
	TIME [epoch: 7.79 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2929410324370285		[learning rate: 0.008684]
	Learning Rate: 0.00868396
	LOSS [training: 3.2929410324370285 | validation: 3.3011204385294546]
	TIME [epoch: 7.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282826595069483		[learning rate: 0.0086635]
	Learning Rate: 0.00866348
	LOSS [training: 3.282826595069483 | validation: 3.295877946727713]
	TIME [epoch: 7.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3241209059024324		[learning rate: 0.008643]
	Learning Rate: 0.00864304
	LOSS [training: 3.3241209059024324 | validation: 3.308562607434556]
	TIME [epoch: 7.75 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351963712160045		[learning rate: 0.0086227]
	Learning Rate: 0.00862265
	LOSS [training: 3.351963712160045 | validation: 3.3314777104571047]
	TIME [epoch: 7.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294734465824037		[learning rate: 0.0086023]
	Learning Rate: 0.00860232
	LOSS [training: 3.294734465824037 | validation: 3.2863831293897032]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2782592244755397		[learning rate: 0.008582]
	Learning Rate: 0.00858202
	LOSS [training: 3.2782592244755397 | validation: 3.327201138667755]
	TIME [epoch: 7.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2846316063950995		[learning rate: 0.0085618]
	Learning Rate: 0.00856178
	LOSS [training: 3.2846316063950995 | validation: 3.2774911891825353]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258861319327657		[learning rate: 0.0085416]
	Learning Rate: 0.00854158
	LOSS [training: 3.258861319327657 | validation: 3.299326006781735]
	TIME [epoch: 7.78 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283190702472138		[learning rate: 0.0085214]
	Learning Rate: 0.00852144
	LOSS [training: 3.283190702472138 | validation: 3.287106256746086]
	TIME [epoch: 7.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278831575015514		[learning rate: 0.0085013]
	Learning Rate: 0.00850134
	LOSS [training: 3.278831575015514 | validation: 4.0694888092017205]
	TIME [epoch: 7.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.946812629449535		[learning rate: 0.0084813]
	Learning Rate: 0.00848128
	LOSS [training: 3.946812629449535 | validation: 3.531058817187313]
	TIME [epoch: 7.72 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4791382035782297		[learning rate: 0.0084613]
	Learning Rate: 0.00846128
	LOSS [training: 3.4791382035782297 | validation: 3.4859470957717313]
	TIME [epoch: 7.72 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4423206079278836		[learning rate: 0.0084413]
	Learning Rate: 0.00844132
	LOSS [training: 3.4423206079278836 | validation: 3.4634878069531316]
	TIME [epoch: 7.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4403873258170306		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 3.4403873258170306 | validation: 3.453735613769968]
	TIME [epoch: 7.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.395227396292475		[learning rate: 0.0084015]
	Learning Rate: 0.00840154
	LOSS [training: 3.395227396292475 | validation: 3.3857261838138952]
	TIME [epoch: 7.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3285448214913878		[learning rate: 0.0083817]
	Learning Rate: 0.00838172
	LOSS [training: 3.3285448214913878 | validation: 3.296789911902636]
	TIME [epoch: 7.73 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2953429171250077		[learning rate: 0.008362]
	Learning Rate: 0.00836195
	LOSS [training: 3.2953429171250077 | validation: 3.3996500269640952]
	TIME [epoch: 7.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.371300162002374		[learning rate: 0.0083422]
	Learning Rate: 0.00834223
	LOSS [training: 3.371300162002374 | validation: 3.3668289416191275]
	TIME [epoch: 7.72 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292883053114197		[learning rate: 0.0083225]
	Learning Rate: 0.00832255
	LOSS [training: 3.292883053114197 | validation: 4.036912834358115]
	TIME [epoch: 7.76 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.024250008844247		[learning rate: 0.0083029]
	Learning Rate: 0.00830292
	LOSS [training: 4.024250008844247 | validation: 3.760382096571896]
	TIME [epoch: 7.74 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.483502188990883		[learning rate: 0.0082833]
	Learning Rate: 0.00828333
	LOSS [training: 3.483502188990883 | validation: 3.3172682863877614]
	TIME [epoch: 7.72 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2940135738844596		[learning rate: 0.0082638]
	Learning Rate: 0.00826379
	LOSS [training: 3.2940135738844596 | validation: 3.3083376177829282]
	TIME [epoch: 7.72 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2791674394873667		[learning rate: 0.0082443]
	Learning Rate: 0.0082443
	LOSS [training: 3.2791674394873667 | validation: 3.5118591855389036]
	TIME [epoch: 7.72 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.493892555140908		[learning rate: 0.0082249]
	Learning Rate: 0.00822485
	LOSS [training: 3.493892555140908 | validation: 3.4081699671386625]
	TIME [epoch: 7.78 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3148266242383038		[learning rate: 0.0082055]
	Learning Rate: 0.00820545
	LOSS [training: 3.3148266242383038 | validation: 3.2938337051402335]
	TIME [epoch: 7.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.276180181551034		[learning rate: 0.0081861]
	Learning Rate: 0.0081861
	LOSS [training: 3.276180181551034 | validation: 3.3267073497755173]
	TIME [epoch: 7.72 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322478650593168		[learning rate: 0.0081668]
	Learning Rate: 0.00816679
	LOSS [training: 3.322478650593168 | validation: 3.290611211877012]
	TIME [epoch: 7.71 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2702476274507024		[learning rate: 0.0081475]
	Learning Rate: 0.00814752
	LOSS [training: 3.2702476274507024 | validation: 3.413916240613247]
	TIME [epoch: 7.72 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32123571622303		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 3.32123571622303 | validation: 3.299424462959199]
	TIME [epoch: 7.78 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.36117474307469		[learning rate: 0.0081091]
	Learning Rate: 0.00810913
	LOSS [training: 3.36117474307469 | validation: 3.3283286680596444]
	TIME [epoch: 7.73 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.293448083948808		[learning rate: 0.00809]
	Learning Rate: 0.00809
	LOSS [training: 3.293448083948808 | validation: 3.326288982716937]
	TIME [epoch: 7.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3892405168545006		[learning rate: 0.0080709]
	Learning Rate: 0.00807092
	LOSS [training: 3.3892405168545006 | validation: 3.477212231442529]
	TIME [epoch: 7.72 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4161749273061512		[learning rate: 0.0080519]
	Learning Rate: 0.00805188
	LOSS [training: 3.4161749273061512 | validation: 3.5494406248995514]
	TIME [epoch: 7.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.412866228872778		[learning rate: 0.0080329]
	Learning Rate: 0.00803289
	LOSS [training: 3.412866228872778 | validation: 3.3918210086360028]
	TIME [epoch: 7.78 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.311858940303869		[learning rate: 0.0080139]
	Learning Rate: 0.00801394
	LOSS [training: 3.311858940303869 | validation: 3.284739182781918]
	TIME [epoch: 7.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3027326809823263		[learning rate: 0.007995]
	Learning Rate: 0.00799504
	LOSS [training: 3.3027326809823263 | validation: 3.3289159182144514]
	TIME [epoch: 7.72 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322397823301457		[learning rate: 0.0079762]
	Learning Rate: 0.00797618
	LOSS [training: 3.322397823301457 | validation: 3.3348545599471815]
	TIME [epoch: 7.72 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.291621248554555		[learning rate: 0.0079574]
	Learning Rate: 0.00795736
	LOSS [training: 3.291621248554555 | validation: 3.2770845390598975]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280532732174861		[learning rate: 0.0079386]
	Learning Rate: 0.00793859
	LOSS [training: 3.280532732174861 | validation: 3.2930434654106895]
	TIME [epoch: 7.79 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3075938166254724		[learning rate: 0.0079199]
	Learning Rate: 0.00791987
	LOSS [training: 3.3075938166254724 | validation: 3.4461814265140616]
	TIME [epoch: 7.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3686051996824604		[learning rate: 0.0079012]
	Learning Rate: 0.00790119
	LOSS [training: 3.3686051996824604 | validation: 3.291909450361325]
	TIME [epoch: 7.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2765616575515755		[learning rate: 0.0078825]
	Learning Rate: 0.00788255
	LOSS [training: 3.2765616575515755 | validation: 3.2883864134643472]
	TIME [epoch: 7.72 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366125095210573		[learning rate: 0.007864]
	Learning Rate: 0.00786396
	LOSS [training: 3.366125095210573 | validation: 3.4261567310122456]
	TIME [epoch: 7.73 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3796712231057384		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 3.3796712231057384 | validation: 3.376847306774864]
	TIME [epoch: 7.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3340648388815906		[learning rate: 0.0078269]
	Learning Rate: 0.0078269
	LOSS [training: 3.3340648388815906 | validation: 3.3127043097549103]
	TIME [epoch: 7.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2850858072679605		[learning rate: 0.0078084]
	Learning Rate: 0.00780844
	LOSS [training: 3.2850858072679605 | validation: 3.2919289052198986]
	TIME [epoch: 7.73 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3261394372749193		[learning rate: 0.00779]
	Learning Rate: 0.00779002
	LOSS [training: 3.3261394372749193 | validation: 3.313666762354381]
	TIME [epoch: 7.72 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280661806383475		[learning rate: 0.0077716]
	Learning Rate: 0.00777164
	LOSS [training: 3.280661806383475 | validation: 3.308936336115721]
	TIME [epoch: 7.75 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281249648004884		[learning rate: 0.0077533]
	Learning Rate: 0.00775331
	LOSS [training: 3.281249648004884 | validation: 3.303446687770478]
	TIME [epoch: 7.75 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.26873905788499		[learning rate: 0.007735]
	Learning Rate: 0.00773502
	LOSS [training: 3.26873905788499 | validation: 3.2845564864667303]
	TIME [epoch: 7.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2672749366813276		[learning rate: 0.0077168]
	Learning Rate: 0.00771678
	LOSS [training: 3.2672749366813276 | validation: 3.3000322013656715]
	TIME [epoch: 7.73 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3633041145050306		[learning rate: 0.0076986]
	Learning Rate: 0.00769857
	LOSS [training: 3.3633041145050306 | validation: 3.341945591982694]
	TIME [epoch: 7.72 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3090271563912004		[learning rate: 0.0076804]
	Learning Rate: 0.00768041
	LOSS [training: 3.3090271563912004 | validation: 3.297063320733857]
	TIME [epoch: 7.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2680153408101527		[learning rate: 0.0076623]
	Learning Rate: 0.0076623
	LOSS [training: 3.2680153408101527 | validation: 3.2781129962176263]
	TIME [epoch: 7.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2730388536087736		[learning rate: 0.0076442]
	Learning Rate: 0.00764422
	LOSS [training: 3.2730388536087736 | validation: 3.3276167531494947]
	TIME [epoch: 7.72 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2777967160758994		[learning rate: 0.0076262]
	Learning Rate: 0.00762619
	LOSS [training: 3.2777967160758994 | validation: 3.2728704647654245]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2728425813405533		[learning rate: 0.0076082]
	Learning Rate: 0.0076082
	LOSS [training: 3.2728425813405533 | validation: 3.2781338608498545]
	TIME [epoch: 7.74 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.256293648279664		[learning rate: 0.0075903]
	Learning Rate: 0.00759025
	LOSS [training: 3.256293648279664 | validation: 3.276404952201387]
	TIME [epoch: 7.78 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253677670215802		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 3.253677670215802 | validation: 3.2972034888156925]
	TIME [epoch: 7.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3046229254838244		[learning rate: 0.0075545]
	Learning Rate: 0.00755449
	LOSS [training: 3.3046229254838244 | validation: 3.3804762357409603]
	TIME [epoch: 7.74 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295726637463561		[learning rate: 0.0075367]
	Learning Rate: 0.00753667
	LOSS [training: 3.295726637463561 | validation: 3.29247774173549]
	TIME [epoch: 7.73 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302650229739089		[learning rate: 0.0075189]
	Learning Rate: 0.00751889
	LOSS [training: 3.302650229739089 | validation: 3.3274728006808414]
	TIME [epoch: 7.74 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2760315390528367		[learning rate: 0.0075012]
	Learning Rate: 0.00750116
	LOSS [training: 3.2760315390528367 | validation: 3.299228901965793]
	TIME [epoch: 7.79 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3593016159377505		[learning rate: 0.0074835]
	Learning Rate: 0.00748346
	LOSS [training: 3.3593016159377505 | validation: 3.338108107874279]
	TIME [epoch: 7.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29222657321607		[learning rate: 0.0074658]
	Learning Rate: 0.00746581
	LOSS [training: 3.29222657321607 | validation: 3.2856565191390765]
	TIME [epoch: 7.74 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.429902926109442		[learning rate: 0.0074482]
	Learning Rate: 0.0074482
	LOSS [training: 3.429902926109442 | validation: 3.5694207054765323]
	TIME [epoch: 7.74 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.453724682910297		[learning rate: 0.0074306]
	Learning Rate: 0.00743063
	LOSS [training: 3.453724682910297 | validation: 3.3526115194480424]
	TIME [epoch: 7.75 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2972482817845696		[learning rate: 0.0074131]
	Learning Rate: 0.0074131
	LOSS [training: 3.2972482817845696 | validation: 3.33104972486559]
	TIME [epoch: 7.79 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.388269500441511		[learning rate: 0.0073956]
	Learning Rate: 0.00739562
	LOSS [training: 3.388269500441511 | validation: 3.3463562191712164]
	TIME [epoch: 7.74 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330750127877851		[learning rate: 0.0073782]
	Learning Rate: 0.00737817
	LOSS [training: 3.330750127877851 | validation: 3.328977175305434]
	TIME [epoch: 7.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2868672626563726		[learning rate: 0.0073608]
	Learning Rate: 0.00736077
	LOSS [training: 3.2868672626563726 | validation: 3.2984943283621533]
	TIME [epoch: 7.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3679719733218887		[learning rate: 0.0073434]
	Learning Rate: 0.0073434
	LOSS [training: 3.3679719733218887 | validation: 3.3127781186886116]
	TIME [epoch: 7.74 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3789114156748625		[learning rate: 0.0073261]
	Learning Rate: 0.00732608
	LOSS [training: 3.3789114156748625 | validation: 3.4675476100376756]
	TIME [epoch: 7.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3762952137792857		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 3.3762952137792857 | validation: 3.33140428632898]
	TIME [epoch: 7.73 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284524622139735		[learning rate: 0.0072916]
	Learning Rate: 0.00729156
	LOSS [training: 3.284524622139735 | validation: 3.2891770994290357]
	TIME [epoch: 7.74 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308403593518038		[learning rate: 0.0072744]
	Learning Rate: 0.00727436
	LOSS [training: 3.308403593518038 | validation: 3.467570209469074]
	TIME [epoch: 7.73 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333180324537122		[learning rate: 0.0072572]
	Learning Rate: 0.0072572
	LOSS [training: 3.333180324537122 | validation: 3.282560275109284]
	TIME [epoch: 7.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294524100366129		[learning rate: 0.0072401]
	Learning Rate: 0.00724008
	LOSS [training: 3.294524100366129 | validation: 3.338343309365573]
	TIME [epoch: 7.75 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331552014133875		[learning rate: 0.007223]
	Learning Rate: 0.007223
	LOSS [training: 3.331552014133875 | validation: 3.322517662485465]
	TIME [epoch: 7.75 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.276454744023994		[learning rate: 0.007206]
	Learning Rate: 0.00720597
	LOSS [training: 3.276454744023994 | validation: 3.291121389717012]
	TIME [epoch: 7.73 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2573515116121605		[learning rate: 0.007189]
	Learning Rate: 0.00718897
	LOSS [training: 3.2573515116121605 | validation: 3.2676762518866003]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255311326083631		[learning rate: 0.007172]
	Learning Rate: 0.00717201
	LOSS [training: 3.255311326083631 | validation: 3.2716044073591104]
	TIME [epoch: 7.79 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271074289933116		[learning rate: 0.0071551]
	Learning Rate: 0.00715509
	LOSS [training: 3.271074289933116 | validation: 3.313084073063628]
	TIME [epoch: 7.75 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.439520557116246		[learning rate: 0.0071382]
	Learning Rate: 0.00713822
	LOSS [training: 3.439520557116246 | validation: 3.3176872632934646]
	TIME [epoch: 7.75 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2830755347200755		[learning rate: 0.0071214]
	Learning Rate: 0.00712138
	LOSS [training: 3.2830755347200755 | validation: 3.2932502380741155]
	TIME [epoch: 7.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.291752754112912		[learning rate: 0.0071046]
	Learning Rate: 0.00710458
	LOSS [training: 3.291752754112912 | validation: 3.313626937011192]
	TIME [epoch: 7.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2692832322374854		[learning rate: 0.0070878]
	Learning Rate: 0.00708782
	LOSS [training: 3.2692832322374854 | validation: 3.2794174569343357]
	TIME [epoch: 7.8 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2787212542400517		[learning rate: 0.0070711]
	Learning Rate: 0.0070711
	LOSS [training: 3.2787212542400517 | validation: 3.279069636124533]
	TIME [epoch: 7.76 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3754836957135708		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 3.3754836957135708 | validation: 3.4511798906521802]
	TIME [epoch: 7.74 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5576634286212907		[learning rate: 0.0070378]
	Learning Rate: 0.00703778
	LOSS [training: 3.5576634286212907 | validation: 3.4460785174057547]
	TIME [epoch: 7.74 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.401898701846154		[learning rate: 0.0070212]
	Learning Rate: 0.00702118
	LOSS [training: 3.401898701846154 | validation: 3.3979276933629006]
	TIME [epoch: 7.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3703327709201725		[learning rate: 0.0070046]
	Learning Rate: 0.00700462
	LOSS [training: 3.3703327709201725 | validation: 3.3934866356811533]
	TIME [epoch: 7.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332255152784166		[learning rate: 0.0069881]
	Learning Rate: 0.0069881
	LOSS [training: 3.332255152784166 | validation: 3.3361081973888664]
	TIME [epoch: 7.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297628657381644		[learning rate: 0.0069716]
	Learning Rate: 0.00697161
	LOSS [training: 3.297628657381644 | validation: 3.2852465710960512]
	TIME [epoch: 7.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4783257398373215		[learning rate: 0.0069552]
	Learning Rate: 0.00695517
	LOSS [training: 3.4783257398373215 | validation: 3.5142460739043795]
	TIME [epoch: 7.75 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.462199283550037		[learning rate: 0.0069388]
	Learning Rate: 0.00693876
	LOSS [training: 3.462199283550037 | validation: 3.63940272213193]
	TIME [epoch: 7.75 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4582289695869415		[learning rate: 0.0069224]
	Learning Rate: 0.00692239
	LOSS [training: 3.4582289695869415 | validation: 3.441957287537363]
	TIME [epoch: 7.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.361444521226943		[learning rate: 0.0069061]
	Learning Rate: 0.00690607
	LOSS [training: 3.361444521226943 | validation: 3.3719801895654196]
	TIME [epoch: 7.75 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335637745250172		[learning rate: 0.0068898]
	Learning Rate: 0.00688978
	LOSS [training: 3.335637745250172 | validation: 3.358696046777106]
	TIME [epoch: 7.74 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287425967295119		[learning rate: 0.0068735]
	Learning Rate: 0.00687352
	LOSS [training: 3.287425967295119 | validation: 3.29631743104554]
	TIME [epoch: 7.75 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2985037854817025		[learning rate: 0.0068573]
	Learning Rate: 0.00685731
	LOSS [training: 3.2985037854817025 | validation: 3.4421566328638824]
	TIME [epoch: 7.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352146946578435		[learning rate: 0.0068411]
	Learning Rate: 0.00684114
	LOSS [training: 3.352146946578435 | validation: 3.33101425603807]
	TIME [epoch: 7.77 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2836192105223905		[learning rate: 0.006825]
	Learning Rate: 0.006825
	LOSS [training: 3.2836192105223905 | validation: 3.2851661005350743]
	TIME [epoch: 7.76 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2696014064590964		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 3.2696014064590964 | validation: 3.288307699801773]
	TIME [epoch: 7.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2602801258461565		[learning rate: 0.0067928]
	Learning Rate: 0.00679284
	LOSS [training: 3.2602801258461565 | validation: 3.2841263536776477]
	TIME [epoch: 7.74 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261648492074488		[learning rate: 0.0067768]
	Learning Rate: 0.00677681
	LOSS [training: 3.261648492074488 | validation: 3.286636348326131]
	TIME [epoch: 7.76 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2533420064938308		[learning rate: 0.0067608]
	Learning Rate: 0.00676083
	LOSS [training: 3.2533420064938308 | validation: 3.311429042818336]
	TIME [epoch: 7.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2571219152861177		[learning rate: 0.0067449]
	Learning Rate: 0.00674488
	LOSS [training: 3.2571219152861177 | validation: 3.2727890643770117]
	TIME [epoch: 7.74 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261337580920091		[learning rate: 0.006729]
	Learning Rate: 0.00672897
	LOSS [training: 3.261337580920091 | validation: 3.299021708588002]
	TIME [epoch: 7.75 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2860547194635537		[learning rate: 0.0067131]
	Learning Rate: 0.0067131
	LOSS [training: 3.2860547194635537 | validation: 3.309850026343746]
	TIME [epoch: 7.75 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30355720605188		[learning rate: 0.0066973]
	Learning Rate: 0.00669726
	LOSS [training: 3.30355720605188 | validation: 3.3321636266033696]
	TIME [epoch: 7.78 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.289276988858498		[learning rate: 0.0066815]
	Learning Rate: 0.00668147
	LOSS [training: 3.289276988858498 | validation: 3.297570889000655]
	TIME [epoch: 7.75 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.250989851370236		[learning rate: 0.0066657]
	Learning Rate: 0.00666571
	LOSS [training: 3.250989851370236 | validation: 3.2736585595564502]
	TIME [epoch: 7.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260646652365458		[learning rate: 0.00665]
	Learning Rate: 0.00664998
	LOSS [training: 3.260646652365458 | validation: 3.3331603306085222]
	TIME [epoch: 7.74 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2843061343295044		[learning rate: 0.0066343]
	Learning Rate: 0.0066343
	LOSS [training: 3.2843061343295044 | validation: 3.3085723348744738]
	TIME [epoch: 7.73 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2680263080268577		[learning rate: 0.0066186]
	Learning Rate: 0.00661865
	LOSS [training: 3.2680263080268577 | validation: 3.28794169480088]
	TIME [epoch: 7.79 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.276038378101468		[learning rate: 0.006603]
	Learning Rate: 0.00660304
	LOSS [training: 3.276038378101468 | validation: 3.273036194517796]
	TIME [epoch: 7.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.25049382986242		[learning rate: 0.0065875]
	Learning Rate: 0.00658746
	LOSS [training: 3.25049382986242 | validation: 3.2757757394895326]
	TIME [epoch: 7.75 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2575864471810156		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 3.2575864471810156 | validation: 3.357334307729749]
	TIME [epoch: 7.74 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3408229884683296		[learning rate: 0.0065564]
	Learning Rate: 0.00655642
	LOSS [training: 3.3408229884683296 | validation: 3.4043532988475684]
	TIME [epoch: 7.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.390805303356322		[learning rate: 0.006541]
	Learning Rate: 0.00654095
	LOSS [training: 3.390805303356322 | validation: 3.4134472693025923]
	TIME [epoch: 7.79 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.36401065093851		[learning rate: 0.0065255]
	Learning Rate: 0.00652552
	LOSS [training: 3.36401065093851 | validation: 3.364589170833371]
	TIME [epoch: 7.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3238924061464874		[learning rate: 0.0065101]
	Learning Rate: 0.00651013
	LOSS [training: 3.3238924061464874 | validation: 3.3229743025154734]
	TIME [epoch: 7.75 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2815093643431803		[learning rate: 0.0064948]
	Learning Rate: 0.00649477
	LOSS [training: 3.2815093643431803 | validation: 3.2768637179204525]
	TIME [epoch: 7.75 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.25410300479476		[learning rate: 0.0064795]
	Learning Rate: 0.00647945
	LOSS [training: 3.25410300479476 | validation: 3.272324372896799]
	TIME [epoch: 7.76 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285049083653805		[learning rate: 0.0064642]
	Learning Rate: 0.00646417
	LOSS [training: 3.285049083653805 | validation: 3.2869410166143673]
	TIME [epoch: 7.78 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2580462645871293		[learning rate: 0.0064489]
	Learning Rate: 0.00644892
	LOSS [training: 3.2580462645871293 | validation: 3.2748985517481675]
	TIME [epoch: 7.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.244932805322956		[learning rate: 0.0064337]
	Learning Rate: 0.00643371
	LOSS [training: 3.244932805322956 | validation: 3.260736272548808]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2422559962546162		[learning rate: 0.0064185]
	Learning Rate: 0.00641853
	LOSS [training: 3.2422559962546162 | validation: 3.494279658088902]
	TIME [epoch: 7.75 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.39808360552259		[learning rate: 0.0064034]
	Learning Rate: 0.00640339
	LOSS [training: 3.39808360552259 | validation: 3.365724477944643]
	TIME [epoch: 7.79 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3119413536557696		[learning rate: 0.0063883]
	Learning Rate: 0.00638829
	LOSS [training: 3.3119413536557696 | validation: 3.2888404615638107]
	TIME [epoch: 7.78 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263062775543447		[learning rate: 0.0063732]
	Learning Rate: 0.00637322
	LOSS [training: 3.263062775543447 | validation: 3.2836547991540703]
	TIME [epoch: 7.76 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2523218824398277		[learning rate: 0.0063582]
	Learning Rate: 0.00635819
	LOSS [training: 3.2523218824398277 | validation: 3.283102166139951]
	TIME [epoch: 7.73 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252495052759254		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 3.252495052759254 | validation: 3.2656649368664383]
	TIME [epoch: 7.74 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.242742783308103		[learning rate: 0.0063282]
	Learning Rate: 0.00632823
	LOSS [training: 3.242742783308103 | validation: 3.2823057245637606]
	TIME [epoch: 7.78 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253074750876866		[learning rate: 0.0063133]
	Learning Rate: 0.0063133
	LOSS [training: 3.253074750876866 | validation: 3.268542140988937]
	TIME [epoch: 7.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3467313928777744		[learning rate: 0.0062984]
	Learning Rate: 0.00629841
	LOSS [training: 3.3467313928777744 | validation: 3.500895503114349]
	TIME [epoch: 7.76 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378554885561426		[learning rate: 0.0062836]
	Learning Rate: 0.00628355
	LOSS [training: 3.378554885561426 | validation: 3.2884702583228753]
	TIME [epoch: 7.75 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2581656663133303		[learning rate: 0.0062687]
	Learning Rate: 0.00626873
	LOSS [training: 3.2581656663133303 | validation: 3.31652548880488]
	TIME [epoch: 7.75 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2695906104582955		[learning rate: 0.0062539]
	Learning Rate: 0.00625394
	LOSS [training: 3.2695906104582955 | validation: 3.269869984502259]
	TIME [epoch: 7.79 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251532872190065		[learning rate: 0.0062392]
	Learning Rate: 0.00623919
	LOSS [training: 3.251532872190065 | validation: 3.2783839177944696]
	TIME [epoch: 7.76 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.276352261813768		[learning rate: 0.0062245]
	Learning Rate: 0.00622447
	LOSS [training: 3.276352261813768 | validation: 3.3224868973740858]
	TIME [epoch: 7.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286004681311577		[learning rate: 0.0062098]
	Learning Rate: 0.00620979
	LOSS [training: 3.286004681311577 | validation: 3.285524538328341]
	TIME [epoch: 7.75 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271932184050676		[learning rate: 0.0061951]
	Learning Rate: 0.00619514
	LOSS [training: 3.271932184050676 | validation: 3.28739680449788]
	TIME [epoch: 7.74 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255692009155248		[learning rate: 0.0061805]
	Learning Rate: 0.00618053
	LOSS [training: 3.255692009155248 | validation: 3.2784737466892184]
	TIME [epoch: 7.78 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2782672628137863		[learning rate: 0.0061659]
	Learning Rate: 0.00616595
	LOSS [training: 3.2782672628137863 | validation: 3.461823046883594]
	TIME [epoch: 7.75 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5247195430457454		[learning rate: 0.0061514]
	Learning Rate: 0.00615141
	LOSS [training: 3.5247195430457454 | validation: 3.4092690625830464]
	TIME [epoch: 7.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3462654988939504		[learning rate: 0.0061369]
	Learning Rate: 0.0061369
	LOSS [training: 3.3462654988939504 | validation: 3.347753461498126]
	TIME [epoch: 7.75 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3094439658992454		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 3.3094439658992454 | validation: 3.3163697880371488]
	TIME [epoch: 7.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2851505436786157		[learning rate: 0.006108]
	Learning Rate: 0.00610798
	LOSS [training: 3.2851505436786157 | validation: 3.2971977031052537]
	TIME [epoch: 7.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2637745210472393		[learning rate: 0.0060936]
	Learning Rate: 0.00609357
	LOSS [training: 3.2637745210472393 | validation: 3.2838452202534656]
	TIME [epoch: 7.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2568888811654735		[learning rate: 0.0060792]
	Learning Rate: 0.0060792
	LOSS [training: 3.2568888811654735 | validation: 3.2813494089578548]
	TIME [epoch: 7.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2625764213944275		[learning rate: 0.0060649]
	Learning Rate: 0.00606486
	LOSS [training: 3.2625764213944275 | validation: 3.277592951482254]
	TIME [epoch: 7.75 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255067554390002		[learning rate: 0.0060505]
	Learning Rate: 0.00605055
	LOSS [training: 3.255067554390002 | validation: 3.272987111391477]
	TIME [epoch: 7.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.250892082175311		[learning rate: 0.0060363]
	Learning Rate: 0.00603628
	LOSS [training: 3.250892082175311 | validation: 3.303387646881144]
	TIME [epoch: 7.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3336482549063913		[learning rate: 0.006022]
	Learning Rate: 0.00602204
	LOSS [training: 3.3336482549063913 | validation: 3.658995062912193]
	TIME [epoch: 7.75 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3963705865025067		[learning rate: 0.0060078]
	Learning Rate: 0.00600783
	LOSS [training: 3.3963705865025067 | validation: 3.3553158524931748]
	TIME [epoch: 7.74 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3236678874729737		[learning rate: 0.0059937]
	Learning Rate: 0.00599366
	LOSS [training: 3.3236678874729737 | validation: 3.272345053449616]
	TIME [epoch: 7.75 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2789401993096137		[learning rate: 0.0059795]
	Learning Rate: 0.00597952
	LOSS [training: 3.2789401993096137 | validation: 3.3378319073722365]
	TIME [epoch: 7.77 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322516994654772		[learning rate: 0.0059654]
	Learning Rate: 0.00596542
	LOSS [training: 3.322516994654772 | validation: 3.30172201160711]
	TIME [epoch: 7.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2652438650935722		[learning rate: 0.0059513]
	Learning Rate: 0.00595135
	LOSS [training: 3.2652438650935722 | validation: 3.2702927828304613]
	TIME [epoch: 7.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2557131159448987		[learning rate: 0.0059373]
	Learning Rate: 0.00593731
	LOSS [training: 3.2557131159448987 | validation: 3.2763570718016033]
	TIME [epoch: 7.75 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2565966470971186		[learning rate: 0.0059233]
	Learning Rate: 0.00592331
	LOSS [training: 3.2565966470971186 | validation: 3.2697229685533076]
	TIME [epoch: 7.75 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.250721147993685		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 3.250721147993685 | validation: 3.269839527142709]
	TIME [epoch: 7.78 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2524099258229553		[learning rate: 0.0058954]
	Learning Rate: 0.00589539
	LOSS [training: 3.2524099258229553 | validation: 3.2636477583708565]
	TIME [epoch: 7.75 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2547429004197217		[learning rate: 0.0058815]
	Learning Rate: 0.00588149
	LOSS [training: 3.2547429004197217 | validation: 3.2774804650326237]
	TIME [epoch: 7.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283234127027114		[learning rate: 0.0058676]
	Learning Rate: 0.00586761
	LOSS [training: 3.283234127027114 | validation: 3.298486564554146]
	TIME [epoch: 7.74 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2604807569757144		[learning rate: 0.0058538]
	Learning Rate: 0.00585377
	LOSS [training: 3.2604807569757144 | validation: 3.3037033195787897]
	TIME [epoch: 7.75 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257192231282235		[learning rate: 0.00584]
	Learning Rate: 0.00583996
	LOSS [training: 3.257192231282235 | validation: 3.2664735929330506]
	TIME [epoch: 7.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2583170874818057		[learning rate: 0.0058262]
	Learning Rate: 0.00582619
	LOSS [training: 3.2583170874818057 | validation: 3.2872437899079565]
	TIME [epoch: 7.76 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286229626167077		[learning rate: 0.0058124]
	Learning Rate: 0.00581245
	LOSS [training: 3.286229626167077 | validation: 3.2647980183109286]
	TIME [epoch: 7.74 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2652332177117036		[learning rate: 0.0057987]
	Learning Rate: 0.00579874
	LOSS [training: 3.2652332177117036 | validation: 3.266042944455129]
	TIME [epoch: 7.75 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2544820439640194		[learning rate: 0.0057851]
	Learning Rate: 0.00578506
	LOSS [training: 3.2544820439640194 | validation: 3.3498497522186694]
	TIME [epoch: 7.73 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.291877152467856		[learning rate: 0.0057714]
	Learning Rate: 0.00577141
	LOSS [training: 3.291877152467856 | validation: 3.299789200330033]
	TIME [epoch: 7.79 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2502552863664893		[learning rate: 0.0057578]
	Learning Rate: 0.0057578
	LOSS [training: 3.2502552863664893 | validation: 3.27440264013641]
	TIME [epoch: 7.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253861105642168		[learning rate: 0.0057442]
	Learning Rate: 0.00574422
	LOSS [training: 3.253861105642168 | validation: 3.2627624941269886]
	TIME [epoch: 7.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240818264310116		[learning rate: 0.0057307]
	Learning Rate: 0.00573067
	LOSS [training: 3.240818264310116 | validation: 3.259081445887113]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240518_164227/states/model_phi1_1a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2491260978956316		[learning rate: 0.0057171]
	Learning Rate: 0.00571715
	LOSS [training: 3.2491260978956316 | validation: 3.2764284546422884]
	TIME [epoch: 7.75 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.27142237783702		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 3.27142237783702 | validation: 3.2624166987292647]
	TIME [epoch: 7.78 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2423969251550613		[learning rate: 0.0056902]
	Learning Rate: 0.00569021
	LOSS [training: 3.2423969251550613 | validation: 3.2802546995234807]
	TIME [epoch: 7.75 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251946692330179		[learning rate: 0.0056768]
	Learning Rate: 0.00567679
	LOSS [training: 3.251946692330179 | validation: 3.2679625452852896]
	TIME [epoch: 7.75 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30887397270873		[learning rate: 0.0056634]
	Learning Rate: 0.0056634
	LOSS [training: 3.30887397270873 | validation: 3.4689907891269147]
	TIME [epoch: 7.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3379650829599212		[learning rate: 0.00565]
	Learning Rate: 0.00565004
	LOSS [training: 3.3379650829599212 | validation: 3.3188257790218048]
	TIME [epoch: 7.75 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3628297227808686		[learning rate: 0.0056367]
	Learning Rate: 0.00563671
	LOSS [training: 3.3628297227808686 | validation: 3.3253311485476504]
	TIME [epoch: 7.77 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5077435613333883		[learning rate: 0.0056234]
	Learning Rate: 0.00562341
	LOSS [training: 3.5077435613333883 | validation: 3.6084554783412264]
	TIME [epoch: 7.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5111056965255205		[learning rate: 0.0056101]
	Learning Rate: 0.00561015
	LOSS [training: 3.5111056965255205 | validation: 3.480406659830784]
	TIME [epoch: 7.75 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4092049594373655		[learning rate: 0.0055969]
	Learning Rate: 0.00559691
	LOSS [training: 3.4092049594373655 | validation: 3.4263208891891246]
	TIME [epoch: 7.73 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4675407988391975		[learning rate: 0.0055837]
	Learning Rate: 0.00558371
	LOSS [training: 3.4675407988391975 | validation: 3.4618712047311426]
	TIME [epoch: 7.77 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.471113191312866		[learning rate: 0.0055705]
	Learning Rate: 0.00557054
	LOSS [training: 3.471113191312866 | validation: 3.4788733498200646]
	TIME [epoch: 7.76 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.39163235817202		[learning rate: 0.0055574]
	Learning Rate: 0.0055574
	LOSS [training: 3.39163235817202 | validation: 3.364266403835315]
	TIME [epoch: 7.74 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.313407941446372		[learning rate: 0.0055443]
	Learning Rate: 0.00554429
	LOSS [training: 3.313407941446372 | validation: 3.3358984571361345]
	TIME [epoch: 7.75 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3260975043541676		[learning rate: 0.0055312]
	Learning Rate: 0.00553121
	LOSS [training: 3.3260975043541676 | validation: 3.3202524778092695]
	TIME [epoch: 7.75 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2903916078395268		[learning rate: 0.0055182]
	Learning Rate: 0.00551817
	LOSS [training: 3.2903916078395268 | validation: 3.299404853139075]
	TIME [epoch: 7.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2769002993249368		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 3.2769002993249368 | validation: 3.308087745975582]
	TIME [epoch: 7.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2922184465481195		[learning rate: 0.0054922]
	Learning Rate: 0.00549216
	LOSS [training: 3.2922184465481195 | validation: 3.369154556011898]
	TIME [epoch: 7.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3990579094865314		[learning rate: 0.0054792]
	Learning Rate: 0.00547921
	LOSS [training: 3.3990579094865314 | validation: 3.8545591251796507]
	TIME [epoch: 7.76 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9250238335381877		[learning rate: 0.0054663]
	Learning Rate: 0.00546629
	LOSS [training: 3.9250238335381877 | validation: 3.8532645632075955]
	TIME [epoch: 7.73 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.618603430591285		[learning rate: 0.0054534]
	Learning Rate: 0.00545339
	LOSS [training: 3.618603430591285 | validation: 3.4834690774478476]
	TIME [epoch: 7.79 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4207320880781285		[learning rate: 0.0054405]
	Learning Rate: 0.00544053
	LOSS [training: 3.4207320880781285 | validation: 3.4581506113658]
	TIME [epoch: 7.75 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.384984767140985		[learning rate: 0.0054277]
	Learning Rate: 0.00542769
	LOSS [training: 3.384984767140985 | validation: 3.4180694680189982]
	TIME [epoch: 7.74 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3521844532472542		[learning rate: 0.0054149]
	Learning Rate: 0.00541489
	LOSS [training: 3.3521844532472542 | validation: 3.375554567044059]
	TIME [epoch: 7.74 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.317077299917499		[learning rate: 0.0054021]
	Learning Rate: 0.00540212
	LOSS [training: 3.317077299917499 | validation: 3.3398291660760435]
	TIME [epoch: 7.73 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2981211347045347		[learning rate: 0.0053894]
	Learning Rate: 0.00538938
	LOSS [training: 3.2981211347045347 | validation: 3.3224869183931585]
	TIME [epoch: 7.79 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2905080227852435		[learning rate: 0.0053767]
	Learning Rate: 0.00537666
	LOSS [training: 3.2905080227852435 | validation: 3.3384624753523084]
	TIME [epoch: 7.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2982507550139366		[learning rate: 0.005364]
	Learning Rate: 0.00536398
	LOSS [training: 3.2982507550139366 | validation: 3.318941159730535]
	TIME [epoch: 7.73 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281893408211709		[learning rate: 0.0053513]
	Learning Rate: 0.00535133
	LOSS [training: 3.281893408211709 | validation: 3.2856716246690296]
	TIME [epoch: 7.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265990284293856		[learning rate: 0.0053387]
	Learning Rate: 0.00533871
	LOSS [training: 3.265990284293856 | validation: 3.3340817039397495]
	TIME [epoch: 7.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.527738356568746		[learning rate: 0.0053261]
	Learning Rate: 0.00532611
	LOSS [training: 3.527738356568746 | validation: 4.586513535766668]
	TIME [epoch: 7.78 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.554224538930001		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 4.554224538930001 | validation: 4.624084792657493]
	TIME [epoch: 7.73 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.551542464204677		[learning rate: 0.005301]
	Learning Rate: 0.00530101
	LOSS [training: 4.551542464204677 | validation: 4.586679474068493]
	TIME [epoch: 7.72 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.43806798657922		[learning rate: 0.0052885]
	Learning Rate: 0.00528851
	LOSS [training: 4.43806798657922 | validation: 4.286333489366884]
	TIME [epoch: 7.72 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.037786224965098		[learning rate: 0.005276]
	Learning Rate: 0.00527604
	LOSS [training: 4.037786224965098 | validation: 3.691372130463056]
	TIME [epoch: 7.74 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6103053048785103		[learning rate: 0.0052636]
	Learning Rate: 0.00526359
	LOSS [training: 3.6103053048785103 | validation: 3.5454092549440404]
	TIME [epoch: 7.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4424780857447836		[learning rate: 0.0052512]
	Learning Rate: 0.00525117
	LOSS [training: 3.4424780857447836 | validation: 3.4538039280537958]
	TIME [epoch: 7.72 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3841611319078404		[learning rate: 0.0052388]
	Learning Rate: 0.00523879
	LOSS [training: 3.3841611319078404 | validation: 3.428136557671663]
	TIME [epoch: 7.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34625926133726		[learning rate: 0.0052264]
	Learning Rate: 0.00522643
	LOSS [training: 3.34625926133726 | validation: 3.403582121606706]
	TIME [epoch: 7.73 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329317259180389		[learning rate: 0.0052141]
	Learning Rate: 0.0052141
	LOSS [training: 3.329317259180389 | validation: 3.3578619918620713]
	TIME [epoch: 7.75 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2932062960722677		[learning rate: 0.0052018]
	Learning Rate: 0.0052018
	LOSS [training: 3.2932062960722677 | validation: 3.304721610824026]
	TIME [epoch: 7.76 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.289902437328398		[learning rate: 0.0051895]
	Learning Rate: 0.00518953
	LOSS [training: 3.289902437328398 | validation: 3.3122848502679414]
	TIME [epoch: 7.73 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2956945062559058		[learning rate: 0.0051773]
	Learning Rate: 0.00517729
	LOSS [training: 3.2956945062559058 | validation: 3.299337877246484]
	TIME [epoch: 7.74 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281055131403012		[learning rate: 0.0051651]
	Learning Rate: 0.00516508
	LOSS [training: 3.281055131403012 | validation: 3.349464105869288]
	TIME [epoch: 7.72 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3371987942412398		[learning rate: 0.0051529]
	Learning Rate: 0.00515289
	LOSS [training: 3.3371987942412398 | validation: 3.3688160752136973]
	TIME [epoch: 7.77 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323389459082863		[learning rate: 0.0051407]
	Learning Rate: 0.00514074
	LOSS [training: 3.323389459082863 | validation: 3.3235667334609573]
	TIME [epoch: 7.73 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2982012851241596		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 3.2982012851241596 | validation: 3.3620356708105525]
	TIME [epoch: 7.73 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325238496903407		[learning rate: 0.0051165]
	Learning Rate: 0.00511652
	LOSS [training: 3.325238496903407 | validation: 3.3285316438468913]
	TIME [epoch: 7.72 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2966137922609144		[learning rate: 0.0051044]
	Learning Rate: 0.00510445
	LOSS [training: 3.2966137922609144 | validation: 3.2980817382648713]
	TIME [epoch: 7.72 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2731624067632143		[learning rate: 0.0050924]
	Learning Rate: 0.00509241
	LOSS [training: 3.2731624067632143 | validation: 3.296623728370443]
	TIME [epoch: 7.78 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2641915406685635		[learning rate: 0.0050804]
	Learning Rate: 0.00508039
	LOSS [training: 3.2641915406685635 | validation: 3.284268516263171]
	TIME [epoch: 7.74 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2578226052252295		[learning rate: 0.0050684]
	Learning Rate: 0.00506841
	LOSS [training: 3.2578226052252295 | validation: 3.28022887303824]
	TIME [epoch: 7.74 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347384192095222		[learning rate: 0.0050565]
	Learning Rate: 0.00505645
	LOSS [training: 3.347384192095222 | validation: 3.4394916697852276]
	TIME [epoch: 7.72 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342361639479161		[learning rate: 0.0050445]
	Learning Rate: 0.00504453
	LOSS [training: 3.342361639479161 | validation: 3.3182854753870483]
	TIME [epoch: 7.73 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284800335894868		[learning rate: 0.0050326]
	Learning Rate: 0.00503263
	LOSS [training: 3.284800335894868 | validation: 3.2924607641645016]
	TIME [epoch: 7.78 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34885589259457		[learning rate: 0.0050208]
	Learning Rate: 0.00502076
	LOSS [training: 3.34885589259457 | validation: 3.324287333723351]
	TIME [epoch: 7.73 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275766487314567		[learning rate: 0.0050089]
	Learning Rate: 0.00500891
	LOSS [training: 3.275766487314567 | validation: 3.3139099178312676]
	TIME [epoch: 7.73 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.311934339553308		[learning rate: 0.0049971]
	Learning Rate: 0.0049971
	LOSS [training: 3.311934339553308 | validation: 3.338496283069073]
	TIME [epoch: 7.72 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2764464476984503		[learning rate: 0.0049853]
	Learning Rate: 0.00498531
	LOSS [training: 3.2764464476984503 | validation: 3.2793562699105165]
	TIME [epoch: 7.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261730123691653		[learning rate: 0.0049736]
	Learning Rate: 0.00497355
	LOSS [training: 3.261730123691653 | validation: 3.2726673999037956]
	TIME [epoch: 7.78 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2718246778003226		[learning rate: 0.0049618]
	Learning Rate: 0.00496182
	LOSS [training: 3.2718246778003226 | validation: 3.2880333071383525]
	TIME [epoch: 7.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2785447271410817		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 3.2785447271410817 | validation: 3.280272382063197]
	TIME [epoch: 7.73 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.37261059688508		[learning rate: 0.0049384]
	Learning Rate: 0.00493844
	LOSS [training: 3.37261059688508 | validation: 3.321698880181881]
	TIME [epoch: 7.73 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.35982242665052		[learning rate: 0.0049268]
	Learning Rate: 0.00492679
	LOSS [training: 3.35982242665052 | validation: 3.3814641449008422]
	TIME [epoch: 7.75 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.483102780709917		[learning rate: 0.0049152]
	Learning Rate: 0.00491517
	LOSS [training: 3.483102780709917 | validation: 3.4625569428753096]
	TIME [epoch: 7.77 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5197529896732664		[learning rate: 0.0049036]
	Learning Rate: 0.00490357
	LOSS [training: 3.5197529896732664 | validation: 3.4544652610007667]
	TIME [epoch: 7.73 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.565569135552793		[learning rate: 0.004892]
	Learning Rate: 0.00489201
	LOSS [training: 3.565569135552793 | validation: 3.5533857048908573]
	TIME [epoch: 7.73 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.658756201135587		[learning rate: 0.0048805]
	Learning Rate: 0.00488047
	LOSS [training: 3.658756201135587 | validation: 3.6511464137594567]
	TIME [epoch: 7.73 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.686030573289273		[learning rate: 0.004869]
	Learning Rate: 0.00486896
	LOSS [training: 3.686030573289273 | validation: 3.5673529523051437]
	TIME [epoch: 7.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.618307487501668		[learning rate: 0.0048575]
	Learning Rate: 0.00485747
	LOSS [training: 3.618307487501668 | validation: 3.526713218490734]
	TIME [epoch: 7.77 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6114849435517864		[learning rate: 0.004846]
	Learning Rate: 0.00484601
	LOSS [training: 3.6114849435517864 | validation: 3.6868722954684543]
	TIME [epoch: 7.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.649951674586747		[learning rate: 0.0048346]
	Learning Rate: 0.00483458
	LOSS [training: 3.649951674586747 | validation: 3.6231778627274425]
	TIME [epoch: 7.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6298037615142933		[learning rate: 0.0048232]
	Learning Rate: 0.00482318
	LOSS [training: 3.6298037615142933 | validation: 3.5155014826542166]
	TIME [epoch: 7.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5873191293997135		[learning rate: 0.0048118]
	Learning Rate: 0.0048118
	LOSS [training: 3.5873191293997135 | validation: 3.5223460760720067]
	TIME [epoch: 7.78 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.646145771815841		[learning rate: 0.0048005]
	Learning Rate: 0.00480045
	LOSS [training: 3.646145771815841 | validation: 3.622486234023169]
	TIME [epoch: 7.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.75708480226091		[learning rate: 0.0047891]
	Learning Rate: 0.00478913
	LOSS [training: 3.75708480226091 | validation: 3.7391344637876527]
	TIME [epoch: 7.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9993330969843406		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 3.9993330969843406 | validation: 3.9490228385114134]
	TIME [epoch: 7.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.973119943255589		[learning rate: 0.0047666]
	Learning Rate: 0.00476656
	LOSS [training: 3.973119943255589 | validation: 3.7048175599609108]
	TIME [epoch: 7.75 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824689820352348		[learning rate: 0.0047553]
	Learning Rate: 0.00475532
	LOSS [training: 3.824689820352348 | validation: 3.7413462521182312]
	TIME [epoch: 7.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9483202218981504		[learning rate: 0.0047441]
	Learning Rate: 0.0047441
	LOSS [training: 3.9483202218981504 | validation: 4.078411846772864]
	TIME [epoch: 7.75 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153415901169471		[learning rate: 0.0047329]
	Learning Rate: 0.00473291
	LOSS [training: 4.153415901169471 | validation: 4.0110877912175695]
	TIME [epoch: 7.73 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.267379199150917		[learning rate: 0.0047217]
	Learning Rate: 0.00472175
	LOSS [training: 4.267379199150917 | validation: 4.744886454945954]
	TIME [epoch: 7.72 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.436832489123482		[learning rate: 0.0047106]
	Learning Rate: 0.00471061
	LOSS [training: 4.436832489123482 | validation: 3.9446659698729363]
	TIME [epoch: 7.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9427319751373218		[learning rate: 0.0046995]
	Learning Rate: 0.0046995
	LOSS [training: 3.9427319751373218 | validation: 3.874991048262411]
	TIME [epoch: 7.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.003716387263822		[learning rate: 0.0046884]
	Learning Rate: 0.00468841
	LOSS [training: 4.003716387263822 | validation: 4.0054549441631755]
	TIME [epoch: 7.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.066205573602003		[learning rate: 0.0046774]
	Learning Rate: 0.00467735
	LOSS [training: 4.066205573602003 | validation: 3.889832014289967]
	TIME [epoch: 7.73 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.966187251080304		[learning rate: 0.0046663]
	Learning Rate: 0.00466632
	LOSS [training: 3.966187251080304 | validation: 3.8257871977666147]
	TIME [epoch: 7.73 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.960202634256839		[learning rate: 0.0046553]
	Learning Rate: 0.00465531
	LOSS [training: 3.960202634256839 | validation: 3.819714615663994]
	TIME [epoch: 7.73 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.929995172013114		[learning rate: 0.0046443]
	Learning Rate: 0.00464433
	LOSS [training: 3.929995172013114 | validation: 3.8517367090644337]
	TIME [epoch: 7.79 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135395398902995		[learning rate: 0.0046334]
	Learning Rate: 0.00463338
	LOSS [training: 4.135395398902995 | validation: 4.393782402475195]
	TIME [epoch: 7.75 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7917209523759485		[learning rate: 0.0046224]
	Learning Rate: 0.00462245
	LOSS [training: 4.7917209523759485 | validation: 5.0623651286966656]
	TIME [epoch: 7.72 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.021776454109508		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 5.021776454109508 | validation: 4.832431665677461]
	TIME [epoch: 7.74 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.747964191176303		[learning rate: 0.0046007]
	Learning Rate: 0.00460066
	LOSS [training: 4.747964191176303 | validation: 4.586389327818206]
	TIME [epoch: 7.74 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.492482431513509		[learning rate: 0.0045898]
	Learning Rate: 0.00458981
	LOSS [training: 4.492482431513509 | validation: 4.338418366922522]
	TIME [epoch: 7.77 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.296202937731988		[learning rate: 0.004579]
	Learning Rate: 0.00457899
	LOSS [training: 4.296202937731988 | validation: 4.1913473603688995]
	TIME [epoch: 7.74 sec]
EPOCH 382/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.31841430209593		[learning rate: 0.0045682]
	Learning Rate: 0.00456818
	LOSS [training: 4.31841430209593 | validation: 5.102609557158356]
	TIME [epoch: 117 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.65410162943093		[learning rate: 0.0045574]
	Learning Rate: 0.00455741
	LOSS [training: 4.65410162943093 | validation: 4.446312505372472]
	TIME [epoch: 7.77 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.38786538845011		[learning rate: 0.0045467]
	Learning Rate: 0.00454666
	LOSS [training: 4.38786538845011 | validation: 4.2503613424518685]
	TIME [epoch: 7.73 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2386537689693355		[learning rate: 0.0045359]
	Learning Rate: 0.00453593
	LOSS [training: 4.2386537689693355 | validation: 4.364719347797385]
	TIME [epoch: 7.76 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.404483499423163		[learning rate: 0.0045252]
	Learning Rate: 0.00452523
	LOSS [training: 4.404483499423163 | validation: 4.332142509316531]
	TIME [epoch: 7.72 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.345609828227906		[learning rate: 0.0045146]
	Learning Rate: 0.00451456
	LOSS [training: 4.345609828227906 | validation: 4.146010801433237]
	TIME [epoch: 7.72 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.111786081027683		[learning rate: 0.0045039]
	Learning Rate: 0.00450391
	LOSS [training: 4.111786081027683 | validation: 3.9817115946682513]
	TIME [epoch: 7.72 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.015605714557731		[learning rate: 0.0044933]
	Learning Rate: 0.00449329
	LOSS [training: 4.015605714557731 | validation: 3.9923489596997976]
	TIME [epoch: 7.73 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.066113627542234		[learning rate: 0.0044827]
	Learning Rate: 0.00448269
	LOSS [training: 4.066113627542234 | validation: 3.9531287100559975]
	TIME [epoch: 7.77 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0760524029933585		[learning rate: 0.0044721]
	Learning Rate: 0.00447211
	LOSS [training: 4.0760524029933585 | validation: 4.2277293234967]
	TIME [epoch: 7.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.358600280105654		[learning rate: 0.0044616]
	Learning Rate: 0.00446156
	LOSS [training: 4.358600280105654 | validation: 4.437998157029721]
	TIME [epoch: 7.72 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.461170293764157		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 4.461170293764157 | validation: 4.529200168814761]
	TIME [epoch: 7.71 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.475716113756417		[learning rate: 0.0044405]
	Learning Rate: 0.00444054
	LOSS [training: 4.475716113756417 | validation: 4.3966099013259825]
	TIME [epoch: 7.72 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.489432782651077		[learning rate: 0.0044301]
	Learning Rate: 0.00443007
	LOSS [training: 4.489432782651077 | validation: 4.600277786763245]
	TIME [epoch: 7.75 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.584132139268958		[learning rate: 0.0044196]
	Learning Rate: 0.00441962
	LOSS [training: 4.584132139268958 | validation: 4.566549069182848]
	TIME [epoch: 7.74 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.562316763945729		[learning rate: 0.0044092]
	Learning Rate: 0.00440919
	LOSS [training: 4.562316763945729 | validation: 4.564404637626301]
	TIME [epoch: 7.72 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.588905648639853		[learning rate: 0.0043988]
	Learning Rate: 0.00439879
	LOSS [training: 4.588905648639853 | validation: 4.555051969516853]
	TIME [epoch: 7.71 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5892669919935205		[learning rate: 0.0043884]
	Learning Rate: 0.00438841
	LOSS [training: 4.5892669919935205 | validation: 4.587363446338199]
	TIME [epoch: 7.71 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.633194912179282		[learning rate: 0.0043781]
	Learning Rate: 0.00437806
	LOSS [training: 4.633194912179282 | validation: 4.6496034279868645]
	TIME [epoch: 7.73 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.625349653500277		[learning rate: 0.0043677]
	Learning Rate: 0.00436774
	LOSS [training: 4.625349653500277 | validation: 4.644604628102771]
	TIME [epoch: 7.76 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.589011986659961		[learning rate: 0.0043574]
	Learning Rate: 0.00435743
	LOSS [training: 4.589011986659961 | validation: 4.617101629232106]
	TIME [epoch: 7.71 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.643412776538599		[learning rate: 0.0043472]
	Learning Rate: 0.00434715
	LOSS [training: 4.643412776538599 | validation: 4.677081259126742]
	TIME [epoch: 7.71 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.615386936363187		[learning rate: 0.0043369]
	Learning Rate: 0.0043369
	LOSS [training: 4.615386936363187 | validation: 4.543387380354956]
	TIME [epoch: 7.71 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.580944180382538		[learning rate: 0.0043267]
	Learning Rate: 0.00432667
	LOSS [training: 4.580944180382538 | validation: 4.636289317399798]
	TIME [epoch: 7.72 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.731422814181821		[learning rate: 0.0043165]
	Learning Rate: 0.00431646
	LOSS [training: 4.731422814181821 | validation: 5.1475439065783615]
	TIME [epoch: 7.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.077261619735215		[learning rate: 0.0043063]
	Learning Rate: 0.00430628
	LOSS [training: 5.077261619735215 | validation: 5.17826425505824]
	TIME [epoch: 7.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.130273985710627		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 5.130273985710627 | validation: 5.120680058225663]
	TIME [epoch: 7.71 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.086432320758147		[learning rate: 0.004286]
	Learning Rate: 0.00428599
	LOSS [training: 5.086432320758147 | validation: 5.211572717002683]
	TIME [epoch: 7.71 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.156188330660218		[learning rate: 0.0042759]
	Learning Rate: 0.00427588
	LOSS [training: 5.156188330660218 | validation: 5.326772838076865]
	TIME [epoch: 7.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.27281870269919		[learning rate: 0.0042658]
	Learning Rate: 0.0042658
	LOSS [training: 5.27281870269919 | validation: 5.339027057047122]
	TIME [epoch: 7.74 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.237848350396129		[learning rate: 0.0042557]
	Learning Rate: 0.00425573
	LOSS [training: 5.237848350396129 | validation: 5.229105201008354]
	TIME [epoch: 7.74 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.097095916516533		[learning rate: 0.0042457]
	Learning Rate: 0.00424569
	LOSS [training: 5.097095916516533 | validation: 5.022003598335795]
	TIME [epoch: 7.71 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.064751887645912		[learning rate: 0.0042357]
	Learning Rate: 0.00423568
	LOSS [training: 5.064751887645912 | validation: 5.08642634943825]
	TIME [epoch: 7.7 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.763491564049134		[learning rate: 0.0042257]
	Learning Rate: 0.00422569
	LOSS [training: 4.763491564049134 | validation: 4.638656142683657]
	TIME [epoch: 7.71 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.586984807399586		[learning rate: 0.0042157]
	Learning Rate: 0.00421572
	LOSS [training: 4.586984807399586 | validation: 4.751342018790871]
	TIME [epoch: 7.75 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.658892469648961		[learning rate: 0.0042058]
	Learning Rate: 0.00420578
	LOSS [training: 4.658892469648961 | validation: 4.947015581938104]
	TIME [epoch: 7.77 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.100851990465451		[learning rate: 0.0041959]
	Learning Rate: 0.00419585
	LOSS [training: 5.100851990465451 | validation: 5.350673416550176]
	TIME [epoch: 7.71 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.30323548106618		[learning rate: 0.004186]
	Learning Rate: 0.00418596
	LOSS [training: 5.30323548106618 | validation: 5.364050440506631]
	TIME [epoch: 7.72 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.161087223120754		[learning rate: 0.0041761]
	Learning Rate: 0.00417608
	LOSS [training: 5.161087223120754 | validation: 4.7368228051506645]
	TIME [epoch: 7.72 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.669650339876055		[learning rate: 0.0041662]
	Learning Rate: 0.00416623
	LOSS [training: 4.669650339876055 | validation: 4.526917090675566]
	TIME [epoch: 7.72 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.552650139692236		[learning rate: 0.0041564]
nan encountered in epoch 421 (validation loss).
	Learning Rate: 0.00415641
	LOSS [training: 4.552650139692236 | validation: nan]
	TIME [epoch: 7.77 sec]
EPOCH 423/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.0010000000000000002
		[batch 4/4] avg loss: 4.503096602392191		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 4.503096602392191 | validation: 4.451650901960424]
	TIME [epoch: 129 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.524808643371549		[learning rate: 0.0041368]
	Learning Rate: 0.00413682
	LOSS [training: 4.524808643371549 | validation: 4.491405398460479]
	TIME [epoch: 7.77 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.561530837055211		[learning rate: 0.0041271]
	Learning Rate: 0.00412706
	LOSS [training: 4.561530837055211 | validation: 4.482294553752495]
	TIME [epoch: 7.72 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.554991041350479		[learning rate: 0.0041173]
	Learning Rate: 0.00411733
	LOSS [training: 4.554991041350479 | validation: 4.477725390129493]
	TIME [epoch: 7.72 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.516056223291644		[learning rate: 0.0041076]
	Learning Rate: 0.00410762
	LOSS [training: 4.516056223291644 | validation: 4.45221251259374]
	TIME [epoch: 7.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.520302078358142		[learning rate: 0.0040979]
	Learning Rate: 0.00409793
	LOSS [training: 4.520302078358142 | validation: 4.465136861710567]
	TIME [epoch: 7.76 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.505188005238534		[learning rate: 0.0040883]
	Learning Rate: 0.00408826
	LOSS [training: 4.505188005238534 | validation: 4.439147342288667]
	TIME [epoch: 7.72 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4793742998766675		[learning rate: 0.0040786]
	Learning Rate: 0.00407862
	LOSS [training: 4.4793742998766675 | validation: 4.4673980533095925]
	TIME [epoch: 7.72 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.646215196610953		[learning rate: 0.004069]
	Learning Rate: 0.004069
	LOSS [training: 4.646215196610953 | validation: 4.868470408775254]
	TIME [epoch: 7.72 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822540570980483		[learning rate: 0.0040594]
	Learning Rate: 0.0040594
	LOSS [training: 4.822540570980483 | validation: 4.944998487088201]
	TIME [epoch: 7.75 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.793548002757434		[learning rate: 0.0040498]
	Learning Rate: 0.00404982
	LOSS [training: 4.793548002757434 | validation: 4.881046523409843]
	TIME [epoch: 7.76 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.234532498173104		[learning rate: 0.0040403]
	Learning Rate: 0.00404027
	LOSS [training: 5.234532498173104 | validation: 4.925899123527333]
	TIME [epoch: 7.71 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837418554799018		[learning rate: 0.0040307]
	Learning Rate: 0.00403074
	LOSS [training: 4.837418554799018 | validation: 4.913028213993128]
	TIME [epoch: 7.72 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8613994177002295		[learning rate: 0.0040212]
	Learning Rate: 0.00402123
	LOSS [training: 4.8613994177002295 | validation: 5.0947171244026475]
	TIME [epoch: 7.72 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3875339763048125		[learning rate: 0.0040117]
	Learning Rate: 0.00401175
	LOSS [training: 5.3875339763048125 | validation: 5.255506713581923]
	TIME [epoch: 7.76 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2358595719593835		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 5.2358595719593835 | validation: 5.186109298861004]
	TIME [epoch: 7.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.227919811551915		[learning rate: 0.0039928]
	Learning Rate: 0.00399284
	LOSS [training: 5.227919811551915 | validation: 5.249379426929827]
	TIME [epoch: 7.72 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.306900178837639		[learning rate: 0.0039834]
	Learning Rate: 0.00398342
	LOSS [training: 5.306900178837639 | validation: 5.701946665438432]
	TIME [epoch: 7.72 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.439314553075232		[learning rate: 0.003974]
	Learning Rate: 0.00397403
	LOSS [training: 5.439314553075232 | validation: 5.306166556321056]
	TIME [epoch: 7.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.522101413107875		[learning rate: 0.0039647]
	Learning Rate: 0.00396465
	LOSS [training: 5.522101413107875 | validation: 5.383873816768711]
	TIME [epoch: 7.76 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3287911101201875		[learning rate: 0.0039553]
	Learning Rate: 0.0039553
	LOSS [training: 5.3287911101201875 | validation: 5.324385180232524]
	TIME [epoch: 7.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.25851751048268		[learning rate: 0.003946]
	Learning Rate: 0.00394597
	LOSS [training: 5.25851751048268 | validation: 5.2920797739784575]
	TIME [epoch: 7.72 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.250363073905511		[learning rate: 0.0039367]
	Learning Rate: 0.00393666
	LOSS [training: 5.250363073905511 | validation: 4.947664234918202]
	TIME [epoch: 7.71 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2420025087819955		[learning rate: 0.0039274]
	Learning Rate: 0.00392738
	LOSS [training: 5.2420025087819955 | validation: 5.657776808218248]
	TIME [epoch: 7.72 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.765252099341435		[learning rate: 0.0039181]
	Learning Rate: 0.00391811
	LOSS [training: 5.765252099341435 | validation: 5.664872899988681]
	TIME [epoch: 7.77 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.701803928921492		[learning rate: 0.0039089]
	Learning Rate: 0.00390887
	LOSS [training: 5.701803928921492 | validation: 5.5583402223945795]
	TIME [epoch: 7.73 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.425002901529028		[learning rate: 0.0038996]
	Learning Rate: 0.00389965
	LOSS [training: 5.425002901529028 | validation: 5.344056773526999]
	TIME [epoch: 7.71 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.163195816103421		[learning rate: 0.0038905]
	Learning Rate: 0.00389045
	LOSS [training: 5.163195816103421 | validation: 4.931482608682153]
	TIME [epoch: 7.71 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939018751856889		[learning rate: 0.0038813]
	Learning Rate: 0.00388127
	LOSS [training: 4.939018751856889 | validation: 4.812135897319176]
	TIME [epoch: 7.72 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.832248136880593		[learning rate: 0.0038721]
	Learning Rate: 0.00387212
	LOSS [training: 4.832248136880593 | validation: 4.714160380262596]
	TIME [epoch: 7.76 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.817770363064216		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 4.817770363064216 | validation: 4.737154192125847]
	TIME [epoch: 7.73 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814228897404279		[learning rate: 0.0038539]
	Learning Rate: 0.00385387
	LOSS [training: 4.814228897404279 | validation: 4.627178082184016]
	TIME [epoch: 7.72 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.716196353946896		[learning rate: 0.0038448]
	Learning Rate: 0.00384478
	LOSS [training: 4.716196353946896 | validation: 4.501201075071652]
	TIME [epoch: 7.72 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.632111506140518		[learning rate: 0.0038357]
	Learning Rate: 0.00383571
	LOSS [training: 4.632111506140518 | validation: 4.224478064639893]
	TIME [epoch: 7.73 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.664350304366088		[learning rate: 0.0038267]
	Learning Rate: 0.00382667
	LOSS [training: 4.664350304366088 | validation: 5.06695490421568]
	TIME [epoch: 7.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.735902608123917		[learning rate: 0.0038176]
	Learning Rate: 0.00381764
	LOSS [training: 4.735902608123917 | validation: 5.358610033588485]
	TIME [epoch: 7.72 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.112644980765705		[learning rate: 0.0038086]
	Learning Rate: 0.00380863
	LOSS [training: 5.112644980765705 | validation: 4.700553397332287]
	TIME [epoch: 7.72 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6405759723489455		[learning rate: 0.0037996]
	Learning Rate: 0.00379965
	LOSS [training: 4.6405759723489455 | validation: 4.483940452485156]
	TIME [epoch: 7.71 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.325124441400842		[learning rate: 0.0037907]
	Learning Rate: 0.00379069
	LOSS [training: 4.325124441400842 | validation: 4.374298353179626]
	TIME [epoch: 7.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.43857284205917		[learning rate: 0.0037817]
	Learning Rate: 0.00378175
	LOSS [training: 4.43857284205917 | validation: 4.540860723112468]
	TIME [epoch: 7.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.447973487959741		[learning rate: 0.0037728]
	Learning Rate: 0.00377283
	LOSS [training: 4.447973487959741 | validation: 4.523260273079084]
	TIME [epoch: 7.71 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6199674614889865		[learning rate: 0.0037639]
	Learning Rate: 0.00376393
	LOSS [training: 4.6199674614889865 | validation: 4.6234743982824735]
	TIME [epoch: 7.73 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.702238621531434		[learning rate: 0.003755]
	Learning Rate: 0.00375505
	LOSS [training: 4.702238621531434 | validation: 4.741226377639197]
	TIME [epoch: 7.73 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.716066480538956		[learning rate: 0.0037462]
	Learning Rate: 0.00374619
	LOSS [training: 4.716066480538956 | validation: 4.806193459103559]
	TIME [epoch: 7.72 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.161454251650843		[learning rate: 0.0037374]
	Learning Rate: 0.00373735
	LOSS [training: 5.161454251650843 | validation: 4.994237249598923]
	TIME [epoch: 7.76 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.084814683695804		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 5.084814683695804 | validation: 5.026894187176248]
	TIME [epoch: 7.71 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.238428751102801		[learning rate: 0.0037197]
	Learning Rate: 0.00371974
	LOSS [training: 5.238428751102801 | validation: 5.338090589217773]
	TIME [epoch: 7.71 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.078902072629191		[learning rate: 0.003711]
	Learning Rate: 0.00371097
	LOSS [training: 5.078902072629191 | validation: 4.710484260716862]
	TIME [epoch: 7.72 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.797153393868836		[learning rate: 0.0037022]
	Learning Rate: 0.00370221
	LOSS [training: 4.797153393868836 | validation: 4.9559148192804]
	TIME [epoch: 7.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028388690592559		[learning rate: 0.0036935]
	Learning Rate: 0.00369348
	LOSS [training: 5.028388690592559 | validation: 5.142200149120399]
	TIME [epoch: 7.77 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.057807565632514		[learning rate: 0.0036848]
	Learning Rate: 0.00368477
	LOSS [training: 5.057807565632514 | validation: 4.627404007008689]
	TIME [epoch: 7.71 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.643342439521928		[learning rate: 0.0036761]
	Learning Rate: 0.00367608
	LOSS [training: 4.643342439521928 | validation: 4.816635752318932]
	TIME [epoch: 7.71 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.210404736120367		[learning rate: 0.0036674]
	Learning Rate: 0.00366741
	LOSS [training: 5.210404736120367 | validation: 5.319117742893148]
	TIME [epoch: 7.72 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.363297181326011		[learning rate: 0.0036588]
	Learning Rate: 0.00365875
	LOSS [training: 5.363297181326011 | validation: 5.24905284391583]
	TIME [epoch: 7.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.235542173547069		[learning rate: 0.0036501]
	Learning Rate: 0.00365012
	LOSS [training: 5.235542173547069 | validation: 4.959523777638031]
	TIME [epoch: 7.76 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.741119133556784		[learning rate: 0.0036415]
	Learning Rate: 0.00364151
	LOSS [training: 4.741119133556784 | validation: 4.872590835707107]
	TIME [epoch: 7.72 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.707606896478339		[learning rate: 0.0036329]
	Learning Rate: 0.00363292
	LOSS [training: 4.707606896478339 | validation: 4.575763340561018]
	TIME [epoch: 7.72 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.562655689296206		[learning rate: 0.0036244]
	Learning Rate: 0.00362436
	LOSS [training: 4.562655689296206 | validation: 4.626838742685428]
	TIME [epoch: 7.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.567644735773213		[learning rate: 0.0036158]
	Learning Rate: 0.00361581
	LOSS [training: 4.567644735773213 | validation: 4.5614121573865996]
	TIME [epoch: 7.74 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.540245287781326		[learning rate: 0.0036073]
	Learning Rate: 0.00360728
	LOSS [training: 4.540245287781326 | validation: 4.530239790913761]
	TIME [epoch: 7.75 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.609467235789404		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 4.609467235789404 | validation: 4.445980853267642]
	TIME [epoch: 7.72 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.488871691738824		[learning rate: 0.0035903]
	Learning Rate: 0.00359028
	LOSS [training: 4.488871691738824 | validation: 4.520158370054431]
	TIME [epoch: 7.72 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.548866950475048		[learning rate: 0.0035818]
	Learning Rate: 0.00358181
	LOSS [training: 4.548866950475048 | validation: 4.488108506022529]
	TIME [epoch: 7.72 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.553535453524746		[learning rate: 0.0035734]
	Learning Rate: 0.00357336
	LOSS [training: 4.553535453524746 | validation: 4.358620968384785]
	TIME [epoch: 7.74 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3861519866482706		[learning rate: 0.0035649]
	Learning Rate: 0.00356493
	LOSS [training: 4.3861519866482706 | validation: 4.348608570063793]
	TIME [epoch: 7.75 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.346619163345503		[learning rate: 0.0035565]
	Learning Rate: 0.00355652
	LOSS [training: 4.346619163345503 | validation: 4.329856709619884]
	TIME [epoch: 7.72 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3207611200741685		[learning rate: 0.0035481]
	Learning Rate: 0.00354813
	LOSS [training: 4.3207611200741685 | validation: 4.30161496356733]
	TIME [epoch: 7.72 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.287808205790554		[learning rate: 0.0035398]
	Learning Rate: 0.00353976
	LOSS [training: 4.287808205790554 | validation: 4.304653757421008]
	TIME [epoch: 7.72 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.291425114128543		[learning rate: 0.0035314]
	Learning Rate: 0.00353141
	LOSS [training: 4.291425114128543 | validation: 4.240594741489176]
	TIME [epoch: 7.75 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249762340602445		[learning rate: 0.0035231]
	Learning Rate: 0.00352308
	LOSS [training: 4.249762340602445 | validation: 4.22617567463878]
	TIME [epoch: 7.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.204767104789079		[learning rate: 0.0035148]
	Learning Rate: 0.00351477
	LOSS [training: 4.204767104789079 | validation: 4.20836191219884]
	TIME [epoch: 7.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.333705255535985		[learning rate: 0.0035065]
	Learning Rate: 0.00350648
	LOSS [training: 4.333705255535985 | validation: 4.639121349000227]
	TIME [epoch: 7.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.526313540198693		[learning rate: 0.0034982]
	Learning Rate: 0.00349821
	LOSS [training: 4.526313540198693 | validation: 4.681577866019606]
	TIME [epoch: 7.72 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.143882034157646		[learning rate: 0.00349]
	Learning Rate: 0.00348996
	LOSS [training: 5.143882034157646 | validation: 5.0555127155193365]
	TIME [epoch: 7.76 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.98503403906549		[learning rate: 0.0034817]
	Learning Rate: 0.00348173
	LOSS [training: 4.98503403906549 | validation: 4.587751277576231]
	TIME [epoch: 7.74 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.709384631045115		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 4.709384631045115 | validation: 4.537278568131294]
	TIME [epoch: 7.72 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.65450240011336		[learning rate: 0.0034653]
	Learning Rate: 0.00346532
	LOSS [training: 4.65450240011336 | validation: 4.3636956277892605]
	TIME [epoch: 7.72 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.404849534632849		[learning rate: 0.0034571]
	Learning Rate: 0.00345715
	LOSS [training: 4.404849534632849 | validation: 4.571975686642325]
	TIME [epoch: 7.72 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.380999928886213		[learning rate: 0.003449]
	Learning Rate: 0.00344899
	LOSS [training: 4.380999928886213 | validation: 4.053059273583447]
	TIME [epoch: 7.76 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0549191388817425		[learning rate: 0.0034409]
	Learning Rate: 0.00344086
	LOSS [training: 4.0549191388817425 | validation: 4.038717819671478]
	TIME [epoch: 7.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.026264832776471		[learning rate: 0.0034327]
	Learning Rate: 0.00343274
	LOSS [training: 4.026264832776471 | validation: 4.041020711192341]
	TIME [epoch: 7.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167291836825186		[learning rate: 0.0034246]
	Learning Rate: 0.00342464
	LOSS [training: 4.167291836825186 | validation: 4.080015430200483]
	TIME [epoch: 7.72 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.06353054486262		[learning rate: 0.0034166]
	Learning Rate: 0.00341657
	LOSS [training: 4.06353054486262 | validation: 4.060088701794046]
	TIME [epoch: 7.73 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.043224510715549		[learning rate: 0.0034085]
	Learning Rate: 0.00340851
	LOSS [training: 4.043224510715549 | validation: 4.119651023592304]
	TIME [epoch: 7.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.038860278980782		[learning rate: 0.0034005]
	Learning Rate: 0.00340047
	LOSS [training: 4.038860278980782 | validation: 4.016332287920239]
	TIME [epoch: 7.75 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.030002902482842		[learning rate: 0.0033924]
	Learning Rate: 0.00339244
	LOSS [training: 4.030002902482842 | validation: 4.05154039010944]
	TIME [epoch: 7.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.092494350089986		[learning rate: 0.0033844]
	Learning Rate: 0.00338444
	LOSS [training: 4.092494350089986 | validation: 4.133881775171493]
	TIME [epoch: 7.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1142704455149595		[learning rate: 0.0033765]
	Learning Rate: 0.00337646
	LOSS [training: 4.1142704455149595 | validation: 4.053521914789878]
	TIME [epoch: 7.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.025977617783821		[learning rate: 0.0033685]
	Learning Rate: 0.0033685
	LOSS [training: 4.025977617783821 | validation: 4.064661091982375]
	TIME [epoch: 7.77 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0742873849429095		[learning rate: 0.0033605]
	Learning Rate: 0.00336055
	LOSS [training: 4.0742873849429095 | validation: 4.094335502553005]
	TIME [epoch: 7.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0447410399767785		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 4.0447410399767785 | validation: 4.135524494579469]
	TIME [epoch: 7.71 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.068244590097242		[learning rate: 0.0033447]
	Learning Rate: 0.00334471
	LOSS [training: 4.068244590097242 | validation: 4.031699012791759]
	TIME [epoch: 7.71 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.074833746374015		[learning rate: 0.0033368]
	Learning Rate: 0.00333682
	LOSS [training: 4.074833746374015 | validation: 4.114617638165584]
	TIME [epoch: 7.72 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170412313478906		[learning rate: 0.003329]
	Learning Rate: 0.00332895
	LOSS [training: 4.170412313478906 | validation: 4.162989277826429]
	TIME [epoch: 7.77 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.202779016984845		[learning rate: 0.0033211]
	Learning Rate: 0.0033211
	LOSS [training: 4.202779016984845 | validation: 4.1359199467325105]
	TIME [epoch: 7.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0674802451769105		[learning rate: 0.0033133]
	Learning Rate: 0.00331327
	LOSS [training: 4.0674802451769105 | validation: 3.975044716648463]
	TIME [epoch: 7.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.976502767233762		[learning rate: 0.0033055]
	Learning Rate: 0.00330545
	LOSS [training: 3.976502767233762 | validation: 4.094677603948768]
	TIME [epoch: 7.71 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.048156870572011		[learning rate: 0.0032977]
	Learning Rate: 0.00329765
	LOSS [training: 4.048156870572011 | validation: 4.036401690629739]
	TIME [epoch: 7.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.063307302183857		[learning rate: 0.0032899]
	Learning Rate: 0.00328988
	LOSS [training: 4.063307302183857 | validation: 4.08860292678775]
	TIME [epoch: 7.76 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.313609343625425		[learning rate: 0.0032821]
	Learning Rate: 0.00328212
	LOSS [training: 4.313609343625425 | validation: 4.351646660530903]
	TIME [epoch: 7.72 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.429227215875914		[learning rate: 0.0032744]
	Learning Rate: 0.00327437
	LOSS [training: 4.429227215875914 | validation: 4.354168601973146]
	TIME [epoch: 7.72 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3972446946835175		[learning rate: 0.0032666]
	Learning Rate: 0.00326665
	LOSS [training: 4.3972446946835175 | validation: 4.285181493092033]
	TIME [epoch: 7.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.349020070790694		[learning rate: 0.0032589]
	Learning Rate: 0.00325894
	LOSS [training: 4.349020070790694 | validation: 4.281143031281392]
	TIME [epoch: 7.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3481523302326694		[learning rate: 0.0032513]
	Learning Rate: 0.00325126
	LOSS [training: 4.3481523302326694 | validation: 4.270413638451156]
	TIME [epoch: 7.77 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.342775190974577		[learning rate: 0.0032436]
	Learning Rate: 0.00324359
	LOSS [training: 4.342775190974577 | validation: 4.278842615853966]
	TIME [epoch: 7.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.316920365259931		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 4.316920365259931 | validation: 4.244232359132052]
	TIME [epoch: 7.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300034300734849		[learning rate: 0.0032283]
	Learning Rate: 0.0032283
	LOSS [training: 4.300034300734849 | validation: 4.244363107464089]
	TIME [epoch: 7.72 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2702755288751995		[learning rate: 0.0032207]
	Learning Rate: 0.00322069
	LOSS [training: 4.2702755288751995 | validation: 4.141653946772384]
	TIME [epoch: 7.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.173429838033672		[learning rate: 0.0032131]
	Learning Rate: 0.00321309
	LOSS [training: 4.173429838033672 | validation: 4.110010158424071]
	TIME [epoch: 7.77 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.163462811332269		[learning rate: 0.0032055]
	Learning Rate: 0.00320551
	LOSS [training: 4.163462811332269 | validation: 4.114801503887627]
	TIME [epoch: 7.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151089291155283		[learning rate: 0.003198]
	Learning Rate: 0.00319795
	LOSS [training: 4.151089291155283 | validation: 4.095508997053505]
	TIME [epoch: 7.72 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142419016985487		[learning rate: 0.0031904]
	Learning Rate: 0.00319041
	LOSS [training: 4.142419016985487 | validation: 4.113249946131051]
	TIME [epoch: 7.71 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127210071031751		[learning rate: 0.0031829]
	Learning Rate: 0.00318288
	LOSS [training: 4.127210071031751 | validation: 4.027614599457668]
	TIME [epoch: 7.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.051595994447615		[learning rate: 0.0031754]
	Learning Rate: 0.00317537
	LOSS [training: 4.051595994447615 | validation: 4.027668192258373]
	TIME [epoch: 7.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.073506561449573		[learning rate: 0.0031679]
	Learning Rate: 0.00316788
	LOSS [training: 4.073506561449573 | validation: 4.062664135730319]
	TIME [epoch: 7.71 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.080963948377998		[learning rate: 0.0031604]
	Learning Rate: 0.00316041
	LOSS [training: 4.080963948377998 | validation: 4.0652427060233425]
	TIME [epoch: 7.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.078285851873242		[learning rate: 0.003153]
	Learning Rate: 0.00315296
	LOSS [training: 4.078285851873242 | validation: 4.029069528428767]
	TIME [epoch: 7.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.064005358536647		[learning rate: 0.0031455]
	Learning Rate: 0.00314552
	LOSS [training: 4.064005358536647 | validation: 4.075751134557107]
	TIME [epoch: 7.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.110188877128838		[learning rate: 0.0031381]
	Learning Rate: 0.0031381
	LOSS [training: 4.110188877128838 | validation: 4.075076344472215]
	TIME [epoch: 7.76 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.094753986170242		[learning rate: 0.0031307]
	Learning Rate: 0.0031307
	LOSS [training: 4.094753986170242 | validation: 4.11359139703837]
	TIME [epoch: 7.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134682187632771		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 4.134682187632771 | validation: 4.104463016359269]
	TIME [epoch: 7.72 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128004040269838		[learning rate: 0.0031159]
	Learning Rate: 0.00311594
	LOSS [training: 4.128004040269838 | validation: 4.078298277358472]
	TIME [epoch: 7.72 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.08194439503518		[learning rate: 0.0031086]
	Learning Rate: 0.0031086
	LOSS [training: 4.08194439503518 | validation: 4.033959908629953]
	TIME [epoch: 7.74 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.060514871985682		[learning rate: 0.0031013]
	Learning Rate: 0.00310126
	LOSS [training: 4.060514871985682 | validation: 4.048806835807676]
	TIME [epoch: 7.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.087469347234122		[learning rate: 0.0030939]
	Learning Rate: 0.00309395
	LOSS [training: 4.087469347234122 | validation: 4.070924305808672]
	TIME [epoch: 7.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1099644911748445		[learning rate: 0.0030866]
	Learning Rate: 0.00308665
	LOSS [training: 4.1099644911748445 | validation: 4.110444574831934]
	TIME [epoch: 7.71 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146894303948391		[learning rate: 0.0030794]
	Learning Rate: 0.00307937
	LOSS [training: 4.146894303948391 | validation: 4.120973667059021]
	TIME [epoch: 7.72 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1437939701324416		[learning rate: 0.0030721]
	Learning Rate: 0.0030721
	LOSS [training: 4.1437939701324416 | validation: 4.097660186216419]
	TIME [epoch: 7.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1301917955204885		[learning rate: 0.0030649]
	Learning Rate: 0.00306486
	LOSS [training: 4.1301917955204885 | validation: 4.0898997850651515]
	TIME [epoch: 7.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.117978246828519		[learning rate: 0.0030576]
	Learning Rate: 0.00305763
	LOSS [training: 4.117978246828519 | validation: 4.101515630305404]
	TIME [epoch: 7.72 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1370514947232335		[learning rate: 0.0030504]
	Learning Rate: 0.00305042
	LOSS [training: 4.1370514947232335 | validation: 4.109388796012685]
	TIME [epoch: 7.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151355497618387		[learning rate: 0.0030432]
	Learning Rate: 0.00304322
	LOSS [training: 4.151355497618387 | validation: 4.12978328626858]
	TIME [epoch: 7.72 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.192305226170458		[learning rate: 0.003036]
	Learning Rate: 0.00303604
	LOSS [training: 4.192305226170458 | validation: 4.161512948277686]
	TIME [epoch: 7.76 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.200544473818578		[learning rate: 0.0030289]
	Learning Rate: 0.00302888
	LOSS [training: 4.200544473818578 | validation: 4.172522734944458]
	TIME [epoch: 7.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.232027801852374		[learning rate: 0.0030217]
	Learning Rate: 0.00302174
	LOSS [training: 4.232027801852374 | validation: 4.1963257795845355]
	TIME [epoch: 7.72 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.261567764348856		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 4.261567764348856 | validation: 4.202676868734873]
	TIME [epoch: 7.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.269775246741796		[learning rate: 0.0030075]
	Learning Rate: 0.0030075
	LOSS [training: 4.269775246741796 | validation: 4.228254312343472]
	TIME [epoch: 7.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.305456222240793		[learning rate: 0.0030004]
	Learning Rate: 0.0030004
	LOSS [training: 4.305456222240793 | validation: 4.259464694616129]
	TIME [epoch: 7.76 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.333823034123823		[learning rate: 0.0029933]
	Learning Rate: 0.00299332
	LOSS [training: 4.333823034123823 | validation: 4.243906717368477]
	TIME [epoch: 7.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.289159306639865		[learning rate: 0.0029863]
	Learning Rate: 0.00298626
	LOSS [training: 4.289159306639865 | validation: 4.18515663722032]
	TIME [epoch: 7.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.235900640093231		[learning rate: 0.0029792]
	Learning Rate: 0.00297922
	LOSS [training: 4.235900640093231 | validation: 4.183678485236225]
	TIME [epoch: 7.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.221138583096365		[learning rate: 0.0029722]
	Learning Rate: 0.00297219
	LOSS [training: 4.221138583096365 | validation: 4.158790440396674]
	TIME [epoch: 7.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184802777358336		[learning rate: 0.0029652]
	Learning Rate: 0.00296518
	LOSS [training: 4.184802777358336 | validation: 4.129372161846629]
	TIME [epoch: 7.77 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138020105178525		[learning rate: 0.0029582]
	Learning Rate: 0.00295819
	LOSS [training: 4.138020105178525 | validation: 4.11847736648572]
	TIME [epoch: 7.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.110181652789026		[learning rate: 0.0029512]
	Learning Rate: 0.00295121
	LOSS [training: 4.110181652789026 | validation: 4.097918622632665]
	TIME [epoch: 7.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.115616184663621		[learning rate: 0.0029442]
	Learning Rate: 0.00294425
	LOSS [training: 4.115616184663621 | validation: 4.131803877601524]
	TIME [epoch: 7.71 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150951032265395		[learning rate: 0.0029373]
	Learning Rate: 0.0029373
	LOSS [training: 4.150951032265395 | validation: 4.150851572513824]
	TIME [epoch: 7.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161292626117088		[learning rate: 0.0029304]
	Learning Rate: 0.00293037
	LOSS [training: 4.161292626117088 | validation: 4.138787065102253]
	TIME [epoch: 7.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138346768109605		[learning rate: 0.0029235]
	Learning Rate: 0.00292346
	LOSS [training: 4.138346768109605 | validation: 4.149934162679822]
	TIME [epoch: 7.72 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154139548765736		[learning rate: 0.0029166]
	Learning Rate: 0.00291657
	LOSS [training: 4.154139548765736 | validation: 4.168846706689699]
	TIME [epoch: 7.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136868539796504		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 4.136868539796504 | validation: 4.134691120900856]
	TIME [epoch: 7.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.116275585951865		[learning rate: 0.0029028]
	Learning Rate: 0.00290282
	LOSS [training: 4.116275585951865 | validation: 4.117928546432099]
	TIME [epoch: 7.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.089744815129967		[learning rate: 0.002896]
	Learning Rate: 0.00289598
	LOSS [training: 4.089744815129967 | validation: 4.099803854124373]
	TIME [epoch: 7.76 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.069973174309221		[learning rate: 0.0028891]
	Learning Rate: 0.00288914
	LOSS [training: 4.069973174309221 | validation: 4.091155671245643]
	TIME [epoch: 7.72 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.058837228438524		[learning rate: 0.0028823]
	Learning Rate: 0.00288233
	LOSS [training: 4.058837228438524 | validation: 4.069932315617926]
	TIME [epoch: 7.71 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.035252859638568		[learning rate: 0.0028755]
	Learning Rate: 0.00287553
	LOSS [training: 4.035252859638568 | validation: 4.059984365100606]
	TIME [epoch: 7.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.034491484889452		[learning rate: 0.0028687]
	Learning Rate: 0.00286875
	LOSS [training: 4.034491484889452 | validation: 4.067239936241149]
	TIME [epoch: 7.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0361861251356315		[learning rate: 0.002862]
	Learning Rate: 0.00286198
	LOSS [training: 4.0361861251356315 | validation: 4.063394689727545]
	TIME [epoch: 7.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.02872088991546		[learning rate: 0.0028552]
	Learning Rate: 0.00285523
	LOSS [training: 4.02872088991546 | validation: 4.055420609914021]
	TIME [epoch: 7.72 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019703778755871		[learning rate: 0.0028485]
	Learning Rate: 0.00284849
	LOSS [training: 4.019703778755871 | validation: 4.035709968200725]
	TIME [epoch: 7.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9909867655358195		[learning rate: 0.0028418]
	Learning Rate: 0.00284178
	LOSS [training: 3.9909867655358195 | validation: 4.0054078499394645]
	TIME [epoch: 7.71 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.971478371907617		[learning rate: 0.0028351]
	Learning Rate: 0.00283507
	LOSS [training: 3.971478371907617 | validation: 3.998548491929915]
	TIME [epoch: 7.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.955227628906059		[learning rate: 0.0028284]
	Learning Rate: 0.00282838
	LOSS [training: 3.955227628906059 | validation: 3.9678638336898997]
	TIME [epoch: 7.77 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9263421411416703		[learning rate: 0.0028217]
	Learning Rate: 0.00282171
	LOSS [training: 3.9263421411416703 | validation: 3.9575431676040633]
	TIME [epoch: 7.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9243267950690464		[learning rate: 0.0028151]
	Learning Rate: 0.00281506
	LOSS [training: 3.9243267950690464 | validation: 3.9603509470170506]
	TIME [epoch: 7.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9375567299390712		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 3.9375567299390712 | validation: 3.991811738897594]
	TIME [epoch: 7.71 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.955553315819388		[learning rate: 0.0028018]
	Learning Rate: 0.00280179
	LOSS [training: 3.955553315819388 | validation: 3.9821923687455865]
	TIME [epoch: 7.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.933153217692754		[learning rate: 0.0027952]
	Learning Rate: 0.00279518
	LOSS [training: 3.933153217692754 | validation: 3.957494118041944]
	TIME [epoch: 7.76 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9242078560323344		[learning rate: 0.0027886]
	Learning Rate: 0.00278859
	LOSS [training: 3.9242078560323344 | validation: 3.9486324335986867]
	TIME [epoch: 7.74 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9182830698743056		[learning rate: 0.002782]
	Learning Rate: 0.00278201
	LOSS [training: 3.9182830698743056 | validation: 3.9391657081005143]
	TIME [epoch: 7.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9047854308560197		[learning rate: 0.0027754]
	Learning Rate: 0.00277545
	LOSS [training: 3.9047854308560197 | validation: 3.9231292733034557]
	TIME [epoch: 7.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8897815921157335		[learning rate: 0.0027689]
	Learning Rate: 0.0027689
	LOSS [training: 3.8897815921157335 | validation: 3.9008696562937093]
	TIME [epoch: 7.75 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.864010103059847		[learning rate: 0.0027624]
	Learning Rate: 0.00276237
	LOSS [training: 3.864010103059847 | validation: 3.874322455167603]
	TIME [epoch: 7.77 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.836855373067787		[learning rate: 0.0027559]
	Learning Rate: 0.00275586
	LOSS [training: 3.836855373067787 | validation: 3.8525747245594415]
	TIME [epoch: 7.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8151926136488035		[learning rate: 0.0027494]
	Learning Rate: 0.00274936
	LOSS [training: 3.8151926136488035 | validation: 3.818779089860057]
	TIME [epoch: 7.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783028036786404		[learning rate: 0.0027429]
	Learning Rate: 0.00274287
	LOSS [training: 3.783028036786404 | validation: 3.791725198026044]
	TIME [epoch: 7.73 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7597343048270435		[learning rate: 0.0027364]
	Learning Rate: 0.0027364
	LOSS [training: 3.7597343048270435 | validation: 3.767588461178768]
	TIME [epoch: 7.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7415216499922828		[learning rate: 0.0027299]
	Learning Rate: 0.00272994
	LOSS [training: 3.7415216499922828 | validation: 3.7501295516103292]
	TIME [epoch: 7.76 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.726476147200991		[learning rate: 0.0027235]
	Learning Rate: 0.00272351
	LOSS [training: 3.726476147200991 | validation: 3.73671995329109]
	TIME [epoch: 7.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716336813687014		[learning rate: 0.0027171]
	Learning Rate: 0.00271708
	LOSS [training: 3.716336813687014 | validation: 3.728082811179947]
	TIME [epoch: 7.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.706843763086536		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 3.706843763086536 | validation: 3.720733682728014]
	TIME [epoch: 7.71 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.701436143812504		[learning rate: 0.0027043]
	Learning Rate: 0.00270428
	LOSS [training: 3.701436143812504 | validation: 3.7139481107827654]
	TIME [epoch: 7.74 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.69594936337958		[learning rate: 0.0026979]
	Learning Rate: 0.0026979
	LOSS [training: 3.69594936337958 | validation: 3.710293473551049]
	TIME [epoch: 7.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.693296312188189		[learning rate: 0.0026915]
	Learning Rate: 0.00269153
	LOSS [training: 3.693296312188189 | validation: 3.7086853118043344]
	TIME [epoch: 7.72 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.68960363693439		[learning rate: 0.0026852]
	Learning Rate: 0.00268519
	LOSS [training: 3.68960363693439 | validation: 3.705048853815528]
	TIME [epoch: 7.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.685978982042558		[learning rate: 0.0026789]
	Learning Rate: 0.00267885
	LOSS [training: 3.685978982042558 | validation: 3.7005255363832488]
	TIME [epoch: 7.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683414355957554		[learning rate: 0.0026725]
	Learning Rate: 0.00267253
	LOSS [training: 3.683414355957554 | validation: 3.700920696491809]
	TIME [epoch: 7.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682965978682671		[learning rate: 0.0026662]
	Learning Rate: 0.00266623
	LOSS [training: 3.682965978682671 | validation: 3.69836984507529]
	TIME [epoch: 7.75 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6802242250503854		[learning rate: 0.0026599]
	Learning Rate: 0.00265994
	LOSS [training: 3.6802242250503854 | validation: 3.696101451008708]
	TIME [epoch: 7.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6789847428971885		[learning rate: 0.0026537]
	Learning Rate: 0.00265367
	LOSS [training: 3.6789847428971885 | validation: 3.6927426285748126]
	TIME [epoch: 7.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6747560274060804		[learning rate: 0.0026474]
	Learning Rate: 0.00264741
	LOSS [training: 3.6747560274060804 | validation: 3.6932141776951073]
	TIME [epoch: 7.71 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67397117943102		[learning rate: 0.0026412]
	Learning Rate: 0.00264116
	LOSS [training: 3.67397117943102 | validation: 3.6877773875139948]
	TIME [epoch: 7.77 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6689964827743915		[learning rate: 0.0026349]
	Learning Rate: 0.00263493
	LOSS [training: 3.6689964827743915 | validation: 3.6839505656265565]
	TIME [epoch: 7.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6660208476136447		[learning rate: 0.0026287]
	Learning Rate: 0.00262872
	LOSS [training: 3.6660208476136447 | validation: 3.680364954970795]
	TIME [epoch: 7.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6612599379647826		[learning rate: 0.0026225]
	Learning Rate: 0.00262251
	LOSS [training: 3.6612599379647826 | validation: 3.677121512713857]
	TIME [epoch: 7.71 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6548666861841235		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 3.6548666861841235 | validation: 3.6716868803544935]
	TIME [epoch: 7.72 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.649807701642522		[learning rate: 0.0026102]
	Learning Rate: 0.00261016
	LOSS [training: 3.649807701642522 | validation: 3.665472861084587]
	TIME [epoch: 7.76 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.64228614941321		[learning rate: 0.002604]
	Learning Rate: 0.002604
	LOSS [training: 3.64228614941321 | validation: 3.660007356206484]
	TIME [epoch: 7.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.638782215768718		[learning rate: 0.0025979]
	Learning Rate: 0.00259786
	LOSS [training: 3.638782215768718 | validation: 3.656310957415971]
	TIME [epoch: 7.72 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.637843190621423		[learning rate: 0.0025917]
	Learning Rate: 0.00259173
	LOSS [training: 3.637843190621423 | validation: 3.655018282181053]
	TIME [epoch: 7.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635141685559643		[learning rate: 0.0025856]
	Learning Rate: 0.00258562
	LOSS [training: 3.635141685559643 | validation: 3.6495150010641906]
	TIME [epoch: 7.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.632368159719251		[learning rate: 0.0025795]
	Learning Rate: 0.00257952
	LOSS [training: 3.632368159719251 | validation: 3.6472570377545224]
	TIME [epoch: 7.76 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6297503228789836		[learning rate: 0.0025734]
	Learning Rate: 0.00257343
	LOSS [training: 3.6297503228789836 | validation: 3.647551570471136]
	TIME [epoch: 7.74 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6266408372252723		[learning rate: 0.0025674]
	Learning Rate: 0.00256736
	LOSS [training: 3.6266408372252723 | validation: 3.642515796362752]
	TIME [epoch: 7.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.620327752918097		[learning rate: 0.0025613]
	Learning Rate: 0.00256131
	LOSS [training: 3.620327752918097 | validation: 3.6397872434342173]
	TIME [epoch: 7.73 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6145315480185722		[learning rate: 0.0025553]
	Learning Rate: 0.00255526
	LOSS [training: 3.6145315480185722 | validation: 3.6317288417618956]
	TIME [epoch: 7.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.606638728946007		[learning rate: 0.0025492]
	Learning Rate: 0.00254924
	LOSS [training: 3.606638728946007 | validation: 3.6199263061262776]
	TIME [epoch: 7.77 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.587370557652985		[learning rate: 0.0025432]
	Learning Rate: 0.00254322
	LOSS [training: 3.587370557652985 | validation: 3.606895513599526]
	TIME [epoch: 7.72 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5905140976379397		[learning rate: 0.0025372]
	Learning Rate: 0.00253722
	LOSS [training: 3.5905140976379397 | validation: 3.649948695719594]
	TIME [epoch: 7.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7065558966331884		[learning rate: 0.0025312]
	Learning Rate: 0.00253124
	LOSS [training: 3.7065558966331884 | validation: 3.9018379687416997]
	TIME [epoch: 7.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.949890052063635		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 3.949890052063635 | validation: 3.989122660747082]
	TIME [epoch: 7.74 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067350314391175		[learning rate: 0.0025193]
	Learning Rate: 0.00251931
	LOSS [training: 4.067350314391175 | validation: 3.940276323183115]
	TIME [epoch: 7.76 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9389439741750114		[learning rate: 0.0025134]
	Learning Rate: 0.00251337
	LOSS [training: 3.9389439741750114 | validation: 3.824102920857473]
	TIME [epoch: 7.73 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7161189098261365		[learning rate: 0.0025074]
	Learning Rate: 0.00250744
	LOSS [training: 3.7161189098261365 | validation: 3.647933744946251]
	TIME [epoch: 7.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6479910998840888		[learning rate: 0.0025015]
	Learning Rate: 0.00250153
	LOSS [training: 3.6479910998840888 | validation: 3.6837316510914837]
	TIME [epoch: 7.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6886178975152886		[learning rate: 0.0024956]
	Learning Rate: 0.00249563
	LOSS [training: 3.6886178975152886 | validation: 3.7448320446734558]
	TIME [epoch: 7.74 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7826112958407303		[learning rate: 0.0024897]
	Learning Rate: 0.00248974
	LOSS [training: 3.7826112958407303 | validation: 3.7439683748511827]
	TIME [epoch: 7.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.799277482843345		[learning rate: 0.0024839]
	Learning Rate: 0.00248387
	LOSS [training: 3.799277482843345 | validation: 3.837093185324126]
	TIME [epoch: 7.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.788640767360209		[learning rate: 0.002478]
	Learning Rate: 0.00247801
	LOSS [training: 3.788640767360209 | validation: 3.7537844457926384]
	TIME [epoch: 7.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7771021798149778		[learning rate: 0.0024722]
	Learning Rate: 0.00247216
	LOSS [training: 3.7771021798149778 | validation: 3.7923808904708673]
	TIME [epoch: 7.71 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752224719873211		[learning rate: 0.0024663]
	Learning Rate: 0.00246633
	LOSS [training: 3.752224719873211 | validation: 3.71304247185208]
	TIME [epoch: 7.74 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.702938079909711		[learning rate: 0.0024605]
	Learning Rate: 0.00246051
	LOSS [training: 3.702938079909711 | validation: 3.6843105883803546]
	TIME [epoch: 7.77 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.644233056992858		[learning rate: 0.0024547]
	Learning Rate: 0.00245471
	LOSS [training: 3.644233056992858 | validation: 3.6226927650321468]
	TIME [epoch: 7.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6148615500399113		[learning rate: 0.0024489]
	Learning Rate: 0.00244892
	LOSS [training: 3.6148615500399113 | validation: 3.6467562578594546]
	TIME [epoch: 7.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6653365993347258		[learning rate: 0.0024431]
	Learning Rate: 0.00244314
	LOSS [training: 3.6653365993347258 | validation: 3.7590195853402015]
	TIME [epoch: 7.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804669101122938		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 3.804669101122938 | validation: 3.8967929410491373]
	TIME [epoch: 7.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.011183168648595		[learning rate: 0.0024316]
	Learning Rate: 0.00243163
	LOSS [training: 4.011183168648595 | validation: 4.0364691468179466]
	TIME [epoch: 7.77 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0209508264473		[learning rate: 0.0024259]
	Learning Rate: 0.00242589
	LOSS [training: 4.0209508264473 | validation: 3.9405285800948437]
	TIME [epoch: 7.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.940656875559093		[learning rate: 0.0024202]
	Learning Rate: 0.00242017
	LOSS [training: 3.940656875559093 | validation: 3.9014848545780927]
	TIME [epoch: 7.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.011726937849937		[learning rate: 0.0024145]
	Learning Rate: 0.00241446
	LOSS [training: 4.011726937849937 | validation: 3.999474245349555]
	TIME [epoch: 7.71 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.024517254472172		[learning rate: 0.0024088]
	Learning Rate: 0.00240877
	LOSS [training: 4.024517254472172 | validation: 4.0374631131665275]
	TIME [epoch: 7.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.09379579774566		[learning rate: 0.0024031]
	Learning Rate: 0.00240309
	LOSS [training: 4.09379579774566 | validation: 4.03781127859625]
	TIME [epoch: 7.78 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.060849481801203		[learning rate: 0.0023974]
	Learning Rate: 0.00239742
	LOSS [training: 4.060849481801203 | validation: 4.0154383572371835]
	TIME [epoch: 7.72 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0134937397597		[learning rate: 0.0023918]
	Learning Rate: 0.00239176
	LOSS [training: 4.0134937397597 | validation: 4.032528269553563]
	TIME [epoch: 7.77 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.106835826778008		[learning rate: 0.0023861]
	Learning Rate: 0.00238612
	LOSS [training: 4.106835826778008 | validation: 4.144580784384334]
	TIME [epoch: 7.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.213756126475026		[learning rate: 0.0023805]
	Learning Rate: 0.00238049
	LOSS [training: 4.213756126475026 | validation: 4.226171198638059]
	TIME [epoch: 7.76 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.236957462909103		[learning rate: 0.0023749]
	Learning Rate: 0.00237488
	LOSS [training: 4.236957462909103 | validation: 4.183672578666522]
	TIME [epoch: 7.77 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.220501942855699		[learning rate: 0.0023693]
	Learning Rate: 0.00236927
	LOSS [training: 4.220501942855699 | validation: 4.187250992581324]
	TIME [epoch: 7.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.24545743656231		[learning rate: 0.0023637]
	Learning Rate: 0.00236369
	LOSS [training: 4.24545743656231 | validation: 4.211618040941595]
	TIME [epoch: 7.72 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.242140474103065		[learning rate: 0.0023581]
	Learning Rate: 0.00235811
	LOSS [training: 4.242140474103065 | validation: 4.209495425320937]
	TIME [epoch: 7.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.241797564399837		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 4.241797564399837 | validation: 4.259834461695258]
	TIME [epoch: 7.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2451255449154965		[learning rate: 0.002347]
	Learning Rate: 0.002347
	LOSS [training: 4.2451255449154965 | validation: 4.206686578988265]
	TIME [epoch: 7.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225571019333602		[learning rate: 0.0023415]
	Learning Rate: 0.00234146
	LOSS [training: 4.225571019333602 | validation: 4.2078310522206195]
	TIME [epoch: 7.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21207495333747		[learning rate: 0.0023359]
	Learning Rate: 0.00233594
	LOSS [training: 4.21207495333747 | validation: 4.179077780650531]
	TIME [epoch: 7.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175237210423217		[learning rate: 0.0023304]
	Learning Rate: 0.00233043
	LOSS [training: 4.175237210423217 | validation: 4.131342184013672]
	TIME [epoch: 7.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154583860767603		[learning rate: 0.0023249]
	Learning Rate: 0.00232493
	LOSS [training: 4.154583860767603 | validation: 4.105736433483493]
	TIME [epoch: 7.76 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157643433104809		[learning rate: 0.0023194]
	Learning Rate: 0.00231945
	LOSS [training: 4.157643433104809 | validation: 4.1280101760828725]
	TIME [epoch: 7.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.196500122812538		[learning rate: 0.002314]
	Learning Rate: 0.00231398
	LOSS [training: 4.196500122812538 | validation: 4.190976514644324]
	TIME [epoch: 7.73 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2279300762841405		[learning rate: 0.0023085]
	Learning Rate: 0.00230852
	LOSS [training: 4.2279300762841405 | validation: 4.14549029740025]
	TIME [epoch: 7.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182473488474027		[learning rate: 0.0023031]
	Learning Rate: 0.00230307
	LOSS [training: 4.182473488474027 | validation: 4.144294759160497]
	TIME [epoch: 7.74 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1867208040234924		[learning rate: 0.0022976]
	Learning Rate: 0.00229764
	LOSS [training: 4.1867208040234924 | validation: 4.20096306103843]
	TIME [epoch: 7.77 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.216255008803053		[learning rate: 0.0022922]
	Learning Rate: 0.00229222
	LOSS [training: 4.216255008803053 | validation: 4.188518609800838]
	TIME [epoch: 7.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.176476029968313		[learning rate: 0.0022868]
	Learning Rate: 0.00228681
	LOSS [training: 4.176476029968313 | validation: 4.107726215025904]
	TIME [epoch: 7.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132831828260003		[learning rate: 0.0022814]
	Learning Rate: 0.00228142
	LOSS [training: 4.132831828260003 | validation: 4.101384636698107]
	TIME [epoch: 7.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.099787848186306		[learning rate: 0.002276]
	Learning Rate: 0.00227604
	LOSS [training: 4.099787848186306 | validation: 4.049384844757768]
	TIME [epoch: 7.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.085163968592491		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 4.085163968592491 | validation: 4.051828597705001]
	TIME [epoch: 7.76 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.033658144670124		[learning rate: 0.0022653]
	Learning Rate: 0.00226531
	LOSS [training: 4.033658144670124 | validation: 3.9420257995461556]
	TIME [epoch: 7.74 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.969904684687254		[learning rate: 0.00226]
	Learning Rate: 0.00225997
	LOSS [training: 3.969904684687254 | validation: 3.927408767529182]
	TIME [epoch: 7.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.033017570332725		[learning rate: 0.0022546]
	Learning Rate: 0.00225464
	LOSS [training: 4.033017570332725 | validation: 4.096805289039325]
	TIME [epoch: 7.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125860103247955		[learning rate: 0.0022493]
	Learning Rate: 0.00224932
	LOSS [training: 4.125860103247955 | validation: 4.091486364420339]
	TIME [epoch: 7.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.10741846734022		[learning rate: 0.002244]
	Learning Rate: 0.00224401
	LOSS [training: 4.10741846734022 | validation: 4.028072613781182]
	TIME [epoch: 7.77 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.020142360039507		[learning rate: 0.0022387]
	Learning Rate: 0.00223872
	LOSS [training: 4.020142360039507 | validation: 3.9609216675483294]
	TIME [epoch: 7.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9760303158464474		[learning rate: 0.0022334]
	Learning Rate: 0.00223344
	LOSS [training: 3.9760303158464474 | validation: 3.928747485329076]
	TIME [epoch: 7.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.007106119511188		[learning rate: 0.0022282]
	Learning Rate: 0.00222817
	LOSS [training: 4.007106119511188 | validation: 4.035830676131905]
	TIME [epoch: 7.71 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.061088112678447		[learning rate: 0.0022229]
	Learning Rate: 0.00222292
	LOSS [training: 4.061088112678447 | validation: 4.018067537728054]
	TIME [epoch: 7.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067740570847836		[learning rate: 0.0022177]
	Learning Rate: 0.00221767
	LOSS [training: 4.067740570847836 | validation: 4.0346275006635075]
	TIME [epoch: 7.77 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.079308655487387		[learning rate: 0.0022124]
	Learning Rate: 0.00221244
	LOSS [training: 4.079308655487387 | validation: 4.070168852789436]
	TIME [epoch: 7.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137394565019956		[learning rate: 0.0022072]
	Learning Rate: 0.00220722
	LOSS [training: 4.137394565019956 | validation: 4.128757196781372]
	TIME [epoch: 7.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2112197347948275		[learning rate: 0.002202]
	Learning Rate: 0.00220202
	LOSS [training: 4.2112197347948275 | validation: 4.187810265949743]
	TIME [epoch: 7.73 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.231950889173509		[learning rate: 0.0021968]
	Learning Rate: 0.00219682
	LOSS [training: 4.231950889173509 | validation: 4.185729020573376]
	TIME [epoch: 7.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2261345934332555		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 4.2261345934332555 | validation: 4.1958164814581185]
	TIME [epoch: 7.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.194907620496455		[learning rate: 0.0021865]
	Learning Rate: 0.00218647
	LOSS [training: 4.194907620496455 | validation: 4.178468746745736]
	TIME [epoch: 7.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.241602338558954		[learning rate: 0.0021813]
	Learning Rate: 0.00218131
	LOSS [training: 4.241602338558954 | validation: 4.202842606284951]
	TIME [epoch: 7.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.248158912267564		[learning rate: 0.0021762]
	Learning Rate: 0.00217617
	LOSS [training: 4.248158912267564 | validation: 4.289732009878035]
	TIME [epoch: 7.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300768471855303		[learning rate: 0.002171]
	Learning Rate: 0.00217103
	LOSS [training: 4.300768471855303 | validation: 4.275854371669071]
	TIME [epoch: 7.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3116555926514515		[learning rate: 0.0021659]
	Learning Rate: 0.00216591
	LOSS [training: 4.3116555926514515 | validation: 4.3088076410551395]
	TIME [epoch: 7.76 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.359478399591256		[learning rate: 0.0021608]
	Learning Rate: 0.0021608
	LOSS [training: 4.359478399591256 | validation: 4.3613812730184565]
	TIME [epoch: 7.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3865516536554665		[learning rate: 0.0021557]
	Learning Rate: 0.00215571
	LOSS [training: 4.3865516536554665 | validation: 4.363317675191775]
	TIME [epoch: 7.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.369425638451603		[learning rate: 0.0021506]
	Learning Rate: 0.00215062
	LOSS [training: 4.369425638451603 | validation: 4.361014985760977]
	TIME [epoch: 7.73 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.376340458227743		[learning rate: 0.0021455]
	Learning Rate: 0.00214555
	LOSS [training: 4.376340458227743 | validation: 4.343380370656198]
	TIME [epoch: 7.75 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.362892507569903		[learning rate: 0.0021405]
	Learning Rate: 0.00214049
	LOSS [training: 4.362892507569903 | validation: 4.296436941622539]
	TIME [epoch: 7.77 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.342669288492511		[learning rate: 0.0021354]
	Learning Rate: 0.00213544
	LOSS [training: 4.342669288492511 | validation: 4.3250896647412285]
	TIME [epoch: 7.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3459124977304295		[learning rate: 0.0021304]
	Learning Rate: 0.0021304
	LOSS [training: 4.3459124977304295 | validation: 4.306269509063508]
	TIME [epoch: 7.72 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.322271698842132		[learning rate: 0.0021254]
	Learning Rate: 0.00212538
	LOSS [training: 4.322271698842132 | validation: 4.2308703803536485]
	TIME [epoch: 7.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.27629577268995		[learning rate: 0.0021204]
	Learning Rate: 0.00212036
	LOSS [training: 4.27629577268995 | validation: 4.252829855264157]
	TIME [epoch: 7.76 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.297365306179481		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 4.297365306179481 | validation: 4.256366297217477]
	TIME [epoch: 7.76 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.266747943770285		[learning rate: 0.0021104]
	Learning Rate: 0.00211037
	LOSS [training: 4.266747943770285 | validation: 4.198803122000589]
	TIME [epoch: 7.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.240007750680924		[learning rate: 0.0021054]
	Learning Rate: 0.00210539
	LOSS [training: 4.240007750680924 | validation: 4.173500766626288]
	TIME [epoch: 7.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.217276080711062		[learning rate: 0.0021004]
	Learning Rate: 0.00210043
	LOSS [training: 4.217276080711062 | validation: 4.1596069628667625]
	TIME [epoch: 7.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.212458852496676		[learning rate: 0.0020955]
	Learning Rate: 0.00209547
	LOSS [training: 4.212458852496676 | validation: 4.184466152303809]
	TIME [epoch: 7.77 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.229729051687911		[learning rate: 0.0020905]
	Learning Rate: 0.00209053
	LOSS [training: 4.229729051687911 | validation: 4.171185019412296]
	TIME [epoch: 7.75 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2196561268557575		[learning rate: 0.0020856]
	Learning Rate: 0.0020856
	LOSS [training: 4.2196561268557575 | validation: 4.16785297331918]
	TIME [epoch: 7.71 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.229537377701364		[learning rate: 0.0020807]
	Learning Rate: 0.00208068
	LOSS [training: 4.229537377701364 | validation: 4.172267438808072]
	TIME [epoch: 7.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.223970630967833		[learning rate: 0.0020758]
	Learning Rate: 0.00207577
	LOSS [training: 4.223970630967833 | validation: 4.173700424518949]
	TIME [epoch: 7.72 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21710999428442		[learning rate: 0.0020709]
	Learning Rate: 0.00207087
	LOSS [training: 4.21710999428442 | validation: 4.141702675330457]
	TIME [epoch: 7.78 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.194624401565285		[learning rate: 0.002066]
	Learning Rate: 0.00206599
	LOSS [training: 4.194624401565285 | validation: 4.157182046664918]
	TIME [epoch: 7.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.20592354335337		[learning rate: 0.0020611]
	Learning Rate: 0.00206112
	LOSS [training: 4.20592354335337 | validation: 4.156604429204576]
	TIME [epoch: 7.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2117350259995865		[learning rate: 0.0020563]
	Learning Rate: 0.00205626
	LOSS [training: 4.2117350259995865 | validation: 4.159475244439031]
	TIME [epoch: 7.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.219583607718199		[learning rate: 0.0020514]
	Learning Rate: 0.0020514
	LOSS [training: 4.219583607718199 | validation: 4.155360643356074]
	TIME [epoch: 7.75 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.197507083741359		[learning rate: 0.0020466]
	Learning Rate: 0.00204657
	LOSS [training: 4.197507083741359 | validation: 4.123087126260723]
	TIME [epoch: 7.79 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.177152117153254		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 4.177152117153254 | validation: 4.117425881752977]
	TIME [epoch: 7.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.177290704185149		[learning rate: 0.0020369]
	Learning Rate: 0.00203692
	LOSS [training: 4.177290704185149 | validation: 4.140036259395128]
	TIME [epoch: 7.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.207265761440141		[learning rate: 0.0020321]
	Learning Rate: 0.00203212
	LOSS [training: 4.207265761440141 | validation: 4.177516061049809]
	TIME [epoch: 7.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22642276370411		[learning rate: 0.0020273]
	Learning Rate: 0.00202732
	LOSS [training: 4.22642276370411 | validation: 4.175619347049267]
	TIME [epoch: 7.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21700385663299		[learning rate: 0.0020225]
	Learning Rate: 0.00202254
	LOSS [training: 4.21700385663299 | validation: 4.144842081193458]
	TIME [epoch: 7.78 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18755107867437		[learning rate: 0.0020178]
	Learning Rate: 0.00201777
	LOSS [training: 4.18755107867437 | validation: 4.138925234419226]
	TIME [epoch: 7.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1959397139742		[learning rate: 0.002013]
	Learning Rate: 0.00201301
	LOSS [training: 4.1959397139742 | validation: 4.138805524868056]
	TIME [epoch: 7.73 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178249795540289		[learning rate: 0.0020083]
	Learning Rate: 0.00200826
	LOSS [training: 4.178249795540289 | validation: 4.111078053427266]
	TIME [epoch: 7.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1372812381968975		[learning rate: 0.0020035]
	Learning Rate: 0.00200353
	LOSS [training: 4.1372812381968975 | validation: 4.067176971032824]
	TIME [epoch: 7.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.097426798616242		[learning rate: 0.0019988]
	Learning Rate: 0.0019988
	LOSS [training: 4.097426798616242 | validation: 4.048156500896043]
	TIME [epoch: 7.78 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.078701063531577		[learning rate: 0.0019941]
	Learning Rate: 0.00199408
	LOSS [training: 4.078701063531577 | validation: 4.044585208818719]
	TIME [epoch: 7.75 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0808207020451075		[learning rate: 0.0019894]
	Learning Rate: 0.00198938
	LOSS [training: 4.0808207020451075 | validation: 4.03340113308957]
	TIME [epoch: 7.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.07423735399443		[learning rate: 0.0019847]
	Learning Rate: 0.00198469
	LOSS [training: 4.07423735399443 | validation: 4.0414448182496905]
	TIME [epoch: 7.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067306517774425		[learning rate: 0.00198]
	Learning Rate: 0.00198001
	LOSS [training: 4.067306517774425 | validation: 4.035519781525094]
	TIME [epoch: 7.75 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.062355035718445		[learning rate: 0.0019753]
	Learning Rate: 0.00197534
	LOSS [training: 4.062355035718445 | validation: 4.0358930681934595]
	TIME [epoch: 7.79 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.073617081854696		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 4.073617081854696 | validation: 4.048976120687627]
	TIME [epoch: 7.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077941938668282		[learning rate: 0.001966]
	Learning Rate: 0.00196603
	LOSS [training: 4.077941938668282 | validation: 4.051594794138026]
	TIME [epoch: 7.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.082469534172446		[learning rate: 0.0019614]
	Learning Rate: 0.00196139
	LOSS [training: 4.082469534172446 | validation: 4.048053164141319]
	TIME [epoch: 7.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0853167066420175		[learning rate: 0.0019568]
	Learning Rate: 0.00195676
	LOSS [training: 4.0853167066420175 | validation: 4.056509493415048]
	TIME [epoch: 7.75 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.103052536269128		[learning rate: 0.0019521]
	Learning Rate: 0.00195215
	LOSS [training: 4.103052536269128 | validation: 4.08854153258053]
	TIME [epoch: 7.78 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132160698306968		[learning rate: 0.0019475]
	Learning Rate: 0.00194754
	LOSS [training: 4.132160698306968 | validation: 4.089473910860983]
	TIME [epoch: 7.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123806452461928		[learning rate: 0.0019429]
	Learning Rate: 0.00194295
	LOSS [training: 4.123806452461928 | validation: 4.092871044900384]
	TIME [epoch: 7.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139652391350731		[learning rate: 0.0019384]
	Learning Rate: 0.00193837
	LOSS [training: 4.139652391350731 | validation: 4.10618291409151]
	TIME [epoch: 7.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149331389083992		[learning rate: 0.0019338]
	Learning Rate: 0.00193379
	LOSS [training: 4.149331389083992 | validation: 4.1220437838100805]
	TIME [epoch: 7.75 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164855503326749		[learning rate: 0.0019292]
	Learning Rate: 0.00192923
	LOSS [training: 4.164855503326749 | validation: 4.109280039947405]
	TIME [epoch: 7.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132730225413429		[learning rate: 0.0019247]
	Learning Rate: 0.00192468
	LOSS [training: 4.132730225413429 | validation: 4.074001061052552]
	TIME [epoch: 7.73 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.113209274986793		[learning rate: 0.0019201]
	Learning Rate: 0.00192014
	LOSS [training: 4.113209274986793 | validation: 4.065958815617655]
	TIME [epoch: 7.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.090041792914849		[learning rate: 0.0019156]
	Learning Rate: 0.00191561
	LOSS [training: 4.090041792914849 | validation: 4.05658090187303]
	TIME [epoch: 7.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.080177999165404		[learning rate: 0.0019111]
	Learning Rate: 0.00191109
	LOSS [training: 4.080177999165404 | validation: 4.0545264199330635]
	TIME [epoch: 7.75 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077962993681651		[learning rate: 0.0019066]
	Learning Rate: 0.00190659
	LOSS [training: 4.077962993681651 | validation: 4.054243119893483]
	TIME [epoch: 7.78 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.079894541807331		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 4.079894541807331 | validation: 4.055043016996811]
	TIME [epoch: 7.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.078409198054707		[learning rate: 0.0018976]
	Learning Rate: 0.0018976
	LOSS [training: 4.078409198054707 | validation: 4.053016960924811]
	TIME [epoch: 7.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.072482161632764		[learning rate: 0.0018931]
	Learning Rate: 0.00189313
	LOSS [training: 4.072482161632764 | validation: 4.042771758064497]
	TIME [epoch: 7.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.058978949519515		[learning rate: 0.0018887]
	Learning Rate: 0.00188866
	LOSS [training: 4.058978949519515 | validation: 4.044304471893275]
	TIME [epoch: 7.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.048304926342785		[learning rate: 0.0018842]
	Learning Rate: 0.00188421
	LOSS [training: 4.048304926342785 | validation: 4.036448797399727]
	TIME [epoch: 7.76 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.037798418765897		[learning rate: 0.0018798]
	Learning Rate: 0.00187976
	LOSS [training: 4.037798418765897 | validation: 4.045062539112079]
	TIME [epoch: 7.76 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.044402599232788		[learning rate: 0.0018753]
	Learning Rate: 0.00187533
	LOSS [training: 4.044402599232788 | validation: 4.0527083415786525]
	TIME [epoch: 7.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.036083867233738		[learning rate: 0.0018709]
	Learning Rate: 0.0018709
	LOSS [training: 4.036083867233738 | validation: 4.042857535422789]
	TIME [epoch: 7.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.025731043868889		[learning rate: 0.0018665]
	Learning Rate: 0.00186649
	LOSS [training: 4.025731043868889 | validation: 4.030266796985343]
	TIME [epoch: 7.75 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.015554359156419		[learning rate: 0.0018621]
	Learning Rate: 0.00186209
	LOSS [training: 4.015554359156419 | validation: 4.012588667187414]
	TIME [epoch: 7.77 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.003196386878511		[learning rate: 0.0018577]
	Learning Rate: 0.00185769
	LOSS [training: 4.003196386878511 | validation: 3.9959387655029324]
	TIME [epoch: 7.73 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9888305510733186		[learning rate: 0.0018533]
	Learning Rate: 0.00185331
	LOSS [training: 3.9888305510733186 | validation: 3.975490797199232]
	TIME [epoch: 7.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.979387759086944		[learning rate: 0.0018489]
	Learning Rate: 0.00184894
	LOSS [training: 3.979387759086944 | validation: 3.956779803862521]
	TIME [epoch: 7.73 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9706257298135164		[learning rate: 0.0018446]
	Learning Rate: 0.00184458
	LOSS [training: 3.9706257298135164 | validation: 3.9458301706455785]
	TIME [epoch: 7.76 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9652118385545445		[learning rate: 0.0018402]
	Learning Rate: 0.00184023
	LOSS [training: 3.9652118385545445 | validation: 3.951644135881661]
	TIME [epoch: 7.75 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.957985881300186		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 3.957985881300186 | validation: 3.9320468655510235]
	TIME [epoch: 7.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.937117313378485		[learning rate: 0.0018316]
	Learning Rate: 0.00183156
	LOSS [training: 3.937117313378485 | validation: 3.933994191053262]
	TIME [epoch: 7.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9758196740366616		[learning rate: 0.0018272]
	Learning Rate: 0.00182724
	LOSS [training: 3.9758196740366616 | validation: 3.9670205365446574]
	TIME [epoch: 7.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9758203713533007		[learning rate: 0.0018229]
	Learning Rate: 0.00182293
	LOSS [training: 3.9758203713533007 | validation: 3.9291500686399656]
	TIME [epoch: 7.77 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.961938831210561		[learning rate: 0.0018186]
	Learning Rate: 0.00181863
	LOSS [training: 3.961938831210561 | validation: 3.9293430043618223]
	TIME [epoch: 7.75 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.942938182757459		[learning rate: 0.0018143]
	Learning Rate: 0.00181434
	LOSS [training: 3.942938182757459 | validation: 3.913780780839608]
	TIME [epoch: 7.74 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9345862758970105		[learning rate: 0.0018101]
	Learning Rate: 0.00181006
	LOSS [training: 3.9345862758970105 | validation: 3.9317995739722935]
	TIME [epoch: 7.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9545472391946426		[learning rate: 0.0018058]
	Learning Rate: 0.00180579
	LOSS [training: 3.9545472391946426 | validation: 3.9281950384759883]
	TIME [epoch: 7.74 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9392368110975045		[learning rate: 0.0018015]
	Learning Rate: 0.00180153
	LOSS [training: 3.9392368110975045 | validation: 3.917160595940568]
	TIME [epoch: 7.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.929944209378766		[learning rate: 0.0017973]
	Learning Rate: 0.00179728
	LOSS [training: 3.929944209378766 | validation: 3.924447574294059]
	TIME [epoch: 7.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.930781410308599		[learning rate: 0.001793]
	Learning Rate: 0.00179304
	LOSS [training: 3.930781410308599 | validation: 3.936230737959754]
	TIME [epoch: 7.71 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9312604359231296		[learning rate: 0.0017888]
	Learning Rate: 0.00178881
	LOSS [training: 3.9312604359231296 | validation: 3.935287306362076]
	TIME [epoch: 7.77 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9400393538158442		[learning rate: 0.0017846]
	Learning Rate: 0.00178459
	LOSS [training: 3.9400393538158442 | validation: 3.9486941431516964]
	TIME [epoch: 7.73 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.937714452603026		[learning rate: 0.0017804]
	Learning Rate: 0.00178038
	LOSS [training: 3.937714452603026 | validation: 3.9431502548705923]
	TIME [epoch: 7.77 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.939547053325793		[learning rate: 0.0017762]
	Learning Rate: 0.00177618
	LOSS [training: 3.939547053325793 | validation: 3.9472614047926804]
	TIME [epoch: 7.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9328896132117923		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 3.9328896132117923 | validation: 3.9375258405728357]
	TIME [epoch: 7.71 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9436386968492423		[learning rate: 0.0017678]
	Learning Rate: 0.00176781
	LOSS [training: 3.9436386968492423 | validation: 3.955554028822391]
	TIME [epoch: 7.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.978596889348916		[learning rate: 0.0017636]
	Learning Rate: 0.00176364
	LOSS [training: 3.978596889348916 | validation: 3.988376173019086]
	TIME [epoch: 7.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019654195836773		[learning rate: 0.0017595]
	Learning Rate: 0.00175948
	LOSS [training: 4.019654195836773 | validation: 4.006802578772184]
	TIME [epoch: 7.78 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.036192218963779		[learning rate: 0.0017553]
	Learning Rate: 0.00175533
	LOSS [training: 4.036192218963779 | validation: 4.012499744006195]
	TIME [epoch: 7.74 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.032548209910641		[learning rate: 0.0017512]
	Learning Rate: 0.00175119
	LOSS [training: 4.032548209910641 | validation: 4.01377611450041]
	TIME [epoch: 7.73 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.029180681358155		[learning rate: 0.0017471]
	Learning Rate: 0.00174706
	LOSS [training: 4.029180681358155 | validation: 4.021988049569538]
	TIME [epoch: 7.74 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.038535618922854		[learning rate: 0.0017429]
	Learning Rate: 0.00174294
	LOSS [training: 4.038535618922854 | validation: 4.033049012935359]
	TIME [epoch: 7.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.055343155122322		[learning rate: 0.0017388]
	Learning Rate: 0.00173883
	LOSS [training: 4.055343155122322 | validation: 4.039707688444324]
	TIME [epoch: 7.78 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.042462221919857		[learning rate: 0.0017347]
	Learning Rate: 0.00173473
	LOSS [training: 4.042462221919857 | validation: 4.028966384349916]
	TIME [epoch: 7.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019096576675808		[learning rate: 0.0017306]
	Learning Rate: 0.00173063
	LOSS [training: 4.019096576675808 | validation: 4.005333386626595]
	TIME [epoch: 7.73 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016805332359285		[learning rate: 0.0017266]
	Learning Rate: 0.00172655
	LOSS [training: 4.016805332359285 | validation: 4.007746599493801]
	TIME [epoch: 7.73 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.021029290856362		[learning rate: 0.0017225]
	Learning Rate: 0.00172248
	LOSS [training: 4.021029290856362 | validation: 4.006872345465405]
	TIME [epoch: 7.73 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.00892185720249		[learning rate: 0.0017184]
	Learning Rate: 0.00171842
	LOSS [training: 4.00892185720249 | validation: 4.014669558733772]
	TIME [epoch: 7.78 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.005432930995799		[learning rate: 0.0017144]
	Learning Rate: 0.00171436
	LOSS [training: 4.005432930995799 | validation: 3.9939971062587274]
	TIME [epoch: 7.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.996105856427822		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 3.996105856427822 | validation: 4.007029829144249]
	TIME [epoch: 7.74 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.012737894635171		[learning rate: 0.0017063]
	Learning Rate: 0.00170628
	LOSS [training: 4.012737894635171 | validation: 4.03065332277033]
	TIME [epoch: 7.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.041351855825586		[learning rate: 0.0017023]
	Learning Rate: 0.00170226
	LOSS [training: 4.041351855825586 | validation: 4.036285695210596]
	TIME [epoch: 7.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0506227367410155		[learning rate: 0.0016982]
	Learning Rate: 0.00169824
	LOSS [training: 4.0506227367410155 | validation: 4.04211183909594]
	TIME [epoch: 7.79 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.048846545300225		[learning rate: 0.0016942]
	Learning Rate: 0.00169424
	LOSS [training: 4.048846545300225 | validation: 4.020394060401287]
	TIME [epoch: 7.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.037369237424294		[learning rate: 0.0016902]
	Learning Rate: 0.00169024
	LOSS [training: 4.037369237424294 | validation: 4.027368512734201]
	TIME [epoch: 7.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.046913054862246		[learning rate: 0.0016863]
	Learning Rate: 0.00168625
	LOSS [training: 4.046913054862246 | validation: 4.0211928860171025]
	TIME [epoch: 7.71 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.04778434102298		[learning rate: 0.0016823]
	Learning Rate: 0.00168228
	LOSS [training: 4.04778434102298 | validation: 4.020839789054884]
	TIME [epoch: 9.07 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.053841060628241		[learning rate: 0.0016783]
	Learning Rate: 0.00167831
	LOSS [training: 4.053841060628241 | validation: 4.0443072697008375]
	TIME [epoch: 7.76 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.070174548329614		[learning rate: 0.0016743]
	Learning Rate: 0.00167435
	LOSS [training: 4.070174548329614 | validation: 4.044864422379812]
	TIME [epoch: 7.71 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.066320763026812		[learning rate: 0.0016704]
	Learning Rate: 0.0016704
	LOSS [training: 4.066320763026812 | validation: 4.032370349664124]
	TIME [epoch: 7.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.062642522968089		[learning rate: 0.0016665]
	Learning Rate: 0.00166646
	LOSS [training: 4.062642522968089 | validation: 4.045785324699248]
	TIME [epoch: 7.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077408468591073		[learning rate: 0.0016625]
	Learning Rate: 0.00166253
	LOSS [training: 4.077408468591073 | validation: 4.048798806319034]
	TIME [epoch: 7.74 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.082152646385713		[learning rate: 0.0016586]
	Learning Rate: 0.00165861
	LOSS [training: 4.082152646385713 | validation: 4.062246655832408]
	TIME [epoch: 7.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.107050351366225		[learning rate: 0.0016547]
	Learning Rate: 0.0016547
	LOSS [training: 4.107050351366225 | validation: 4.077587820621613]
	TIME [epoch: 7.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141994320275696		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 4.141994320275696 | validation: 4.093025501479762]
	TIME [epoch: 7.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147842855813795		[learning rate: 0.0016469]
	Learning Rate: 0.0016469
	LOSS [training: 4.147842855813795 | validation: 4.11579107929934]
	TIME [epoch: 7.71 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.17262077084462		[learning rate: 0.001643]
	Learning Rate: 0.00164301
	LOSS [training: 4.17262077084462 | validation: 4.105898875921355]
	TIME [epoch: 7.74 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142887620004692		[learning rate: 0.0016391]
	Learning Rate: 0.00163914
	LOSS [training: 4.142887620004692 | validation: 4.06558780637352]
	TIME [epoch: 7.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.109556968622337		[learning rate: 0.0016353]
	Learning Rate: 0.00163527
	LOSS [training: 4.109556968622337 | validation: 4.0636712480607144]
	TIME [epoch: 7.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122780870850392		[learning rate: 0.0016314]
	Learning Rate: 0.00163141
	LOSS [training: 4.122780870850392 | validation: 4.085795161522143]
	TIME [epoch: 7.71 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166828050271121		[learning rate: 0.0016276]
	Learning Rate: 0.00162757
	LOSS [training: 4.166828050271121 | validation: 4.135769523760376]
	TIME [epoch: 7.72 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.190976605674831		[learning rate: 0.0016237]
	Learning Rate: 0.00162373
	LOSS [training: 4.190976605674831 | validation: 4.136617154157928]
	TIME [epoch: 7.76 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.199075940283461		[learning rate: 0.0016199]
	Learning Rate: 0.0016199
	LOSS [training: 4.199075940283461 | validation: 4.148524846604934]
	TIME [epoch: 7.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.20789352576176		[learning rate: 0.0016161]
	Learning Rate: 0.00161608
	LOSS [training: 4.20789352576176 | validation: 4.142678463567528]
	TIME [epoch: 7.72 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.171647099556927		[learning rate: 0.0016123]
	Learning Rate: 0.00161226
	LOSS [training: 4.171647099556927 | validation: 4.097400521292207]
	TIME [epoch: 7.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1601306888317335		[learning rate: 0.0016085]
	Learning Rate: 0.00160846
	LOSS [training: 4.1601306888317335 | validation: 4.130048940318007]
	TIME [epoch: 7.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172437779343462		[learning rate: 0.0016047]
	Learning Rate: 0.00160467
	LOSS [training: 4.172437779343462 | validation: 4.095099874943148]
	TIME [epoch: 7.77 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.11239975885545		[learning rate: 0.0016009]
	Learning Rate: 0.00160088
	LOSS [training: 4.11239975885545 | validation: 4.044245482535219]
	TIME [epoch: 7.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.088956296797927		[learning rate: 0.0015971]
	Learning Rate: 0.0015971
	LOSS [training: 4.088956296797927 | validation: 4.053202683013792]
	TIME [epoch: 7.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.117012827554292		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 4.117012827554292 | validation: 4.102925059848491]
	TIME [epoch: 7.71 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.168173249853244		[learning rate: 0.0015896]
	Learning Rate: 0.00158958
	LOSS [training: 4.168173249853244 | validation: 4.151248089247212]
	TIME [epoch: 7.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.235742544677029		[learning rate: 0.0015858]
	Learning Rate: 0.00158583
	LOSS [training: 4.235742544677029 | validation: 4.197093647670791]
	TIME [epoch: 7.76 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2315911534424675		[learning rate: 0.0015821]
	Learning Rate: 0.00158209
	LOSS [training: 4.2315911534424675 | validation: 4.111590978332931]
	TIME [epoch: 7.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160719983836274		[learning rate: 0.0015784]
	Learning Rate: 0.00157836
	LOSS [training: 4.160719983836274 | validation: 4.10981651815428]
	TIME [epoch: 7.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14762781224992		[learning rate: 0.0015746]
	Learning Rate: 0.00157463
	LOSS [training: 4.14762781224992 | validation: 4.062599008585115]
	TIME [epoch: 7.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.104671609347943		[learning rate: 0.0015709]
	Learning Rate: 0.00157092
	LOSS [training: 4.104671609347943 | validation: 4.059127815751056]
	TIME [epoch: 7.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133260296996727		[learning rate: 0.0015672]
	Learning Rate: 0.00156721
	LOSS [training: 4.133260296996727 | validation: 4.13495267973661]
	TIME [epoch: 7.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.209564329961511		[learning rate: 0.0015635]
	Learning Rate: 0.00156352
	LOSS [training: 4.209564329961511 | validation: 4.15899143811864]
	TIME [epoch: 7.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.206410162087383		[learning rate: 0.0015598]
	Learning Rate: 0.00155983
	LOSS [training: 4.206410162087383 | validation: 4.1305727458459405]
	TIME [epoch: 7.71 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160466431137471		[learning rate: 0.0015561]
	Learning Rate: 0.00155615
	LOSS [training: 4.160466431137471 | validation: 4.081149733890477]
	TIME [epoch: 7.71 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158083996906298		[learning rate: 0.0015525]
	Learning Rate: 0.00155248
	LOSS [training: 4.158083996906298 | validation: 4.137802427990977]
	TIME [epoch: 7.71 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1852863096411514		[learning rate: 0.0015488]
	Learning Rate: 0.00154882
	LOSS [training: 4.1852863096411514 | validation: 4.134452039948209]
	TIME [epoch: 7.76 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.204392460668338		[learning rate: 0.0015452]
	Learning Rate: 0.00154516
	LOSS [training: 4.204392460668338 | validation: 4.141517135048983]
	TIME [epoch: 7.71 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18515549096141		[learning rate: 0.0015415]
	Learning Rate: 0.00154152
	LOSS [training: 4.18515549096141 | validation: 4.108152166124313]
	TIME [epoch: 7.75 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174709519559942		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 4.174709519559942 | validation: 4.142104412468017]
	TIME [epoch: 7.73 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.224847451548482		[learning rate: 0.0015343]
	Learning Rate: 0.00153425
	LOSS [training: 4.224847451548482 | validation: 4.206564033700152]
	TIME [epoch: 7.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.254370703890803		[learning rate: 0.0015306]
	Learning Rate: 0.00153064
	LOSS [training: 4.254370703890803 | validation: 4.1993004807775165]
	TIME [epoch: 7.77 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.264559901291141		[learning rate: 0.001527]
	Learning Rate: 0.00152703
	LOSS [training: 4.264559901291141 | validation: 4.217778847868901]
	TIME [epoch: 7.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2520334424564465		[learning rate: 0.0015234]
	Learning Rate: 0.00152342
	LOSS [training: 4.2520334424564465 | validation: 4.182241442636345]
	TIME [epoch: 7.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.262724913076279		[learning rate: 0.0015198]
	Learning Rate: 0.00151983
	LOSS [training: 4.262724913076279 | validation: 4.244437556004498]
	TIME [epoch: 7.73 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.307074017901692		[learning rate: 0.0015162]
	Learning Rate: 0.00151624
	LOSS [training: 4.307074017901692 | validation: 4.2586704022366835]
	TIME [epoch: 7.73 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.319600026970401		[learning rate: 0.0015127]
	Learning Rate: 0.00151267
	LOSS [training: 4.319600026970401 | validation: 4.26550087820079]
	TIME [epoch: 7.77 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268965416926772		[learning rate: 0.0015091]
	Learning Rate: 0.0015091
	LOSS [training: 4.268965416926772 | validation: 4.128343283569185]
	TIME [epoch: 7.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158847586593567		[learning rate: 0.0015055]
	Learning Rate: 0.00150554
	LOSS [training: 4.158847586593567 | validation: 4.117732141665574]
	TIME [epoch: 7.71 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164268102327762		[learning rate: 0.001502]
	Learning Rate: 0.00150199
	LOSS [training: 4.164268102327762 | validation: 4.139164396842187]
	TIME [epoch: 7.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166506598467227		[learning rate: 0.0014984]
	Learning Rate: 0.00149845
	LOSS [training: 4.166506598467227 | validation: 4.097933832794716]
	TIME [epoch: 7.75 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.105859331552865		[learning rate: 0.0014949]
	Learning Rate: 0.00149491
	LOSS [training: 4.105859331552865 | validation: 4.020679465008589]
	TIME [epoch: 7.79 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.041120783367368		[learning rate: 0.0014914]
	Learning Rate: 0.00149139
	LOSS [training: 4.041120783367368 | validation: 4.007428226087608]
	TIME [epoch: 7.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.045224883371167		[learning rate: 0.0014879]
	Learning Rate: 0.00148787
	LOSS [training: 4.045224883371167 | validation: 3.9993588174024906]
	TIME [epoch: 7.73 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002743738965818		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 4.002743738965818 | validation: 3.9546499653221208]
	TIME [epoch: 7.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016738664457121		[learning rate: 0.0014809]
	Learning Rate: 0.00148086
	LOSS [training: 4.016738664457121 | validation: 4.008327407607545]
	TIME [epoch: 7.75 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067528993148925		[learning rate: 0.0014774]
	Learning Rate: 0.00147736
	LOSS [training: 4.067528993148925 | validation: 4.042982950388511]
	TIME [epoch: 7.76 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.098786421797393		[learning rate: 0.0014739]
	Learning Rate: 0.00147388
	LOSS [training: 4.098786421797393 | validation: 4.0517068806510785]
	TIME [epoch: 7.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.097772454666599		[learning rate: 0.0014704]
	Learning Rate: 0.0014704
	LOSS [training: 4.097772454666599 | validation: 4.06074432308141]
	TIME [epoch: 7.73 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.102946450268912		[learning rate: 0.0014669]
	Learning Rate: 0.00146693
	LOSS [training: 4.102946450268912 | validation: 4.031128937297758]
	TIME [epoch: 7.72 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.038831416418137		[learning rate: 0.0014635]
	Learning Rate: 0.00146347
	LOSS [training: 4.038831416418137 | validation: 3.9637208958327528]
	TIME [epoch: 7.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001177622138567		[learning rate: 0.00146]
	Learning Rate: 0.00146002
	LOSS [training: 4.001177622138567 | validation: 3.9593725621686797]
	TIME [epoch: 7.76 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.969586062154855		[learning rate: 0.0014566]
	Learning Rate: 0.00145658
	LOSS [training: 3.969586062154855 | validation: 3.942462387812357]
	TIME [epoch: 7.72 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.961126715732236		[learning rate: 0.0014531]
	Learning Rate: 0.00145314
	LOSS [training: 3.961126715732236 | validation: 3.9545509899912417]
	TIME [epoch: 7.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.975459278695379		[learning rate: 0.0014497]
	Learning Rate: 0.00144971
	LOSS [training: 3.975459278695379 | validation: 3.954790341692152]
	TIME [epoch: 7.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9781781227154274		[learning rate: 0.0014463]
	Learning Rate: 0.00144629
	LOSS [training: 3.9781781227154274 | validation: 3.934881723434743]
	TIME [epoch: 7.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.94100188463018		[learning rate: 0.0014429]
	Learning Rate: 0.00144288
	LOSS [training: 3.94100188463018 | validation: 3.92092255842965]
	TIME [epoch: 7.75 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9066620058502157		[learning rate: 0.0014395]
	Learning Rate: 0.00143948
	LOSS [training: 3.9066620058502157 | validation: 3.895152641529026]
	TIME [epoch: 7.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.893323868251252		[learning rate: 0.0014361]
	Learning Rate: 0.00143608
	LOSS [training: 3.893323868251252 | validation: 3.8892018614595276]
	TIME [epoch: 7.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.891549943455512		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 3.891549943455512 | validation: 3.8889727601562134]
	TIME [epoch: 7.73 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.890157797488209		[learning rate: 0.0014293]
	Learning Rate: 0.00142932
	LOSS [training: 3.890157797488209 | validation: 3.8900004830612285]
	TIME [epoch: 7.75 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8918159887306256		[learning rate: 0.0014259]
	Learning Rate: 0.00142594
	LOSS [training: 3.8918159887306256 | validation: 3.892533225661942]
	TIME [epoch: 7.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888468003964655		[learning rate: 0.0014226]
	Learning Rate: 0.00142258
	LOSS [training: 3.888468003964655 | validation: 3.8906796664421854]
	TIME [epoch: 7.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8909898618138636		[learning rate: 0.0014192]
	Learning Rate: 0.00141923
	LOSS [training: 3.8909898618138636 | validation: 3.899374966451063]
	TIME [epoch: 7.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.894504321285606		[learning rate: 0.0014159]
	Learning Rate: 0.00141588
	LOSS [training: 3.894504321285606 | validation: 3.9006199184593657]
	TIME [epoch: 7.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.897258716247141		[learning rate: 0.0014125]
	Learning Rate: 0.00141254
	LOSS [training: 3.897258716247141 | validation: 3.9100164485470437]
	TIME [epoch: 7.76 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8993977454769766		[learning rate: 0.0014092]
	Learning Rate: 0.00140921
	LOSS [training: 3.8993977454769766 | validation: 3.9078518789915315]
	TIME [epoch: 7.73 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8932697325555234		[learning rate: 0.0014059]
	Learning Rate: 0.00140588
	LOSS [training: 3.8932697325555234 | validation: 3.895057869525727]
	TIME [epoch: 7.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8876651685460515		[learning rate: 0.0014026]
	Learning Rate: 0.00140257
	LOSS [training: 3.8876651685460515 | validation: 3.894693191770126]
	TIME [epoch: 7.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9005348221348104		[learning rate: 0.0013993]
	Learning Rate: 0.00139926
	LOSS [training: 3.9005348221348104 | validation: 3.9106910820062932]
	TIME [epoch: 7.73 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9271189049486006		[learning rate: 0.001396]
	Learning Rate: 0.00139596
	LOSS [training: 3.9271189049486006 | validation: 3.9382603185103564]
	TIME [epoch: 7.76 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.964179122071772		[learning rate: 0.0013927]
	Learning Rate: 0.00139266
	LOSS [training: 3.964179122071772 | validation: 3.954747712990323]
	TIME [epoch: 7.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9927559664004764		[learning rate: 0.0013894]
	Learning Rate: 0.00138938
	LOSS [training: 3.9927559664004764 | validation: 3.9709834228314183]
	TIME [epoch: 7.71 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9995645335040564		[learning rate: 0.0013861]
	Learning Rate: 0.0013861
	LOSS [training: 3.9995645335040564 | validation: 3.9795899483145774]
	TIME [epoch: 7.72 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019701232673006		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 4.019701232673006 | validation: 3.974155997341695]
	TIME [epoch: 7.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9850176632533882		[learning rate: 0.0013796]
	Learning Rate: 0.00137957
	LOSS [training: 3.9850176632533882 | validation: 3.964569382559099]
	TIME [epoch: 7.77 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9950955515376183		[learning rate: 0.0013763]
	Learning Rate: 0.00137632
	LOSS [training: 3.9950955515376183 | validation: 3.9807827912424187]
	TIME [epoch: 7.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.988450284648842		[learning rate: 0.0013731]
	Learning Rate: 0.00137307
	LOSS [training: 3.988450284648842 | validation: 3.9638145523487376]
	TIME [epoch: 7.72 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9701947799817416		[learning rate: 0.0013698]
	Learning Rate: 0.00136983
	LOSS [training: 3.9701947799817416 | validation: 3.9717979271725534]
	TIME [epoch: 7.71 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9727223709756316		[learning rate: 0.0013666]
	Learning Rate: 0.0013666
	LOSS [training: 3.9727223709756316 | validation: 3.9602156901085928]
	TIME [epoch: 7.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9757733003349816		[learning rate: 0.0013634]
	Learning Rate: 0.00136338
	LOSS [training: 3.9757733003349816 | validation: 3.978988371700658]
	TIME [epoch: 7.76 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001332603657668		[learning rate: 0.0013602]
	Learning Rate: 0.00136016
	LOSS [training: 4.001332603657668 | validation: 3.997264740018814]
	TIME [epoch: 7.72 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.034667897945979		[learning rate: 0.001357]
	Learning Rate: 0.00135695
	LOSS [training: 4.034667897945979 | validation: 4.024026129812375]
	TIME [epoch: 7.72 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0580328820198295		[learning rate: 0.0013538]
	Learning Rate: 0.00135375
	LOSS [training: 4.0580328820198295 | validation: 4.018336981063745]
	TIME [epoch: 7.71 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0583123271700074		[learning rate: 0.0013506]
	Learning Rate: 0.00135056
	LOSS [training: 4.0583123271700074 | validation: 4.010420544339977]
	TIME [epoch: 7.72 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0354091160882195		[learning rate: 0.0013474]
	Learning Rate: 0.00134737
	LOSS [training: 4.0354091160882195 | validation: 4.001115972317091]
	TIME [epoch: 7.77 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.041791177834627		[learning rate: 0.0013442]
	Learning Rate: 0.00134419
	LOSS [training: 4.041791177834627 | validation: 4.011044364196051]
	TIME [epoch: 7.72 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.046237399948586		[learning rate: 0.001341]
	Learning Rate: 0.00134102
	LOSS [training: 4.046237399948586 | validation: 4.008788411409354]
	TIME [epoch: 7.7 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.037986095958873		[learning rate: 0.0013379]
	Learning Rate: 0.00133786
	LOSS [training: 4.037986095958873 | validation: 3.988044137914422]
	TIME [epoch: 7.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016369702578845		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 4.016369702578845 | validation: 3.9914704556800285]
	TIME [epoch: 7.72 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0250481990813105		[learning rate: 0.0013316]
	Learning Rate: 0.00133155
	LOSS [training: 4.0250481990813105 | validation: 3.992591416645455]
	TIME [epoch: 7.76 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.018396085142736		[learning rate: 0.0013284]
	Learning Rate: 0.00132841
	LOSS [training: 4.018396085142736 | validation: 3.9817872759116257]
	TIME [epoch: 7.73 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.010248684598861		[learning rate: 0.0013253]
	Learning Rate: 0.00132528
	LOSS [training: 4.010248684598861 | validation: 3.9908176448309396]
	TIME [epoch: 7.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0151901624561415		[learning rate: 0.0013222]
	Learning Rate: 0.00132215
	LOSS [training: 4.0151901624561415 | validation: 3.972665211625655]
	TIME [epoch: 7.71 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9995666755076886		[learning rate: 0.001319]
	Learning Rate: 0.00131904
	LOSS [training: 3.9995666755076886 | validation: 3.9666512817989186]
	TIME [epoch: 7.72 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9810795486223096		[learning rate: 0.0013159]
	Learning Rate: 0.00131592
	LOSS [training: 3.9810795486223096 | validation: 3.9315253839331623]
	TIME [epoch: 7.77 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.943417655063556		[learning rate: 0.0013128]
	Learning Rate: 0.00131282
	LOSS [training: 3.943417655063556 | validation: 3.909922840938301]
	TIME [epoch: 7.72 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9244285125284475		[learning rate: 0.0013097]
	Learning Rate: 0.00130972
	LOSS [training: 3.9244285125284475 | validation: 3.9299725498130726]
	TIME [epoch: 7.71 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9631743147321514		[learning rate: 0.0013066]
	Learning Rate: 0.00130663
	LOSS [training: 3.9631743147321514 | validation: 3.964618170409879]
	TIME [epoch: 7.71 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9859158620901862		[learning rate: 0.0013036]
	Learning Rate: 0.00130355
	LOSS [training: 3.9859158620901862 | validation: 3.9589198619659536]
	TIME [epoch: 7.73 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9792623248286683		[learning rate: 0.0013005]
	Learning Rate: 0.00130048
	LOSS [training: 3.9792623248286683 | validation: 3.9758611150388763]
	TIME [epoch: 7.76 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.987805640652341		[learning rate: 0.0012974]
	Learning Rate: 0.00129741
	LOSS [training: 3.987805640652341 | validation: 3.9769633916785434]
	TIME [epoch: 7.71 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9833539315484963		[learning rate: 0.0012943]
	Learning Rate: 0.00129435
	LOSS [training: 3.9833539315484963 | validation: 3.9773154392519006]
	TIME [epoch: 7.72 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9769232070592917		[learning rate: 0.0012913]
	Learning Rate: 0.0012913
	LOSS [training: 3.9769232070592917 | validation: 3.981431285256953]
	TIME [epoch: 7.72 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.97570751994334		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 3.97570751994334 | validation: 3.9755663517681104]
	TIME [epoch: 7.74 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9654309760067337		[learning rate: 0.0012852]
	Learning Rate: 0.00128521
	LOSS [training: 3.9654309760067337 | validation: 3.979774888477935]
	TIME [epoch: 7.74 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9661847322252672		[learning rate: 0.0012822]
	Learning Rate: 0.00128218
	LOSS [training: 3.9661847322252672 | validation: 3.9752662206486677]
	TIME [epoch: 7.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.943543531316866		[learning rate: 0.0012792]
	Learning Rate: 0.00127915
	LOSS [training: 3.943543531316866 | validation: 3.9683465409032195]
	TIME [epoch: 7.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9381231876507217		[learning rate: 0.0012761]
	Learning Rate: 0.00127614
	LOSS [training: 3.9381231876507217 | validation: 3.956649784010086]
	TIME [epoch: 7.71 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9265370866670186		[learning rate: 0.0012731]
	Learning Rate: 0.00127313
	LOSS [training: 3.9265370866670186 | validation: 3.951601051562344]
	TIME [epoch: 7.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923174251412111		[learning rate: 0.0012701]
	Learning Rate: 0.00127012
	LOSS [training: 3.923174251412111 | validation: 3.9536225248820123]
	TIME [epoch: 7.75 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9344658034055824		[learning rate: 0.0012671]
	Learning Rate: 0.00126713
	LOSS [training: 3.9344658034055824 | validation: 3.9541778834891317]
	TIME [epoch: 7.71 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9320106404405886		[learning rate: 0.0012641]
	Learning Rate: 0.00126414
	LOSS [training: 3.9320106404405886 | validation: 3.96060426161364]
	TIME [epoch: 7.71 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9405533782721087		[learning rate: 0.0012612]
	Learning Rate: 0.00126116
	LOSS [training: 3.9405533782721087 | validation: 3.972926101839673]
	TIME [epoch: 7.77 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.938132880478342		[learning rate: 0.0012582]
	Learning Rate: 0.00125818
	LOSS [training: 3.938132880478342 | validation: 3.9611143780264326]
	TIME [epoch: 7.74 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9281591153802533		[learning rate: 0.0012552]
	Learning Rate: 0.00125521
	LOSS [training: 3.9281591153802533 | validation: 3.943165973122955]
	TIME [epoch: 7.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.916503361297576		[learning rate: 0.0012523]
	Learning Rate: 0.00125225
	LOSS [training: 3.916503361297576 | validation: 3.9425266738351175]
	TIME [epoch: 7.71 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923102219571927		[learning rate: 0.0012493]
	Learning Rate: 0.0012493
	LOSS [training: 3.923102219571927 | validation: 3.9533051253517972]
	TIME [epoch: 7.71 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9290572793007033		[learning rate: 0.0012464]
	Learning Rate: 0.00124635
	LOSS [training: 3.9290572793007033 | validation: 3.942785307958765]
	TIME [epoch: 7.71 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9232161044322327		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 3.9232161044322327 | validation: 3.948289409661287]
	TIME [epoch: 7.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9365041997843595		[learning rate: 0.0012405]
	Learning Rate: 0.00124048
	LOSS [training: 3.9365041997843595 | validation: 3.9474580627578764]
	TIME [epoch: 7.72 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9306804763091585		[learning rate: 0.0012376]
	Learning Rate: 0.00123755
	LOSS [training: 3.9306804763091585 | validation: 3.9440334448338827]
	TIME [epoch: 7.71 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9216530309149236		[learning rate: 0.0012346]
	Learning Rate: 0.00123463
	LOSS [training: 3.9216530309149236 | validation: 3.9506239972038415]
	TIME [epoch: 7.72 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.921793397392659		[learning rate: 0.0012317]
	Learning Rate: 0.00123172
	LOSS [training: 3.921793397392659 | validation: 3.9471086984564088]
	TIME [epoch: 7.71 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9151033031049804		[learning rate: 0.0012288]
	Learning Rate: 0.00122882
	LOSS [training: 3.9151033031049804 | validation: 3.939034872966201]
	TIME [epoch: 7.76 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909156753209845		[learning rate: 0.0012259]
	Learning Rate: 0.00122592
	LOSS [training: 3.909156753209845 | validation: 3.9374187787371513]
	TIME [epoch: 7.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.90964587328496		[learning rate: 0.001223]
	Learning Rate: 0.00122303
	LOSS [training: 3.90964587328496 | validation: 3.935168988789505]
	TIME [epoch: 7.71 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909076277593061		[learning rate: 0.0012201]
	Learning Rate: 0.00122014
	LOSS [training: 3.909076277593061 | validation: 3.940183981347352]
	TIME [epoch: 7.71 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.915442405609415		[learning rate: 0.0012173]
	Learning Rate: 0.00121726
	LOSS [training: 3.915442405609415 | validation: 3.949684478823281]
	TIME [epoch: 7.72 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.917748480661163		[learning rate: 0.0012144]
	Learning Rate: 0.00121439
	LOSS [training: 3.917748480661163 | validation: 3.9387071518817756]
	TIME [epoch: 7.75 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9076978747318467		[learning rate: 0.0012115]
	Learning Rate: 0.00121153
	LOSS [training: 3.9076978747318467 | validation: 3.922754451519995]
	TIME [epoch: 7.72 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9049776685288813		[learning rate: 0.0012087]
	Learning Rate: 0.00120867
	LOSS [training: 3.9049776685288813 | validation: 3.937564842871976]
	TIME [epoch: 7.71 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9141011328161284		[learning rate: 0.0012058]
	Learning Rate: 0.00120582
	LOSS [training: 3.9141011328161284 | validation: 3.9405908284011204]
	TIME [epoch: 7.71 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.917043420430612		[learning rate: 0.001203]
	Learning Rate: 0.00120297
	LOSS [training: 3.917043420430612 | validation: 3.9475861065554625]
	TIME [epoch: 7.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.91872268064623		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 3.91872268064623 | validation: 3.934519193119979]
	TIME [epoch: 7.76 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9115572263242946		[learning rate: 0.0011973]
	Learning Rate: 0.00119731
	LOSS [training: 3.9115572263242946 | validation: 3.9418562882370445]
	TIME [epoch: 7.71 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.912558797078633		[learning rate: 0.0011945]
	Learning Rate: 0.00119448
	LOSS [training: 3.912558797078633 | validation: 3.930479482860197]
	TIME [epoch: 7.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909018997570273		[learning rate: 0.0011917]
	Learning Rate: 0.00119166
	LOSS [training: 3.909018997570273 | validation: 3.943216228564553]
	TIME [epoch: 7.71 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9191153199004667		[learning rate: 0.0011889]
	Learning Rate: 0.00118885
	LOSS [training: 3.9191153199004667 | validation: 3.953556531499273]
	TIME [epoch: 7.72 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9207097397591175		[learning rate: 0.001186]
	Learning Rate: 0.00118605
	LOSS [training: 3.9207097397591175 | validation: 3.9451573800595456]
	TIME [epoch: 7.76 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.914546117305955		[learning rate: 0.0011833]
	Learning Rate: 0.00118325
	LOSS [training: 3.914546117305955 | validation: 3.938449821409994]
	TIME [epoch: 7.71 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9089689428633436		[learning rate: 0.0011805]
	Learning Rate: 0.00118046
	LOSS [training: 3.9089689428633436 | validation: 3.942175417119778]
	TIME [epoch: 7.71 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9174556296180487		[learning rate: 0.0011777]
	Learning Rate: 0.00117768
	LOSS [training: 3.9174556296180487 | validation: 3.9533683016546863]
	TIME [epoch: 7.71 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9220812623074326		[learning rate: 0.0011749]
	Learning Rate: 0.0011749
	LOSS [training: 3.9220812623074326 | validation: 3.949059795324785]
	TIME [epoch: 7.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9208594522815305		[learning rate: 0.0011721]
	Learning Rate: 0.00117213
	LOSS [training: 3.9208594522815305 | validation: 3.9553393481745154]
	TIME [epoch: 7.76 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9266159719362967		[learning rate: 0.0011694]
	Learning Rate: 0.00116936
	LOSS [training: 3.9266159719362967 | validation: 3.951034956696752]
	TIME [epoch: 7.71 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.920874088473842		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 3.920874088473842 | validation: 3.948060182860395]
	TIME [epoch: 7.71 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.921089894663735		[learning rate: 0.0011639]
	Learning Rate: 0.00116385
	LOSS [training: 3.921089894663735 | validation: 3.954221985873439]
	TIME [epoch: 7.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9336968664823524		[learning rate: 0.0011611]
	Learning Rate: 0.00116111
	LOSS [training: 3.9336968664823524 | validation: 3.9650086283469443]
	TIME [epoch: 7.72 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9280370695242572		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 3.9280370695242572 | validation: 3.9503164348558246]
	TIME [epoch: 7.76 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.921948605121134		[learning rate: 0.0011556]
	Learning Rate: 0.00115563
	LOSS [training: 3.921948605121134 | validation: 3.950391382420551]
	TIME [epoch: 7.71 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.91872057994064		[learning rate: 0.0011529]
	Learning Rate: 0.00115291
	LOSS [training: 3.91872057994064 | validation: 3.9440347485574914]
	TIME [epoch: 7.72 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9128530966420074		[learning rate: 0.0011502]
	Learning Rate: 0.00115019
	LOSS [training: 3.9128530966420074 | validation: 3.938961454883051]
	TIME [epoch: 7.71 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.906583485034218		[learning rate: 0.0011475]
	Learning Rate: 0.00114748
	LOSS [training: 3.906583485034218 | validation: 3.9247891470530027]
	TIME [epoch: 7.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.894758269213507		[learning rate: 0.0011448]
	Learning Rate: 0.00114477
	LOSS [training: 3.894758269213507 | validation: 3.9126037954766213]
	TIME [epoch: 7.76 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8834964274421244		[learning rate: 0.0011421]
	Learning Rate: 0.00114207
	LOSS [training: 3.8834964274421244 | validation: 3.8949961050943838]
	TIME [epoch: 7.71 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872281685723751		[learning rate: 0.0011394]
	Learning Rate: 0.00113937
	LOSS [training: 3.872281685723751 | validation: 3.8881091909099714]
	TIME [epoch: 7.71 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.892482952897671		[learning rate: 0.0011367]
	Learning Rate: 0.00113669
	LOSS [training: 3.892482952897671 | validation: 3.909168073636348]
	TIME [epoch: 7.71 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8981222279315366		[learning rate: 0.001134]
	Learning Rate: 0.00113401
	LOSS [training: 3.8981222279315366 | validation: 3.907816237721132]
	TIME [epoch: 7.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888215105381897		[learning rate: 0.0011313]
	Learning Rate: 0.00113133
	LOSS [training: 3.888215105381897 | validation: 3.9103990538993427]
	TIME [epoch: 7.75 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8853214676861025		[learning rate: 0.0011287]
	Learning Rate: 0.00112866
	LOSS [training: 3.8853214676861025 | validation: 3.900631034977776]
	TIME [epoch: 7.71 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8790915150242804		[learning rate: 0.001126]
	Learning Rate: 0.001126
	LOSS [training: 3.8790915150242804 | validation: 3.9130782789126446]
	TIME [epoch: 7.71 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8915787508199267		[learning rate: 0.0011233]
	Learning Rate: 0.00112334
	LOSS [training: 3.8915787508199267 | validation: 3.925325177837883]
	TIME [epoch: 7.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9037576130654426		[learning rate: 0.0011207]
	Learning Rate: 0.00112069
	LOSS [training: 3.9037576130654426 | validation: 3.9374209467672703]
	TIME [epoch: 7.73 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909585278107267		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 3.909585278107267 | validation: 3.9278715796664008]
	TIME [epoch: 7.75 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9021353514870993		[learning rate: 0.0011154]
	Learning Rate: 0.00111541
	LOSS [training: 3.9021353514870993 | validation: 3.9300371756139056]
	TIME [epoch: 7.71 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8951498209427404		[learning rate: 0.0011128]
	Learning Rate: 0.00111278
	LOSS [training: 3.8951498209427404 | validation: 3.9195553611984373]
	TIME [epoch: 7.72 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888312111585752		[learning rate: 0.0011102]
	Learning Rate: 0.00111016
	LOSS [training: 3.888312111585752 | validation: 3.912405570296841]
	TIME [epoch: 7.72 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8857006862679357		[learning rate: 0.0011075]
	Learning Rate: 0.00110754
	LOSS [training: 3.8857006862679357 | validation: 3.9112759683824376]
	TIME [epoch: 7.74 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.880220664952445		[learning rate: 0.0011049]
	Learning Rate: 0.00110493
	LOSS [training: 3.880220664952445 | validation: 3.894282270298822]
	TIME [epoch: 7.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.860345269859316		[learning rate: 0.0011023]
	Learning Rate: 0.00110232
	LOSS [training: 3.860345269859316 | validation: 3.876567180406022]
	TIME [epoch: 7.71 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.853412595788918		[learning rate: 0.0010997]
	Learning Rate: 0.00109972
	LOSS [training: 3.853412595788918 | validation: 3.8676017449081224]
	TIME [epoch: 7.71 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8436254550824396		[learning rate: 0.0010971]
	Learning Rate: 0.00109713
	LOSS [training: 3.8436254550824396 | validation: 3.8600272831573026]
	TIME [epoch: 7.71 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8393978698671543		[learning rate: 0.0010945]
	Learning Rate: 0.00109454
	LOSS [training: 3.8393978698671543 | validation: 3.8527464076352063]
	TIME [epoch: 7.75 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8414379544110613		[learning rate: 0.001092]
	Learning Rate: 0.00109196
	LOSS [training: 3.8414379544110613 | validation: 3.864883371401477]
	TIME [epoch: 7.74 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841534589327835		[learning rate: 0.0010894]
	Learning Rate: 0.00108938
	LOSS [training: 3.841534589327835 | validation: 3.846176436299194]
	TIME [epoch: 7.71 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8348021440352746		[learning rate: 0.0010868]
	Learning Rate: 0.00108681
	LOSS [training: 3.8348021440352746 | validation: 3.847740351637994]
	TIME [epoch: 7.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.832450125294383		[learning rate: 0.0010842]
	Learning Rate: 0.00108425
	LOSS [training: 3.832450125294383 | validation: 3.838843664472752]
	TIME [epoch: 7.72 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824748535193014		[learning rate: 0.0010817]
	Learning Rate: 0.00108169
	LOSS [training: 3.824748535193014 | validation: 3.8283832354958323]
	TIME [epoch: 7.76 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821457462261465		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 3.821457462261465 | validation: 3.828401471459842]
	TIME [epoch: 7.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819739938789935		[learning rate: 0.0010766]
	Learning Rate: 0.00107659
	LOSS [training: 3.819739938789935 | validation: 3.823880740905329]
	TIME [epoch: 7.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8348754276725807		[learning rate: 0.0010741]
	Learning Rate: 0.00107405
	LOSS [training: 3.8348754276725807 | validation: 3.8428386022914633]
	TIME [epoch: 7.71 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840466685068368		[learning rate: 0.0010715]
	Learning Rate: 0.00107152
	LOSS [training: 3.840466685068368 | validation: 3.8361197648862175]
	TIME [epoch: 7.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.827909521176782		[learning rate: 0.001069]
	Learning Rate: 0.00106899
	LOSS [training: 3.827909521176782 | validation: 3.833867690545465]
	TIME [epoch: 7.76 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824182272398218		[learning rate: 0.0010665]
	Learning Rate: 0.00106647
	LOSS [training: 3.824182272398218 | validation: 3.823416778173149]
	TIME [epoch: 7.72 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8123590452333365		[learning rate: 0.001064]
	Learning Rate: 0.00106395
	LOSS [training: 3.8123590452333365 | validation: 3.8184688440158103]
	TIME [epoch: 7.71 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811223711685727		[learning rate: 0.0010614]
	Learning Rate: 0.00106144
	LOSS [training: 3.811223711685727 | validation: 3.8178469226078215]
	TIME [epoch: 7.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.812757510515326		[learning rate: 0.0010589]
	Learning Rate: 0.00105894
	LOSS [training: 3.812757510515326 | validation: 3.8214626275969543]
	TIME [epoch: 7.72 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819717092585132		[learning rate: 0.0010564]
	Learning Rate: 0.00105644
	LOSS [training: 3.819717092585132 | validation: 3.8264298418221685]
	TIME [epoch: 7.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8186298259989213		[learning rate: 0.001054]
	Learning Rate: 0.00105395
	LOSS [training: 3.8186298259989213 | validation: 3.827712068393283]
	TIME [epoch: 7.72 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821700907050748		[learning rate: 0.0010515]
	Learning Rate: 0.00105147
	LOSS [training: 3.821700907050748 | validation: 3.8313826370337907]
	TIME [epoch: 7.72 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.830423696310656		[learning rate: 0.001049]
	Learning Rate: 0.00104898
	LOSS [training: 3.830423696310656 | validation: 3.855284026045794]
	TIME [epoch: 7.72 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8439259179923284		[learning rate: 0.0010465]
	Learning Rate: 0.00104651
	LOSS [training: 3.8439259179923284 | validation: 3.855821645761792]
	TIME [epoch: 7.73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8372613364554584		[learning rate: 0.001044]
	Learning Rate: 0.00104404
	LOSS [training: 3.8372613364554584 | validation: 3.8462005528951826]
	TIME [epoch: 7.77 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8310214522346113		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 3.8310214522346113 | validation: 3.841251094069906]
	TIME [epoch: 7.71 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8313042913597872		[learning rate: 0.0010391]
	Learning Rate: 0.00103912
	LOSS [training: 3.8313042913597872 | validation: 3.8334601644480104]
	TIME [epoch: 7.71 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8269040208349745		[learning rate: 0.0010367]
	Learning Rate: 0.00103667
	LOSS [training: 3.8269040208349745 | validation: 3.828973599557991]
	TIME [epoch: 7.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826036008457517		[learning rate: 0.0010342]
	Learning Rate: 0.00103423
	LOSS [training: 3.826036008457517 | validation: 3.8338320566937654]
	TIME [epoch: 7.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8303450972525788		[learning rate: 0.0010318]
	Learning Rate: 0.00103179
	LOSS [training: 3.8303450972525788 | validation: 3.834270229317567]
	TIME [epoch: 7.76 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.827840640191673		[learning rate: 0.0010294]
	Learning Rate: 0.00102935
	LOSS [training: 3.827840640191673 | validation: 3.8342986506535457]
	TIME [epoch: 7.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.828060395627439		[learning rate: 0.0010269]
	Learning Rate: 0.00102692
	LOSS [training: 3.828060395627439 | validation: 3.8264595812288325]
	TIME [epoch: 7.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8234336563780627		[learning rate: 0.0010245]
	Learning Rate: 0.0010245
	LOSS [training: 3.8234336563780627 | validation: 3.8235477574108296]
	TIME [epoch: 7.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8247709907718197		[learning rate: 0.0010221]
	Learning Rate: 0.00102209
	LOSS [training: 3.8247709907718197 | validation: 3.8235908281414943]
	TIME [epoch: 7.74 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826978840235208		[learning rate: 0.0010197]
	Learning Rate: 0.00101967
	LOSS [training: 3.826978840235208 | validation: 3.8264535585483097]
	TIME [epoch: 7.75 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8321664588365874		[learning rate: 0.0010173]
	Learning Rate: 0.00101727
	LOSS [training: 3.8321664588365874 | validation: 3.829335784375812]
	TIME [epoch: 7.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.834086356787383		[learning rate: 0.0010149]
	Learning Rate: 0.00101487
	LOSS [training: 3.834086356787383 | validation: 3.8313900598459463]
	TIME [epoch: 7.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8393593971039404		[learning rate: 0.0010125]
	Learning Rate: 0.00101248
	LOSS [training: 3.8393593971039404 | validation: 3.8345172008243686]
	TIME [epoch: 7.72 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8379357752675167		[learning rate: 0.0010101]
	Learning Rate: 0.00101009
	LOSS [training: 3.8379357752675167 | validation: 3.830448245433602]
	TIME [epoch: 7.73 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.835819117804		[learning rate: 0.0010077]
	Learning Rate: 0.0010077
	LOSS [training: 3.835819117804 | validation: 3.833655280447995]
	TIME [epoch: 7.76 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8385580862800506		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 3.8385580862800506 | validation: 3.8380495155482057]
	TIME [epoch: 7.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8414304925763427		[learning rate: 0.001003]
	Learning Rate: 0.00100296
	LOSS [training: 3.8414304925763427 | validation: 3.8384123327781055]
	TIME [epoch: 7.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.846598972277012		[learning rate: 0.0010006]
	Learning Rate: 0.00100059
	LOSS [training: 3.846598972277012 | validation: 3.8417381577907066]
	TIME [epoch: 7.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8530882960845205		[learning rate: 0.00099823]
	Learning Rate: 0.000998231
	LOSS [training: 3.8530882960845205 | validation: 3.847276805061858]
	TIME [epoch: 7.74 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8594426239217086		[learning rate: 0.00099588]
	Learning Rate: 0.000995876
	LOSS [training: 3.8594426239217086 | validation: 3.8536424547007235]
	TIME [epoch: 7.75 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8664017909768287		[learning rate: 0.00099353]
	Learning Rate: 0.000993527
	LOSS [training: 3.8664017909768287 | validation: 3.857619435357506]
	TIME [epoch: 7.72 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8691191066292028		[learning rate: 0.00099118]
	Learning Rate: 0.000991183
	LOSS [training: 3.8691191066292028 | validation: 3.8581303925179684]
	TIME [epoch: 7.72 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879165213351718		[learning rate: 0.00098885]
	Learning Rate: 0.000988845
	LOSS [training: 3.879165213351718 | validation: 3.8832779307608494]
	TIME [epoch: 7.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.903459668157499		[learning rate: 0.00098651]
	Learning Rate: 0.000986513
	LOSS [training: 3.903459668157499 | validation: 3.904984960432543]
	TIME [epoch: 7.75 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9271225464067867		[learning rate: 0.00098419]
	Learning Rate: 0.000984185
	LOSS [training: 3.9271225464067867 | validation: 3.890715245739761]
	TIME [epoch: 7.76 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8986508573611283		[learning rate: 0.00098186]
	Learning Rate: 0.000981864
	LOSS [training: 3.8986508573611283 | validation: 3.8780406961047476]
	TIME [epoch: 7.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8830047938041505		[learning rate: 0.00097955]
	Learning Rate: 0.000979548
	LOSS [training: 3.8830047938041505 | validation: 3.8559658935578867]
	TIME [epoch: 7.72 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8637151266625684		[learning rate: 0.00097724]
	Learning Rate: 0.000977237
	LOSS [training: 3.8637151266625684 | validation: 3.8445890076096716]
	TIME [epoch: 7.72 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.844690337448288		[learning rate: 0.00097493]
	Learning Rate: 0.000974932
	LOSS [training: 3.844690337448288 | validation: 3.8275162943249867]
	TIME [epoch: 7.75 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.834852315128635		[learning rate: 0.00097263]
	Learning Rate: 0.000972632
	LOSS [training: 3.834852315128635 | validation: 3.833767890532643]
	TIME [epoch: 7.76 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841719981024582		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 3.841719981024582 | validation: 3.8378762595996516]
	TIME [epoch: 7.72 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8385327510083584		[learning rate: 0.00096805]
	Learning Rate: 0.000968049
	LOSS [training: 3.8385327510083584 | validation: 3.8305287680554096]
	TIME [epoch: 7.72 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.833789008103666		[learning rate: 0.00096577]
	Learning Rate: 0.000965766
	LOSS [training: 3.833789008103666 | validation: 3.8281010701532443]
	TIME [epoch: 7.72 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.835288620226955		[learning rate: 0.00096349]
	Learning Rate: 0.000963488
	LOSS [training: 3.835288620226955 | validation: 3.8288393576877207]
	TIME [epoch: 7.76 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8349850254529443		[learning rate: 0.00096121]
	Learning Rate: 0.000961215
	LOSS [training: 3.8349850254529443 | validation: 3.8252770361488544]
	TIME [epoch: 7.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8291553683517607		[learning rate: 0.00095895]
	Learning Rate: 0.000958948
	LOSS [training: 3.8291553683517607 | validation: 3.82441066072594]
	TIME [epoch: 7.72 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826590325838765		[learning rate: 0.00095669]
	Learning Rate: 0.000956686
	LOSS [training: 3.826590325838765 | validation: 3.8237453531058896]
	TIME [epoch: 7.72 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.823989804970795		[learning rate: 0.00095443]
	Learning Rate: 0.000954429
	LOSS [training: 3.823989804970795 | validation: 3.81871768923756]
	TIME [epoch: 7.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8213920521158875		[learning rate: 0.00095218]
	Learning Rate: 0.000952178
	LOSS [training: 3.8213920521158875 | validation: 3.8174993864388203]
	TIME [epoch: 7.76 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8210589386066856		[learning rate: 0.00094993]
	Learning Rate: 0.000949932
	LOSS [training: 3.8210589386066856 | validation: 3.8145833145582304]
	TIME [epoch: 7.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816659436460376		[learning rate: 0.00094769]
	Learning Rate: 0.000947691
	LOSS [training: 3.816659436460376 | validation: 3.811150894930887]
	TIME [epoch: 7.72 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8148445609874946		[learning rate: 0.00094546]
	Learning Rate: 0.000945455
	LOSS [training: 3.8148445609874946 | validation: 3.81213043635548]
	TIME [epoch: 7.72 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8148427383030805		[learning rate: 0.00094323]
	Learning Rate: 0.000943225
	LOSS [training: 3.8148427383030805 | validation: 3.8151042594488054]
	TIME [epoch: 7.72 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8188968881674747		[learning rate: 0.000941]
	Learning Rate: 0.000941
	LOSS [training: 3.8188968881674747 | validation: 3.818089335284128]
	TIME [epoch: 7.76 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820931704939866		[learning rate: 0.00093878]
	Learning Rate: 0.000938781
	LOSS [training: 3.820931704939866 | validation: 3.8186245248260797]
	TIME [epoch: 7.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8207505789453964		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 3.8207505789453964 | validation: 3.8225285442244923]
	TIME [epoch: 7.72 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8182007542817487		[learning rate: 0.00093436]
	Learning Rate: 0.000934357
	LOSS [training: 3.8182007542817487 | validation: 3.823145485617962]
	TIME [epoch: 7.72 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8215197940069743		[learning rate: 0.00093215]
	Learning Rate: 0.000932153
	LOSS [training: 3.8215197940069743 | validation: 3.820834096171798]
	TIME [epoch: 7.72 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8238545445944654		[learning rate: 0.00092995]
	Learning Rate: 0.000929954
	LOSS [training: 3.8238545445944654 | validation: 3.8240340068286693]
	TIME [epoch: 7.77 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8252740854559137		[learning rate: 0.00092776]
	Learning Rate: 0.000927761
	LOSS [training: 3.8252740854559137 | validation: 3.83070322628849]
	TIME [epoch: 7.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.827695046262762		[learning rate: 0.00092557]
	Learning Rate: 0.000925572
	LOSS [training: 3.827695046262762 | validation: 3.834710284722801]
	TIME [epoch: 7.72 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8315923998237027		[learning rate: 0.00092339]
	Learning Rate: 0.000923389
	LOSS [training: 3.8315923998237027 | validation: 3.847654386253078]
	TIME [epoch: 7.73 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840520518078646		[learning rate: 0.00092121]
	Learning Rate: 0.000921211
	LOSS [training: 3.840520518078646 | validation: 3.84991870093011]
	TIME [epoch: 7.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840236311986133		[learning rate: 0.00091904]
	Learning Rate: 0.000919038
	LOSS [training: 3.840236311986133 | validation: 3.8551398237978196]
	TIME [epoch: 7.76 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8452620272269407		[learning rate: 0.00091687]
	Learning Rate: 0.00091687
	LOSS [training: 3.8452620272269407 | validation: 3.8531456622844074]
	TIME [epoch: 7.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8467409009724807		[learning rate: 0.00091471]
	Learning Rate: 0.000914707
	LOSS [training: 3.8467409009724807 | validation: 3.848684045880603]
	TIME [epoch: 7.71 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8422652682015004		[learning rate: 0.00091255]
	Learning Rate: 0.00091255
	LOSS [training: 3.8422652682015004 | validation: 3.8409345666635843]
	TIME [epoch: 7.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8351608687230403		[learning rate: 0.0009104]
	Learning Rate: 0.000910397
	LOSS [training: 3.8351608687230403 | validation: 3.8388990731558543]
	TIME [epoch: 7.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.836295808704093		[learning rate: 0.00090825]
	Learning Rate: 0.00090825
	LOSS [training: 3.836295808704093 | validation: 3.834764514192809]
	TIME [epoch: 7.76 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8331998832906433		[learning rate: 0.00090611]
	Learning Rate: 0.000906107
	LOSS [training: 3.8331998832906433 | validation: 3.829116597326193]
	TIME [epoch: 7.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8306812864933395		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 3.8306812864933395 | validation: 3.828422828918468]
	TIME [epoch: 7.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8392354987419735		[learning rate: 0.00090184]
	Learning Rate: 0.000901837
	LOSS [training: 3.8392354987419735 | validation: 3.847779828599177]
	TIME [epoch: 7.74 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8661590739671707		[learning rate: 0.00089971]
	Learning Rate: 0.00089971
	LOSS [training: 3.8661590739671707 | validation: 3.855759855159169]
	TIME [epoch: 7.73 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863689715199443		[learning rate: 0.00089759]
	Learning Rate: 0.000897588
	LOSS [training: 3.863689715199443 | validation: 3.841616559119398]
	TIME [epoch: 7.77 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8519466462827046		[learning rate: 0.00089547]
	Learning Rate: 0.00089547
	LOSS [training: 3.8519466462827046 | validation: 3.835870378706341]
	TIME [epoch: 7.72 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840772819377845		[learning rate: 0.00089336]
	Learning Rate: 0.000893358
	LOSS [training: 3.840772819377845 | validation: 3.8287438166280903]
	TIME [epoch: 7.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831213606747867		[learning rate: 0.00089125]
	Learning Rate: 0.000891251
	LOSS [training: 3.831213606747867 | validation: 3.8271719267050077]
	TIME [epoch: 7.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8301180360117804		[learning rate: 0.00088915]
	Learning Rate: 0.000889149
	LOSS [training: 3.8301180360117804 | validation: 3.829054146153535]
	TIME [epoch: 7.73 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831555254060019		[learning rate: 0.00088705]
	Learning Rate: 0.000887051
	LOSS [training: 3.831555254060019 | validation: 3.8391843926662776]
	TIME [epoch: 7.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.834936954964813		[learning rate: 0.00088496]
	Learning Rate: 0.000884959
	LOSS [training: 3.834936954964813 | validation: 3.838496113943081]
	TIME [epoch: 7.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8340302044329873		[learning rate: 0.00088287]
	Learning Rate: 0.000882871
	LOSS [training: 3.8340302044329873 | validation: 3.8466415092074846]
	TIME [epoch: 7.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8380710687355917		[learning rate: 0.00088079]
	Learning Rate: 0.000880789
	LOSS [training: 3.8380710687355917 | validation: 3.8411700327725615]
	TIME [epoch: 7.72 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8357569315398616		[learning rate: 0.00087871]
	Learning Rate: 0.000878711
	LOSS [training: 3.8357569315398616 | validation: 3.846931157855764]
	TIME [epoch: 7.74 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840260314508715		[learning rate: 0.00087664]
	Learning Rate: 0.000876639
	LOSS [training: 3.840260314508715 | validation: 3.84904581776866]
	TIME [epoch: 7.75 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8404027665768274		[learning rate: 0.00087457]
	Learning Rate: 0.000874571
	LOSS [training: 3.8404027665768274 | validation: 3.8490017532727343]
	TIME [epoch: 7.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8401302130078325		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 3.8401302130078325 | validation: 3.844496134155502]
	TIME [epoch: 7.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8356711434535566		[learning rate: 0.00087045]
	Learning Rate: 0.00087045
	LOSS [training: 3.8356711434535566 | validation: 3.837239494449582]
	TIME [epoch: 7.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8307069346275275		[learning rate: 0.0008684]
	Learning Rate: 0.000868396
	LOSS [training: 3.8307069346275275 | validation: 3.8414439504578524]
	TIME [epoch: 7.74 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8360739088017706		[learning rate: 0.00086635]
	Learning Rate: 0.000866348
	LOSS [training: 3.8360739088017706 | validation: 3.8490061003408176]
	TIME [epoch: 7.76 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840899042091638		[learning rate: 0.0008643]
	Learning Rate: 0.000864304
	LOSS [training: 3.840899042091638 | validation: 3.8490748088619386]
	TIME [epoch: 7.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845008273220675		[learning rate: 0.00086227]
	Learning Rate: 0.000862266
	LOSS [training: 3.845008273220675 | validation: 3.849673676010543]
	TIME [epoch: 7.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8454078244695626		[learning rate: 0.00086023]
	Learning Rate: 0.000860232
	LOSS [training: 3.8454078244695626 | validation: 3.8554144186780164]
	TIME [epoch: 7.72 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848871600690159		[learning rate: 0.0008582]
	Learning Rate: 0.000858202
	LOSS [training: 3.848871600690159 | validation: 3.8492843207426217]
	TIME [epoch: 7.76 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845066603038655		[learning rate: 0.00085618]
	Learning Rate: 0.000856178
	LOSS [training: 3.845066603038655 | validation: 3.848264945022291]
	TIME [epoch: 7.74 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.843926977345486		[learning rate: 0.00085416]
	Learning Rate: 0.000854159
	LOSS [training: 3.843926977345486 | validation: 3.8505727681868986]
	TIME [epoch: 7.72 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8491003152158774		[learning rate: 0.00085214]
	Learning Rate: 0.000852144
	LOSS [training: 3.8491003152158774 | validation: 3.8531525262011193]
	TIME [epoch: 7.72 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8556717456657332		[learning rate: 0.00085013]
	Learning Rate: 0.000850134
	LOSS [training: 3.8556717456657332 | validation: 3.860130059148461]
	TIME [epoch: 7.73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8563386872863825		[learning rate: 0.00084813]
	Learning Rate: 0.000848128
	LOSS [training: 3.8563386872863825 | validation: 3.8582036770269195]
	TIME [epoch: 7.76 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8541271129226757		[learning rate: 0.00084613]
	Learning Rate: 0.000846128
	LOSS [training: 3.8541271129226757 | validation: 3.8514458612812534]
	TIME [epoch: 7.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8534247189378097		[learning rate: 0.00084413]
	Learning Rate: 0.000844132
	LOSS [training: 3.8534247189378097 | validation: 3.846679729336927]
	TIME [epoch: 7.72 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847692410767952		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 3.847692410767952 | validation: 3.8437072235273817]
	TIME [epoch: 7.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8499346842478896		[learning rate: 0.00084015]
	Learning Rate: 0.000840154
	LOSS [training: 3.8499346842478896 | validation: 3.844465121245746]
	TIME [epoch: 7.72 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845197956109957		[learning rate: 0.00083817]
	Learning Rate: 0.000838173
	LOSS [training: 3.845197956109957 | validation: 3.838582343033612]
	TIME [epoch: 7.76 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405302031479636		[learning rate: 0.0008362]
	Learning Rate: 0.000836195
	LOSS [training: 3.8405302031479636 | validation: 3.839453961774585]
	TIME [epoch: 7.72 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8444292787205043		[learning rate: 0.00083422]
	Learning Rate: 0.000834223
	LOSS [training: 3.8444292787205043 | validation: 3.8373578430429287]
	TIME [epoch: 7.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842532681677236		[learning rate: 0.00083226]
	Learning Rate: 0.000832255
	LOSS [training: 3.842532681677236 | validation: 3.8392970402485695]
	TIME [epoch: 7.71 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8491769335494017		[learning rate: 0.00083029]
	Learning Rate: 0.000830292
	LOSS [training: 3.8491769335494017 | validation: 3.8466599526965517]
	TIME [epoch: 7.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857177104924388		[learning rate: 0.00082833]
	Learning Rate: 0.000828333
	LOSS [training: 3.857177104924388 | validation: 3.844198864852923]
	TIME [epoch: 7.76 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8446226265718044		[learning rate: 0.00082638]
	Learning Rate: 0.00082638
	LOSS [training: 3.8446226265718044 | validation: 3.835252249817863]
	TIME [epoch: 7.72 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8480035992528636		[learning rate: 0.00082443]
	Learning Rate: 0.00082443
	LOSS [training: 3.8480035992528636 | validation: 3.847185736056197]
	TIME [epoch: 7.72 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8622407809142425		[learning rate: 0.00082249]
	Learning Rate: 0.000822485
	LOSS [training: 3.8622407809142425 | validation: 3.8561648845737286]
	TIME [epoch: 7.72 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8667143430382342		[learning rate: 0.00082055]
	Learning Rate: 0.000820545
	LOSS [training: 3.8667143430382342 | validation: 3.8438362321118786]
	TIME [epoch: 7.72 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852607859827267		[learning rate: 0.00081861]
	Learning Rate: 0.00081861
	LOSS [training: 3.852607859827267 | validation: 3.846663324035786]
	TIME [epoch: 7.76 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8508642537110944		[learning rate: 0.00081668]
	Learning Rate: 0.000816679
	LOSS [training: 3.8508642537110944 | validation: 3.842364153419622]
	TIME [epoch: 7.72 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850046858544931		[learning rate: 0.00081475]
	Learning Rate: 0.000814752
	LOSS [training: 3.850046858544931 | validation: 3.8441379844498833]
	TIME [epoch: 7.71 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8494961189494363		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 3.8494961189494363 | validation: 3.847001810866562]
	TIME [epoch: 7.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8555593440253086		[learning rate: 0.00081091]
	Learning Rate: 0.000810913
	LOSS [training: 3.8555593440253086 | validation: 3.8533278891078044]
	TIME [epoch: 7.73 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8640810190242667		[learning rate: 0.000809]
	Learning Rate: 0.000809
	LOSS [training: 3.8640810190242667 | validation: 3.85360686430852]
	TIME [epoch: 7.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8614269683941633		[learning rate: 0.00080709]
	Learning Rate: 0.000807092
	LOSS [training: 3.8614269683941633 | validation: 3.8493184546275914]
	TIME [epoch: 7.72 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.85389965134078		[learning rate: 0.00080519]
	Learning Rate: 0.000805188
	LOSS [training: 3.85389965134078 | validation: 3.8384994474841507]
	TIME [epoch: 7.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840080337391968		[learning rate: 0.00080329]
	Learning Rate: 0.000803289
	LOSS [training: 3.840080337391968 | validation: 3.8302767236067727]
	TIME [epoch: 7.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8376309737697016		[learning rate: 0.00080139]
	Learning Rate: 0.000801394
	LOSS [training: 3.8376309737697016 | validation: 3.8356538147743278]
	TIME [epoch: 7.72 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405713125755048		[learning rate: 0.0007995]
	Learning Rate: 0.000799504
	LOSS [training: 3.8405713125755048 | validation: 3.8381423743654177]
	TIME [epoch: 7.76 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8502617382958193		[learning rate: 0.00079762]
	Learning Rate: 0.000797618
	LOSS [training: 3.8502617382958193 | validation: 3.8453952002005543]
	TIME [epoch: 7.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8558756838312087		[learning rate: 0.00079574]
	Learning Rate: 0.000795736
	LOSS [training: 3.8558756838312087 | validation: 3.8431008230659125]
	TIME [epoch: 7.74 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850913507555296		[learning rate: 0.00079386]
	Learning Rate: 0.000793859
	LOSS [training: 3.850913507555296 | validation: 3.840667677700039]
	TIME [epoch: 7.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849756346957624		[learning rate: 0.00079199]
	Learning Rate: 0.000791987
	LOSS [training: 3.849756346957624 | validation: 3.8414959238336337]
	TIME [epoch: 7.73 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.846512127479163		[learning rate: 0.00079012]
	Learning Rate: 0.000790119
	LOSS [training: 3.846512127479163 | validation: 3.838405446702417]
	TIME [epoch: 7.75 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8406212566077143		[learning rate: 0.00078826]
	Learning Rate: 0.000788255
	LOSS [training: 3.8406212566077143 | validation: 3.8305779538058107]
	TIME [epoch: 7.72 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8379593458337427		[learning rate: 0.0007864]
	Learning Rate: 0.000786396
	LOSS [training: 3.8379593458337427 | validation: 3.8326810932487865]
	TIME [epoch: 7.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8334267655486034		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 3.8334267655486034 | validation: 3.8273692776345367]
	TIME [epoch: 7.71 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.828986310096997		[learning rate: 0.00078269]
	Learning Rate: 0.00078269
	LOSS [training: 3.828986310096997 | validation: 3.827525880081989]
	TIME [epoch: 7.74 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.830922348447656		[learning rate: 0.00078084]
	Learning Rate: 0.000780844
	LOSS [training: 3.830922348447656 | validation: 3.8277532741224727]
	TIME [epoch: 7.75 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8299649415911228		[learning rate: 0.000779]
	Learning Rate: 0.000779002
	LOSS [training: 3.8299649415911228 | validation: 3.826068489308885]
	TIME [epoch: 7.71 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8314400039623013		[learning rate: 0.00077716]
	Learning Rate: 0.000777164
	LOSS [training: 3.8314400039623013 | validation: 3.8290978977167827]
	TIME [epoch: 7.71 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.827193331563268		[learning rate: 0.00077533]
	Learning Rate: 0.000775331
	LOSS [training: 3.827193331563268 | validation: 3.821350898082482]
	TIME [epoch: 7.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8189854472416225		[learning rate: 0.0007735]
	Learning Rate: 0.000773502
	LOSS [training: 3.8189854472416225 | validation: 3.8155742431420974]
	TIME [epoch: 7.74 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.817100321669758		[learning rate: 0.00077168]
	Learning Rate: 0.000771678
	LOSS [training: 3.817100321669758 | validation: 3.8168236719463167]
	TIME [epoch: 7.75 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8163866304841214		[learning rate: 0.00076986]
	Learning Rate: 0.000769857
	LOSS [training: 3.8163866304841214 | validation: 3.8114937780808082]
	TIME [epoch: 7.71 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8142115601676028		[learning rate: 0.00076804]
	Learning Rate: 0.000768041
	LOSS [training: 3.8142115601676028 | validation: 3.8161832871836645]
	TIME [epoch: 7.72 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820382881394437		[learning rate: 0.00076623]
	Learning Rate: 0.00076623
	LOSS [training: 3.820382881394437 | validation: 3.818538291528809]
	TIME [epoch: 7.71 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819716961907054		[learning rate: 0.00076442]
	Learning Rate: 0.000764422
	LOSS [training: 3.819716961907054 | validation: 3.819973465464233]
	TIME [epoch: 7.74 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8176972054404077		[learning rate: 0.00076262]
	Learning Rate: 0.000762619
	LOSS [training: 3.8176972054404077 | validation: 3.822666984995293]
	TIME [epoch: 7.76 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8208109805782535		[learning rate: 0.00076082]
	Learning Rate: 0.00076082
	LOSS [training: 3.8208109805782535 | validation: 3.8180915054232543]
	TIME [epoch: 7.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8179999041438424		[learning rate: 0.00075903]
	Learning Rate: 0.000759026
	LOSS [training: 3.8179999041438424 | validation: 3.8194822598857225]
	TIME [epoch: 7.72 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8177900590862643		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 3.8177900590862643 | validation: 3.822347740073497]
	TIME [epoch: 7.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8200919828606654		[learning rate: 0.00075545]
	Learning Rate: 0.000755449
	LOSS [training: 3.8200919828606654 | validation: 3.8205381775293867]
	TIME [epoch: 7.75 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820252998261995		[learning rate: 0.00075367]
	Learning Rate: 0.000753667
	LOSS [training: 3.820252998261995 | validation: 3.823437340786053]
	TIME [epoch: 7.74 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8229494376014665		[learning rate: 0.00075189]
	Learning Rate: 0.000751889
	LOSS [training: 3.8229494376014665 | validation: 3.8245930407090194]
	TIME [epoch: 7.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821497862587476		[learning rate: 0.00075012]
	Learning Rate: 0.000750116
	LOSS [training: 3.821497862587476 | validation: 3.8214190075684247]
	TIME [epoch: 7.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819109629074519		[learning rate: 0.00074835]
	Learning Rate: 0.000748346
	LOSS [training: 3.819109629074519 | validation: 3.8211637759691435]
	TIME [epoch: 7.72 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8201605356630073		[learning rate: 0.00074658]
	Learning Rate: 0.000746581
	LOSS [training: 3.8201605356630073 | validation: 3.827574073609708]
	TIME [epoch: 7.76 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820947613328171		[learning rate: 0.00074482]
	Learning Rate: 0.00074482
	LOSS [training: 3.820947613328171 | validation: 3.8231751387749693]
	TIME [epoch: 7.73 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819505992004062		[learning rate: 0.00074306]
	Learning Rate: 0.000743063
	LOSS [training: 3.819505992004062 | validation: 3.8211722656362657]
	TIME [epoch: 7.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8172972197235224		[learning rate: 0.00074131]
	Learning Rate: 0.00074131
	LOSS [training: 3.8172972197235224 | validation: 3.823705087307001]
	TIME [epoch: 7.72 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8192569701681913		[learning rate: 0.00073956]
	Learning Rate: 0.000739562
	LOSS [training: 3.8192569701681913 | validation: 3.8174060166353687]
	TIME [epoch: 7.73 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8176621258703354		[learning rate: 0.00073782]
	Learning Rate: 0.000737817
	LOSS [training: 3.8176621258703354 | validation: 3.8133805308116075]
	TIME [epoch: 7.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816438965538639		[learning rate: 0.00073608]
	Learning Rate: 0.000736077
	LOSS [training: 3.816438965538639 | validation: 3.821248896298709]
	TIME [epoch: 7.73 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8198280782913057		[learning rate: 0.00073434]
	Learning Rate: 0.000734341
	LOSS [training: 3.8198280782913057 | validation: 3.8176973128897833]
	TIME [epoch: 7.72 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821810825810448		[learning rate: 0.00073261]
	Learning Rate: 0.000732608
	LOSS [training: 3.821810825810448 | validation: 3.819971412796288]
	TIME [epoch: 7.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8262874060180403		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 3.8262874060180403 | validation: 3.822950118412737]
	TIME [epoch: 7.73 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8230440608485976		[learning rate: 0.00072916]
	Learning Rate: 0.000729156
	LOSS [training: 3.8230440608485976 | validation: 3.814158973633952]
	TIME [epoch: 7.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8151610430090632		[learning rate: 0.00072744]
	Learning Rate: 0.000727436
	LOSS [training: 3.8151610430090632 | validation: 3.814732936659855]
	TIME [epoch: 7.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.815616584754764		[learning rate: 0.00072572]
	Learning Rate: 0.00072572
	LOSS [training: 3.815616584754764 | validation: 3.813510427805463]
	TIME [epoch: 7.77 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811356673005079		[learning rate: 0.00072401]
	Learning Rate: 0.000724008
	LOSS [training: 3.811356673005079 | validation: 3.809414325695106]
	TIME [epoch: 7.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810204493036037		[learning rate: 0.0007223]
	Learning Rate: 0.000722301
	LOSS [training: 3.810204493036037 | validation: 3.811575155209319]
	TIME [epoch: 7.75 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8111784308827525		[learning rate: 0.0007206]
	Learning Rate: 0.000720597
	LOSS [training: 3.8111784308827525 | validation: 3.807728350491911]
	TIME [epoch: 7.77 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8188957977709164		[learning rate: 0.0007189]
	Learning Rate: 0.000718897
	LOSS [training: 3.8188957977709164 | validation: 3.816658300137866]
	TIME [epoch: 7.72 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.822340609642446		[learning rate: 0.0007172]
	Learning Rate: 0.000717201
	LOSS [training: 3.822340609642446 | validation: 3.819648753502704]
	TIME [epoch: 7.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.832889910828115		[learning rate: 0.00071551]
	Learning Rate: 0.00071551
	LOSS [training: 3.832889910828115 | validation: 3.8433949131562555]
	TIME [epoch: 7.72 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8585621874991745		[learning rate: 0.00071382]
	Learning Rate: 0.000713822
	LOSS [training: 3.8585621874991745 | validation: 3.843594921909472]
	TIME [epoch: 7.73 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857769935509358		[learning rate: 0.00071214]
	Learning Rate: 0.000712138
	LOSS [training: 3.857769935509358 | validation: 3.8525921952084534]
	TIME [epoch: 7.77 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8831265392617347		[learning rate: 0.00071046]
	Learning Rate: 0.000710458
	LOSS [training: 3.8831265392617347 | validation: 3.882799626992619]
	TIME [epoch: 7.72 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.908862742554616		[learning rate: 0.00070878]
	Learning Rate: 0.000708782
	LOSS [training: 3.908862742554616 | validation: 3.8829516884909037]
	TIME [epoch: 7.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8865646842953296		[learning rate: 0.00070711]
	Learning Rate: 0.00070711
	LOSS [training: 3.8865646842953296 | validation: 3.8501348870630636]
	TIME [epoch: 7.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.873514797439145		[learning rate: 0.00070544]
	Learning Rate: 0.000705443
	LOSS [training: 3.873514797439145 | validation: 3.8614103215204363]
	TIME [epoch: 7.75 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.873089043436722		[learning rate: 0.00070378]
	Learning Rate: 0.000703778
	LOSS [training: 3.873089043436722 | validation: 3.8649109003624087]
	TIME [epoch: 7.76 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8865576468818364		[learning rate: 0.00070212]
	Learning Rate: 0.000702118
	LOSS [training: 3.8865576468818364 | validation: 3.8691593196295795]
	TIME [epoch: 7.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.893873472876969		[learning rate: 0.00070046]
	Learning Rate: 0.000700462
	LOSS [training: 3.893873472876969 | validation: 3.887083918515583]
	TIME [epoch: 7.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.915565864277481		[learning rate: 0.00069881]
	Learning Rate: 0.00069881
	LOSS [training: 3.915565864277481 | validation: 3.9243890944374886]
	TIME [epoch: 7.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.952670478813656		[learning rate: 0.00069716]
	Learning Rate: 0.000697161
	LOSS [training: 3.952670478813656 | validation: 3.938597266677171]
	TIME [epoch: 7.74 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.957242890099736		[learning rate: 0.00069552]
	Learning Rate: 0.000695517
	LOSS [training: 3.957242890099736 | validation: 3.9365310373850786]
	TIME [epoch: 7.75 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9704394461529264		[learning rate: 0.00069388]
	Learning Rate: 0.000693876
	LOSS [training: 3.9704394461529264 | validation: 3.9397867676729508]
	TIME [epoch: 7.73 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9530162011418564		[learning rate: 0.00069224]
	Learning Rate: 0.00069224
	LOSS [training: 3.9530162011418564 | validation: 3.916291857541691]
	TIME [epoch: 7.72 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9449398047396214		[learning rate: 0.00069061]
	Learning Rate: 0.000690607
	LOSS [training: 3.9449398047396214 | validation: 3.9272967782100228]
	TIME [epoch: 7.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9515815957941047		[learning rate: 0.00068898]
	Learning Rate: 0.000688978
	LOSS [training: 3.9515815957941047 | validation: 3.9257642121118703]
	TIME [epoch: 7.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.95659929644191		[learning rate: 0.00068735]
	Learning Rate: 0.000687352
	LOSS [training: 3.95659929644191 | validation: 3.9428433496717163]
	TIME [epoch: 7.76 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9688176783997093		[learning rate: 0.00068573]
	Learning Rate: 0.000685731
	LOSS [training: 3.9688176783997093 | validation: 3.943025195311054]
	TIME [epoch: 7.71 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9801166344194527		[learning rate: 0.00068411]
	Learning Rate: 0.000684114
	LOSS [training: 3.9801166344194527 | validation: 3.963670427873601]
	TIME [epoch: 7.72 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9898191346200713		[learning rate: 0.0006825]
	Learning Rate: 0.0006825
	LOSS [training: 3.9898191346200713 | validation: 3.953287687252399]
	TIME [epoch: 7.71 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.975862694843535		[learning rate: 0.00068089]
	Learning Rate: 0.00068089
	LOSS [training: 3.975862694843535 | validation: 3.9465291365977815]
	TIME [epoch: 7.76 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.982158360849466		[learning rate: 0.00067928]
	Learning Rate: 0.000679284
	LOSS [training: 3.982158360849466 | validation: 3.9687209598574227]
	TIME [epoch: 7.75 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.994508993934519		[learning rate: 0.00067768]
	Learning Rate: 0.000677682
	LOSS [training: 3.994508993934519 | validation: 3.9614045950935974]
	TIME [epoch: 7.71 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9922926483258174		[learning rate: 0.00067608]
	Learning Rate: 0.000676083
	LOSS [training: 3.9922926483258174 | validation: 3.9566536759101862]
	TIME [epoch: 7.71 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980420352146201		[learning rate: 0.00067449]
	Learning Rate: 0.000674488
	LOSS [training: 3.980420352146201 | validation: 3.947278291212118]
	TIME [epoch: 7.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9856417712275607		[learning rate: 0.0006729]
	Learning Rate: 0.000672897
	LOSS [training: 3.9856417712275607 | validation: 3.9530941486822946]
	TIME [epoch: 7.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9846794714209466		[learning rate: 0.00067131]
	Learning Rate: 0.00067131
	LOSS [training: 3.9846794714209466 | validation: 3.9576902026355008]
	TIME [epoch: 7.73 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9820017189137933		[learning rate: 0.00066973]
	Learning Rate: 0.000669726
	LOSS [training: 3.9820017189137933 | validation: 3.9476140669417337]
	TIME [epoch: 7.72 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9753266059684633		[learning rate: 0.00066815]
	Learning Rate: 0.000668147
	LOSS [training: 3.9753266059684633 | validation: 3.954584385504046]
	TIME [epoch: 7.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9930730072977587		[learning rate: 0.00066657]
	Learning Rate: 0.000666571
	LOSS [training: 3.9930730072977587 | validation: 3.9701984541673903]
	TIME [epoch: 7.71 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9941237582365594		[learning rate: 0.000665]
	Learning Rate: 0.000664998
	LOSS [training: 3.9941237582365594 | validation: 3.9521654229402494]
	TIME [epoch: 7.76 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.985172536773666		[learning rate: 0.00066343]
	Learning Rate: 0.00066343
	LOSS [training: 3.985172536773666 | validation: 3.956762355050003]
	TIME [epoch: 7.72 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9845136003131874		[learning rate: 0.00066186]
	Learning Rate: 0.000661865
	LOSS [training: 3.9845136003131874 | validation: 3.9577690387007625]
	TIME [epoch: 7.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9931772462039987		[learning rate: 0.0006603]
	Learning Rate: 0.000660304
	LOSS [training: 3.9931772462039987 | validation: 3.959566487436443]
	TIME [epoch: 7.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9883245249246526		[learning rate: 0.00065875]
	Learning Rate: 0.000658746
	LOSS [training: 3.9883245249246526 | validation: 3.9407419154226995]
	TIME [epoch: 7.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.972907880531046		[learning rate: 0.00065719]
	Learning Rate: 0.000657192
	LOSS [training: 3.972907880531046 | validation: 3.9420359478176747]
	TIME [epoch: 7.75 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9706676473127143		[learning rate: 0.00065564]
	Learning Rate: 0.000655642
	LOSS [training: 3.9706676473127143 | validation: 3.9379724078943816]
	TIME [epoch: 7.73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9749997794000125		[learning rate: 0.0006541]
	Learning Rate: 0.000654095
	LOSS [training: 3.9749997794000125 | validation: 3.949865153968088]
	TIME [epoch: 7.71 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9852413651061522		[learning rate: 0.00065255]
	Learning Rate: 0.000652552
	LOSS [training: 3.9852413651061522 | validation: 3.9508160461195896]
	TIME [epoch: 7.71 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9875158422901165		[learning rate: 0.00065101]
	Learning Rate: 0.000651013
	LOSS [training: 3.9875158422901165 | validation: 3.9607450903701915]
	TIME [epoch: 7.72 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.988476894466075		[learning rate: 0.00064948]
	Learning Rate: 0.000649478
	LOSS [training: 3.988476894466075 | validation: 3.960125296306974]
	TIME [epoch: 7.76 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9916927551701065		[learning rate: 0.00064795]
	Learning Rate: 0.000647945
	LOSS [training: 3.9916927551701065 | validation: 3.9590074376891318]
	TIME [epoch: 7.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9799050139255283		[learning rate: 0.00064642]
	Learning Rate: 0.000646417
	LOSS [training: 3.9799050139255283 | validation: 3.9368302645059137]
	TIME [epoch: 7.72 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.965240038643452		[learning rate: 0.00064489]
	Learning Rate: 0.000644892
	LOSS [training: 3.965240038643452 | validation: 3.937772168104808]
	TIME [epoch: 7.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9710111274316895		[learning rate: 0.00064337]
	Learning Rate: 0.000643371
	LOSS [training: 3.9710111274316895 | validation: 3.9473522957977147]
	TIME [epoch: 7.72 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9829190719753984		[learning rate: 0.00064185]
	Learning Rate: 0.000641854
	LOSS [training: 3.9829190719753984 | validation: 3.95916439314734]
	TIME [epoch: 7.76 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9946186636728545		[learning rate: 0.00064034]
	Learning Rate: 0.00064034
	LOSS [training: 3.9946186636728545 | validation: 3.974246116476877]
	TIME [epoch: 7.72 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.000262636219886		[learning rate: 0.00063883]
	Learning Rate: 0.000638829
	LOSS [training: 4.000262636219886 | validation: 3.977651747571695]
	TIME [epoch: 7.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.007506632483318		[learning rate: 0.00063732]
	Learning Rate: 0.000637322
	LOSS [training: 4.007506632483318 | validation: 3.9749375832911555]
	TIME [epoch: 7.72 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016033049457487		[learning rate: 0.00063582]
	Learning Rate: 0.000635819
	LOSS [training: 4.016033049457487 | validation: 3.982892899466493]
	TIME [epoch: 7.74 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0055386271654525		[learning rate: 0.00063432]
	Learning Rate: 0.000634319
	LOSS [training: 4.0055386271654525 | validation: 3.975328758860894]
	TIME [epoch: 7.77 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.00571687857182		[learning rate: 0.00063282]
	Learning Rate: 0.000632823
	LOSS [training: 4.00571687857182 | validation: 3.9700263615735913]
	TIME [epoch: 7.72 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001118307479555		[learning rate: 0.00063133]
	Learning Rate: 0.00063133
	LOSS [training: 4.001118307479555 | validation: 3.9698645364334624]
	TIME [epoch: 7.73 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.006354272079338		[learning rate: 0.00062984]
	Learning Rate: 0.000629841
	LOSS [training: 4.006354272079338 | validation: 3.9911989018614396]
	TIME [epoch: 7.71 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.020539933403854		[learning rate: 0.00062836]
	Learning Rate: 0.000628355
	LOSS [training: 4.020539933403854 | validation: 4.00462858141076]
	TIME [epoch: 7.73 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.037486752309976		[learning rate: 0.00062687]
	Learning Rate: 0.000626873
	LOSS [training: 4.037486752309976 | validation: 4.005375526026149]
	TIME [epoch: 7.76 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.038482774148503		[learning rate: 0.00062539]
	Learning Rate: 0.000625394
	LOSS [training: 4.038482774148503 | validation: 4.015952680166306]
	TIME [epoch: 7.72 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.042813454328811		[learning rate: 0.00062392]
	Learning Rate: 0.000623919
	LOSS [training: 4.042813454328811 | validation: 4.0025737442523255]
	TIME [epoch: 7.71 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.028443587682616		[learning rate: 0.00062245]
	Learning Rate: 0.000622447
	LOSS [training: 4.028443587682616 | validation: 3.995404201198268]
	TIME [epoch: 7.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.031432061520931		[learning rate: 0.00062098]
	Learning Rate: 0.000620979
	LOSS [training: 4.031432061520931 | validation: 4.005295798303]
	TIME [epoch: 7.73 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.032190571661971		[learning rate: 0.00061951]
	Learning Rate: 0.000619514
	LOSS [training: 4.032190571661971 | validation: 4.008523192610993]
	TIME [epoch: 7.75 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.041751072088588		[learning rate: 0.00061805]
	Learning Rate: 0.000618053
	LOSS [training: 4.041751072088588 | validation: 4.0175531726695635]
	TIME [epoch: 7.71 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.053834600246925		[learning rate: 0.0006166]
	Learning Rate: 0.000616595
	LOSS [training: 4.053834600246925 | validation: 4.033259067722549]
	TIME [epoch: 7.72 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.060875130510945		[learning rate: 0.00061514]
	Learning Rate: 0.000615141
	LOSS [training: 4.060875130510945 | validation: 4.0358981881481135]
	TIME [epoch: 7.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.065369616466992		[learning rate: 0.00061369]
	Learning Rate: 0.00061369
	LOSS [training: 4.065369616466992 | validation: 4.030735009671996]
	TIME [epoch: 7.73 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.052882661121595		[learning rate: 0.00061224]
	Learning Rate: 0.000612242
	LOSS [training: 4.052882661121595 | validation: 4.0232603660401205]
	TIME [epoch: 7.75 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.051751446800002		[learning rate: 0.0006108]
	Learning Rate: 0.000610798
	LOSS [training: 4.051751446800002 | validation: 4.036029851460395]
	TIME [epoch: 7.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0746444830153346		[learning rate: 0.00060936]
	Learning Rate: 0.000609357
	LOSS [training: 4.0746444830153346 | validation: 4.048877021722188]
	TIME [epoch: 7.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.079210545523644		[learning rate: 0.00060792]
	Learning Rate: 0.00060792
	LOSS [training: 4.079210545523644 | validation: 4.037153279253994]
	TIME [epoch: 7.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.074942118726485		[learning rate: 0.00060649]
	Learning Rate: 0.000606486
	LOSS [training: 4.074942118726485 | validation: 4.047881576061551]
	TIME [epoch: 7.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.083153174263295		[learning rate: 0.00060506]
	Learning Rate: 0.000605055
	LOSS [training: 4.083153174263295 | validation: 4.0455321477635895]
	TIME [epoch: 7.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.068863506249965		[learning rate: 0.00060363]
	Learning Rate: 0.000603628
	LOSS [training: 4.068863506249965 | validation: 4.029257201082526]
	TIME [epoch: 7.71 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.059662632827968		[learning rate: 0.0006022]
	Learning Rate: 0.000602204
	LOSS [training: 4.059662632827968 | validation: 4.036861637316038]
	TIME [epoch: 7.71 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0694163190604575		[learning rate: 0.00060078]
	Learning Rate: 0.000600784
	LOSS [training: 4.0694163190604575 | validation: 4.051335555921025]
	TIME [epoch: 7.71 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.07337592164606		[learning rate: 0.00059937]
	Learning Rate: 0.000599366
	LOSS [training: 4.07337592164606 | validation: 4.043362582308378]
	TIME [epoch: 7.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.065527336100553		[learning rate: 0.00059795]
	Learning Rate: 0.000597953
	LOSS [training: 4.065527336100553 | validation: 4.033870686788964]
	TIME [epoch: 7.75 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067194465198698		[learning rate: 0.00059654]
	Learning Rate: 0.000596542
	LOSS [training: 4.067194465198698 | validation: 4.039452480672872]
	TIME [epoch: 7.71 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.069096762206203		[learning rate: 0.00059513]
	Learning Rate: 0.000595135
	LOSS [training: 4.069096762206203 | validation: 4.050384932689344]
	TIME [epoch: 7.71 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.074290119307344		[learning rate: 0.00059373]
	Learning Rate: 0.000593731
	LOSS [training: 4.074290119307344 | validation: 4.040404202863183]
	TIME [epoch: 7.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.068283677500908		[learning rate: 0.00059233]
	Learning Rate: 0.000592331
	LOSS [training: 4.068283677500908 | validation: 4.049844688246179]
	TIME [epoch: 7.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.080154345498313		[learning rate: 0.00059093]
	Learning Rate: 0.000590933
	LOSS [training: 4.080154345498313 | validation: 4.060239531410245]
	TIME [epoch: 7.73 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0868435825903635		[learning rate: 0.00058954]
	Learning Rate: 0.000589539
	LOSS [training: 4.0868435825903635 | validation: 4.062210499530179]
	TIME [epoch: 7.71 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.085408149293643		[learning rate: 0.00058815]
	Learning Rate: 0.000588149
	LOSS [training: 4.085408149293643 | validation: 4.055564802932588]
	TIME [epoch: 7.71 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.084885396213194		[learning rate: 0.00058676]
	Learning Rate: 0.000586761
	LOSS [training: 4.084885396213194 | validation: 4.063540431672639]
	TIME [epoch: 7.71 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.087885691671797		[learning rate: 0.00058538]
	Learning Rate: 0.000585377
	LOSS [training: 4.087885691671797 | validation: 4.0666555941241125]
	TIME [epoch: 7.76 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.086154548372505		[learning rate: 0.000584]
	Learning Rate: 0.000583997
	LOSS [training: 4.086154548372505 | validation: 4.049093957469748]
	TIME [epoch: 7.73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.082179786978049		[learning rate: 0.00058262]
	Learning Rate: 0.000582619
	LOSS [training: 4.082179786978049 | validation: 4.061525364924307]
	TIME [epoch: 7.72 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0863349652746646		[learning rate: 0.00058124]
	Learning Rate: 0.000581245
	LOSS [training: 4.0863349652746646 | validation: 4.063064183805176]
	TIME [epoch: 7.71 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.089109472135455		[learning rate: 0.00057987]
	Learning Rate: 0.000579874
	LOSS [training: 4.089109472135455 | validation: 4.057889017407707]
	TIME [epoch: 7.72 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.085480555207597		[learning rate: 0.00057851]
	Learning Rate: 0.000578506
	LOSS [training: 4.085480555207597 | validation: 4.062413313630694]
	TIME [epoch: 7.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0880916159375085		[learning rate: 0.00057714]
	Learning Rate: 0.000577141
	LOSS [training: 4.0880916159375085 | validation: 4.078014677851118]
	TIME [epoch: 7.73 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.095225420705825		[learning rate: 0.00057578]
	Learning Rate: 0.00057578
	LOSS [training: 4.095225420705825 | validation: 4.065191422489763]
	TIME [epoch: 7.72 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0868066950038635		[learning rate: 0.00057442]
	Learning Rate: 0.000574422
	LOSS [training: 4.0868066950038635 | validation: 4.0542031001567596]
	TIME [epoch: 7.71 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.08341192233569		[learning rate: 0.00057307]
	Learning Rate: 0.000573067
	LOSS [training: 4.08341192233569 | validation: 4.061629284690911]
	TIME [epoch: 7.72 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.08281487649724		[learning rate: 0.00057171]
	Learning Rate: 0.000571715
	LOSS [training: 4.08281487649724 | validation: 4.045016142745848]
	TIME [epoch: 7.76 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.06826357692259		[learning rate: 0.00057037]
	Learning Rate: 0.000570366
	LOSS [training: 4.06826357692259 | validation: 4.0408580409291455]
	TIME [epoch: 7.72 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.059355263766953		[learning rate: 0.00056902]
	Learning Rate: 0.000569021
	LOSS [training: 4.059355263766953 | validation: 4.031994425631279]
	TIME [epoch: 7.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.054684635375929		[learning rate: 0.00056768]
	Learning Rate: 0.000567679
	LOSS [training: 4.054684635375929 | validation: 4.043592301562568]
	TIME [epoch: 7.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.071546800726005		[learning rate: 0.00056634]
	Learning Rate: 0.00056634
	LOSS [training: 4.071546800726005 | validation: 4.052357238199608]
	TIME [epoch: 7.72 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077506851852967		[learning rate: 0.000565]
	Learning Rate: 0.000565004
	LOSS [training: 4.077506851852967 | validation: 4.0718366253246305]
	TIME [epoch: 7.77 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.094180371742258		[learning rate: 0.00056367]
	Learning Rate: 0.000563671
	LOSS [training: 4.094180371742258 | validation: 4.081820840137712]
	TIME [epoch: 7.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.097124867747938		[learning rate: 0.00056234]
	Learning Rate: 0.000562341
	LOSS [training: 4.097124867747938 | validation: 4.076575919368807]
	TIME [epoch: 7.73 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.09454804753712		[learning rate: 0.00056101]
	Learning Rate: 0.000561015
	LOSS [training: 4.09454804753712 | validation: 4.072733831081898]
	TIME [epoch: 7.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.096774418915935		[learning rate: 0.00055969]
	Learning Rate: 0.000559692
	LOSS [training: 4.096774418915935 | validation: 4.089532997659102]
	TIME [epoch: 7.73 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.097666612808238		[learning rate: 0.00055837]
	Learning Rate: 0.000558371
	LOSS [training: 4.097666612808238 | validation: 4.0619107305419355]
	TIME [epoch: 7.76 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.072735372772712		[learning rate: 0.00055705]
	Learning Rate: 0.000557054
	LOSS [training: 4.072735372772712 | validation: 4.042806516039002]
	TIME [epoch: 7.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.066091791609295		[learning rate: 0.00055574]
	Learning Rate: 0.00055574
	LOSS [training: 4.066091791609295 | validation: 4.043007631748505]
	TIME [epoch: 7.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.057895585631181		[learning rate: 0.00055443]
	Learning Rate: 0.000554429
	LOSS [training: 4.057895585631181 | validation: 4.0371729943001675]
	TIME [epoch: 7.71 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.05822730976193		[learning rate: 0.00055312]
	Learning Rate: 0.000553121
	LOSS [training: 4.05822730976193 | validation: 4.023367227302806]
	TIME [epoch: 7.73 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0461790067078764		[learning rate: 0.00055182]
	Learning Rate: 0.000551817
	LOSS [training: 4.0461790067078764 | validation: 4.036585169458839]
	TIME [epoch: 7.77 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.063401268676445		[learning rate: 0.00055052]
	Learning Rate: 0.000550515
	LOSS [training: 4.063401268676445 | validation: 4.040498889510372]
	TIME [epoch: 7.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.063175982970818		[learning rate: 0.00054922]
	Learning Rate: 0.000549216
	LOSS [training: 4.063175982970818 | validation: 4.024606149921667]
	TIME [epoch: 7.74 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.036720933792642		[learning rate: 0.00054792]
	Learning Rate: 0.000547921
	LOSS [training: 4.036720933792642 | validation: 4.001830219070711]
	TIME [epoch: 7.73 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016518767722033		[learning rate: 0.00054663]
	Learning Rate: 0.000546629
	LOSS [training: 4.016518767722033 | validation: 3.9915236990586145]
	TIME [epoch: 7.74 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.013654701710019		[learning rate: 0.00054534]
	Learning Rate: 0.000545339
	LOSS [training: 4.013654701710019 | validation: 4.008822046690816]
	TIME [epoch: 7.76 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.039554185456209		[learning rate: 0.00054405]
	Learning Rate: 0.000544053
	LOSS [training: 4.039554185456209 | validation: 4.023627753229061]
	TIME [epoch: 7.73 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.042028356574905		[learning rate: 0.00054277]
	Learning Rate: 0.000542769
	LOSS [training: 4.042028356574905 | validation: 4.02421506385126]
	TIME [epoch: 7.71 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.03660512928571		[learning rate: 0.00054149]
	Learning Rate: 0.000541489
	LOSS [training: 4.03660512928571 | validation: 4.004016219167528]
	TIME [epoch: 7.73 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.024643265023526		[learning rate: 0.00054021]
	Learning Rate: 0.000540212
	LOSS [training: 4.024643265023526 | validation: 4.001020605368355]
	TIME [epoch: 7.74 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.014831768392886		[learning rate: 0.00053894]
	Learning Rate: 0.000538938
	LOSS [training: 4.014831768392886 | validation: 3.9741114803546522]
	TIME [epoch: 7.75 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9925642717461223		[learning rate: 0.00053767]
	Learning Rate: 0.000537666
	LOSS [training: 3.9925642717461223 | validation: 3.975447959135874]
	TIME [epoch: 7.71 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9930410104137604		[learning rate: 0.0005364]
	Learning Rate: 0.000536398
	LOSS [training: 3.9930410104137604 | validation: 3.980987534660371]
	TIME [epoch: 7.71 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.992534632388659		[learning rate: 0.00053513]
	Learning Rate: 0.000535133
	LOSS [training: 3.992534632388659 | validation: 3.9759268018383227]
	TIME [epoch: 7.71 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9869010453955736		[learning rate: 0.00053387]
	Learning Rate: 0.00053387
	LOSS [training: 3.9869010453955736 | validation: 3.9630897038409114]
	TIME [epoch: 7.73 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9785748616304653		[learning rate: 0.00053261]
	Learning Rate: 0.000532611
	LOSS [training: 3.9785748616304653 | validation: 3.94234912879996]
	TIME [epoch: 7.75 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9606211151851363		[learning rate: 0.00053135]
	Learning Rate: 0.000531355
	LOSS [training: 3.9606211151851363 | validation: 3.9506062622619256]
	TIME [epoch: 7.73 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9674065013085236		[learning rate: 0.0005301]
	Learning Rate: 0.000530101
	LOSS [training: 3.9674065013085236 | validation: 3.957555637552023]
	TIME [epoch: 7.71 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.969404387573662		[learning rate: 0.00052885]
	Learning Rate: 0.000528851
	LOSS [training: 3.969404387573662 | validation: 3.94689795018928]
	TIME [epoch: 7.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9529171748606644		[learning rate: 0.0005276]
	Learning Rate: 0.000527603
	LOSS [training: 3.9529171748606644 | validation: 3.9290632246869084]
	TIME [epoch: 7.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9432359296991595		[learning rate: 0.00052636]
	Learning Rate: 0.000526359
	LOSS [training: 3.9432359296991595 | validation: 3.931362884207315]
	TIME [epoch: 7.73 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9297003510976003		[learning rate: 0.00052512]
	Learning Rate: 0.000525117
	LOSS [training: 3.9297003510976003 | validation: 3.9043499853620287]
	TIME [epoch: 7.71 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.907272791803983		[learning rate: 0.00052388]
	Learning Rate: 0.000523879
	LOSS [training: 3.907272791803983 | validation: 3.895231452660159]
	TIME [epoch: 7.71 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9111825383121315		[learning rate: 0.00052264]
	Learning Rate: 0.000522643
	LOSS [training: 3.9111825383121315 | validation: 3.8961541645467817]
	TIME [epoch: 7.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.895213370498071		[learning rate: 0.00052141]
	Learning Rate: 0.00052141
	LOSS [training: 3.895213370498071 | validation: 3.883672061105342]
	TIME [epoch: 7.76 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9147785235623007		[learning rate: 0.00052018]
	Learning Rate: 0.00052018
	LOSS [training: 3.9147785235623007 | validation: 3.922282976628524]
	TIME [epoch: 7.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9268632174092843		[learning rate: 0.00051895]
	Learning Rate: 0.000518953
	LOSS [training: 3.9268632174092843 | validation: 3.908287408945073]
	TIME [epoch: 7.72 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9163029633804576		[learning rate: 0.00051773]
	Learning Rate: 0.000517729
	LOSS [training: 3.9163029633804576 | validation: 3.901985701941675]
	TIME [epoch: 7.71 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9120218875908654		[learning rate: 0.00051651]
	Learning Rate: 0.000516508
	LOSS [training: 3.9120218875908654 | validation: 3.8941723084854103]
	TIME [epoch: 7.72 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9116813212805037		[learning rate: 0.00051529]
	Learning Rate: 0.000515289
	LOSS [training: 3.9116813212805037 | validation: 3.917549735684424]
	TIME [epoch: 7.75 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.947476806965968		[learning rate: 0.00051407]
	Learning Rate: 0.000514074
	LOSS [training: 3.947476806965968 | validation: 3.948303876266584]
	TIME [epoch: 7.72 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9529057302713806		[learning rate: 0.00051286]
	Learning Rate: 0.000512861
	LOSS [training: 3.9529057302713806 | validation: 3.930091824273246]
	TIME [epoch: 7.71 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.946037288880664		[learning rate: 0.00051165]
	Learning Rate: 0.000511652
	LOSS [training: 3.946037288880664 | validation: 3.93655136647761]
	TIME [epoch: 7.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9484542696231912		[learning rate: 0.00051044]
	Learning Rate: 0.000510445
	LOSS [training: 3.9484542696231912 | validation: 3.9416405965987047]
	TIME [epoch: 7.72 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.950865596966458		[learning rate: 0.00050924]
	Learning Rate: 0.000509241
	LOSS [training: 3.950865596966458 | validation: 3.9368908862012506]
	TIME [epoch: 7.75 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9494395195148164		[learning rate: 0.00050804]
	Learning Rate: 0.000508039
	LOSS [training: 3.9494395195148164 | validation: 3.9438287660292817]
	TIME [epoch: 7.73 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.946238012409312		[learning rate: 0.00050684]
	Learning Rate: 0.000506841
	LOSS [training: 3.946238012409312 | validation: 3.9282644375599935]
	TIME [epoch: 7.73 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9415218134230625		[learning rate: 0.00050565]
	Learning Rate: 0.000505646
	LOSS [training: 3.9415218134230625 | validation: 3.935108162619242]
	TIME [epoch: 7.71 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.94778021991569		[learning rate: 0.00050445]
	Learning Rate: 0.000504453
	LOSS [training: 3.94778021991569 | validation: 3.9194096308357427]
	TIME [epoch: 7.73 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9231042836749705		[learning rate: 0.00050326]
	Learning Rate: 0.000503263
	LOSS [training: 3.9231042836749705 | validation: 3.8968390125007817]
	TIME [epoch: 7.76 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.901155613749909		[learning rate: 0.00050208]
	Learning Rate: 0.000502076
	LOSS [training: 3.901155613749909 | validation: 3.8740270906392924]
	TIME [epoch: 7.71 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8817361200295397		[learning rate: 0.00050089]
	Learning Rate: 0.000500891
	LOSS [training: 3.8817361200295397 | validation: 3.873838662702094]
	TIME [epoch: 7.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879933089178903		[learning rate: 0.00049971]
	Learning Rate: 0.00049971
	LOSS [training: 3.879933089178903 | validation: 3.869546308213182]
	TIME [epoch: 7.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.868419599550525		[learning rate: 0.00049853]
	Learning Rate: 0.000498531
	LOSS [training: 3.868419599550525 | validation: 3.863271511825184]
	TIME [epoch: 7.72 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879119285545877		[learning rate: 0.00049736]
	Learning Rate: 0.000497355
	LOSS [training: 3.879119285545877 | validation: 3.860999373268145]
	TIME [epoch: 7.76 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8658908285401674		[learning rate: 0.00049618]
	Learning Rate: 0.000496182
	LOSS [training: 3.8658908285401674 | validation: 3.8475754698764213]
	TIME [epoch: 7.71 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8613833234885506		[learning rate: 0.00049501]
	Learning Rate: 0.000495012
	LOSS [training: 3.8613833234885506 | validation: 3.857657571940501]
	TIME [epoch: 7.71 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8620478198252437		[learning rate: 0.00049384]
	Learning Rate: 0.000493844
	LOSS [training: 3.8620478198252437 | validation: 3.8403906626976787]
	TIME [epoch: 7.71 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8387598455814227		[learning rate: 0.00049268]
	Learning Rate: 0.000492679
	LOSS [training: 3.8387598455814227 | validation: 3.827313160155902]
	TIME [epoch: 7.72 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8306596452382005		[learning rate: 0.00049152]
	Learning Rate: 0.000491517
	LOSS [training: 3.8306596452382005 | validation: 3.82436453829618]
	TIME [epoch: 7.76 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8278848441631412		[learning rate: 0.00049036]
	Learning Rate: 0.000490358
	LOSS [training: 3.8278848441631412 | validation: 3.817430039619401]
	TIME [epoch: 7.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.815215964798041		[learning rate: 0.0004892]
	Learning Rate: 0.000489201
	LOSS [training: 3.815215964798041 | validation: 3.8085381565329257]
	TIME [epoch: 7.71 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8053215007208037		[learning rate: 0.00048805]
	Learning Rate: 0.000488047
	LOSS [training: 3.8053215007208037 | validation: 3.795271087341625]
	TIME [epoch: 7.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7925935731521467		[learning rate: 0.0004869]
	Learning Rate: 0.000486896
	LOSS [training: 3.7925935731521467 | validation: 3.794530655236642]
	TIME [epoch: 7.73 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7973358682305762		[learning rate: 0.00048575]
	Learning Rate: 0.000485747
	LOSS [training: 3.7973358682305762 | validation: 3.8026100407515315]
	TIME [epoch: 7.76 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.798397569841711		[learning rate: 0.0004846]
	Learning Rate: 0.000484601
	LOSS [training: 3.798397569841711 | validation: 3.794384124196254]
	TIME [epoch: 7.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7940453960710263		[learning rate: 0.00048346]
	Learning Rate: 0.000483458
	LOSS [training: 3.7940453960710263 | validation: 3.7982415633363193]
	TIME [epoch: 7.72 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.79417564934549		[learning rate: 0.00048232]
	Learning Rate: 0.000482318
	LOSS [training: 3.79417564934549 | validation: 3.7984636315551312]
	TIME [epoch: 7.72 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7995416308797916		[learning rate: 0.00048118]
	Learning Rate: 0.00048118
	LOSS [training: 3.7995416308797916 | validation: 3.7979230353690627]
	TIME [epoch: 7.74 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795029442329609		[learning rate: 0.00048005]
	Learning Rate: 0.000480045
	LOSS [training: 3.795029442329609 | validation: 3.795043284083163]
	TIME [epoch: 7.75 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.796898401464578		[learning rate: 0.00047891]
	Learning Rate: 0.000478913
	LOSS [training: 3.796898401464578 | validation: 3.8056086436223175]
	TIME [epoch: 7.71 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.803221774008485		[learning rate: 0.00047778]
	Learning Rate: 0.000477783
	LOSS [training: 3.803221774008485 | validation: 3.796275461341473]
	TIME [epoch: 7.71 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7953392978013456		[learning rate: 0.00047666]
	Learning Rate: 0.000476656
	LOSS [training: 3.7953392978013456 | validation: 3.7906777218683274]
	TIME [epoch: 7.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.789611153583536		[learning rate: 0.00047553]
	Learning Rate: 0.000475532
	LOSS [training: 3.789611153583536 | validation: 3.785909313142742]
	TIME [epoch: 7.74 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7854917451935934		[learning rate: 0.00047441]
	Learning Rate: 0.00047441
	LOSS [training: 3.7854917451935934 | validation: 3.782558521892236]
	TIME [epoch: 7.76 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7796136738478223		[learning rate: 0.00047329]
	Learning Rate: 0.000473291
	LOSS [training: 3.7796136738478223 | validation: 3.7812820503270714]
	TIME [epoch: 7.73 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7784564381914665		[learning rate: 0.00047217]
	Learning Rate: 0.000472175
	LOSS [training: 3.7784564381914665 | validation: 3.7760337386239606]
	TIME [epoch: 7.73 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7722396018312154		[learning rate: 0.00047106]
	Learning Rate: 0.000471061
	LOSS [training: 3.7722396018312154 | validation: 3.773236146980212]
	TIME [epoch: 7.72 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7722545463861703		[learning rate: 0.00046995]
	Learning Rate: 0.00046995
	LOSS [training: 3.7722545463861703 | validation: 3.782529595437098]
	TIME [epoch: 7.74 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.781962296173896		[learning rate: 0.00046884]
	Learning Rate: 0.000468841
	LOSS [training: 3.781962296173896 | validation: 3.7890013994648957]
	TIME [epoch: 7.76 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78550875630465		[learning rate: 0.00046774]
	Learning Rate: 0.000467735
	LOSS [training: 3.78550875630465 | validation: 3.792522902071935]
	TIME [epoch: 7.74 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.794387866210144		[learning rate: 0.00046663]
	Learning Rate: 0.000466632
	LOSS [training: 3.794387866210144 | validation: 3.8063596116644156]
	TIME [epoch: 7.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.80666028446118		[learning rate: 0.00046553]
	Learning Rate: 0.000465531
	LOSS [training: 3.80666028446118 | validation: 3.8112819803300675]
	TIME [epoch: 7.72 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8067004171918475		[learning rate: 0.00046443]
	Learning Rate: 0.000464433
	LOSS [training: 3.8067004171918475 | validation: 3.802985792855217]
	TIME [epoch: 7.75 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8082481055930013		[learning rate: 0.00046334]
	Learning Rate: 0.000463338
	LOSS [training: 3.8082481055930013 | validation: 3.799748359791974]
	TIME [epoch: 7.73 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7983702033791378		[learning rate: 0.00046224]
	Learning Rate: 0.000462245
	LOSS [training: 3.7983702033791378 | validation: 3.794162179421033]
	TIME [epoch: 7.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7917551625865746		[learning rate: 0.00046115]
	Learning Rate: 0.000461154
	LOSS [training: 3.7917551625865746 | validation: 3.788390879771586]
	TIME [epoch: 7.72 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7826176822866437		[learning rate: 0.00046007]
	Learning Rate: 0.000460066
	LOSS [training: 3.7826176822866437 | validation: 3.7899474802454227]
	TIME [epoch: 7.72 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.790817192746918		[learning rate: 0.00045898]
	Learning Rate: 0.000458981
	LOSS [training: 3.790817192746918 | validation: 3.799836927988813]
	TIME [epoch: 7.76 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7951044117264296		[learning rate: 0.0004579]
	Learning Rate: 0.000457898
	LOSS [training: 3.7951044117264296 | validation: 3.788024112454009]
	TIME [epoch: 7.73 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7857000528758675		[learning rate: 0.00045682]
	Learning Rate: 0.000456818
	LOSS [training: 3.7857000528758675 | validation: 3.7854485361783268]
	TIME [epoch: 7.72 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779548003584684		[learning rate: 0.00045574]
	Learning Rate: 0.000455741
	LOSS [training: 3.779548003584684 | validation: 3.7744204389470877]
	TIME [epoch: 7.72 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.773859522654744		[learning rate: 0.00045467]
	Learning Rate: 0.000454666
	LOSS [training: 3.773859522654744 | validation: 3.7805738482972764]
	TIME [epoch: 7.72 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7816447787861636		[learning rate: 0.00045359]
	Learning Rate: 0.000453593
	LOSS [training: 3.7816447787861636 | validation: 3.7818416844318787]
	TIME [epoch: 7.76 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7783559912016678		[learning rate: 0.00045252]
	Learning Rate: 0.000452523
	LOSS [training: 3.7783559912016678 | validation: 3.7694521347028145]
	TIME [epoch: 7.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7679226639571435		[learning rate: 0.00045146]
	Learning Rate: 0.000451456
	LOSS [training: 3.7679226639571435 | validation: 3.767725853508126]
	TIME [epoch: 7.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7600838814472914		[learning rate: 0.00045039]
	Learning Rate: 0.000450391
	LOSS [training: 3.7600838814472914 | validation: 3.764750633374782]
	TIME [epoch: 7.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7628122046990713		[learning rate: 0.00044933]
	Learning Rate: 0.000449329
	LOSS [training: 3.7628122046990713 | validation: 3.768459465999517]
	TIME [epoch: 7.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766579235151773		[learning rate: 0.00044827]
	Learning Rate: 0.000448269
	LOSS [training: 3.766579235151773 | validation: 3.769492329832147]
	TIME [epoch: 7.77 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768142614854198		[learning rate: 0.00044721]
	Learning Rate: 0.000447211
	LOSS [training: 3.768142614854198 | validation: 3.7710191583605672]
	TIME [epoch: 7.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7692413335332136		[learning rate: 0.00044616]
	Learning Rate: 0.000446157
	LOSS [training: 3.7692413335332136 | validation: 3.77608381437176]
	TIME [epoch: 7.71 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.781197477398442		[learning rate: 0.0004451]
	Learning Rate: 0.000445104
	LOSS [training: 3.781197477398442 | validation: 3.79202814100348]
	TIME [epoch: 7.72 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7949061378069864		[learning rate: 0.00044405]
	Learning Rate: 0.000444054
	LOSS [training: 3.7949061378069864 | validation: 3.7972669574693505]
	TIME [epoch: 7.72 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.792342610858555		[learning rate: 0.00044301]
	Learning Rate: 0.000443007
	LOSS [training: 3.792342610858555 | validation: 3.7986628341691304]
	TIME [epoch: 7.76 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800856674420311		[learning rate: 0.00044196]
	Learning Rate: 0.000441962
	LOSS [training: 3.800856674420311 | validation: 3.7995519262091655]
	TIME [epoch: 7.71 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8014797810620733		[learning rate: 0.00044092]
	Learning Rate: 0.000440919
	LOSS [training: 3.8014797810620733 | validation: 3.8004294395387888]
	TIME [epoch: 7.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7993378301719183		[learning rate: 0.00043988]
	Learning Rate: 0.000439879
	LOSS [training: 3.7993378301719183 | validation: 3.8009218885277685]
	TIME [epoch: 7.72 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8003007896550027		[learning rate: 0.00043884]
	Learning Rate: 0.000438842
	LOSS [training: 3.8003007896550027 | validation: 3.806500374207771]
	TIME [epoch: 7.73 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.809555010160901		[learning rate: 0.00043781]
	Learning Rate: 0.000437806
	LOSS [training: 3.809555010160901 | validation: 3.8067229341382878]
	TIME [epoch: 7.76 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.803686113509957		[learning rate: 0.00043677]
	Learning Rate: 0.000436774
	LOSS [training: 3.803686113509957 | validation: 3.804111928508104]
	TIME [epoch: 7.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804710292651403		[learning rate: 0.00043574]
	Learning Rate: 0.000435743
	LOSS [training: 3.804710292651403 | validation: 3.80579644725261]
	TIME [epoch: 7.71 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8161427824079555		[learning rate: 0.00043472]
	Learning Rate: 0.000434715
	LOSS [training: 3.8161427824079555 | validation: 3.8269384015742767]
	TIME [epoch: 7.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8369551264229873		[learning rate: 0.00043369]
	Learning Rate: 0.00043369
	LOSS [training: 3.8369551264229873 | validation: 3.8379713290347537]
	TIME [epoch: 7.73 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840687057795296		[learning rate: 0.00043267]
	Learning Rate: 0.000432667
	LOSS [training: 3.840687057795296 | validation: 3.83654963172934]
	TIME [epoch: 7.76 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.833538982882261		[learning rate: 0.00043165]
	Learning Rate: 0.000431647
	LOSS [training: 3.833538982882261 | validation: 3.830574521836801]
	TIME [epoch: 7.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8264836849328168		[learning rate: 0.00043063]
	Learning Rate: 0.000430628
	LOSS [training: 3.8264836849328168 | validation: 3.8181168610339515]
	TIME [epoch: 7.71 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8247815099722917		[learning rate: 0.00042961]
	Learning Rate: 0.000429613
	LOSS [training: 3.8247815099722917 | validation: 3.8240513705540335]
	TIME [epoch: 7.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8247396167994987		[learning rate: 0.0004286]
	Learning Rate: 0.000428599
	LOSS [training: 3.8247396167994987 | validation: 3.825897291724709]
	TIME [epoch: 7.74 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8284167470389647		[learning rate: 0.00042759]
	Learning Rate: 0.000427588
	LOSS [training: 3.8284167470389647 | validation: 3.824241075535815]
	TIME [epoch: 7.75 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820434454428785		[learning rate: 0.00042658]
	Learning Rate: 0.000426579
	LOSS [training: 3.820434454428785 | validation: 3.816373724727948]
	TIME [epoch: 7.71 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819502739856534		[learning rate: 0.00042557]
	Learning Rate: 0.000425573
	LOSS [training: 3.819502739856534 | validation: 3.8168301084494907]
	TIME [epoch: 7.71 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81741704149748		[learning rate: 0.00042457]
	Learning Rate: 0.000424569
	LOSS [training: 3.81741704149748 | validation: 3.8119076778971794]
	TIME [epoch: 7.71 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810486067328651		[learning rate: 0.00042357]
	Learning Rate: 0.000423568
	LOSS [training: 3.810486067328651 | validation: 3.809957212480435]
	TIME [epoch: 7.73 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814091125836122		[learning rate: 0.00042257]
	Learning Rate: 0.000422569
	LOSS [training: 3.814091125836122 | validation: 3.8227021005670965]
	TIME [epoch: 7.75 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8245990480117076		[learning rate: 0.00042157]
	Learning Rate: 0.000421572
	LOSS [training: 3.8245990480117076 | validation: 3.8239046816886315]
	TIME [epoch: 7.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8218530311039567		[learning rate: 0.00042058]
	Learning Rate: 0.000420578
	LOSS [training: 3.8218530311039567 | validation: 3.833912012309928]
	TIME [epoch: 7.72 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8400601965296364		[learning rate: 0.00041959]
	Learning Rate: 0.000419586
	LOSS [training: 3.8400601965296364 | validation: 3.848429495808994]
	TIME [epoch: 7.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8561626298066196		[learning rate: 0.0004186]
	Learning Rate: 0.000418596
	LOSS [training: 3.8561626298066196 | validation: 3.8505663020641654]
	TIME [epoch: 7.73 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850168451367842		[learning rate: 0.00041761]
	Learning Rate: 0.000417608
	LOSS [training: 3.850168451367842 | validation: 3.846397408176036]
	TIME [epoch: 7.75 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8357964013946226		[learning rate: 0.00041662]
	Learning Rate: 0.000416623
	LOSS [training: 3.8357964013946226 | validation: 3.835318715738297]
	TIME [epoch: 7.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8259182257426496		[learning rate: 0.00041564]
	Learning Rate: 0.000415641
	LOSS [training: 3.8259182257426496 | validation: 3.825747296086857]
	TIME [epoch: 7.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8319333333769094		[learning rate: 0.00041466]
	Learning Rate: 0.00041466
	LOSS [training: 3.8319333333769094 | validation: 3.8391491773248205]
	TIME [epoch: 7.72 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.834381031246039		[learning rate: 0.00041368]
	Learning Rate: 0.000413682
	LOSS [training: 3.834381031246039 | validation: 3.842449521013473]
	TIME [epoch: 7.75 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8430140324528685		[learning rate: 0.00041271]
	Learning Rate: 0.000412706
	LOSS [training: 3.8430140324528685 | validation: 3.841990584503857]
	TIME [epoch: 7.73 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8372372323265314		[learning rate: 0.00041173]
	Learning Rate: 0.000411733
	LOSS [training: 3.8372372323265314 | validation: 3.8366611400779065]
	TIME [epoch: 7.71 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8338529625126663		[learning rate: 0.00041076]
	Learning Rate: 0.000410761
	LOSS [training: 3.8338529625126663 | validation: 3.8196261113757872]
	TIME [epoch: 7.71 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8123491363070405		[learning rate: 0.00040979]
	Learning Rate: 0.000409793
	LOSS [training: 3.8123491363070405 | validation: 3.815197400425638]
	TIME [epoch: 7.72 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.815249343341754		[learning rate: 0.00040883]
	Learning Rate: 0.000408826
	LOSS [training: 3.815249343341754 | validation: 3.8240643622717667]
	TIME [epoch: 7.75 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8223625613506256		[learning rate: 0.00040786]
	Learning Rate: 0.000407862
	LOSS [training: 3.8223625613506256 | validation: 3.81786962615631]
	TIME [epoch: 7.73 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811895940087895		[learning rate: 0.0004069]
	Learning Rate: 0.0004069
	LOSS [training: 3.811895940087895 | validation: 3.8128285238229225]
	TIME [epoch: 7.71 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.808275870968131		[learning rate: 0.00040594]
	Learning Rate: 0.00040594
	LOSS [training: 3.808275870968131 | validation: 3.8108418702479128]
	TIME [epoch: 7.71 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8207874125099734		[learning rate: 0.00040498]
	Learning Rate: 0.000404982
	LOSS [training: 3.8207874125099734 | validation: 3.8312477858019824]
	TIME [epoch: 7.72 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842584274597504		[learning rate: 0.00040403]
	Learning Rate: 0.000404027
	LOSS [training: 3.842584274597504 | validation: 3.8469451584695857]
	TIME [epoch: 7.77 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852929571755671		[learning rate: 0.00040307]
	Learning Rate: 0.000403074
	LOSS [training: 3.852929571755671 | validation: 3.8533871946064187]
	TIME [epoch: 7.74 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8542387080079212		[learning rate: 0.00040212]
	Learning Rate: 0.000402123
	LOSS [training: 3.8542387080079212 | validation: 3.8529721258853984]
	TIME [epoch: 7.73 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8582781806525386		[learning rate: 0.00040117]
	Learning Rate: 0.000401175
	LOSS [training: 3.8582781806525386 | validation: 3.8552461066663732]
	TIME [epoch: 7.72 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854890200338862		[learning rate: 0.00040023]
	Learning Rate: 0.000400228
	LOSS [training: 3.854890200338862 | validation: 3.84177295466773]
	TIME [epoch: 7.72 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838199849521207		[learning rate: 0.00039928]
	Learning Rate: 0.000399284
	LOSS [training: 3.838199849521207 | validation: 3.8193265666465335]
	TIME [epoch: 7.77 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8154996172091415		[learning rate: 0.00039834]
	Learning Rate: 0.000398342
	LOSS [training: 3.8154996172091415 | validation: 3.8152199934339834]
	TIME [epoch: 7.71 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8108475864995635		[learning rate: 0.0003974]
	Learning Rate: 0.000397403
	LOSS [training: 3.8108475864995635 | validation: 3.81438771108264]
	TIME [epoch: 7.71 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8126879643784424		[learning rate: 0.00039647]
	Learning Rate: 0.000396465
	LOSS [training: 3.8126879643784424 | validation: 3.8138452468312103]
	TIME [epoch: 7.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8131748884806016		[learning rate: 0.00039553]
	Learning Rate: 0.00039553
	LOSS [training: 3.8131748884806016 | validation: 3.8108203965914504]
	TIME [epoch: 7.72 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8094063572392463		[learning rate: 0.0003946]
	Learning Rate: 0.000394597
	LOSS [training: 3.8094063572392463 | validation: 3.8056796495881207]
	TIME [epoch: 7.76 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.797800054085591		[learning rate: 0.00039367]
	Learning Rate: 0.000393666
	LOSS [training: 3.797800054085591 | validation: 3.7993644438759855]
	TIME [epoch: 7.71 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.797045948639102		[learning rate: 0.00039274]
	Learning Rate: 0.000392738
	LOSS [training: 3.797045948639102 | validation: 3.7903619685623577]
	TIME [epoch: 7.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.793788597058033		[learning rate: 0.00039181]
	Learning Rate: 0.000391811
	LOSS [training: 3.793788597058033 | validation: 3.7936413619428984]
	TIME [epoch: 7.71 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7928011673807367		[learning rate: 0.00039089]
	Learning Rate: 0.000390887
	LOSS [training: 3.7928011673807367 | validation: 3.796921393572542]
	TIME [epoch: 7.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7918907941609588		[learning rate: 0.00038997]
	Learning Rate: 0.000389965
	LOSS [training: 3.7918907941609588 | validation: 3.796169529351576]
	TIME [epoch: 7.76 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.793071961745122		[learning rate: 0.00038905]
	Learning Rate: 0.000389045
	LOSS [training: 3.793071961745122 | validation: 3.8020004624876407]
	TIME [epoch: 7.72 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7950332732297802		[learning rate: 0.00038813]
	Learning Rate: 0.000388127
	LOSS [training: 3.7950332732297802 | validation: 3.805703953840847]
	TIME [epoch: 7.72 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.806113425369992		[learning rate: 0.00038721]
	Learning Rate: 0.000387212
	LOSS [training: 3.806113425369992 | validation: 3.817267980315744]
	TIME [epoch: 7.71 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814779696234594		[learning rate: 0.0003863]
	Learning Rate: 0.000386299
	LOSS [training: 3.814779696234594 | validation: 3.8075032841313052]
	TIME [epoch: 7.73 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8014130283979752		[learning rate: 0.00038539]
	Learning Rate: 0.000385387
	LOSS [training: 3.8014130283979752 | validation: 3.7966661177837158]
	TIME [epoch: 7.75 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795808588124762		[learning rate: 0.00038448]
	Learning Rate: 0.000384478
	LOSS [training: 3.795808588124762 | validation: 3.8010707630264338]
	TIME [epoch: 7.72 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8001725009030487		[learning rate: 0.00038357]
	Learning Rate: 0.000383571
	LOSS [training: 3.8001725009030487 | validation: 3.804089728979914]
	TIME [epoch: 7.71 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795754200864593		[learning rate: 0.00038267]
	Learning Rate: 0.000382667
	LOSS [training: 3.795754200864593 | validation: 3.8003350795141975]
	TIME [epoch: 7.71 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.798000521149035		[learning rate: 0.00038176]
	Learning Rate: 0.000381764
	LOSS [training: 3.798000521149035 | validation: 3.8151272749563327]
	TIME [epoch: 7.74 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.815632563563988		[learning rate: 0.00038086]
	Learning Rate: 0.000380863
	LOSS [training: 3.815632563563988 | validation: 3.820294155796173]
	TIME [epoch: 7.74 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.818251758117866		[learning rate: 0.00037997]
	Learning Rate: 0.000379965
	LOSS [training: 3.818251758117866 | validation: 3.818088853348356]
	TIME [epoch: 7.71 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8113776639209513		[learning rate: 0.00037907]
	Learning Rate: 0.000379069
	LOSS [training: 3.8113776639209513 | validation: 3.8091009439775547]
	TIME [epoch: 7.71 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8045236235603817		[learning rate: 0.00037817]
	Learning Rate: 0.000378175
	LOSS [training: 3.8045236235603817 | validation: 3.8018443610210664]
	TIME [epoch: 7.71 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7966882035324936		[learning rate: 0.00037728]
	Learning Rate: 0.000377283
	LOSS [training: 3.7966882035324936 | validation: 3.796876920653192]
	TIME [epoch: 7.74 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.792275793944696		[learning rate: 0.00037639]
	Learning Rate: 0.000376393
	LOSS [training: 3.792275793944696 | validation: 3.803091766498259]
	TIME [epoch: 7.75 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.806378016084838		[learning rate: 0.0003755]
	Learning Rate: 0.000375505
	LOSS [training: 3.806378016084838 | validation: 3.8142132797430284]
	TIME [epoch: 7.72 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8138777175489698		[learning rate: 0.00037462]
	Learning Rate: 0.000374619
	LOSS [training: 3.8138777175489698 | validation: 3.811203953738712]
	TIME [epoch: 7.73 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8118713255770604		[learning rate: 0.00037374]
	Learning Rate: 0.000373735
	LOSS [training: 3.8118713255770604 | validation: 3.819474925886924]
	TIME [epoch: 7.72 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8108999531765617		[learning rate: 0.00037285]
	Learning Rate: 0.000372854
	LOSS [training: 3.8108999531765617 | validation: 3.8105220806062388]
	TIME [epoch: 7.74 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8095129585122156		[learning rate: 0.00037197]
	Learning Rate: 0.000371974
	LOSS [training: 3.8095129585122156 | validation: 3.8110404425292526]
	TIME [epoch: 7.75 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804567295875664		[learning rate: 0.0003711]
	Learning Rate: 0.000371097
	LOSS [training: 3.804567295875664 | validation: 3.8116230034834073]
	TIME [epoch: 7.72 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8132507163068055		[learning rate: 0.00037022]
	Learning Rate: 0.000370221
	LOSS [training: 3.8132507163068055 | validation: 3.826420671644365]
	TIME [epoch: 7.72 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824441856865384		[learning rate: 0.00036935]
	Learning Rate: 0.000369348
	LOSS [training: 3.824441856865384 | validation: 3.8378783368053755]
	TIME [epoch: 7.73 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405431420418763		[learning rate: 0.00036848]
	Learning Rate: 0.000368477
	LOSS [training: 3.8405431420418763 | validation: 3.8444045688386]
	TIME [epoch: 7.75 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8483726722533187		[learning rate: 0.00036761]
	Learning Rate: 0.000367608
	LOSS [training: 3.8483726722533187 | validation: 3.8546105299751208]
	TIME [epoch: 7.74 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8576792396832458		[learning rate: 0.00036674]
	Learning Rate: 0.000366741
	LOSS [training: 3.8576792396832458 | validation: 3.854299145812564]
	TIME [epoch: 7.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8535904382847144		[learning rate: 0.00036588]
	Learning Rate: 0.000365876
	LOSS [training: 3.8535904382847144 | validation: 3.853545807476012]
	TIME [epoch: 7.72 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8566659684223197		[learning rate: 0.00036501]
	Learning Rate: 0.000365012
	LOSS [training: 3.8566659684223197 | validation: 3.8485239490943175]
	TIME [epoch: 7.73 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8506096820472586		[learning rate: 0.00036415]
	Learning Rate: 0.000364152
	LOSS [training: 3.8506096820472586 | validation: 3.8518278676685678]
	TIME [epoch: 7.77 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8533645960312177		[learning rate: 0.00036329]
	Learning Rate: 0.000363293
	LOSS [training: 3.8533645960312177 | validation: 3.8549527245657433]
	TIME [epoch: 7.72 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8590110030351426		[learning rate: 0.00036244]
	Learning Rate: 0.000362436
	LOSS [training: 3.8590110030351426 | validation: 3.85654158942613]
	TIME [epoch: 7.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858561581657896		[learning rate: 0.00036158]
	Learning Rate: 0.000361581
	LOSS [training: 3.858561581657896 | validation: 3.852470077988601]
	TIME [epoch: 7.71 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.84969276457399		[learning rate: 0.00036073]
	Learning Rate: 0.000360728
	LOSS [training: 3.84969276457399 | validation: 3.847817746250268]
	TIME [epoch: 7.72 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8424538556775008		[learning rate: 0.00035988]
	Learning Rate: 0.000359877
	LOSS [training: 3.8424538556775008 | validation: 3.843863845146351]
	TIME [epoch: 7.76 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8498447912394624		[learning rate: 0.00035903]
	Learning Rate: 0.000359028
	LOSS [training: 3.8498447912394624 | validation: 3.8540810973922977]
	TIME [epoch: 7.73 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854056621039141		[learning rate: 0.00035818]
	Learning Rate: 0.000358181
	LOSS [training: 3.854056621039141 | validation: 3.847419969259768]
	TIME [epoch: 7.71 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854971281821453		[learning rate: 0.00035734]
	Learning Rate: 0.000357336
	LOSS [training: 3.854971281821453 | validation: 3.8536180777309754]
	TIME [epoch: 7.72 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8596621850839026		[learning rate: 0.00035649]
	Learning Rate: 0.000356493
	LOSS [training: 3.8596621850839026 | validation: 3.8640080738472484]
	TIME [epoch: 7.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8599976975396597		[learning rate: 0.00035565]
	Learning Rate: 0.000355652
	LOSS [training: 3.8599976975396597 | validation: 3.862112788623918]
	TIME [epoch: 7.76 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8609974291796316		[learning rate: 0.00035481]
	Learning Rate: 0.000354813
	LOSS [training: 3.8609974291796316 | validation: 3.870330814822805]
	TIME [epoch: 7.72 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.87597622523057		[learning rate: 0.00035398]
	Learning Rate: 0.000353976
	LOSS [training: 3.87597622523057 | validation: 3.879308232580878]
	TIME [epoch: 7.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8755657031552224		[learning rate: 0.00035314]
	Learning Rate: 0.000353141
	LOSS [training: 3.8755657031552224 | validation: 3.868477663345205]
	TIME [epoch: 7.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8683509420694584		[learning rate: 0.00035231]
	Learning Rate: 0.000352309
	LOSS [training: 3.8683509420694584 | validation: 3.861536835622406]
	TIME [epoch: 7.73 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8624338058684087		[learning rate: 0.00035148]
	Learning Rate: 0.000351477
	LOSS [training: 3.8624338058684087 | validation: 3.861510840346348]
	TIME [epoch: 7.76 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8581925433250936		[learning rate: 0.00035065]
	Learning Rate: 0.000350648
	LOSS [training: 3.8581925433250936 | validation: 3.84867899246086]
	TIME [epoch: 7.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847651868352665		[learning rate: 0.00034982]
	Learning Rate: 0.000349821
	LOSS [training: 3.847651868352665 | validation: 3.853648033888769]
	TIME [epoch: 7.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848240334413213		[learning rate: 0.000349]
	Learning Rate: 0.000348996
	LOSS [training: 3.848240334413213 | validation: 3.8489199569289987]
	TIME [epoch: 7.71 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8501768112130654		[learning rate: 0.00034817]
	Learning Rate: 0.000348173
	LOSS [training: 3.8501768112130654 | validation: 3.8656651324922238]
	TIME [epoch: 7.73 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8625651241187358		[learning rate: 0.00034735]
	Learning Rate: 0.000347352
	LOSS [training: 3.8625651241187358 | validation: 3.8605845905347653]
	TIME [epoch: 7.77 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8666629463732827		[learning rate: 0.00034653]
	Learning Rate: 0.000346532
	LOSS [training: 3.8666629463732827 | validation: 3.863487738955678]
	TIME [epoch: 7.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8708362651680637		[learning rate: 0.00034571]
	Learning Rate: 0.000345715
	LOSS [training: 3.8708362651680637 | validation: 3.8774305577482258]
	TIME [epoch: 7.71 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8797884394020166		[learning rate: 0.0003449]
	Learning Rate: 0.000344899
	LOSS [training: 3.8797884394020166 | validation: 3.8829430690655737]
	TIME [epoch: 7.72 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8797075902702294		[learning rate: 0.00034409]
	Learning Rate: 0.000344086
	LOSS [training: 3.8797075902702294 | validation: 3.8742774443116152]
	TIME [epoch: 7.72 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.86932807872023		[learning rate: 0.00034327]
	Learning Rate: 0.000343274
	LOSS [training: 3.86932807872023 | validation: 3.8723749334128605]
	TIME [epoch: 7.75 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8796288778109913		[learning rate: 0.00034246]
	Learning Rate: 0.000342464
	LOSS [training: 3.8796288778109913 | validation: 3.8851600152898063]
	TIME [epoch: 7.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.898065377297521		[learning rate: 0.00034166]
	Learning Rate: 0.000341657
	LOSS [training: 3.898065377297521 | validation: 3.905762228773144]
	TIME [epoch: 7.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.905398684474921		[learning rate: 0.00034085]
	Learning Rate: 0.000340851
	LOSS [training: 3.905398684474921 | validation: 3.9089814502320346]
	TIME [epoch: 7.71 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9211550678311777		[learning rate: 0.00034005]
	Learning Rate: 0.000340047
	LOSS [training: 3.9211550678311777 | validation: 3.921395225628907]
	TIME [epoch: 7.73 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.931467838752767		[learning rate: 0.00033924]
	Learning Rate: 0.000339245
	LOSS [training: 3.931467838752767 | validation: 3.928210882545044]
	TIME [epoch: 7.75 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9458299718645167		[learning rate: 0.00033844]
	Learning Rate: 0.000338444
	LOSS [training: 3.9458299718645167 | validation: 3.945734443026006]
	TIME [epoch: 7.72 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.959215451734763		[learning rate: 0.00033765]
	Learning Rate: 0.000337646
	LOSS [training: 3.959215451734763 | validation: 3.9505850714617488]
	TIME [epoch: 7.71 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9653986516711965		[learning rate: 0.00033685]
	Learning Rate: 0.00033685
	LOSS [training: 3.9653986516711965 | validation: 3.969116422744893]
	TIME [epoch: 7.72 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9740058503460016		[learning rate: 0.00033605]
	Learning Rate: 0.000336055
	LOSS [training: 3.9740058503460016 | validation: 3.9650527279382795]
	TIME [epoch: 7.74 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9750528409494703		[learning rate: 0.00033526]
	Learning Rate: 0.000335262
	LOSS [training: 3.9750528409494703 | validation: 3.9751013713796395]
	TIME [epoch: 7.75 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9871820967117415		[learning rate: 0.00033447]
	Learning Rate: 0.000334471
	LOSS [training: 3.9871820967117415 | validation: 3.9870321774751347]
	TIME [epoch: 7.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.999722070665655		[learning rate: 0.00033368]
	Learning Rate: 0.000333682
	LOSS [training: 3.999722070665655 | validation: 3.999174949409185]
	TIME [epoch: 7.71 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9986055916072996		[learning rate: 0.0003329]
	Learning Rate: 0.000332895
	LOSS [training: 3.9986055916072996 | validation: 3.9819632021176727]
	TIME [epoch: 7.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.987657898226725		[learning rate: 0.00033211]
	Learning Rate: 0.00033211
	LOSS [training: 3.987657898226725 | validation: 3.9738765747002582]
	TIME [epoch: 7.75 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9862246449217142		[learning rate: 0.00033133]
	Learning Rate: 0.000331327
	LOSS [training: 3.9862246449217142 | validation: 3.979052731762886]
	TIME [epoch: 7.74 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9934010014939076		[learning rate: 0.00033055]
	Learning Rate: 0.000330545
	LOSS [training: 3.9934010014939076 | validation: 3.9978910521270814]
	TIME [epoch: 7.72 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.011066385804441		[learning rate: 0.00032977]
	Learning Rate: 0.000329765
	LOSS [training: 4.011066385804441 | validation: 4.008451265047349]
	TIME [epoch: 7.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.022830504828561		[learning rate: 0.00032899]
	Learning Rate: 0.000328988
	LOSS [training: 4.022830504828561 | validation: 4.013163807509008]
	TIME [epoch: 7.71 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.027247933151095		[learning rate: 0.00032821]
	Learning Rate: 0.000328212
	LOSS [training: 4.027247933151095 | validation: 4.021599683462969]
	TIME [epoch: 7.76 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0300813025763125		[learning rate: 0.00032744]
	Learning Rate: 0.000327437
	LOSS [training: 4.0300813025763125 | validation: 4.013775987617932]
	TIME [epoch: 7.73 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019973932311975		[learning rate: 0.00032666]
	Learning Rate: 0.000326665
	LOSS [training: 4.019973932311975 | validation: 4.001445995919709]
	TIME [epoch: 7.71 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001597686428495		[learning rate: 0.00032589]
	Learning Rate: 0.000325894
	LOSS [training: 4.001597686428495 | validation: 3.9885819990676783]
	TIME [epoch: 7.72 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002764568901669		[learning rate: 0.00032513]
	Learning Rate: 0.000325126
	LOSS [training: 4.002764568901669 | validation: 4.001374206999422]
	TIME [epoch: 7.72 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.007151746811243		[learning rate: 0.00032436]
	Learning Rate: 0.000324359
	LOSS [training: 4.007151746811243 | validation: 3.9854031123003884]
	TIME [epoch: 7.76 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.997802643021555		[learning rate: 0.00032359]
	Learning Rate: 0.000323594
	LOSS [training: 3.997802643021555 | validation: 3.986214110287218]
	TIME [epoch: 7.73 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9982446377402305		[learning rate: 0.00032283]
	Learning Rate: 0.00032283
	LOSS [training: 3.9982446377402305 | validation: 3.9814084499953903]
	TIME [epoch: 7.72 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9879458134829715		[learning rate: 0.00032207]
	Learning Rate: 0.000322069
	LOSS [training: 3.9879458134829715 | validation: 3.977216282378487]
	TIME [epoch: 7.71 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9852088536327495		[learning rate: 0.00032131]
	Learning Rate: 0.000321309
	LOSS [training: 3.9852088536327495 | validation: 3.976986048257379]
	TIME [epoch: 7.72 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9858840866382423		[learning rate: 0.00032055]
	Learning Rate: 0.000320551
	LOSS [training: 3.9858840866382423 | validation: 3.974694259669149]
	TIME [epoch: 7.77 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.986637731549081		[learning rate: 0.0003198]
	Learning Rate: 0.000319795
	LOSS [training: 3.986637731549081 | validation: 3.972425610052748]
	TIME [epoch: 7.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980052022787969		[learning rate: 0.00031904]
	Learning Rate: 0.000319041
	LOSS [training: 3.980052022787969 | validation: 3.966903861310593]
	TIME [epoch: 7.71 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9814253408139693		[learning rate: 0.00031829]
	Learning Rate: 0.000318288
	LOSS [training: 3.9814253408139693 | validation: 3.973865794452409]
	TIME [epoch: 7.72 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.979785612425477		[learning rate: 0.00031754]
	Learning Rate: 0.000317537
	LOSS [training: 3.979785612425477 | validation: 3.9628226045290047]
	TIME [epoch: 7.73 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9722170600172753		[learning rate: 0.00031679]
	Learning Rate: 0.000316788
	LOSS [training: 3.9722170600172753 | validation: 3.965684135965348]
	TIME [epoch: 7.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980873278559861		[learning rate: 0.00031604]
	Learning Rate: 0.000316041
	LOSS [training: 3.980873278559861 | validation: 3.9652869143591922]
	TIME [epoch: 7.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.978104575055713		[learning rate: 0.0003153]
	Learning Rate: 0.000315296
	LOSS [training: 3.978104575055713 | validation: 3.9616296492132683]
	TIME [epoch: 7.72 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9681498135008653		[learning rate: 0.00031455]
	Learning Rate: 0.000314552
	LOSS [training: 3.9681498135008653 | validation: 3.950473489720905]
	TIME [epoch: 7.72 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9614339495229576		[learning rate: 0.00031381]
	Learning Rate: 0.00031381
	LOSS [training: 3.9614339495229576 | validation: 3.952493098388911]
	TIME [epoch: 7.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.963999040393398		[learning rate: 0.00031307]
	Learning Rate: 0.00031307
	LOSS [training: 3.963999040393398 | validation: 3.953428095985351]
	TIME [epoch: 7.77 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.972225425320976		[learning rate: 0.00031233]
	Learning Rate: 0.000312331
	LOSS [training: 3.972225425320976 | validation: 3.9681319397709314]
	TIME [epoch: 7.72 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9758432073671015		[learning rate: 0.00031159]
	Learning Rate: 0.000311594
	LOSS [training: 3.9758432073671015 | validation: 3.9583037042664637]
	TIME [epoch: 7.71 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9771554525837183		[learning rate: 0.00031086]
	Learning Rate: 0.00031086
	LOSS [training: 3.9771554525837183 | validation: 3.9680315095150855]
	TIME [epoch: 7.72 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9779177430084616		[learning rate: 0.00031013]
	Learning Rate: 0.000310126
	LOSS [training: 3.9779177430084616 | validation: 3.9595382141007445]
	TIME [epoch: 7.72 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9679116783544344		[learning rate: 0.00030939]
	Learning Rate: 0.000309395
	LOSS [training: 3.9679116783544344 | validation: 3.9519111138270278]
	TIME [epoch: 7.76 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.966784083841263		[learning rate: 0.00030866]
	Learning Rate: 0.000308665
	LOSS [training: 3.966784083841263 | validation: 3.9576900805022626]
	TIME [epoch: 7.72 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9712236291731142		[learning rate: 0.00030794]
	Learning Rate: 0.000307937
	LOSS [training: 3.9712236291731142 | validation: 3.962182343168127]
	TIME [epoch: 7.71 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9778219980384932		[learning rate: 0.00030721]
	Learning Rate: 0.00030721
	LOSS [training: 3.9778219980384932 | validation: 3.9648989992309667]
	TIME [epoch: 7.71 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.977066364005622		[learning rate: 0.00030649]
	Learning Rate: 0.000306486
	LOSS [training: 3.977066364005622 | validation: 3.9596604254562346]
	TIME [epoch: 7.74 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.974284387854137		[learning rate: 0.00030576]
	Learning Rate: 0.000305763
	LOSS [training: 3.974284387854137 | validation: 3.9689334130048985]
	TIME [epoch: 7.76 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9840062153341274		[learning rate: 0.00030504]
	Learning Rate: 0.000305042
	LOSS [training: 3.9840062153341274 | validation: 3.9674843617151474]
	TIME [epoch: 7.72 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9820696501489405		[learning rate: 0.00030432]
	Learning Rate: 0.000304322
	LOSS [training: 3.9820696501489405 | validation: 3.9706443477094204]
	TIME [epoch: 7.72 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9791789929775687		[learning rate: 0.0003036]
	Learning Rate: 0.000303604
	LOSS [training: 3.9791789929775687 | validation: 3.969422247714088]
	TIME [epoch: 7.72 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980595973986752		[learning rate: 0.00030289]
	Learning Rate: 0.000302888
	LOSS [training: 3.980595973986752 | validation: 3.972366749149953]
	TIME [epoch: 7.75 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.985135080052913		[learning rate: 0.00030217]
	Learning Rate: 0.000302174
	LOSS [training: 3.985135080052913 | validation: 3.970199555940379]
	TIME [epoch: 7.75 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9820825128251327		[learning rate: 0.00030146]
	Learning Rate: 0.000301461
	LOSS [training: 3.9820825128251327 | validation: 3.963292653660653]
	TIME [epoch: 7.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9768204537708423		[learning rate: 0.00030075]
	Learning Rate: 0.00030075
	LOSS [training: 3.9768204537708423 | validation: 3.955227197581439]
	TIME [epoch: 7.74 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9650868725414266		[learning rate: 0.00030004]
	Learning Rate: 0.00030004
	LOSS [training: 3.9650868725414266 | validation: 3.9452280540549447]
	TIME [epoch: 7.73 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.954062800481011		[learning rate: 0.00029933]
	Learning Rate: 0.000299332
	LOSS [training: 3.954062800481011 | validation: 3.938258534054132]
	TIME [epoch: 7.75 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9434371525117378		[learning rate: 0.00029863]
	Learning Rate: 0.000298626
	LOSS [training: 3.9434371525117378 | validation: 3.927290846301844]
	TIME [epoch: 7.76 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.936753703383286		[learning rate: 0.00029792]
	Learning Rate: 0.000297922
	LOSS [training: 3.936753703383286 | validation: 3.9195902320133826]
	TIME [epoch: 7.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.932487248220497		[learning rate: 0.00029722]
	Learning Rate: 0.000297219
	LOSS [training: 3.932487248220497 | validation: 3.9188512817291983]
	TIME [epoch: 7.72 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923246409676668		[learning rate: 0.00029652]
	Learning Rate: 0.000296518
	LOSS [training: 3.923246409676668 | validation: 3.915593909199397]
	TIME [epoch: 7.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9218764621468063		[learning rate: 0.00029582]
	Learning Rate: 0.000295819
	LOSS [training: 3.9218764621468063 | validation: 3.9080500827176317]
	TIME [epoch: 7.75 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.924918231640503		[learning rate: 0.00029512]
	Learning Rate: 0.000295121
	LOSS [training: 3.924918231640503 | validation: 3.9171928890920373]
	TIME [epoch: 7.74 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9292534458257986		[learning rate: 0.00029442]
	Learning Rate: 0.000294425
	LOSS [training: 3.9292534458257986 | validation: 3.9155006137298076]
	TIME [epoch: 7.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9273026068493646		[learning rate: 0.00029373]
	Learning Rate: 0.00029373
	LOSS [training: 3.9273026068493646 | validation: 3.9215094631513256]
	TIME [epoch: 7.72 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.942129407502245		[learning rate: 0.00029304]
	Learning Rate: 0.000293037
	LOSS [training: 3.942129407502245 | validation: 3.9335129806482865]
	TIME [epoch: 7.72 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.950382573290794		[learning rate: 0.00029235]
	Learning Rate: 0.000292346
	LOSS [training: 3.950382573290794 | validation: 3.936658048009438]
	TIME [epoch: 7.76 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944427328485916		[learning rate: 0.00029166]
	Learning Rate: 0.000291657
	LOSS [training: 3.944427328485916 | validation: 3.9328590574139333]
	TIME [epoch: 7.73 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.940378121605442		[learning rate: 0.00029097]
	Learning Rate: 0.000290969
	LOSS [training: 3.940378121605442 | validation: 3.9308365405431642]
	TIME [epoch: 7.71 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9425446945663216		[learning rate: 0.00029028]
	Learning Rate: 0.000290282
	LOSS [training: 3.9425446945663216 | validation: 3.924034205312055]
	TIME [epoch: 7.71 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9387111064687708		[learning rate: 0.0002896]
	Learning Rate: 0.000289598
	LOSS [training: 3.9387111064687708 | validation: 3.9230455379425218]
	TIME [epoch: 7.73 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.937190903135204		[learning rate: 0.00028891]
	Learning Rate: 0.000288914
	LOSS [training: 3.937190903135204 | validation: 3.9274893346045503]
	TIME [epoch: 7.76 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.932622307649352		[learning rate: 0.00028823]
	Learning Rate: 0.000288233
	LOSS [training: 3.932622307649352 | validation: 3.9174997632065853]
	TIME [epoch: 7.73 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9320968158109766		[learning rate: 0.00028755]
	Learning Rate: 0.000287553
	LOSS [training: 3.9320968158109766 | validation: 3.9325911757447383]
	TIME [epoch: 7.72 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9448350121287508		[learning rate: 0.00028687]
	Learning Rate: 0.000286875
	LOSS [training: 3.9448350121287508 | validation: 3.9438117237454606]
	TIME [epoch: 7.72 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9534862097433257		[learning rate: 0.0002862]
	Learning Rate: 0.000286198
	LOSS [training: 3.9534862097433257 | validation: 3.9324017397819646]
	TIME [epoch: 7.72 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944399655836108		[learning rate: 0.00028552]
	Learning Rate: 0.000285523
	LOSS [training: 3.944399655836108 | validation: 3.9296407421210073]
	TIME [epoch: 7.78 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944184113943628		[learning rate: 0.00028485]
	Learning Rate: 0.000284849
	LOSS [training: 3.944184113943628 | validation: 3.9319322922097086]
	TIME [epoch: 7.72 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9449713160685853		[learning rate: 0.00028418]
	Learning Rate: 0.000284178
	LOSS [training: 3.9449713160685853 | validation: 3.941576653406675]
	TIME [epoch: 7.71 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9507691553400557		[learning rate: 0.00028351]
	Learning Rate: 0.000283507
	LOSS [training: 3.9507691553400557 | validation: 3.9346872673894038]
	TIME [epoch: 7.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9391306217752655		[learning rate: 0.00028284]
	Learning Rate: 0.000282839
	LOSS [training: 3.9391306217752655 | validation: 3.9212100455548775]
	TIME [epoch: 7.72 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9245188021242496		[learning rate: 0.00028217]
	Learning Rate: 0.000282171
	LOSS [training: 3.9245188021242496 | validation: 3.9064962174695275]
	TIME [epoch: 7.77 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9123691480403266		[learning rate: 0.00028151]
	Learning Rate: 0.000281506
	LOSS [training: 3.9123691480403266 | validation: 3.890065035382195]
	TIME [epoch: 7.71 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.893989753754264		[learning rate: 0.00028084]
	Learning Rate: 0.000280842
	LOSS [training: 3.893989753754264 | validation: 3.879091222518705]
	TIME [epoch: 7.72 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8814918104471117		[learning rate: 0.00028018]
	Learning Rate: 0.000280179
	LOSS [training: 3.8814918104471117 | validation: 3.875750642098578]
	TIME [epoch: 7.72 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875276710275185		[learning rate: 0.00027952]
	Learning Rate: 0.000279518
	LOSS [training: 3.875276710275185 | validation: 3.8749306849556713]
	TIME [epoch: 7.73 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8809658163454013		[learning rate: 0.00027886]
	Learning Rate: 0.000278859
	LOSS [training: 3.8809658163454013 | validation: 3.887504956719888]
	TIME [epoch: 7.76 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8928212908208186		[learning rate: 0.0002782]
	Learning Rate: 0.000278201
	LOSS [training: 3.8928212908208186 | validation: 3.8849688225021004]
	TIME [epoch: 7.72 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.892010233439426		[learning rate: 0.00027755]
	Learning Rate: 0.000277545
	LOSS [training: 3.892010233439426 | validation: 3.884888178607336]
	TIME [epoch: 7.72 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.892498683254767		[learning rate: 0.00027689]
	Learning Rate: 0.00027689
	LOSS [training: 3.892498683254767 | validation: 3.8757675870808193]
	TIME [epoch: 7.72 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.885734460780278		[learning rate: 0.00027624]
	Learning Rate: 0.000276237
	LOSS [training: 3.885734460780278 | validation: 3.8780947363392837]
	TIME [epoch: 7.73 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8866411281694955		[learning rate: 0.00027559]
	Learning Rate: 0.000275586
	LOSS [training: 3.8866411281694955 | validation: 3.877450461832906]
	TIME [epoch: 7.76 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8926667040730094		[learning rate: 0.00027494]
	Learning Rate: 0.000274935
	LOSS [training: 3.8926667040730094 | validation: 3.892189525876807]
	TIME [epoch: 7.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.896726024950999		[learning rate: 0.00027429]
	Learning Rate: 0.000274287
	LOSS [training: 3.896726024950999 | validation: 3.882209196539749]
	TIME [epoch: 7.72 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9003101611042394		[learning rate: 0.00027364]
	Learning Rate: 0.00027364
	LOSS [training: 3.9003101611042394 | validation: 3.8956042619483267]
	TIME [epoch: 7.71 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.897138196352647		[learning rate: 0.00027299]
	Learning Rate: 0.000272994
	LOSS [training: 3.897138196352647 | validation: 3.8859619153488323]
	TIME [epoch: 7.74 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8898678331645735		[learning rate: 0.00027235]
	Learning Rate: 0.000272351
	LOSS [training: 3.8898678331645735 | validation: 3.878867115985811]
	TIME [epoch: 7.76 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.886735282294358		[learning rate: 0.00027171]
	Learning Rate: 0.000271708
	LOSS [training: 3.886735282294358 | validation: 3.8853595465510673]
	TIME [epoch: 7.72 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8961907239590605		[learning rate: 0.00027107]
	Learning Rate: 0.000271067
	LOSS [training: 3.8961907239590605 | validation: 3.887675351921425]
	TIME [epoch: 7.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8979201777099606		[learning rate: 0.00027043]
	Learning Rate: 0.000270428
	LOSS [training: 3.8979201777099606 | validation: 3.8833620856899227]
	TIME [epoch: 7.72 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.892681037041488		[learning rate: 0.00026979]
	Learning Rate: 0.00026979
	LOSS [training: 3.892681037041488 | validation: 3.8854780441443575]
	TIME [epoch: 7.74 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8967249379767037		[learning rate: 0.00026915]
	Learning Rate: 0.000269154
	LOSS [training: 3.8967249379767037 | validation: 3.8907057184543383]
	TIME [epoch: 7.76 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.902924291874381		[learning rate: 0.00026852]
	Learning Rate: 0.000268519
	LOSS [training: 3.902924291874381 | validation: 3.89503138105637]
	TIME [epoch: 7.72 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.903628135335079		[learning rate: 0.00026789]
	Learning Rate: 0.000267885
	LOSS [training: 3.903628135335079 | validation: 3.8900667913257774]
	TIME [epoch: 7.73 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9043126982607337		[learning rate: 0.00026725]
	Learning Rate: 0.000267253
	LOSS [training: 3.9043126982607337 | validation: 3.8931279999941966]
	TIME [epoch: 7.72 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8999632595966194		[learning rate: 0.00026662]
	Learning Rate: 0.000266623
	LOSS [training: 3.8999632595966194 | validation: 3.8887858723856237]
	TIME [epoch: 7.77 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9070228563143297		[learning rate: 0.00026599]
	Learning Rate: 0.000265994
	LOSS [training: 3.9070228563143297 | validation: 3.911162703204853]
	TIME [epoch: 7.74 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9203229227206		[learning rate: 0.00026537]
	Learning Rate: 0.000265367
	LOSS [training: 3.9203229227206 | validation: 3.9173893995219347]
	TIME [epoch: 7.72 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9273702432840674		[learning rate: 0.00026474]
	Learning Rate: 0.000264741
	LOSS [training: 3.9273702432840674 | validation: 3.921989912390953]
	TIME [epoch: 7.73 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9330532810006913		[learning rate: 0.00026412]
	Learning Rate: 0.000264116
	LOSS [training: 3.9330532810006913 | validation: 3.920827730293065]
	TIME [epoch: 7.72 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9326265142628594		[learning rate: 0.00026349]
	Learning Rate: 0.000263493
	LOSS [training: 3.9326265142628594 | validation: 3.925801250445569]
	TIME [epoch: 7.77 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.935143168868303		[learning rate: 0.00026287]
	Learning Rate: 0.000262872
	LOSS [training: 3.935143168868303 | validation: 3.917303894478905]
	TIME [epoch: 7.73 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9218286170426073		[learning rate: 0.00026225]
	Learning Rate: 0.000262252
	LOSS [training: 3.9218286170426073 | validation: 3.9105045956968034]
	TIME [epoch: 7.72 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9116873305414672		[learning rate: 0.00026163]
	Learning Rate: 0.000261633
	LOSS [training: 3.9116873305414672 | validation: 3.909439910140821]
	TIME [epoch: 7.73 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.920191621798214		[learning rate: 0.00026102]
	Learning Rate: 0.000261016
	LOSS [training: 3.920191621798214 | validation: 3.919373456881077]
	TIME [epoch: 7.72 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9250528665418796		[learning rate: 0.0002604]
	Learning Rate: 0.0002604
	LOSS [training: 3.9250528665418796 | validation: 3.9216665751732753]
	TIME [epoch: 7.79 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.925370599746696		[learning rate: 0.00025979]
	Learning Rate: 0.000259786
	LOSS [training: 3.925370599746696 | validation: 3.9128226664296033]
	TIME [epoch: 7.74 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.91864668475515		[learning rate: 0.00025917]
	Learning Rate: 0.000259173
	LOSS [training: 3.91864668475515 | validation: 3.9041166198596295]
	TIME [epoch: 7.73 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9109202186804817		[learning rate: 0.00025856]
	Learning Rate: 0.000258562
	LOSS [training: 3.9109202186804817 | validation: 3.9055227305293814]
	TIME [epoch: 7.74 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9098230932526024		[learning rate: 0.00025795]
	Learning Rate: 0.000257952
	LOSS [training: 3.9098230932526024 | validation: 3.9067204736458403]
	TIME [epoch: 7.74 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9090047574434235		[learning rate: 0.00025734]
	Learning Rate: 0.000257343
	LOSS [training: 3.9090047574434235 | validation: 3.9032308737896444]
	TIME [epoch: 7.79 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9054283097823603		[learning rate: 0.00025674]
	Learning Rate: 0.000256736
	LOSS [training: 3.9054283097823603 | validation: 3.9016189065998583]
	TIME [epoch: 7.74 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9093402870809872		[learning rate: 0.00025613]
	Learning Rate: 0.000256131
	LOSS [training: 3.9093402870809872 | validation: 3.8978063604565887]
	TIME [epoch: 7.73 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.903251023864943		[learning rate: 0.00025553]
	Learning Rate: 0.000255527
	LOSS [training: 3.903251023864943 | validation: 3.8919709590794143]
	TIME [epoch: 7.74 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8938761948802467		[learning rate: 0.00025492]
	Learning Rate: 0.000254924
	LOSS [training: 3.8938761948802467 | validation: 3.8855445582207757]
	TIME [epoch: 7.74 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.88975262109526		[learning rate: 0.00025432]
	Learning Rate: 0.000254322
	LOSS [training: 3.88975262109526 | validation: 3.887988682184372]
	TIME [epoch: 7.77 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.89893746916997		[learning rate: 0.00025372]
	Learning Rate: 0.000253723
	LOSS [training: 3.89893746916997 | validation: 3.8952111685770148]
	TIME [epoch: 7.74 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.905148294649789		[learning rate: 0.00025312]
	Learning Rate: 0.000253124
	LOSS [training: 3.905148294649789 | validation: 3.893503009482095]
	TIME [epoch: 7.74 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.901977615621237		[learning rate: 0.00025253]
	Learning Rate: 0.000252527
	LOSS [training: 3.901977615621237 | validation: 3.8991293101329845]
	TIME [epoch: 7.73 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9088194478678693		[learning rate: 0.00025193]
	Learning Rate: 0.000251931
	LOSS [training: 3.9088194478678693 | validation: 3.900889886716807]
	TIME [epoch: 7.75 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.916359328196476		[learning rate: 0.00025134]
	Learning Rate: 0.000251337
	LOSS [training: 3.916359328196476 | validation: 3.9170139293911053]
	TIME [epoch: 7.78 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9147823762569		[learning rate: 0.00025074]
	Learning Rate: 0.000250744
	LOSS [training: 3.9147823762569 | validation: 3.8984620734035342]
	TIME [epoch: 7.74 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9073535630949703		[learning rate: 0.00025015]
	Learning Rate: 0.000250153
	LOSS [training: 3.9073535630949703 | validation: 3.902323336614975]
	TIME [epoch: 7.73 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.911373943088675		[learning rate: 0.00024956]
	Learning Rate: 0.000249563
	LOSS [training: 3.911373943088675 | validation: 3.9028998964393007]
	TIME [epoch: 7.75 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909682258799657		[learning rate: 0.00024897]
	Learning Rate: 0.000248974
	LOSS [training: 3.909682258799657 | validation: 3.906582699690116]
	TIME [epoch: 7.74 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9140353157294054		[learning rate: 0.00024839]
	Learning Rate: 0.000248387
	LOSS [training: 3.9140353157294054 | validation: 3.9120222660428094]
	TIME [epoch: 7.79 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.925242737722255		[learning rate: 0.0002478]
	Learning Rate: 0.000247801
	LOSS [training: 3.925242737722255 | validation: 3.925951300242062]
	TIME [epoch: 7.74 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.93029531650098		[learning rate: 0.00024722]
	Learning Rate: 0.000247216
	LOSS [training: 3.93029531650098 | validation: 3.921908629604536]
	TIME [epoch: 7.73 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.925769664815091		[learning rate: 0.00024663]
	Learning Rate: 0.000246633
	LOSS [training: 3.925769664815091 | validation: 3.9141654366916017]
	TIME [epoch: 7.73 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9206339834360557		[learning rate: 0.00024605]
	Learning Rate: 0.000246051
	LOSS [training: 3.9206339834360557 | validation: 3.917942246687359]
	TIME [epoch: 7.75 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9208358366873513		[learning rate: 0.00024547]
	Learning Rate: 0.000245471
	LOSS [training: 3.9208358366873513 | validation: 3.9088046892308594]
	TIME [epoch: 7.77 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.917691352046078		[learning rate: 0.00024489]
	Learning Rate: 0.000244892
	LOSS [training: 3.917691352046078 | validation: 3.905592098526113]
	TIME [epoch: 7.73 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.912364673329897		[learning rate: 0.00024431]
	Learning Rate: 0.000244314
	LOSS [training: 3.912364673329897 | validation: 3.9077313315913944]
	TIME [epoch: 7.73 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9225601893758593		[learning rate: 0.00024374]
	Learning Rate: 0.000243738
	LOSS [training: 3.9225601893758593 | validation: 3.9262252708261]
	TIME [epoch: 7.72 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9393718667274995		[learning rate: 0.00024316]
	Learning Rate: 0.000243163
	LOSS [training: 3.9393718667274995 | validation: 3.932723344790669]
	TIME [epoch: 7.75 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944394245498132		[learning rate: 0.00024259]
	Learning Rate: 0.000242589
	LOSS [training: 3.944394245498132 | validation: 3.9387941791989665]
	TIME [epoch: 7.76 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9495450729714996		[learning rate: 0.00024202]
	Learning Rate: 0.000242017
	LOSS [training: 3.9495450729714996 | validation: 3.945622520589489]
	TIME [epoch: 7.72 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.952592929720084		[learning rate: 0.00024145]
	Learning Rate: 0.000241446
	LOSS [training: 3.952592929720084 | validation: 3.9441570934854746]
	TIME [epoch: 7.72 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.960866349595369		[learning rate: 0.00024088]
	Learning Rate: 0.000240877
	LOSS [training: 3.960866349595369 | validation: 3.9621603987190643]
	TIME [epoch: 7.73 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.973791309327472		[learning rate: 0.00024031]
	Learning Rate: 0.000240309
	LOSS [training: 3.973791309327472 | validation: 3.9665454143458847]
	TIME [epoch: 7.75 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.979925802699997		[learning rate: 0.00023974]
	Learning Rate: 0.000239742
	LOSS [training: 3.979925802699997 | validation: 3.969783827486105]
	TIME [epoch: 7.77 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980086176831758		[learning rate: 0.00023918]
	Learning Rate: 0.000239176
	LOSS [training: 3.980086176831758 | validation: 3.9732905924897857]
	TIME [epoch: 7.72 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9794896561024116		[learning rate: 0.00023861]
	Learning Rate: 0.000238612
	LOSS [training: 3.9794896561024116 | validation: 3.9732190972877146]
	TIME [epoch: 7.73 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9760025104121026		[learning rate: 0.00023805]
	Learning Rate: 0.000238049
	LOSS [training: 3.9760025104121026 | validation: 3.9643897684300438]
	TIME [epoch: 7.74 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9726870388887416		[learning rate: 0.00023749]
	Learning Rate: 0.000237488
	LOSS [training: 3.9726870388887416 | validation: 3.9591243732161256]
	TIME [epoch: 7.76 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.970873571597478		[learning rate: 0.00023693]
	Learning Rate: 0.000236927
	LOSS [training: 3.970873571597478 | validation: 3.96352334171357]
	TIME [epoch: 7.74 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9765498781534685		[learning rate: 0.00023637]
	Learning Rate: 0.000236369
	LOSS [training: 3.9765498781534685 | validation: 3.9696265631771803]
	TIME [epoch: 7.72 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9769898890526596		[learning rate: 0.00023581]
	Learning Rate: 0.000235811
	LOSS [training: 3.9769898890526596 | validation: 3.9661557094743065]
	TIME [epoch: 7.72 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.97759029532169		[learning rate: 0.00023525]
	Learning Rate: 0.000235255
	LOSS [training: 3.97759029532169 | validation: 3.9742757827720236]
	TIME [epoch: 7.73 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9783186152539254		[learning rate: 0.0002347]
	Learning Rate: 0.0002347
	LOSS [training: 3.9783186152539254 | validation: 3.96769643358095]
	TIME [epoch: 7.77 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.979451923053171		[learning rate: 0.00023415]
	Learning Rate: 0.000234146
	LOSS [training: 3.979451923053171 | validation: 3.986496712781638]
	TIME [epoch: 7.73 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9953433042814517		[learning rate: 0.00023359]
	Learning Rate: 0.000233594
	LOSS [training: 3.9953433042814517 | validation: 3.9869655081611706]
	TIME [epoch: 7.72 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.990879309192736		[learning rate: 0.00023304]
	Learning Rate: 0.000233043
	LOSS [training: 3.990879309192736 | validation: 3.979377456559508]
	TIME [epoch: 7.73 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.994814034235004		[learning rate: 0.00023249]
	Learning Rate: 0.000232493
	LOSS [training: 3.994814034235004 | validation: 3.984997690291038]
	TIME [epoch: 7.74 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9905190859352913		[learning rate: 0.00023194]
	Learning Rate: 0.000231945
	LOSS [training: 3.9905190859352913 | validation: 3.9658710908271315]
	TIME [epoch: 7.77 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.985446077080868		[learning rate: 0.0002314]
	Learning Rate: 0.000231398
	LOSS [training: 3.985446077080868 | validation: 3.9796100124661677]
	TIME [epoch: 7.75 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9897232091825128		[learning rate: 0.00023085]
	Learning Rate: 0.000230852
	LOSS [training: 3.9897232091825128 | validation: 3.9837440928356305]
	TIME [epoch: 7.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.991608895961646		[learning rate: 0.00023031]
	Learning Rate: 0.000230307
	LOSS [training: 3.991608895961646 | validation: 3.9831215394828474]
	TIME [epoch: 7.74 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9938854335158		[learning rate: 0.00022976]
	Learning Rate: 0.000229764
	LOSS [training: 3.9938854335158 | validation: 3.9893011549879076]
	TIME [epoch: 7.73 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001893104415922		[learning rate: 0.00022922]
	Learning Rate: 0.000229222
	LOSS [training: 4.001893104415922 | validation: 3.9955922976977316]
	TIME [epoch: 7.78 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0027700948920515		[learning rate: 0.00022868]
	Learning Rate: 0.000228681
	LOSS [training: 4.0027700948920515 | validation: 3.9853237587075263]
	TIME [epoch: 7.72 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.989193525285638		[learning rate: 0.00022814]
	Learning Rate: 0.000228142
	LOSS [training: 3.989193525285638 | validation: 3.964774063909865]
	TIME [epoch: 7.73 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9691968929929953		[learning rate: 0.0002276]
	Learning Rate: 0.000227604
	LOSS [training: 3.9691968929929953 | validation: 3.9541446015509316]
	TIME [epoch: 7.73 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.962143380227213		[learning rate: 0.00022707]
	Learning Rate: 0.000227067
	LOSS [training: 3.962143380227213 | validation: 3.9496808057365724]
	TIME [epoch: 7.74 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9506327308972504		[learning rate: 0.00022653]
	Learning Rate: 0.000226531
	LOSS [training: 3.9506327308972504 | validation: 3.935378698529129]
	TIME [epoch: 7.78 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9346534083231615		[learning rate: 0.000226]
	Learning Rate: 0.000225997
	LOSS [training: 3.9346534083231615 | validation: 3.9293177227289027]
	TIME [epoch: 7.73 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.932081911888687		[learning rate: 0.00022546]
	Learning Rate: 0.000225464
	LOSS [training: 3.932081911888687 | validation: 3.9184996819819746]
	TIME [epoch: 7.73 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923607570223547		[learning rate: 0.00022493]
	Learning Rate: 0.000224932
	LOSS [training: 3.923607570223547 | validation: 3.9175089295485925]
	TIME [epoch: 7.74 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.919590607633183		[learning rate: 0.0002244]
	Learning Rate: 0.000224401
	LOSS [training: 3.919590607633183 | validation: 3.9126617033592703]
	TIME [epoch: 7.75 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.918384333503155		[learning rate: 0.00022387]
	Learning Rate: 0.000223872
	LOSS [training: 3.918384333503155 | validation: 3.915963375464731]
	TIME [epoch: 7.79 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9186646047857208		[learning rate: 0.00022334]
	Learning Rate: 0.000223344
	LOSS [training: 3.9186646047857208 | validation: 3.9103499295941924]
	TIME [epoch: 7.73 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9130273208068838		[learning rate: 0.00022282]
	Learning Rate: 0.000222817
	LOSS [training: 3.9130273208068838 | validation: 3.9040830741714396]
	TIME [epoch: 7.73 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909016382407556		[learning rate: 0.00022229]
	Learning Rate: 0.000222292
	LOSS [training: 3.909016382407556 | validation: 3.8939932038259446]
	TIME [epoch: 7.74 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8947282690628295		[learning rate: 0.00022177]
	Learning Rate: 0.000221767
	LOSS [training: 3.8947282690628295 | validation: 3.8834994473405504]
	TIME [epoch: 7.75 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.881248156773154		[learning rate: 0.00022124]
	Learning Rate: 0.000221244
	LOSS [training: 3.881248156773154 | validation: 3.871875082507722]
	TIME [epoch: 7.77 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870426495438054		[learning rate: 0.00022072]
	Learning Rate: 0.000220722
	LOSS [training: 3.870426495438054 | validation: 3.8703803760139226]
	TIME [epoch: 7.74 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875298009040053		[learning rate: 0.0002202]
	Learning Rate: 0.000220202
	LOSS [training: 3.875298009040053 | validation: 3.871196588224435]
	TIME [epoch: 7.73 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8708396260433373		[learning rate: 0.00021968]
	Learning Rate: 0.000219682
	LOSS [training: 3.8708396260433373 | validation: 3.8622182815195174]
	TIME [epoch: 7.73 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.864075304836125		[learning rate: 0.00021916]
	Learning Rate: 0.000219164
	LOSS [training: 3.864075304836125 | validation: 3.8591992428092707]
	TIME [epoch: 7.74 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.865868182820602		[learning rate: 0.00021865]
	Learning Rate: 0.000218647
	LOSS [training: 3.865868182820602 | validation: 3.864348137855523]
	TIME [epoch: 7.76 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863043418521964		[learning rate: 0.00021813]
	Learning Rate: 0.000218131
	LOSS [training: 3.863043418521964 | validation: 3.8634477316350204]
	TIME [epoch: 7.72 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.864559020187606		[learning rate: 0.00021762]
	Learning Rate: 0.000217617
	LOSS [training: 3.864559020187606 | validation: 3.85949225588327]
	TIME [epoch: 7.72 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.864949629361374		[learning rate: 0.0002171]
	Learning Rate: 0.000217103
	LOSS [training: 3.864949629361374 | validation: 3.8608090718612016]
	TIME [epoch: 7.73 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854132366142451		[learning rate: 0.00021659]
	Learning Rate: 0.000216591
	LOSS [training: 3.854132366142451 | validation: 3.8506557838769604]
	TIME [epoch: 7.76 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8550312690877675		[learning rate: 0.00021608]
	Learning Rate: 0.00021608
	LOSS [training: 3.8550312690877675 | validation: 3.8504345465782555]
	TIME [epoch: 7.76 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8531029375195263		[learning rate: 0.00021557]
	Learning Rate: 0.000215571
	LOSS [training: 3.8531029375195263 | validation: 3.8613558495710105]
	TIME [epoch: 7.74 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8623061825696716		[learning rate: 0.00021506]
	Learning Rate: 0.000215062
	LOSS [training: 3.8623061825696716 | validation: 3.86482327149123]
	TIME [epoch: 7.74 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8723183510101307		[learning rate: 0.00021455]
	Learning Rate: 0.000214555
	LOSS [training: 3.8723183510101307 | validation: 3.868862537600532]
	TIME [epoch: 7.74 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.866058075195192		[learning rate: 0.00021405]
	Learning Rate: 0.000214049
	LOSS [training: 3.866058075195192 | validation: 3.8593112458872456]
	TIME [epoch: 7.77 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.859401916530844		[learning rate: 0.00021354]
	Learning Rate: 0.000213544
	LOSS [training: 3.859401916530844 | validation: 3.860941156446966]
	TIME [epoch: 7.74 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8541251432185524		[learning rate: 0.00021304]
	Learning Rate: 0.00021304
	LOSS [training: 3.8541251432185524 | validation: 3.849808302106653]
	TIME [epoch: 7.74 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.853960521477711		[learning rate: 0.00021254]
	Learning Rate: 0.000212538
	LOSS [training: 3.853960521477711 | validation: 3.855759667555081]
	TIME [epoch: 7.72 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858064317263764		[learning rate: 0.00021204]
	Learning Rate: 0.000212036
	LOSS [training: 3.858064317263764 | validation: 3.8468995829822594]
	TIME [epoch: 7.73 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8470794914390876		[learning rate: 0.00021154]
	Learning Rate: 0.000211536
	LOSS [training: 3.8470794914390876 | validation: 3.8470325502763045]
	TIME [epoch: 7.77 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8393609482330566		[learning rate: 0.00021104]
	Learning Rate: 0.000211037
	LOSS [training: 3.8393609482330566 | validation: 3.8381747795837793]
	TIME [epoch: 7.74 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838715034522961		[learning rate: 0.00021054]
	Learning Rate: 0.000210539
	LOSS [training: 3.838715034522961 | validation: 3.844518192015327]
	TIME [epoch: 7.73 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841618695966416		[learning rate: 0.00021004]
	Learning Rate: 0.000210043
	LOSS [training: 3.841618695966416 | validation: 3.842219398984522]
	TIME [epoch: 7.72 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842322448997736		[learning rate: 0.00020955]
	Learning Rate: 0.000209547
	LOSS [training: 3.842322448997736 | validation: 3.8426724400825423]
	TIME [epoch: 7.74 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8395553485634646		[learning rate: 0.00020905]
	Learning Rate: 0.000209053
	LOSS [training: 3.8395553485634646 | validation: 3.8426827548283375]
	TIME [epoch: 7.78 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847102118166738		[learning rate: 0.00020856]
	Learning Rate: 0.00020856
	LOSS [training: 3.847102118166738 | validation: 3.8470249366025717]
	TIME [epoch: 7.74 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848504450679998		[learning rate: 0.00020807]
	Learning Rate: 0.000208068
	LOSS [training: 3.848504450679998 | validation: 3.851002510169757]
	TIME [epoch: 7.72 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8506157238535423		[learning rate: 0.00020758]
	Learning Rate: 0.000207577
	LOSS [training: 3.8506157238535423 | validation: 3.857516142397383]
	TIME [epoch: 7.73 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8529530949478814		[learning rate: 0.00020709]
	Learning Rate: 0.000207087
	LOSS [training: 3.8529530949478814 | validation: 3.8553519570999804]
	TIME [epoch: 7.74 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8570571333039902		[learning rate: 0.0002066]
	Learning Rate: 0.000206599
	LOSS [training: 3.8570571333039902 | validation: 3.8511318029327963]
	TIME [epoch: 7.78 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8531716484085683		[learning rate: 0.00020611]
	Learning Rate: 0.000206112
	LOSS [training: 3.8531716484085683 | validation: 3.853007666768928]
	TIME [epoch: 7.73 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8540512190330216		[learning rate: 0.00020563]
	Learning Rate: 0.000205625
	LOSS [training: 3.8540512190330216 | validation: 3.8547359818985796]
	TIME [epoch: 7.72 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852017825262047		[learning rate: 0.00020514]
	Learning Rate: 0.00020514
	LOSS [training: 3.852017825262047 | validation: 3.8523382526242984]
	TIME [epoch: 7.72 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8480830160129726		[learning rate: 0.00020466]
	Learning Rate: 0.000204657
	LOSS [training: 3.8480830160129726 | validation: 3.846509131187781]
	TIME [epoch: 7.75 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840853757622587		[learning rate: 0.00020417]
	Learning Rate: 0.000204174
	LOSS [training: 3.840853757622587 | validation: 3.839611695500941]
	TIME [epoch: 7.77 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.830416224357795		[learning rate: 0.00020369]
	Learning Rate: 0.000203692
	LOSS [training: 3.830416224357795 | validation: 3.826883772478622]
	TIME [epoch: 7.74 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.825246844296788		[learning rate: 0.00020321]
	Learning Rate: 0.000203212
	LOSS [training: 3.825246844296788 | validation: 3.8250970018452977]
	TIME [epoch: 7.72 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8290978935459012		[learning rate: 0.00020273]
	Learning Rate: 0.000202732
	LOSS [training: 3.8290978935459012 | validation: 3.8331130963841904]
	TIME [epoch: 7.72 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.835567059440203		[learning rate: 0.00020225]
	Learning Rate: 0.000202254
	LOSS [training: 3.835567059440203 | validation: 3.8339051034757077]
	TIME [epoch: 7.74 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841350725178056		[learning rate: 0.00020178]
	Learning Rate: 0.000201777
	LOSS [training: 3.841350725178056 | validation: 3.8494151157706753]
	TIME [epoch: 7.77 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8439204372777667		[learning rate: 0.0002013]
	Learning Rate: 0.000201301
	LOSS [training: 3.8439204372777667 | validation: 3.8406009018113796]
	TIME [epoch: 7.73 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405435525829787		[learning rate: 0.00020083]
	Learning Rate: 0.000200826
	LOSS [training: 3.8405435525829787 | validation: 3.8434768003548276]
	TIME [epoch: 7.73 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.843178281368186		[learning rate: 0.00020035]
	Learning Rate: 0.000200353
	LOSS [training: 3.843178281368186 | validation: 3.843658397540132]
	TIME [epoch: 7.72 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847863063140859		[learning rate: 0.00019988]
	Learning Rate: 0.00019988
	LOSS [training: 3.847863063140859 | validation: 3.8459824506079716]
	TIME [epoch: 7.75 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852824893766387		[learning rate: 0.00019941]
	Learning Rate: 0.000199408
	LOSS [training: 3.852824893766387 | validation: 3.8546456785867718]
	TIME [epoch: 7.76 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8585868251934445		[learning rate: 0.00019894]
	Learning Rate: 0.000198938
	LOSS [training: 3.8585868251934445 | validation: 3.861073835422288]
	TIME [epoch: 7.73 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.860082382404551		[learning rate: 0.00019847]
	Learning Rate: 0.000198469
	LOSS [training: 3.860082382404551 | validation: 3.8582718388006776]
	TIME [epoch: 7.74 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8618133783823767		[learning rate: 0.000198]
	Learning Rate: 0.000198001
	LOSS [training: 3.8618133783823767 | validation: 3.858680019684341]
	TIME [epoch: 7.72 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.861766891504218		[learning rate: 0.00019753]
	Learning Rate: 0.000197534
	LOSS [training: 3.861766891504218 | validation: 3.8654281845859275]
	TIME [epoch: 7.75 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8678100046705337		[learning rate: 0.00019707]
	Learning Rate: 0.000197068
	LOSS [training: 3.8678100046705337 | validation: 3.857613758473217]
	TIME [epoch: 7.76 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8558084101895442		[learning rate: 0.0001966]
	Learning Rate: 0.000196603
	LOSS [training: 3.8558084101895442 | validation: 3.8468918001352175]
	TIME [epoch: 7.74 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845655459308259		[learning rate: 0.00019614]
	Learning Rate: 0.000196139
	LOSS [training: 3.845655459308259 | validation: 3.8366093878747165]
	TIME [epoch: 7.74 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.844904390801504		[learning rate: 0.00019568]
	Learning Rate: 0.000195676
	LOSS [training: 3.844904390801504 | validation: 3.840602489981258]
	TIME [epoch: 7.74 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8448566073479		[learning rate: 0.00019521]
	Learning Rate: 0.000195215
	LOSS [training: 3.8448566073479 | validation: 3.843271794656787]
	TIME [epoch: 7.75 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405550749783384		[learning rate: 0.00019475]
	Learning Rate: 0.000194754
	LOSS [training: 3.8405550749783384 | validation: 3.846169739623309]
	TIME [epoch: 7.76 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8449887759769594		[learning rate: 0.00019429]
	Learning Rate: 0.000194295
	LOSS [training: 3.8449887759769594 | validation: 3.837349974575048]
	TIME [epoch: 7.74 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8357957932028217		[learning rate: 0.00019384]
	Learning Rate: 0.000193837
	LOSS [training: 3.8357957932028217 | validation: 3.8348748367431287]
	TIME [epoch: 7.72 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8314444606925786		[learning rate: 0.00019338]
	Learning Rate: 0.000193379
	LOSS [training: 3.8314444606925786 | validation: 3.8312898624724943]
	TIME [epoch: 7.75 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831502058686684		[learning rate: 0.00019292]
	Learning Rate: 0.000192923
	LOSS [training: 3.831502058686684 | validation: 3.8405058285010654]
	TIME [epoch: 7.77 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8383095475825764		[learning rate: 0.00019247]
	Learning Rate: 0.000192468
	LOSS [training: 3.8383095475825764 | validation: 3.845951767685412]
	TIME [epoch: 7.75 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849832325649286		[learning rate: 0.00019201]
	Learning Rate: 0.000192014
	LOSS [training: 3.849832325649286 | validation: 3.8589947128398583]
	TIME [epoch: 7.73 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8613538350952457		[learning rate: 0.00019156]
	Learning Rate: 0.000191561
	LOSS [training: 3.8613538350952457 | validation: 3.8638054704647145]
	TIME [epoch: 7.73 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8640285630362103		[learning rate: 0.00019111]
	Learning Rate: 0.000191109
	LOSS [training: 3.8640285630362103 | validation: 3.8621199403912394]
	TIME [epoch: 7.73 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8614555244459003		[learning rate: 0.00019066]
	Learning Rate: 0.000190659
	LOSS [training: 3.8614555244459003 | validation: 3.858812579269816]
	TIME [epoch: 7.78 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.856344010827199		[learning rate: 0.00019021]
	Learning Rate: 0.000190209
	LOSS [training: 3.856344010827199 | validation: 3.8530717257771627]
	TIME [epoch: 7.73 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8522910866756286		[learning rate: 0.00018976]
	Learning Rate: 0.00018976
	LOSS [training: 3.8522910866756286 | validation: 3.8482840717968596]
	TIME [epoch: 7.74 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8545383239102256		[learning rate: 0.00018931]
	Learning Rate: 0.000189313
	LOSS [training: 3.8545383239102256 | validation: 3.8600379895728234]
	TIME [epoch: 7.73 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8633778650160098		[learning rate: 0.00018887]
	Learning Rate: 0.000188866
	LOSS [training: 3.8633778650160098 | validation: 3.8617279955933044]
	TIME [epoch: 7.74 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863497027670994		[learning rate: 0.00018842]
	Learning Rate: 0.00018842
	LOSS [training: 3.863497027670994 | validation: 3.86003578933364]
	TIME [epoch: 7.76 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857072021160114		[learning rate: 0.00018798]
	Learning Rate: 0.000187976
	LOSS [training: 3.857072021160114 | validation: 3.856406911800237]
	TIME [epoch: 7.74 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8550355942065933		[learning rate: 0.00018753]
	Learning Rate: 0.000187533
	LOSS [training: 3.8550355942065933 | validation: 3.862316209804793]
	TIME [epoch: 7.73 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8605322596010545		[learning rate: 0.00018709]
	Learning Rate: 0.00018709
	LOSS [training: 3.8605322596010545 | validation: 3.87195732623937]
	TIME [epoch: 7.73 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8715596497672844		[learning rate: 0.00018665]
	Learning Rate: 0.000186649
	LOSS [training: 3.8715596497672844 | validation: 3.8674598513044494]
	TIME [epoch: 7.74 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870610607178199		[learning rate: 0.00018621]
	Learning Rate: 0.000186209
	LOSS [training: 3.870610607178199 | validation: 3.8706379867924943]
	TIME [epoch: 7.78 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8772419444804562		[learning rate: 0.00018577]
	Learning Rate: 0.000185769
	LOSS [training: 3.8772419444804562 | validation: 3.882372423772841]
	TIME [epoch: 7.74 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8862360375892804		[learning rate: 0.00018533]
	Learning Rate: 0.000185331
	LOSS [training: 3.8862360375892804 | validation: 3.876555083249674]
	TIME [epoch: 7.73 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8790482227304612		[learning rate: 0.00018489]
	Learning Rate: 0.000184894
	LOSS [training: 3.8790482227304612 | validation: 3.8720957260384337]
	TIME [epoch: 7.74 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875265115509843		[learning rate: 0.00018446]
	Learning Rate: 0.000184458
	LOSS [training: 3.875265115509843 | validation: 3.8791012507686067]
	TIME [epoch: 7.74 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879333233171612		[learning rate: 0.00018402]
	Learning Rate: 0.000184023
	LOSS [training: 3.879333233171612 | validation: 3.8702141712200326]
	TIME [epoch: 7.77 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8696278684032017		[learning rate: 0.00018359]
	Learning Rate: 0.000183589
	LOSS [training: 3.8696278684032017 | validation: 3.868434970052359]
	TIME [epoch: 7.73 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8721069992190853		[learning rate: 0.00018316]
	Learning Rate: 0.000183156
	LOSS [training: 3.8721069992190853 | validation: 3.8712882144898773]
	TIME [epoch: 7.73 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8739841391655045		[learning rate: 0.00018272]
	Learning Rate: 0.000182724
	LOSS [training: 3.8739841391655045 | validation: 3.8695462891133414]
	TIME [epoch: 7.73 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8662968042254993		[learning rate: 0.00018229]
	Learning Rate: 0.000182293
	LOSS [training: 3.8662968042254993 | validation: 3.8612540991624233]
	TIME [epoch: 7.74 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8600718924353257		[learning rate: 0.00018186]
	Learning Rate: 0.000181863
	LOSS [training: 3.8600718924353257 | validation: 3.8588710609897783]
	TIME [epoch: 7.77 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.861084619581572		[learning rate: 0.00018143]
	Learning Rate: 0.000181434
	LOSS [training: 3.861084619581572 | validation: 3.8614395825599237]
	TIME [epoch: 7.72 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872355582364506		[learning rate: 0.00018101]
	Learning Rate: 0.000181006
	LOSS [training: 3.872355582364506 | validation: 3.8742691704474397]
	TIME [epoch: 7.73 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.874271971382144		[learning rate: 0.00018058]
	Learning Rate: 0.000180579
	LOSS [training: 3.874271971382144 | validation: 3.8784606209837236]
	TIME [epoch: 7.73 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8749280131163752		[learning rate: 0.00018015]
	Learning Rate: 0.000180153
	LOSS [training: 3.8749280131163752 | validation: 3.8665836037367733]
	TIME [epoch: 7.74 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863151586971037		[learning rate: 0.00017973]
	Learning Rate: 0.000179728
	LOSS [training: 3.863151586971037 | validation: 3.8623151119506085]
	TIME [epoch: 7.77 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858808665849928		[learning rate: 0.0001793]
	Learning Rate: 0.000179304
	LOSS [training: 3.858808665849928 | validation: 3.8522019828950853]
	TIME [epoch: 7.75 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845753636473889		[learning rate: 0.00017888]
	Learning Rate: 0.000178881
	LOSS [training: 3.845753636473889 | validation: 3.8410040152994194]
	TIME [epoch: 7.72 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841309847578426		[learning rate: 0.00017846]
	Learning Rate: 0.000178459
	LOSS [training: 3.841309847578426 | validation: 3.8432121379785613]
	TIME [epoch: 7.71 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8453588509375978		[learning rate: 0.00017804]
	Learning Rate: 0.000178038
	LOSS [training: 3.8453588509375978 | validation: 3.8470810633851844]
	TIME [epoch: 7.74 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8483002988567874		[learning rate: 0.00017762]
	Learning Rate: 0.000177618
	LOSS [training: 3.8483002988567874 | validation: 3.8502257697343083]
	TIME [epoch: 7.76 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8502431536742274		[learning rate: 0.0001772]
	Learning Rate: 0.000177199
	LOSS [training: 3.8502431536742274 | validation: 3.8454067002432173]
	TIME [epoch: 7.72 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8433959528178168		[learning rate: 0.00017678]
	Learning Rate: 0.000176781
	LOSS [training: 3.8433959528178168 | validation: 3.8443341304728182]
	TIME [epoch: 7.72 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8458491862725737		[learning rate: 0.00017636]
	Learning Rate: 0.000176364
	LOSS [training: 3.8458491862725737 | validation: 3.843342806702051]
	TIME [epoch: 7.73 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8475343844485703		[learning rate: 0.00017595]
	Learning Rate: 0.000175948
	LOSS [training: 3.8475343844485703 | validation: 3.852784738824779]
	TIME [epoch: 7.76 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.85431162995295		[learning rate: 0.00017553]
	Learning Rate: 0.000175533
	LOSS [training: 3.85431162995295 | validation: 3.8489915051211083]
	TIME [epoch: 7.75 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8513391391043634		[learning rate: 0.00017512]
	Learning Rate: 0.000175119
	LOSS [training: 3.8513391391043634 | validation: 3.8433205384131517]
	TIME [epoch: 7.73 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.839033824725451		[learning rate: 0.00017471]
	Learning Rate: 0.000174706
	LOSS [training: 3.839033824725451 | validation: 3.829769884658484]
	TIME [epoch: 7.72 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.829375911286955		[learning rate: 0.00017429]
	Learning Rate: 0.000174294
	LOSS [training: 3.829375911286955 | validation: 3.8262423846807603]
	TIME [epoch: 7.73 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.82342943473863		[learning rate: 0.00017388]
	Learning Rate: 0.000173883
	LOSS [training: 3.82342943473863 | validation: 3.8243808536387403]
	TIME [epoch: 7.77 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821034187390876		[learning rate: 0.00017347]
	Learning Rate: 0.000173473
	LOSS [training: 3.821034187390876 | validation: 3.8258749625550434]
	TIME [epoch: 7.74 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8212405473687343		[learning rate: 0.00017306]
	Learning Rate: 0.000173063
	LOSS [training: 3.8212405473687343 | validation: 3.827502885895826]
	TIME [epoch: 7.73 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8187954888552995		[learning rate: 0.00017266]
	Learning Rate: 0.000172655
	LOSS [training: 3.8187954888552995 | validation: 3.824623278117345]
	TIME [epoch: 7.72 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8211726993614508		[learning rate: 0.00017225]
	Learning Rate: 0.000172248
	LOSS [training: 3.8211726993614508 | validation: 3.8234097521945474]
	TIME [epoch: 7.73 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826341438156742		[learning rate: 0.00017184]
	Learning Rate: 0.000171842
	LOSS [training: 3.826341438156742 | validation: 3.830380536024327]
	TIME [epoch: 7.78 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8306009355136723		[learning rate: 0.00017144]
	Learning Rate: 0.000171436
	LOSS [training: 3.8306009355136723 | validation: 3.8319822010309714]
	TIME [epoch: 7.72 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8382169383601052		[learning rate: 0.00017103]
	Learning Rate: 0.000171032
	LOSS [training: 3.8382169383601052 | validation: 3.838792170358165]
	TIME [epoch: 7.72 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8398423394382655		[learning rate: 0.00017063]
	Learning Rate: 0.000170628
	LOSS [training: 3.8398423394382655 | validation: 3.838026976328897]
	TIME [epoch: 7.73 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831897075494682		[learning rate: 0.00017023]
	Learning Rate: 0.000170226
	LOSS [training: 3.831897075494682 | validation: 3.8305897452318014]
	TIME [epoch: 7.74 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8331052190262414		[learning rate: 0.00016982]
	Learning Rate: 0.000169824
	LOSS [training: 3.8331052190262414 | validation: 3.8309745120300973]
	TIME [epoch: 7.77 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.823876467762466		[learning rate: 0.00016942]
	Learning Rate: 0.000169424
	LOSS [training: 3.823876467762466 | validation: 3.819173811467947]
	TIME [epoch: 7.73 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8154341982672824		[learning rate: 0.00016902]
	Learning Rate: 0.000169024
	LOSS [training: 3.8154341982672824 | validation: 3.8121721064995455]
	TIME [epoch: 7.73 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.806688872746424		[learning rate: 0.00016863]
	Learning Rate: 0.000168625
	LOSS [training: 3.806688872746424 | validation: 3.8057814760630975]
	TIME [epoch: 7.73 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8009683949631947		[learning rate: 0.00016823]
	Learning Rate: 0.000168228
	LOSS [training: 3.8009683949631947 | validation: 3.8033637828622036]
	TIME [epoch: 7.73 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8010720231801436		[learning rate: 0.00016783]
	Learning Rate: 0.000167831
	LOSS [training: 3.8010720231801436 | validation: 3.8049099496278056]
	TIME [epoch: 7.77 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8105180460474832		[learning rate: 0.00016743]
	Learning Rate: 0.000167435
	LOSS [training: 3.8105180460474832 | validation: 3.8097330329332717]
	TIME [epoch: 7.74 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.806998274254741		[learning rate: 0.00016704]
	Learning Rate: 0.00016704
	LOSS [training: 3.806998274254741 | validation: 3.8063844559542894]
	TIME [epoch: 7.72 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8055543250178854		[learning rate: 0.00016665]
	Learning Rate: 0.000166646
	LOSS [training: 3.8055543250178854 | validation: 3.811954687534006]
	TIME [epoch: 7.73 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8070532101288306		[learning rate: 0.00016625]
	Learning Rate: 0.000166253
	LOSS [training: 3.8070532101288306 | validation: 3.80704061717749]
	TIME [epoch: 7.74 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8042041100630968		[learning rate: 0.00016586]
	Learning Rate: 0.000165861
	LOSS [training: 3.8042041100630968 | validation: 3.8065648601892157]
	TIME [epoch: 7.78 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.797963055295432		[learning rate: 0.00016547]
	Learning Rate: 0.00016547
	LOSS [training: 3.797963055295432 | validation: 3.80125767155228]
	TIME [epoch: 7.72 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7998135761630696		[learning rate: 0.00016508]
	Learning Rate: 0.000165079
	LOSS [training: 3.7998135761630696 | validation: 3.8093944944513973]
	TIME [epoch: 7.73 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.805763587590518		[learning rate: 0.00016469]
	Learning Rate: 0.00016469
	LOSS [training: 3.805763587590518 | validation: 3.8065721279170184]
	TIME [epoch: 7.73 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7998928515010992		[learning rate: 0.0001643]
	Learning Rate: 0.000164301
	LOSS [training: 3.7998928515010992 | validation: 3.804094251482139]
	TIME [epoch: 7.73 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7989339868872145		[learning rate: 0.00016391]
	Learning Rate: 0.000163914
	LOSS [training: 3.7989339868872145 | validation: 3.810417405813986]
	TIME [epoch: 7.77 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7990241087622216		[learning rate: 0.00016353]
	Learning Rate: 0.000163527
	LOSS [training: 3.7990241087622216 | validation: 3.799235658704199]
	TIME [epoch: 7.72 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7977173142698697		[learning rate: 0.00016314]
	Learning Rate: 0.000163141
	LOSS [training: 3.7977173142698697 | validation: 3.804801837374083]
	TIME [epoch: 7.72 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8010092083285163		[learning rate: 0.00016276]
	Learning Rate: 0.000162757
	LOSS [training: 3.8010092083285163 | validation: 3.807220356607551]
	TIME [epoch: 7.73 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.805174300126721		[learning rate: 0.00016237]
	Learning Rate: 0.000162373
	LOSS [training: 3.805174300126721 | validation: 3.8154554733970443]
	TIME [epoch: 7.75 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.808716760154721		[learning rate: 0.00016199]
	Learning Rate: 0.00016199
	LOSS [training: 3.808716760154721 | validation: 3.810193430347165]
	TIME [epoch: 7.77 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8069336857476928		[learning rate: 0.00016161]
	Learning Rate: 0.000161608
	LOSS [training: 3.8069336857476928 | validation: 3.806058900811078]
	TIME [epoch: 7.72 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8052528268681893		[learning rate: 0.00016123]
	Learning Rate: 0.000161226
	LOSS [training: 3.8052528268681893 | validation: 3.809530203016275]
	TIME [epoch: 7.73 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8001264578905736		[learning rate: 0.00016085]
	Learning Rate: 0.000160846
	LOSS [training: 3.8001264578905736 | validation: 3.80663164522223]
	TIME [epoch: 7.73 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7974468133481505		[learning rate: 0.00016047]
	Learning Rate: 0.000160467
	LOSS [training: 3.7974468133481505 | validation: 3.806401984430831]
	TIME [epoch: 7.75 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7993575859897826		[learning rate: 0.00016009]
	Learning Rate: 0.000160088
	LOSS [training: 3.7993575859897826 | validation: 3.8043794387072345]
	TIME [epoch: 7.77 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.799804486519279		[learning rate: 0.00015971]
	Learning Rate: 0.00015971
	LOSS [training: 3.799804486519279 | validation: 3.8068780688537736]
	TIME [epoch: 7.72 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.801092438771059		[learning rate: 0.00015933]
	Learning Rate: 0.000159334
	LOSS [training: 3.801092438771059 | validation: 3.807228933137882]
	TIME [epoch: 7.73 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8054018788023907		[learning rate: 0.00015896]
	Learning Rate: 0.000158958
	LOSS [training: 3.8054018788023907 | validation: 3.813198884820836]
	TIME [epoch: 7.74 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.807813116344143		[learning rate: 0.00015858]
	Learning Rate: 0.000158583
	LOSS [training: 3.807813116344143 | validation: 3.810684041535817]
	TIME [epoch: 7.76 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8054508114560166		[learning rate: 0.00015821]
	Learning Rate: 0.000158209
	LOSS [training: 3.8054508114560166 | validation: 3.814334337712748]
	TIME [epoch: 7.74 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811851066399151		[learning rate: 0.00015784]
	Learning Rate: 0.000157836
	LOSS [training: 3.811851066399151 | validation: 3.8157741792957767]
	TIME [epoch: 7.73 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8114372838208865		[learning rate: 0.00015746]
	Learning Rate: 0.000157463
	LOSS [training: 3.8114372838208865 | validation: 3.8158713067563994]
	TIME [epoch: 7.72 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8093836035114514		[learning rate: 0.00015709]
	Learning Rate: 0.000157092
	LOSS [training: 3.8093836035114514 | validation: 3.8123008969394214]
	TIME [epoch: 7.73 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8035472605355354		[learning rate: 0.00015672]
	Learning Rate: 0.000156721
	LOSS [training: 3.8035472605355354 | validation: 3.8092820741617963]
	TIME [epoch: 7.78 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.801190925283565		[learning rate: 0.00015635]
	Learning Rate: 0.000156352
	LOSS [training: 3.801190925283565 | validation: 3.802101286551924]
	TIME [epoch: 7.74 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795518343589736		[learning rate: 0.00015598]
	Learning Rate: 0.000155983
	LOSS [training: 3.795518343589736 | validation: 3.7951676978401814]
	TIME [epoch: 7.74 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7938045945226744		[learning rate: 0.00015561]
	Learning Rate: 0.000155615
	LOSS [training: 3.7938045945226744 | validation: 3.8004214351402066]
	TIME [epoch: 7.75 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7932240537778075		[learning rate: 0.00015525]
	Learning Rate: 0.000155248
	LOSS [training: 3.7932240537778075 | validation: 3.8001774403763005]
	TIME [epoch: 7.73 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7931117032341404		[learning rate: 0.00015488]
	Learning Rate: 0.000154882
	LOSS [training: 3.7931117032341404 | validation: 3.8016459262885034]
	TIME [epoch: 7.78 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7923314628496256		[learning rate: 0.00015452]
	Learning Rate: 0.000154516
	LOSS [training: 3.7923314628496256 | validation: 3.7961144444680657]
	TIME [epoch: 7.74 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7936175575402964		[learning rate: 0.00015415]
	Learning Rate: 0.000154152
	LOSS [training: 3.7936175575402964 | validation: 3.7995099548145985]
	TIME [epoch: 7.75 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8011120285219397		[learning rate: 0.00015379]
	Learning Rate: 0.000153788
	LOSS [training: 3.8011120285219397 | validation: 3.807281209653385]
	TIME [epoch: 7.73 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804612238267968		[learning rate: 0.00015343]
	Learning Rate: 0.000153425
	LOSS [training: 3.804612238267968 | validation: 3.8119298812743545]
	TIME [epoch: 7.74 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8090995369719165		[learning rate: 0.00015306]
	Learning Rate: 0.000153064
	LOSS [training: 3.8090995369719165 | validation: 3.812250071580627]
	TIME [epoch: 7.78 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8064482623534075		[learning rate: 0.0001527]
	Learning Rate: 0.000152703
	LOSS [training: 3.8064482623534075 | validation: 3.8115399744174105]
	TIME [epoch: 7.74 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8056514713701195		[learning rate: 0.00015234]
	Learning Rate: 0.000152342
	LOSS [training: 3.8056514713701195 | validation: 3.8081943382636725]
	TIME [epoch: 7.74 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8083569631174834		[learning rate: 0.00015198]
	Learning Rate: 0.000151983
	LOSS [training: 3.8083569631174834 | validation: 3.811250988492573]
	TIME [epoch: 7.73 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8100282383010864		[learning rate: 0.00015162]
	Learning Rate: 0.000151624
	LOSS [training: 3.8100282383010864 | validation: 3.81158757976628]
	TIME [epoch: 7.74 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811031206324772		[learning rate: 0.00015127]
	Learning Rate: 0.000151267
	LOSS [training: 3.811031206324772 | validation: 3.812884036214691]
	TIME [epoch: 7.78 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810332912737321		[learning rate: 0.00015091]
	Learning Rate: 0.00015091
	LOSS [training: 3.810332912737321 | validation: 3.811036675035912]
	TIME [epoch: 7.73 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.812060000456961		[learning rate: 0.00015055]
	Learning Rate: 0.000150554
	LOSS [training: 3.812060000456961 | validation: 3.8182339048566885]
	TIME [epoch: 7.73 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8160082680937766		[learning rate: 0.0001502]
	Learning Rate: 0.000150199
	LOSS [training: 3.8160082680937766 | validation: 3.821365490664041]
	TIME [epoch: 7.72 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820651530311522		[learning rate: 0.00014984]
	Learning Rate: 0.000149845
	LOSS [training: 3.820651530311522 | validation: 3.8222621004868316]
	TIME [epoch: 7.74 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.817510620847402		[learning rate: 0.00014949]
	Learning Rate: 0.000149491
	LOSS [training: 3.817510620847402 | validation: 3.8168463058941153]
	TIME [epoch: 7.78 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814392630075622		[learning rate: 0.00014914]
	Learning Rate: 0.000149139
	LOSS [training: 3.814392630075622 | validation: 3.8157534901663004]
	TIME [epoch: 7.73 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810346051472081		[learning rate: 0.00014879]
	Learning Rate: 0.000148787
	LOSS [training: 3.810346051472081 | validation: 3.812696070691496]
	TIME [epoch: 7.74 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8114030817870947		[learning rate: 0.00014844]
	Learning Rate: 0.000148436
	LOSS [training: 3.8114030817870947 | validation: 3.8143130671032814]
	TIME [epoch: 7.73 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.813403801132249		[learning rate: 0.00014809]
	Learning Rate: 0.000148086
	LOSS [training: 3.813403801132249 | validation: 3.822320996296847]
	TIME [epoch: 7.74 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820340942983909		[learning rate: 0.00014774]
	Learning Rate: 0.000147736
	LOSS [training: 3.820340942983909 | validation: 3.820834425445863]
	TIME [epoch: 7.76 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81601520094023		[learning rate: 0.00014739]
	Learning Rate: 0.000147388
	LOSS [training: 3.81601520094023 | validation: 3.8198091856438205]
	TIME [epoch: 7.73 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814912080923045		[learning rate: 0.00014704]
	Learning Rate: 0.00014704
	LOSS [training: 3.814912080923045 | validation: 3.820453007286983]
	TIME [epoch: 7.72 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8165745310814048		[learning rate: 0.00014669]
	Learning Rate: 0.000146693
	LOSS [training: 3.8165745310814048 | validation: 3.8142243728283134]
	TIME [epoch: 7.73 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8154018549529227		[learning rate: 0.00014635]
	Learning Rate: 0.000146347
	LOSS [training: 3.8154018549529227 | validation: 3.8254938651031845]
	TIME [epoch: 7.75 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8200261926415626		[learning rate: 0.000146]
	Learning Rate: 0.000146002
	LOSS [training: 3.8200261926415626 | validation: 3.823146552764257]
	TIME [epoch: 7.77 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.822638375047568		[learning rate: 0.00014566]
	Learning Rate: 0.000145658
	LOSS [training: 3.822638375047568 | validation: 3.830925972359805]
	TIME [epoch: 7.73 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8290723306712175		[learning rate: 0.00014531]
	Learning Rate: 0.000145314
	LOSS [training: 3.8290723306712175 | validation: 3.8324227222738205]
	TIME [epoch: 7.73 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8324229559310212		[learning rate: 0.00014497]
	Learning Rate: 0.000144971
	LOSS [training: 3.8324229559310212 | validation: 3.8358495716099714]
	TIME [epoch: 7.74 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8380031817711155		[learning rate: 0.00014463]
	Learning Rate: 0.000144629
	LOSS [training: 3.8380031817711155 | validation: 3.8466442153812244]
	TIME [epoch: 7.76 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8408481580667395		[learning rate: 0.00014429]
	Learning Rate: 0.000144288
	LOSS [training: 3.8408481580667395 | validation: 3.838522990623683]
	TIME [epoch: 7.75 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.843867530841808		[learning rate: 0.00014395]
	Learning Rate: 0.000143948
	LOSS [training: 3.843867530841808 | validation: 3.850796299907251]
	TIME [epoch: 7.72 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8434458861756227		[learning rate: 0.00014361]
	Learning Rate: 0.000143608
	LOSS [training: 3.8434458861756227 | validation: 3.8430464232925994]
	TIME [epoch: 7.72 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8407176651002386		[learning rate: 0.00014327]
	Learning Rate: 0.00014327
	LOSS [training: 3.8407176651002386 | validation: 3.843451704694224]
	TIME [epoch: 7.73 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838179867860653		[learning rate: 0.00014293]
	Learning Rate: 0.000142932
	LOSS [training: 3.838179867860653 | validation: 3.832107127980119]
	TIME [epoch: 7.78 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8369395786085683		[learning rate: 0.00014259]
	Learning Rate: 0.000142594
	LOSS [training: 3.8369395786085683 | validation: 3.8413581180777925]
	TIME [epoch: 7.74 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8426344957127534		[learning rate: 0.00014226]
	Learning Rate: 0.000142258
	LOSS [training: 3.8426344957127534 | validation: 3.8424603927588494]
	TIME [epoch: 7.73 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8429089639534966		[learning rate: 0.00014192]
	Learning Rate: 0.000141923
	LOSS [training: 3.8429089639534966 | validation: 3.841966903981101]
	TIME [epoch: 7.72 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842828386308197		[learning rate: 0.00014159]
	Learning Rate: 0.000141588
	LOSS [training: 3.842828386308197 | validation: 3.8452063962354326]
	TIME [epoch: 7.73 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8453092172933574		[learning rate: 0.00014125]
	Learning Rate: 0.000141254
	LOSS [training: 3.8453092172933574 | validation: 3.847902112685987]
	TIME [epoch: 7.78 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849013152482882		[learning rate: 0.00014092]
	Learning Rate: 0.000140921
	LOSS [training: 3.849013152482882 | validation: 3.847722669087008]
	TIME [epoch: 7.74 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8492367101096887		[learning rate: 0.00014059]
	Learning Rate: 0.000140588
	LOSS [training: 3.8492367101096887 | validation: 3.850607674670327]
	TIME [epoch: 7.74 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8517733588217684		[learning rate: 0.00014026]
	Learning Rate: 0.000140257
	LOSS [training: 3.8517733588217684 | validation: 3.8553847428454833]
	TIME [epoch: 7.74 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8593217493001237		[learning rate: 0.00013993]
	Learning Rate: 0.000139926
	LOSS [training: 3.8593217493001237 | validation: 3.867053770637548]
	TIME [epoch: 7.74 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8623686081652977		[learning rate: 0.0001396]
	Learning Rate: 0.000139596
	LOSS [training: 3.8623686081652977 | validation: 3.8614516322286265]
	TIME [epoch: 7.77 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8624652468823935		[learning rate: 0.00013927]
	Learning Rate: 0.000139266
	LOSS [training: 3.8624652468823935 | validation: 3.8642125327029593]
	TIME [epoch: 7.73 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.862052789710649		[learning rate: 0.00013894]
	Learning Rate: 0.000138938
	LOSS [training: 3.862052789710649 | validation: 3.8602401190722313]
	TIME [epoch: 7.74 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8616939488914275		[learning rate: 0.00013861]
	Learning Rate: 0.00013861
	LOSS [training: 3.8616939488914275 | validation: 3.8605476337787445]
	TIME [epoch: 7.73 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8612679067427953		[learning rate: 0.00013828]
	Learning Rate: 0.000138283
	LOSS [training: 3.8612679067427953 | validation: 3.8599816924779917]
	TIME [epoch: 7.74 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.862226602177201		[learning rate: 0.00013796]
	Learning Rate: 0.000137957
	LOSS [training: 3.862226602177201 | validation: 3.860212454180264]
	TIME [epoch: 7.78 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858202217579045		[learning rate: 0.00013763]
	Learning Rate: 0.000137632
	LOSS [training: 3.858202217579045 | validation: 3.860348416898964]
	TIME [epoch: 7.72 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854550000899335		[learning rate: 0.00013731]
	Learning Rate: 0.000137307
	LOSS [training: 3.854550000899335 | validation: 3.851043408844702]
	TIME [epoch: 7.74 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8520510231198073		[learning rate: 0.00013698]
	Learning Rate: 0.000136983
	LOSS [training: 3.8520510231198073 | validation: 3.851545816326677]
	TIME [epoch: 7.72 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8514563342736436		[learning rate: 0.00013666]
	Learning Rate: 0.00013666
	LOSS [training: 3.8514563342736436 | validation: 3.8494792928410866]
	TIME [epoch: 7.74 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8491490111160864		[learning rate: 0.00013634]
	Learning Rate: 0.000136338
	LOSS [training: 3.8491490111160864 | validation: 3.8490955170557415]
	TIME [epoch: 7.77 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847004473310811		[learning rate: 0.00013602]
	Learning Rate: 0.000136016
	LOSS [training: 3.847004473310811 | validation: 3.8454354251562717]
	TIME [epoch: 7.74 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841998717988778		[learning rate: 0.0001357]
	Learning Rate: 0.000135695
	LOSS [training: 3.841998717988778 | validation: 3.842182269521671]
	TIME [epoch: 7.73 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.841533396023013		[learning rate: 0.00013538]
	Learning Rate: 0.000135375
	LOSS [training: 3.841533396023013 | validation: 3.8443051153927144]
	TIME [epoch: 7.72 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8430983369922775		[learning rate: 0.00013506]
	Learning Rate: 0.000135056
	LOSS [training: 3.8430983369922775 | validation: 3.8456111914644238]
	TIME [epoch: 7.75 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8475082558983082		[learning rate: 0.00013474]
	Learning Rate: 0.000134737
	LOSS [training: 3.8475082558983082 | validation: 3.8482326041536146]
	TIME [epoch: 7.77 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8510489550463642		[learning rate: 0.00013442]
	Learning Rate: 0.000134419
	LOSS [training: 3.8510489550463642 | validation: 3.8555816960277975]
	TIME [epoch: 7.73 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.846877030548239		[learning rate: 0.0001341]
	Learning Rate: 0.000134102
	LOSS [training: 3.846877030548239 | validation: 3.848292378573069]
	TIME [epoch: 7.72 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849353438351537		[learning rate: 0.00013379]
	Learning Rate: 0.000133786
	LOSS [training: 3.849353438351537 | validation: 3.8470211027201016]
	TIME [epoch: 7.72 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847487288904846		[learning rate: 0.00013347]
	Learning Rate: 0.00013347
	LOSS [training: 3.847487288904846 | validation: 3.8490311056011706]
	TIME [epoch: 7.75 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8520539270985403		[learning rate: 0.00013316]
	Learning Rate: 0.000133155
	LOSS [training: 3.8520539270985403 | validation: 3.8519155533501572]
	TIME [epoch: 7.78 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854413585911008		[learning rate: 0.00013284]
	Learning Rate: 0.000132841
	LOSS [training: 3.854413585911008 | validation: 3.8603233189625277]
	TIME [epoch: 7.73 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8562189124236843		[learning rate: 0.00013253]
	Learning Rate: 0.000132528
	LOSS [training: 3.8562189124236843 | validation: 3.857011535526926]
	TIME [epoch: 7.73 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.85202807563402		[learning rate: 0.00013222]
	Learning Rate: 0.000132215
	LOSS [training: 3.85202807563402 | validation: 3.8495687251671757]
	TIME [epoch: 7.74 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8496181781652496		[learning rate: 0.0001319]
	Learning Rate: 0.000131904
	LOSS [training: 3.8496181781652496 | validation: 3.8534358082607185]
	TIME [epoch: 7.76 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852306546434857		[learning rate: 0.00013159]
	Learning Rate: 0.000131592
	LOSS [training: 3.852306546434857 | validation: 3.847989599351992]
	TIME [epoch: 7.79 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8450977797247345		[learning rate: 0.00013128]
	Learning Rate: 0.000131282
	LOSS [training: 3.8450977797247345 | validation: 3.8429950983549492]
	TIME [epoch: 7.73 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8378535618060066		[learning rate: 0.00013097]
	Learning Rate: 0.000130972
	LOSS [training: 3.8378535618060066 | validation: 3.840690238650013]
	TIME [epoch: 7.74 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.837884298135814		[learning rate: 0.00013066]
	Learning Rate: 0.000130663
	LOSS [training: 3.837884298135814 | validation: 3.8479902868165765]
	TIME [epoch: 7.74 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.843848528519022		[learning rate: 0.00013036]
	Learning Rate: 0.000130355
	LOSS [training: 3.843848528519022 | validation: 3.84221796966417]
	TIME [epoch: 7.76 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840521412621132		[learning rate: 0.00013005]
	Learning Rate: 0.000130048
	LOSS [training: 3.840521412621132 | validation: 3.8479552476500283]
	TIME [epoch: 7.76 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8412663682939483		[learning rate: 0.00012974]
	Learning Rate: 0.000129741
	LOSS [training: 3.8412663682939483 | validation: 3.8445449009367643]
	TIME [epoch: 7.73 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8358520559783367		[learning rate: 0.00012943]
	Learning Rate: 0.000129435
	LOSS [training: 3.8358520559783367 | validation: 3.8342898985716807]
	TIME [epoch: 7.73 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8360499140407986		[learning rate: 0.00012913]
	Learning Rate: 0.00012913
	LOSS [training: 3.8360499140407986 | validation: 3.838109834826047]
	TIME [epoch: 7.73 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8359623375590046		[learning rate: 0.00012882]
	Learning Rate: 0.000128825
	LOSS [training: 3.8359623375590046 | validation: 3.8394198689371763]
	TIME [epoch: 7.77 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8362233497839036		[learning rate: 0.00012852]
	Learning Rate: 0.000128521
	LOSS [training: 3.8362233497839036 | validation: 3.842821720905265]
	TIME [epoch: 7.74 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.833828012219781		[learning rate: 0.00012822]
	Learning Rate: 0.000128218
	LOSS [training: 3.833828012219781 | validation: 3.833599323306202]
	TIME [epoch: 7.73 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.834538338078594		[learning rate: 0.00012792]
	Learning Rate: 0.000127915
	LOSS [training: 3.834538338078594 | validation: 3.8315270056572213]
	TIME [epoch: 7.72 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8321806832886236		[learning rate: 0.00012761]
	Learning Rate: 0.000127614
	LOSS [training: 3.8321806832886236 | validation: 3.8355590509448723]
	TIME [epoch: 7.73 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.837605697299742		[learning rate: 0.00012731]
	Learning Rate: 0.000127313
	LOSS [training: 3.837605697299742 | validation: 3.8497214472127594]
	TIME [epoch: 7.76 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.839613786171765		[learning rate: 0.00012701]
	Learning Rate: 0.000127012
	LOSS [training: 3.839613786171765 | validation: 3.839159386440818]
	TIME [epoch: 7.79 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8359597890815884		[learning rate: 0.00012671]
	Learning Rate: 0.000126713
	LOSS [training: 3.8359597890815884 | validation: 3.839065440318575]
	TIME [epoch: 7.73 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.835968169349637		[learning rate: 0.00012641]
	Learning Rate: 0.000126414
	LOSS [training: 3.835968169349637 | validation: 3.837424171176793]
	TIME [epoch: 7.73 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8394031210024835		[learning rate: 0.00012612]
	Learning Rate: 0.000126116
	LOSS [training: 3.8394031210024835 | validation: 3.844083149061632]
	TIME [epoch: 7.75 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8412016363370918		[learning rate: 0.00012582]
	Learning Rate: 0.000125818
	LOSS [training: 3.8412016363370918 | validation: 3.8411159233241348]
	TIME [epoch: 7.77 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8448309448749214		[learning rate: 0.00012552]
	Learning Rate: 0.000125521
	LOSS [training: 3.8448309448749214 | validation: 3.851900870200871]
	TIME [epoch: 7.74 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850285919266449		[learning rate: 0.00012523]
	Learning Rate: 0.000125225
	LOSS [training: 3.850285919266449 | validation: 3.861735314657815]
	TIME [epoch: 7.73 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.859972874946542		[learning rate: 0.00012493]
	Learning Rate: 0.00012493
	LOSS [training: 3.859972874946542 | validation: 3.8641371826407913]
	TIME [epoch: 7.73 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.868492055093882		[learning rate: 0.00012464]
	Learning Rate: 0.000124635
	LOSS [training: 3.868492055093882 | validation: 3.8700599584468067]
	TIME [epoch: 7.74 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8785865342000045		[learning rate: 0.00012434]
	Learning Rate: 0.000124341
	LOSS [training: 3.8785865342000045 | validation: 3.875876555080439]
	TIME [epoch: 7.8 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8794556307148684		[learning rate: 0.00012405]
	Learning Rate: 0.000124048
	LOSS [training: 3.8794556307148684 | validation: 3.8761773732137197]
	TIME [epoch: 7.74 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8787029935698456		[learning rate: 0.00012376]
	Learning Rate: 0.000123755
	LOSS [training: 3.8787029935698456 | validation: 3.878733107407437]
	TIME [epoch: 7.74 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879529679288794		[learning rate: 0.00012346]
	Learning Rate: 0.000123463
	LOSS [training: 3.879529679288794 | validation: 3.879696253603668]
	TIME [epoch: 7.75 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.876878001107518		[learning rate: 0.00012317]
	Learning Rate: 0.000123172
	LOSS [training: 3.876878001107518 | validation: 3.8732142730536028]
	TIME [epoch: 7.75 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8725141303839443		[learning rate: 0.00012288]
	Learning Rate: 0.000122882
	LOSS [training: 3.8725141303839443 | validation: 3.8692714321020274]
	TIME [epoch: 7.8 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8690694231007186		[learning rate: 0.00012259]
	Learning Rate: 0.000122592
	LOSS [training: 3.8690694231007186 | validation: 3.8699577772017664]
	TIME [epoch: 7.73 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870877985923707		[learning rate: 0.0001223]
	Learning Rate: 0.000122303
	LOSS [training: 3.870877985923707 | validation: 3.8734821761597686]
	TIME [epoch: 7.73 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.86948862541942		[learning rate: 0.00012201]
	Learning Rate: 0.000122014
	LOSS [training: 3.86948862541942 | validation: 3.8700402245420955]
	TIME [epoch: 7.73 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8691045947243117		[learning rate: 0.00012173]
	Learning Rate: 0.000121726
	LOSS [training: 3.8691045947243117 | validation: 3.8657344761729675]
	TIME [epoch: 7.75 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8675589440056877		[learning rate: 0.00012144]
	Learning Rate: 0.000121439
	LOSS [training: 3.8675589440056877 | validation: 3.8669763373313946]
	TIME [epoch: 7.77 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8697156844092495		[learning rate: 0.00012115]
	Learning Rate: 0.000121153
	LOSS [training: 3.8697156844092495 | validation: 3.8778007260955216]
	TIME [epoch: 7.73 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875121971997691		[learning rate: 0.00012087]
	Learning Rate: 0.000120867
	LOSS [training: 3.875121971997691 | validation: 3.87452604288794]
	TIME [epoch: 7.73 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.869955770652844		[learning rate: 0.00012058]
	Learning Rate: 0.000120582
	LOSS [training: 3.869955770652844 | validation: 3.8702053923475086]
	TIME [epoch: 7.73 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8665530196668714		[learning rate: 0.0001203]
	Learning Rate: 0.000120297
	LOSS [training: 3.8665530196668714 | validation: 3.8637337513483403]
	TIME [epoch: 7.75 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8655893158331684		[learning rate: 0.00012001]
	Learning Rate: 0.000120014
	LOSS [training: 3.8655893158331684 | validation: 3.8643874682662944]
	TIME [epoch: 7.78 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8641525430733785		[learning rate: 0.00011973]
	Learning Rate: 0.000119731
	LOSS [training: 3.8641525430733785 | validation: 3.864500087777592]
	TIME [epoch: 7.73 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8605302233574728		[learning rate: 0.00011945]
	Learning Rate: 0.000119448
	LOSS [training: 3.8605302233574728 | validation: 3.864486548134642]
	TIME [epoch: 7.74 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8592399538163096		[learning rate: 0.00011917]
	Learning Rate: 0.000119166
	LOSS [training: 3.8592399538163096 | validation: 3.863054974354804]
	TIME [epoch: 7.73 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8701756569756656		[learning rate: 0.00011889]
	Learning Rate: 0.000118885
	LOSS [training: 3.8701756569756656 | validation: 3.869534970032565]
	TIME [epoch: 7.76 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8690815804121206		[learning rate: 0.0001186]
	Learning Rate: 0.000118605
	LOSS [training: 3.8690815804121206 | validation: 3.8588151121210874]
	TIME [epoch: 7.75 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8634569855858905		[learning rate: 0.00011833]
	Learning Rate: 0.000118325
	LOSS [training: 3.8634569855858905 | validation: 3.861016491009345]
	TIME [epoch: 7.73 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8625282679253417		[learning rate: 0.00011805]
	Learning Rate: 0.000118046
	LOSS [training: 3.8625282679253417 | validation: 3.865166689691233]
	TIME [epoch: 7.73 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.860317694847928		[learning rate: 0.00011777]
	Learning Rate: 0.000117768
	LOSS [training: 3.860317694847928 | validation: 3.8575612670621755]
	TIME [epoch: 7.75 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8601957428069573		[learning rate: 0.00011749]
	Learning Rate: 0.00011749
	LOSS [training: 3.8601957428069573 | validation: 3.8673397326556405]
	TIME [epoch: 7.76 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872229202816912		[learning rate: 0.00011721]
	Learning Rate: 0.000117213
	LOSS [training: 3.872229202816912 | validation: 3.878821659000379]
	TIME [epoch: 7.74 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.878667786126076		[learning rate: 0.00011694]
	Learning Rate: 0.000116936
	LOSS [training: 3.878667786126076 | validation: 3.8778148281627365]
	TIME [epoch: 7.74 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8815726084541025		[learning rate: 0.00011666]
	Learning Rate: 0.00011666
	LOSS [training: 3.8815726084541025 | validation: 3.8880650797682073]
	TIME [epoch: 7.75 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.887501285547697		[learning rate: 0.00011639]
	Learning Rate: 0.000116385
	LOSS [training: 3.887501285547697 | validation: 3.8790019496394583]
	TIME [epoch: 7.74 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8830869523735947		[learning rate: 0.00011611]
	Learning Rate: 0.000116111
	LOSS [training: 3.8830869523735947 | validation: 3.8791023685512416]
	TIME [epoch: 7.79 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8788372825796316		[learning rate: 0.00011584]
	Learning Rate: 0.000115837
	LOSS [training: 3.8788372825796316 | validation: 3.8819513462139863]
	TIME [epoch: 7.75 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8814353586564287		[learning rate: 0.00011556]
	Learning Rate: 0.000115563
	LOSS [training: 3.8814353586564287 | validation: 3.872935472361781]
	TIME [epoch: 7.73 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8782105620426033		[learning rate: 0.00011529]
	Learning Rate: 0.000115291
	LOSS [training: 3.8782105620426033 | validation: 3.8789780890358196]
	TIME [epoch: 7.74 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8776270562777433		[learning rate: 0.00011502]
	Learning Rate: 0.000115019
	LOSS [training: 3.8776270562777433 | validation: 3.8731013683851057]
	TIME [epoch: 7.75 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.873108169705925		[learning rate: 0.00011475]
	Learning Rate: 0.000114748
	LOSS [training: 3.873108169705925 | validation: 3.875575666018837]
	TIME [epoch: 7.79 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8781901352346884		[learning rate: 0.00011448]
	Learning Rate: 0.000114477
	LOSS [training: 3.8781901352346884 | validation: 3.879898222100532]
	TIME [epoch: 7.73 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8868221033000347		[learning rate: 0.00011421]
	Learning Rate: 0.000114207
	LOSS [training: 3.8868221033000347 | validation: 3.889228281394012]
	TIME [epoch: 7.73 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.889475676771272		[learning rate: 0.00011394]
	Learning Rate: 0.000113938
	LOSS [training: 3.889475676771272 | validation: 3.889864572939505]
	TIME [epoch: 7.73 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8904307491638486		[learning rate: 0.00011367]
	Learning Rate: 0.000113669
	LOSS [training: 3.8904307491638486 | validation: 3.8946999852003406]
	TIME [epoch: 7.74 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.899827839166589		[learning rate: 0.0001134]
	Learning Rate: 0.000113401
	LOSS [training: 3.899827839166589 | validation: 3.8993487354902636]
	TIME [epoch: 7.78 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9014942491684876		[learning rate: 0.00011313]
	Learning Rate: 0.000113133
	LOSS [training: 3.9014942491684876 | validation: 3.8978987494138053]
	TIME [epoch: 7.74 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8985643032749837		[learning rate: 0.00011287]
	Learning Rate: 0.000112866
	LOSS [training: 3.8985643032749837 | validation: 3.90150995815367]
	TIME [epoch: 7.73 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.901109893118179		[learning rate: 0.0001126]
	Learning Rate: 0.0001126
	LOSS [training: 3.901109893118179 | validation: 3.899269332945118]
	TIME [epoch: 7.73 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.899752092237991		[learning rate: 0.00011233]
	Learning Rate: 0.000112334
	LOSS [training: 3.899752092237991 | validation: 3.8965910215384865]
	TIME [epoch: 7.76 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9028352428043647		[learning rate: 0.00011207]
	Learning Rate: 0.000112069
	LOSS [training: 3.9028352428043647 | validation: 3.9001854946506596]
	TIME [epoch: 7.77 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8985691075087145		[learning rate: 0.00011181]
	Learning Rate: 0.000111805
	LOSS [training: 3.8985691075087145 | validation: 3.8988741322079505]
	TIME [epoch: 7.73 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9027596502546555		[learning rate: 0.00011154]
	Learning Rate: 0.000111541
	LOSS [training: 3.9027596502546555 | validation: 3.9036406698446733]
	TIME [epoch: 7.73 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.901630555199444		[learning rate: 0.00011128]
	Learning Rate: 0.000111278
	LOSS [training: 3.901630555199444 | validation: 3.897374554868348]
	TIME [epoch: 7.74 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9032973598487293		[learning rate: 0.00011102]
	Learning Rate: 0.000111016
	LOSS [training: 3.9032973598487293 | validation: 3.9001994185647666]
	TIME [epoch: 7.74 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.897191652599759		[learning rate: 0.00011075]
	Learning Rate: 0.000110754
	LOSS [training: 3.897191652599759 | validation: 3.890672251379346]
	TIME [epoch: 7.78 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8886191967379196		[learning rate: 0.00011049]
	Learning Rate: 0.000110493
	LOSS [training: 3.8886191967379196 | validation: 3.8832074775888423]
	TIME [epoch: 7.74 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8823138801265293		[learning rate: 0.00011023]
	Learning Rate: 0.000110232
	LOSS [training: 3.8823138801265293 | validation: 3.875931908211458]
	TIME [epoch: 7.73 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8790213631996875		[learning rate: 0.00010997]
	Learning Rate: 0.000109972
	LOSS [training: 3.8790213631996875 | validation: 3.873664469867038]
	TIME [epoch: 7.74 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.882168750368867		[learning rate: 0.00010971]
	Learning Rate: 0.000109713
	LOSS [training: 3.882168750368867 | validation: 3.8853455795003473]
	TIME [epoch: 7.76 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.885953975604272		[learning rate: 0.00010945]
	Learning Rate: 0.000109454
	LOSS [training: 3.885953975604272 | validation: 3.885592422587095]
	TIME [epoch: 7.77 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.889407179368444		[learning rate: 0.0001092]
	Learning Rate: 0.000109196
	LOSS [training: 3.889407179368444 | validation: 3.8889643972441705]
	TIME [epoch: 7.73 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8940130157543265		[learning rate: 0.00010894]
	Learning Rate: 0.000108938
	LOSS [training: 3.8940130157543265 | validation: 3.8994660317987635]
	TIME [epoch: 7.73 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8965721692271496		[learning rate: 0.00010868]
	Learning Rate: 0.000108681
	LOSS [training: 3.8965721692271496 | validation: 3.894218317538877]
	TIME [epoch: 7.74 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8893582115556167		[learning rate: 0.00010842]
	Learning Rate: 0.000108425
	LOSS [training: 3.8893582115556167 | validation: 3.881126990665901]
	TIME [epoch: 7.77 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8825171702393444		[learning rate: 0.00010817]
	Learning Rate: 0.000108169
	LOSS [training: 3.8825171702393444 | validation: 3.881266228118784]
	TIME [epoch: 7.77 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.881284498615979		[learning rate: 0.00010791]
	Learning Rate: 0.000107914
	LOSS [training: 3.881284498615979 | validation: 3.8864868772250007]
	TIME [epoch: 7.75 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.88227251876824		[learning rate: 0.00010766]
	Learning Rate: 0.000107659
	LOSS [training: 3.88227251876824 | validation: 3.881931778535204]
	TIME [epoch: 7.74 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8797874093669398		[learning rate: 0.00010741]
	Learning Rate: 0.000107405
	LOSS [training: 3.8797874093669398 | validation: 3.8745000330218335]
	TIME [epoch: 7.73 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8758911003370584		[learning rate: 0.00010715]
	Learning Rate: 0.000107152
	LOSS [training: 3.8758911003370584 | validation: 3.877707329158685]
	TIME [epoch: 7.75 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8754648037506554		[learning rate: 0.0001069]
	Learning Rate: 0.000106899
	LOSS [training: 3.8754648037506554 | validation: 3.8743798544892387]
	TIME [epoch: 7.75 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.873438920212857		[learning rate: 0.00010665]
	Learning Rate: 0.000106647
	LOSS [training: 3.873438920212857 | validation: 3.8735645703938886]
	TIME [epoch: 7.73 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872306833377176		[learning rate: 0.0001064]
	Learning Rate: 0.000106395
	LOSS [training: 3.872306833377176 | validation: 3.869134786140541]
	TIME [epoch: 7.74 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.87660917353898		[learning rate: 0.00010614]
	Learning Rate: 0.000106145
	LOSS [training: 3.87660917353898 | validation: 3.874289312367731]
	TIME [epoch: 7.73 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8771300485797626		[learning rate: 0.00010589]
	Learning Rate: 0.000105894
	LOSS [training: 3.8771300485797626 | validation: 3.8805218861293467]
	TIME [epoch: 7.79 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.884085690911563		[learning rate: 0.00010564]
	Learning Rate: 0.000105644
	LOSS [training: 3.884085690911563 | validation: 3.8816962246236217]
	TIME [epoch: 7.75 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8843831992770883		[learning rate: 0.0001054]
	Learning Rate: 0.000105395
	LOSS [training: 3.8843831992770883 | validation: 3.883738612703575]
	TIME [epoch: 7.74 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8822313330988307		[learning rate: 0.00010515]
	Learning Rate: 0.000105147
	LOSS [training: 3.8822313330988307 | validation: 3.8788082099245766]
	TIME [epoch: 7.74 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8789039956196527		[learning rate: 0.0001049]
	Learning Rate: 0.000104899
	LOSS [training: 3.8789039956196527 | validation: 3.8780103703381883]
	TIME [epoch: 7.74 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.878496156178036		[learning rate: 0.00010465]
	Learning Rate: 0.000104651
	LOSS [training: 3.878496156178036 | validation: 3.876406930771969]
	TIME [epoch: 7.78 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.876931159235638		[learning rate: 0.0001044]
	Learning Rate: 0.000104404
	LOSS [training: 3.876931159235638 | validation: 3.873536565123233]
	TIME [epoch: 7.74 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.874948828047488		[learning rate: 0.00010416]
	Learning Rate: 0.000104158
	LOSS [training: 3.874948828047488 | validation: 3.8780679798763886]
	TIME [epoch: 7.74 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8752456885934174		[learning rate: 0.00010391]
	Learning Rate: 0.000103912
	LOSS [training: 3.8752456885934174 | validation: 3.8679227512775554]
	TIME [epoch: 7.76 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8651171875066037		[learning rate: 0.00010367]
	Learning Rate: 0.000103667
	LOSS [training: 3.8651171875066037 | validation: 3.8622526212108523]
	TIME [epoch: 7.73 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8556755910223783		[learning rate: 0.00010342]
	Learning Rate: 0.000103423
	LOSS [training: 3.8556755910223783 | validation: 3.8513305655327033]
	TIME [epoch: 7.78 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848442567694846		[learning rate: 0.00010318]
	Learning Rate: 0.000103179
	LOSS [training: 3.848442567694846 | validation: 3.8485205267786418]
	TIME [epoch: 7.75 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850142601543939		[learning rate: 0.00010294]
	Learning Rate: 0.000102935
	LOSS [training: 3.850142601543939 | validation: 3.85046396631677]
	TIME [epoch: 7.74 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8502481021141968		[learning rate: 0.00010269]
	Learning Rate: 0.000102692
	LOSS [training: 3.8502481021141968 | validation: 3.852515961988747]
	TIME [epoch: 7.72 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849604922450838		[learning rate: 0.00010245]
	Learning Rate: 0.00010245
	LOSS [training: 3.849604922450838 | validation: 3.8474163235159193]
	TIME [epoch: 7.76 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8489784795202584		[learning rate: 0.00010221]
	Learning Rate: 0.000102209
	LOSS [training: 3.8489784795202584 | validation: 3.852959161436915]
	TIME [epoch: 7.78 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852085239058826		[learning rate: 0.00010197]
	Learning Rate: 0.000101967
	LOSS [training: 3.852085239058826 | validation: 3.853412968493667]
	TIME [epoch: 7.74 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8478089947539265		[learning rate: 0.00010173]
	Learning Rate: 0.000101727
	LOSS [training: 3.8478089947539265 | validation: 3.8528878799445128]
	TIME [epoch: 7.74 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848505592690156		[learning rate: 0.00010149]
	Learning Rate: 0.000101487
	LOSS [training: 3.848505592690156 | validation: 3.8475900449299174]
	TIME [epoch: 7.75 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.846582229052315		[learning rate: 0.00010125]
	Learning Rate: 0.000101248
	LOSS [training: 3.846582229052315 | validation: 3.8485033422136543]
	TIME [epoch: 7.76 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.848917374996686		[learning rate: 0.00010101]
	Learning Rate: 0.000101009
	LOSS [training: 3.848917374996686 | validation: 3.8477306871720214]
	TIME [epoch: 7.78 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.845236336970903		[learning rate: 0.00010077]
	Learning Rate: 0.000100771
	LOSS [training: 3.845236336970903 | validation: 3.8525044004259708]
	TIME [epoch: 7.73 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8446942675296736		[learning rate: 0.00010053]
	Learning Rate: 0.000100533
	LOSS [training: 3.8446942675296736 | validation: 3.849601282771882]
	TIME [epoch: 7.73 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8465961780725473		[learning rate: 0.0001003]
	Learning Rate: 0.000100296
	LOSS [training: 3.8465961780725473 | validation: 3.844683025923538]
	TIME [epoch: 7.74 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8432560784632277		[learning rate: 0.00010006]
	Learning Rate: 0.000100059
	LOSS [training: 3.8432560784632277 | validation: 3.842837945625918]
	TIME [epoch: 7.76 sec]
Finished training in 15965.901 seconds.
