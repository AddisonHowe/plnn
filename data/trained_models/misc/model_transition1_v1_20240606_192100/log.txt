Args:
Namespace(name='model_transition1_v1', outdir='out/model_training/model_transition1_v1', training_data='data/training_data/data_transition1_subset_epi_trans_ce_an_pc12/training', validation_data='data/training_data/data_transition1_subset_epi_trans_ce_an_pc12/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=10, patience=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.001, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 418734776

Training model...

Saving initial model state to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.934734207406686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.934734207406686 | validation: 0.8597536510623561]
	TIME [epoch: 49.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7962140355359644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962140355359644 | validation: 0.7998964439771089]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6598495454175112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6598495454175112 | validation: 0.7508319801428867]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6730332794401035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730332794401035 | validation: 0.7653879939466602]
	TIME [epoch: 21.6 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7061643405878114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061643405878114 | validation: 0.7518266761934783]
	TIME [epoch: 21.6 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6899749585101196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899749585101196 | validation: 0.7486143956436253]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968223884836015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968223884836015 | validation: 0.7601342003578959]
	TIME [epoch: 21.6 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7249718895321268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7249718895321268 | validation: 0.6993579689674473]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5984883027339423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5984883027339423 | validation: 0.6898460526869084]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7095586544969973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7095586544969973 | validation: 0.6821436783377047]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.556477276393595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.556477276393595 | validation: 0.654031702868974]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.632963617726132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632963617726132 | validation: 0.6594437749456911]
	TIME [epoch: 21.6 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5720200136798363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5720200136798363 | validation: 0.6700366882392784]
	TIME [epoch: 21.7 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6487776892782973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6487776892782973 | validation: 0.6057694924253427]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48780011292844544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48780011292844544 | validation: 0.5684413162852976]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5892643760247646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5892643760247646 | validation: 0.5701580572120629]
	TIME [epoch: 21.7 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4857381968420989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4857381968420989 | validation: 0.717005765047539]
	TIME [epoch: 21.6 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5374842204843582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5374842204843582 | validation: 0.5123060891811358]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.543022249374698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.543022249374698 | validation: 0.4879792697428018]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4178389181086393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4178389181086393 | validation: 0.534474424185505]
	TIME [epoch: 21.7 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49638612386675424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49638612386675424 | validation: 0.4483977803336933]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40635160039555834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40635160039555834 | validation: 0.4170037077006245]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36936034957028896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36936034957028896 | validation: 0.4178506805529348]
	TIME [epoch: 21.7 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3635461386372709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3635461386372709 | validation: 0.40268464685675526]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40477902700619967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40477902700619967 | validation: 0.3848549992581464]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39770015365629346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39770015365629346 | validation: 0.3662940301139011]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36340275359619817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36340275359619817 | validation: 0.3554397948756023]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35703257011341905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35703257011341905 | validation: 0.3562871643976875]
	TIME [epoch: 21.6 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35885826413352834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35885826413352834 | validation: 0.3359694684858823]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3112386209536966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3112386209536966 | validation: 0.331668670144337]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31581185460298467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31581185460298467 | validation: 0.3264931903051362]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2901216260582503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2901216260582503 | validation: 0.38176973889214677]
	TIME [epoch: 21.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3164175981600509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3164175981600509 | validation: 0.3099566146029808]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3123888485654916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3123888485654916 | validation: 0.3044306126046777]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30227838238901955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30227838238901955 | validation: 0.3047321097620217]
	TIME [epoch: 21.4 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28437853564324145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28437853564324145 | validation: 0.31271099395852514]
	TIME [epoch: 21.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2994181636498417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2994181636498417 | validation: 0.33159042106776276]
	TIME [epoch: 21.5 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26672019154458065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26672019154458065 | validation: 0.36287652297989376]
	TIME [epoch: 21.4 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3145167860500403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3145167860500403 | validation: 0.30221567843676406]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29946954255521685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29946954255521685 | validation: 0.294513353545282]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24714293582719513		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.24714293582719513 | validation: 0.29906419998785466]
	TIME [epoch: 21.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2686451576972698		[learning rate: 0.0099206]
	Learning Rate: 0.00992061
	LOSS [training: 0.2686451576972698 | validation: 0.30585490232750323]
	TIME [epoch: 21.3 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2930580938204582		[learning rate: 0.0098768]
	Learning Rate: 0.00987678
	LOSS [training: 0.2930580938204582 | validation: 0.28373620429895174]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2680789023155714		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.2680789023155714 | validation: 0.27770738806731604]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2917928487504563		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.2917928487504563 | validation: 0.2868650959677033]
	TIME [epoch: 21.5 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27741046401966357		[learning rate: 0.0097464]
	Learning Rate: 0.00974644
	LOSS [training: 0.27741046401966357 | validation: 0.2772760095604809]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2726250743339933		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 0.2726250743339933 | validation: 0.31734539140899604]
	TIME [epoch: 21.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3312056149552341		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.3312056149552341 | validation: 0.27051068636318953]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30319795705250197		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.30319795705250197 | validation: 0.27026936874005275]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23312357342366363		[learning rate: 0.0095753]
	Learning Rate: 0.00957533
	LOSS [training: 0.23312357342366363 | validation: 0.2649911768983339]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2330915670425453		[learning rate: 0.009533]
	Learning Rate: 0.00953303
	LOSS [training: 0.2330915670425453 | validation: 0.2678487356856185]
	TIME [epoch: 21.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2788949982252018		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.2788949982252018 | validation: 0.2532503480681395]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22784346793390223		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.22784346793390223 | validation: 0.24763378452429477]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2368471722525814		[learning rate: 0.0094072]
	Learning Rate: 0.00940722
	LOSS [training: 0.2368471722525814 | validation: 0.2677051340259918]
	TIME [epoch: 21.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2581744133587891		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 0.2581744133587891 | validation: 0.27000230003510367]
	TIME [epoch: 21.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27094943271769506		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.27094943271769506 | validation: 0.2508029025939552]
	TIME [epoch: 21.4 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2517892531370686		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.2517892531370686 | validation: 0.27842072597307216]
	TIME [epoch: 21.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2408918043869138		[learning rate: 0.0092421]
	Learning Rate: 0.00924207
	LOSS [training: 0.2408918043869138 | validation: 0.23806445813170019]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2303093439380839		[learning rate: 0.0092012]
	Learning Rate: 0.00920124
	LOSS [training: 0.2303093439380839 | validation: 0.2713660773636684]
	TIME [epoch: 21.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2470068442452181		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.2470068442452181 | validation: 0.23963701580133323]
	TIME [epoch: 21.3 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2513918019247944		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.2513918019247944 | validation: 0.27716318707512616]
	TIME [epoch: 21.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24535837390475726		[learning rate: 0.0090798]
	Learning Rate: 0.00907981
	LOSS [training: 0.24535837390475726 | validation: 0.267156980335812]
	TIME [epoch: 21.2 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24309416807591658		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 0.24309416807591658 | validation: 0.22263485595388358]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2537895598348004		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.2537895598348004 | validation: 0.23741647462867385]
	TIME [epoch: 21.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2197097279050318		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.2197097279050318 | validation: 0.25651835163229436]
	TIME [epoch: 21.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2790409181905794		[learning rate: 0.0089204]
	Learning Rate: 0.00892041
	LOSS [training: 0.2790409181905794 | validation: 0.2718628457868365]
	TIME [epoch: 21.3 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22128636498828982		[learning rate: 0.008881]
	Learning Rate: 0.00888099
	LOSS [training: 0.22128636498828982 | validation: 0.2805732461282548]
	TIME [epoch: 21.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25500732255992825		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.25500732255992825 | validation: 0.27683038931715537]
	TIME [epoch: 21.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23180200943048365		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.23180200943048365 | validation: 0.26331414857298757]
	TIME [epoch: 21.5 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25838216061948616		[learning rate: 0.0087638]
	Learning Rate: 0.0087638
	LOSS [training: 0.25838216061948616 | validation: 0.2308878570376438]
	TIME [epoch: 21.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21489146371397666		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 0.21489146371397666 | validation: 0.255154911415949]
	TIME [epoch: 21.5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22450498761845009		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.22450498761845009 | validation: 0.2168222091182861]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20317559918972056		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.20317559918972056 | validation: 0.2192485558458691]
	TIME [epoch: 21.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21541650333534562		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.21541650333534562 | validation: 0.2169006792565812]
	TIME [epoch: 21.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20751001826045354		[learning rate: 0.0085719]
	Learning Rate: 0.0085719
	LOSS [training: 0.20751001826045354 | validation: 0.27081117773562524]
	TIME [epoch: 21.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2365491067109157		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.2365491067109157 | validation: 0.27641576464104206]
	TIME [epoch: 21.5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26534651878144855		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.26534651878144855 | validation: 0.21920487870748354]
	TIME [epoch: 21.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23278383073841122		[learning rate: 0.0084588]
	Learning Rate: 0.00845878
	LOSS [training: 0.23278383073841122 | validation: 0.22588304853826144]
	TIME [epoch: 21.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2021050608136715		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 0.2021050608136715 | validation: 0.2490689308930686]
	TIME [epoch: 21.4 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23013127924125384		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.23013127924125384 | validation: 0.23982360947286205]
	TIME [epoch: 21.4 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23144798739267453		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.23144798739267453 | validation: 0.22493014411390608]
	TIME [epoch: 21.5 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21870271077092926		[learning rate: 0.0083103]
	Learning Rate: 0.00831027
	LOSS [training: 0.21870271077092926 | validation: 0.21183691115874845]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19843078987837845		[learning rate: 0.0082736]
	Learning Rate: 0.00827356
	LOSS [training: 0.19843078987837845 | validation: 0.2739651197713499]
	TIME [epoch: 21.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2498020627297836		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2498020627297836 | validation: 0.21984041174004876]
	TIME [epoch: 21.4 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18062328344421658		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.18062328344421658 | validation: 0.2098039815621784]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22215540460781283		[learning rate: 0.0081644]
	Learning Rate: 0.00816438
	LOSS [training: 0.22215540460781283 | validation: 0.20034948128297433]
	TIME [epoch: 21.5 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2198850195187954		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 0.2198850195187954 | validation: 0.23695512365474483]
	TIME [epoch: 21.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18290987669218606		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.18290987669218606 | validation: 0.19592147428452758]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18721378167081631		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.18721378167081631 | validation: 0.2207778687496698]
	TIME [epoch: 21.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19521342135867478		[learning rate: 0.008021]
	Learning Rate: 0.00802104
	LOSS [training: 0.19521342135867478 | validation: 0.23921011786251242]
	TIME [epoch: 21.6 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19548959128191926		[learning rate: 0.0079856]
	Learning Rate: 0.0079856
	LOSS [training: 0.19548959128191926 | validation: 0.2000903001663941]
	TIME [epoch: 21.6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18861592830770949		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.18861592830770949 | validation: 0.20743730911069536]
	TIME [epoch: 21.6 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20614654704514374		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.20614654704514374 | validation: 0.23719121919355227]
	TIME [epoch: 21.6 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22658613780774467		[learning rate: 0.0078802]
	Learning Rate: 0.00788022
	LOSS [training: 0.22658613780774467 | validation: 0.20217632814672537]
	TIME [epoch: 21.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2028448230029519		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.2028448230029519 | validation: 0.23326973879861182]
	TIME [epoch: 21.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1984975911313994		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.1984975911313994 | validation: 0.2165103506767129]
	TIME [epoch: 21.5 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.209928970815282		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.209928970815282 | validation: 0.2099547417109534]
	TIME [epoch: 21.6 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19286930613070769		[learning rate: 0.0077419]
	Learning Rate: 0.00774188
	LOSS [training: 0.19286930613070769 | validation: 0.23774895983828168]
	TIME [epoch: 21.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21204251718240053		[learning rate: 0.0077077]
	Learning Rate: 0.00770767
	LOSS [training: 0.21204251718240053 | validation: 0.2372675937006458]
	TIME [epoch: 21.5 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19475385124429642		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.19475385124429642 | validation: 0.20115309292353808]
	TIME [epoch: 21.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19283537955353086		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.19283537955353086 | validation: 0.1985677118108576]
	TIME [epoch: 21.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18154423482020016		[learning rate: 0.007606]
	Learning Rate: 0.00760596
	LOSS [training: 0.18154423482020016 | validation: 0.20637009643287238]
	TIME [epoch: 21.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17041885039200838		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.17041885039200838 | validation: 0.2000014613127399]
	TIME [epoch: 21.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22135839101630053		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.22135839101630053 | validation: 0.2044534406569557]
	TIME [epoch: 21.6 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21524649049367123		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.21524649049367123 | validation: 0.2219154606045514]
	TIME [epoch: 21.6 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19340196661153305		[learning rate: 0.0074724]
	Learning Rate: 0.00747242
	LOSS [training: 0.19340196661153305 | validation: 0.21548578169569146]
	TIME [epoch: 21.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21701801818602556		[learning rate: 0.0074394]
	Learning Rate: 0.00743941
	LOSS [training: 0.21701801818602556 | validation: 0.2068237506297231]
	TIME [epoch: 21.6 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19065506291141837		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.19065506291141837 | validation: 0.201977955693554]
	TIME [epoch: 21.6 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1751021886272369		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.1751021886272369 | validation: 0.2076413502457027]
	TIME [epoch: 21.6 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17677595715191347		[learning rate: 0.0073412]
	Learning Rate: 0.00734124
	LOSS [training: 0.17677595715191347 | validation: 0.17632340427611828]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16971046890070057		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.16971046890070057 | validation: 0.21333661275194124]
	TIME [epoch: 21.5 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20185194792614772		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.20185194792614772 | validation: 0.19485225929557348]
	TIME [epoch: 21.5 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17050818371001736		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.17050818371001736 | validation: 0.24074863435441665]
	TIME [epoch: 21.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17818712727843034		[learning rate: 0.0072124]
	Learning Rate: 0.00721235
	LOSS [training: 0.17818712727843034 | validation: 0.21492330215472924]
	TIME [epoch: 21.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18859372068560823		[learning rate: 0.0071805]
	Learning Rate: 0.00718049
	LOSS [training: 0.18859372068560823 | validation: 0.1989700333134096]
	TIME [epoch: 21.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20892828888171594		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.20892828888171594 | validation: 0.18133359638272709]
	TIME [epoch: 21.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20337813826216022		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.20337813826216022 | validation: 0.19444199635437753]
	TIME [epoch: 21.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19250783529119292		[learning rate: 0.0070857]
	Learning Rate: 0.00708573
	LOSS [training: 0.19250783529119292 | validation: 0.21048879640801807]
	TIME [epoch: 21.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16490342993904425		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.16490342993904425 | validation: 0.18245732008848395]
	TIME [epoch: 21.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1659860159633295		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.1659860159633295 | validation: 0.17576308865166532]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19883160645319017		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.19883160645319017 | validation: 0.17452003745993974]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1691203543394054		[learning rate: 0.0069613]
	Learning Rate: 0.00696133
	LOSS [training: 0.1691203543394054 | validation: 0.17699089100950424]
	TIME [epoch: 21.2 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2018104323740773		[learning rate: 0.0069306]
	Learning Rate: 0.00693057
	LOSS [training: 0.2018104323740773 | validation: 0.16760157821474056]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15837379191223225		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.15837379191223225 | validation: 0.19662886466876958]
	TIME [epoch: 21.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18898233143702678		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.18898233143702678 | validation: 0.19254752545873988]
	TIME [epoch: 21.2 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16457590329319438		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.16457590329319438 | validation: 0.2080319496856672]
	TIME [epoch: 21.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2039073699792761		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.2039073699792761 | validation: 0.22233179438902725]
	TIME [epoch: 21.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20148657206544668		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.20148657206544668 | validation: 0.17156419748017604]
	TIME [epoch: 21.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16509559541632762		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.16509559541632762 | validation: 0.18830646069020163]
	TIME [epoch: 21.2 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1611594130656404		[learning rate: 0.006719]
	Learning Rate: 0.00671905
	LOSS [training: 0.1611594130656404 | validation: 0.17776593877809668]
	TIME [epoch: 21.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20484648244187328		[learning rate: 0.0066894]
	Learning Rate: 0.00668936
	LOSS [training: 0.20484648244187328 | validation: 0.2618633758271265]
	TIME [epoch: 21.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20738011643947854		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.20738011643947854 | validation: 0.19380538785537063]
	TIME [epoch: 21.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17938528293767683		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.17938528293767683 | validation: 0.20660791692332994]
	TIME [epoch: 21.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16851888431273238		[learning rate: 0.0066011]
	Learning Rate: 0.00660109
	LOSS [training: 0.16851888431273238 | validation: 0.18253438371832487]
	TIME [epoch: 21.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18316260718847024		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.18316260718847024 | validation: 0.1824128588943225]
	TIME [epoch: 21.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19166508754401917		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.19166508754401917 | validation: 0.1906615017766017]
	TIME [epoch: 21.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17824952537872418		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.17824952537872418 | validation: 0.1923097063958302]
	TIME [epoch: 21.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16497855639084813		[learning rate: 0.0064852]
	Learning Rate: 0.00648519
	LOSS [training: 0.16497855639084813 | validation: 0.18042415731541717]
	TIME [epoch: 21.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16157200807266267		[learning rate: 0.0064565]
	Learning Rate: 0.00645654
	LOSS [training: 0.16157200807266267 | validation: 0.1995260275886097]
	TIME [epoch: 21.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16092218825877216		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.16092218825877216 | validation: 0.17333859245236902]
	TIME [epoch: 21.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18469202844031912		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.18469202844031912 | validation: 0.18939207405271566]
	TIME [epoch: 21.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15444366744667004		[learning rate: 0.0063713]
	Learning Rate: 0.00637134
	LOSS [training: 0.15444366744667004 | validation: 0.16541570994386653]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14094912113196015		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.14094912113196015 | validation: 0.15889539613934306]
	TIME [epoch: 21.4 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17044837227174478		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.17044837227174478 | validation: 0.18679134845194556]
	TIME [epoch: 21.2 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17966568840069583		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.17966568840069583 | validation: 0.1792051138323111]
	TIME [epoch: 21.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16332904127156206		[learning rate: 0.0062595]
	Learning Rate: 0.00625948
	LOSS [training: 0.16332904127156206 | validation: 0.17893829772638103]
	TIME [epoch: 21.2 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16462326573466904		[learning rate: 0.0062318]
	Learning Rate: 0.00623183
	LOSS [training: 0.16462326573466904 | validation: 0.1945029324974601]
	TIME [epoch: 21.2 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17042216690231693		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.17042216690231693 | validation: 0.17282736801146367]
	TIME [epoch: 21.1 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16677238040725953		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.16677238040725953 | validation: 0.18536667509307722]
	TIME [epoch: 21.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16968075421072917		[learning rate: 0.0061496]
	Learning Rate: 0.00614959
	LOSS [training: 0.16968075421072917 | validation: 0.18137376671262576]
	TIME [epoch: 21.1 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15250356466849854		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.15250356466849854 | validation: 0.17057876374780415]
	TIME [epoch: 21.2 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14729924936287384		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.14729924936287384 | validation: 0.19774396599841584]
	TIME [epoch: 21.1 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15962114058977683		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.15962114058977683 | validation: 0.16358439156197355]
	TIME [epoch: 21.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15138026713276923		[learning rate: 0.0060416]
	Learning Rate: 0.00604163
	LOSS [training: 0.15138026713276923 | validation: 0.17768131620025637]
	TIME [epoch: 21.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16315508510330395		[learning rate: 0.0060149]
	Learning Rate: 0.00601493
	LOSS [training: 0.16315508510330395 | validation: 0.17061163484750078]
	TIME [epoch: 21.2 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17525449788229924		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.17525449788229924 | validation: 0.17773413026239604]
	TIME [epoch: 21.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17965978184501852		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.17965978184501852 | validation: 0.19298749830063078]
	TIME [epoch: 21.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1664273844716034		[learning rate: 0.0059356]
	Learning Rate: 0.00593556
	LOSS [training: 0.1664273844716034 | validation: 0.22825258002118787]
	TIME [epoch: 21.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18588364802782692		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.18588364802782692 | validation: 0.16063549241259015]
	TIME [epoch: 21.2 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14724233678916226		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.14724233678916226 | validation: 0.17535719205339242]
	TIME [epoch: 21.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17082177225368822		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.17082177225368822 | validation: 0.17491149170440487]
	TIME [epoch: 21.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15594963949073506		[learning rate: 0.0058314]
	Learning Rate: 0.00583135
	LOSS [training: 0.15594963949073506 | validation: 0.21380826236442863]
	TIME [epoch: 21.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18903191424260643		[learning rate: 0.0058056]
	Learning Rate: 0.00580559
	LOSS [training: 0.18903191424260643 | validation: 0.15144554631328047]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1708736863498594		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.1708736863498594 | validation: 0.15687044263990066]
	TIME [epoch: 21.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1548145336003386		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.1548145336003386 | validation: 0.15649079034118415]
	TIME [epoch: 21.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15490695648513014		[learning rate: 0.005729]
	Learning Rate: 0.00572897
	LOSS [training: 0.15490695648513014 | validation: 0.18081419389570788]
	TIME [epoch: 21.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1695871503295335		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.1695871503295335 | validation: 0.1615338438612907]
	TIME [epoch: 21.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1878389464018017		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1878389464018017 | validation: 0.16748330918006524]
	TIME [epoch: 21.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16744050330820273		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.16744050330820273 | validation: 0.1848630610488096]
	TIME [epoch: 21.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.168335456389712		[learning rate: 0.0056284]
	Learning Rate: 0.0056284
	LOSS [training: 0.168335456389712 | validation: 0.17438648578501126]
	TIME [epoch: 21.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17256662275471776		[learning rate: 0.0056035]
	Learning Rate: 0.00560353
	LOSS [training: 0.17256662275471776 | validation: 0.16366501520872942]
	TIME [epoch: 21.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15657898545404547		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.15657898545404547 | validation: 0.194941615934861]
	TIME [epoch: 21.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1554866515894433		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.1554866515894433 | validation: 0.17252539728953523]
	TIME [epoch: 21.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1853721957631709		[learning rate: 0.0055296]
	Learning Rate: 0.00552958
	LOSS [training: 0.1853721957631709 | validation: 0.16741381503085365]
	TIME [epoch: 21.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16945787607430268		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.16945787607430268 | validation: 0.17793098742852295]
	TIME [epoch: 21.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18113064372352689		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.18113064372352689 | validation: 0.17006658172141242]
	TIME [epoch: 21.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1488160781184356		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.1488160781184356 | validation: 0.21234883541905725]
	TIME [epoch: 21.4 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16915913390017723		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.16915913390017723 | validation: 0.15881094867854226]
	TIME [epoch: 21.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16468963895307492		[learning rate: 0.0054085]
	Learning Rate: 0.0054085
	LOSS [training: 0.16468963895307492 | validation: 0.15203986459230664]
	TIME [epoch: 21.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16088548089822644		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.16088548089822644 | validation: 0.18435299532122648]
	TIME [epoch: 21.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15937807495470366		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.15937807495470366 | validation: 0.16519109891549763]
	TIME [epoch: 21.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15705994539434717		[learning rate: 0.0053371]
	Learning Rate: 0.00533713
	LOSS [training: 0.15705994539434717 | validation: 0.17941400467539012]
	TIME [epoch: 21.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1701863369630149		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.1701863369630149 | validation: 0.20258571015000756]
	TIME [epoch: 21.4 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15610260492262926		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.15610260492262926 | validation: 0.1667987130821714]
	TIME [epoch: 21.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16223428649171753		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.16223428649171753 | validation: 0.17632593675803285]
	TIME [epoch: 21.4 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15656834341417403		[learning rate: 0.0052434]
	Learning Rate: 0.00524343
	LOSS [training: 0.15656834341417403 | validation: 0.16682154588910328]
	TIME [epoch: 21.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17294422386454517		[learning rate: 0.0052203]
	Learning Rate: 0.00522026
	LOSS [training: 0.17294422386454517 | validation: 0.16683322336301942]
	TIME [epoch: 21.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16973740928229997		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.16973740928229997 | validation: 0.19311670976214615]
	TIME [epoch: 21.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17795265069736496		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.17795265069736496 | validation: 0.1959117033194381]
	TIME [epoch: 21.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2094530862021184		[learning rate: 0.0051514]
	Learning Rate: 0.00515137
	LOSS [training: 0.2094530862021184 | validation: 0.153761145624656]
	TIME [epoch: 21.2 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16171908971022267		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.16171908971022267 | validation: 0.15066224833310135]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1716756709112391		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.1716756709112391 | validation: 0.15397923668107766]
	TIME [epoch: 21.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17484021034122793		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.17484021034122793 | validation: 0.1571034197080812]
	TIME [epoch: 21.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16499959429979674		[learning rate: 0.0050609]
	Learning Rate: 0.00506093
	LOSS [training: 0.16499959429979674 | validation: 0.16036259108070836]
	TIME [epoch: 21.2 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13989282840602016		[learning rate: 0.0050386]
	Learning Rate: 0.00503857
	LOSS [training: 0.13989282840602016 | validation: 0.17734342897069819]
	TIME [epoch: 21.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1489281227103614		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.1489281227103614 | validation: 0.16321333955699588]
	TIME [epoch: 21.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1568525540275302		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.1568525540275302 | validation: 0.15379990878903213]
	TIME [epoch: 21.2 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18901014265743038		[learning rate: 0.0049721]
	Learning Rate: 0.00497208
	LOSS [training: 0.18901014265743038 | validation: 0.1522691967572045]
	TIME [epoch: 21.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20144121707400223		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.20144121707400223 | validation: 0.16187918699160195]
	TIME [epoch: 21.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1678432466598873		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.1678432466598873 | validation: 0.14589341255779897]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16075346819170291		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.16075346819170291 | validation: 0.1834392446196038]
	TIME [epoch: 21.2 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1862797026983615		[learning rate: 0.0048848]
	Learning Rate: 0.00488479
	LOSS [training: 0.1862797026983615 | validation: 0.1825283420498519]
	TIME [epoch: 21.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15374670784051353		[learning rate: 0.0048632]
	Learning Rate: 0.00486321
	LOSS [training: 0.15374670784051353 | validation: 0.16464556539616668]
	TIME [epoch: 21.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14627822143362898		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.14627822143362898 | validation: 0.16299723156574739]
	TIME [epoch: 21.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15278773884725563		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.15278773884725563 | validation: 0.1704778789073608]
	TIME [epoch: 21.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17540694668227733		[learning rate: 0.004799]
	Learning Rate: 0.00479903
	LOSS [training: 0.17540694668227733 | validation: 0.16095781839067005]
	TIME [epoch: 21.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1502574029100015		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.1502574029100015 | validation: 0.16375896646325372]
	TIME [epoch: 21.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1583651072940966		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1583651072940966 | validation: 0.1868251320399382]
	TIME [epoch: 21.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16209912139467672		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.16209912139467672 | validation: 0.1694829348839596]
	TIME [epoch: 21.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16080520759608766		[learning rate: 0.0047148]
	Learning Rate: 0.00471478
	LOSS [training: 0.16080520759608766 | validation: 0.15459999563428348]
	TIME [epoch: 21.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1421262199879004		[learning rate: 0.0046939]
	Learning Rate: 0.00469395
	LOSS [training: 0.1421262199879004 | validation: 0.1834903640581199]
	TIME [epoch: 21.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1370998431793182		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.1370998431793182 | validation: 0.18215540169128433]
	TIME [epoch: 21.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16882094480200588		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.16882094480200588 | validation: 0.15353691384717724]
	TIME [epoch: 21.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1565899419202516		[learning rate: 0.004632]
	Learning Rate: 0.00463201
	LOSS [training: 0.1565899419202516 | validation: 0.17463568706076202]
	TIME [epoch: 21.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16530247320934338		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.16530247320934338 | validation: 0.151073974173866]
	TIME [epoch: 21.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15908203165002852		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.15908203165002852 | validation: 0.14838593418050772]
	TIME [epoch: 21.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16554696786619577		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.16554696786619577 | validation: 0.16392503720463839]
	TIME [epoch: 21.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18739119716231983		[learning rate: 0.0045507]
	Learning Rate: 0.00455069
	LOSS [training: 0.18739119716231983 | validation: 0.14835040038624997]
	TIME [epoch: 21.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16816037220160224		[learning rate: 0.0045306]
	Learning Rate: 0.00453058
	LOSS [training: 0.16816037220160224 | validation: 0.15503155265200544]
	TIME [epoch: 21.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15071087648871068		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.15071087648871068 | validation: 0.15959052441009178]
	TIME [epoch: 21.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14132385134208614		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.14132385134208614 | validation: 0.15465352340704538]
	TIME [epoch: 21.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14175097770190243		[learning rate: 0.0044708]
	Learning Rate: 0.00447079
	LOSS [training: 0.14175097770190243 | validation: 0.18324030336724037]
	TIME [epoch: 21.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14904786743604767		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.14904786743604767 | validation: 0.16981677452752905]
	TIME [epoch: 21.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15485443753295264		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.15485443753295264 | validation: 0.15831354723604654]
	TIME [epoch: 21.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17288373031519472		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.17288373031519472 | validation: 0.16640168055870094]
	TIME [epoch: 21.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21660718244992583		[learning rate: 0.0043923]
	Learning Rate: 0.0043923
	LOSS [training: 0.21660718244992583 | validation: 0.15340933372433607]
	TIME [epoch: 21.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15211489190151176		[learning rate: 0.0043729]
	Learning Rate: 0.0043729
	LOSS [training: 0.15211489190151176 | validation: 0.1599882455581195]
	TIME [epoch: 21.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16176115845266917		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.16176115845266917 | validation: 0.1729989826127135]
	TIME [epoch: 21.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15556911434364765		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.15556911434364765 | validation: 0.16418493379722152]
	TIME [epoch: 21.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14966252473954417		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.14966252473954417 | validation: 0.17035277320577144]
	TIME [epoch: 21.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1529542191074745		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.1529542191074745 | validation: 0.16302112383871742]
	TIME [epoch: 21.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13776004303999542		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.13776004303999542 | validation: 0.1581668932141675]
	TIME [epoch: 21.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15065998889064872		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.15065998889064872 | validation: 0.1603268251264311]
	TIME [epoch: 21.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1743413154513449		[learning rate: 0.0042394]
	Learning Rate: 0.00423943
	LOSS [training: 0.1743413154513449 | validation: 0.14809166486554895]
	TIME [epoch: 21.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1622445044561821		[learning rate: 0.0042207]
	Learning Rate: 0.0042207
	LOSS [training: 0.1622445044561821 | validation: 0.14871158480820515]
	TIME [epoch: 21.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1440409368751358		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.1440409368751358 | validation: 0.16529206378368025]
	TIME [epoch: 21.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14090130712191967		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.14090130712191967 | validation: 0.1632804660364112]
	TIME [epoch: 21.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14674625290256915		[learning rate: 0.004165]
	Learning Rate: 0.004165
	LOSS [training: 0.14674625290256915 | validation: 0.17206732528216326]
	TIME [epoch: 21.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15281049081633696		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.15281049081633696 | validation: 0.1693326373972844]
	TIME [epoch: 21.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15197029259727918		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.15197029259727918 | validation: 0.16928234378131793]
	TIME [epoch: 21.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1381871842128521		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.1381871842128521 | validation: 0.1777488994813368]
	TIME [epoch: 21.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16390167811705045		[learning rate: 0.0040919]
	Learning Rate: 0.00409188
	LOSS [training: 0.16390167811705045 | validation: 0.16403899963736165]
	TIME [epoch: 21.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1593521356727064		[learning rate: 0.0040738]
	Learning Rate: 0.0040738
	LOSS [training: 0.1593521356727064 | validation: 0.1664904336293299]
	TIME [epoch: 21.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15289676815599154		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.15289676815599154 | validation: 0.14725581851840783]
	TIME [epoch: 21.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1667902450363549		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.1667902450363549 | validation: 0.15525766822245182]
	TIME [epoch: 21.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15206886024599028		[learning rate: 0.00402]
	Learning Rate: 0.00402004
	LOSS [training: 0.15206886024599028 | validation: 0.14560798885270018]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16650135505232733		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 0.16650135505232733 | validation: 0.17468202673892805]
	TIME [epoch: 21.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14688058133411572		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.14688058133411572 | validation: 0.1514343261301865]
	TIME [epoch: 21.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14659937868781825		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.14659937868781825 | validation: 0.15522023892096687]
	TIME [epoch: 21.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18194848805067929		[learning rate: 0.0039495]
	Learning Rate: 0.00394947
	LOSS [training: 0.18194848805067929 | validation: 0.15308423437568763]
	TIME [epoch: 21.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15414668787022218		[learning rate: 0.003932]
	Learning Rate: 0.00393202
	LOSS [training: 0.15414668787022218 | validation: 0.1782265501994528]
	TIME [epoch: 21.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1587142336943467		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.1587142336943467 | validation: 0.14771298142366898]
	TIME [epoch: 21.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1484270542035595		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.1484270542035595 | validation: 0.15997870871261938]
	TIME [epoch: 21.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1563928775980091		[learning rate: 0.0038801]
	Learning Rate: 0.00388013
	LOSS [training: 0.1563928775980091 | validation: 0.14960347249868058]
	TIME [epoch: 21.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1476433641643065		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 0.1476433641643065 | validation: 0.166149508227435]
	TIME [epoch: 21.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14116738265701415		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.14116738265701415 | validation: 0.15661136100644496]
	TIME [epoch: 21.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17887144200506255		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.17887144200506255 | validation: 0.16833762118321585]
	TIME [epoch: 21.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16861256769984495		[learning rate: 0.003812]
	Learning Rate: 0.00381201
	LOSS [training: 0.16861256769984495 | validation: 0.1501024960665682]
	TIME [epoch: 21.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15996287224364136		[learning rate: 0.0037952]
	Learning Rate: 0.00379517
	LOSS [training: 0.15996287224364136 | validation: 0.16445453711304922]
	TIME [epoch: 21.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1558241316678713		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.1558241316678713 | validation: 0.15624730596166717]
	TIME [epoch: 21.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14393201921416618		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.14393201921416618 | validation: 0.16716220610021995]
	TIME [epoch: 21.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1838758527359843		[learning rate: 0.0037451]
	Learning Rate: 0.00374508
	LOSS [training: 0.1838758527359843 | validation: 0.1504161646448979]
	TIME [epoch: 21.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1357074891389139		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 0.1357074891389139 | validation: 0.153523122854969]
	TIME [epoch: 21.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14134465931182244		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.14134465931182244 | validation: 0.15243901179719474]
	TIME [epoch: 21.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1851051166196358		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.1851051166196358 | validation: 0.14685280086904934]
	TIME [epoch: 21.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16605170175655545		[learning rate: 0.0036793]
	Learning Rate: 0.00367933
	LOSS [training: 0.16605170175655545 | validation: 0.1661782009232621]
	TIME [epoch: 21.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16118832922263465		[learning rate: 0.0036631]
	Learning Rate: 0.00366308
	LOSS [training: 0.16118832922263465 | validation: 0.14610526137446703]
	TIME [epoch: 21.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1571073249075028		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.1571073249075028 | validation: 0.14818295374817678]
	TIME [epoch: 21.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16593363847855447		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.16593363847855447 | validation: 0.14351705902804365]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1548436290799172		[learning rate: 0.0036147]
	Learning Rate: 0.00361474
	LOSS [training: 0.1548436290799172 | validation: 0.1608580616667792]
	TIME [epoch: 21.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16604072920214366		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 0.16604072920214366 | validation: 0.1569871074611213]
	TIME [epoch: 21.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1709630946376604		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.1709630946376604 | validation: 0.15912310720069084]
	TIME [epoch: 21.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13825627745112423		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.13825627745112423 | validation: 0.1559545341681096]
	TIME [epoch: 21.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14516444423232552		[learning rate: 0.0035513]
	Learning Rate: 0.00355128
	LOSS [training: 0.14516444423232552 | validation: 0.14770427416689993]
	TIME [epoch: 21.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1508093848764523		[learning rate: 0.0035356]
	Learning Rate: 0.00353559
	LOSS [training: 0.1508093848764523 | validation: 0.18998875799588655]
	TIME [epoch: 21.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16716659424367894		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.16716659424367894 | validation: 0.15055376532949785]
	TIME [epoch: 21.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14301283421075006		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.14301283421075006 | validation: 0.15871260707774695]
	TIME [epoch: 21.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15052270504403262		[learning rate: 0.0034889]
	Learning Rate: 0.00348893
	LOSS [training: 0.15052270504403262 | validation: 0.15376864394561285]
	TIME [epoch: 21.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13517446810040196		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 0.13517446810040196 | validation: 0.1477104943416246]
	TIME [epoch: 21.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14900051725619848		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.14900051725619848 | validation: 0.15555044943804525]
	TIME [epoch: 21.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15680132011408388		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.15680132011408388 | validation: 0.1501293207847991]
	TIME [epoch: 21.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15930686566585667		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.15930686566585667 | validation: 0.14758944172604363]
	TIME [epoch: 21.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1622693253286308		[learning rate: 0.0034125]
	Learning Rate: 0.00341253
	LOSS [training: 0.1622693253286308 | validation: 0.1546448185899331]
	TIME [epoch: 21.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1526321817582957		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.1526321817582957 | validation: 0.14450867669279097]
	TIME [epoch: 21.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15385718162566636		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.15385718162566636 | validation: 0.15264570942821326]
	TIME [epoch: 21.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12668835981534335		[learning rate: 0.0033675]
	Learning Rate: 0.0033675
	LOSS [training: 0.12668835981534335 | validation: 0.1529658311706341]
	TIME [epoch: 21.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14550940972017268		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 0.14550940972017268 | validation: 0.14370290314844678]
	TIME [epoch: 21.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14273048181910286		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.14273048181910286 | validation: 0.15416323118436626]
	TIME [epoch: 21.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15360537224203694		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.15360537224203694 | validation: 0.15457319075125406]
	TIME [epoch: 21.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1460088302512931		[learning rate: 0.0033084]
	Learning Rate: 0.00330838
	LOSS [training: 0.1460088302512931 | validation: 0.1636987661881024]
	TIME [epoch: 21.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14298168501275935		[learning rate: 0.0032938]
	Learning Rate: 0.00329376
	LOSS [training: 0.14298168501275935 | validation: 0.14057424699829293]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15273420771821064		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.15273420771821064 | validation: 0.14854180263619074]
	TIME [epoch: 21.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14782113498521318		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.14782113498521318 | validation: 0.16178461646842612]
	TIME [epoch: 21.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15039887480689035		[learning rate: 0.0032503]
	Learning Rate: 0.0032503
	LOSS [training: 0.15039887480689035 | validation: 0.14254743842891618]
	TIME [epoch: 21.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16635422941043115		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 0.16635422941043115 | validation: 0.16529844936266994]
	TIME [epoch: 21.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16190364500052604		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.16190364500052604 | validation: 0.15123622489888616]
	TIME [epoch: 21.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15200879499196635		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.15200879499196635 | validation: 0.13768300491922267]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1443859518626339		[learning rate: 0.0031932]
	Learning Rate: 0.00319323
	LOSS [training: 0.1443859518626339 | validation: 0.14054022378058098]
	TIME [epoch: 21.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13402229759233508		[learning rate: 0.0031791]
	Learning Rate: 0.00317913
	LOSS [training: 0.13402229759233508 | validation: 0.1653309171851526]
	TIME [epoch: 21.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14749740840107015		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.14749740840107015 | validation: 0.14438018281697626]
	TIME [epoch: 21.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14397956452695476		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.14397956452695476 | validation: 0.15018386830019728]
	TIME [epoch: 21.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16081626772710098		[learning rate: 0.0031372]
	Learning Rate: 0.00313717
	LOSS [training: 0.16081626772710098 | validation: 0.16695995280891884]
	TIME [epoch: 21.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1517647212910051		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 0.1517647212910051 | validation: 0.16497864686490224]
	TIME [epoch: 21.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16427898234615962		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.16427898234615962 | validation: 0.15418308600678612]
	TIME [epoch: 21.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15869256134247034		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.15869256134247034 | validation: 0.1453012510881628]
	TIME [epoch: 21.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16710317662981947		[learning rate: 0.0030821]
	Learning Rate: 0.0030821
	LOSS [training: 0.16710317662981947 | validation: 0.15273677797359]
	TIME [epoch: 21.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.160034765490274		[learning rate: 0.0030685]
	Learning Rate: 0.00306848
	LOSS [training: 0.160034765490274 | validation: 0.16248138799574197]
	TIME [epoch: 21.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1424874475233085		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.1424874475233085 | validation: 0.14429039607947555]
	TIME [epoch: 21.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1407135515429764		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1407135515429764 | validation: 0.1544808130919681]
	TIME [epoch: 21.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14967542798481553		[learning rate: 0.003028]
	Learning Rate: 0.00302799
	LOSS [training: 0.14967542798481553 | validation: 0.16026400647307734]
	TIME [epoch: 21.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13980609347122588		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 0.13980609347122588 | validation: 0.15649909715025487]
	TIME [epoch: 21.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14813663279625738		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.14813663279625738 | validation: 0.16078388987291173]
	TIME [epoch: 21.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14217577085688995		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.14217577085688995 | validation: 0.14904790342138297]
	TIME [epoch: 21.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14202057368964321		[learning rate: 0.0029748]
	Learning Rate: 0.00297483
	LOSS [training: 0.14202057368964321 | validation: 0.14463888321161594]
	TIME [epoch: 21.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15228236743123053		[learning rate: 0.0029617]
	Learning Rate: 0.00296168
	LOSS [training: 0.15228236743123053 | validation: 0.14253663803361993]
	TIME [epoch: 21.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15626150373162978		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.15626150373162978 | validation: 0.1464916694762375]
	TIME [epoch: 21.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1488305249476478		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.1488305249476478 | validation: 0.14580902254732345]
	TIME [epoch: 21.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1505823525124527		[learning rate: 0.0029226]
	Learning Rate: 0.0029226
	LOSS [training: 0.1505823525124527 | validation: 0.1447077361777991]
	TIME [epoch: 21.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15423851036768937		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 0.15423851036768937 | validation: 0.14315599081403865]
	TIME [epoch: 21.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14204206477771794		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.14204206477771794 | validation: 0.1516984993416374]
	TIME [epoch: 21.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14973149133980623		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.14973149133980623 | validation: 0.1521703091861094]
	TIME [epoch: 21.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14698341364906625		[learning rate: 0.0028713]
	Learning Rate: 0.00287129
	LOSS [training: 0.14698341364906625 | validation: 0.15762998831486832]
	TIME [epoch: 21.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1393666859515557		[learning rate: 0.0028586]
	Learning Rate: 0.0028586
	LOSS [training: 0.1393666859515557 | validation: 0.1541558853879365]
	TIME [epoch: 21.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14455981349008548		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.14455981349008548 | validation: 0.1470841273130757]
	TIME [epoch: 21.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12675500570822662		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.12675500570822662 | validation: 0.14300906911950298]
	TIME [epoch: 21.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15895827215198738		[learning rate: 0.0028209]
	Learning Rate: 0.00282088
	LOSS [training: 0.15895827215198738 | validation: 0.14809858498403453]
	TIME [epoch: 21.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14873701031391146		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 0.14873701031391146 | validation: 0.1531085027109008]
	TIME [epoch: 21.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14253526420297774		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.14253526420297774 | validation: 0.1533026035382803]
	TIME [epoch: 21.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13446174131069197		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.13446174131069197 | validation: 0.14739904046361396]
	TIME [epoch: 21.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1406964684115238		[learning rate: 0.0027714]
	Learning Rate: 0.00277136
	LOSS [training: 0.1406964684115238 | validation: 0.1473785589666121]
	TIME [epoch: 21.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14251588217091435		[learning rate: 0.0027591]
	Learning Rate: 0.00275911
	LOSS [training: 0.14251588217091435 | validation: 0.16616004018538963]
	TIME [epoch: 21.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18303928912996753		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.18303928912996753 | validation: 0.1405384610533563]
	TIME [epoch: 21.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1455468342315868		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1455468342315868 | validation: 0.14832276111660161]
	TIME [epoch: 21.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1430742440840806		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.1430742440840806 | validation: 0.13850541901670424]
	TIME [epoch: 21.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13653902783470376		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 0.13653902783470376 | validation: 0.13351031976024347]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14256113134913975		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.14256113134913975 | validation: 0.16319388186447084]
	TIME [epoch: 21.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13783792226245026		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.13783792226245026 | validation: 0.15251254536420736]
	TIME [epoch: 21.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14208129528686322		[learning rate: 0.0026749]
	Learning Rate: 0.0026749
	LOSS [training: 0.14208129528686322 | validation: 0.14678486933721158]
	TIME [epoch: 21.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13564025585544945		[learning rate: 0.0026631]
	Learning Rate: 0.00266308
	LOSS [training: 0.13564025585544945 | validation: 0.1481133895118114]
	TIME [epoch: 21.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13163414461687128		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.13163414461687128 | validation: 0.1500098046265121]
	TIME [epoch: 21.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15046868052521128		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.15046868052521128 | validation: 0.14275818880767469]
	TIME [epoch: 21.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13286606753389757		[learning rate: 0.0026279]
	Learning Rate: 0.00262794
	LOSS [training: 0.13286606753389757 | validation: 0.16757157999288555]
	TIME [epoch: 21.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1623790036547012		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 0.1623790036547012 | validation: 0.1456121041435813]
	TIME [epoch: 21.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1549361331974871		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.1549361331974871 | validation: 0.14428772403575665]
	TIME [epoch: 21.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.143015261429578		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.143015261429578 | validation: 0.16384500333783297]
	TIME [epoch: 21.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1448693174921109		[learning rate: 0.0025818]
	Learning Rate: 0.0025818
	LOSS [training: 0.1448693174921109 | validation: 0.14439567291362004]
	TIME [epoch: 21.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17427822841197646		[learning rate: 0.0025704]
	Learning Rate: 0.0025704
	LOSS [training: 0.17427822841197646 | validation: 0.13947221975091126]
	TIME [epoch: 21.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14592824850024386		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.14592824850024386 | validation: 0.14459259269615068]
	TIME [epoch: 21.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13409183695304253		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.13409183695304253 | validation: 0.14182120023697464]
	TIME [epoch: 21.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1358013381736741		[learning rate: 0.0025365]
	Learning Rate: 0.00253648
	LOSS [training: 0.1358013381736741 | validation: 0.1524531402734839]
	TIME [epoch: 21.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1561418083101831		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 0.1561418083101831 | validation: 0.14780692879880988]
	TIME [epoch: 21.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14913537020230833		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.14913537020230833 | validation: 0.14287433680098047]
	TIME [epoch: 21.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13995660871589358		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.13995660871589358 | validation: 0.15687283765085427]
	TIME [epoch: 21.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15087962231329105		[learning rate: 0.0024919]
	Learning Rate: 0.00249194
	LOSS [training: 0.15087962231329105 | validation: 0.13868768535266415]
	TIME [epoch: 21.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1372983153888206		[learning rate: 0.0024809]
	Learning Rate: 0.00248093
	LOSS [training: 0.1372983153888206 | validation: 0.15812217399732528]
	TIME [epoch: 21.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13869633677965676		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.13869633677965676 | validation: 0.15241213858162522]
	TIME [epoch: 21.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12872232362149588		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.12872232362149588 | validation: 0.1544227148608708]
	TIME [epoch: 21.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13937699347470428		[learning rate: 0.0024482]
	Learning Rate: 0.0024482
	LOSS [training: 0.13937699347470428 | validation: 0.1359411374562933]
	TIME [epoch: 21.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14635235242603975		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 0.14635235242603975 | validation: 0.1667780659732073]
	TIME [epoch: 21.1 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14701193341292582		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.14701193341292582 | validation: 0.15566715947289983]
	TIME [epoch: 21.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15782214340996742		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.15782214340996742 | validation: 0.14115807364425234]
	TIME [epoch: 21.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14201699511828814		[learning rate: 0.0024052]
	Learning Rate: 0.00240521
	LOSS [training: 0.14201699511828814 | validation: 0.1388268448856515]
	TIME [epoch: 21.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14926897489953134		[learning rate: 0.0023946]
	Learning Rate: 0.00239459
	LOSS [training: 0.14926897489953134 | validation: 0.14188770350213262]
	TIME [epoch: 21.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1801413521059016		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.1801413521059016 | validation: 0.1558430432939196]
	TIME [epoch: 21.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1351685599305717		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.1351685599305717 | validation: 0.1373029152438053]
	TIME [epoch: 21.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16095730526484475		[learning rate: 0.002363]
	Learning Rate: 0.00236299
	LOSS [training: 0.16095730526484475 | validation: 0.13549853946607585]
	TIME [epoch: 21.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16991354624465285		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 0.16991354624465285 | validation: 0.1582411988711128]
	TIME [epoch: 21.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14904110495663578		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.14904110495663578 | validation: 0.1469217014175228]
	TIME [epoch: 21.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1342953916121222		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.1342953916121222 | validation: 0.1440534664138854]
	TIME [epoch: 21.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14014020430980872		[learning rate: 0.0023215]
	Learning Rate: 0.0023215
	LOSS [training: 0.14014020430980872 | validation: 0.13807445066383336]
	TIME [epoch: 21.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14132905151810876		[learning rate: 0.0023112]
	Learning Rate: 0.00231125
	LOSS [training: 0.14132905151810876 | validation: 0.17253936542707052]
	TIME [epoch: 21.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13968391991108978		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.13968391991108978 | validation: 0.15147432425254104]
	TIME [epoch: 21.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13924735841622377		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.13924735841622377 | validation: 0.15275156863119227]
	TIME [epoch: 21.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13538454568361186		[learning rate: 0.0022807]
	Learning Rate: 0.00228075
	LOSS [training: 0.13538454568361186 | validation: 0.1509833248769699]
	TIME [epoch: 21.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18179511077194838		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 0.18179511077194838 | validation: 0.1527087143350652]
	TIME [epoch: 21.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16292694884677908		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.16292694884677908 | validation: 0.15851739019563965]
	TIME [epoch: 21.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13510577520902198		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.13510577520902198 | validation: 0.13912176034999202]
	TIME [epoch: 21.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14425489703546274		[learning rate: 0.0022407]
	Learning Rate: 0.0022407
	LOSS [training: 0.14425489703546274 | validation: 0.1468034698256419]
	TIME [epoch: 21.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13508404764575568		[learning rate: 0.0022308]
	Learning Rate: 0.0022308
	LOSS [training: 0.13508404764575568 | validation: 0.14795122741504513]
	TIME [epoch: 21.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1439122571450359		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.1439122571450359 | validation: 0.14001719867816334]
	TIME [epoch: 21.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15015248521028252		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.15015248521028252 | validation: 0.13131518436603856]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1270885357615258		[learning rate: 0.0022014]
	Learning Rate: 0.00220137
	LOSS [training: 0.1270885357615258 | validation: 0.15717617636202613]
	TIME [epoch: 21.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13585762852597627		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 0.13585762852597627 | validation: 0.14924948979885658]
	TIME [epoch: 21.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13471548480962067		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.13471548480962067 | validation: 0.15563969663706473]
	TIME [epoch: 21.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15036454148531483		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.15036454148531483 | validation: 0.15681075332931918]
	TIME [epoch: 21.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13843680060088698		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.13843680060088698 | validation: 0.1484869331138691]
	TIME [epoch: 21.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1334346922724328		[learning rate: 0.0021532]
	Learning Rate: 0.00215316
	LOSS [training: 0.1334346922724328 | validation: 0.14218644761435237]
	TIME [epoch: 21.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1518586929155401		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.1518586929155401 | validation: 0.1417372570394517]
	TIME [epoch: 21.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15373303327200669		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.15373303327200669 | validation: 0.14844382425722177]
	TIME [epoch: 21.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1503498003498951		[learning rate: 0.0021247]
	Learning Rate: 0.00212475
	LOSS [training: 0.1503498003498951 | validation: 0.1454358899217555]
	TIME [epoch: 21.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13807734892644258		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 0.13807734892644258 | validation: 0.13892048855778408]
	TIME [epoch: 21.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15258918629347762		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.15258918629347762 | validation: 0.1481680682869765]
	TIME [epoch: 21.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14431450934921342		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.14431450934921342 | validation: 0.14753047202809746]
	TIME [epoch: 21.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1385271239649838		[learning rate: 0.0020874]
	Learning Rate: 0.00208745
	LOSS [training: 0.1385271239649838 | validation: 0.1523262927712631]
	TIME [epoch: 21.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1431836737935946		[learning rate: 0.0020782]
	Learning Rate: 0.00207822
	LOSS [training: 0.1431836737935946 | validation: 0.15506704511896796]
	TIME [epoch: 21.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14949781096284134		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.14949781096284134 | validation: 0.14905600245574838]
	TIME [epoch: 21.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1450716278208109		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.1450716278208109 | validation: 0.139941712043562]
	TIME [epoch: 21.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17260444826829735		[learning rate: 0.0020508]
	Learning Rate: 0.0020508
	LOSS [training: 0.17260444826829735 | validation: 0.13926157300017353]
	TIME [epoch: 21.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14301614609790467		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 0.14301614609790467 | validation: 0.1463466068555357]
	TIME [epoch: 21.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16744761202665964		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.16744761202665964 | validation: 0.141254882434792]
	TIME [epoch: 21.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14677046614226413		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.14677046614226413 | validation: 0.14594303977804196]
	TIME [epoch: 21.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14630004272842773		[learning rate: 0.0020148]
	Learning Rate: 0.00201479
	LOSS [training: 0.14630004272842773 | validation: 0.14962595882550708]
	TIME [epoch: 21.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16654776320822184		[learning rate: 0.0020059]
	Learning Rate: 0.00200589
	LOSS [training: 0.16654776320822184 | validation: 0.13969474832064988]
	TIME [epoch: 21.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14103828381843914		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.14103828381843914 | validation: 0.15060781991912117]
	TIME [epoch: 21.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14877523823537714		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.14877523823537714 | validation: 0.1517045373867416]
	TIME [epoch: 21.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14143225713286417		[learning rate: 0.0019794]
	Learning Rate: 0.00197942
	LOSS [training: 0.14143225713286417 | validation: 0.14471142031161013]
	TIME [epoch: 21.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1315700556337558		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 0.1315700556337558 | validation: 0.14692626243327464]
	TIME [epoch: 21.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14680920589942267		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.14680920589942267 | validation: 0.14531599917767526]
	TIME [epoch: 21.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13261867046092304		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.13261867046092304 | validation: 0.13810782241538325]
	TIME [epoch: 21.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15536740361690185		[learning rate: 0.0019447]
	Learning Rate: 0.00194467
	LOSS [training: 0.15536740361690185 | validation: 0.14500098884353876]
	TIME [epoch: 21.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14312882754134248		[learning rate: 0.0019361]
	Learning Rate: 0.00193608
	LOSS [training: 0.14312882754134248 | validation: 0.15180680913842234]
	TIME [epoch: 21.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15623305014134137		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.15623305014134137 | validation: 0.1464369853756301]
	TIME [epoch: 21.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13750218546342402		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.13750218546342402 | validation: 0.1569590784572765]
	TIME [epoch: 21.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14714534257329331		[learning rate: 0.0019105]
	Learning Rate: 0.00191053
	LOSS [training: 0.14714534257329331 | validation: 0.14291109099058313]
	TIME [epoch: 21.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17956179797717475		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 0.17956179797717475 | validation: 0.14232244168104707]
	TIME [epoch: 21.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13841486902881253		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.13841486902881253 | validation: 0.14533425779163578]
	TIME [epoch: 21.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15236712262656124		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.15236712262656124 | validation: 0.15786028403480237]
	TIME [epoch: 21.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13807261515399202		[learning rate: 0.001877]
	Learning Rate: 0.00187699
	LOSS [training: 0.13807261515399202 | validation: 0.1330802003455128]
	TIME [epoch: 21.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1411685161322276		[learning rate: 0.0018687]
	Learning Rate: 0.0018687
	LOSS [training: 0.1411685161322276 | validation: 0.14626369043577903]
	TIME [epoch: 21.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1548062680289956		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.1548062680289956 | validation: 0.1491804150845798]
	TIME [epoch: 21.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13943222566450256		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.13943222566450256 | validation: 0.14167964683163856]
	TIME [epoch: 21.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1477320903392561		[learning rate: 0.001844]
	Learning Rate: 0.00184404
	LOSS [training: 0.1477320903392561 | validation: 0.15164599058384384]
	TIME [epoch: 21.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14106232391916484		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 0.14106232391916484 | validation: 0.14100607068831741]
	TIME [epoch: 21.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1535324258189415		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.1535324258189415 | validation: 0.15531299764973322]
	TIME [epoch: 21.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14693892881975076		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.14693892881975076 | validation: 0.14896036262663484]
	TIME [epoch: 21.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13584382231327732		[learning rate: 0.0018117]
	Learning Rate: 0.00181166
	LOSS [training: 0.13584382231327732 | validation: 0.13667851403404302]
	TIME [epoch: 21.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13664402630549044		[learning rate: 0.0018037]
	Learning Rate: 0.00180366
	LOSS [training: 0.13664402630549044 | validation: 0.14233536631400728]
	TIME [epoch: 21.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1498408326439657		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.1498408326439657 | validation: 0.14111732333381607]
	TIME [epoch: 21.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14754562785325195		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.14754562785325195 | validation: 0.1552281642818637]
	TIME [epoch: 21.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14083458657642225		[learning rate: 0.0017799]
	Learning Rate: 0.00177985
	LOSS [training: 0.14083458657642225 | validation: 0.15072773858067548]
	TIME [epoch: 21.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13460655202786778		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 0.13460655202786778 | validation: 0.1489555615004929]
	TIME [epoch: 21.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1348537745532555		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.1348537745532555 | validation: 0.14511008420144225]
	TIME [epoch: 21.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14234168911156034		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.14234168911156034 | validation: 0.14495437690660073]
	TIME [epoch: 21.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16108446805760093		[learning rate: 0.0017486]
	Learning Rate: 0.00174861
	LOSS [training: 0.16108446805760093 | validation: 0.13818619211495822]
	TIME [epoch: 21.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1483847213521791		[learning rate: 0.0017409]
	Learning Rate: 0.00174088
	LOSS [training: 0.1483847213521791 | validation: 0.1383310714141793]
	TIME [epoch: 21.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1397832601055443		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.1397832601055443 | validation: 0.14587608308103625]
	TIME [epoch: 21.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15749131422935284		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.15749131422935284 | validation: 0.139094991507516]
	TIME [epoch: 21.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1604203652521437		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.1604203652521437 | validation: 0.13802907557775165]
	TIME [epoch: 21.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13030971934257507		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 0.13030971934257507 | validation: 0.137668808682638]
	TIME [epoch: 21.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13998325066901668		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.13998325066901668 | validation: 0.1398078413405316]
	TIME [epoch: 21.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13164840791975158		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.13164840791975158 | validation: 0.1437189968386565]
	TIME [epoch: 21.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14561220766982438		[learning rate: 0.0016877]
	Learning Rate: 0.00168775
	LOSS [training: 0.14561220766982438 | validation: 0.14391650063833228]
	TIME [epoch: 21.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14168792788079634		[learning rate: 0.0016803]
	Learning Rate: 0.00168029
	LOSS [training: 0.14168792788079634 | validation: 0.14052815726339887]
	TIME [epoch: 21.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17927863700288135		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.17927863700288135 | validation: 0.14290293222706313]
	TIME [epoch: 21.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14611627127754123		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.14611627127754123 | validation: 0.14077022297981936]
	TIME [epoch: 21.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1362763211846856		[learning rate: 0.0016581]
	Learning Rate: 0.00165812
	LOSS [training: 0.1362763211846856 | validation: 0.14735826623589887]
	TIME [epoch: 21.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11842560048232606		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 0.11842560048232606 | validation: 0.14551286287291212]
	TIME [epoch: 21.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12709898267404737		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.12709898267404737 | validation: 0.1451166371272022]
	TIME [epoch: 21.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12861569243525642		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.12861569243525642 | validation: 0.14897401258869394]
	TIME [epoch: 21.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1335981683084117		[learning rate: 0.001629]
	Learning Rate: 0.00162901
	LOSS [training: 0.1335981683084117 | validation: 0.14353238145237465]
	TIME [epoch: 21.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14433420664519		[learning rate: 0.0016218]
	Learning Rate: 0.00162181
	LOSS [training: 0.14433420664519 | validation: 0.1438959825941537]
	TIME [epoch: 21.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.162780628787415		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.162780628787415 | validation: 0.13682376949909095]
	TIME [epoch: 21.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13499744067528643		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.13499744067528643 | validation: 0.15752487206294755]
	TIME [epoch: 21.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1439096389896537		[learning rate: 0.0016004]
	Learning Rate: 0.00160041
	LOSS [training: 0.1439096389896537 | validation: 0.13965927027196368]
	TIME [epoch: 21.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16320110119335896		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 0.16320110119335896 | validation: 0.13855156686038098]
	TIME [epoch: 21.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14314601395684745		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.14314601395684745 | validation: 0.14333081303310352]
	TIME [epoch: 21.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12503304791824052		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.12503304791824052 | validation: 0.13587986698990623]
	TIME [epoch: 21.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1419751467053542		[learning rate: 0.0015723]
	Learning Rate: 0.00157231
	LOSS [training: 0.1419751467053542 | validation: 0.1395380490390115]
	TIME [epoch: 21.4 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1341686604985592		[learning rate: 0.0015654]
	Learning Rate: 0.00156536
	LOSS [training: 0.1341686604985592 | validation: 0.14390209278728733]
	TIME [epoch: 21.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14911286861530737		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.14911286861530737 | validation: 0.1399841762884235]
	TIME [epoch: 21.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13605213657210363		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.13605213657210363 | validation: 0.15408539971085022]
	TIME [epoch: 21.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12598284885868072		[learning rate: 0.0015447]
	Learning Rate: 0.00154471
	LOSS [training: 0.12598284885868072 | validation: 0.13580012675251907]
	TIME [epoch: 21.4 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15632016215953967		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.15632016215953967 | validation: 0.149771677770882]
	TIME [epoch: 21.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14596718750784074		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.14596718750784074 | validation: 0.14381379391401053]
	TIME [epoch: 21.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15295509159174653		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.15295509159174653 | validation: 0.153773549973606]
	TIME [epoch: 21.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13755313887083362		[learning rate: 0.0015176]
	Learning Rate: 0.00151759
	LOSS [training: 0.13755313887083362 | validation: 0.14027265838284916]
	TIME [epoch: 21.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14879329814448097		[learning rate: 0.0015109]
	Learning Rate: 0.00151088
	LOSS [training: 0.14879329814448097 | validation: 0.13548037332693127]
	TIME [epoch: 21.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14217191292479742		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.14217191292479742 | validation: 0.14179773146702634]
	TIME [epoch: 21.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13028657611222746		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.13028657611222746 | validation: 0.13945409235461068]
	TIME [epoch: 21.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13893732859948274		[learning rate: 0.0014909]
	Learning Rate: 0.00149094
	LOSS [training: 0.13893732859948274 | validation: 0.1440798713985026]
	TIME [epoch: 21.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13800541715507744		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 0.13800541715507744 | validation: 0.14237164672925787]
	TIME [epoch: 21.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1381324206559926		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.1381324206559926 | validation: 0.15162882372072356]
	TIME [epoch: 21.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14885706768921392		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.14885706768921392 | validation: 0.15501856935784064]
	TIME [epoch: 21.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15388585990001172		[learning rate: 0.0014648]
	Learning Rate: 0.00146477
	LOSS [training: 0.15388585990001172 | validation: 0.15287749395642522]
	TIME [epoch: 21.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16095693950161685		[learning rate: 0.0014583]
	Learning Rate: 0.0014583
	LOSS [training: 0.16095693950161685 | validation: 0.1336878780334668]
	TIME [epoch: 21.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.149958702845057		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.149958702845057 | validation: 0.14798148896181793]
	TIME [epoch: 21.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14361431489898344		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.14361431489898344 | validation: 0.1394012713325985]
	TIME [epoch: 21.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11801330663406442		[learning rate: 0.0014391]
	Learning Rate: 0.00143905
	LOSS [training: 0.11801330663406442 | validation: 0.14325543239329205]
	TIME [epoch: 21.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1436579236045689		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 0.1436579236045689 | validation: 0.1382667292632796]
	TIME [epoch: 21.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.129427545914038		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.129427545914038 | validation: 0.1377448678715821]
	TIME [epoch: 21.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14084379618648996		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.14084379618648996 | validation: 0.1400311771883378]
	TIME [epoch: 21.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1368528093973434		[learning rate: 0.0014138]
	Learning Rate: 0.00141379
	LOSS [training: 0.1368528093973434 | validation: 0.1453068454755339]
	TIME [epoch: 21.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12640426896937124		[learning rate: 0.0014075]
	Learning Rate: 0.00140754
	LOSS [training: 0.12640426896937124 | validation: 0.1673140624339807]
	TIME [epoch: 21.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1424562531297051		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.1424562531297051 | validation: 0.1471154080747379]
	TIME [epoch: 21.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14194456921143778		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.14194456921143778 | validation: 0.14142801447880946]
	TIME [epoch: 21.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14893121227525394		[learning rate: 0.001389]
	Learning Rate: 0.00138897
	LOSS [training: 0.14893121227525394 | validation: 0.1382515165735194]
	TIME [epoch: 21.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15298170778641595		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 0.15298170778641595 | validation: 0.15079992656518584]
	TIME [epoch: 21.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14490789050978942		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.14490789050978942 | validation: 0.1315048692551189]
	TIME [epoch: 21.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13160203112752428		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.13160203112752428 | validation: 0.14133421496142154]
	TIME [epoch: 21.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1299637765221669		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.1299637765221669 | validation: 0.13969612152194966]
	TIME [epoch: 21.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13166593048949826		[learning rate: 0.0013586]
	Learning Rate: 0.00135855
	LOSS [training: 0.13166593048949826 | validation: 0.14320988548013816]
	TIME [epoch: 21.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15613596084807319		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.15613596084807319 | validation: 0.1426562350569783]
	TIME [epoch: 21.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1529168091088698		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.1529168091088698 | validation: 0.15570053701526929]
	TIME [epoch: 21.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1391872773319371		[learning rate: 0.0013406]
	Learning Rate: 0.00134063
	LOSS [training: 0.1391872773319371 | validation: 0.14906202510337357]
	TIME [epoch: 21.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1438451265360722		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 0.1438451265360722 | validation: 0.1564218407501972]
	TIME [epoch: 21.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.129713616675676		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.129713616675676 | validation: 0.14183131399065899]
	TIME [epoch: 21.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1337054123139748		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1337054123139748 | validation: 0.14121411967986272]
	TIME [epoch: 21.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1576101125460555		[learning rate: 0.0013171]
	Learning Rate: 0.00131709
	LOSS [training: 0.1576101125460555 | validation: 0.1492819934082803]
	TIME [epoch: 21.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13358498936305743		[learning rate: 0.0013113]
	Learning Rate: 0.00131127
	LOSS [training: 0.13358498936305743 | validation: 0.15669551750592034]
	TIME [epoch: 21.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14691265515372104		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.14691265515372104 | validation: 0.14436058145026912]
	TIME [epoch: 21.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1294160370595351		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.1294160370595351 | validation: 0.14462188367348727]
	TIME [epoch: 21.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15999542442816322		[learning rate: 0.001294]
	Learning Rate: 0.00129397
	LOSS [training: 0.15999542442816322 | validation: 0.13836728594519312]
	TIME [epoch: 21.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14185957986427666		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 0.14185957986427666 | validation: 0.1421753981127429]
	TIME [epoch: 21.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14530834148195113		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.14530834148195113 | validation: 0.131694758173897]
	TIME [epoch: 21.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15285839021742703		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.15285839021742703 | validation: 0.1413695923221011]
	TIME [epoch: 21.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1333730010012661		[learning rate: 0.0012712]
	Learning Rate: 0.00127125
	LOSS [training: 0.1333730010012661 | validation: 0.14779044409672873]
	TIME [epoch: 21.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.146900805665449		[learning rate: 0.0012656]
	Learning Rate: 0.00126563
	LOSS [training: 0.146900805665449 | validation: 0.1359662502336583]
	TIME [epoch: 21.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14527748107071342		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.14527748107071342 | validation: 0.1326483871045943]
	TIME [epoch: 21.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15507979939054373		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.15507979939054373 | validation: 0.14163352629352147]
	TIME [epoch: 21.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13110417868590077		[learning rate: 0.0012489]
	Learning Rate: 0.00124893
	LOSS [training: 0.13110417868590077 | validation: 0.14294758243804773]
	TIME [epoch: 21.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14830173331290827		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 0.14830173331290827 | validation: 0.1382805007153495]
	TIME [epoch: 21.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14202907697604567		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.14202907697604567 | validation: 0.13498483618368945]
	TIME [epoch: 21.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14514855911296365		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.14514855911296365 | validation: 0.14878768319106875]
	TIME [epoch: 21.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1377728669061898		[learning rate: 0.001227]
	Learning Rate: 0.001227
	LOSS [training: 0.1377728669061898 | validation: 0.13778306838104995]
	TIME [epoch: 21.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13642624426131708		[learning rate: 0.0012216]
	Learning Rate: 0.00122158
	LOSS [training: 0.13642624426131708 | validation: 0.15237175907083678]
	TIME [epoch: 21.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15134561664708995		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.15134561664708995 | validation: 0.14802470815578758]
	TIME [epoch: 21.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1218647521653983		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.1218647521653983 | validation: 0.1369966791655969]
	TIME [epoch: 21.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14531459748743275		[learning rate: 0.0012055]
	Learning Rate: 0.00120546
	LOSS [training: 0.14531459748743275 | validation: 0.14165243210459535]
	TIME [epoch: 21.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13667492175069756		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 0.13667492175069756 | validation: 0.1400218478667026]
	TIME [epoch: 21.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14225760208067612		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.14225760208067612 | validation: 0.13311896574522772]
	TIME [epoch: 21.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14180548212375563		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.14180548212375563 | validation: 0.15127105932056206]
	TIME [epoch: 21.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1502355935617265		[learning rate: 0.0011843]
	Learning Rate: 0.0011843
	LOSS [training: 0.1502355935617265 | validation: 0.14187235090805247]
	TIME [epoch: 21.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15438785685744064		[learning rate: 0.0011791]
	Learning Rate: 0.00117907
	LOSS [training: 0.15438785685744064 | validation: 0.13806441858879756]
	TIME [epoch: 21.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13928840871028889		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.13928840871028889 | validation: 0.1399787984843811]
	TIME [epoch: 21.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13792465008224286		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.13792465008224286 | validation: 0.1452372547597416]
	TIME [epoch: 21.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16554023586614222		[learning rate: 0.0011635]
	Learning Rate: 0.00116351
	LOSS [training: 0.16554023586614222 | validation: 0.13757150564637569]
	TIME [epoch: 21.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15469593612697247		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 0.15469593612697247 | validation: 0.13506978436605613]
	TIME [epoch: 21.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1439706330576786		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.1439706330576786 | validation: 0.14630082728702368]
	TIME [epoch: 21.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13374746435437088		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.13374746435437088 | validation: 0.14474506723707026]
	TIME [epoch: 21.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1482791740636485		[learning rate: 0.0011431]
	Learning Rate: 0.00114308
	LOSS [training: 0.1482791740636485 | validation: 0.14446393039314612]
	TIME [epoch: 21.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1414719979884288		[learning rate: 0.001138]
	Learning Rate: 0.00113803
	LOSS [training: 0.1414719979884288 | validation: 0.15255848035147268]
	TIME [epoch: 21.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1454962763990332		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.1454962763990332 | validation: 0.1506553642607597]
	TIME [epoch: 21.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1370135080171015		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.1370135080171015 | validation: 0.14077307777151415]
	TIME [epoch: 21.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13851482476669438		[learning rate: 0.001123]
	Learning Rate: 0.00112301
	LOSS [training: 0.13851482476669438 | validation: 0.13603320724266532]
	TIME [epoch: 21.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14795901912709047		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 0.14795901912709047 | validation: 0.13983841125323637]
	TIME [epoch: 21.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14245364492718093		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.14245364492718093 | validation: 0.14354777223223386]
	TIME [epoch: 21.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12938891466723298		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.12938891466723298 | validation: 0.13474694294473397]
	TIME [epoch: 21.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1296551636922767		[learning rate: 0.0011033]
	Learning Rate: 0.0011033
	LOSS [training: 0.1296551636922767 | validation: 0.14570604622537356]
	TIME [epoch: 21.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1360345995919179		[learning rate: 0.0010984]
	Learning Rate: 0.00109842
	LOSS [training: 0.1360345995919179 | validation: 0.13850288113541318]
	TIME [epoch: 21.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15190908929055913		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.15190908929055913 | validation: 0.1321592882301218]
	TIME [epoch: 21.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1513939919415766		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.1513939919415766 | validation: 0.13802752058823622]
	TIME [epoch: 21.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13317073201864518		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.13317073201864518 | validation: 0.1367217127495958]
	TIME [epoch: 21.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12920738407988944		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 0.12920738407988944 | validation: 0.15053376590018686]
	TIME [epoch: 21.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15823901191333511		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.15823901191333511 | validation: 0.13554977465228255]
	TIME [epoch: 21.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15115994847030367		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.15115994847030367 | validation: 0.13143841041408352]
	TIME [epoch: 21.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14102232369578682		[learning rate: 0.0010649]
	Learning Rate: 0.0010649
	LOSS [training: 0.14102232369578682 | validation: 0.1366236912757827]
	TIME [epoch: 21.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14292982594560433		[learning rate: 0.0010602]
	Learning Rate: 0.00106019
	LOSS [training: 0.14292982594560433 | validation: 0.1382497034392178]
	TIME [epoch: 21.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1487405869758005		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.1487405869758005 | validation: 0.14722565699672707]
	TIME [epoch: 21.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12843952827721503		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.12843952827721503 | validation: 0.14850118080446395]
	TIME [epoch: 21.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13742552535568905		[learning rate: 0.0010462]
	Learning Rate: 0.0010462
	LOSS [training: 0.13742552535568905 | validation: 0.1392113811054654]
	TIME [epoch: 21.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12738721801611247		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 0.12738721801611247 | validation: 0.13445892728688913]
	TIME [epoch: 21.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1447220981492792		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.1447220981492792 | validation: 0.12655626549760948]
	TIME [epoch: 21.3 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14122380711857502		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.14122380711857502 | validation: 0.14018727191467537]
	TIME [epoch: 21.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1388114048925952		[learning rate: 0.0010278]
	Learning Rate: 0.00102783
	LOSS [training: 0.1388114048925952 | validation: 0.13573089995049747]
	TIME [epoch: 21.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13448423312455932		[learning rate: 0.0010233]
	Learning Rate: 0.00102329
	LOSS [training: 0.13448423312455932 | validation: 0.1334791522812215]
	TIME [epoch: 21.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13238367170031956		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.13238367170031956 | validation: 0.13725684357029314]
	TIME [epoch: 21.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15154281280755796		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.15154281280755796 | validation: 0.14911174131006913]
	TIME [epoch: 21.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14908200625367646		[learning rate: 0.0010098]
	Learning Rate: 0.00100979
	LOSS [training: 0.14908200625367646 | validation: 0.1361672942529675]
	TIME [epoch: 21.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15293682354590646		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 0.15293682354590646 | validation: 0.149791373231255]
	TIME [epoch: 21.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1726591509442701		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.1726591509442701 | validation: 0.14398648299933958]
	TIME [epoch: 21.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1323096701105457		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.1323096701105457 | validation: 0.13812690087439497]
	TIME [epoch: 21.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14514111522906714		[learning rate: 0.00099206]
	Learning Rate: 0.000992061
	LOSS [training: 0.14514111522906714 | validation: 0.15013147465462656]
	TIME [epoch: 21.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13402009891728445		[learning rate: 0.00098768]
	Learning Rate: 0.000987678
	LOSS [training: 0.13402009891728445 | validation: 0.14800008635737086]
	TIME [epoch: 21.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14857214083629955		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.14857214083629955 | validation: 0.1410239341008949]
	TIME [epoch: 21.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13424699860693667		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.13424699860693667 | validation: 0.1372193880294451]
	TIME [epoch: 21.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15361891725854537		[learning rate: 0.00097464]
	Learning Rate: 0.000974644
	LOSS [training: 0.15361891725854537 | validation: 0.14403434576510785]
	TIME [epoch: 21.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1392999154761566		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 0.1392999154761566 | validation: 0.13389935741301506]
	TIME [epoch: 21.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13728630939409864		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.13728630939409864 | validation: 0.13810077346278254]
	TIME [epoch: 21.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15513891679668745		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.15513891679668745 | validation: 0.13655132210376764]
	TIME [epoch: 21.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14654774605915993		[learning rate: 0.00095753]
	Learning Rate: 0.000957533
	LOSS [training: 0.14654774605915993 | validation: 0.1408912156217002]
	TIME [epoch: 21.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1344761222577468		[learning rate: 0.0009533]
	Learning Rate: 0.000953303
	LOSS [training: 0.1344761222577468 | validation: 0.1426878687277967]
	TIME [epoch: 21.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13711305992287812		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.13711305992287812 | validation: 0.1419063958512343]
	TIME [epoch: 21.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14898510633787213		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.14898510633787213 | validation: 0.14842776659189374]
	TIME [epoch: 21.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12951308587025392		[learning rate: 0.00094072]
	Learning Rate: 0.000940723
	LOSS [training: 0.12951308587025392 | validation: 0.14289542100110078]
	TIME [epoch: 21.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14941974437088212		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 0.14941974437088212 | validation: 0.14088834762893102]
	TIME [epoch: 21.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12768238746164054		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.12768238746164054 | validation: 0.1497004453779038]
	TIME [epoch: 21.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13941873516972197		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.13941873516972197 | validation: 0.14298547475715456]
	TIME [epoch: 21.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1310994243487536		[learning rate: 0.00092421]
	Learning Rate: 0.000924207
	LOSS [training: 0.1310994243487536 | validation: 0.139436072056027]
	TIME [epoch: 21.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14663797753415903		[learning rate: 0.00092012]
	Learning Rate: 0.000920124
	LOSS [training: 0.14663797753415903 | validation: 0.15176206061587436]
	TIME [epoch: 21.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13680648717311997		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.13680648717311997 | validation: 0.14388290035643048]
	TIME [epoch: 21.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1314116894154617		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.1314116894154617 | validation: 0.15165063098645273]
	TIME [epoch: 21.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12030728276025257		[learning rate: 0.00090798]
	Learning Rate: 0.000907981
	LOSS [training: 0.12030728276025257 | validation: 0.14514928761424892]
	TIME [epoch: 21.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1350751997758025		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 0.1350751997758025 | validation: 0.1391478350272658]
	TIME [epoch: 21.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1285658225156961		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.1285658225156961 | validation: 0.1450723998667531]
	TIME [epoch: 21.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13348470704834242		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.13348470704834242 | validation: 0.1412594088194132]
	TIME [epoch: 21.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14349804814138897		[learning rate: 0.00089204]
	Learning Rate: 0.000892041
	LOSS [training: 0.14349804814138897 | validation: 0.14477999472198258]
	TIME [epoch: 21.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13131213968023386		[learning rate: 0.0008881]
	Learning Rate: 0.000888099
	LOSS [training: 0.13131213968023386 | validation: 0.15389087000655624]
	TIME [epoch: 21.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14604436149195296		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.14604436149195296 | validation: 0.133852029198641]
	TIME [epoch: 21.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15342991495590716		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.15342991495590716 | validation: 0.13990142669060912]
	TIME [epoch: 21.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14221676987803694		[learning rate: 0.00087638]
	Learning Rate: 0.00087638
	LOSS [training: 0.14221676987803694 | validation: 0.14248296428564577]
	TIME [epoch: 21.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1277145596804489		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 0.1277145596804489 | validation: 0.1366243316313898]
	TIME [epoch: 21.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13783028683408766		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.13783028683408766 | validation: 0.14791200357385317]
	TIME [epoch: 21.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14206103048914592		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.14206103048914592 | validation: 0.13427466973432597]
	TIME [epoch: 21.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1274793016569305		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.1274793016569305 | validation: 0.1469899091809385]
	TIME [epoch: 21.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15218136323322057		[learning rate: 0.00085719]
	Learning Rate: 0.00085719
	LOSS [training: 0.15218136323322057 | validation: 0.1451888405369042]
	TIME [epoch: 21.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16593603834180126		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.16593603834180126 | validation: 0.13620718971366788]
	TIME [epoch: 21.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12484359961875183		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.12484359961875183 | validation: 0.13755800537431276]
	TIME [epoch: 21.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14490064546851772		[learning rate: 0.00084588]
	Learning Rate: 0.000845878
	LOSS [training: 0.14490064546851772 | validation: 0.1413901872161135]
	TIME [epoch: 21.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14001900344558352		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 0.14001900344558352 | validation: 0.13851061610708984]
	TIME [epoch: 21.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13346883319283762		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.13346883319283762 | validation: 0.13942474227587187]
	TIME [epoch: 21.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12570821440485486		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.12570821440485486 | validation: 0.13240134595599756]
	TIME [epoch: 21.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15948809709400702		[learning rate: 0.00083103]
	Learning Rate: 0.000831028
	LOSS [training: 0.15948809709400702 | validation: 0.14404385789092033]
	TIME [epoch: 21.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1256399882714389		[learning rate: 0.00082736]
	Learning Rate: 0.000827356
	LOSS [training: 0.1256399882714389 | validation: 0.1386614290872348]
	TIME [epoch: 21.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12325243481323866		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.12325243481323866 | validation: 0.14547185694977402]
	TIME [epoch: 21.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13990132275645903		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.13990132275645903 | validation: 0.14602883710854273]
	TIME [epoch: 21.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15376203166633515		[learning rate: 0.00081644]
	Learning Rate: 0.000816438
	LOSS [training: 0.15376203166633515 | validation: 0.14152644224383257]
	TIME [epoch: 21.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1329708326004508		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 0.1329708326004508 | validation: 0.1468391537684886]
	TIME [epoch: 21.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1321803758224156		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.1321803758224156 | validation: 0.1406989749484661]
	TIME [epoch: 21.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13848754955564366		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.13848754955564366 | validation: 0.13797202777406187]
	TIME [epoch: 21.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12898261061967964		[learning rate: 0.0008021]
	Learning Rate: 0.000802104
	LOSS [training: 0.12898261061967964 | validation: 0.13324379945628298]
	TIME [epoch: 21.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15669952379452806		[learning rate: 0.00079856]
	Learning Rate: 0.00079856
	LOSS [training: 0.15669952379452806 | validation: 0.1481743946434135]
	TIME [epoch: 21.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1407555903177966		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.1407555903177966 | validation: 0.13828570632795376]
	TIME [epoch: 21.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1418788853644684		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.1418788853644684 | validation: 0.1377556062929154]
	TIME [epoch: 21.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14475043106415122		[learning rate: 0.00078802]
	Learning Rate: 0.000788022
	LOSS [training: 0.14475043106415122 | validation: 0.1376001961392246]
	TIME [epoch: 21.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14788718161026554		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 0.14788718161026554 | validation: 0.13734656254701552]
	TIME [epoch: 21.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1389937309318713		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.1389937309318713 | validation: 0.13257098375165904]
	TIME [epoch: 21.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1480215679973828		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.1480215679973828 | validation: 0.1408483477088226]
	TIME [epoch: 21.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1698076790026948		[learning rate: 0.00077419]
	Learning Rate: 0.000774188
	LOSS [training: 0.1698076790026948 | validation: 0.14053128455914982]
	TIME [epoch: 21.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12711964801621217		[learning rate: 0.00077077]
	Learning Rate: 0.000770767
	LOSS [training: 0.12711964801621217 | validation: 0.1354854908944797]
	TIME [epoch: 21.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.139441566232711		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.139441566232711 | validation: 0.13553500022252615]
	TIME [epoch: 21.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14490540227163423		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.14490540227163423 | validation: 0.14728707286252213]
	TIME [epoch: 21.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14815548111174912		[learning rate: 0.0007606]
	Learning Rate: 0.000760596
	LOSS [training: 0.14815548111174912 | validation: 0.13828986827089254]
	TIME [epoch: 21.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15248353825217517		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 0.15248353825217517 | validation: 0.13704950000337732]
	TIME [epoch: 21.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12410313457439086		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.12410313457439086 | validation: 0.14715524514286513]
	TIME [epoch: 21.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13075242174208504		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.13075242174208504 | validation: 0.13627684314594998]
	TIME [epoch: 21.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1572491689981491		[learning rate: 0.00074724]
	Learning Rate: 0.000747242
	LOSS [training: 0.1572491689981491 | validation: 0.1434273859074245]
	TIME [epoch: 21.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13339742963631576		[learning rate: 0.00074394]
	Learning Rate: 0.000743941
	LOSS [training: 0.13339742963631576 | validation: 0.13691577392610635]
	TIME [epoch: 21.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1564003431465604		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.1564003431465604 | validation: 0.1413291554905262]
	TIME [epoch: 21.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1403585760991596		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.1403585760991596 | validation: 0.14489181203308282]
	TIME [epoch: 21.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15620129042161607		[learning rate: 0.00073412]
	Learning Rate: 0.000734124
	LOSS [training: 0.15620129042161607 | validation: 0.14453729740862187]
	TIME [epoch: 21.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14273851760002715		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 0.14273851760002715 | validation: 0.14181319045050486]
	TIME [epoch: 21.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14131787659473277		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.14131787659473277 | validation: 0.13863982761449334]
	TIME [epoch: 21.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14628670877422292		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.14628670877422292 | validation: 0.1344927930071502]
	TIME [epoch: 21.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13174919275762412		[learning rate: 0.00072124]
	Learning Rate: 0.000721235
	LOSS [training: 0.13174919275762412 | validation: 0.150627286983865]
	TIME [epoch: 21.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13689705454859918		[learning rate: 0.00071805]
	Learning Rate: 0.000718049
	LOSS [training: 0.13689705454859918 | validation: 0.13609884702176053]
	TIME [epoch: 21.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13842372111665352		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.13842372111665352 | validation: 0.14116194075312996]
	TIME [epoch: 21.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13363616589995053		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.13363616589995053 | validation: 0.1426215902077122]
	TIME [epoch: 21.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1312193912547533		[learning rate: 0.00070857]
	Learning Rate: 0.000708573
	LOSS [training: 0.1312193912547533 | validation: 0.14760957500642544]
	TIME [epoch: 21.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15250807917629408		[learning rate: 0.00070544]
	Learning Rate: 0.000705442
	LOSS [training: 0.15250807917629408 | validation: 0.13437300079657669]
	TIME [epoch: 21.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13686976927082597		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.13686976927082597 | validation: 0.1377400984698622]
	TIME [epoch: 21.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12459755778236592		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.12459755778236592 | validation: 0.1511020364874244]
	TIME [epoch: 21.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13561908461478375		[learning rate: 0.00069613]
	Learning Rate: 0.000696133
	LOSS [training: 0.13561908461478375 | validation: 0.1466625647579317]
	TIME [epoch: 21.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14132743026474232		[learning rate: 0.00069306]
	Learning Rate: 0.000693058
	LOSS [training: 0.14132743026474232 | validation: 0.1417768397389889]
	TIME [epoch: 21.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1435274980867752		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.1435274980867752 | validation: 0.1298752616142319]
	TIME [epoch: 21.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14017847777909495		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.14017847777909495 | validation: 0.13802635530889626]
	TIME [epoch: 21.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12773980319730371		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.12773980319730371 | validation: 0.14567730877574267]
	TIME [epoch: 21.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1472462266648159		[learning rate: 0.00068089]
	Learning Rate: 0.00068089
	LOSS [training: 0.1472462266648159 | validation: 0.14426510338653253]
	TIME [epoch: 21.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12928768842512423		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.12928768842512423 | validation: 0.1352811835980854]
	TIME [epoch: 21.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1421787537753261		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.1421787537753261 | validation: 0.14418886281271043]
	TIME [epoch: 21.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.140389002953121		[learning rate: 0.0006719]
	Learning Rate: 0.000671905
	LOSS [training: 0.140389002953121 | validation: 0.13822648594915823]
	TIME [epoch: 21.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1234804888776055		[learning rate: 0.00066894]
	Learning Rate: 0.000668936
	LOSS [training: 0.1234804888776055 | validation: 0.14272856356677807]
	TIME [epoch: 21.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16065313673931786		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.16065313673931786 | validation: 0.15454625437589561]
	TIME [epoch: 21.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14707565017501759		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.14707565017501759 | validation: 0.13553060988656282]
	TIME [epoch: 21.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1355956755393223		[learning rate: 0.00066011]
	Learning Rate: 0.000660109
	LOSS [training: 0.1355956755393223 | validation: 0.14854001025163185]
	TIME [epoch: 21.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13709402265024898		[learning rate: 0.00065719]
	Learning Rate: 0.000657192
	LOSS [training: 0.13709402265024898 | validation: 0.14154466966446885]
	TIME [epoch: 21.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1282107282562238		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.1282107282562238 | validation: 0.14023100528805674]
	TIME [epoch: 21.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13534884108568984		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.13534884108568984 | validation: 0.14102761679304174]
	TIME [epoch: 21.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13504343299402038		[learning rate: 0.00064852]
	Learning Rate: 0.00064852
	LOSS [training: 0.13504343299402038 | validation: 0.13675055747532944]
	TIME [epoch: 21.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1327372662186827		[learning rate: 0.00064565]
	Learning Rate: 0.000645654
	LOSS [training: 0.1327372662186827 | validation: 0.1431955315660904]
	TIME [epoch: 21.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12866571934092208		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.12866571934092208 | validation: 0.14643510981258376]
	TIME [epoch: 21.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1337179802142357		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.1337179802142357 | validation: 0.12820152666350862]
	TIME [epoch: 21.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.135939258154683		[learning rate: 0.00063713]
	Learning Rate: 0.000637134
	LOSS [training: 0.135939258154683 | validation: 0.1403648149465139]
	TIME [epoch: 21.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1700982350837908		[learning rate: 0.00063432]
	Learning Rate: 0.000634319
	LOSS [training: 0.1700982350837908 | validation: 0.14274837350227276]
	TIME [epoch: 21.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14400767384631133		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.14400767384631133 | validation: 0.14442506299201266]
	TIME [epoch: 21.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13307252263363092		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.13307252263363092 | validation: 0.13396916500580472]
	TIME [epoch: 21.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12502018109467652		[learning rate: 0.00062595]
	Learning Rate: 0.000625948
	LOSS [training: 0.12502018109467652 | validation: 0.1368133263409211]
	TIME [epoch: 21.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1409530564888763		[learning rate: 0.00062318]
	Learning Rate: 0.000623183
	LOSS [training: 0.1409530564888763 | validation: 0.1387001080177341]
	TIME [epoch: 21.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1212981084094578		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.1212981084094578 | validation: 0.13348372981134893]
	TIME [epoch: 21.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14221025899161327		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.14221025899161327 | validation: 0.1372936680254873]
	TIME [epoch: 21.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13869922242323923		[learning rate: 0.00061496]
	Learning Rate: 0.000614959
	LOSS [training: 0.13869922242323923 | validation: 0.1405543011607565]
	TIME [epoch: 21.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13068256361241473		[learning rate: 0.00061224]
	Learning Rate: 0.000612242
	LOSS [training: 0.13068256361241473 | validation: 0.14195158037877112]
	TIME [epoch: 21.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14811637675245257		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.14811637675245257 | validation: 0.1384843226291718]
	TIME [epoch: 21.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13005745769411475		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.13005745769411475 | validation: 0.13792860216209704]
	TIME [epoch: 21.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1316802986244488		[learning rate: 0.00060416]
	Learning Rate: 0.000604163
	LOSS [training: 0.1316802986244488 | validation: 0.1354055466356305]
	TIME [epoch: 21.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12881776069294623		[learning rate: 0.00060149]
	Learning Rate: 0.000601493
	LOSS [training: 0.12881776069294623 | validation: 0.14072681673334717]
	TIME [epoch: 21.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14117643845468075		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.14117643845468075 | validation: 0.14135767335284033]
	TIME [epoch: 21.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1317562653606478		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.1317562653606478 | validation: 0.13932930908394567]
	TIME [epoch: 21.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12988129721039599		[learning rate: 0.00059356]
	Learning Rate: 0.000593556
	LOSS [training: 0.12988129721039599 | validation: 0.14010889250388414]
	TIME [epoch: 21.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17195232094086732		[learning rate: 0.00059093]
	Learning Rate: 0.000590933
	LOSS [training: 0.17195232094086732 | validation: 0.14026346469727324]
	TIME [epoch: 21.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16947442986041333		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.16947442986041333 | validation: 0.14813538244410732]
	TIME [epoch: 21.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12455793295814895		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.12455793295814895 | validation: 0.14293540004720703]
	TIME [epoch: 21.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14906670208329656		[learning rate: 0.00058314]
	Learning Rate: 0.000583135
	LOSS [training: 0.14906670208329656 | validation: 0.13083680370257744]
	TIME [epoch: 21.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13634505278465234		[learning rate: 0.00058056]
	Learning Rate: 0.000580559
	LOSS [training: 0.13634505278465234 | validation: 0.13951259768515364]
	TIME [epoch: 21.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13417145960756655		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.13417145960756655 | validation: 0.13595710376768308]
	TIME [epoch: 21.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13574122228966817		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.13574122228966817 | validation: 0.13164916032991736]
	TIME [epoch: 21.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15691853838414155		[learning rate: 0.0005729]
	Learning Rate: 0.000572898
	LOSS [training: 0.15691853838414155 | validation: 0.1423840339837655]
	TIME [epoch: 21.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15845066236439082		[learning rate: 0.00057037]
	Learning Rate: 0.000570366
	LOSS [training: 0.15845066236439082 | validation: 0.14042890399170213]
	TIME [epoch: 21.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1274199652134596		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.1274199652134596 | validation: 0.14138756441391956]
	TIME [epoch: 21.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13474696366676137		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.13474696366676137 | validation: 0.15399583836839906]
	TIME [epoch: 21.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1634814598617595		[learning rate: 0.00056284]
	Learning Rate: 0.00056284
	LOSS [training: 0.1634814598617595 | validation: 0.13805199995773546]
	TIME [epoch: 21.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1424913632623957		[learning rate: 0.00056035]
	Learning Rate: 0.000560353
	LOSS [training: 0.1424913632623957 | validation: 0.14071668688146857]
	TIME [epoch: 21.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15678035493242154		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.15678035493242154 | validation: 0.14381603645339475]
	TIME [epoch: 21.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15439344092283108		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.15439344092283108 | validation: 0.1341436982095761]
	TIME [epoch: 21.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14801103701326962		[learning rate: 0.00055296]
	Learning Rate: 0.000552958
	LOSS [training: 0.14801103701326962 | validation: 0.14143474864701386]
	TIME [epoch: 21.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1287760144299459		[learning rate: 0.00055052]
	Learning Rate: 0.000550515
	LOSS [training: 0.1287760144299459 | validation: 0.1437909446109597]
	TIME [epoch: 21.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14013259674889467		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.14013259674889467 | validation: 0.1419875424300594]
	TIME [epoch: 21.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1270495998170443		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.1270495998170443 | validation: 0.14237817605976086]
	TIME [epoch: 21.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19546941993879302		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 0.19546941993879302 | validation: 0.14206579112883594]
	TIME [epoch: 21.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1448192487626369		[learning rate: 0.00054085]
	Learning Rate: 0.00054085
	LOSS [training: 0.1448192487626369 | validation: 0.1441875789797002]
	TIME [epoch: 21.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1493359813299669		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.1493359813299669 | validation: 0.13411151042238248]
	TIME [epoch: 21.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14562665825941146		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.14562665825941146 | validation: 0.13182107798901707]
	TIME [epoch: 21.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13511127939095785		[learning rate: 0.00053371]
	Learning Rate: 0.000533713
	LOSS [training: 0.13511127939095785 | validation: 0.1346006382698817]
	TIME [epoch: 21.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1309518621167723		[learning rate: 0.00053135]
	Learning Rate: 0.000531355
	LOSS [training: 0.1309518621167723 | validation: 0.14047811031781832]
	TIME [epoch: 21.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12731268083894132		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.12731268083894132 | validation: 0.14616294932919685]
	TIME [epoch: 21.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14282313389014029		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.14282313389014029 | validation: 0.1390960745605568]
	TIME [epoch: 21.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1441172279505817		[learning rate: 0.00052434]
	Learning Rate: 0.000524343
	LOSS [training: 0.1441172279505817 | validation: 0.13629448628764668]
	TIME [epoch: 21.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14344043693307962		[learning rate: 0.00052203]
	Learning Rate: 0.000522026
	LOSS [training: 0.14344043693307962 | validation: 0.13614578021517357]
	TIME [epoch: 21.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1580286578606863		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.1580286578606863 | validation: 0.15006748751864682]
	TIME [epoch: 21.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13237497657469557		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.13237497657469557 | validation: 0.15156616965567257]
	TIME [epoch: 21.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15939026272175977		[learning rate: 0.00051514]
	Learning Rate: 0.000515137
	LOSS [training: 0.15939026272175977 | validation: 0.13638278447810637]
	TIME [epoch: 21.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15065208461832186		[learning rate: 0.00051286]
	Learning Rate: 0.000512861
	LOSS [training: 0.15065208461832186 | validation: 0.15440189371356397]
	TIME [epoch: 21.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1341317992161684		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.1341317992161684 | validation: 0.13281998241389184]
	TIME [epoch: 21.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14988233672103374		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.14988233672103374 | validation: 0.1465377082117559]
	TIME [epoch: 21.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1269956322215407		[learning rate: 0.00050609]
	Learning Rate: 0.000506094
	LOSS [training: 0.1269956322215407 | validation: 0.13671904768277968]
	TIME [epoch: 21.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1294621345683071		[learning rate: 0.00050386]
	Learning Rate: 0.000503858
	LOSS [training: 0.1294621345683071 | validation: 0.13766720860605852]
	TIME [epoch: 21.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13583993297641395		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.13583993297641395 | validation: 0.13419960491932537]
	TIME [epoch: 21.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13714474297760232		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.13714474297760232 | validation: 0.13919082815877246]
	TIME [epoch: 21.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13848396287039028		[learning rate: 0.00049721]
	Learning Rate: 0.000497208
	LOSS [training: 0.13848396287039028 | validation: 0.13882346016071778]
	TIME [epoch: 21.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1307544358864357		[learning rate: 0.00049501]
	Learning Rate: 0.000495012
	LOSS [training: 0.1307544358864357 | validation: 0.1402024821038724]
	TIME [epoch: 21.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13072738004031112		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.13072738004031112 | validation: 0.13547754927966152]
	TIME [epoch: 21.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13046725494678432		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.13046725494678432 | validation: 0.1509710011337336]
	TIME [epoch: 21.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13899717615983184		[learning rate: 0.00048848]
	Learning Rate: 0.000488479
	LOSS [training: 0.13899717615983184 | validation: 0.14437599258873612]
	TIME [epoch: 21.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13368735412428262		[learning rate: 0.00048632]
	Learning Rate: 0.000486321
	LOSS [training: 0.13368735412428262 | validation: 0.14013917517603658]
	TIME [epoch: 21.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14263127793933067		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.14263127793933067 | validation: 0.13910804330036275]
	TIME [epoch: 21.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12807546653177687		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.12807546653177687 | validation: 0.13373151359604593]
	TIME [epoch: 21.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13215473787977605		[learning rate: 0.0004799]
	Learning Rate: 0.000479903
	LOSS [training: 0.13215473787977605 | validation: 0.1463754689182943]
	TIME [epoch: 21.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12996865870749835		[learning rate: 0.00047778]
	Learning Rate: 0.000477783
	LOSS [training: 0.12996865870749835 | validation: 0.14491039312220283]
	TIME [epoch: 21.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1410935593841236		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.1410935593841236 | validation: 0.13257582350877806]
	TIME [epoch: 21.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12648589783153957		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.12648589783153957 | validation: 0.13589474285131975]
	TIME [epoch: 21.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14919528895882755		[learning rate: 0.00047148]
	Learning Rate: 0.000471478
	LOSS [training: 0.14919528895882755 | validation: 0.13332352491274557]
	TIME [epoch: 21.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13450782691495505		[learning rate: 0.0004694]
	Learning Rate: 0.000469395
	LOSS [training: 0.13450782691495505 | validation: 0.1424443097824375]
	TIME [epoch: 21.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12662713248288812		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.12662713248288812 | validation: 0.1387489641514318]
	TIME [epoch: 21.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16382984350018212		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.16382984350018212 | validation: 0.136186680711495]
	TIME [epoch: 21.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1369391252239839		[learning rate: 0.0004632]
	Learning Rate: 0.000463201
	LOSS [training: 0.1369391252239839 | validation: 0.1371430585356053]
	TIME [epoch: 21.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12484466698862591		[learning rate: 0.00046115]
	Learning Rate: 0.000461154
	LOSS [training: 0.12484466698862591 | validation: 0.14407250503114002]
	TIME [epoch: 21.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13385469922938256		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.13385469922938256 | validation: 0.13545927718805292]
	TIME [epoch: 21.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1271665435611027		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.1271665435611027 | validation: 0.13991274899460854]
	TIME [epoch: 21.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12679729767894982		[learning rate: 0.00045507]
	Learning Rate: 0.000455069
	LOSS [training: 0.12679729767894982 | validation: 0.14123217814523406]
	TIME [epoch: 21.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14497693273723605		[learning rate: 0.00045306]
	Learning Rate: 0.000453058
	LOSS [training: 0.14497693273723605 | validation: 0.14682186267869665]
	TIME [epoch: 21.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13503344617887786		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.13503344617887786 | validation: 0.14365918812878326]
	TIME [epoch: 21.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16633931918379533		[learning rate: 0.00044906]
	Learning Rate: 0.000449064
	LOSS [training: 0.16633931918379533 | validation: 0.13840901923125537]
	TIME [epoch: 21.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1402017841738122		[learning rate: 0.00044708]
	Learning Rate: 0.000447079
	LOSS [training: 0.1402017841738122 | validation: 0.14413724653928436]
	TIME [epoch: 21.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14233419169214917		[learning rate: 0.0004451]
	Learning Rate: 0.000445104
	LOSS [training: 0.14233419169214917 | validation: 0.13531010456376288]
	TIME [epoch: 21.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13935341422167755		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.13935341422167755 | validation: 0.13744512927527353]
	TIME [epoch: 21.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14665178647515392		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.14665178647515392 | validation: 0.1431165462290025]
	TIME [epoch: 21.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15633677397844886		[learning rate: 0.00043923]
	Learning Rate: 0.00043923
	LOSS [training: 0.15633677397844886 | validation: 0.14731976888853412]
	TIME [epoch: 21.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1532360111094739		[learning rate: 0.00043729]
	Learning Rate: 0.00043729
	LOSS [training: 0.1532360111094739 | validation: 0.1366241078053285]
	TIME [epoch: 21.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1390824733919889		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.1390824733919889 | validation: 0.15371809050629917]
	TIME [epoch: 21.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12464843418605573		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.12464843418605573 | validation: 0.13814836011554432]
	TIME [epoch: 21.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14319386762780034		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 0.14319386762780034 | validation: 0.13893095579765022]
	TIME [epoch: 21.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13356132520259906		[learning rate: 0.00042961]
	Learning Rate: 0.000429613
	LOSS [training: 0.13356132520259906 | validation: 0.13962476134328014]
	TIME [epoch: 21.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16292886968810247		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.16292886968810247 | validation: 0.13657170000945398]
	TIME [epoch: 21.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1407641921901705		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1407641921901705 | validation: 0.13875869369280722]
	TIME [epoch: 21.2 sec]
	Saving model to: out/model_training/model_transition1_v1_20240606_192100/states/model_transition1_v1_753.pth
Halted early. No improvement in validation loss for 200 epochs.
Finished training in 16182.783 seconds.
