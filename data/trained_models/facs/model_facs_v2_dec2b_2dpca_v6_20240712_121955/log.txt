Args:
Namespace(name='model_facs_v2_dec2b_2dpca_v6', outdir='out/model_training/model_facs_v2_dec2b_2dpca_v6', training_data='data/training_data/facs_v2/pca/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/training', validation_data='data/training_data/facs_v2/pca/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=10, batch_size=50, patience=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=100, ncells_sample=100, model_do_sample=False, dt=0.005, dt_schedule='stepped', dt_schedule_bounds=[500, 1000, 1500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.02, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.1, 1.3, 1.5, 2.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1502677404

Training model...

Saving initial model state to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7809661228717524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809661228717524 | validation: 0.905354589431203]
	TIME [epoch: 31.6 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6595212021441385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595212021441385 | validation: 0.7514259170843047]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5921491768131716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5921491768131716 | validation: 0.7026705677248133]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5505293099846851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5505293099846851 | validation: 0.7418170537980306]
	TIME [epoch: 4.19 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5249724774271952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5249724774271952 | validation: 0.6443662424017358]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5179930125939054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179930125939054 | validation: 0.6716337609317751]
	TIME [epoch: 4.17 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48750600661787724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48750600661787724 | validation: 0.6901611281345369]
	TIME [epoch: 4.18 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4643220711438216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4643220711438216 | validation: 0.7924818760637702]
	TIME [epoch: 4.19 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4679129577975255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4679129577975255 | validation: 0.5824138989549336]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5509762765086655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5509762765086655 | validation: 0.6735243751742139]
	TIME [epoch: 4.18 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4078508528087023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4078508528087023 | validation: 0.5630127574143986]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3742706231959866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3742706231959866 | validation: 0.5387720043462039]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33690024730325935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33690024730325935 | validation: 0.5946165413886271]
	TIME [epoch: 4.18 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38635789522510955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38635789522510955 | validation: 0.5162237213695988]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3440862316841363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3440862316841363 | validation: 0.5066249988008706]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3711955751643904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3711955751643904 | validation: 0.49993552678145264]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3138032773825192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3138032773825192 | validation: 0.5252114939006789]
	TIME [epoch: 4.19 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34981080792342806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34981080792342806 | validation: 0.5422025363372812]
	TIME [epoch: 4.17 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33313882217746305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33313882217746305 | validation: 0.5218200177677436]
	TIME [epoch: 4.17 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38259568696303814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38259568696303814 | validation: 0.5033979579367263]
	TIME [epoch: 4.18 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30793604313121137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30793604313121137 | validation: 0.5195108753580896]
	TIME [epoch: 4.18 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3061568471696836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3061568471696836 | validation: 0.5058867178237454]
	TIME [epoch: 4.17 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.342680241643634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.342680241643634 | validation: 0.4868905817522429]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30871575676042784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30871575676042784 | validation: 0.524230814987931]
	TIME [epoch: 4.18 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3446578733510402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3446578733510402 | validation: 0.5190441889852617]
	TIME [epoch: 4.17 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3245878030493634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3245878030493634 | validation: 0.48437259647144854]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30314304170078993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30314304170078993 | validation: 0.588625943299478]
	TIME [epoch: 4.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3507346228616627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3507346228616627 | validation: 0.48614104599981844]
	TIME [epoch: 4.19 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2952119161072316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2952119161072316 | validation: 0.49114402542005886]
	TIME [epoch: 4.18 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33492576616144065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33492576616144065 | validation: 0.511962629826439]
	TIME [epoch: 4.17 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31288621707700615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31288621707700615 | validation: 0.483768156660003]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3128621079342612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3128621079342612 | validation: 0.5040337334320344]
	TIME [epoch: 4.18 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.288598048469862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.288598048469862 | validation: 0.47625361144294076]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3259229448246872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259229448246872 | validation: 0.46064409234283765]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3008581832144105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3008581832144105 | validation: 0.529431006695216]
	TIME [epoch: 4.18 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3154881952973142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3154881952973142 | validation: 0.461938028678217]
	TIME [epoch: 4.18 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2894489708008504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2894489708008504 | validation: 0.5005760282331584]
	TIME [epoch: 4.18 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2890858266718749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2890858266718749 | validation: 0.4965860712997302]
	TIME [epoch: 4.18 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2863386960001185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2863386960001185 | validation: 0.4941745022851883]
	TIME [epoch: 4.18 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30160388017368356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30160388017368356 | validation: 0.462533335082595]
	TIME [epoch: 4.17 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2672859060438896		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.2672859060438896 | validation: 0.45844914744230986]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2738212843942077		[learning rate: 0.0099206]
	Learning Rate: 0.00992061
	LOSS [training: 0.2738212843942077 | validation: 0.4305855846627591]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26280732344029545		[learning rate: 0.0098768]
	Learning Rate: 0.00987678
	LOSS [training: 0.26280732344029545 | validation: 0.41626467969150815]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2693177163932336		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.2693177163932336 | validation: 0.4418466807121072]
	TIME [epoch: 4.18 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32777039697852095		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.32777039697852095 | validation: 0.44952382435406635]
	TIME [epoch: 4.19 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26267365586278596		[learning rate: 0.0097464]
	Learning Rate: 0.00974644
	LOSS [training: 0.26267365586278596 | validation: 0.4757900776853905]
	TIME [epoch: 4.23 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26393317639760716		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 0.26393317639760716 | validation: 0.4123666582013299]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24645792471851186		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.24645792471851186 | validation: 0.442541722705169]
	TIME [epoch: 4.19 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26918873245855146		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.26918873245855146 | validation: 0.3768677357187476]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26820565816354197		[learning rate: 0.0095753]
	Learning Rate: 0.00957533
	LOSS [training: 0.26820565816354197 | validation: 0.4633653880820233]
	TIME [epoch: 4.18 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2893931622392933		[learning rate: 0.009533]
	Learning Rate: 0.00953303
	LOSS [training: 0.2893931622392933 | validation: 0.4223466329610274]
	TIME [epoch: 4.17 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22952867426694645		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.22952867426694645 | validation: 0.44401474169709654]
	TIME [epoch: 4.18 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28689634942201286		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.28689634942201286 | validation: 0.39484531728120376]
	TIME [epoch: 4.17 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22087988261679525		[learning rate: 0.0094072]
	Learning Rate: 0.00940722
	LOSS [training: 0.22087988261679525 | validation: 0.4439307567666749]
	TIME [epoch: 4.17 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27891246933954916		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 0.27891246933954916 | validation: 0.43275318393047496]
	TIME [epoch: 4.18 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23432414092509193		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.23432414092509193 | validation: 0.3789088532789398]
	TIME [epoch: 4.17 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23959113060221143		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.23959113060221143 | validation: 0.3715765388736818]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2637594424562706		[learning rate: 0.0092421]
	Learning Rate: 0.00924207
	LOSS [training: 0.2637594424562706 | validation: 0.4127235578489658]
	TIME [epoch: 4.18 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2832910527206911		[learning rate: 0.0092012]
	Learning Rate: 0.00920124
	LOSS [training: 0.2832910527206911 | validation: 0.4335735189871309]
	TIME [epoch: 4.17 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21244341855989884		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.21244341855989884 | validation: 0.367082901164169]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23624470338707151		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.23624470338707151 | validation: 0.588914370979752]
	TIME [epoch: 4.18 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3187973821546885		[learning rate: 0.0090798]
	Learning Rate: 0.00907981
	LOSS [training: 0.3187973821546885 | validation: 0.397331702231318]
	TIME [epoch: 4.18 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22505766305422137		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 0.22505766305422137 | validation: 0.32045182358471647]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22168730708139214		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.22168730708139214 | validation: 0.44139398980477507]
	TIME [epoch: 4.19 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22201048345771907		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.22201048345771907 | validation: 0.4247491310970313]
	TIME [epoch: 4.19 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22213478888040422		[learning rate: 0.0089204]
	Learning Rate: 0.00892041
	LOSS [training: 0.22213478888040422 | validation: 0.3300410822608031]
	TIME [epoch: 4.19 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25170366029770597		[learning rate: 0.008881]
	Learning Rate: 0.00888099
	LOSS [training: 0.25170366029770597 | validation: 0.3472211337987463]
	TIME [epoch: 4.2 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24259877933789173		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.24259877933789173 | validation: 0.41971822375074247]
	TIME [epoch: 4.19 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22943000513586762		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.22943000513586762 | validation: 0.311643486273047]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19294742995006559		[learning rate: 0.0087638]
	Learning Rate: 0.0087638
	LOSS [training: 0.19294742995006559 | validation: 0.3536317347852513]
	TIME [epoch: 4.18 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26847802737244997		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 0.26847802737244997 | validation: 0.36150983594680763]
	TIME [epoch: 4.17 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21071630807350417		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.21071630807350417 | validation: 0.3376059056209405]
	TIME [epoch: 4.18 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.221290586297548		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.221290586297548 | validation: 0.3125333643147105]
	TIME [epoch: 4.18 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2241836531866689		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.2241836531866689 | validation: 0.4139352054193023]
	TIME [epoch: 4.18 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23483159925640917		[learning rate: 0.0085719]
	Learning Rate: 0.0085719
	LOSS [training: 0.23483159925640917 | validation: 0.4171809361325574]
	TIME [epoch: 4.17 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24544323731782133		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.24544323731782133 | validation: 0.4961982201726475]
	TIME [epoch: 4.17 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23072316067332405		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.23072316067332405 | validation: 0.39524867833697]
	TIME [epoch: 4.17 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23512686608062575		[learning rate: 0.0084588]
	Learning Rate: 0.00845878
	LOSS [training: 0.23512686608062575 | validation: 0.41531413856588256]
	TIME [epoch: 4.17 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25072551739627424		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 0.25072551739627424 | validation: 0.3617053606494331]
	TIME [epoch: 4.19 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22509247543579866		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.22509247543579866 | validation: 0.3575770071686299]
	TIME [epoch: 4.19 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21993293383079524		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.21993293383079524 | validation: 0.4261451697362705]
	TIME [epoch: 4.18 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2550416443966569		[learning rate: 0.0083103]
	Learning Rate: 0.00831027
	LOSS [training: 0.2550416443966569 | validation: 0.32973981697368354]
	TIME [epoch: 4.18 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1987708642614267		[learning rate: 0.0082736]
	Learning Rate: 0.00827356
	LOSS [training: 0.1987708642614267 | validation: 0.33473993067150337]
	TIME [epoch: 4.19 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23095601603109084		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.23095601603109084 | validation: 0.3502715571736321]
	TIME [epoch: 4.18 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24928729883882483		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.24928729883882483 | validation: 0.4779465858903853]
	TIME [epoch: 4.18 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24093045800700227		[learning rate: 0.0081644]
	Learning Rate: 0.00816438
	LOSS [training: 0.24093045800700227 | validation: 0.4866719709220268]
	TIME [epoch: 4.18 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33056008044761914		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 0.33056008044761914 | validation: 0.4108940269526831]
	TIME [epoch: 4.18 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25785199285411275		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.25785199285411275 | validation: 0.37063578401585545]
	TIME [epoch: 4.18 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2149562677805433		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.2149562677805433 | validation: 0.34611443350610444]
	TIME [epoch: 4.18 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2155450557770978		[learning rate: 0.008021]
	Learning Rate: 0.00802104
	LOSS [training: 0.2155450557770978 | validation: 0.482339693578477]
	TIME [epoch: 4.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22167820005784283		[learning rate: 0.0079856]
	Learning Rate: 0.0079856
	LOSS [training: 0.22167820005784283 | validation: 0.4485628209903586]
	TIME [epoch: 4.18 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2041735182257725		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2041735182257725 | validation: 0.372968525501764]
	TIME [epoch: 4.18 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2609676827323223		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.2609676827323223 | validation: 0.31910800123335187]
	TIME [epoch: 4.17 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20176590535491828		[learning rate: 0.0078802]
	Learning Rate: 0.00788022
	LOSS [training: 0.20176590535491828 | validation: 0.34337968352572795]
	TIME [epoch: 4.19 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18752240172868295		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.18752240172868295 | validation: 0.36016162562192483]
	TIME [epoch: 4.17 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19965415731896818		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.19965415731896818 | validation: 0.3340457952327432]
	TIME [epoch: 4.18 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21460026153143907		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.21460026153143907 | validation: 0.33256859863455457]
	TIME [epoch: 4.18 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22875394737433963		[learning rate: 0.0077419]
	Learning Rate: 0.00774188
	LOSS [training: 0.22875394737433963 | validation: 0.472984598679714]
	TIME [epoch: 4.19 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32992847153269705		[learning rate: 0.0077077]
	Learning Rate: 0.00770767
	LOSS [training: 0.32992847153269705 | validation: 0.44664992168697043]
	TIME [epoch: 4.18 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26708664131361126		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.26708664131361126 | validation: 0.3549298205977452]
	TIME [epoch: 4.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21315822835642634		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.21315822835642634 | validation: 0.35633761107737205]
	TIME [epoch: 4.22 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24507947987105977		[learning rate: 0.007606]
	Learning Rate: 0.00760596
	LOSS [training: 0.24507947987105977 | validation: 0.3423108837366833]
	TIME [epoch: 4.18 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22621558581892484		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.22621558581892484 | validation: 0.3300012534884695]
	TIME [epoch: 4.18 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2028336712057826		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2028336712057826 | validation: 0.3512403368167939]
	TIME [epoch: 4.17 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2238108542456411		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.2238108542456411 | validation: 0.400181294443819]
	TIME [epoch: 4.17 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21876964537162644		[learning rate: 0.0074724]
	Learning Rate: 0.00747242
	LOSS [training: 0.21876964537162644 | validation: 0.32584392238858373]
	TIME [epoch: 4.18 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1976673691294666		[learning rate: 0.0074394]
	Learning Rate: 0.00743941
	LOSS [training: 0.1976673691294666 | validation: 0.3376325224440343]
	TIME [epoch: 4.18 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20383234327613597		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.20383234327613597 | validation: 0.3358876327574681]
	TIME [epoch: 4.19 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20410039670645932		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.20410039670645932 | validation: 0.3405401472808568]
	TIME [epoch: 4.18 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24688714314756827		[learning rate: 0.0073412]
	Learning Rate: 0.00734124
	LOSS [training: 0.24688714314756827 | validation: 0.4063850480228042]
	TIME [epoch: 4.17 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23123480424160472		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.23123480424160472 | validation: 0.4321568364326342]
	TIME [epoch: 4.18 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33036345290466984		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.33036345290466984 | validation: 0.40544086965685366]
	TIME [epoch: 4.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2417445371673595		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.2417445371673595 | validation: 0.30829487231676633]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2241118608403272		[learning rate: 0.0072124]
	Learning Rate: 0.00721235
	LOSS [training: 0.2241118608403272 | validation: 0.3433679597352933]
	TIME [epoch: 4.18 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19963070223953594		[learning rate: 0.0071805]
	Learning Rate: 0.00718049
	LOSS [training: 0.19963070223953594 | validation: 0.32327819489022747]
	TIME [epoch: 4.18 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2100396388678937		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.2100396388678937 | validation: 0.30861394503666284]
	TIME [epoch: 4.18 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22338863214709942		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.22338863214709942 | validation: 0.474199588252791]
	TIME [epoch: 4.17 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2162894589577895		[learning rate: 0.0070857]
	Learning Rate: 0.00708573
	LOSS [training: 0.2162894589577895 | validation: 0.36958822672146396]
	TIME [epoch: 4.17 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2168361467857609		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.2168361467857609 | validation: 0.3556973765744845]
	TIME [epoch: 4.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3436093057188798		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.3436093057188798 | validation: 0.45633271500235195]
	TIME [epoch: 4.17 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2955132242400664		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.2955132242400664 | validation: 0.4382551841340043]
	TIME [epoch: 4.17 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2660748386459513		[learning rate: 0.0069613]
	Learning Rate: 0.00696133
	LOSS [training: 0.2660748386459513 | validation: 0.3884872757405234]
	TIME [epoch: 4.18 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2463998315050912		[learning rate: 0.0069306]
	Learning Rate: 0.00693057
	LOSS [training: 0.2463998315050912 | validation: 0.4419406311500167]
	TIME [epoch: 4.18 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21879286092495867		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.21879286092495867 | validation: 0.3796009916348845]
	TIME [epoch: 4.21 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19055503231875698		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.19055503231875698 | validation: 0.3555085415167601]
	TIME [epoch: 4.21 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.200573608140395		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.200573608140395 | validation: 0.44493284844215825]
	TIME [epoch: 4.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2740671905367288		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.2740671905367288 | validation: 0.38117706792675854]
	TIME [epoch: 4.18 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2236722170876347		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2236722170876347 | validation: 0.35382252625490584]
	TIME [epoch: 4.2 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21982619590014618		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.21982619590014618 | validation: 0.33680887916206986]
	TIME [epoch: 4.2 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2336701901025741		[learning rate: 0.006719]
	Learning Rate: 0.00671905
	LOSS [training: 0.2336701901025741 | validation: 0.37768061407403664]
	TIME [epoch: 4.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23367857977046774		[learning rate: 0.0066894]
	Learning Rate: 0.00668936
	LOSS [training: 0.23367857977046774 | validation: 0.3391579460001447]
	TIME [epoch: 4.19 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2298329260177466		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.2298329260177466 | validation: 0.358338709010057]
	TIME [epoch: 4.19 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22817587976201065		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.22817587976201065 | validation: 0.37347996606112316]
	TIME [epoch: 4.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22967541693854257		[learning rate: 0.0066011]
	Learning Rate: 0.00660109
	LOSS [training: 0.22967541693854257 | validation: 0.33814044645877217]
	TIME [epoch: 4.18 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2154233957265109		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.2154233957265109 | validation: 0.3417673946424441]
	TIME [epoch: 4.19 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2216836927343302		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.2216836927343302 | validation: 0.37960642038265424]
	TIME [epoch: 4.18 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.232731728450175		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.232731728450175 | validation: 0.39108538820377464]
	TIME [epoch: 4.18 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2273541200400248		[learning rate: 0.0064852]
	Learning Rate: 0.00648519
	LOSS [training: 0.2273541200400248 | validation: 0.4243571489282244]
	TIME [epoch: 4.18 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22943497847162844		[learning rate: 0.0064565]
	Learning Rate: 0.00645654
	LOSS [training: 0.22943497847162844 | validation: 0.38661052710001137]
	TIME [epoch: 4.18 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2330924597106519		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.2330924597106519 | validation: 0.39344665033999876]
	TIME [epoch: 4.21 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2638363269808788		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.2638363269808788 | validation: 0.37699749091410406]
	TIME [epoch: 4.18 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22410251343592663		[learning rate: 0.0063713]
	Learning Rate: 0.00637134
	LOSS [training: 0.22410251343592663 | validation: 0.3742483895964761]
	TIME [epoch: 4.21 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24202172815437178		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.24202172815437178 | validation: 0.3636208903871164]
	TIME [epoch: 4.21 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24224221897248585		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.24224221897248585 | validation: 0.41550681114969734]
	TIME [epoch: 4.2 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2778410426481853		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.2778410426481853 | validation: 0.4387840586083959]
	TIME [epoch: 4.19 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2665932867248541		[learning rate: 0.0062595]
	Learning Rate: 0.00625948
	LOSS [training: 0.2665932867248541 | validation: 0.3486492131501457]
	TIME [epoch: 4.17 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2313379058574992		[learning rate: 0.0062318]
	Learning Rate: 0.00623183
	LOSS [training: 0.2313379058574992 | validation: 0.3478648014896868]
	TIME [epoch: 4.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2453383875483841		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.2453383875483841 | validation: 0.35580699098776314]
	TIME [epoch: 4.18 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22059605648648564		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.22059605648648564 | validation: 0.4169272499417479]
	TIME [epoch: 4.17 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24191762713473644		[learning rate: 0.0061496]
	Learning Rate: 0.00614959
	LOSS [training: 0.24191762713473644 | validation: 0.48720358706132744]
	TIME [epoch: 4.17 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2949253644022728		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.2949253644022728 | validation: 0.37808185750255413]
	TIME [epoch: 4.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23456210714560086		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.23456210714560086 | validation: 0.375245784904047]
	TIME [epoch: 4.17 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20358281402748246		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.20358281402748246 | validation: 0.4169955318211486]
	TIME [epoch: 4.18 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21224877802655123		[learning rate: 0.0060416]
	Learning Rate: 0.00604163
	LOSS [training: 0.21224877802655123 | validation: 0.357631660765839]
	TIME [epoch: 4.17 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22029068990752174		[learning rate: 0.0060149]
	Learning Rate: 0.00601493
	LOSS [training: 0.22029068990752174 | validation: 0.4203763520504174]
	TIME [epoch: 4.2 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2737844630649132		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.2737844630649132 | validation: 0.3778118141019907]
	TIME [epoch: 4.18 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24199490369402227		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.24199490369402227 | validation: 0.3833198986139463]
	TIME [epoch: 4.18 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24461633532081076		[learning rate: 0.0059356]
	Learning Rate: 0.00593556
	LOSS [training: 0.24461633532081076 | validation: 0.3609401874081207]
	TIME [epoch: 4.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25897692566836783		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.25897692566836783 | validation: 0.3785023207738083]
	TIME [epoch: 4.18 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3055472906241462		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.3055472906241462 | validation: 0.3777230018738212]
	TIME [epoch: 4.18 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2702806985435541		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.2702806985435541 | validation: 0.4046868665118345]
	TIME [epoch: 4.17 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2623311664721946		[learning rate: 0.0058314]
	Learning Rate: 0.00583135
	LOSS [training: 0.2623311664721946 | validation: 0.4300248276102798]
	TIME [epoch: 4.18 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26919781656455355		[learning rate: 0.0058056]
	Learning Rate: 0.00580559
	LOSS [training: 0.26919781656455355 | validation: 0.4415498617436673]
	TIME [epoch: 4.17 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26607670742839606		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.26607670742839606 | validation: 0.464277096923554]
	TIME [epoch: 4.17 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2692031720739566		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.2692031720739566 | validation: 0.4529172651941819]
	TIME [epoch: 4.19 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2917008407083138		[learning rate: 0.005729]
	Learning Rate: 0.00572897
	LOSS [training: 0.2917008407083138 | validation: 0.42356112233018617]
	TIME [epoch: 4.18 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28779222903372403		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.28779222903372403 | validation: 0.4001035491711659]
	TIME [epoch: 4.17 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3169697022790716		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.3169697022790716 | validation: 0.42423118140831373]
	TIME [epoch: 4.18 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3091036312247962		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.3091036312247962 | validation: 0.44602329051803274]
	TIME [epoch: 4.18 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30288030804827837		[learning rate: 0.0056284]
	Learning Rate: 0.0056284
	LOSS [training: 0.30288030804827837 | validation: 0.4411773775506081]
	TIME [epoch: 4.18 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2976472757939065		[learning rate: 0.0056035]
	Learning Rate: 0.00560353
	LOSS [training: 0.2976472757939065 | validation: 0.4909045238676004]
	TIME [epoch: 4.17 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31690306955118175		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.31690306955118175 | validation: 0.4351502756365778]
	TIME [epoch: 4.18 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3048591774763326		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.3048591774763326 | validation: 0.41550855099031747]
	TIME [epoch: 4.19 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29215480191013116		[learning rate: 0.0055296]
	Learning Rate: 0.00552958
	LOSS [training: 0.29215480191013116 | validation: 0.45250552708522307]
	TIME [epoch: 4.19 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3064307047455578		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.3064307047455578 | validation: 0.5766076214567962]
	TIME [epoch: 4.19 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36520479246016085		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.36520479246016085 | validation: 0.4850882735791955]
	TIME [epoch: 4.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3105975512789166		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.3105975512789166 | validation: 0.5045332164669919]
	TIME [epoch: 4.19 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3546525311633572		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.3546525311633572 | validation: 0.4863320527709107]
	TIME [epoch: 4.18 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.361493055200928		[learning rate: 0.0054085]
	Learning Rate: 0.0054085
	LOSS [training: 0.361493055200928 | validation: 0.6262933956548238]
	TIME [epoch: 4.18 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4650954751744597		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.4650954751744597 | validation: 0.5241507665167638]
	TIME [epoch: 4.17 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43186412722671796		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.43186412722671796 | validation: 0.5085248033942035]
	TIME [epoch: 4.17 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.377224196000871		[learning rate: 0.0053371]
	Learning Rate: 0.00533713
	LOSS [training: 0.377224196000871 | validation: 0.47293989829534727]
	TIME [epoch: 4.22 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34796580542107347		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.34796580542107347 | validation: 0.4324347975624219]
	TIME [epoch: 4.22 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35065377739161274		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.35065377739161274 | validation: 0.45779665400941455]
	TIME [epoch: 4.21 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3521114063379403		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.3521114063379403 | validation: 0.4474040704707063]
	TIME [epoch: 4.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3200425578389713		[learning rate: 0.0052434]
	Learning Rate: 0.00524343
	LOSS [training: 0.3200425578389713 | validation: 0.4236072276957267]
	TIME [epoch: 4.21 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3081220601031882		[learning rate: 0.0052203]
	Learning Rate: 0.00522026
	LOSS [training: 0.3081220601031882 | validation: 0.42708091314268387]
	TIME [epoch: 4.21 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3332542861615434		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.3332542861615434 | validation: 0.48556646242346674]
	TIME [epoch: 4.22 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3351563557361076		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.3351563557361076 | validation: 0.4796405347949523]
	TIME [epoch: 4.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3406057409567434		[learning rate: 0.0051514]
	Learning Rate: 0.00515137
	LOSS [training: 0.3406057409567434 | validation: 0.46751221659880315]
	TIME [epoch: 4.21 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3266133006541733		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.3266133006541733 | validation: 0.4396590631756167]
	TIME [epoch: 4.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29156258691851067		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.29156258691851067 | validation: 0.42896064023653324]
	TIME [epoch: 4.19 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2861542900454356		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.2861542900454356 | validation: 0.40027093386706336]
	TIME [epoch: 4.19 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3440999305243064		[learning rate: 0.0050609]
	Learning Rate: 0.00506093
	LOSS [training: 0.3440999305243064 | validation: 0.5327847194620194]
	TIME [epoch: 4.19 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3661334340664785		[learning rate: 0.0050386]
	Learning Rate: 0.00503857
	LOSS [training: 0.3661334340664785 | validation: 0.4792198875937927]
	TIME [epoch: 4.17 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36774605246762754		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.36774605246762754 | validation: 0.480181902866104]
	TIME [epoch: 4.18 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3444041159922536		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.3444041159922536 | validation: 0.47338119777160104]
	TIME [epoch: 4.18 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34517632953669475		[learning rate: 0.0049721]
	Learning Rate: 0.00497208
	LOSS [training: 0.34517632953669475 | validation: 0.48727133398399536]
	TIME [epoch: 4.22 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36223557256837713		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.36223557256837713 | validation: 0.4799105913746007]
	TIME [epoch: 4.18 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33897082326882727		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.33897082326882727 | validation: 0.43886211180087964]
	TIME [epoch: 4.18 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30361441340146456		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.30361441340146456 | validation: 0.47564297005437245]
	TIME [epoch: 4.18 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3392124424183781		[learning rate: 0.0048848]
	Learning Rate: 0.00488479
	LOSS [training: 0.3392124424183781 | validation: 0.47010434057203776]
	TIME [epoch: 4.21 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32845037838031227		[learning rate: 0.0048632]
	Learning Rate: 0.00486321
	LOSS [training: 0.32845037838031227 | validation: 0.5067375010992597]
	TIME [epoch: 4.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34267827772740633		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.34267827772740633 | validation: 0.46182935644131334]
	TIME [epoch: 4.21 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32614837595662594		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.32614837595662594 | validation: 0.5264127926699232]
	TIME [epoch: 4.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3416838547719213		[learning rate: 0.004799]
	Learning Rate: 0.00479903
	LOSS [training: 0.3416838547719213 | validation: 0.48626733361511576]
	TIME [epoch: 4.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3354821233185593		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.3354821233185593 | validation: 0.531844980850182]
	TIME [epoch: 4.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36886140853993293		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.36886140853993293 | validation: 0.506706497229392]
	TIME [epoch: 4.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37762919104320547		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.37762919104320547 | validation: 0.5074281509091325]
	TIME [epoch: 4.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3755232619107954		[learning rate: 0.0047148]
	Learning Rate: 0.00471478
	LOSS [training: 0.3755232619107954 | validation: 0.5144761167597729]
	TIME [epoch: 4.19 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32403378309617575		[learning rate: 0.0046939]
	Learning Rate: 0.00469395
	LOSS [training: 0.32403378309617575 | validation: 0.4494401650944183]
	TIME [epoch: 4.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3104577652394107		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.3104577652394107 | validation: 0.471804663184595]
	TIME [epoch: 4.19 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31459293211972483		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.31459293211972483 | validation: 0.46940988736768197]
	TIME [epoch: 4.18 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34106503323730253		[learning rate: 0.004632]
	Learning Rate: 0.00463201
	LOSS [training: 0.34106503323730253 | validation: 0.5917640285262729]
	TIME [epoch: 4.17 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35119612197388184		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.35119612197388184 | validation: 0.5050841423239605]
	TIME [epoch: 4.18 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32561739313988086		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.32561739313988086 | validation: 0.5032378543711449]
	TIME [epoch: 4.18 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3487320380121978		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3487320380121978 | validation: 0.5335674852284733]
	TIME [epoch: 4.17 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32815638158562066		[learning rate: 0.0045507]
	Learning Rate: 0.00455069
	LOSS [training: 0.32815638158562066 | validation: 0.4566148424438264]
	TIME [epoch: 4.17 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33652735945052836		[learning rate: 0.0045306]
	Learning Rate: 0.00453058
	LOSS [training: 0.33652735945052836 | validation: 0.4715087385591059]
	TIME [epoch: 4.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3181472390910976		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.3181472390910976 | validation: 0.4959941656327861]
	TIME [epoch: 4.19 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3082870874435596		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.3082870874435596 | validation: 0.4627600877495064]
	TIME [epoch: 4.17 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30746974545611305		[learning rate: 0.0044708]
	Learning Rate: 0.00447079
	LOSS [training: 0.30746974545611305 | validation: 0.46571567529030444]
	TIME [epoch: 4.18 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28963311634853073		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.28963311634853073 | validation: 0.45948822746449286]
	TIME [epoch: 4.17 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.303038882205534		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.303038882205534 | validation: 0.534992649269056]
	TIME [epoch: 4.18 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32170968149518286		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.32170968149518286 | validation: 0.4617723095876594]
	TIME [epoch: 4.21 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2832717454162362		[learning rate: 0.0043923]
	Learning Rate: 0.0043923
	LOSS [training: 0.2832717454162362 | validation: 0.43027800010834855]
	TIME [epoch: 4.18 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2612693194509884		[learning rate: 0.0043729]
	Learning Rate: 0.0043729
	LOSS [training: 0.2612693194509884 | validation: 0.4622481094548846]
	TIME [epoch: 4.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27261919109939187		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.27261919109939187 | validation: 0.4489042888358663]
	TIME [epoch: 4.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27659437041780005		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.27659437041780005 | validation: 0.4390923666625817]
	TIME [epoch: 4.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2715043690840716		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.2715043690840716 | validation: 0.43073079201816467]
	TIME [epoch: 4.19 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25480797989348675		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.25480797989348675 | validation: 0.4422974050740571]
	TIME [epoch: 4.21 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2633915947874039		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.2633915947874039 | validation: 0.41475411313650346]
	TIME [epoch: 4.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.259741930084262		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.259741930084262 | validation: 0.3966346339208178]
	TIME [epoch: 4.19 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23566538778505902		[learning rate: 0.0042394]
	Learning Rate: 0.00423943
	LOSS [training: 0.23566538778505902 | validation: 0.4199468868867788]
	TIME [epoch: 4.19 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24365977428555968		[learning rate: 0.0042207]
	Learning Rate: 0.0042207
	LOSS [training: 0.24365977428555968 | validation: 0.40115032056165756]
	TIME [epoch: 4.19 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2349232831246098		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.2349232831246098 | validation: 0.39522097088832997]
	TIME [epoch: 4.18 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20932116455313832		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.20932116455313832 | validation: 0.3579684819083698]
	TIME [epoch: 4.18 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2355627454339902		[learning rate: 0.004165]
	Learning Rate: 0.004165
	LOSS [training: 0.2355627454339902 | validation: 0.3697384806239774]
	TIME [epoch: 4.18 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.222616737181034		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.222616737181034 | validation: 0.3615987522764505]
	TIME [epoch: 4.17 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2487964334063378		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.2487964334063378 | validation: 0.3824419117458341]
	TIME [epoch: 4.22 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2139008148996237		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.2139008148996237 | validation: 0.4476755178794234]
	TIME [epoch: 4.22 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26052810164012047		[learning rate: 0.0040919]
	Learning Rate: 0.00409188
	LOSS [training: 0.26052810164012047 | validation: 0.3824973010890789]
	TIME [epoch: 4.22 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21681814648129505		[learning rate: 0.0040738]
	Learning Rate: 0.0040738
	LOSS [training: 0.21681814648129505 | validation: 0.3700226383021201]
	TIME [epoch: 4.21 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22603246448559608		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.22603246448559608 | validation: 0.3879998282286657]
	TIME [epoch: 4.19 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22301670392752682		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.22301670392752682 | validation: 0.3658037875325339]
	TIME [epoch: 4.21 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21919277939924284		[learning rate: 0.00402]
	Learning Rate: 0.00402004
	LOSS [training: 0.21919277939924284 | validation: 0.3748591512130005]
	TIME [epoch: 4.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23238309097301246		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 0.23238309097301246 | validation: 0.3621187000075275]
	TIME [epoch: 4.21 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22951222592959422		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.22951222592959422 | validation: 0.37357472990053814]
	TIME [epoch: 4.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2132086533591769		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2132086533591769 | validation: 0.38169668195815387]
	TIME [epoch: 4.19 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4062304398828064		[learning rate: 0.0039495]
	Learning Rate: 0.00394947
	LOSS [training: 0.4062304398828064 | validation: 0.5909107262865717]
	TIME [epoch: 4.19 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3104603064601187		[learning rate: 0.003932]
	Learning Rate: 0.00393202
	LOSS [training: 0.3104603064601187 | validation: 0.3743420034841701]
	TIME [epoch: 4.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22736000288857064		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.22736000288857064 | validation: 0.3722177100194054]
	TIME [epoch: 4.19 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2015137605224969		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.2015137605224969 | validation: 0.3628343412771831]
	TIME [epoch: 4.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20517941371727302		[learning rate: 0.0038801]
	Learning Rate: 0.00388013
	LOSS [training: 0.20517941371727302 | validation: 0.40918924929194284]
	TIME [epoch: 4.19 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21573734297219743		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 0.21573734297219743 | validation: 0.35306483978398007]
	TIME [epoch: 4.19 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21465179951996224		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.21465179951996224 | validation: 0.36162403490737394]
	TIME [epoch: 4.18 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2041762281526836		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.2041762281526836 | validation: 0.3722647921403305]
	TIME [epoch: 4.18 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2027029341409728		[learning rate: 0.003812]
	Learning Rate: 0.00381201
	LOSS [training: 0.2027029341409728 | validation: 0.36204354257165594]
	TIME [epoch: 4.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20803020744074763		[learning rate: 0.0037952]
	Learning Rate: 0.00379517
	LOSS [training: 0.20803020744074763 | validation: 0.45030605246200006]
	TIME [epoch: 4.23 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21926052534426002		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.21926052534426002 | validation: 0.368411487846582]
	TIME [epoch: 4.22 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18970654923760435		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.18970654923760435 | validation: 0.35847370804482515]
	TIME [epoch: 4.21 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.209026502130311		[learning rate: 0.0037451]
	Learning Rate: 0.00374508
	LOSS [training: 0.209026502130311 | validation: 0.38821192220843165]
	TIME [epoch: 4.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7999940326335693		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 0.7999940326335693 | validation: 1.1630349389736847]
	TIME [epoch: 4.18 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9899416947944898		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.9899416947944898 | validation: 0.7371942734212791]
	TIME [epoch: 4.21 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6459454015062686		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6459454015062686 | validation: 0.6713450914887891]
	TIME [epoch: 4.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6068324177039912		[learning rate: 0.0036793]
	Learning Rate: 0.00367933
	LOSS [training: 0.6068324177039912 | validation: 0.6479997617088653]
	TIME [epoch: 4.18 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5860792310530416		[learning rate: 0.0036631]
	Learning Rate: 0.00366308
	LOSS [training: 0.5860792310530416 | validation: 0.5932654731476065]
	TIME [epoch: 4.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5498080562374474		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.5498080562374474 | validation: 0.6027974740828046]
	TIME [epoch: 4.17 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5435912199946984		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5435912199946984 | validation: 0.5809535574736798]
	TIME [epoch: 4.19 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5318916702967745		[learning rate: 0.0036147]
	Learning Rate: 0.00361474
	LOSS [training: 0.5318916702967745 | validation: 0.5129097001289171]
	TIME [epoch: 4.17 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48795635393621917		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 0.48795635393621917 | validation: 0.5314292319312]
	TIME [epoch: 4.19 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4695742874940166		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.4695742874940166 | validation: 0.5029846045763415]
	TIME [epoch: 4.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.454082061911401		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.454082061911401 | validation: 0.5086029626406426]
	TIME [epoch: 4.18 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4612273095816848		[learning rate: 0.0035513]
	Learning Rate: 0.00355128
	LOSS [training: 0.4612273095816848 | validation: 0.4683989631414722]
	TIME [epoch: 4.19 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45067804623489244		[learning rate: 0.0035356]
	Learning Rate: 0.00353559
	LOSS [training: 0.45067804623489244 | validation: 0.4823533949041734]
	TIME [epoch: 4.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41225129712099073		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.41225129712099073 | validation: 0.43564903796682886]
	TIME [epoch: 4.19 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3611462391461512		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.3611462391461512 | validation: 0.3852656290820665]
	TIME [epoch: 4.17 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26358090408147533		[learning rate: 0.0034889]
	Learning Rate: 0.00348893
	LOSS [training: 0.26358090408147533 | validation: 0.3364914586356782]
	TIME [epoch: 4.22 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20756315056711477		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 0.20756315056711477 | validation: 0.33271595472595117]
	TIME [epoch: 4.17 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22321199798373584		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.22321199798373584 | validation: 0.32945403975690446]
	TIME [epoch: 4.19 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21040056692631315		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.21040056692631315 | validation: 0.37939488668958865]
	TIME [epoch: 4.21 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23726155500062956		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.23726155500062956 | validation: 0.3164818417637062]
	TIME [epoch: 4.17 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2010972770478771		[learning rate: 0.0034125]
	Learning Rate: 0.00341253
	LOSS [training: 0.2010972770478771 | validation: 0.32737863165711895]
	TIME [epoch: 4.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19385465610981234		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.19385465610981234 | validation: 0.33146061525511883]
	TIME [epoch: 4.21 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22144473098745956		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.22144473098745956 | validation: 0.3270655791556482]
	TIME [epoch: 4.21 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19185620395547728		[learning rate: 0.0033675]
	Learning Rate: 0.0033675
	LOSS [training: 0.19185620395547728 | validation: 0.35562838692999116]
	TIME [epoch: 4.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19554195549532533		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 0.19554195549532533 | validation: 0.339749597353856]
	TIME [epoch: 4.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21650418557274054		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.21650418557274054 | validation: 0.3391931533988216]
	TIME [epoch: 4.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1991049498210531		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.1991049498210531 | validation: 0.3322055572745231]
	TIME [epoch: 4.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2102723500090519		[learning rate: 0.0033084]
	Learning Rate: 0.00330838
	LOSS [training: 0.2102723500090519 | validation: 0.3536689136414419]
	TIME [epoch: 4.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2054499371031576		[learning rate: 0.0032938]
	Learning Rate: 0.00329376
	LOSS [training: 0.2054499371031576 | validation: 0.32461211532058365]
	TIME [epoch: 4.17 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20332308137011354		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.20332308137011354 | validation: 0.3564775374425404]
	TIME [epoch: 4.19 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19171487553010497		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.19171487553010497 | validation: 0.701510000266304]
	TIME [epoch: 4.18 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4224120777601728		[learning rate: 0.0032503]
	Learning Rate: 0.0032503
	LOSS [training: 0.4224120777601728 | validation: 0.37415092682787104]
	TIME [epoch: 4.19 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19986113551786205		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 0.19986113551786205 | validation: 0.32867015565847474]
	TIME [epoch: 4.18 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26194827522127423		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.26194827522127423 | validation: 0.7016797797822655]
	TIME [epoch: 4.19 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6001152076437966		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.6001152076437966 | validation: 0.5430246456004137]
	TIME [epoch: 4.19 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37288306222716955		[learning rate: 0.0031932]
	Learning Rate: 0.00319323
	LOSS [training: 0.37288306222716955 | validation: 0.382781470180255]
	TIME [epoch: 4.22 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24300152747287634		[learning rate: 0.0031791]
	Learning Rate: 0.00317913
	LOSS [training: 0.24300152747287634 | validation: 0.3607164292277163]
	TIME [epoch: 4.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21240529816109546		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.21240529816109546 | validation: 0.35504033742390134]
	TIME [epoch: 4.21 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19603018455881288		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.19603018455881288 | validation: 0.33211889132078626]
	TIME [epoch: 4.19 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18337837598763437		[learning rate: 0.0031372]
	Learning Rate: 0.00313717
	LOSS [training: 0.18337837598763437 | validation: 0.4363482171850634]
	TIME [epoch: 4.21 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4405508504977793		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 0.4405508504977793 | validation: 0.6101131465921572]
	TIME [epoch: 4.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4900726696913217		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.4900726696913217 | validation: 0.6239440997206661]
	TIME [epoch: 4.18 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48969851399149766		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.48969851399149766 | validation: 0.6040842624385945]
	TIME [epoch: 4.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44836067456575684		[learning rate: 0.0030821]
	Learning Rate: 0.0030821
	LOSS [training: 0.44836067456575684 | validation: 0.5682559261764027]
	TIME [epoch: 4.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4471756279937359		[learning rate: 0.0030685]
	Learning Rate: 0.00306848
	LOSS [training: 0.4471756279937359 | validation: 0.5631826852678191]
	TIME [epoch: 4.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42191989848203715		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.42191989848203715 | validation: 0.5299434432559971]
	TIME [epoch: 4.19 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42575689229117286		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.42575689229117286 | validation: 0.5313561547072707]
	TIME [epoch: 4.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42336040351884086		[learning rate: 0.003028]
	Learning Rate: 0.00302799
	LOSS [training: 0.42336040351884086 | validation: 0.5115114852176381]
	TIME [epoch: 4.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4023941528075133		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 0.4023941528075133 | validation: 0.6640273779339166]
	TIME [epoch: 4.19 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5790736146600494		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.5790736146600494 | validation: 0.8577986990350658]
	TIME [epoch: 4.18 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1196721351206083		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.1196721351206083 | validation: 1.2523473791981463]
	TIME [epoch: 4.18 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1910797492919047		[learning rate: 0.0029748]
	Learning Rate: 0.00297483
	LOSS [training: 1.1910797492919047 | validation: 0.9580564741198698]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v6_20240712_121955/states/model_facs_v2_dec2b_2dpca_v6_314.pth
Halted early. No improvement in validation loss for 200 epochs.
Finished training in 1364.046 seconds.
