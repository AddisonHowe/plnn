Args:
Namespace(name='model_facs_dec2b_2dnmf_v1', outdir='out/model_training/model_facs_dec2b_2dnmf_v1', training_data='data/training_data/facs/nmf/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/training', validation_data='data/training_data/facs/nmf/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=5, patience=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.001, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 703088495

Training model...

Saving initial model state to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7797815211047325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7797815211047325 | validation: 1.431503801108987]
	TIME [epoch: 44.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8900883554813208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8900883554813208 | validation: 1.91174596235411]
	TIME [epoch: 20.9 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0691062026422309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0691062026422309 | validation: 1.4575673089776369]
	TIME [epoch: 20.9 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7370096210317657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7370096210317657 | validation: 0.9427442235148149]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7437561872562798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437561872562798 | validation: 0.5218694828697293]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6275880621219031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6275880621219031 | validation: 0.7845806693394138]
	TIME [epoch: 20.9 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5513879350818163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5513879350818163 | validation: 0.48738406691974756]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5525231065077563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5525231065077563 | validation: 0.44279869601850236]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36747990640509964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36747990640509964 | validation: 0.4833559114798443]
	TIME [epoch: 20.9 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5445507309623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445507309623 | validation: 0.5086551694320949]
	TIME [epoch: 20.9 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44548145874783646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44548145874783646 | validation: 0.6835757419099288]
	TIME [epoch: 20.9 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48512872627345016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48512872627345016 | validation: 0.6866220527651674]
	TIME [epoch: 20.9 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3960705478535246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3960705478535246 | validation: 0.362439497776881]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4439565322253727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4439565322253727 | validation: 0.34310954682266237]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45756133663756743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45756133663756743 | validation: 0.4021055152093133]
	TIME [epoch: 20.9 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3992817592323739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3992817592323739 | validation: 0.3248471924330964]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2919012816567834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2919012816567834 | validation: 0.27006635085781766]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2658453084296337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2658453084296337 | validation: 0.2855778558853632]
	TIME [epoch: 20.9 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25950677712984627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25950677712984627 | validation: 0.3020899469819706]
	TIME [epoch: 20.9 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23891625269466382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23891625269466382 | validation: 0.24773778333582014]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21241642415677756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21241642415677756 | validation: 0.2561888591012027]
	TIME [epoch: 20.9 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21522842823876515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21522842823876515 | validation: 0.195484501245589]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17918299337953253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17918299337953253 | validation: 0.2372355093148127]
	TIME [epoch: 20.9 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16574377340767749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16574377340767749 | validation: 0.1315023748789143]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.136295359134814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.136295359134814 | validation: 0.1162407206730199]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1134850438843931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1134850438843931 | validation: 0.10541584098319487]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14481034346976984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14481034346976984 | validation: 0.12433672179309624]
	TIME [epoch: 20.9 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13231463421547177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13231463421547177 | validation: 0.07758511297837531]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.06527397657662001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06527397657662001 | validation: 0.0831295978375028]
	TIME [epoch: 20.9 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.06654391459446692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06654391459446692 | validation: 0.05954102329620277]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.04950193063907059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04950193063907059 | validation: 0.07501060648106994]
	TIME [epoch: 20.9 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.06791762258316422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06791762258316422 | validation: 0.05218990678072875]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.03503543962693205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03503543962693205 | validation: 0.05005092808601966]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0370294973658343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0370294973658343 | validation: 0.039627502729567166]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.022985506904523566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022985506904523566 | validation: 0.037491112821490785]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.02030048592130915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02030048592130915 | validation: 0.044068996495016256]
	TIME [epoch: 20.9 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01965349782477329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01965349782477329 | validation: 0.027119580395743605]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014910474976837979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014910474976837979 | validation: 0.030614100074487262]
	TIME [epoch: 20.9 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01237422311940517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01237422311940517 | validation: 0.03564129597692154]
	TIME [epoch: 20.9 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.018264296122200242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018264296122200242 | validation: 0.031133593417360193]
	TIME [epoch: 20.9 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015827154255442503		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.015827154255442503 | validation: 0.02635802493094732]
	TIME [epoch: 20.9 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010699751713627427		[learning rate: 0.0099206]
	Learning Rate: 0.00992061
	LOSS [training: 0.010699751713627427 | validation: 0.023720254085130368]
	TIME [epoch: 21 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014462883989924874		[learning rate: 0.0098768]
	Learning Rate: 0.00987678
	LOSS [training: 0.014462883989924874 | validation: 0.027184859299399535]
	TIME [epoch: 20.9 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012496667434624893		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.012496667434624893 | validation: 0.03075074817068062]
	TIME [epoch: 20.9 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010903692854376978		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.010903692854376978 | validation: 0.027052788868494805]
	TIME [epoch: 20.9 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015683208314401488		[learning rate: 0.0097464]
	Learning Rate: 0.00974644
	LOSS [training: 0.015683208314401488 | validation: 0.027401359851548478]
	TIME [epoch: 20.9 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009373495386920876		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 0.009373495386920876 | validation: 0.034777001876106106]
	TIME [epoch: 20.9 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01220115371772611		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.01220115371772611 | validation: 0.029861051631650454]
	TIME [epoch: 20.9 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011486163119836577		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.011486163119836577 | validation: 0.03517386994220642]
	TIME [epoch: 20.9 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012899743270722719		[learning rate: 0.0095753]
	Learning Rate: 0.00957533
	LOSS [training: 0.012899743270722719 | validation: 0.03463417646626936]
	TIME [epoch: 20.9 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015161326694762878		[learning rate: 0.009533]
	Learning Rate: 0.00953303
	LOSS [training: 0.015161326694762878 | validation: 0.029438931315783765]
	TIME [epoch: 20.9 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010574166761070886		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.010574166761070886 | validation: 0.027083412934941008]
	TIME [epoch: 20.9 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009662403797422399		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.009662403797422399 | validation: 0.037772122566581245]
	TIME [epoch: 20.9 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.020124617047252442		[learning rate: 0.0094072]
	Learning Rate: 0.00940722
	LOSS [training: 0.020124617047252442 | validation: 0.02882201815453396]
	TIME [epoch: 20.9 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010054515851797004		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 0.010054515851797004 | validation: 0.03090103923881298]
	TIME [epoch: 20.9 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010319111882581491		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.010319111882581491 | validation: 0.02967722634145522]
	TIME [epoch: 20.9 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01252369069155391		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.01252369069155391 | validation: 0.02619517623322856]
	TIME [epoch: 20.9 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009217062630348018		[learning rate: 0.0092421]
	Learning Rate: 0.00924207
	LOSS [training: 0.009217062630348018 | validation: 0.02908760242546704]
	TIME [epoch: 20.9 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010168827594905557		[learning rate: 0.0092012]
	Learning Rate: 0.00920124
	LOSS [training: 0.010168827594905557 | validation: 0.03388973876760207]
	TIME [epoch: 20.9 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014136510142205395		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.014136510142205395 | validation: 0.02744585175169223]
	TIME [epoch: 20.9 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01517003762971659		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.01517003762971659 | validation: 0.024601130811400848]
	TIME [epoch: 20.9 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014021006528685829		[learning rate: 0.0090798]
	Learning Rate: 0.00907981
	LOSS [training: 0.014021006528685829 | validation: 0.030136612890627346]
	TIME [epoch: 20.9 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012662966000710296		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 0.012662966000710296 | validation: 0.025407963727486882]
	TIME [epoch: 20.9 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012515921555727639		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.012515921555727639 | validation: 0.029411072953432375]
	TIME [epoch: 20.9 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012373881888878745		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.012373881888878745 | validation: 0.0335412763719835]
	TIME [epoch: 20.9 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009897101461985903		[learning rate: 0.0089204]
	Learning Rate: 0.00892041
	LOSS [training: 0.009897101461985903 | validation: 0.030041599622285208]
	TIME [epoch: 20.9 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014536895635347494		[learning rate: 0.008881]
	Learning Rate: 0.00888099
	LOSS [training: 0.014536895635347494 | validation: 0.03473051424168958]
	TIME [epoch: 20.9 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015220893710419176		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.015220893710419176 | validation: 0.02939098992463245]
	TIME [epoch: 20.9 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.013609642595092638		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.013609642595092638 | validation: 0.02737284617866101]
	TIME [epoch: 20.9 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01192970463526844		[learning rate: 0.0087638]
	Learning Rate: 0.0087638
	LOSS [training: 0.01192970463526844 | validation: 0.03159328909432517]
	TIME [epoch: 20.9 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011817660062040191		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 0.011817660062040191 | validation: 0.03566827027481634]
	TIME [epoch: 20.9 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014546766938518432		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.014546766938518432 | validation: 0.03245252742797231]
	TIME [epoch: 20.9 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015102363321856726		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.015102363321856726 | validation: 0.0289413473411142]
	TIME [epoch: 20.9 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010521510877957092		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.010521510877957092 | validation: 0.03220130305688722]
	TIME [epoch: 20.9 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011495764147899533		[learning rate: 0.0085719]
	Learning Rate: 0.0085719
	LOSS [training: 0.011495764147899533 | validation: 0.026605301960995958]
	TIME [epoch: 20.9 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011491420005442739		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.011491420005442739 | validation: 0.033175472966397425]
	TIME [epoch: 20.9 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01243217873336749		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.01243217873336749 | validation: 0.035855218283403774]
	TIME [epoch: 20.9 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014012863536787868		[learning rate: 0.0084588]
	Learning Rate: 0.00845878
	LOSS [training: 0.014012863536787868 | validation: 0.029252835731022355]
	TIME [epoch: 20.9 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011487979601804695		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 0.011487979601804695 | validation: 0.029033759412421173]
	TIME [epoch: 20.9 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00859599048006297		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.00859599048006297 | validation: 0.026784037827741258]
	TIME [epoch: 20.9 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012574868982318197		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.012574868982318197 | validation: 0.028477780516154193]
	TIME [epoch: 20.9 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.015503531767420798		[learning rate: 0.0083103]
	Learning Rate: 0.00831027
	LOSS [training: 0.015503531767420798 | validation: 0.025333921976392018]
	TIME [epoch: 20.9 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014483268733573324		[learning rate: 0.0082736]
	Learning Rate: 0.00827356
	LOSS [training: 0.014483268733573324 | validation: 0.029714818135748836]
	TIME [epoch: 20.9 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011125181599381586		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.011125181599381586 | validation: 0.030427318729728262]
	TIME [epoch: 20.9 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.013174839153330295		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.013174839153330295 | validation: 0.03177367937501749]
	TIME [epoch: 20.9 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0125962169269282		[learning rate: 0.0081644]
	Learning Rate: 0.00816438
	LOSS [training: 0.0125962169269282 | validation: 0.028922740885082555]
	TIME [epoch: 20.9 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010766613798840067		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 0.010766613798840067 | validation: 0.03133679457495327]
	TIME [epoch: 20.9 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.014382633124737271		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.014382633124737271 | validation: 0.02849616463729523]
	TIME [epoch: 20.9 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008267504295293602		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.008267504295293602 | validation: 0.0318832248379179]
	TIME [epoch: 20.9 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01091895456213996		[learning rate: 0.008021]
	Learning Rate: 0.00802104
	LOSS [training: 0.01091895456213996 | validation: 0.024582601277918937]
	TIME [epoch: 20.9 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011278444363797848		[learning rate: 0.0079856]
	Learning Rate: 0.0079856
	LOSS [training: 0.011278444363797848 | validation: 0.027522085640510088]
	TIME [epoch: 20.9 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012781636370031912		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.012781636370031912 | validation: 0.02865107429530405]
	TIME [epoch: 20.9 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010492373673551332		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.010492373673551332 | validation: 0.028589650108502965]
	TIME [epoch: 20.9 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012075962014843636		[learning rate: 0.0078802]
	Learning Rate: 0.00788022
	LOSS [training: 0.012075962014843636 | validation: 0.03198504014407837]
	TIME [epoch: 20.9 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01142999783414446		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.01142999783414446 | validation: 0.030685611081486732]
	TIME [epoch: 20.9 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012353824584572353		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.012353824584572353 | validation: 0.02706411139220415]
	TIME [epoch: 20.9 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011535399566135052		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.011535399566135052 | validation: 0.03234430099219401]
	TIME [epoch: 20.9 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012281286835379065		[learning rate: 0.0077419]
	Learning Rate: 0.00774188
	LOSS [training: 0.012281286835379065 | validation: 0.027341020212404386]
	TIME [epoch: 20.9 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01190214539384445		[learning rate: 0.0077077]
	Learning Rate: 0.00770767
	LOSS [training: 0.01190214539384445 | validation: 0.027010789652391964]
	TIME [epoch: 20.9 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008593447043521288		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.008593447043521288 | validation: 0.028959090176345393]
	TIME [epoch: 20.9 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01143131731453706		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.01143131731453706 | validation: 0.029620230364489825]
	TIME [epoch: 20.9 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012705862756729257		[learning rate: 0.007606]
	Learning Rate: 0.00760596
	LOSS [training: 0.012705862756729257 | validation: 0.030626930348529747]
	TIME [epoch: 20.9 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.013950042858119977		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.013950042858119977 | validation: 0.030481204524643583]
	TIME [epoch: 20.9 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011563719140700672		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.011563719140700672 | validation: 0.028918825158830996]
	TIME [epoch: 20.9 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009998528933546153		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.009998528933546153 | validation: 0.028544740252056024]
	TIME [epoch: 20.9 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011247543702481076		[learning rate: 0.0074724]
	Learning Rate: 0.00747242
	LOSS [training: 0.011247543702481076 | validation: 0.029776012711555833]
	TIME [epoch: 20.9 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011566640795489662		[learning rate: 0.0074394]
	Learning Rate: 0.00743941
	LOSS [training: 0.011566640795489662 | validation: 0.025233897047599197]
	TIME [epoch: 20.9 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009349743665219932		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.009349743665219932 | validation: 0.029972200707815148]
	TIME [epoch: 20.9 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010373402973457263		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.010373402973457263 | validation: 0.02639238069730197]
	TIME [epoch: 20.9 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009061879218276423		[learning rate: 0.0073412]
	Learning Rate: 0.00734124
	LOSS [training: 0.009061879218276423 | validation: 0.02729240571990659]
	TIME [epoch: 20.9 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009618716046440703		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.009618716046440703 | validation: 0.02931179895111049]
	TIME [epoch: 20.9 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012229127842183374		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.012229127842183374 | validation: 0.028875451509128403]
	TIME [epoch: 20.9 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011706710051859625		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.011706710051859625 | validation: 0.030208612209922926]
	TIME [epoch: 20.9 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010673169239060618		[learning rate: 0.0072124]
	Learning Rate: 0.00721235
	LOSS [training: 0.010673169239060618 | validation: 0.030618459745143302]
	TIME [epoch: 20.9 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009068212677342		[learning rate: 0.0071805]
	Learning Rate: 0.00718049
	LOSS [training: 0.009068212677342 | validation: 0.030207545953229522]
	TIME [epoch: 20.9 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011640022366740204		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.011640022366740204 | validation: 0.02848757588872031]
	TIME [epoch: 20.9 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012998475270791787		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.012998475270791787 | validation: 0.0275083890816753]
	TIME [epoch: 20.9 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009095288738956193		[learning rate: 0.0070857]
	Learning Rate: 0.00708573
	LOSS [training: 0.009095288738956193 | validation: 0.026702707847687867]
	TIME [epoch: 20.9 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009214853156853237		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.009214853156853237 | validation: 0.0280991916420448]
	TIME [epoch: 20.9 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008578554257722597		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.008578554257722597 | validation: 0.025806170344326862]
	TIME [epoch: 20.9 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012060352871103875		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.012060352871103875 | validation: 0.028089888791299835]
	TIME [epoch: 20.9 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00875430675048241		[learning rate: 0.0069613]
	Learning Rate: 0.00696133
	LOSS [training: 0.00875430675048241 | validation: 0.02645507685432558]
	TIME [epoch: 20.9 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010827459681935905		[learning rate: 0.0069306]
	Learning Rate: 0.00693057
	LOSS [training: 0.010827459681935905 | validation: 0.03415828586901626]
	TIME [epoch: 20.9 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012080592904153555		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.012080592904153555 | validation: 0.028690388759747745]
	TIME [epoch: 20.9 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008834859856691194		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.008834859856691194 | validation: 0.028539222966959724]
	TIME [epoch: 20.9 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009834618473138353		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.009834618473138353 | validation: 0.028057015889004645]
	TIME [epoch: 20.9 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011018767293751655		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.011018767293751655 | validation: 0.028904250505173805]
	TIME [epoch: 20.9 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009009222792302204		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.009009222792302204 | validation: 0.03152412576493562]
	TIME [epoch: 20.9 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008787879736288029		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.008787879736288029 | validation: 0.027293364875674]
	TIME [epoch: 20.9 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010624894880577437		[learning rate: 0.006719]
	Learning Rate: 0.00671905
	LOSS [training: 0.010624894880577437 | validation: 0.028210380393795288]
	TIME [epoch: 20.9 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01057260452609531		[learning rate: 0.0066894]
	Learning Rate: 0.00668936
	LOSS [training: 0.01057260452609531 | validation: 0.027412563885233967]
	TIME [epoch: 20.9 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009380486160956214		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.009380486160956214 | validation: 0.0262315429240132]
	TIME [epoch: 20.9 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010657209341095316		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.010657209341095316 | validation: 0.029565360204736102]
	TIME [epoch: 20.9 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010163886195802304		[learning rate: 0.0066011]
	Learning Rate: 0.00660109
	LOSS [training: 0.010163886195802304 | validation: 0.02991545133280813]
	TIME [epoch: 20.9 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010577432186800628		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.010577432186800628 | validation: 0.029617782190280884]
	TIME [epoch: 20.9 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01228675344185462		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.01228675344185462 | validation: 0.026512380430794402]
	TIME [epoch: 20.9 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009373142895830676		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.009373142895830676 | validation: 0.0293037229268869]
	TIME [epoch: 20.9 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011594848606382328		[learning rate: 0.0064852]
	Learning Rate: 0.00648519
	LOSS [training: 0.011594848606382328 | validation: 0.027726722369481926]
	TIME [epoch: 20.9 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010723752857615782		[learning rate: 0.0064565]
	Learning Rate: 0.00645654
	LOSS [training: 0.010723752857615782 | validation: 0.02569407091274605]
	TIME [epoch: 20.9 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008892191392935792		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.008892191392935792 | validation: 0.029218035236296958]
	TIME [epoch: 20.9 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007707878838183161		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.007707878838183161 | validation: 0.028065829938119968]
	TIME [epoch: 20.9 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010613078612225699		[learning rate: 0.0063713]
	Learning Rate: 0.00637134
	LOSS [training: 0.010613078612225699 | validation: 0.03698152379450916]
	TIME [epoch: 20.9 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010169962583375863		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.010169962583375863 | validation: 0.0269239378403366]
	TIME [epoch: 20.9 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011117383203037578		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.011117383203037578 | validation: 0.0337012909477824]
	TIME [epoch: 20.9 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010084041507985905		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.010084041507985905 | validation: 0.02729972043911111]
	TIME [epoch: 20.9 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00878509755059612		[learning rate: 0.0062595]
	Learning Rate: 0.00625948
	LOSS [training: 0.00878509755059612 | validation: 0.029155220459519102]
	TIME [epoch: 20.9 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012286053541868727		[learning rate: 0.0062318]
	Learning Rate: 0.00623183
	LOSS [training: 0.012286053541868727 | validation: 0.030504183948419097]
	TIME [epoch: 20.9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010368348456478054		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.010368348456478054 | validation: 0.02828910561667777]
	TIME [epoch: 20.9 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007652942781004826		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.007652942781004826 | validation: 0.026872373235421115]
	TIME [epoch: 20.9 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011441668164622484		[learning rate: 0.0061496]
	Learning Rate: 0.00614959
	LOSS [training: 0.011441668164622484 | validation: 0.03372688324757401]
	TIME [epoch: 20.9 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011127996332141291		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.011127996332141291 | validation: 0.02775114298643071]
	TIME [epoch: 20.9 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01063163781145628		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.01063163781145628 | validation: 0.02696043292238619]
	TIME [epoch: 20.9 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008547183616223775		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.008547183616223775 | validation: 0.02721532060116834]
	TIME [epoch: 20.9 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010174859853349894		[learning rate: 0.0060416]
	Learning Rate: 0.00604163
	LOSS [training: 0.010174859853349894 | validation: 0.03165853641674761]
	TIME [epoch: 20.9 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010270992621783256		[learning rate: 0.0060149]
	Learning Rate: 0.00601493
	LOSS [training: 0.010270992621783256 | validation: 0.027703256651652786]
	TIME [epoch: 20.9 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009737541544197034		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.009737541544197034 | validation: 0.027583262942385202]
	TIME [epoch: 20.9 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008227251416143666		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.008227251416143666 | validation: 0.030265367923579124]
	TIME [epoch: 20.9 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009936747394952644		[learning rate: 0.0059356]
	Learning Rate: 0.00593556
	LOSS [training: 0.009936747394952644 | validation: 0.03004863973499743]
	TIME [epoch: 20.9 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012549998688832106		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.012549998688832106 | validation: 0.028060109527759766]
	TIME [epoch: 20.9 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00966090391261608		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.00966090391261608 | validation: 0.0245304685116031]
	TIME [epoch: 20.9 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009415663019658519		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.009415663019658519 | validation: 0.025843900626755834]
	TIME [epoch: 20.9 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011007532890840985		[learning rate: 0.0058314]
	Learning Rate: 0.00583135
	LOSS [training: 0.011007532890840985 | validation: 0.03379727425470789]
	TIME [epoch: 20.9 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01324802342669476		[learning rate: 0.0058056]
	Learning Rate: 0.00580559
	LOSS [training: 0.01324802342669476 | validation: 0.02743651104719425]
	TIME [epoch: 20.9 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009991348287728595		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.009991348287728595 | validation: 0.0283080018792506]
	TIME [epoch: 20.9 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009770628496692439		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.009770628496692439 | validation: 0.027096732571548897]
	TIME [epoch: 20.9 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009625831743216207		[learning rate: 0.005729]
	Learning Rate: 0.00572897
	LOSS [training: 0.009625831743216207 | validation: 0.02516090326321916]
	TIME [epoch: 20.9 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009899648387393187		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.009899648387393187 | validation: 0.025996311179166124]
	TIME [epoch: 20.9 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007787380317457191		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.007787380317457191 | validation: 0.030584062509206644]
	TIME [epoch: 20.9 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00888731430235331		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.00888731430235331 | validation: 0.029356687892834656]
	TIME [epoch: 20.9 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011099451311989857		[learning rate: 0.0056284]
	Learning Rate: 0.0056284
	LOSS [training: 0.011099451311989857 | validation: 0.028178343569670947]
	TIME [epoch: 20.9 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008328540500730935		[learning rate: 0.0056035]
	Learning Rate: 0.00560353
	LOSS [training: 0.008328540500730935 | validation: 0.02948699021096948]
	TIME [epoch: 20.9 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010481406834965349		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.010481406834965349 | validation: 0.030082175528461708]
	TIME [epoch: 20.9 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008535942556147083		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.008535942556147083 | validation: 0.030784515783190063]
	TIME [epoch: 20.9 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011196945647976686		[learning rate: 0.0055296]
	Learning Rate: 0.00552958
	LOSS [training: 0.011196945647976686 | validation: 0.02780683971410798]
	TIME [epoch: 20.9 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01104791636546977		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.01104791636546977 | validation: 0.033818606204174186]
	TIME [epoch: 20.9 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011024463290536982		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.011024463290536982 | validation: 0.03157565552542403]
	TIME [epoch: 20.9 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012555985973283912		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.012555985973283912 | validation: 0.027060071217909924]
	TIME [epoch: 20.9 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009645522891471817		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.009645522891471817 | validation: 0.030443516790161843]
	TIME [epoch: 20.9 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008903352207148465		[learning rate: 0.0054085]
	Learning Rate: 0.0054085
	LOSS [training: 0.008903352207148465 | validation: 0.02673050844846834]
	TIME [epoch: 20.9 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009361128970684774		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.009361128970684774 | validation: 0.029460843303777053]
	TIME [epoch: 20.9 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009441495650579923		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.009441495650579923 | validation: 0.03029423004397809]
	TIME [epoch: 20.9 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009286168797398258		[learning rate: 0.0053371]
	Learning Rate: 0.00533713
	LOSS [training: 0.009286168797398258 | validation: 0.030736919857623102]
	TIME [epoch: 20.9 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00907223547295041		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.00907223547295041 | validation: 0.02658462266593662]
	TIME [epoch: 20.9 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010308017085905443		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.010308017085905443 | validation: 0.027291427391540692]
	TIME [epoch: 20.9 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00876422734597818		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.00876422734597818 | validation: 0.0256462771883041]
	TIME [epoch: 20.9 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010539169605310672		[learning rate: 0.0052434]
	Learning Rate: 0.00524343
	LOSS [training: 0.010539169605310672 | validation: 0.028578105502546605]
	TIME [epoch: 20.9 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008779120808154125		[learning rate: 0.0052203]
	Learning Rate: 0.00522026
	LOSS [training: 0.008779120808154125 | validation: 0.02841778084988904]
	TIME [epoch: 20.9 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008526914168565583		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.008526914168565583 | validation: 0.03215649668588383]
	TIME [epoch: 20.9 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012304588717748791		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.012304588717748791 | validation: 0.02786549879548206]
	TIME [epoch: 20.9 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010738953073295199		[learning rate: 0.0051514]
	Learning Rate: 0.00515137
	LOSS [training: 0.010738953073295199 | validation: 0.030187162253296897]
	TIME [epoch: 20.9 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009946171542865253		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.009946171542865253 | validation: 0.026887278483893873]
	TIME [epoch: 20.9 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007952703660318883		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.007952703660318883 | validation: 0.02866744614263406]
	TIME [epoch: 20.9 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008986079966998446		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.008986079966998446 | validation: 0.027830077339826342]
	TIME [epoch: 20.9 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011103337618316745		[learning rate: 0.0050609]
	Learning Rate: 0.00506093
	LOSS [training: 0.011103337618316745 | validation: 0.029106656003599365]
	TIME [epoch: 20.9 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010995379275706183		[learning rate: 0.0050386]
	Learning Rate: 0.00503857
	LOSS [training: 0.010995379275706183 | validation: 0.0270845935402992]
	TIME [epoch: 20.9 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007778992243498721		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.007778992243498721 | validation: 0.028838353544882893]
	TIME [epoch: 20.9 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00790019192216136		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.00790019192216136 | validation: 0.029584983167896598]
	TIME [epoch: 20.9 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0077052910441272856		[learning rate: 0.0049721]
	Learning Rate: 0.00497208
	LOSS [training: 0.0077052910441272856 | validation: 0.027733615101458397]
	TIME [epoch: 20.9 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012102910831890242		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.012102910831890242 | validation: 0.029634864551857334]
	TIME [epoch: 20.9 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00984926524698301		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.00984926524698301 | validation: 0.029313084009461535]
	TIME [epoch: 20.9 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009533430375626003		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.009533430375626003 | validation: 0.03074049831288206]
	TIME [epoch: 20.9 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00860123463180138		[learning rate: 0.0048848]
	Learning Rate: 0.00488479
	LOSS [training: 0.00860123463180138 | validation: 0.029019076043411033]
	TIME [epoch: 20.9 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009571169128297816		[learning rate: 0.0048632]
	Learning Rate: 0.00486321
	LOSS [training: 0.009571169128297816 | validation: 0.027207558597757187]
	TIME [epoch: 20.9 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00849246148960873		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.00849246148960873 | validation: 0.028092217179505344]
	TIME [epoch: 20.9 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008824545954984773		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.008824545954984773 | validation: 0.02598131875305818]
	TIME [epoch: 20.9 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008283686051908209		[learning rate: 0.004799]
	Learning Rate: 0.00479903
	LOSS [training: 0.008283686051908209 | validation: 0.029643835597346048]
	TIME [epoch: 20.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009985479836947336		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.009985479836947336 | validation: 0.026492995448343756]
	TIME [epoch: 20.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01069673161134435		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.01069673161134435 | validation: 0.03153857223833592]
	TIME [epoch: 20.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007620266692203221		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.007620266692203221 | validation: 0.033459129261870585]
	TIME [epoch: 20.9 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011057086873512631		[learning rate: 0.0047148]
	Learning Rate: 0.00471478
	LOSS [training: 0.011057086873512631 | validation: 0.03768836416895995]
	TIME [epoch: 20.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009934721856522201		[learning rate: 0.0046939]
	Learning Rate: 0.00469395
	LOSS [training: 0.009934721856522201 | validation: 0.02935663147851735]
	TIME [epoch: 20.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011402160122305275		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.011402160122305275 | validation: 0.029836007669724763]
	TIME [epoch: 20.9 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008817474068767909		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.008817474068767909 | validation: 0.026592434528093123]
	TIME [epoch: 20.9 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010946623669770408		[learning rate: 0.004632]
	Learning Rate: 0.00463201
	LOSS [training: 0.010946623669770408 | validation: 0.027533203814548404]
	TIME [epoch: 20.9 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007834211128041231		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.007834211128041231 | validation: 0.02972640150931192]
	TIME [epoch: 20.9 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008022406020510827		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.008022406020510827 | validation: 0.026560657962162983]
	TIME [epoch: 20.9 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009370122706023793		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.009370122706023793 | validation: 0.027624547868338126]
	TIME [epoch: 20.9 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008309778933521032		[learning rate: 0.0045507]
	Learning Rate: 0.00455069
	LOSS [training: 0.008309778933521032 | validation: 0.028869711029134118]
	TIME [epoch: 20.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012768731575211328		[learning rate: 0.0045306]
	Learning Rate: 0.00453058
	LOSS [training: 0.012768731575211328 | validation: 0.030331830913504533]
	TIME [epoch: 20.9 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011637055638636023		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.011637055638636023 | validation: 0.026757249355043915]
	TIME [epoch: 20.9 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008459737090594107		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.008459737090594107 | validation: 0.02628035955815749]
	TIME [epoch: 20.9 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008366689522604349		[learning rate: 0.0044708]
	Learning Rate: 0.00447079
	LOSS [training: 0.008366689522604349 | validation: 0.03158977467149375]
	TIME [epoch: 20.9 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009431086845208535		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.009431086845208535 | validation: 0.028091058514373858]
	TIME [epoch: 20.9 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008383650769738944		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.008383650769738944 | validation: 0.029811498426126563]
	TIME [epoch: 20.9 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008501605490023906		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.008501605490023906 | validation: 0.03196808642554099]
	TIME [epoch: 20.9 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009395925674981726		[learning rate: 0.0043923]
	Learning Rate: 0.0043923
	LOSS [training: 0.009395925674981726 | validation: 0.027845540625361778]
	TIME [epoch: 20.9 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010167862068788591		[learning rate: 0.0043729]
	Learning Rate: 0.0043729
	LOSS [training: 0.010167862068788591 | validation: 0.030188337843982338]
	TIME [epoch: 20.9 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01068614530993836		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.01068614530993836 | validation: 0.0320309921116926]
	TIME [epoch: 20.9 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.007704059367758297		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.007704059367758297 | validation: 0.02778055406942146]
	TIME [epoch: 20.9 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009683779217888714		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.009683779217888714 | validation: 0.0288211219567327]
	TIME [epoch: 20.9 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00916426398213747		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.00916426398213747 | validation: 0.028835349439689593]
	TIME [epoch: 20.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.00967527330640173		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.00967527330640173 | validation: 0.02699191319677872]
	TIME [epoch: 20.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.011307450438806902		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.011307450438806902 | validation: 0.028075115931188478]
	TIME [epoch: 20.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.012158490955395559		[learning rate: 0.0042394]
	Learning Rate: 0.00423943
	LOSS [training: 0.012158490955395559 | validation: 0.027387250971610346]
	TIME [epoch: 20.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008973148164544332		[learning rate: 0.0042207]
	Learning Rate: 0.0042207
	LOSS [training: 0.008973148164544332 | validation: 0.02903247463942207]
	TIME [epoch: 20.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009191764961224373		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.009191764961224373 | validation: 0.03251713814471145]
	TIME [epoch: 20.9 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.01056001535998494		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.01056001535998494 | validation: 0.03257648146878389]
	TIME [epoch: 20.9 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010194694105323907		[learning rate: 0.004165]
	Learning Rate: 0.004165
	LOSS [training: 0.010194694105323907 | validation: 0.02670712312863173]
	TIME [epoch: 20.9 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008052439794512724		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.008052439794512724 | validation: 0.0260939640088798]
	TIME [epoch: 20.9 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.009392551738689529		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.009392551738689529 | validation: 0.026451336674030167]
	TIME [epoch: 20.9 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.010901036522217678		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.010901036522217678 | validation: 0.02929874285121299]
	TIME [epoch: 20.9 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008894626574016096		[learning rate: 0.0040919]
	Learning Rate: 0.00409188
	LOSS [training: 0.008894626574016096 | validation: 0.030372018792619908]
	TIME [epoch: 20.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.008124299784126298		[learning rate: 0.0040738]
	Learning Rate: 0.0040738
	LOSS [training: 0.008124299784126298 | validation: 0.028962269751143445]
	TIME [epoch: 20.8 sec]
	Saving model to: out/model_training/model_facs_dec2b_2dnmf_v1_20240624_135140/states/model_facs_dec2b_2dnmf_v1_243.pth
Halted early. No improvement in validation loss for 200 epochs.
Finished training in 5122.567 seconds.
