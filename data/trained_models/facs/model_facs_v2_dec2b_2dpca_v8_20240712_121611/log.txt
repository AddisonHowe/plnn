Args:
Namespace(name='model_facs_v2_dec2b_2dpca_v8', outdir='out/model_training/model_facs_v2_dec2b_2dpca_v8', training_data='data/training_data/facs_v2/pca/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/training', validation_data='data/training_data/facs_v2/pca/dec2_fitonsubset/transition2_subset_ce_pn_m_pc12/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=10, batch_size=50, patience=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=400, ncells_sample=400, model_do_sample=False, dt=0.005, dt_schedule='stepped', dt_schedule_bounds=[500, 1000, 1500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.02, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.1, 1.3, 1.5, 2.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 633205224

Training model...

Saving initial model state to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7698975400129888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7698975400129888 | validation: 1.08551363039727]
	TIME [epoch: 35.2 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8726115465496257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8726115465496257 | validation: 0.8906251692862954]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045083347883182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045083347883182 | validation: 0.8402290491007686]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6501603510404337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501603510404337 | validation: 0.8212287886227205]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6070846197855173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6070846197855173 | validation: 0.705104777029407]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5700672484976224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5700672484976224 | validation: 0.6200330293121322]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4921760003328492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4921760003328492 | validation: 0.5950729521722388]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4695342957460767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4695342957460767 | validation: 0.6042498641225108]
	TIME [epoch: 6.01 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4515289271436901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4515289271436901 | validation: 0.5719945715155026]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46726934188760183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46726934188760183 | validation: 0.5792168110150013]
	TIME [epoch: 6.03 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41666390637685424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41666390637685424 | validation: 0.5444497140485028]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4008396621284973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4008396621284973 | validation: 0.535295070142336]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4117923297254418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117923297254418 | validation: 0.521973550417102]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.428681275823428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.428681275823428 | validation: 0.5049887559277301]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3962455914026141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3962455914026141 | validation: 0.5388272103917853]
	TIME [epoch: 6.02 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40125702070149283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40125702070149283 | validation: 0.503625603380982]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.353712214563753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.353712214563753 | validation: 0.4909283928732728]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3855974713982285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3855974713982285 | validation: 0.5224330435992923]
	TIME [epoch: 6.03 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.361656448195398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.361656448195398 | validation: 0.5011605168542745]
	TIME [epoch: 6.01 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33841016442232597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33841016442232597 | validation: 0.49270720610148133]
	TIME [epoch: 6.01 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39498109051622066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39498109051622066 | validation: 0.42704093728824694]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39113767478928585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39113767478928585 | validation: 0.46159069667433056]
	TIME [epoch: 6.01 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33574488919582834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33574488919582834 | validation: 0.4439313479955501]
	TIME [epoch: 6.01 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3200922882607357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3200922882607357 | validation: 0.5480608644646883]
	TIME [epoch: 6.29 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33111345004518855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33111345004518855 | validation: 0.436749900751361]
	TIME [epoch: 6.03 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30108173016511675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30108173016511675 | validation: 0.5127642097485209]
	TIME [epoch: 6.03 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38069641059401343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38069641059401343 | validation: 0.40511967842396307]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2860659224658487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2860659224658487 | validation: 0.47457701560539556]
	TIME [epoch: 6.02 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2835050138512894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2835050138512894 | validation: 0.43139684562045655]
	TIME [epoch: 6.02 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33960493758661736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33960493758661736 | validation: 0.5194739113790843]
	TIME [epoch: 6.01 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3411658212236818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3411658212236818 | validation: 0.45353480952416514]
	TIME [epoch: 6.02 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27972769409973264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27972769409973264 | validation: 0.3867791709417703]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3329176399679056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3329176399679056 | validation: 0.3683358606746264]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2756313957540387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2756313957540387 | validation: 0.48249698782978223]
	TIME [epoch: 6.02 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29072349039817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29072349039817 | validation: 0.4014226750848434]
	TIME [epoch: 6.01 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30585254768902803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30585254768902803 | validation: 0.3761429740778196]
	TIME [epoch: 6.01 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28952508210399247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28952508210399247 | validation: 0.4586425291748768]
	TIME [epoch: 6.01 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30132484430128487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30132484430128487 | validation: 0.34203779397325385]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25939143568119133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25939143568119133 | validation: 0.40600735359320866]
	TIME [epoch: 6.02 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.275412331451593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.275412331451593 | validation: 0.4061344838433255]
	TIME [epoch: 6.03 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2855355093141002		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.2855355093141002 | validation: 0.403079329267142]
	TIME [epoch: 6.02 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29816657964182575		[learning rate: 0.0099206]
	Learning Rate: 0.00992061
	LOSS [training: 0.29816657964182575 | validation: 0.4082983199654283]
	TIME [epoch: 6.02 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27044073401390245		[learning rate: 0.0098768]
	Learning Rate: 0.00987678
	LOSS [training: 0.27044073401390245 | validation: 0.429347307217033]
	TIME [epoch: 6.02 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26964072851082094		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.26964072851082094 | validation: 0.3467572557691406]
	TIME [epoch: 6.02 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28679170018031586		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.28679170018031586 | validation: 0.41289330427465243]
	TIME [epoch: 6.02 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2779180270312435		[learning rate: 0.0097464]
	Learning Rate: 0.00974644
	LOSS [training: 0.2779180270312435 | validation: 0.40562220037081487]
	TIME [epoch: 6.01 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2932149692677941		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 0.2932149692677941 | validation: 0.4806012266517162]
	TIME [epoch: 6.02 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25994483284512554		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.25994483284512554 | validation: 0.3421051182224666]
	TIME [epoch: 6.02 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3065902013520756		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.3065902013520756 | validation: 0.47592786141854443]
	TIME [epoch: 6.02 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27999292179255697		[learning rate: 0.0095753]
	Learning Rate: 0.00957533
	LOSS [training: 0.27999292179255697 | validation: 0.44313678671348655]
	TIME [epoch: 6.02 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2772760291497466		[learning rate: 0.009533]
	Learning Rate: 0.00953303
	LOSS [training: 0.2772760291497466 | validation: 0.42805697246048974]
	TIME [epoch: 6.02 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24179114240642113		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.24179114240642113 | validation: 0.36602560056850475]
	TIME [epoch: 6.01 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24351414257553086		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.24351414257553086 | validation: 0.4350010051882361]
	TIME [epoch: 6.01 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2801803709581983		[learning rate: 0.0094072]
	Learning Rate: 0.00940722
	LOSS [training: 0.2801803709581983 | validation: 0.4377729346386499]
	TIME [epoch: 6.01 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2670536520192463		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 0.2670536520192463 | validation: 0.37147021116668644]
	TIME [epoch: 6.02 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28067894616891265		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.28067894616891265 | validation: 0.3287919620506846]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2909630845081675		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.2909630845081675 | validation: 0.35623098805093834]
	TIME [epoch: 6.02 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25362120832301993		[learning rate: 0.0092421]
	Learning Rate: 0.00924207
	LOSS [training: 0.25362120832301993 | validation: 0.368977836301279]
	TIME [epoch: 6.02 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2627233175879107		[learning rate: 0.0092012]
	Learning Rate: 0.00920124
	LOSS [training: 0.2627233175879107 | validation: 0.4072419053493766]
	TIME [epoch: 6.01 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28667231312899016		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.28667231312899016 | validation: 0.4153870999629443]
	TIME [epoch: 6.01 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2818335373228839		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.2818335373228839 | validation: 0.3986521227161178]
	TIME [epoch: 6.01 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25462676043419574		[learning rate: 0.0090798]
	Learning Rate: 0.00907981
	LOSS [training: 0.25462676043419574 | validation: 0.3745084009506432]
	TIME [epoch: 6.02 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2719564678734002		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 0.2719564678734002 | validation: 0.35610860625055807]
	TIME [epoch: 6.02 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2378977744276234		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.2378977744276234 | validation: 0.3538216667714086]
	TIME [epoch: 6.02 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24201001995977958		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.24201001995977958 | validation: 0.3313448262064585]
	TIME [epoch: 6.01 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24871251705834227		[learning rate: 0.0089204]
	Learning Rate: 0.00892041
	LOSS [training: 0.24871251705834227 | validation: 0.45191978853117476]
	TIME [epoch: 6.01 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2851010424295851		[learning rate: 0.008881]
	Learning Rate: 0.00888099
	LOSS [training: 0.2851010424295851 | validation: 0.34043628172590273]
	TIME [epoch: 6.01 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23073442313739526		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.23073442313739526 | validation: 0.33721473385799455]
	TIME [epoch: 6.02 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22300548207507126		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.22300548207507126 | validation: 0.3687189664544763]
	TIME [epoch: 6.01 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24668607557802208		[learning rate: 0.0087638]
	Learning Rate: 0.0087638
	LOSS [training: 0.24668607557802208 | validation: 0.32388074262249544]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23881799123637598		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 0.23881799123637598 | validation: 0.39659374182642027]
	TIME [epoch: 6.02 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22078390713173435		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.22078390713173435 | validation: 0.4340150076141092]
	TIME [epoch: 6.01 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2952858397960505		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.2952858397960505 | validation: 0.3481382454121504]
	TIME [epoch: 6.01 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23093592444054895		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.23093592444054895 | validation: 0.40262044848350026]
	TIME [epoch: 6.01 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2485641683171794		[learning rate: 0.0085719]
	Learning Rate: 0.0085719
	LOSS [training: 0.2485641683171794 | validation: 0.3573775205949804]
	TIME [epoch: 6.01 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21916505633532773		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.21916505633532773 | validation: 0.4760282125577284]
	TIME [epoch: 6.01 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23224095302715417		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.23224095302715417 | validation: 0.34194228374992236]
	TIME [epoch: 6.01 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23293416012123438		[learning rate: 0.0084588]
	Learning Rate: 0.00845878
	LOSS [training: 0.23293416012123438 | validation: 0.34013606936266716]
	TIME [epoch: 6.01 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2286498092285038		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 0.2286498092285038 | validation: 0.3702321626075106]
	TIME [epoch: 6.01 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.220262022844716		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.220262022844716 | validation: 0.3058346772507806]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21986130962771117		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.21986130962771117 | validation: 0.33113314920372033]
	TIME [epoch: 6.01 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23346859342748033		[learning rate: 0.0083103]
	Learning Rate: 0.00831027
	LOSS [training: 0.23346859342748033 | validation: 0.4286815940177673]
	TIME [epoch: 6.01 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2325714826839976		[learning rate: 0.0082736]
	Learning Rate: 0.00827356
	LOSS [training: 0.2325714826839976 | validation: 0.39520577164288295]
	TIME [epoch: 6.01 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25321201161784507		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.25321201161784507 | validation: 0.3736875978927371]
	TIME [epoch: 6.01 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23231497609935312		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.23231497609935312 | validation: 0.3744215692132692]
	TIME [epoch: 6.02 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22786037426387118		[learning rate: 0.0081644]
	Learning Rate: 0.00816438
	LOSS [training: 0.22786037426387118 | validation: 0.3407257452088759]
	TIME [epoch: 6.01 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21292146344342394		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 0.21292146344342394 | validation: 0.377373650247777]
	TIME [epoch: 6.01 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23905494451725723		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.23905494451725723 | validation: 0.33281377367625825]
	TIME [epoch: 6.01 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23385661555656503		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.23385661555656503 | validation: 0.3731156716831317]
	TIME [epoch: 6.01 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23957443702740272		[learning rate: 0.008021]
	Learning Rate: 0.00802104
	LOSS [training: 0.23957443702740272 | validation: 0.34932612584236317]
	TIME [epoch: 6.01 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22231105604408027		[learning rate: 0.0079856]
	Learning Rate: 0.0079856
	LOSS [training: 0.22231105604408027 | validation: 0.3929861091998501]
	TIME [epoch: 6.01 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27488002929182426		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.27488002929182426 | validation: 0.33551206625674335]
	TIME [epoch: 6.02 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2425562124170198		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.2425562124170198 | validation: 0.37434940989614807]
	TIME [epoch: 6.01 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23357276161380985		[learning rate: 0.0078802]
	Learning Rate: 0.00788022
	LOSS [training: 0.23357276161380985 | validation: 0.35433549212422927]
	TIME [epoch: 6.01 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.210873983875241		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.210873983875241 | validation: 0.3500473501056554]
	TIME [epoch: 6.01 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24568701374408342		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.24568701374408342 | validation: 0.3182217125631678]
	TIME [epoch: 6.01 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.243553244612046		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.243553244612046 | validation: 0.4687145466083286]
	TIME [epoch: 6.01 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3803504301730691		[learning rate: 0.0077419]
	Learning Rate: 0.00774188
	LOSS [training: 0.3803504301730691 | validation: 0.43088301821634273]
	TIME [epoch: 6 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3302897561151261		[learning rate: 0.0077077]
	Learning Rate: 0.00770767
	LOSS [training: 0.3302897561151261 | validation: 0.4006739916693404]
	TIME [epoch: 6.01 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28829154754742026		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.28829154754742026 | validation: 0.42567011014413064]
	TIME [epoch: 6.02 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2501295855928355		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.2501295855928355 | validation: 0.40122876811551944]
	TIME [epoch: 6.03 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2437827349832813		[learning rate: 0.007606]
	Learning Rate: 0.00760596
	LOSS [training: 0.2437827349832813 | validation: 0.3867673337740234]
	TIME [epoch: 6.02 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22887272534712094		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.22887272534712094 | validation: 0.32631538002224864]
	TIME [epoch: 6.01 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2180887492167642		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2180887492167642 | validation: 0.3856192271161718]
	TIME [epoch: 6.01 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21282785023242753		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.21282785023242753 | validation: 0.34314998553633935]
	TIME [epoch: 6.02 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22316243598241553		[learning rate: 0.0074724]
	Learning Rate: 0.00747242
	LOSS [training: 0.22316243598241553 | validation: 0.3740841142109098]
	TIME [epoch: 6.01 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25374860939124666		[learning rate: 0.0074394]
	Learning Rate: 0.00743941
	LOSS [training: 0.25374860939124666 | validation: 0.40907446007459664]
	TIME [epoch: 6.01 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22217009557133202		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.22217009557133202 | validation: 0.41763609959125814]
	TIME [epoch: 6.02 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22801191773585497		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.22801191773585497 | validation: 0.3405160062844603]
	TIME [epoch: 6.01 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2321718779632763		[learning rate: 0.0073412]
	Learning Rate: 0.00734124
	LOSS [training: 0.2321718779632763 | validation: 0.34798060111404283]
	TIME [epoch: 6.01 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21156542724220406		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.21156542724220406 | validation: 0.3427915247068045]
	TIME [epoch: 6.01 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20321456348831238		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.20321456348831238 | validation: 0.3950639702120575]
	TIME [epoch: 6.02 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.295798155295017		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.295798155295017 | validation: 0.49596911465810967]
	TIME [epoch: 6.01 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24430149626271874		[learning rate: 0.0072124]
	Learning Rate: 0.00721235
	LOSS [training: 0.24430149626271874 | validation: 0.34000393658272854]
	TIME [epoch: 6.01 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20476797779975814		[learning rate: 0.0071805]
	Learning Rate: 0.00718049
	LOSS [training: 0.20476797779975814 | validation: 0.33804944074125715]
	TIME [epoch: 6.02 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2165422052222472		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.2165422052222472 | validation: 0.3489145152073544]
	TIME [epoch: 6.02 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2209183307370745		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.2209183307370745 | validation: 0.3235608217630493]
	TIME [epoch: 6.02 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24647952924107508		[learning rate: 0.0070857]
	Learning Rate: 0.00708573
	LOSS [training: 0.24647952924107508 | validation: 0.38596154618217393]
	TIME [epoch: 6.02 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23366152123822173		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.23366152123822173 | validation: 0.322930378366238]
	TIME [epoch: 6.02 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23534683128141315		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.23534683128141315 | validation: 0.4772064848209023]
	TIME [epoch: 6.02 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22897294115454664		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.22897294115454664 | validation: 0.35341737370388043]
	TIME [epoch: 6.02 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19401103200827494		[learning rate: 0.0069613]
	Learning Rate: 0.00696133
	LOSS [training: 0.19401103200827494 | validation: 0.32826024399219256]
	TIME [epoch: 6.01 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25365871825984704		[learning rate: 0.0069306]
	Learning Rate: 0.00693057
	LOSS [training: 0.25365871825984704 | validation: 0.32281705775063657]
	TIME [epoch: 6.03 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21222029256919522		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.21222029256919522 | validation: 0.33798104134808604]
	TIME [epoch: 6.02 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20016526704198823		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.20016526704198823 | validation: 0.39912187354502066]
	TIME [epoch: 6.01 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23384052347288908		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.23384052347288908 | validation: 0.4859623287329933]
	TIME [epoch: 6.02 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2148951219718263		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.2148951219718263 | validation: 0.33830667168221396]
	TIME [epoch: 6.01 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2093649852135452		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2093649852135452 | validation: 0.3195017222300332]
	TIME [epoch: 6.01 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2574037665460639		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.2574037665460639 | validation: 0.32405261162887905]
	TIME [epoch: 6.01 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20206478097923272		[learning rate: 0.006719]
	Learning Rate: 0.00671905
	LOSS [training: 0.20206478097923272 | validation: 0.3497093260417451]
	TIME [epoch: 6.02 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20982089912692933		[learning rate: 0.0066894]
	Learning Rate: 0.00668936
	LOSS [training: 0.20982089912692933 | validation: 0.4323511798151933]
	TIME [epoch: 6.02 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21127353541927016		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.21127353541927016 | validation: 0.3985249738335377]
	TIME [epoch: 6.01 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2158786352388163		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.2158786352388163 | validation: 0.35789170653422225]
	TIME [epoch: 6.01 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2010279287035337		[learning rate: 0.0066011]
	Learning Rate: 0.00660109
	LOSS [training: 0.2010279287035337 | validation: 0.38278955281399096]
	TIME [epoch: 6.01 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20672794974469805		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.20672794974469805 | validation: 0.34074102144846685]
	TIME [epoch: 6.02 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19703752287166695		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.19703752287166695 | validation: 0.3895448511384416]
	TIME [epoch: 6.01 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2232577737818961		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.2232577737818961 | validation: 0.38090205643799663]
	TIME [epoch: 6.01 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19908370807673847		[learning rate: 0.0064852]
	Learning Rate: 0.00648519
	LOSS [training: 0.19908370807673847 | validation: 0.34335691852233674]
	TIME [epoch: 6.02 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20360736384012812		[learning rate: 0.0064565]
	Learning Rate: 0.00645654
	LOSS [training: 0.20360736384012812 | validation: 0.32269072113932795]
	TIME [epoch: 6.01 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20066321916678534		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.20066321916678534 | validation: 0.3794719545107743]
	TIME [epoch: 6.02 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28193763940787503		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.28193763940787503 | validation: 0.35808255564790165]
	TIME [epoch: 6.02 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2265678624225759		[learning rate: 0.0063713]
	Learning Rate: 0.00637134
	LOSS [training: 0.2265678624225759 | validation: 0.3534499082272963]
	TIME [epoch: 6.01 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2124165536535675		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.2124165536535675 | validation: 0.34048832758733644]
	TIME [epoch: 6.01 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21993004358211948		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.21993004358211948 | validation: 0.3256442770330132]
	TIME [epoch: 6.11 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20723662828977257		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.20723662828977257 | validation: 0.38908423898780176]
	TIME [epoch: 6.02 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20898337590115443		[learning rate: 0.0062595]
	Learning Rate: 0.00625948
	LOSS [training: 0.20898337590115443 | validation: 0.419871167085773]
	TIME [epoch: 6.02 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21135988960412777		[learning rate: 0.0062318]
	Learning Rate: 0.00623183
	LOSS [training: 0.21135988960412777 | validation: 0.35048401578115884]
	TIME [epoch: 6.02 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20924458870489754		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.20924458870489754 | validation: 0.3255465556437221]
	TIME [epoch: 6.02 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20628234080786592		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.20628234080786592 | validation: 0.31127006477616087]
	TIME [epoch: 6.01 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20397558239043798		[learning rate: 0.0061496]
	Learning Rate: 0.00614959
	LOSS [training: 0.20397558239043798 | validation: 0.3666567601416853]
	TIME [epoch: 6.01 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22814126916431285		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.22814126916431285 | validation: 0.332689899371527]
	TIME [epoch: 6.01 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20163139904751218		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.20163139904751218 | validation: 0.3132572148827336]
	TIME [epoch: 6.01 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21412677772034994		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.21412677772034994 | validation: 0.36261867745170184]
	TIME [epoch: 6.03 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19549015509901643		[learning rate: 0.0060416]
	Learning Rate: 0.00604163
	LOSS [training: 0.19549015509901643 | validation: 0.33664157625576685]
	TIME [epoch: 6.02 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19368334278686214		[learning rate: 0.0060149]
	Learning Rate: 0.00601493
	LOSS [training: 0.19368334278686214 | validation: 0.33201441304203927]
	TIME [epoch: 6.01 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20338180017590674		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.20338180017590674 | validation: 0.38723312701232443]
	TIME [epoch: 6.01 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19201196772972068		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.19201196772972068 | validation: 0.3567896685647289]
	TIME [epoch: 6.01 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21204466702703706		[learning rate: 0.0059356]
	Learning Rate: 0.00593556
	LOSS [training: 0.21204466702703706 | validation: 0.4171494522233784]
	TIME [epoch: 6.01 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23253984451077375		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.23253984451077375 | validation: 0.38370796435988846]
	TIME [epoch: 6.02 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2149968433209392		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.2149968433209392 | validation: 0.3285768291216114]
	TIME [epoch: 6.03 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2174653602765259		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.2174653602765259 | validation: 0.32658920479708203]
	TIME [epoch: 6.02 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20713277433265223		[learning rate: 0.0058314]
	Learning Rate: 0.00583135
	LOSS [training: 0.20713277433265223 | validation: 0.3160000674116214]
	TIME [epoch: 6.02 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21459372539774768		[learning rate: 0.0058056]
	Learning Rate: 0.00580559
	LOSS [training: 0.21459372539774768 | validation: 0.35179157488712887]
	TIME [epoch: 6.02 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20819660629007936		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.20819660629007936 | validation: 0.32076540784395846]
	TIME [epoch: 6.01 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19711072001130517		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.19711072001130517 | validation: 0.32836071258755234]
	TIME [epoch: 6.02 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2119981729422764		[learning rate: 0.005729]
	Learning Rate: 0.00572897
	LOSS [training: 0.2119981729422764 | validation: 0.3845448485665241]
	TIME [epoch: 6.02 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19132158212606282		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.19132158212606282 | validation: 0.35743652600863074]
	TIME [epoch: 6.01 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.238767912339859		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.238767912339859 | validation: 0.43940811232357335]
	TIME [epoch: 6.04 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28252667670203735		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.28252667670203735 | validation: 0.3837166467707984]
	TIME [epoch: 6.02 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23295471337605184		[learning rate: 0.0056284]
	Learning Rate: 0.0056284
	LOSS [training: 0.23295471337605184 | validation: 0.3419581262475829]
	TIME [epoch: 6.01 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21140504254887635		[learning rate: 0.0056035]
	Learning Rate: 0.00560353
	LOSS [training: 0.21140504254887635 | validation: 0.3084992726051633]
	TIME [epoch: 6.02 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18800529723621887		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.18800529723621887 | validation: 0.3576387348805067]
	TIME [epoch: 6.01 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19493407248074263		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.19493407248074263 | validation: 0.3330357392478529]
	TIME [epoch: 6.01 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1971510987980296		[learning rate: 0.0055296]
	Learning Rate: 0.00552958
	LOSS [training: 0.1971510987980296 | validation: 0.3433512383632545]
	TIME [epoch: 6.01 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1869859611347213		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.1869859611347213 | validation: 0.3629517618088609]
	TIME [epoch: 6.02 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21625874403813067		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.21625874403813067 | validation: 0.32014237658322964]
	TIME [epoch: 6.02 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19597644286193655		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.19597644286193655 | validation: 0.3398659041983775]
	TIME [epoch: 6.01 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19386959703777204		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.19386959703777204 | validation: 0.35397906506459054]
	TIME [epoch: 6.01 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20712361072189006		[learning rate: 0.0054085]
	Learning Rate: 0.0054085
	LOSS [training: 0.20712361072189006 | validation: 0.32636738871157006]
	TIME [epoch: 6.01 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.189350670709758		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.189350670709758 | validation: 0.3262105819484442]
	TIME [epoch: 6.01 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20165135549615618		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.20165135549615618 | validation: 0.3375956305962391]
	TIME [epoch: 6.02 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2012636455955037		[learning rate: 0.0053371]
	Learning Rate: 0.00533713
	LOSS [training: 0.2012636455955037 | validation: 0.36880438576703106]
	TIME [epoch: 6.01 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22013980599448382		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.22013980599448382 | validation: 0.31799783631018586]
	TIME [epoch: 6.03 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20847737957838403		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.20847737957838403 | validation: 0.37850915069770497]
	TIME [epoch: 6.01 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23888546199947952		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.23888546199947952 | validation: 0.3221209092168238]
	TIME [epoch: 6.01 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18358137976311453		[learning rate: 0.0052434]
	Learning Rate: 0.00524343
	LOSS [training: 0.18358137976311453 | validation: 0.3066064686288068]
	TIME [epoch: 6.02 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20629069625675248		[learning rate: 0.0052203]
	Learning Rate: 0.00522026
	LOSS [training: 0.20629069625675248 | validation: 0.32148793862291986]
	TIME [epoch: 6.01 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1899492364325069		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.1899492364325069 | validation: 0.317533592706959]
	TIME [epoch: 6.01 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21777763867869632		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.21777763867869632 | validation: 0.4021044059429117]
	TIME [epoch: 6.02 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26235656978384825		[learning rate: 0.0051514]
	Learning Rate: 0.00515137
	LOSS [training: 0.26235656978384825 | validation: 0.33166843935497536]
	TIME [epoch: 6.02 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20185082327453271		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.20185082327453271 | validation: 0.3327638824511819]
	TIME [epoch: 6.02 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19402250913410174		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.19402250913410174 | validation: 0.3430652373687929]
	TIME [epoch: 6.01 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1914382162261629		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.1914382162261629 | validation: 0.3031165031041203]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1835378395796998		[learning rate: 0.0050609]
	Learning Rate: 0.00506093
	LOSS [training: 0.1835378395796998 | validation: 0.3399314320778586]
	TIME [epoch: 6.02 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1958782938823606		[learning rate: 0.0050386]
	Learning Rate: 0.00503857
	LOSS [training: 0.1958782938823606 | validation: 0.3037206737667668]
	TIME [epoch: 6.01 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1896777647545659		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.1896777647545659 | validation: 0.320216036125369]
	TIME [epoch: 6.01 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21172532589440488		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.21172532589440488 | validation: 0.42331126773653693]
	TIME [epoch: 6.01 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21065283144417854		[learning rate: 0.0049721]
	Learning Rate: 0.00497208
	LOSS [training: 0.21065283144417854 | validation: 0.3482609733443869]
	TIME [epoch: 6.02 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1860737633151934		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.1860737633151934 | validation: 0.321174192898576]
	TIME [epoch: 6.01 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1878655163031715		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.1878655163031715 | validation: 0.3562759746364646]
	TIME [epoch: 6.01 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20277617810638282		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.20277617810638282 | validation: 0.329362641766475]
	TIME [epoch: 6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18303787675103347		[learning rate: 0.0048848]
	Learning Rate: 0.00488479
	LOSS [training: 0.18303787675103347 | validation: 0.31146958341283615]
	TIME [epoch: 6 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18778642596129919		[learning rate: 0.0048632]
	Learning Rate: 0.00486321
	LOSS [training: 0.18778642596129919 | validation: 0.3472714018563047]
	TIME [epoch: 6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19540596484882475		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.19540596484882475 | validation: 0.3389405460748797]
	TIME [epoch: 6.01 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18349678954006404		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.18349678954006404 | validation: 0.3972206877844168]
	TIME [epoch: 6.03 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18872743260657993		[learning rate: 0.004799]
	Learning Rate: 0.00479903
	LOSS [training: 0.18872743260657993 | validation: 0.4104987754842395]
	TIME [epoch: 6.02 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25132289044653244		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.25132289044653244 | validation: 0.339083650966476]
	TIME [epoch: 6.02 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2099585544252304		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.2099585544252304 | validation: 0.3108978112395008]
	TIME [epoch: 6.01 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19749712660112687		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.19749712660112687 | validation: 0.3249492648660542]
	TIME [epoch: 6.02 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1943189171978842		[learning rate: 0.0047148]
	Learning Rate: 0.00471478
	LOSS [training: 0.1943189171978842 | validation: 0.347011625321278]
	TIME [epoch: 6.01 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2068158522295513		[learning rate: 0.0046939]
	Learning Rate: 0.00469395
	LOSS [training: 0.2068158522295513 | validation: 0.32027086003045635]
	TIME [epoch: 6.01 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19350903824755913		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.19350903824755913 | validation: 0.3255862369750901]
	TIME [epoch: 6.01 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1854224775591518		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.1854224775591518 | validation: 0.305920746343825]
	TIME [epoch: 6.02 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18874936082157504		[learning rate: 0.004632]
	Learning Rate: 0.00463201
	LOSS [training: 0.18874936082157504 | validation: 0.3408042046360366]
	TIME [epoch: 6.01 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18735312947127924		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.18735312947127924 | validation: 0.33210029073513125]
	TIME [epoch: 6.01 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17959576333346258		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.17959576333346258 | validation: 0.3709699877074405]
	TIME [epoch: 6.01 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19862950129894355		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.19862950129894355 | validation: 0.3176062642455445]
	TIME [epoch: 6.01 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22130481620496312		[learning rate: 0.0045507]
	Learning Rate: 0.00455069
	LOSS [training: 0.22130481620496312 | validation: 0.3822479343978147]
	TIME [epoch: 6.01 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24330715631940233		[learning rate: 0.0045306]
	Learning Rate: 0.00453058
	LOSS [training: 0.24330715631940233 | validation: 0.32230699572077415]
	TIME [epoch: 6.02 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1880365613850595		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.1880365613850595 | validation: 0.37708365981659203]
	TIME [epoch: 6.01 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19031665495231498		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.19031665495231498 | validation: 0.38391477310351674]
	TIME [epoch: 6.01 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18358984201888381		[learning rate: 0.0044708]
	Learning Rate: 0.00447079
	LOSS [training: 0.18358984201888381 | validation: 0.3073115269040546]
	TIME [epoch: 6.01 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18535854989983913		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.18535854989983913 | validation: 0.3456474790120883]
	TIME [epoch: 6.01 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1913356553209959		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.1913356553209959 | validation: 0.3180109479479315]
	TIME [epoch: 6.01 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1961916520370021		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.1961916520370021 | validation: 0.3198697345543386]
	TIME [epoch: 6.01 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19655639180860232		[learning rate: 0.0043923]
	Learning Rate: 0.0043923
	LOSS [training: 0.19655639180860232 | validation: 0.3361567839477009]
	TIME [epoch: 6.01 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1759978865432122		[learning rate: 0.0043729]
	Learning Rate: 0.0043729
	LOSS [training: 0.1759978865432122 | validation: 0.3139376063412595]
	TIME [epoch: 6.01 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1738032833027868		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.1738032833027868 | validation: 0.3228771928204382]
	TIME [epoch: 6.02 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17249153768580863		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.17249153768580863 | validation: 0.3055991597798196]
	TIME [epoch: 6.01 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18321106614726557		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.18321106614726557 | validation: 0.32092505654773656]
	TIME [epoch: 6.01 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17894358669051882		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.17894358669051882 | validation: 0.307756520971534]
	TIME [epoch: 6.01 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18444454293252444		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.18444454293252444 | validation: 0.35300098097198135]
	TIME [epoch: 6.01 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1849985912486656		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.1849985912486656 | validation: 0.3349617021121788]
	TIME [epoch: 6.01 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18125249273052452		[learning rate: 0.0042394]
	Learning Rate: 0.00423943
	LOSS [training: 0.18125249273052452 | validation: 0.3243163701689365]
	TIME [epoch: 6.01 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18586162034442852		[learning rate: 0.0042207]
	Learning Rate: 0.0042207
	LOSS [training: 0.18586162034442852 | validation: 0.3164749291427471]
	TIME [epoch: 6.01 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1797848465719803		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.1797848465719803 | validation: 0.33163776161224134]
	TIME [epoch: 6.01 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19152514767947265		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.19152514767947265 | validation: 0.31697350782703826]
	TIME [epoch: 6.01 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1854536691259555		[learning rate: 0.004165]
	Learning Rate: 0.004165
	LOSS [training: 0.1854536691259555 | validation: 0.3127710459463281]
	TIME [epoch: 6.01 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1988762682233748		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.1988762682233748 | validation: 0.43325076111476746]
	TIME [epoch: 6.01 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21467614375991112		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.21467614375991112 | validation: 0.2937800400588448]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1843909066395302		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.1843909066395302 | validation: 0.3221404395401321]
	TIME [epoch: 6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19946422991493776		[learning rate: 0.0040919]
	Learning Rate: 0.00409188
	LOSS [training: 0.19946422991493776 | validation: 0.3194443499855109]
	TIME [epoch: 6.01 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1751301199507895		[learning rate: 0.0040738]
	Learning Rate: 0.0040738
	LOSS [training: 0.1751301199507895 | validation: 0.3162501181738219]
	TIME [epoch: 6.01 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18795337138479123		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.18795337138479123 | validation: 0.3088373184271134]
	TIME [epoch: 6.01 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19309480091212466		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.19309480091212466 | validation: 0.3522878205957246]
	TIME [epoch: 6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18397210036475892		[learning rate: 0.00402]
	Learning Rate: 0.00402004
	LOSS [training: 0.18397210036475892 | validation: 0.3290777008240189]
	TIME [epoch: 6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1837089586462207		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 0.1837089586462207 | validation: 0.3014745811874181]
	TIME [epoch: 6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18465406903394946		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.18465406903394946 | validation: 0.31313747223620564]
	TIME [epoch: 6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17384121557646354		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.17384121557646354 | validation: 0.34308973965219863]
	TIME [epoch: 6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20634885327935493		[learning rate: 0.0039495]
	Learning Rate: 0.00394947
	LOSS [training: 0.20634885327935493 | validation: 0.3129656987128206]
	TIME [epoch: 6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17801753062781395		[learning rate: 0.003932]
	Learning Rate: 0.00393202
	LOSS [training: 0.17801753062781395 | validation: 0.30286507173337074]
	TIME [epoch: 6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1936032624855058		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.1936032624855058 | validation: 0.3262018857830479]
	TIME [epoch: 6.01 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17891429087230964		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.17891429087230964 | validation: 0.32743662462637413]
	TIME [epoch: 6.01 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17932670427695027		[learning rate: 0.0038801]
	Learning Rate: 0.00388013
	LOSS [training: 0.17932670427695027 | validation: 0.31117412600934974]
	TIME [epoch: 6.01 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18324269777861685		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 0.18324269777861685 | validation: 0.2983358007955245]
	TIME [epoch: 6.01 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21682895640795827		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.21682895640795827 | validation: 0.3473452748779667]
	TIME [epoch: 6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19605696680274529		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.19605696680274529 | validation: 0.3484165769541763]
	TIME [epoch: 6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18650325908131543		[learning rate: 0.003812]
	Learning Rate: 0.00381201
	LOSS [training: 0.18650325908131543 | validation: 0.3337948878038519]
	TIME [epoch: 6.01 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18528163500556682		[learning rate: 0.0037952]
	Learning Rate: 0.00379517
	LOSS [training: 0.18528163500556682 | validation: 0.2977700910576773]
	TIME [epoch: 6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17627845020408683		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.17627845020408683 | validation: 0.3206665858430722]
	TIME [epoch: 6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17300079834772544		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.17300079834772544 | validation: 0.3345703772770583]
	TIME [epoch: 6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1845929327091182		[learning rate: 0.0037451]
	Learning Rate: 0.00374508
	LOSS [training: 0.1845929327091182 | validation: 0.32367422230957266]
	TIME [epoch: 6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16986102650364898		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 0.16986102650364898 | validation: 0.36826354504566006]
	TIME [epoch: 6.01 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20286523752990165		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.20286523752990165 | validation: 0.3697683658294811]
	TIME [epoch: 6 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19119270444603034		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.19119270444603034 | validation: 0.30869624032447635]
	TIME [epoch: 6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18015785962516223		[learning rate: 0.0036793]
	Learning Rate: 0.00367933
	LOSS [training: 0.18015785962516223 | validation: 0.29884469206108816]
	TIME [epoch: 6.01 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18100873197506956		[learning rate: 0.0036631]
	Learning Rate: 0.00366308
	LOSS [training: 0.18100873197506956 | validation: 0.32482412060006294]
	TIME [epoch: 6.07 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21641568234524805		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.21641568234524805 | validation: 0.3235758994280969]
	TIME [epoch: 6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.186093431827891		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.186093431827891 | validation: 0.2998318735772971]
	TIME [epoch: 6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1846205005177212		[learning rate: 0.0036147]
	Learning Rate: 0.00361474
	LOSS [training: 0.1846205005177212 | validation: 0.30709334238669145]
	TIME [epoch: 6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1776183219925288		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 0.1776183219925288 | validation: 0.3133752287423206]
	TIME [epoch: 6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17985669152920397		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.17985669152920397 | validation: 0.2957274220462051]
	TIME [epoch: 6.01 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18864041354224886		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.18864041354224886 | validation: 0.3113160635385605]
	TIME [epoch: 6.01 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1803977759801298		[learning rate: 0.0035513]
	Learning Rate: 0.00355128
	LOSS [training: 0.1803977759801298 | validation: 0.31723473693266846]
	TIME [epoch: 6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17832159260478267		[learning rate: 0.0035356]
	Learning Rate: 0.00353559
	LOSS [training: 0.17832159260478267 | validation: 0.3514720440549472]
	TIME [epoch: 6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18289097797551093		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.18289097797551093 | validation: 0.3314662087104584]
	TIME [epoch: 6 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1876938207867153		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.1876938207867153 | validation: 0.32613296892012794]
	TIME [epoch: 6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19684741482642962		[learning rate: 0.0034889]
	Learning Rate: 0.00348893
	LOSS [training: 0.19684741482642962 | validation: 0.321373097254082]
	TIME [epoch: 6.01 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17890478712755248		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 0.17890478712755248 | validation: 0.3358932808295019]
	TIME [epoch: 6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19205557228275943		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.19205557228275943 | validation: 0.31554256442511847]
	TIME [epoch: 6.01 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18347151549311808		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.18347151549311808 | validation: 0.32864997651517913]
	TIME [epoch: 6.01 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18260472925720284		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.18260472925720284 | validation: 0.31733627885653104]
	TIME [epoch: 6.01 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17540256068697266		[learning rate: 0.0034125]
	Learning Rate: 0.00341253
	LOSS [training: 0.17540256068697266 | validation: 0.30501420363848386]
	TIME [epoch: 6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18045714272794644		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.18045714272794644 | validation: 0.29957269251961705]
	TIME [epoch: 6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20559026099600364		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.20559026099600364 | validation: 0.3384872096944075]
	TIME [epoch: 6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17540057994155403		[learning rate: 0.0033675]
	Learning Rate: 0.0033675
	LOSS [training: 0.17540057994155403 | validation: 0.29977658133700064]
	TIME [epoch: 6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17510814457834556		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 0.17510814457834556 | validation: 0.3258916826945614]
	TIME [epoch: 6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1867677702890274		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.1867677702890274 | validation: 0.3066923965021825]
	TIME [epoch: 6.01 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18549359888304692		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.18549359888304692 | validation: 0.3070986592551957]
	TIME [epoch: 6 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18661579149798802		[learning rate: 0.0033084]
	Learning Rate: 0.00330838
	LOSS [training: 0.18661579149798802 | validation: 0.2956065046598163]
	TIME [epoch: 6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17458323533740414		[learning rate: 0.0032938]
	Learning Rate: 0.00329376
	LOSS [training: 0.17458323533740414 | validation: 0.2984388726560163]
	TIME [epoch: 6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17042302742182244		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.17042302742182244 | validation: 0.2976206202973357]
	TIME [epoch: 6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17983920840678388		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.17983920840678388 | validation: 0.31152822550526654]
	TIME [epoch: 6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17390451370733667		[learning rate: 0.0032503]
	Learning Rate: 0.0032503
	LOSS [training: 0.17390451370733667 | validation: 0.3049490362526962]
	TIME [epoch: 6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20077551998149393		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 0.20077551998149393 | validation: 0.3018172909003317]
	TIME [epoch: 6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1789794588588623		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.1789794588588623 | validation: 0.3267421888708627]
	TIME [epoch: 6.01 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17244689007138997		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.17244689007138997 | validation: 0.3179229597577801]
	TIME [epoch: 6.01 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17598940504445282		[learning rate: 0.0031932]
	Learning Rate: 0.00319323
	LOSS [training: 0.17598940504445282 | validation: 0.313943577267548]
	TIME [epoch: 6.01 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18013827161895976		[learning rate: 0.0031791]
	Learning Rate: 0.00317913
	LOSS [training: 0.18013827161895976 | validation: 0.3191531022523579]
	TIME [epoch: 6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1776013260082767		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.1776013260082767 | validation: 0.3168051976916101]
	TIME [epoch: 6.01 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18672253721079649		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.18672253721079649 | validation: 0.30824412230718223]
	TIME [epoch: 6.02 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17419575198304388		[learning rate: 0.0031372]
	Learning Rate: 0.00313717
	LOSS [training: 0.17419575198304388 | validation: 0.2996497378934907]
	TIME [epoch: 6.01 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16850152834319904		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 0.16850152834319904 | validation: 0.3120177537524962]
	TIME [epoch: 6.02 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18489878884249075		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.18489878884249075 | validation: 0.35959873811655557]
	TIME [epoch: 6.01 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19244423837900354		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.19244423837900354 | validation: 0.33151669740671874]
	TIME [epoch: 6.01 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18237116894964583		[learning rate: 0.0030821]
	Learning Rate: 0.0030821
	LOSS [training: 0.18237116894964583 | validation: 0.30908588827196143]
	TIME [epoch: 6.01 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17828007011022734		[learning rate: 0.0030685]
	Learning Rate: 0.00306848
	LOSS [training: 0.17828007011022734 | validation: 0.3194253550036048]
	TIME [epoch: 6.01 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1737903259465283		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.1737903259465283 | validation: 0.3011340198133104]
	TIME [epoch: 6.01 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19476904729953548		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.19476904729953548 | validation: 0.316051028175864]
	TIME [epoch: 6.01 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1807370645644417		[learning rate: 0.003028]
	Learning Rate: 0.00302799
	LOSS [training: 0.1807370645644417 | validation: 0.30948352430190723]
	TIME [epoch: 6.01 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17818722948433152		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 0.17818722948433152 | validation: 0.32391614652878314]
	TIME [epoch: 6.02 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18293335928825938		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.18293335928825938 | validation: 0.3562650061742233]
	TIME [epoch: 6.01 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17570731538060747		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.17570731538060747 | validation: 0.30845150037304214]
	TIME [epoch: 6.01 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.177992278528989		[learning rate: 0.0029748]
	Learning Rate: 0.00297483
	LOSS [training: 0.177992278528989 | validation: 0.32119666334193264]
	TIME [epoch: 6.01 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16725863758913811		[learning rate: 0.0029617]
	Learning Rate: 0.00296168
	LOSS [training: 0.16725863758913811 | validation: 0.2972164142349095]
	TIME [epoch: 6.02 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17923912429404973		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.17923912429404973 | validation: 0.32642690162013477]
	TIME [epoch: 6.01 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17900115269269107		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.17900115269269107 | validation: 0.3078418330899136]
	TIME [epoch: 6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17653650749193234		[learning rate: 0.0029226]
	Learning Rate: 0.0029226
	LOSS [training: 0.17653650749193234 | validation: 0.30224897069171663]
	TIME [epoch: 6.02 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1871032896792313		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 0.1871032896792313 | validation: 0.3006701714128709]
	TIME [epoch: 6.01 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17617290537742347		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.17617290537742347 | validation: 0.3199307451362697]
	TIME [epoch: 6.01 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17388529119785148		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.17388529119785148 | validation: 0.3000976248700769]
	TIME [epoch: 6.01 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1870867767495324		[learning rate: 0.0028713]
	Learning Rate: 0.00287129
	LOSS [training: 0.1870867767495324 | validation: 0.31903435794050966]
	TIME [epoch: 6.01 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1701315979345641		[learning rate: 0.0028586]
	Learning Rate: 0.0028586
	LOSS [training: 0.1701315979345641 | validation: 0.32659780826417556]
	TIME [epoch: 6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17476593755262262		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.17476593755262262 | validation: 0.30229177118124]
	TIME [epoch: 6 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16951901248714693		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.16951901248714693 | validation: 0.3112547332640491]
	TIME [epoch: 6.01 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17644294071086689		[learning rate: 0.0028209]
	Learning Rate: 0.00282088
	LOSS [training: 0.17644294071086689 | validation: 0.2918023318038835]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17170926881632959		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 0.17170926881632959 | validation: 0.306004720199129]
	TIME [epoch: 6 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16751682097107068		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.16751682097107068 | validation: 0.3000374494008306]
	TIME [epoch: 6.01 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17391924027656325		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.17391924027656325 | validation: 0.29785865987786525]
	TIME [epoch: 6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18225379605479375		[learning rate: 0.0027714]
	Learning Rate: 0.00277136
	LOSS [training: 0.18225379605479375 | validation: 0.31529591710129345]
	TIME [epoch: 6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1822053886065556		[learning rate: 0.0027591]
	Learning Rate: 0.00275911
	LOSS [training: 0.1822053886065556 | validation: 0.31466992291745144]
	TIME [epoch: 6.01 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17952029433159128		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.17952029433159128 | validation: 0.29451431070522144]
	TIME [epoch: 6.01 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17410038240681386		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.17410038240681386 | validation: 0.31073462141923736]
	TIME [epoch: 6.01 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1816186919634808		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.1816186919634808 | validation: 0.3176772357367721]
	TIME [epoch: 6.01 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16836450176027118		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 0.16836450176027118 | validation: 0.3134321406615512]
	TIME [epoch: 6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16486368514457211		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.16486368514457211 | validation: 0.3164104405438282]
	TIME [epoch: 6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1693683493657444		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.1693683493657444 | validation: 0.3223554892838611]
	TIME [epoch: 6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17592728290528808		[learning rate: 0.0026749]
	Learning Rate: 0.0026749
	LOSS [training: 0.17592728290528808 | validation: 0.2971023417200809]
	TIME [epoch: 6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17148176018200406		[learning rate: 0.0026631]
	Learning Rate: 0.00266308
	LOSS [training: 0.17148176018200406 | validation: 0.3052579385697382]
	TIME [epoch: 6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1762573918590584		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.1762573918590584 | validation: 0.2956917733076178]
	TIME [epoch: 6.01 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18087959824340621		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.18087959824340621 | validation: 0.31247623410567266]
	TIME [epoch: 6.01 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17846299985644457		[learning rate: 0.0026279]
	Learning Rate: 0.00262794
	LOSS [training: 0.17846299985644457 | validation: 0.2957244592929568]
	TIME [epoch: 6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1690157993717263		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 0.1690157993717263 | validation: 0.3492431945114812]
	TIME [epoch: 6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17728870155614856		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.17728870155614856 | validation: 0.3065260290178694]
	TIME [epoch: 6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17041126676620946		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.17041126676620946 | validation: 0.31614732042220217]
	TIME [epoch: 6 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16669463585914335		[learning rate: 0.0025818]
	Learning Rate: 0.0025818
	LOSS [training: 0.16669463585914335 | validation: 0.30977826670493624]
	TIME [epoch: 6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1774711991949442		[learning rate: 0.0025704]
	Learning Rate: 0.0025704
	LOSS [training: 0.1774711991949442 | validation: 0.28355076881775576]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17662609426874132		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.17662609426874132 | validation: 0.3017669233253178]
	TIME [epoch: 6.02 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17098239724607586		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.17098239724607586 | validation: 0.29939429059263595]
	TIME [epoch: 6.01 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17391329957860355		[learning rate: 0.0025365]
	Learning Rate: 0.00253648
	LOSS [training: 0.17391329957860355 | validation: 0.31698303330650374]
	TIME [epoch: 6.29 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1762082824747302		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 0.1762082824747302 | validation: 0.3235575937303379]
	TIME [epoch: 6.02 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18053880565576055		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.18053880565576055 | validation: 0.32105117043167]
	TIME [epoch: 6.02 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17254491012235632		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.17254491012235632 | validation: 0.30552562290109675]
	TIME [epoch: 6.02 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17034468679897005		[learning rate: 0.0024919]
	Learning Rate: 0.00249194
	LOSS [training: 0.17034468679897005 | validation: 0.32851966503428387]
	TIME [epoch: 6.02 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16944339349676552		[learning rate: 0.0024809]
	Learning Rate: 0.00248093
	LOSS [training: 0.16944339349676552 | validation: 0.31867776863214875]
	TIME [epoch: 6.02 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17158027890408448		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.17158027890408448 | validation: 0.39255884616493714]
	TIME [epoch: 6.02 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1683905256846609		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.1683905256846609 | validation: 0.3186199148516645]
	TIME [epoch: 6.01 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16465053214695113		[learning rate: 0.0024482]
	Learning Rate: 0.0024482
	LOSS [training: 0.16465053214695113 | validation: 0.3413097099135235]
	TIME [epoch: 6.02 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1749051405502034		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 0.1749051405502034 | validation: 0.34249867582306687]
	TIME [epoch: 6.02 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1675332607897409		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.1675332607897409 | validation: 0.3058424222671028]
	TIME [epoch: 6.01 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17152612521787897		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.17152612521787897 | validation: 0.3280758038596008]
	TIME [epoch: 6.01 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21022635888382557		[learning rate: 0.0024052]
	Learning Rate: 0.00240521
	LOSS [training: 0.21022635888382557 | validation: 0.29097796297162665]
	TIME [epoch: 6.01 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1704246359242402		[learning rate: 0.0023946]
	Learning Rate: 0.00239459
	LOSS [training: 0.1704246359242402 | validation: 0.2914241078357613]
	TIME [epoch: 6.02 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17160949191306937		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.17160949191306937 | validation: 0.32397133243729315]
	TIME [epoch: 6.02 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1677128628801463		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.1677128628801463 | validation: 0.2939684210825914]
	TIME [epoch: 6.02 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16945699685391466		[learning rate: 0.002363]
	Learning Rate: 0.00236299
	LOSS [training: 0.16945699685391466 | validation: 0.2940847353779767]
	TIME [epoch: 6.01 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1702933969374148		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 0.1702933969374148 | validation: 0.3204007393633574]
	TIME [epoch: 6.02 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1709393748310066		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.1709393748310066 | validation: 0.322142209363969]
	TIME [epoch: 6.01 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16860536732433745		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.16860536732433745 | validation: 0.32349266009030947]
	TIME [epoch: 6.01 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17425581867449072		[learning rate: 0.0023215]
	Learning Rate: 0.0023215
	LOSS [training: 0.17425581867449072 | validation: 0.2991429347104948]
	TIME [epoch: 6.02 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16913771813826367		[learning rate: 0.0023112]
	Learning Rate: 0.00231125
	LOSS [training: 0.16913771813826367 | validation: 0.3059340935740998]
	TIME [epoch: 6.03 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16730063797572986		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.16730063797572986 | validation: 0.2923650733113811]
	TIME [epoch: 6.01 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17069483463043528		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.17069483463043528 | validation: 0.2958893484065555]
	TIME [epoch: 6.01 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16935247233036999		[learning rate: 0.0022807]
	Learning Rate: 0.00228075
	LOSS [training: 0.16935247233036999 | validation: 0.3089061903307992]
	TIME [epoch: 6.02 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16184279244910366		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 0.16184279244910366 | validation: 0.29125997353242256]
	TIME [epoch: 6.02 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17016490746338672		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.17016490746338672 | validation: 0.36162524547748987]
	TIME [epoch: 6.01 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20305564543684457		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.20305564543684457 | validation: 0.28425910257209597]
	TIME [epoch: 6.02 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17627003649240583		[learning rate: 0.0022407]
	Learning Rate: 0.0022407
	LOSS [training: 0.17627003649240583 | validation: 0.31770546877556666]
	TIME [epoch: 6.02 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17655849610491942		[learning rate: 0.0022308]
	Learning Rate: 0.0022308
	LOSS [training: 0.17655849610491942 | validation: 0.30689208097528936]
	TIME [epoch: 6.02 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1647501746913087		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.1647501746913087 | validation: 0.2911181322747043]
	TIME [epoch: 6.02 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16634804008668094		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.16634804008668094 | validation: 0.3191401642244648]
	TIME [epoch: 6.02 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17788984413900186		[learning rate: 0.0022014]
	Learning Rate: 0.00220137
	LOSS [training: 0.17788984413900186 | validation: 0.31143769286443085]
	TIME [epoch: 6.02 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1690721397411105		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 0.1690721397411105 | validation: 0.3120363873836309]
	TIME [epoch: 6.02 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16781953564574292		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.16781953564574292 | validation: 0.3127512929791746]
	TIME [epoch: 6.02 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1665440717753202		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.1665440717753202 | validation: 0.32973737487437416]
	TIME [epoch: 6.02 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16381490520787775		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.16381490520787775 | validation: 0.32310554695694366]
	TIME [epoch: 6.03 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.176323525771917		[learning rate: 0.0021532]
	Learning Rate: 0.00215316
	LOSS [training: 0.176323525771917 | validation: 0.30263180917432586]
	TIME [epoch: 6.02 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16893604007680268		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.16893604007680268 | validation: 0.3083641542994312]
	TIME [epoch: 6.02 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.171413932820768		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.171413932820768 | validation: 0.30238130646457695]
	TIME [epoch: 6.02 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18150529149738748		[learning rate: 0.0021247]
	Learning Rate: 0.00212475
	LOSS [training: 0.18150529149738748 | validation: 0.3057824497267046]
	TIME [epoch: 6.01 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16506174503338733		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 0.16506174503338733 | validation: 0.3197280162223784]
	TIME [epoch: 6.01 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16633688859046597		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.16633688859046597 | validation: 0.30456218551092057]
	TIME [epoch: 6.02 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16207173560182764		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.16207173560182764 | validation: 0.30298946424506296]
	TIME [epoch: 6.02 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17030314460106205		[learning rate: 0.0020874]
	Learning Rate: 0.00208745
	LOSS [training: 0.17030314460106205 | validation: 0.2914058752691795]
	TIME [epoch: 6.04 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18128254050138892		[learning rate: 0.0020782]
	Learning Rate: 0.00207822
	LOSS [training: 0.18128254050138892 | validation: 0.3017289376167556]
	TIME [epoch: 6.02 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1661675100131222		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.1661675100131222 | validation: 0.2963012978429809]
	TIME [epoch: 6.02 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15931081772510058		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.15931081772510058 | validation: 0.3157184146702952]
	TIME [epoch: 6.03 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17214647979282155		[learning rate: 0.0020508]
	Learning Rate: 0.0020508
	LOSS [training: 0.17214647979282155 | validation: 0.3026602050052519]
	TIME [epoch: 6.02 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16698216672187494		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 0.16698216672187494 | validation: 0.30788440259840866]
	TIME [epoch: 6.02 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1645047604011362		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.1645047604011362 | validation: 0.32004996333312175]
	TIME [epoch: 6.02 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1640039178385379		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.1640039178385379 | validation: 0.31251634628978725]
	TIME [epoch: 6.03 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16721152476634632		[learning rate: 0.0020148]
	Learning Rate: 0.00201479
	LOSS [training: 0.16721152476634632 | validation: 0.32529501244065384]
	TIME [epoch: 6.02 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16708391484415328		[learning rate: 0.0020059]
	Learning Rate: 0.00200589
	LOSS [training: 0.16708391484415328 | validation: 0.30384758639879583]
	TIME [epoch: 6.01 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18471361601718259		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.18471361601718259 | validation: 0.3068738648909276]
	TIME [epoch: 6.01 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.166750026247897		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.166750026247897 | validation: 0.3197511678506749]
	TIME [epoch: 6.01 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16054426353616744		[learning rate: 0.0019794]
	Learning Rate: 0.00197942
	LOSS [training: 0.16054426353616744 | validation: 0.3105707740266534]
	TIME [epoch: 6.01 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16573687993553873		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 0.16573687993553873 | validation: 0.29306950823682126]
	TIME [epoch: 6.01 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16313500883457405		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.16313500883457405 | validation: 0.3069420041240312]
	TIME [epoch: 6.02 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17096705249708652		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.17096705249708652 | validation: 0.29079364576197064]
	TIME [epoch: 6.03 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16144805352937003		[learning rate: 0.0019447]
	Learning Rate: 0.00194467
	LOSS [training: 0.16144805352937003 | validation: 0.2893709607880609]
	TIME [epoch: 6.02 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17012809248621216		[learning rate: 0.0019361]
	Learning Rate: 0.00193608
	LOSS [training: 0.17012809248621216 | validation: 0.33120848198384484]
	TIME [epoch: 6.01 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17189045017122648		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.17189045017122648 | validation: 0.3241130627911794]
	TIME [epoch: 6.01 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17159602737381846		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.17159602737381846 | validation: 0.2915595852218613]
	TIME [epoch: 6.01 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16815419481107918		[learning rate: 0.0019105]
	Learning Rate: 0.00191053
	LOSS [training: 0.16815419481107918 | validation: 0.3060292391966698]
	TIME [epoch: 6.01 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16835382860151055		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 0.16835382860151055 | validation: 0.32541308152988785]
	TIME [epoch: 6.01 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17057481279875936		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.17057481279875936 | validation: 0.3029486764141484]
	TIME [epoch: 6.02 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16871327427931743		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.16871327427931743 | validation: 0.29412472111541244]
	TIME [epoch: 6.02 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16789602034712453		[learning rate: 0.001877]
	Learning Rate: 0.00187699
	LOSS [training: 0.16789602034712453 | validation: 0.28851071271908146]
	TIME [epoch: 6.01 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16507431364421624		[learning rate: 0.0018687]
	Learning Rate: 0.0018687
	LOSS [training: 0.16507431364421624 | validation: 0.3066673740745159]
	TIME [epoch: 6.01 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1609741389073575		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.1609741389073575 | validation: 0.28780677335440585]
	TIME [epoch: 6.01 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1588726962002942		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.1588726962002942 | validation: 0.298769714838936]
	TIME [epoch: 6.01 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1717893173762904		[learning rate: 0.001844]
	Learning Rate: 0.00184404
	LOSS [training: 0.1717893173762904 | validation: 0.29997183426429797]
	TIME [epoch: 6.01 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16593119091422345		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 0.16593119091422345 | validation: 0.3281783173545832]
	TIME [epoch: 6.01 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16129379472651015		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.16129379472651015 | validation: 0.3180665927514098]
	TIME [epoch: 6.02 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1620080981286729		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1620080981286729 | validation: 0.3054087236743725]
	TIME [epoch: 6.01 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16314832210666558		[learning rate: 0.0018117]
	Learning Rate: 0.00181166
	LOSS [training: 0.16314832210666558 | validation: 0.31179648884818084]
	TIME [epoch: 6.01 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16314881350418237		[learning rate: 0.0018037]
	Learning Rate: 0.00180366
	LOSS [training: 0.16314881350418237 | validation: 0.3200372364012563]
	TIME [epoch: 6.01 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.175706829894707		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.175706829894707 | validation: 0.32256113821646215]
	TIME [epoch: 6.02 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17513858456128756		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.17513858456128756 | validation: 0.30360178574292646]
	TIME [epoch: 6.01 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16890816193300223		[learning rate: 0.0017799]
	Learning Rate: 0.00177985
	LOSS [training: 0.16890816193300223 | validation: 0.29381824599411027]
	TIME [epoch: 6.01 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16207242510653935		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 0.16207242510653935 | validation: 0.30073725105589655]
	TIME [epoch: 6.02 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1587394635171126		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.1587394635171126 | validation: 0.3069099625195857]
	TIME [epoch: 6.01 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16348968905143324		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.16348968905143324 | validation: 0.30756892653805923]
	TIME [epoch: 6.01 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17044608319151347		[learning rate: 0.0017486]
	Learning Rate: 0.00174861
	LOSS [training: 0.17044608319151347 | validation: 0.29264914982538726]
	TIME [epoch: 6.01 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1597836945385515		[learning rate: 0.0017409]
	Learning Rate: 0.00174088
	LOSS [training: 0.1597836945385515 | validation: 0.29792807711273417]
	TIME [epoch: 6.01 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1697084311384756		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.1697084311384756 | validation: 0.29962516718305215]
	TIME [epoch: 6.01 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17229538055612134		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.17229538055612134 | validation: 0.29005701991800287]
	TIME [epoch: 6 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16455768820804922		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.16455768820804922 | validation: 0.3030271216767194]
	TIME [epoch: 6.01 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1634952335300931		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 0.1634952335300931 | validation: 0.2956194183759881]
	TIME [epoch: 6.02 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1652837781601833		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.1652837781601833 | validation: 0.2974561842651947]
	TIME [epoch: 6.01 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17307972673736344		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.17307972673736344 | validation: 0.29131236885341094]
	TIME [epoch: 6.01 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1675751749109139		[learning rate: 0.0016877]
	Learning Rate: 0.00168775
	LOSS [training: 0.1675751749109139 | validation: 0.3208097946691402]
	TIME [epoch: 6.01 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16635506894785523		[learning rate: 0.0016803]
	Learning Rate: 0.00168029
	LOSS [training: 0.16635506894785523 | validation: 0.2922883598890539]
	TIME [epoch: 6.01 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16387216973069102		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.16387216973069102 | validation: 0.2915678837366165]
	TIME [epoch: 6.01 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16533539275460124		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.16533539275460124 | validation: 0.28948891889503625]
	TIME [epoch: 6.01 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16691421250705943		[learning rate: 0.0016581]
	Learning Rate: 0.00165812
	LOSS [training: 0.16691421250705943 | validation: 0.27462362316850575]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16323460466559536		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 0.16323460466559536 | validation: 0.29886722835024193]
	TIME [epoch: 6.02 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15957567594829364		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.15957567594829364 | validation: 0.3095264365406354]
	TIME [epoch: 6.01 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1772431656038888		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1772431656038888 | validation: 0.32732233587663856]
	TIME [epoch: 6.01 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18396692662905753		[learning rate: 0.001629]
	Learning Rate: 0.00162901
	LOSS [training: 0.18396692662905753 | validation: 0.30000080504138776]
	TIME [epoch: 6.01 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16805917512890256		[learning rate: 0.0016218]
	Learning Rate: 0.00162181
	LOSS [training: 0.16805917512890256 | validation: 0.31442873382890046]
	TIME [epoch: 6.01 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16299082751476793		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.16299082751476793 | validation: 0.29962435537626847]
	TIME [epoch: 6 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16458370565404296		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.16458370565404296 | validation: 0.30358787485213334]
	TIME [epoch: 6.01 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16080857142266977		[learning rate: 0.0016004]
	Learning Rate: 0.00160041
	LOSS [training: 0.16080857142266977 | validation: 0.2975222685144761]
	TIME [epoch: 6.01 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1611170257348819		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 0.1611170257348819 | validation: 0.2937453842353663]
	TIME [epoch: 6.01 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16068231153072832		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.16068231153072832 | validation: 0.31174958011616266]
	TIME [epoch: 6 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16246663136365092		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.16246663136365092 | validation: 0.300182951469008]
	TIME [epoch: 6.01 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16430723459326896		[learning rate: 0.0015723]
	Learning Rate: 0.00157231
	LOSS [training: 0.16430723459326896 | validation: 0.3021756917094667]
	TIME [epoch: 6.02 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16269716545325721		[learning rate: 0.0015654]
	Learning Rate: 0.00156536
	LOSS [training: 0.16269716545325721 | validation: 0.2964410812609427]
	TIME [epoch: 6.01 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16176646847381362		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.16176646847381362 | validation: 0.2951993142012425]
	TIME [epoch: 6.01 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16191312649992962		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.16191312649992962 | validation: 0.29682090546213397]
	TIME [epoch: 6.02 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15937781168608683		[learning rate: 0.0015447]
	Learning Rate: 0.00154471
	LOSS [training: 0.15937781168608683 | validation: 0.32987782525798387]
	TIME [epoch: 6.01 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16466751743122374		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.16466751743122374 | validation: 0.29676272654139146]
	TIME [epoch: 6.01 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16368301335938057		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.16368301335938057 | validation: 0.2964882197816882]
	TIME [epoch: 6.01 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16249933737883193		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.16249933737883193 | validation: 0.3161796383710006]
	TIME [epoch: 6.01 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1618824871753603		[learning rate: 0.0015176]
	Learning Rate: 0.00151759
	LOSS [training: 0.1618824871753603 | validation: 0.2877159697071811]
	TIME [epoch: 6.01 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16426250272659654		[learning rate: 0.0015109]
	Learning Rate: 0.00151088
	LOSS [training: 0.16426250272659654 | validation: 0.32088847821424665]
	TIME [epoch: 6.01 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16674945412154893		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.16674945412154893 | validation: 0.3161562756430997]
	TIME [epoch: 6.01 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16327575626704138		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.16327575626704138 | validation: 0.29466060202079714]
	TIME [epoch: 6.02 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16120077644092987		[learning rate: 0.0014909]
	Learning Rate: 0.00149094
	LOSS [training: 0.16120077644092987 | validation: 0.2987156611511657]
	TIME [epoch: 6.01 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16239329194749041		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 0.16239329194749041 | validation: 0.2984808394307812]
	TIME [epoch: 6.01 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16594189292550776		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.16594189292550776 | validation: 0.3007715834623088]
	TIME [epoch: 6.01 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17278583554574237		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.17278583554574237 | validation: 0.29745380556933737]
	TIME [epoch: 6.01 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16698306563949672		[learning rate: 0.0014648]
	Learning Rate: 0.00146477
	LOSS [training: 0.16698306563949672 | validation: 0.33888776472189325]
	TIME [epoch: 6.01 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15986301532910327		[learning rate: 0.0014583]
	Learning Rate: 0.0014583
	LOSS [training: 0.15986301532910327 | validation: 0.29122867207171793]
	TIME [epoch: 6.01 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1661190271603233		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.1661190271603233 | validation: 0.3022691323448642]
	TIME [epoch: 6.02 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1585347209817068		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1585347209817068 | validation: 0.2866083385523825]
	TIME [epoch: 6.01 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15996922273804257		[learning rate: 0.0014391]
	Learning Rate: 0.00143905
	LOSS [training: 0.15996922273804257 | validation: 0.3082553227762215]
	TIME [epoch: 6.01 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16854727405624878		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 0.16854727405624878 | validation: 0.30324458021473816]
	TIME [epoch: 6.01 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16979046332490463		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.16979046332490463 | validation: 0.30193392717487866]
	TIME [epoch: 6.01 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16316610751651978		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.16316610751651978 | validation: 0.3314988167876339]
	TIME [epoch: 6.01 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1677965800040472		[learning rate: 0.0014138]
	Learning Rate: 0.00141379
	LOSS [training: 0.1677965800040472 | validation: 0.30191987339118226]
	TIME [epoch: 6.02 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1569902113499313		[learning rate: 0.0014075]
	Learning Rate: 0.00140754
	LOSS [training: 0.1569902113499313 | validation: 0.3084470876344173]
	TIME [epoch: 6.02 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15741828208890657		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.15741828208890657 | validation: 0.29697578159202354]
	TIME [epoch: 6.01 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15765097314724388		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.15765097314724388 | validation: 0.3136281041506713]
	TIME [epoch: 6.02 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16608936811336564		[learning rate: 0.001389]
	Learning Rate: 0.00138897
	LOSS [training: 0.16608936811336564 | validation: 0.31274123857207925]
	TIME [epoch: 6.01 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16655289822755756		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 0.16655289822755756 | validation: 0.30310979462708687]
	TIME [epoch: 6.02 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15689847395582696		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.15689847395582696 | validation: 0.2940784815582357]
	TIME [epoch: 6.02 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15631127998095726		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.15631127998095726 | validation: 0.293109845751941]
	TIME [epoch: 6.01 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1618496173552268		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.1618496173552268 | validation: 0.33528457901201475]
	TIME [epoch: 6.02 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15256134554124148		[learning rate: 0.0013586]
	Learning Rate: 0.00135855
	LOSS [training: 0.15256134554124148 | validation: 0.3096206559473074]
	TIME [epoch: 6.03 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16184728994333497		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.16184728994333497 | validation: 0.2814918918103376]
	TIME [epoch: 6.02 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16446700894344948		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.16446700894344948 | validation: 0.3146293036889025]
	TIME [epoch: 6.01 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16420918461682038		[learning rate: 0.0013406]
	Learning Rate: 0.00134063
	LOSS [training: 0.16420918461682038 | validation: 0.2921230360728308]
	TIME [epoch: 6.02 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15903712584891544		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 0.15903712584891544 | validation: 0.31912009241196215]
	TIME [epoch: 6.01 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16221923602751862		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.16221923602751862 | validation: 0.32161991140514584]
	TIME [epoch: 6.02 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16202139140947122		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.16202139140947122 | validation: 0.326490622195737]
	TIME [epoch: 6.01 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15914413707749295		[learning rate: 0.0013171]
	Learning Rate: 0.00131709
	LOSS [training: 0.15914413707749295 | validation: 0.30243309766139176]
	TIME [epoch: 6.02 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15607159077988478		[learning rate: 0.0013113]
	Learning Rate: 0.00131127
	LOSS [training: 0.15607159077988478 | validation: 0.3265904858413707]
	TIME [epoch: 6.03 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16098194989287684		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.16098194989287684 | validation: 0.3156602590999434]
	TIME [epoch: 6.02 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16676031680207484		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.16676031680207484 | validation: 0.3053035721059843]
	TIME [epoch: 32.8 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15645593473640307		[learning rate: 0.001294]
	Learning Rate: 0.00129397
	LOSS [training: 0.15645593473640307 | validation: 0.30369086975539183]
	TIME [epoch: 11.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15553297228201443		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 0.15553297228201443 | validation: 0.31604479507658506]
	TIME [epoch: 11.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16253303864249763		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.16253303864249763 | validation: 0.298167832239732]
	TIME [epoch: 11.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16387615499422065		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.16387615499422065 | validation: 0.2921207774999918]
	TIME [epoch: 11.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16214467966952745		[learning rate: 0.0012712]
	Learning Rate: 0.00127125
	LOSS [training: 0.16214467966952745 | validation: 0.31173037319535674]
	TIME [epoch: 11.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1599242421840751		[learning rate: 0.0012656]
	Learning Rate: 0.00126563
	LOSS [training: 0.1599242421840751 | validation: 0.299904033559595]
	TIME [epoch: 11.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16203208480791367		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.16203208480791367 | validation: 0.3014675405039491]
	TIME [epoch: 11.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16369368295281325		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.16369368295281325 | validation: 0.3149949344163101]
	TIME [epoch: 11.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.161800574508995		[learning rate: 0.0012489]
	Learning Rate: 0.00124893
	LOSS [training: 0.161800574508995 | validation: 0.296486929676678]
	TIME [epoch: 11.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16148902093653453		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 0.16148902093653453 | validation: 0.2953103202421807]
	TIME [epoch: 11.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16062919064805672		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.16062919064805672 | validation: 0.30329965037687096]
	TIME [epoch: 11.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16101110266351643		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.16101110266351643 | validation: 0.2888906616069636]
	TIME [epoch: 11.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16282195958418538		[learning rate: 0.001227]
	Learning Rate: 0.001227
	LOSS [training: 0.16282195958418538 | validation: 0.2843695079179332]
	TIME [epoch: 11.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1637504225743855		[learning rate: 0.0012216]
	Learning Rate: 0.00122158
	LOSS [training: 0.1637504225743855 | validation: 0.31561647231733075]
	TIME [epoch: 11.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15928412273850956		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.15928412273850956 | validation: 0.31383347769843145]
	TIME [epoch: 11.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16577232442617754		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.16577232442617754 | validation: 0.2911753600756944]
	TIME [epoch: 11.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16121546807660542		[learning rate: 0.0012055]
	Learning Rate: 0.00120546
	LOSS [training: 0.16121546807660542 | validation: 0.3047903010394444]
	TIME [epoch: 11.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1577320958377475		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 0.1577320958377475 | validation: 0.2971239090772531]
	TIME [epoch: 11.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1627597526659327		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.1627597526659327 | validation: 0.2889723343951303]
	TIME [epoch: 11.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1649149722222939		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.1649149722222939 | validation: 0.30198126335647374]
	TIME [epoch: 11.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15814346308546035		[learning rate: 0.0011843]
	Learning Rate: 0.0011843
	LOSS [training: 0.15814346308546035 | validation: 0.29372897906321627]
	TIME [epoch: 11.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16412591605349525		[learning rate: 0.0011791]
	Learning Rate: 0.00117907
	LOSS [training: 0.16412591605349525 | validation: 0.31754268932317065]
	TIME [epoch: 11.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17184990779984904		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.17184990779984904 | validation: 0.2841865918108193]
	TIME [epoch: 11.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15797551398775148		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.15797551398775148 | validation: 0.29584451131457723]
	TIME [epoch: 11.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16187907800077436		[learning rate: 0.0011635]
	Learning Rate: 0.00116351
	LOSS [training: 0.16187907800077436 | validation: 0.30779404801957094]
	TIME [epoch: 11.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15619689421143595		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 0.15619689421143595 | validation: 0.2941775773131448]
	TIME [epoch: 11.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1559836552852551		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.1559836552852551 | validation: 0.29849894772022595]
	TIME [epoch: 11.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16500175142225226		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.16500175142225226 | validation: 0.2846773400841467]
	TIME [epoch: 11.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15342470381447748		[learning rate: 0.0011431]
	Learning Rate: 0.00114308
	LOSS [training: 0.15342470381447748 | validation: 0.2987440596864679]
	TIME [epoch: 11.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15560613393336867		[learning rate: 0.001138]
	Learning Rate: 0.00113803
	LOSS [training: 0.15560613393336867 | validation: 0.30746108898539826]
	TIME [epoch: 11.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15963231617308024		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.15963231617308024 | validation: 0.302604602642622]
	TIME [epoch: 11.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15573608091751778		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.15573608091751778 | validation: 0.2939653389964589]
	TIME [epoch: 11.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16019401279602644		[learning rate: 0.001123]
	Learning Rate: 0.00112301
	LOSS [training: 0.16019401279602644 | validation: 0.30666146038120834]
	TIME [epoch: 11.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1596045447755271		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 0.1596045447755271 | validation: 0.28873850786926286]
	TIME [epoch: 11.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15691266919769778		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.15691266919769778 | validation: 0.308894689244615]
	TIME [epoch: 11.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16290166514396692		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.16290166514396692 | validation: 0.29011374444641685]
	TIME [epoch: 11.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15698819633731456		[learning rate: 0.0011033]
	Learning Rate: 0.0011033
	LOSS [training: 0.15698819633731456 | validation: 0.3019363088629094]
	TIME [epoch: 11.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15844090101856872		[learning rate: 0.0010984]
	Learning Rate: 0.00109842
	LOSS [training: 0.15844090101856872 | validation: 0.30418794175034997]
	TIME [epoch: 11.7 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16454036332968958		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.16454036332968958 | validation: 0.3022690448794869]
	TIME [epoch: 11.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16057656946774138		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.16057656946774138 | validation: 0.28593856446563803]
	TIME [epoch: 11.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1545921949977388		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.1545921949977388 | validation: 0.30421176054191523]
	TIME [epoch: 11.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15815343118640898		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 0.15815343118640898 | validation: 0.30475306652410694]
	TIME [epoch: 11.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15884348813910298		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.15884348813910298 | validation: 0.2926036182095698]
	TIME [epoch: 11.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15371385928311773		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.15371385928311773 | validation: 0.3030957199906619]
	TIME [epoch: 11.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16332437728230675		[learning rate: 0.0010649]
	Learning Rate: 0.0010649
	LOSS [training: 0.16332437728230675 | validation: 0.2953504191263135]
	TIME [epoch: 11.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1588485621669415		[learning rate: 0.0010602]
	Learning Rate: 0.00106019
	LOSS [training: 0.1588485621669415 | validation: 0.31352279851807663]
	TIME [epoch: 11.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.164021925674978		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.164021925674978 | validation: 0.30377580568444656]
	TIME [epoch: 11.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15912947056584978		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.15912947056584978 | validation: 0.31327557215787954]
	TIME [epoch: 11.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16265382944533435		[learning rate: 0.0010462]
	Learning Rate: 0.0010462
	LOSS [training: 0.16265382944533435 | validation: 0.30318739667114775]
	TIME [epoch: 11.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15999684089733507		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 0.15999684089733507 | validation: 0.28945591558916894]
	TIME [epoch: 11.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1564962299714412		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.1564962299714412 | validation: 0.30480637270279604]
	TIME [epoch: 11.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15564820332557763		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.15564820332557763 | validation: 0.2886368107361397]
	TIME [epoch: 11.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1615174424337678		[learning rate: 0.0010278]
	Learning Rate: 0.00102783
	LOSS [training: 0.1615174424337678 | validation: 0.29005085919863866]
	TIME [epoch: 11.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15961339362233795		[learning rate: 0.0010233]
	Learning Rate: 0.00102329
	LOSS [training: 0.15961339362233795 | validation: 0.29880935193474123]
	TIME [epoch: 11.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15779142508200925		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.15779142508200925 | validation: 0.31246254168183873]
	TIME [epoch: 11.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15795148312292384		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.15795148312292384 | validation: 0.3061582535980728]
	TIME [epoch: 11.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16996518425291496		[learning rate: 0.0010098]
	Learning Rate: 0.00100979
	LOSS [training: 0.16996518425291496 | validation: 0.29392633335263785]
	TIME [epoch: 11.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1520055286976995		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 0.1520055286976995 | validation: 0.2994010965983176]
	TIME [epoch: 11.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15767051990934855		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.15767051990934855 | validation: 0.31248400075411414]
	TIME [epoch: 11.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15574147741040945		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.15574147741040945 | validation: 0.3039127043781593]
	TIME [epoch: 11.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16161570270052814		[learning rate: 0.00099206]
	Learning Rate: 0.000992061
	LOSS [training: 0.16161570270052814 | validation: 0.2895188082267435]
	TIME [epoch: 11.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1594581012209834		[learning rate: 0.00098768]
	Learning Rate: 0.000987678
	LOSS [training: 0.1594581012209834 | validation: 0.30059302301670415]
	TIME [epoch: 11.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16245884473282712		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.16245884473282712 | validation: 0.3088329311503137]
	TIME [epoch: 11.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16079699473235404		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.16079699473235404 | validation: 0.3022221322939697]
	TIME [epoch: 11.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1585143921027152		[learning rate: 0.00097464]
	Learning Rate: 0.000974644
	LOSS [training: 0.1585143921027152 | validation: 0.3034225189530873]
	TIME [epoch: 11.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15827895386266017		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 0.15827895386266017 | validation: 0.2991700970191824]
	TIME [epoch: 11.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1586655325261757		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.1586655325261757 | validation: 0.29649109022784154]
	TIME [epoch: 11.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16595328236563994		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.16595328236563994 | validation: 0.29246138604895827]
	TIME [epoch: 11.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15359136136429657		[learning rate: 0.00095753]
	Learning Rate: 0.000957533
	LOSS [training: 0.15359136136429657 | validation: 0.3022764110454194]
	TIME [epoch: 11.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15899940861072884		[learning rate: 0.0009533]
	Learning Rate: 0.000953303
	LOSS [training: 0.15899940861072884 | validation: 0.3027941111531462]
	TIME [epoch: 11.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1627136613108065		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.1627136613108065 | validation: 0.29851233541610184]
	TIME [epoch: 11.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1555408927429278		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.1555408927429278 | validation: 0.3171609225493943]
	TIME [epoch: 11.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15633861101825386		[learning rate: 0.00094072]
	Learning Rate: 0.000940723
	LOSS [training: 0.15633861101825386 | validation: 0.3031134747798921]
	TIME [epoch: 11.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15634136432219134		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 0.15634136432219134 | validation: 0.30410962492332677]
	TIME [epoch: 11.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15664940012270878		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.15664940012270878 | validation: 0.3018799487526813]
	TIME [epoch: 11.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15652454285479772		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.15652454285479772 | validation: 0.2931878691036689]
	TIME [epoch: 11.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16158574286581265		[learning rate: 0.00092421]
	Learning Rate: 0.000924207
	LOSS [training: 0.16158574286581265 | validation: 0.30806137101041586]
	TIME [epoch: 11.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1641097373688576		[learning rate: 0.00092012]
	Learning Rate: 0.000920124
	LOSS [training: 0.1641097373688576 | validation: 0.30024623445768145]
	TIME [epoch: 11.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15604600439868163		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.15604600439868163 | validation: 0.30376743833377967]
	TIME [epoch: 11.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15915377090875066		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.15915377090875066 | validation: 0.2891615951206158]
	TIME [epoch: 11.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16110028952070388		[learning rate: 0.00090798]
	Learning Rate: 0.000907981
	LOSS [training: 0.16110028952070388 | validation: 0.30156269811087294]
	TIME [epoch: 11.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15604642362656485		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 0.15604642362656485 | validation: 0.2981539776057272]
	TIME [epoch: 11.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16045804189793114		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.16045804189793114 | validation: 0.2967819574773996]
	TIME [epoch: 11.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1551581304562054		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.1551581304562054 | validation: 0.30859716693905204]
	TIME [epoch: 11.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15987803252775326		[learning rate: 0.00089204]
	Learning Rate: 0.000892041
	LOSS [training: 0.15987803252775326 | validation: 0.2953430548174035]
	TIME [epoch: 11.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1556813605315494		[learning rate: 0.0008881]
	Learning Rate: 0.000888099
	LOSS [training: 0.1556813605315494 | validation: 0.2840033911868386]
	TIME [epoch: 11.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16211151356620204		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.16211151356620204 | validation: 0.3098026075730181]
	TIME [epoch: 11.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15280584378424322		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.15280584378424322 | validation: 0.28789068452322136]
	TIME [epoch: 11.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.157349773316112		[learning rate: 0.00087638]
	Learning Rate: 0.00087638
	LOSS [training: 0.157349773316112 | validation: 0.2957330682868464]
	TIME [epoch: 11.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15701078758699683		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 0.15701078758699683 | validation: 0.29515900904744646]
	TIME [epoch: 11.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15256177558345008		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.15256177558345008 | validation: 0.2945947639561384]
	TIME [epoch: 11.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15880981471955963		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.15880981471955963 | validation: 0.31926945357493897]
	TIME [epoch: 11.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1560321601437422		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.1560321601437422 | validation: 0.3070606689112358]
	TIME [epoch: 11.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15606764849275973		[learning rate: 0.00085719]
	Learning Rate: 0.00085719
	LOSS [training: 0.15606764849275973 | validation: 0.2955773888067124]
	TIME [epoch: 11.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15578530309118419		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.15578530309118419 | validation: 0.3032430079335097]
	TIME [epoch: 11.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16368869571052244		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.16368869571052244 | validation: 0.32651322826739526]
	TIME [epoch: 11.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15863865721510403		[learning rate: 0.00084588]
	Learning Rate: 0.000845878
	LOSS [training: 0.15863865721510403 | validation: 0.31122528907606267]
	TIME [epoch: 11.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16074155169451726		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 0.16074155169451726 | validation: 0.29749479641890375]
	TIME [epoch: 11.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15917742518710817		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.15917742518710817 | validation: 0.3220755547827981]
	TIME [epoch: 11.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15899518008467745		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.15899518008467745 | validation: 0.28804148602180096]
	TIME [epoch: 11.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1598227900463863		[learning rate: 0.00083103]
	Learning Rate: 0.000831028
	LOSS [training: 0.1598227900463863 | validation: 0.2973176941673271]
	TIME [epoch: 11.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15283392310868835		[learning rate: 0.00082736]
	Learning Rate: 0.000827356
	LOSS [training: 0.15283392310868835 | validation: 0.30673909887123507]
	TIME [epoch: 11.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15949446566284334		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.15949446566284334 | validation: 0.3216108205338877]
	TIME [epoch: 11.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15607079029827603		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.15607079029827603 | validation: 0.31113111552468214]
	TIME [epoch: 11.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15729736322431687		[learning rate: 0.00081644]
	Learning Rate: 0.000816438
	LOSS [training: 0.15729736322431687 | validation: 0.2959050212809995]
	TIME [epoch: 11.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15867857644621686		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 0.15867857644621686 | validation: 0.2947829890811742]
	TIME [epoch: 11.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1552432396367061		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.1552432396367061 | validation: 0.2961441831586904]
	TIME [epoch: 11.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16356551329507346		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.16356551329507346 | validation: 0.30591449531632686]
	TIME [epoch: 11.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16039801579927393		[learning rate: 0.0008021]
	Learning Rate: 0.000802104
	LOSS [training: 0.16039801579927393 | validation: 0.2982350265886686]
	TIME [epoch: 11.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16266014374694013		[learning rate: 0.00079856]
	Learning Rate: 0.00079856
	LOSS [training: 0.16266014374694013 | validation: 0.30155860514942545]
	TIME [epoch: 11.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16154071307782822		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.16154071307782822 | validation: 0.3046132380863315]
	TIME [epoch: 11.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16568768259772812		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.16568768259772812 | validation: 0.29668871431905675]
	TIME [epoch: 11.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1513395609036206		[learning rate: 0.00078802]
	Learning Rate: 0.000788022
	LOSS [training: 0.1513395609036206 | validation: 0.2997163869237183]
	TIME [epoch: 11.7 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15250297872448934		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 0.15250297872448934 | validation: 0.2952724467696607]
	TIME [epoch: 11.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15524164119882314		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.15524164119882314 | validation: 0.2925950722650185]
	TIME [epoch: 11.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16186308811554542		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.16186308811554542 | validation: 0.32756271506221535]
	TIME [epoch: 11.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15559153554601454		[learning rate: 0.00077419]
	Learning Rate: 0.000774188
	LOSS [training: 0.15559153554601454 | validation: 0.2967473073411459]
	TIME [epoch: 11.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1549851284387949		[learning rate: 0.00077077]
	Learning Rate: 0.000770767
	LOSS [training: 0.1549851284387949 | validation: 0.30379349935640415]
	TIME [epoch: 11.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1587059108460713		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.1587059108460713 | validation: 0.31032290579316185]
	TIME [epoch: 11.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15728092657080697		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.15728092657080697 | validation: 0.2883964204567296]
	TIME [epoch: 11.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15969879623545544		[learning rate: 0.0007606]
	Learning Rate: 0.000760596
	LOSS [training: 0.15969879623545544 | validation: 0.3007402307506947]
	TIME [epoch: 11.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15910442362792007		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 0.15910442362792007 | validation: 0.32162857161567915]
	TIME [epoch: 11.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16519218841937952		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.16519218841937952 | validation: 0.2901044080988677]
	TIME [epoch: 11.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15835110429897184		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.15835110429897184 | validation: 0.29579372637997836]
	TIME [epoch: 11.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15517246205497573		[learning rate: 0.00074724]
	Learning Rate: 0.000747242
	LOSS [training: 0.15517246205497573 | validation: 0.2995937382698923]
	TIME [epoch: 11.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15435509162080238		[learning rate: 0.00074394]
	Learning Rate: 0.000743941
	LOSS [training: 0.15435509162080238 | validation: 0.3079522734161985]
	TIME [epoch: 11.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15383075724006487		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.15383075724006487 | validation: 0.2844829253229287]
	TIME [epoch: 11.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15613563381441004		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.15613563381441004 | validation: 0.29318165685164305]
	TIME [epoch: 11.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15794984443458093		[learning rate: 0.00073412]
	Learning Rate: 0.000734124
	LOSS [training: 0.15794984443458093 | validation: 0.3069323416033542]
	TIME [epoch: 11.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15642601412501295		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 0.15642601412501295 | validation: 0.2782851334838311]
	TIME [epoch: 11.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15639422675019607		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.15639422675019607 | validation: 0.2970734563595915]
	TIME [epoch: 11.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1576440602118852		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.1576440602118852 | validation: 0.30981639824809964]
	TIME [epoch: 11.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15682105032829952		[learning rate: 0.00072124]
	Learning Rate: 0.000721235
	LOSS [training: 0.15682105032829952 | validation: 0.30788899888448157]
	TIME [epoch: 11.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1576091093486862		[learning rate: 0.00071805]
	Learning Rate: 0.000718049
	LOSS [training: 0.1576091093486862 | validation: 0.2984641929642339]
	TIME [epoch: 11.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15638328116821462		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.15638328116821462 | validation: 0.2958255725506606]
	TIME [epoch: 11.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16152049086653422		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.16152049086653422 | validation: 0.3013374985335267]
	TIME [epoch: 11.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1499798226908758		[learning rate: 0.00070857]
	Learning Rate: 0.000708573
	LOSS [training: 0.1499798226908758 | validation: 0.30056059195257223]
	TIME [epoch: 11.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1637023155087093		[learning rate: 0.00070544]
	Learning Rate: 0.000705442
	LOSS [training: 0.1637023155087093 | validation: 0.2894148029035357]
	TIME [epoch: 11.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1533709812372715		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.1533709812372715 | validation: 0.30056247412289255]
	TIME [epoch: 11.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15706028836118796		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.15706028836118796 | validation: 0.292290463018764]
	TIME [epoch: 11.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15946793920731567		[learning rate: 0.00069613]
	Learning Rate: 0.000696133
	LOSS [training: 0.15946793920731567 | validation: 0.3015908743316075]
	TIME [epoch: 11.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15250443478419468		[learning rate: 0.00069306]
	Learning Rate: 0.000693058
	LOSS [training: 0.15250443478419468 | validation: 0.2988432791120388]
	TIME [epoch: 11.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15555743955600007		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.15555743955600007 | validation: 0.305226174925811]
	TIME [epoch: 11.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1555904375318932		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.1555904375318932 | validation: 0.3018499307490239]
	TIME [epoch: 11.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1526678003829432		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.1526678003829432 | validation: 0.3013493767140832]
	TIME [epoch: 11.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15853792443903655		[learning rate: 0.00068089]
	Learning Rate: 0.00068089
	LOSS [training: 0.15853792443903655 | validation: 0.28803415075661476]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_facs_v2_dec2b_2dpca_v8_20240712_121611/states/model_facs_v2_dec2b_2dpca_v8_647.pth
Halted early. No improvement in validation loss for 200 epochs.
Finished training in 4807.723 seconds.
