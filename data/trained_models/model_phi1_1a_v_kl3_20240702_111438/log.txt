Args:
Namespace(name='model_phi1_1a_v_kl3', outdir='out/model_training/model_phi1_1a_v_kl3', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1403886225

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 12.050954257414123		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 12.050954257414123 | validation: 12.019930340974366]
	TIME [epoch: 112 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.448854553037421		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 11.448854553037421 | validation: 10.972771024406175]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.64175215551221		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 10.64175215551221 | validation: 10.348958816868581]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.932994316490626		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 9.932994316490626 | validation: 9.750182608063628]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.61161587532039		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 9.61161587532039 | validation: 9.19886861607232]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.107417433101212		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 9.107417433101212 | validation: 9.10601704545753]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.70419082983439		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 8.70419082983439 | validation: 8.476858617630077]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.251170429050521		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 8.251170429050521 | validation: 8.541109884395325]
	TIME [epoch: 7.66 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.877010450020315		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 7.877010450020315 | validation: 7.612924001736174]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.379385669971645		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 7.379385669971645 | validation: 7.350304849806243]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.070036303519041		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 7.070036303519041 | validation: 7.027992151175344]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.092509402900131		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 7.092509402900131 | validation: 7.014208404091063]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.860800647603698		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 6.860800647603698 | validation: 6.898758550884472]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.856879036744306		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 6.856879036744306 | validation: 6.856029642431784]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.848319081563697		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 6.848319081563697 | validation: 6.792609090327962]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.892503637502019		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 6.892503637502019 | validation: 7.012621149641829]
	TIME [epoch: 7.69 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.842773340902239		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 6.842773340902239 | validation: 6.876288944244633]
	TIME [epoch: 7.67 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.7905772420034225		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 6.7905772420034225 | validation: 6.845993944620659]
	TIME [epoch: 7.69 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.730864648589342		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 6.730864648589342 | validation: 6.854710964026739]
	TIME [epoch: 7.68 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.122194269957691		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 7.122194269957691 | validation: 7.1192022074832035]
	TIME [epoch: 7.73 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.8582656017498715		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 6.8582656017498715 | validation: 6.831700802120237]
	TIME [epoch: 7.66 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.60655364787433		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 6.60655364787433 | validation: 6.662295164344377]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.42460180936401		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 6.42460180936401 | validation: 6.3257636576871565]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.018409150629336		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 6.018409150629336 | validation: 6.431461118091709]
	TIME [epoch: 7.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.579666689953438		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 5.579666689953438 | validation: 8.115908832131513]
	TIME [epoch: 7.74 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.2372440700166685		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 6.2372440700166685 | validation: 6.132405670240205]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.510696163444496		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 5.510696163444496 | validation: 6.142652322703336]
	TIME [epoch: 7.73 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.488960507623439		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 5.488960507623439 | validation: 5.941180043139852]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.372561256695189		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 5.372561256695189 | validation: 6.506115631909749]
	TIME [epoch: 7.75 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.819306051606844		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 5.819306051606844 | validation: 5.916654436004921]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.6125902454906855		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 5.6125902454906855 | validation: 6.558806307263664]
	TIME [epoch: 7.71 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.436294270397503		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 6.436294270397503 | validation: 6.366142271726897]
	TIME [epoch: 7.73 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.9850292033232995		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 5.9850292033232995 | validation: 5.7272462614746935]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.771754655027737		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 5.771754655027737 | validation: 7.192651889359219]
	TIME [epoch: 7.78 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.98255169067718		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 5.98255169067718 | validation: 5.07649758409141]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.149656010250935		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 5.149656010250935 | validation: 5.11481294285312]
	TIME [epoch: 7.74 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.84197531303775		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 4.84197531303775 | validation: 4.984818529489813]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878707578228765		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 4.878707578228765 | validation: 4.6576043185963485]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.289611370792584		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 4.289611370792584 | validation: 3.9710016018948577]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9266195500664764		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 3.9266195500664764 | validation: 4.672983370523244]
	TIME [epoch: 7.72 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.877944897509672		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 4.877944897509672 | validation: 5.926889516671982]
	TIME [epoch: 7.7 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.983137344064021		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 4.983137344064021 | validation: 5.974837309864917]
	TIME [epoch: 7.75 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028952931605801		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 5.028952931605801 | validation: 5.937315103365954]
	TIME [epoch: 7.71 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011099135008255		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 5.011099135008255 | validation: 5.854744639952904]
	TIME [epoch: 7.75 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917238050270246		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 4.917238050270246 | validation: 5.846707289772022]
	TIME [epoch: 7.7 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.82064453010498		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 4.82064453010498 | validation: 5.998966321679715]
	TIME [epoch: 7.71 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.090641145840527		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 5.090641145840527 | validation: 10.86604984221343]
	TIME [epoch: 7.71 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.299005767947458		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 9.299005767947458 | validation: 10.322410281372456]
	TIME [epoch: 7.71 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.71411919373282		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 8.71411919373282 | validation: 9.868926542675265]
	TIME [epoch: 7.73 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.881758288385404		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 6.881758288385404 | validation: 5.718093095864383]
	TIME [epoch: 7.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.986547191316655		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 4.986547191316655 | validation: 4.85705623066026]
	TIME [epoch: 7.71 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1776868847634985		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 4.1776868847634985 | validation: 4.8324287794276355]
	TIME [epoch: 7.7 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8112344483921587		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 3.8112344483921587 | validation: 4.051165003149759]
	TIME [epoch: 7.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.020812802328067		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 4.020812802328067 | validation: 3.7977176725487367]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.662469857050407		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 3.662469857050407 | validation: 3.3729565896671607]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.553742134683596		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 4.553742134683596 | validation: 3.608957860626748]
	TIME [epoch: 7.69 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3945729563771696		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 3.3945729563771696 | validation: 3.1686266927443927]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273301969856331		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 3.273301969856331 | validation: 3.486316874780773]
	TIME [epoch: 7.74 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4053927289445203		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 3.4053927289445203 | validation: 2.9426543298984127]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8357810795780924		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 2.8357810795780924 | validation: 2.955832210591247]
	TIME [epoch: 7.66 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3484027614991336		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 3.3484027614991336 | validation: 3.4516735391132602]
	TIME [epoch: 7.7 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9245510195601145		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 2.9245510195601145 | validation: 2.5416077140832796]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842756478274973		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 2.842756478274973 | validation: 2.7401216293499253]
	TIME [epoch: 7.74 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.782237995510114		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 2.782237995510114 | validation: 3.9969797315212547]
	TIME [epoch: 7.66 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6656995166981408		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 2.6656995166981408 | validation: 2.832870659864762]
	TIME [epoch: 7.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6365982327017337		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 2.6365982327017337 | validation: 2.3576592111431625]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4486582882413717		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 2.4486582882413717 | validation: 3.436668642128262]
	TIME [epoch: 7.65 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4698778353947257		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 2.4698778353947257 | validation: 2.430488002055635]
	TIME [epoch: 7.66 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.373898478237325		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 2.373898478237325 | validation: 2.6388023692709175]
	TIME [epoch: 7.63 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3729651549748665		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 2.3729651549748665 | validation: 4.380327976069065]
	TIME [epoch: 7.61 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8339902251515947		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 2.8339902251515947 | validation: 2.1605569268313474]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.090268649106734		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 2.090268649106734 | validation: 4.933697682580913]
	TIME [epoch: 7.66 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7603232501198174		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 3.7603232501198174 | validation: 4.03184398566985]
	TIME [epoch: 7.64 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7507370380444582		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 3.7507370380444582 | validation: 3.0016097950653737]
	TIME [epoch: 7.62 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.912839088632457		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 2.912839088632457 | validation: 2.407139775943331]
	TIME [epoch: 7.62 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5802426365874536		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 2.5802426365874536 | validation: 2.665040734740616]
	TIME [epoch: 7.62 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6013390744099087		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 2.6013390744099087 | validation: 2.4863437387470144]
	TIME [epoch: 7.66 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.571632838201574		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 2.571632838201574 | validation: 2.7132804469803995]
	TIME [epoch: 7.63 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9128724999818614		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 2.9128724999818614 | validation: 2.9885224727200908]
	TIME [epoch: 7.61 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.686568989668196		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 2.686568989668196 | validation: 2.5675517793380056]
	TIME [epoch: 7.61 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3681947450422633		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 2.3681947450422633 | validation: 3.488216666779384]
	TIME [epoch: 7.61 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8694633200936654		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 2.8694633200936654 | validation: 2.56007703409879]
	TIME [epoch: 7.66 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4888187375473456		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 2.4888187375473456 | validation: 2.226412127488782]
	TIME [epoch: 7.62 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4144617594747335		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 2.4144617594747335 | validation: 2.081052491052904]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4260261465619717		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 2.4260261465619717 | validation: 1.9320417629749604]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3028660226127653		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 2.3028660226127653 | validation: 4.59589022324586]
	TIME [epoch: 7.62 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.978339683823444		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 2.978339683823444 | validation: 3.839845307356681]
	TIME [epoch: 7.67 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9061843963707408		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 2.9061843963707408 | validation: 2.796575451668091]
	TIME [epoch: 7.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.907634415594912		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 2.907634415594912 | validation: 3.5393298598807528]
	TIME [epoch: 7.65 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0269913272283366		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 3.0269913272283366 | validation: 5.060623612754888]
	TIME [epoch: 7.66 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7479477050992		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 3.7479477050992 | validation: 3.0864744372776944]
	TIME [epoch: 7.68 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1120333655539487		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 3.1120333655539487 | validation: 2.9791529520374573]
	TIME [epoch: 7.72 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5937738798019296		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 2.5937738798019296 | validation: 3.1048054106500445]
	TIME [epoch: 7.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1352013604255977		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 3.1352013604255977 | validation: 6.0405292400328]
	TIME [epoch: 7.64 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.53479807406687		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 4.53479807406687 | validation: 5.816797522457668]
	TIME [epoch: 7.63 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.291512718218838		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 4.291512718218838 | validation: 5.85714084176042]
	TIME [epoch: 7.66 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.211859149517552		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 4.211859149517552 | validation: 5.775217406978404]
	TIME [epoch: 7.73 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2858827000464		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 4.2858827000464 | validation: 5.5065700206137915]
	TIME [epoch: 7.65 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.435792124535119		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 4.435792124535119 | validation: 6.187911646346168]
	TIME [epoch: 7.64 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.705552532154947		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 5.705552532154947 | validation: 4.244355040261604]
	TIME [epoch: 7.64 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.604932010097244		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 5.604932010097244 | validation: 4.142083508436279]
	TIME [epoch: 7.67 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.759773576828161		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 3.759773576828161 | validation: 3.7748939510443327]
	TIME [epoch: 7.71 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9866309187811444		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 2.9866309187811444 | validation: 2.6294976120096436]
	TIME [epoch: 7.66 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5471762438722076		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 2.5471762438722076 | validation: 3.0406662287814807]
	TIME [epoch: 7.67 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.053271480675395		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 3.053271480675395 | validation: 3.44541270966716]
	TIME [epoch: 7.7 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0119542000703547		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 3.0119542000703547 | validation: 2.770727211221476]
	TIME [epoch: 7.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7543644638484777		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 2.7543644638484777 | validation: 3.1724638145177515]
	TIME [epoch: 7.73 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4927082790732484		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 2.4927082790732484 | validation: 3.457243257822314]
	TIME [epoch: 7.67 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6568676036023966		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 2.6568676036023966 | validation: 2.6171724066078723]
	TIME [epoch: 7.66 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372581570587183		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 2.372581570587183 | validation: 2.1541621329263485]
	TIME [epoch: 7.67 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.178918079208815		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 2.178918079208815 | validation: 2.9713467519335106]
	TIME [epoch: 7.72 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4592784616466816		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 2.4592784616466816 | validation: 2.868222386554101]
	TIME [epoch: 7.74 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3232442996099896		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 2.3232442996099896 | validation: 2.6139019365475056]
	TIME [epoch: 7.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.170514044426638		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 2.170514044426638 | validation: 2.7171455567590757]
	TIME [epoch: 7.74 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.509624304963195		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 2.509624304963195 | validation: 2.444409722077543]
	TIME [epoch: 7.76 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.19417465569598		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 2.19417465569598 | validation: 2.4863799382851473]
	TIME [epoch: 7.78 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.957648714381725		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 2.957648714381725 | validation: 4.038200643325643]
	TIME [epoch: 7.74 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.05656106990501		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 3.05656106990501 | validation: 2.377849053891154]
	TIME [epoch: 7.72 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5125196465724517		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 3.5125196465724517 | validation: 2.3345462671829793]
	TIME [epoch: 7.74 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.147220136900557		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 2.147220136900557 | validation: 2.4122573084329413]
	TIME [epoch: 7.75 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.006355465264643		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 2.006355465264643 | validation: 2.0026356499198217]
	TIME [epoch: 7.79 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.084597103657407		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 2.084597103657407 | validation: 2.64765421687394]
	TIME [epoch: 7.75 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0181676109196047		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 2.0181676109196047 | validation: 2.0070501477296663]
	TIME [epoch: 7.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313392790685433		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 2.313392790685433 | validation: 2.699335796964693]
	TIME [epoch: 7.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2925910713773634		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 3.2925910713773634 | validation: 3.1975544223530203]
	TIME [epoch: 7.75 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4915703245879963		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 2.4915703245879963 | validation: 2.5337381987874608]
	TIME [epoch: 7.81 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1750456056465994		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 2.1750456056465994 | validation: 2.486933389620683]
	TIME [epoch: 7.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2147860795617467		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 2.2147860795617467 | validation: 2.6242448620095304]
	TIME [epoch: 7.72 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.135538018053894		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 2.135538018053894 | validation: 2.150505390289823]
	TIME [epoch: 7.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2695584998005884		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 2.2695584998005884 | validation: 2.3945262417972035]
	TIME [epoch: 7.75 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5027625840810717		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 2.5027625840810717 | validation: 3.0561821924843042]
	TIME [epoch: 7.78 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.863749390507742		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 2.863749390507742 | validation: 2.8140706352755114]
	TIME [epoch: 7.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6954647353333883		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 2.6954647353333883 | validation: 2.2155939479196887]
	TIME [epoch: 7.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9715208443990935		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 1.9715208443990935 | validation: 2.3378816497454684]
	TIME [epoch: 7.72 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9884557938216638		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 1.9884557938216638 | validation: 2.791381687664047]
	TIME [epoch: 7.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.146993020021573		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 2.146993020021573 | validation: 3.7722319514729543]
	TIME [epoch: 7.78 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1633274639585443		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 2.1633274639585443 | validation: 2.220662617117314]
	TIME [epoch: 7.72 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.160743226223273		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 2.160743226223273 | validation: 2.3781597880307883]
	TIME [epoch: 7.74 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9205213968484618		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 1.9205213968484618 | validation: 2.2604971287828977]
	TIME [epoch: 7.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1994541406607055		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 2.1994541406607055 | validation: 1.9741432972571142]
	TIME [epoch: 7.76 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9874851243809184		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 1.9874851243809184 | validation: 2.1036804425053486]
	TIME [epoch: 7.8 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.99059026433445		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 1.99059026433445 | validation: 2.2329165876156294]
	TIME [epoch: 7.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0942958942689778		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 2.0942958942689778 | validation: 3.3191816607846585]
	TIME [epoch: 7.71 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.27385068460545		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 2.27385068460545 | validation: 1.7472017005944551]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.831951989126784		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 1.831951989126784 | validation: 2.1231721468307683]
	TIME [epoch: 7.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2782451594225273		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 2.2782451594225273 | validation: 3.653955478660156]
	TIME [epoch: 7.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1458038048052224		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 3.1458038048052224 | validation: 2.487068611251096]
	TIME [epoch: 7.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.470720335583278		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 2.470720335583278 | validation: 2.697646554206891]
	TIME [epoch: 7.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2237918751215937		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 2.2237918751215937 | validation: 2.607012206013576]
	TIME [epoch: 7.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.18649590136928		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 2.18649590136928 | validation: 5.339848690818711]
	TIME [epoch: 7.75 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67463073519563		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 3.67463073519563 | validation: 2.1730758557911423]
	TIME [epoch: 7.73 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.425080253826553		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 2.425080253826553 | validation: 2.3495524355194037]
	TIME [epoch: 7.67 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.569099908160954		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 2.569099908160954 | validation: 2.265913933253989]
	TIME [epoch: 7.62 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.386637640437778		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 2.386637640437778 | validation: 2.142508878662472]
	TIME [epoch: 7.63 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1713396799858793		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 2.1713396799858793 | validation: 2.5859886081158816]
	TIME [epoch: 7.68 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.649688904592155		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 2.649688904592155 | validation: 2.3459267140130624]
	TIME [epoch: 7.62 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2214846389355247		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 3.2214846389355247 | validation: 2.0764794729524816]
	TIME [epoch: 7.62 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.857703098409963		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 2.857703098409963 | validation: 2.0612788952458985]
	TIME [epoch: 7.61 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2897636457964583		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 2.2897636457964583 | validation: 1.9580849484671097]
	TIME [epoch: 7.61 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.418548788347241		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 2.418548788347241 | validation: 1.6941685993683877]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7815095032579285		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 2.7815095032579285 | validation: 2.3389955221443097]
	TIME [epoch: 7.62 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3947805695478346		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 2.3947805695478346 | validation: 1.8574608183037675]
	TIME [epoch: 7.61 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5036073212497625		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 2.5036073212497625 | validation: 2.5071020159924435]
	TIME [epoch: 7.6 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6463299544360637		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 2.6463299544360637 | validation: 2.502506888476377]
	TIME [epoch: 7.61 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6486289525280764		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 2.6486289525280764 | validation: 2.4710131339226544]
	TIME [epoch: 7.66 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4301373789286673		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 2.4301373789286673 | validation: 1.9377948634138262]
	TIME [epoch: 7.62 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.392957987905241		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 2.392957987905241 | validation: 3.413478987335626]
	TIME [epoch: 7.61 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4564572391730852		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 2.4564572391730852 | validation: 2.171027107163747]
	TIME [epoch: 7.61 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9680731093053756		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 1.9680731093053756 | validation: 2.437381510120669]
	TIME [epoch: 7.61 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3708535878523245		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 2.3708535878523245 | validation: 1.698729513074932]
	TIME [epoch: 7.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.075536925095774		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 2.075536925095774 | validation: 2.114167943239818]
	TIME [epoch: 7.61 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5017795977457045		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 2.5017795977457045 | validation: 2.814589264826358]
	TIME [epoch: 7.62 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2653391377287138		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 2.2653391377287138 | validation: 1.9173617472076927]
	TIME [epoch: 7.61 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1745875931352856		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 2.1745875931352856 | validation: 2.137104232826456]
	TIME [epoch: 7.63 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1579781300106555		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 2.1579781300106555 | validation: 2.0067825333515286]
	TIME [epoch: 7.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.285852561584645		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 2.285852561584645 | validation: 2.610226180820214]
	TIME [epoch: 7.62 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2623167961265715		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 2.2623167961265715 | validation: 1.9193076458216076]
	TIME [epoch: 7.6 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.994118234561587		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 1.994118234561587 | validation: 1.5865620270012886]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7275284022939161		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 1.7275284022939161 | validation: 1.797338938445779]
	TIME [epoch: 7.66 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8941191866234666		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 1.8941191866234666 | validation: 2.6709684035519876]
	TIME [epoch: 7.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.486696253391882		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 2.486696253391882 | validation: 1.9795256714915472]
	TIME [epoch: 7.63 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3667138045390117		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 2.3667138045390117 | validation: 2.4892071818321626]
	TIME [epoch: 7.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2494571244729693		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 2.2494571244729693 | validation: 1.474876410334462]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v_kl3_20240702_111438/states/model_phi1_1a_v_kl3_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1182368028105842		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 2.1182368028105842 | validation: 1.5878111360623164]
	TIME [epoch: 7.69 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6086144051071716		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 1.6086144051071716 | validation: 1.7089862781049732]
	TIME [epoch: 7.67 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2267394478364815		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 2.2267394478364815 | validation: 1.9840842442383642]
	TIME [epoch: 7.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.895895041306286		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 1.895895041306286 | validation: 1.973980906492306]
	TIME [epoch: 7.66 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9207215147411034		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 1.9207215147411034 | validation: 1.7769050560118262]
	TIME [epoch: 7.67 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1741209993706327		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 2.1741209993706327 | validation: 3.497418594983306]
	TIME [epoch: 7.7 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8688669132503586		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 2.8688669132503586 | validation: 3.445901065123058]
	TIME [epoch: 7.66 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4955464257003985		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 2.4955464257003985 | validation: 2.5830072913374407]
	TIME [epoch: 7.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1945559322458994		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 2.1945559322458994 | validation: 2.4442070770526194]
	TIME [epoch: 7.64 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0958311172232937		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 2.0958311172232937 | validation: 1.7297609965098057]
	TIME [epoch: 7.65 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0727589211203314		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 2.0727589211203314 | validation: 2.081280953545443]
	TIME [epoch: 7.73 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487219915420903		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 1.8487219915420903 | validation: 2.1112986837027075]
	TIME [epoch: 7.67 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6766547074099396		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 2.6766547074099396 | validation: 1.9408455677297023]
	TIME [epoch: 7.66 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.255963656508702		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 4.255963656508702 | validation: 3.983091471857925]
	TIME [epoch: 7.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.751049994065323		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 4.751049994065323 | validation: 4.261830668105983]
	TIME [epoch: 7.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.144930645710113		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 5.144930645710113 | validation: 5.722806087824903]
	TIME [epoch: 7.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.986914003804207		[learning rate: 0.019695]
	Learning Rate: 0.0196952
	LOSS [training: 4.986914003804207 | validation: 3.357601043083832]
	TIME [epoch: 7.72 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.324292767957984		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 5.324292767957984 | validation: 3.9208193832358615]
	TIME [epoch: 7.71 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.335292639811288		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 5.335292639811288 | validation: 3.7265968997035683]
	TIME [epoch: 7.7 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.181909989679866		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 5.181909989679866 | validation: 3.6407371819446577]
	TIME [epoch: 7.71 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.198667692338499		[learning rate: 0.019679]
nan encountered in epoch 204 (validation loss).
	Learning Rate: 0.0196788
	LOSS [training: 5.198667692338499 | validation: nan]
	TIME [epoch: 7.74 sec]
EPOCH 205/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 7.63452778878114		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 7.63452778878114 | validation: 8.812893205444936]
	TIME [epoch: 123 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.971903788375428		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 7.971903788375428 | validation: 6.609104576173163]
	TIME [epoch: 7.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.856113349409781		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 6.856113349409781 | validation: 5.103732884668848]
	TIME [epoch: 7.72 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.181544468177426		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 6.181544468177426 | validation: 5.517954031501443]
	TIME [epoch: 7.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.1132875636718005		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 6.1132875636718005 | validation: 5.140436396035258]
	TIME [epoch: 7.73 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.839157332547445		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 5.839157332547445 | validation: 4.453312193434028]
	TIME [epoch: 7.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.754748105832213		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 5.754748105832213 | validation: 4.353435130501897]
	TIME [epoch: 7.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.670743976620033		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 5.670743976620033 | validation: 4.734915562926065]
	TIME [epoch: 7.71 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.645760841318758		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 5.645760841318758 | validation: 4.54711972924871]
	TIME [epoch: 7.76 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.089708982189055		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 6.089708982189055 | validation: 4.8451618158348575]
	TIME [epoch: 7.72 sec]
EPOCH 215/2000:
	Training over batches...
ERROR:
!!! UPDATED MODEL HAS NAN VALUES IN PHI.W[0] !!!
