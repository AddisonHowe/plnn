Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1158947078

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.426392711804053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.426392711804053 | validation: 4.991984132414466]
	TIME [epoch: 111 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.542710469755479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.542710469755479 | validation: 4.148883809987309]
	TIME [epoch: 8.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.032568700358168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032568700358168 | validation: 4.053434740217254]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9726556006249427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9726556006249427 | validation: 3.077713270627382]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9740585708045257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9740585708045257 | validation: 2.7035239169293357]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9533239459451055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9533239459451055 | validation: 2.9319351828482194]
	TIME [epoch: 8.34 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.771526907678475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.771526907678475 | validation: 2.6259252355261387]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.543876560444099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.543876560444099 | validation: 2.4791287344499704]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6771907346412824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6771907346412824 | validation: 2.372646700361892]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6025620463798314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6025620463798314 | validation: 2.4254625044514686]
	TIME [epoch: 8.31 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3339990383961267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3339990383961267 | validation: 2.1887939426293563]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3541598088455413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3541598088455413 | validation: 2.2040685019978703]
	TIME [epoch: 8.37 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1810376079424527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1810376079424527 | validation: 2.1452559101789257]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31466303969707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.31466303969707 | validation: 2.0829870000135204]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1168344799961574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1168344799961574 | validation: 1.9717666733521972]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.975620548004739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.975620548004739 | validation: 2.0280733421869743]
	TIME [epoch: 8.35 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.94060723024886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.94060723024886 | validation: 1.8630498812262686]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8967960003034636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8967960003034636 | validation: 2.8746074037747817]
	TIME [epoch: 8.35 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0041106799984467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0041106799984467 | validation: 1.8279329930047914]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850423751634541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.850423751634541 | validation: 1.851206155611922]
	TIME [epoch: 8.32 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6104228464157673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6104228464157673 | validation: 2.409312579375892]
	TIME [epoch: 8.33 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1867840094649584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1867840094649584 | validation: 1.604530745128483]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5135121379665244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5135121379665244 | validation: 1.508659255665996]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4113914712540303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4113914712540303 | validation: 1.599549997528856]
	TIME [epoch: 8.31 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4454283113341877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4454283113341877 | validation: 1.162735739139129]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5683178780626514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5683178780626514 | validation: 2.349300402673069]
	TIME [epoch: 8.33 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6804237944479907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6804237944479907 | validation: 1.8607884943870354]
	TIME [epoch: 8.33 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.493305520606338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493305520606338 | validation: 1.4414862676123918]
	TIME [epoch: 8.38 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.633892841052095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.633892841052095 | validation: 1.7149296167352421]
	TIME [epoch: 8.31 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.311307680250042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.311307680250042 | validation: 1.329732658038659]
	TIME [epoch: 8.33 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2773647845903153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2773647845903153 | validation: 1.26504911076759]
	TIME [epoch: 8.33 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3509885311230958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3509885311230958 | validation: 1.292716108423114]
	TIME [epoch: 8.33 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2606565901435351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2606565901435351 | validation: 1.3112318820470334]
	TIME [epoch: 8.33 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1918920737023408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1918920737023408 | validation: 0.9943359695756777]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1658231423795231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1658231423795231 | validation: 0.9204406526323137]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.124126347600459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.124126347600459 | validation: 1.032107169889961]
	TIME [epoch: 8.36 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0306235570654987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0306235570654987 | validation: 1.2950457482824422]
	TIME [epoch: 8.35 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2293074007182634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2293074007182634 | validation: 1.3800770563717146]
	TIME [epoch: 8.34 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9876280418355909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9876280418355909 | validation: 0.9490415775770704]
	TIME [epoch: 8.39 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9542667804390551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9542667804390551 | validation: 1.2570464954569882]
	TIME [epoch: 8.36 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0958977637306218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0958977637306218 | validation: 0.8537677165242398]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1017081067283108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1017081067283108 | validation: 0.8107251442046493]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8446029271760528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446029271760528 | validation: 0.6092805214260377]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8739801292164113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8739801292164113 | validation: 0.605658658710528]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9507152298373501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507152298373501 | validation: 0.71503405800839]
	TIME [epoch: 8.34 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9805693034760102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9805693034760102 | validation: 1.323242573906255]
	TIME [epoch: 8.32 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1250997903505182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1250997903505182 | validation: 0.7491423936593786]
	TIME [epoch: 8.32 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7354870706782858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7354870706782858 | validation: 0.9219424239537455]
	TIME [epoch: 8.32 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6803900031199434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803900031199434 | validation: 0.5585047075176687]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8997832061749624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997832061749624 | validation: 1.025536370206736]
	TIME [epoch: 8.35 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7798680229664393		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7798680229664393 | validation: 0.8862952386207605]
	TIME [epoch: 8.33 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6909771275263089		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.6909771275263089 | validation: 1.0474451847404476]
	TIME [epoch: 8.32 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7883711396048367		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.7883711396048367 | validation: 0.5601586605840737]
	TIME [epoch: 8.32 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6524082956106969		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.6524082956106969 | validation: 0.4891557347868033]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5280825344902488		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.5280825344902488 | validation: 1.1153769761164518]
	TIME [epoch: 8.36 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7531110491049697		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.7531110491049697 | validation: 0.6981084092736427]
	TIME [epoch: 8.33 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.580857971766116		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.580857971766116 | validation: 0.59157424228884]
	TIME [epoch: 8.32 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7416645862048772		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.7416645862048772 | validation: 0.9814608099515045]
	TIME [epoch: 8.32 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6101061942955545		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6101061942955545 | validation: 0.6578056500929503]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7788382088587368		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.7788382088587368 | validation: 0.6732817856806412]
	TIME [epoch: 8.31 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5726611520388345		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5726611520388345 | validation: 0.47791722000381276]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5047593614032924		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.5047593614032924 | validation: 0.4481704146802735]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6875602565102872		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.6875602565102872 | validation: 0.6395264749093765]
	TIME [epoch: 8.36 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5639033739545559		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5639033739545559 | validation: 0.7269865081648867]
	TIME [epoch: 8.35 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5984800759466989		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.5984800759466989 | validation: 0.9023117540106471]
	TIME [epoch: 8.33 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7673481171010859		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.7673481171010859 | validation: 0.5267657909821357]
	TIME [epoch: 8.39 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5351685294953729		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5351685294953729 | validation: 0.5743117193198432]
	TIME [epoch: 8.37 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7058414409921171		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.7058414409921171 | validation: 0.645979451820744]
	TIME [epoch: 8.36 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5359187566850298		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.5359187566850298 | validation: 0.41681987732654735]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6094815613343718		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.6094815613343718 | validation: 0.5888099914778134]
	TIME [epoch: 8.34 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7488612705689398		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.7488612705689398 | validation: 0.5296895506991313]
	TIME [epoch: 8.34 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.550337572372902		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.550337572372902 | validation: 0.5480405795718349]
	TIME [epoch: 8.38 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5659500121541117		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5659500121541117 | validation: 0.40446838774905874]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7000662328235556		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.7000662328235556 | validation: 0.5184859965230353]
	TIME [epoch: 8.34 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5911288713144528		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.5911288713144528 | validation: 0.5697445737205318]
	TIME [epoch: 8.35 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5153831332998638		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5153831332998638 | validation: 0.48614185691548906]
	TIME [epoch: 8.33 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7251769713485045		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.7251769713485045 | validation: 0.5964816223395372]
	TIME [epoch: 8.39 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6616700086785188		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6616700086785188 | validation: 1.049114154871959]
	TIME [epoch: 8.34 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6765600034534127		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.6765600034534127 | validation: 0.42737936713473323]
	TIME [epoch: 8.34 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.599016556039995		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.599016556039995 | validation: 0.5174894014805744]
	TIME [epoch: 8.34 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42605803453747604		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.42605803453747604 | validation: 0.35554719253944067]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4907455748509535		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.4907455748509535 | validation: 0.615047600349224]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5286718548325071		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5286718548325071 | validation: 0.5386927200432925]
	TIME [epoch: 8.35 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5191687928498467		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.5191687928498467 | validation: 0.37126807659166683]
	TIME [epoch: 8.32 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5884052298312505		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5884052298312505 | validation: 0.563042618560541]
	TIME [epoch: 8.32 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4052958417096173		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.4052958417096173 | validation: 0.5370811094230807]
	TIME [epoch: 8.32 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.405424129311807		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.405424129311807 | validation: 0.3340786372067668]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4659277693768634		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.4659277693768634 | validation: 0.7974361033308566]
	TIME [epoch: 8.36 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5494786198067089		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5494786198067089 | validation: 0.4286298515572241]
	TIME [epoch: 8.33 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3601734189713177		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.3601734189713177 | validation: 0.5992797423802106]
	TIME [epoch: 8.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.55821507849746		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.55821507849746 | validation: 0.37676879492256393]
	TIME [epoch: 8.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3710285229034167		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.3710285229034167 | validation: 0.4084405130507732]
	TIME [epoch: 8.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4230226011822841		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.4230226011822841 | validation: 0.3998460639376677]
	TIME [epoch: 8.32 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4397669599547449		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.4397669599547449 | validation: 0.43946176414724825]
	TIME [epoch: 8.36 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39572476165104054		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.39572476165104054 | validation: 0.3230278472540196]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3788188286368473		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.3788188286368473 | validation: 0.2783988595553267]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43299721443172884		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.43299721443172884 | validation: 0.49730341931932276]
	TIME [epoch: 8.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46498016171219547		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.46498016171219547 | validation: 0.36550103386481814]
	TIME [epoch: 8.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30904295161958084		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.30904295161958084 | validation: 0.31866817447612406]
	TIME [epoch: 8.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35287552445844883		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.35287552445844883 | validation: 0.2862039416984559]
	TIME [epoch: 8.32 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30811178395830724		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.30811178395830724 | validation: 0.30411447654342527]
	TIME [epoch: 8.32 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48508778621948884		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.48508778621948884 | validation: 0.4337155351724572]
	TIME [epoch: 8.29 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34515949434942655		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.34515949434942655 | validation: 0.2621988752040817]
	TIME [epoch: 8.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2525498267213429		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.2525498267213429 | validation: 0.36342582715574123]
	TIME [epoch: 8.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38403126723678155		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.38403126723678155 | validation: 0.4259708826149468]
	TIME [epoch: 8.38 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35343222894210147		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.35343222894210147 | validation: 0.2592156401563024]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2435010296076572		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2435010296076572 | validation: 0.2600896763396813]
	TIME [epoch: 8.34 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3422006781352931		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3422006781352931 | validation: 0.44768276448824373]
	TIME [epoch: 8.33 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4050793789155561		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.4050793789155561 | validation: 0.29974153447100155]
	TIME [epoch: 8.34 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3023116355537738		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.3023116355537738 | validation: 0.30893576614018975]
	TIME [epoch: 8.39 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27221534322192265		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.27221534322192265 | validation: 0.30452117033807957]
	TIME [epoch: 8.35 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630680548024966		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.2630680548024966 | validation: 0.28847000634979447]
	TIME [epoch: 8.34 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34589030259342757		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.34589030259342757 | validation: 0.3701555745049673]
	TIME [epoch: 8.32 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3397603137178224		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.3397603137178224 | validation: 0.37459854103052576]
	TIME [epoch: 8.34 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34714072009162095		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.34714072009162095 | validation: 0.3605030661677461]
	TIME [epoch: 8.37 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3062670334882077		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.3062670334882077 | validation: 0.24896152013553152]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23898673378935942		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.23898673378935942 | validation: 0.2611793208261749]
	TIME [epoch: 8.32 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23851115719703797		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.23851115719703797 | validation: 0.2311964019685232]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26847623665060405		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.26847623665060405 | validation: 0.3047182366890098]
	TIME [epoch: 8.31 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2160910319640288		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.2160910319640288 | validation: 0.20578393087903268]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2720399330294607		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2720399330294607 | validation: 0.41712913171289734]
	TIME [epoch: 8.36 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3509605742836636		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3509605742836636 | validation: 0.20923747778349794]
	TIME [epoch: 8.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25569861093578156		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.25569861093578156 | validation: 0.37093836076961584]
	TIME [epoch: 8.31 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2548837940175177		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.2548837940175177 | validation: 0.22236935323107282]
	TIME [epoch: 8.29 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582261551574966		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2582261551574966 | validation: 0.20732929431218555]
	TIME [epoch: 8.31 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19947123678696171		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.19947123678696171 | validation: 0.21678295938345185]
	TIME [epoch: 8.33 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617425194120501		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2617425194120501 | validation: 0.19505740913133962]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22973223706884163		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.22973223706884163 | validation: 0.1902489555903154]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1907322702391266		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.1907322702391266 | validation: 0.20486538294215487]
	TIME [epoch: 8.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23717811813462208		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.23717811813462208 | validation: 0.2139909940746687]
	TIME [epoch: 8.31 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20743026910158746		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.20743026910158746 | validation: 0.2479626918061601]
	TIME [epoch: 8.31 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23678566519443034		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.23678566519443034 | validation: 0.44652833308834117]
	TIME [epoch: 8.36 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31533196164401234		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.31533196164401234 | validation: 0.32661036456771475]
	TIME [epoch: 8.31 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23631963483590815		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.23631963483590815 | validation: 0.17728129605002035]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16966150287516027		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.16966150287516027 | validation: 0.2062003863772915]
	TIME [epoch: 8.34 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23396932166116824		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.23396932166116824 | validation: 0.17480824921246674]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20994397136914522		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.20994397136914522 | validation: 0.2014552906422981]
	TIME [epoch: 8.38 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16484537507593996		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.16484537507593996 | validation: 0.17543757870351634]
	TIME [epoch: 8.36 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2064731679591601		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2064731679591601 | validation: 0.21004600053266392]
	TIME [epoch: 8.34 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20321425582703276		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.20321425582703276 | validation: 0.34536374567913797]
	TIME [epoch: 8.32 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28058907609263495		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.28058907609263495 | validation: 0.16893192005194096]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17702519675887698		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.17702519675887698 | validation: 0.15608293203146137]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15730929940020835		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.15730929940020835 | validation: 0.2364485574203147]
	TIME [epoch: 8.37 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25497216313636806		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.25497216313636806 | validation: 0.2282957552159836]
	TIME [epoch: 8.32 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17685820493657547		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.17685820493657547 | validation: 0.1840775985473338]
	TIME [epoch: 8.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19113048533871083		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.19113048533871083 | validation: 0.26758325607145284]
	TIME [epoch: 8.31 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20838716115330602		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.20838716115330602 | validation: 0.2053222632298176]
	TIME [epoch: 8.32 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1582642964400836		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.1582642964400836 | validation: 0.14095173276492964]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18427334831260977		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.18427334831260977 | validation: 0.32076913768826515]
	TIME [epoch: 8.33 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23958321420364498		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.23958321420364498 | validation: 0.15489809361932327]
	TIME [epoch: 8.29 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1540979204044876		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.1540979204044876 | validation: 0.1639262798532382]
	TIME [epoch: 8.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1489023576101604		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.1489023576101604 | validation: 0.144649084313275]
	TIME [epoch: 8.31 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1442171052430104		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.1442171052430104 | validation: 0.27417228486488393]
	TIME [epoch: 8.31 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20536489343503483		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.20536489343503483 | validation: 0.16773562856205923]
	TIME [epoch: 8.36 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16582093113267132		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.16582093113267132 | validation: 0.16102880791841648]
	TIME [epoch: 8.31 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13950942950588893		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.13950942950588893 | validation: 0.15238753498963625]
	TIME [epoch: 8.29 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17466708422882155		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.17466708422882155 | validation: 0.13168014538539158]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1500978860157513		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.1500978860157513 | validation: 0.14333123898528016]
	TIME [epoch: 8.31 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1661384315765746		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.1661384315765746 | validation: 0.1379320220507006]
	TIME [epoch: 8.34 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14398465987671413		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.14398465987671413 | validation: 0.18716501274936634]
	TIME [epoch: 8.33 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1655761915005188		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.1655761915005188 | validation: 0.2940916518074559]
	TIME [epoch: 8.29 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2024347892633944		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.2024347892633944 | validation: 0.1208057905691553]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14722909857408287		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.14722909857408287 | validation: 0.13414987757814992]
	TIME [epoch: 8.32 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1412391581734932		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.1412391581734932 | validation: 0.16099149567721577]
	TIME [epoch: 8.32 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13089154332828182		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.13089154332828182 | validation: 0.1056804982814264]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11102005595718878		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.11102005595718878 | validation: 0.11501886272583153]
	TIME [epoch: 8.35 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15109872534298105		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.15109872534298105 | validation: 0.2165110807529867]
	TIME [epoch: 8.33 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16144979134514742		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.16144979134514742 | validation: 0.09797855135936404]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10856533825660972		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.10856533825660972 | validation: 0.114722421970086]
	TIME [epoch: 8.34 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1821579697357179		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1821579697357179 | validation: 0.14224453399312886]
	TIME [epoch: 8.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13152961270136174		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.13152961270136174 | validation: 0.1124994408122954]
	TIME [epoch: 8.36 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1224865000841296		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.1224865000841296 | validation: 0.09602853321910496]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11629563872740388		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.11629563872740388 | validation: 0.19149583332700293]
	TIME [epoch: 8.34 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18238099185439127		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.18238099185439127 | validation: 0.15659327928059508]
	TIME [epoch: 8.33 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1581339840321947		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.1581339840321947 | validation: 0.12199200078118548]
	TIME [epoch: 8.35 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10203073601821305		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.10203073601821305 | validation: 0.08713134549895074]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1267497174682814		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.1267497174682814 | validation: 0.1402719187656274]
	TIME [epoch: 8.31 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11188862379710557		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.11188862379710557 | validation: 0.10132143670483444]
	TIME [epoch: 8.32 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16056877565560465		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.16056877565560465 | validation: 0.11706903115705822]
	TIME [epoch: 8.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12823503634979605		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.12823503634979605 | validation: 0.11063610773560095]
	TIME [epoch: 8.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11672818735886625		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.11672818735886625 | validation: 0.08425063696428373]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11576435454604661		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.11576435454604661 | validation: 0.09077457919868114]
	TIME [epoch: 8.31 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10408372712508729		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.10408372712508729 | validation: 0.16711983944380118]
	TIME [epoch: 8.29 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.144860262646427		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.144860262646427 | validation: 0.09190756247508447]
	TIME [epoch: 8.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09621224571918871		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.09621224571918871 | validation: 0.2033667247802981]
	TIME [epoch: 8.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1647404181753083		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.1647404181753083 | validation: 0.12750156514347738]
	TIME [epoch: 8.32 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10973856913860905		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.10973856913860905 | validation: 0.13561837316127984]
	TIME [epoch: 8.34 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10095863817525552		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.10095863817525552 | validation: 0.07732277122598542]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10212796199973384		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.10212796199973384 | validation: 0.14789525639305942]
	TIME [epoch: 8.31 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1272685436590577		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.1272685436590577 | validation: 0.10973634582234557]
	TIME [epoch: 8.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11376623783439235		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.11376623783439235 | validation: 0.07274729744434677]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11192429442182789		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.11192429442182789 | validation: 0.1120053616660465]
	TIME [epoch: 8.35 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0927142072076495		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.0927142072076495 | validation: 0.0810023370262387]
	TIME [epoch: 8.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11484128892758455		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.11484128892758455 | validation: 0.13670571656766667]
	TIME [epoch: 8.29 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10962292039642366		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.10962292039642366 | validation: 0.08916324401233655]
	TIME [epoch: 8.31 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10112878792954205		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.10112878792954205 | validation: 0.08419597590520794]
	TIME [epoch: 8.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07558378208029175		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.07558378208029175 | validation: 0.07137793771098692]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08553519751852942		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.08553519751852942 | validation: 0.18617593872109597]
	TIME [epoch: 8.34 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1468091730863527		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.1468091730863527 | validation: 0.07782503422822551]
	TIME [epoch: 8.29 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08116582759253664		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.08116582759253664 | validation: 0.21875883898268744]
	TIME [epoch: 8.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13668189975082234		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.13668189975082234 | validation: 0.10387946629883155]
	TIME [epoch: 8.35 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08191615570104813		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.08191615570104813 | validation: 0.09017181771715976]
	TIME [epoch: 8.35 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11724279499616075		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.11724279499616075 | validation: 0.10973900532345049]
	TIME [epoch: 8.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10383185729771485		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.10383185729771485 | validation: 0.08567054924558289]
	TIME [epoch: 8.32 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08555945682787519		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.08555945682787519 | validation: 0.0870118137148028]
	TIME [epoch: 8.34 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08706303666898042		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.08706303666898042 | validation: 0.10260434710638333]
	TIME [epoch: 8.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09746193543159119		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.09746193543159119 | validation: 0.1501202972473027]
	TIME [epoch: 8.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10141710485551124		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.10141710485551124 | validation: 0.12125602631832577]
	TIME [epoch: 8.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11735905992617754		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.11735905992617754 | validation: 0.07478440464616737]
	TIME [epoch: 8.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07218937478472384		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.07218937478472384 | validation: 0.05965413142762063]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09540675879215003		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.09540675879215003 | validation: 0.12613993005074353]
	TIME [epoch: 8.34 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09232794986628583		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.09232794986628583 | validation: 0.06710086084838054]
	TIME [epoch: 8.33 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07075302708128887		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.07075302708128887 | validation: 0.07668527041975223]
	TIME [epoch: 8.33 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10254961253693726		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.10254961253693726 | validation: 0.15283363261730176]
	TIME [epoch: 8.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10031522433742741		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.10031522433742741 | validation: 0.0643123359537589]
	TIME [epoch: 8.31 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07287011191342202		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.07287011191342202 | validation: 0.06281109054307779]
	TIME [epoch: 8.33 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09989855496510255		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.09989855496510255 | validation: 0.10532612839883414]
	TIME [epoch: 8.33 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08576206047312344		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.08576206047312344 | validation: 0.06361492875035302]
	TIME [epoch: 8.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06962230714502494		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.06962230714502494 | validation: 0.09248047505322113]
	TIME [epoch: 8.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10615318873877677		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.10615318873877677 | validation: 0.0750055004551042]
	TIME [epoch: 8.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06705750704248421		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.06705750704248421 | validation: 0.05059358512570673]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09412998643350412		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.09412998643350412 | validation: 0.10655798943797881]
	TIME [epoch: 8.32 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07315679310343157		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.07315679310343157 | validation: 0.06755827389875242]
	TIME [epoch: 8.31 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08744658754331303		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.08744658754331303 | validation: 0.0770540498785541]
	TIME [epoch: 8.31 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07240970753925088		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.07240970753925088 | validation: 0.091568349474944]
	TIME [epoch: 8.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10830910640664476		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.10830910640664476 | validation: 0.09979116283448294]
	TIME [epoch: 8.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0742119474852806		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.0742119474852806 | validation: 0.14570106488797496]
	TIME [epoch: 8.31 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10569409062673704		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.10569409062673704 | validation: 0.07480039190622678]
	TIME [epoch: 8.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0772294150201998		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.0772294150201998 | validation: 0.07869781318747479]
	TIME [epoch: 8.31 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07783310555684618		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.07783310555684618 | validation: 0.058284907098456525]
	TIME [epoch: 8.31 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06099217700839048		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.06099217700839048 | validation: 0.049601314373276695]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07699272007835363		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.07699272007835363 | validation: 0.09234256695595111]
	TIME [epoch: 8.29 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09156552880924573		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.09156552880924573 | validation: 0.06136905375886938]
	TIME [epoch: 8.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05889657488973797		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.05889657488973797 | validation: 0.07954117764020953]
	TIME [epoch: 8.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07094916909247935		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.07094916909247935 | validation: 0.08428989564612568]
	TIME [epoch: 8.29 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07699748222970337		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.07699748222970337 | validation: 0.07423897951438783]
	TIME [epoch: 8.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07949710507754199		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.07949710507754199 | validation: 0.06597110978448213]
	TIME [epoch: 8.29 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08266424281204923		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.08266424281204923 | validation: 0.07246932354506791]
	TIME [epoch: 8.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186416272224727		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.06186416272224727 | validation: 0.06328053088104073]
	TIME [epoch: 8.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09826814375247289		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.09826814375247289 | validation: 0.08102818305057588]
	TIME [epoch: 8.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07695780229909963		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.07695780229909963 | validation: 0.05273757115944158]
	TIME [epoch: 8.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0823663089675721		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.0823663089675721 | validation: 0.11588624108490682]
	TIME [epoch: 8.34 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08780910272971902		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.08780910272971902 | validation: 0.047103767600878824]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06831998067276215		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.06831998067276215 | validation: 0.07295043679900425]
	TIME [epoch: 8.31 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06447954593915067		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.06447954593915067 | validation: 0.05928080025521969]
	TIME [epoch: 8.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059498934010678975		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.059498934010678975 | validation: 0.10926867565079598]
	TIME [epoch: 8.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07027911692615485		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.07027911692615485 | validation: 0.044151599216678133]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0734442615076677		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.0734442615076677 | validation: 0.07853819746693136]
	TIME [epoch: 8.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05946685134217244		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.05946685134217244 | validation: 0.07642737070679775]
	TIME [epoch: 8.29 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06199221349345451		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.06199221349345451 | validation: 0.06095268963596334]
	TIME [epoch: 8.31 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08825711403198433		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.08825711403198433 | validation: 0.05123966902015179]
	TIME [epoch: 8.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0508016953204796		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.0508016953204796 | validation: 0.062395422276988535]
	TIME [epoch: 8.31 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07880807719702411		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.07880807719702411 | validation: 0.05735673083478867]
	TIME [epoch: 8.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049653986884193105		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.049653986884193105 | validation: 0.054907142367111215]
	TIME [epoch: 8.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07693371404353253		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.07693371404353253 | validation: 0.09525132221424301]
	TIME [epoch: 8.31 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06587024642130399		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.06587024642130399 | validation: 0.051999037371331575]
	TIME [epoch: 8.31 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058265950365689656		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.058265950365689656 | validation: 0.04186466395453052]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04240935999930051		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.04240935999930051 | validation: 0.04660500223138465]
	TIME [epoch: 8.34 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06823575234543929		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.06823575234543929 | validation: 0.1145832475121355]
	TIME [epoch: 8.32 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06985736857254476		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.06985736857254476 | validation: 0.05310972932166365]
	TIME [epoch: 8.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05782523612734236		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.05782523612734236 | validation: 0.07191445118211033]
	TIME [epoch: 8.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05603694847868318		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.05603694847868318 | validation: 0.047693039592820105]
	TIME [epoch: 8.31 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05067567419188725		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.05067567419188725 | validation: 0.04604322428211934]
	TIME [epoch: 8.31 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05211580149455944		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.05211580149455944 | validation: 0.05442959085365548]
	TIME [epoch: 8.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06027427888943262		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.06027427888943262 | validation: 0.0697178691625005]
	TIME [epoch: 8.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049577992008244656		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.049577992008244656 | validation: 0.05018607847005449]
	TIME [epoch: 8.32 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0705066289068301		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.0705066289068301 | validation: 0.06915528303299853]
	TIME [epoch: 8.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06220566748968349		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.06220566748968349 | validation: 0.039486083928138]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037517237574124375		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.037517237574124375 | validation: 0.03962026786185924]
	TIME [epoch: 8.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06856370724118109		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.06856370724118109 | validation: 0.1299225510817062]
	TIME [epoch: 8.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06549002859887942		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.06549002859887942 | validation: 0.04431787525514527]
	TIME [epoch: 8.33 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047774178157106356		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.047774178157106356 | validation: 0.07150411478059293]
	TIME [epoch: 8.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04861719347006084		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.04861719347006084 | validation: 0.05701957309965747]
	TIME [epoch: 8.33 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0852894678155111		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.0852894678155111 | validation: 0.06852553805752298]
	TIME [epoch: 8.34 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05493666582661004		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.05493666582661004 | validation: 0.04233122215167998]
	TIME [epoch: 8.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03904357882336046		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.03904357882336046 | validation: 0.05611362719731407]
	TIME [epoch: 8.32 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0646259334318568		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.0646259334318568 | validation: 0.07726552939131313]
	TIME [epoch: 8.34 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050448432911778016		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.050448432911778016 | validation: 0.05746193560148144]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046279565139126747		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.046279565139126747 | validation: 0.03903580322809634]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040434255933645726		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.040434255933645726 | validation: 0.04402001408811031]
	TIME [epoch: 8.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07455354079796364		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.07455354079796364 | validation: 0.09727030399786543]
	TIME [epoch: 8.34 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07305688046263803		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.07305688046263803 | validation: 0.03335618061964574]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03358575557260508		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.03358575557260508 | validation: 0.04005235031752242]
	TIME [epoch: 8.32 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045526910052947456		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.045526910052947456 | validation: 0.04630403596107549]
	TIME [epoch: 8.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047152735888643535		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.047152735888643535 | validation: 0.059192094092462594]
	TIME [epoch: 8.33 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053728502749981276		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.053728502749981276 | validation: 0.037730295945006626]
	TIME [epoch: 8.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05680295954703399		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.05680295954703399 | validation: 0.08654124486338335]
	TIME [epoch: 8.31 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06687191379608398		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.06687191379608398 | validation: 0.03154632732302853]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030573436423771458		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.030573436423771458 | validation: 0.044802226967070105]
	TIME [epoch: 8.31 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05726377106960294		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.05726377106960294 | validation: 0.042639447409889605]
	TIME [epoch: 8.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04813160542176849		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.04813160542176849 | validation: 0.07310388740345938]
	TIME [epoch: 8.34 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050133538948806816		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.050133538948806816 | validation: 0.12071189108418487]
	TIME [epoch: 8.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07976309056999636		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.07976309056999636 | validation: 0.050999283699845674]
	TIME [epoch: 8.29 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03890652521156933		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.03890652521156933 | validation: 0.03144142547721204]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029501581611198813		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.029501581611198813 | validation: 0.031224590657539995]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035749563875127344		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.035749563875127344 | validation: 0.06282645059731916]
	TIME [epoch: 8.31 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09000787601054185		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.09000787601054185 | validation: 0.04333850201699875]
	TIME [epoch: 8.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035122030325222035		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.035122030325222035 | validation: 0.03195164671354645]
	TIME [epoch: 8.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028663771582448472		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.028663771582448472 | validation: 0.03969841572848656]
	TIME [epoch: 8.31 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03623306439906655		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.03623306439906655 | validation: 0.04897597689040441]
	TIME [epoch: 8.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058620319462340024		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.058620319462340024 | validation: 0.05799736425712339]
	TIME [epoch: 8.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04250715674960608		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.04250715674960608 | validation: 0.037057126651462796]
	TIME [epoch: 8.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04374389932193601		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.04374389932193601 | validation: 0.040418514789068855]
	TIME [epoch: 8.32 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04942259031908255		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.04942259031908255 | validation: 0.03173100604351055]
	TIME [epoch: 8.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03449743869033367		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.03449743869033367 | validation: 0.05725384534308334]
	TIME [epoch: 8.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05281448369541861		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.05281448369541861 | validation: 0.05406259788244913]
	TIME [epoch: 8.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04013696002076643		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.04013696002076643 | validation: 0.03297268529170744]
	TIME [epoch: 8.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04341504776073745		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.04341504776073745 | validation: 0.048776556541105236]
	TIME [epoch: 8.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03822256496299421		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03822256496299421 | validation: 0.02813696028218234]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032374852333009706		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.032374852333009706 | validation: 0.03264922627145043]
	TIME [epoch: 8.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0407675757744454		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.0407675757744454 | validation: 0.045571531077815704]
	TIME [epoch: 8.34 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04290281924180335		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.04290281924180335 | validation: 0.029766876104970294]
	TIME [epoch: 8.33 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025785614631448274		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.025785614631448274 | validation: 0.022155641275202434]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050300559679959875		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.050300559679959875 | validation: 0.051140356739106205]
	TIME [epoch: 8.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030796902244376683		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.030796902244376683 | validation: 0.042672387795268114]
	TIME [epoch: 8.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045807546875793		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.045807546875793 | validation: 0.062167032993880586]
	TIME [epoch: 8.33 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03709817764000379		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.03709817764000379 | validation: 0.030850721957217636]
	TIME [epoch: 8.32 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029394391815025074		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.029394391815025074 | validation: 0.026826564435392282]
	TIME [epoch: 8.33 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027854871202445647		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.027854871202445647 | validation: 0.04874939337630338]
	TIME [epoch: 8.37 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047503442473191895		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.047503442473191895 | validation: 0.037756910447231284]
	TIME [epoch: 8.32 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030438230903332225		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.030438230903332225 | validation: 0.026452846247922414]
	TIME [epoch: 8.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03330195033789474		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.03330195033789474 | validation: 0.03917775087117805]
	TIME [epoch: 8.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03760140209878307		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.03760140209878307 | validation: 0.02646334744208121]
	TIME [epoch: 8.32 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024792132893141375		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.024792132893141375 | validation: 0.04539060419591677]
	TIME [epoch: 8.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03322777242925859		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.03322777242925859 | validation: 0.029996896349903553]
	TIME [epoch: 8.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042526975508314205		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.042526975508314205 | validation: 0.03399266890815829]
	TIME [epoch: 8.31 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025362930014302995		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.025362930014302995 | validation: 0.02036522981258101]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030259122355508043		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.030259122355508043 | validation: 0.05839392713970014]
	TIME [epoch: 8.32 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03794802253214827		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.03794802253214827 | validation: 0.027226087963149376]
	TIME [epoch: 8.33 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033103406996561446		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.033103406996561446 | validation: 0.03238027955174078]
	TIME [epoch: 8.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02747891342831517		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.02747891342831517 | validation: 0.029762353851715745]
	TIME [epoch: 8.32 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03672442942804245		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.03672442942804245 | validation: 0.022819064980113594]
	TIME [epoch: 8.31 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02485973695492982		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.02485973695492982 | validation: 0.0604392913425759]
	TIME [epoch: 8.32 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047132254021937624		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.047132254021937624 | validation: 0.04218132446446429]
	TIME [epoch: 8.31 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0395775818987433		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.0395775818987433 | validation: 0.044548914633640975]
	TIME [epoch: 8.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028771106322623727		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.028771106322623727 | validation: 0.03284159066071726]
	TIME [epoch: 8.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034290990239010166		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.034290990239010166 | validation: 0.042693016464919575]
	TIME [epoch: 8.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027065641639728813		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.027065641639728813 | validation: 0.018577841297887514]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023950855651670946		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.023950855651670946 | validation: 0.03717367203509073]
	TIME [epoch: 8.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027115037875670652		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.027115037875670652 | validation: 0.023353147373634177]
	TIME [epoch: 8.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022476694890771073		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.022476694890771073 | validation: 0.034380231316865226]
	TIME [epoch: 8.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03892887327994575		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.03892887327994575 | validation: 0.023994819316867585]
	TIME [epoch: 8.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024936276044515412		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.024936276044515412 | validation: 0.056764480459655736]
	TIME [epoch: 8.29 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03764069917595701		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.03764069917595701 | validation: 0.02465330620289852]
	TIME [epoch: 8.31 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022623005705835825		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.022623005705835825 | validation: 0.03367299906296535]
	TIME [epoch: 8.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04253029605925187		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.04253029605925187 | validation: 0.052515732514300625]
	TIME [epoch: 8.32 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03682160640096124		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.03682160640096124 | validation: 0.03497389224775266]
	TIME [epoch: 8.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028294428142431834		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.028294428142431834 | validation: 0.03258291295836982]
	TIME [epoch: 8.31 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028315441098688877		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.028315441098688877 | validation: 0.016904614033193537]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015782444583693208		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.015782444583693208 | validation: 0.02723523859503204]
	TIME [epoch: 8.31 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044740793768207485		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.044740793768207485 | validation: 0.04086999296483493]
	TIME [epoch: 8.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026008930037005794		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.026008930037005794 | validation: 0.019841056788385443]
	TIME [epoch: 8.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01678382311065424		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.01678382311065424 | validation: 0.01914469390435665]
	TIME [epoch: 8.31 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03641653608525976		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.03641653608525976 | validation: 0.042904100841387335]
	TIME [epoch: 8.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03715952043888528		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.03715952043888528 | validation: 0.031977421583210805]
	TIME [epoch: 8.31 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020634105111220165		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.020634105111220165 | validation: 0.020515773598485467]
	TIME [epoch: 8.31 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022454753948608356		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.022454753948608356 | validation: 0.034488704294991054]
	TIME [epoch: 8.32 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022597197101363455		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.022597197101363455 | validation: 0.024233741324710248]
	TIME [epoch: 8.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022565879350388052		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.022565879350388052 | validation: 0.021597138976944555]
	TIME [epoch: 8.31 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02346481511983377		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.02346481511983377 | validation: 0.03696975616600982]
	TIME [epoch: 8.29 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03159501719562609		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.03159501719562609 | validation: 0.014062031679462024]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019291278342099788		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.019291278342099788 | validation: 0.024841667448143118]
	TIME [epoch: 8.31 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02738758177380965		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.02738758177380965 | validation: 0.02538565594653381]
	TIME [epoch: 8.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020980443000043373		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.020980443000043373 | validation: 0.019462853308942075]
	TIME [epoch: 8.32 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031687172647309136		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.031687172647309136 | validation: 0.022455762329181173]
	TIME [epoch: 8.31 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01750559210719517		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.01750559210719517 | validation: 0.01638128322802921]
	TIME [epoch: 8.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015215438037056193		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.015215438037056193 | validation: 0.01562842554834235]
	TIME [epoch: 8.31 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026323167942921763		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.026323167942921763 | validation: 0.07213423982523537]
	TIME [epoch: 8.31 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032902986456609856		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.032902986456609856 | validation: 0.02640916944641882]
	TIME [epoch: 8.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020945104067738966		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.020945104067738966 | validation: 0.014282015897834259]
	TIME [epoch: 8.32 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012113522007900593		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.012113522007900593 | validation: 0.01736642874502156]
	TIME [epoch: 8.29 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08818552624813226		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.08818552624813226 | validation: 0.11062507218357953]
	TIME [epoch: 8.32 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09636005469077444		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.09636005469077444 | validation: 0.04833387931016383]
	TIME [epoch: 8.31 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03179325158932058		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.03179325158932058 | validation: 0.017175013246279625]
	TIME [epoch: 8.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020588543367815817		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.020588543367815817 | validation: 0.023220176499060493]
	TIME [epoch: 8.33 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023951678138885633		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.023951678138885633 | validation: 0.015012579943909123]
	TIME [epoch: 8.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017313316821958094		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.017313316821958094 | validation: 0.029671553950155728]
	TIME [epoch: 8.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02367869522310953		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.02367869522310953 | validation: 0.020452124419160313]
	TIME [epoch: 8.32 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018107048737405203		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.018107048737405203 | validation: 0.01942046457398012]
	TIME [epoch: 8.31 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02089598275979186		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.02089598275979186 | validation: 0.020575914870161326]
	TIME [epoch: 8.37 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020511927462174288		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.020511927462174288 | validation: 0.023166103406817858]
	TIME [epoch: 8.32 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022939237188517266		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.022939237188517266 | validation: 0.017659686085160227]
	TIME [epoch: 8.32 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05428832110292313		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.05428832110292313 | validation: 0.07024900979682397]
	TIME [epoch: 8.31 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0466192056532017		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.0466192056532017 | validation: 0.013357428878585337]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01744288558840214		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.01744288558840214 | validation: 0.016294852097763927]
	TIME [epoch: 8.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02502729621296542		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.02502729621296542 | validation: 0.015067184532569302]
	TIME [epoch: 8.32 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016081046086099844		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.016081046086099844 | validation: 0.02484299833020813]
	TIME [epoch: 8.31 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017971973788818704		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.017971973788818704 | validation: 0.01957200560946481]
	TIME [epoch: 8.28 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01974410473558462		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.01974410473558462 | validation: 0.026345635919580383]
	TIME [epoch: 8.31 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021221950155100366		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.021221950155100366 | validation: 0.019056613887969934]
	TIME [epoch: 8.31 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03057074557367207		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.03057074557367207 | validation: 0.019589989274826997]
	TIME [epoch: 8.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016428884541764457		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.016428884541764457 | validation: 0.013008187808326495]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018515679057424666		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.018515679057424666 | validation: 0.034655227546066245]
	TIME [epoch: 8.34 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019422470060819673		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.019422470060819673 | validation: 0.015702531578680616]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01714657196529479		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.01714657196529479 | validation: 0.033545664675933014]
	TIME [epoch: 8.34 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024992489853750608		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.024992489853750608 | validation: 0.021136111003646776]
	TIME [epoch: 8.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019015814729176673		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.019015814729176673 | validation: 0.015516877879784906]
	TIME [epoch: 8.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011191400179147576		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.011191400179147576 | validation: 0.014591009376023159]
	TIME [epoch: 8.34 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145275385580663		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.03145275385580663 | validation: 0.01771082942680146]
	TIME [epoch: 8.34 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018946188001447924		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.018946188001447924 | validation: 0.013264746202195124]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014634127246110136		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.014634127246110136 | validation: 0.017735110405197423]
	TIME [epoch: 8.33 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01538905446671626		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.01538905446671626 | validation: 0.017774445182569147]
	TIME [epoch: 8.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01803040769212579		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.01803040769212579 | validation: 0.022147432477604014]
	TIME [epoch: 8.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014391395876356312		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.014391395876356312 | validation: 0.014497485392296026]
	TIME [epoch: 8.32 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009585157147209053		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.009585157147209053 | validation: 0.01525027672285045]
	TIME [epoch: 8.31 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03323138220590643		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.03323138220590643 | validation: 0.026487718406256792]
	TIME [epoch: 8.32 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024096238553194017		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.024096238553194017 | validation: 0.02904479539816983]
	TIME [epoch: 8.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01539568814947112		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.01539568814947112 | validation: 0.009990380743694155]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012743516269607215		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.012743516269607215 | validation: 0.01916859327853111]
	TIME [epoch: 8.32 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023505284210944766		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.023505284210944766 | validation: 0.014457507815716381]
	TIME [epoch: 8.32 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017661665398781914		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.017661665398781914 | validation: 0.016660674888804752]
	TIME [epoch: 8.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019498102382401843		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.019498102382401843 | validation: 0.0146149721434512]
	TIME [epoch: 8.32 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018284610942062275		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.018284610942062275 | validation: 0.015404898629367536]
	TIME [epoch: 8.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013633486912942243		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.013633486912942243 | validation: 0.01824639498897057]
	TIME [epoch: 8.32 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014746192937661405		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.014746192937661405 | validation: 0.019153943527422196]
	TIME [epoch: 8.31 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0586152049766168		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.0586152049766168 | validation: 0.04230131822543178]
	TIME [epoch: 8.31 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022855394887054896		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.022855394887054896 | validation: 0.013602101754835413]
	TIME [epoch: 8.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012804521060456386		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.012804521060456386 | validation: 0.014908413907586831]
	TIME [epoch: 8.34 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686814464493074		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.01686814464493074 | validation: 0.030494452704828437]
	TIME [epoch: 8.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021727520388948512		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.021727520388948512 | validation: 0.01123132813546713]
	TIME [epoch: 8.31 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010847520405673687		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.010847520405673687 | validation: 0.009530160724614337]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009341145895045925		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.009341145895045925 | validation: 0.015443167307531255]
	TIME [epoch: 8.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02886131047580648		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.02886131047580648 | validation: 0.0151977441615401]
	TIME [epoch: 8.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01124448504473332		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.01124448504473332 | validation: 0.008726896176556791]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009983752288370382		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.009983752288370382 | validation: 0.012852449717680065]
	TIME [epoch: 8.31 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018555380454050162		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.018555380454050162 | validation: 0.014577030353648608]
	TIME [epoch: 8.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017692160661949338		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.017692160661949338 | validation: 0.014330364444938943]
	TIME [epoch: 8.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020405518994255484		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.020405518994255484 | validation: 0.01826140684655559]
	TIME [epoch: 8.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01669545330695321		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01669545330695321 | validation: 0.015436806525883381]
	TIME [epoch: 8.32 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012620863857670327		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.012620863857670327 | validation: 0.01857331583158059]
	TIME [epoch: 8.34 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012767904248297005		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.012767904248297005 | validation: 0.019793397671943172]
	TIME [epoch: 8.31 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018387502859571102		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.018387502859571102 | validation: 0.013452438387258424]
	TIME [epoch: 8.31 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012926254859855331		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.012926254859855331 | validation: 0.008444327963856888]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014741906121775755		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.014741906121775755 | validation: 0.022437621249608228]
	TIME [epoch: 8.29 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012251985479563481		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.012251985479563481 | validation: 0.012568991928691098]
	TIME [epoch: 8.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009835678902077673		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.009835678902077673 | validation: 0.01763327516974673]
	TIME [epoch: 8.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014768875817453609		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.014768875817453609 | validation: 0.011180555065319432]
	TIME [epoch: 8.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015881091370396276		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.015881091370396276 | validation: 0.037927133021264506]
	TIME [epoch: 8.31 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017371338045687537		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.017371338045687537 | validation: 0.008987798592416852]
	TIME [epoch: 8.29 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009001587050851453		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.009001587050851453 | validation: 0.015431143542896639]
	TIME [epoch: 8.32 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019566052081216177		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.019566052081216177 | validation: 0.01162396333642882]
	TIME [epoch: 8.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011631404887654361		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.011631404887654361 | validation: 0.012356727826834307]
	TIME [epoch: 8.31 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01056815264578502		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.01056815264578502 | validation: 0.02026426968639474]
	TIME [epoch: 8.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016288015514726854		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.016288015514726854 | validation: 0.017751505368043322]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011302893430581887		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.011302893430581887 | validation: 0.0227313290957123]
	TIME [epoch: 8.29 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018937263294509257		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.018937263294509257 | validation: 0.008826091000507892]
	TIME [epoch: 8.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006941446144702359		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.006941446144702359 | validation: 0.007231500712902776]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007690833314145083		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.007690833314145083 | validation: 0.023041107372596657]
	TIME [epoch: 8.31 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02102278928286579		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.02102278928286579 | validation: 0.010915047027395958]
	TIME [epoch: 8.31 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012212814701911778		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.012212814701911778 | validation: 0.009396199058110472]
	TIME [epoch: 8.31 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010106260971711477		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.010106260971711477 | validation: 0.012633423380260835]
	TIME [epoch: 8.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014411199361968278		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.014411199361968278 | validation: 0.025068227448952526]
	TIME [epoch: 8.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01403387100605279		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.01403387100605279 | validation: 0.014826770063423509]
	TIME [epoch: 8.32 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010091868618608394		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.010091868618608394 | validation: 0.008767964184325534]
	TIME [epoch: 8.31 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010294912146946698		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.010294912146946698 | validation: 0.03533284545545479]
	TIME [epoch: 8.31 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025141625745296572		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.025141625745296572 | validation: 0.012808949648144257]
	TIME [epoch: 8.31 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010098171090147257		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.010098171090147257 | validation: 0.011791759641701437]
	TIME [epoch: 8.33 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008500255268180084		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.008500255268180084 | validation: 0.013694399293670664]
	TIME [epoch: 8.33 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012824524330361934		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.012824524330361934 | validation: 0.009895605846349558]
	TIME [epoch: 8.31 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01273839029109074		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.01273839029109074 | validation: 0.01705400670093323]
	TIME [epoch: 8.31 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010684670415715407		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.010684670415715407 | validation: 0.010928971889695608]
	TIME [epoch: 8.31 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01211073065027542		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.01211073065027542 | validation: 0.008986366953589713]
	TIME [epoch: 8.31 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076940872415316094		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.0076940872415316094 | validation: 0.016333783484699656]
	TIME [epoch: 8.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01966073851011543		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.01966073851011543 | validation: 0.011566133379985202]
	TIME [epoch: 8.32 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009594201690605222		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.009594201690605222 | validation: 0.007816005480462953]
	TIME [epoch: 8.32 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009126227074511209		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.009126227074511209 | validation: 0.012287391151216335]
	TIME [epoch: 8.32 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011792344651578112		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.011792344651578112 | validation: 0.015563762035903556]
	TIME [epoch: 8.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012316281197693804		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.012316281197693804 | validation: 0.026921544619759288]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012777155576786898		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.012777155576786898 | validation: 0.013574914364864256]
	TIME [epoch: 8.33 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014574898025782695		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.014574898025782695 | validation: 0.01158404503918907]
	TIME [epoch: 8.31 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006690144426142195		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.006690144426142195 | validation: 0.009314193223803351]
	TIME [epoch: 8.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011862229027411745		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.011862229027411745 | validation: 0.028471235462505358]
	TIME [epoch: 8.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016573307815429206		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.016573307815429206 | validation: 0.011132442390040066]
	TIME [epoch: 8.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008862268718150027		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.008862268718150027 | validation: 0.007979554259485266]
	TIME [epoch: 8.34 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00871239841314432		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.00871239841314432 | validation: 0.012730814361076932]
	TIME [epoch: 8.32 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008683569253063395		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.008683569253063395 | validation: 0.014132069825548335]
	TIME [epoch: 8.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015805988815407738		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.015805988815407738 | validation: 0.015167980268612751]
	TIME [epoch: 8.31 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009640294579757274		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.009640294579757274 | validation: 0.0172742925392481]
	TIME [epoch: 8.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013246281054594944		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.013246281054594944 | validation: 0.011279487020367154]
	TIME [epoch: 8.32 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009621281213290568		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.009621281213290568 | validation: 0.006524229412693466]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071933187735189665		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.0071933187735189665 | validation: 0.021613952277583733]
	TIME [epoch: 8.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014545443076870952		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.014545443076870952 | validation: 0.00808134764004309]
	TIME [epoch: 8.34 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011023523974926112		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.011023523974926112 | validation: 0.01756124302081513]
	TIME [epoch: 8.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009331881027995407		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.009331881027995407 | validation: 0.010712935993530574]
	TIME [epoch: 8.34 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013743611370957698		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.013743611370957698 | validation: 0.008730403258040326]
	TIME [epoch: 8.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021156798698294947		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.021156798698294947 | validation: 0.02364027270989144]
	TIME [epoch: 8.34 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01983770267743234		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.01983770267743234 | validation: 0.013325124748517023]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074836215916002915		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.0074836215916002915 | validation: 0.006848910394668948]
	TIME [epoch: 8.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009785336770240446		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.009785336770240446 | validation: 0.01156400990510428]
	TIME [epoch: 8.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007404648302103462		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.007404648302103462 | validation: 0.010686514255333263]
	TIME [epoch: 8.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00998286184253826		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.00998286184253826 | validation: 0.008379533561798552]
	TIME [epoch: 8.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010940706953600804		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.010940706953600804 | validation: 0.01878697040482031]
	TIME [epoch: 8.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02188161027237662		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.02188161027237662 | validation: 0.012175535962706277]
	TIME [epoch: 8.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009893326546534978		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.009893326546534978 | validation: 0.010713600362934372]
	TIME [epoch: 8.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013392784496160895		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.013392784496160895 | validation: 0.012519162306734445]
	TIME [epoch: 8.34 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008162746809493531		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.008162746809493531 | validation: 0.00960541603978745]
	TIME [epoch: 8.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008283856942896033		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.008283856942896033 | validation: 0.009331413605139446]
	TIME [epoch: 8.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009195591363814967		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.009195591363814967 | validation: 0.010309466328925095]
	TIME [epoch: 8.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0121861794381657		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0121861794381657 | validation: 0.00991002879936358]
	TIME [epoch: 8.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01162148284704688		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.01162148284704688 | validation: 0.007683478378458636]
	TIME [epoch: 8.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008171677106139034		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.008171677106139034 | validation: 0.015804823458550613]
	TIME [epoch: 8.36 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009432603141760457		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.009432603141760457 | validation: 0.007192110415996677]
	TIME [epoch: 8.36 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012174960795417283		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.012174960795417283 | validation: 0.014436429330150699]
	TIME [epoch: 8.32 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00880465943945769		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.00880465943945769 | validation: 0.008870172626981998]
	TIME [epoch: 8.33 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005751476070759876		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.005751476070759876 | validation: 0.007107779412447]
	TIME [epoch: 8.34 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010357536771699628		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.010357536771699628 | validation: 0.009615693654127072]
	TIME [epoch: 8.33 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00859121004951503		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.00859121004951503 | validation: 0.010566044437384521]
	TIME [epoch: 8.38 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009813721948911764		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.009813721948911764 | validation: 0.00873140753827648]
	TIME [epoch: 8.34 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010166226572466698		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.010166226572466698 | validation: 0.009273356278496578]
	TIME [epoch: 8.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008002963038975203		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.008002963038975203 | validation: 0.01697404292131586]
	TIME [epoch: 8.33 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010963708931627578		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.010963708931627578 | validation: 0.007478529374945961]
	TIME [epoch: 8.33 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006163484301458327		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.006163484301458327 | validation: 0.0076051025084438555]
	TIME [epoch: 8.34 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008852311283094667		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.008852311283094667 | validation: 0.010572419666820541]
	TIME [epoch: 8.37 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014077981282306917		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.014077981282306917 | validation: 0.011804545211785651]
	TIME [epoch: 8.33 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007893432600851106		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.007893432600851106 | validation: 0.0075511099113485455]
	TIME [epoch: 8.31 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010380900793866325		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.010380900793866325 | validation: 0.007706146665407324]
	TIME [epoch: 8.33 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007589827573076785		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.007589827573076785 | validation: 0.008909061153691404]
	TIME [epoch: 8.32 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009353162368547741		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.009353162368547741 | validation: 0.013676844492145282]
	TIME [epoch: 8.37 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008518186668036344		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.008518186668036344 | validation: 0.007221354995001823]
	TIME [epoch: 8.34 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007789621697480745		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.007789621697480745 | validation: 0.014487874521071299]
	TIME [epoch: 8.31 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007967866613221287		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.007967866613221287 | validation: 0.00716024263378362]
	TIME [epoch: 8.32 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068848940328952255		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.0068848940328952255 | validation: 0.010509467287121842]
	TIME [epoch: 8.32 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010449012635725962		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.010449012635725962 | validation: 0.0230487978976191]
	TIME [epoch: 8.33 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010027218357459799		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.010027218357459799 | validation: 0.007933793604084187]
	TIME [epoch: 8.38 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075861447046826225		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.0075861447046826225 | validation: 0.007855394549406728]
	TIME [epoch: 8.33 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00763993942033094		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.00763993942033094 | validation: 0.009210997405258301]
	TIME [epoch: 8.31 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008649864322669319		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.008649864322669319 | validation: 0.007275678771607848]
	TIME [epoch: 8.33 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008341960918238734		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.008341960918238734 | validation: 0.013155134043136384]
	TIME [epoch: 8.33 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008274987781101887		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.008274987781101887 | validation: 0.009476015583559472]
	TIME [epoch: 8.35 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006969815070257438		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.006969815070257438 | validation: 0.009955475857508937]
	TIME [epoch: 8.35 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007032000146351415		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.007032000146351415 | validation: 0.01257832561909749]
	TIME [epoch: 8.32 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007869134199712306		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.007869134199712306 | validation: 0.01080863147266849]
	TIME [epoch: 8.31 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013624204973772026		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.013624204973772026 | validation: 0.009121584414956065]
	TIME [epoch: 8.33 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005759612837992929		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.005759612837992929 | validation: 0.008800750362397452]
	TIME [epoch: 8.32 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006231784889876213		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.006231784889876213 | validation: 0.011486878396302844]
	TIME [epoch: 8.38 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008680196490800554		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.008680196490800554 | validation: 0.006686579185206439]
	TIME [epoch: 8.33 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007884689384056802		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.007884689384056802 | validation: 0.010684896639620975]
	TIME [epoch: 8.33 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006761066700350757		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.006761066700350757 | validation: 0.006891260249025494]
	TIME [epoch: 8.31 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006461364464521658		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.006461364464521658 | validation: 0.010879869586090416]
	TIME [epoch: 8.33 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01090544546825778		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.01090544546825778 | validation: 0.005537129290834824]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004568607004708565		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.004568607004708565 | validation: 0.01000081155242305]
	TIME [epoch: 8.35 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008004778014653886		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.008004778014653886 | validation: 0.009113518649347292]
	TIME [epoch: 8.32 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009367135556337886		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.009367135556337886 | validation: 0.008757014052387234]
	TIME [epoch: 8.31 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006468567594260764		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.006468567594260764 | validation: 0.0068877467592062935]
	TIME [epoch: 8.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009044302635808698		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.009044302635808698 | validation: 0.009483389688175203]
	TIME [epoch: 8.31 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010429191110909567		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.010429191110909567 | validation: 0.005782412067329056]
	TIME [epoch: 8.37 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005197220122546043		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.005197220122546043 | validation: 0.008589101040265109]
	TIME [epoch: 8.32 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008378791928613124		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.008378791928613124 | validation: 0.011414656705414892]
	TIME [epoch: 8.32 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006518613708366079		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.006518613708366079 | validation: 0.006590546553215332]
	TIME [epoch: 8.31 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007533077863391826		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.007533077863391826 | validation: 0.018267421805102033]
	TIME [epoch: 8.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009248708389054776		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.009248708389054776 | validation: 0.0059096565836617605]
	TIME [epoch: 8.33 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00485624410368041		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.00485624410368041 | validation: 0.00603803189777543]
	TIME [epoch: 8.36 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070251812784326625		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.0070251812784326625 | validation: 0.007491310019365236]
	TIME [epoch: 8.31 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005992285089243632		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.005992285089243632 | validation: 0.01428749425154539]
	TIME [epoch: 8.31 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010673537894619892		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.010673537894619892 | validation: 0.00827336325611744]
	TIME [epoch: 8.29 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006719433987591391		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.006719433987591391 | validation: 0.00894464866478889]
	TIME [epoch: 8.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005711507605092059		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.005711507605092059 | validation: 0.005759655692207192]
	TIME [epoch: 8.36 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0088795125867847		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.0088795125867847 | validation: 0.007395436561936766]
	TIME [epoch: 8.33 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007402623212322192		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.007402623212322192 | validation: 0.007016346669061936]
	TIME [epoch: 8.31 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00784236521680325		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.00784236521680325 | validation: 0.008944799633119005]
	TIME [epoch: 8.31 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005992764455787773		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.005992764455787773 | validation: 0.009269287181796782]
	TIME [epoch: 8.29 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00799906288663976		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.00799906288663976 | validation: 0.005203160312015958]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005810450139859912		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.005810450139859912 | validation: 0.017205682748495175]
	TIME [epoch: 8.35 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008574989426060393		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.008574989426060393 | validation: 0.006124984871772329]
	TIME [epoch: 8.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004922592189395663		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.004922592189395663 | validation: 0.006534213592605777]
	TIME [epoch: 8.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009334129946468652		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.009334129946468652 | validation: 0.007997586313774164]
	TIME [epoch: 8.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005753532067570058		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.005753532067570058 | validation: 0.004254764172340422]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006984748364337818		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.006984748364337818 | validation: 0.017357633598763446]
	TIME [epoch: 8.34 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009813501309245125		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.009813501309245125 | validation: 0.008370994139910983]
	TIME [epoch: 8.32 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005611333508899545		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.005611333508899545 | validation: 0.005309234469071159]
	TIME [epoch: 8.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004104055193027454		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.004104055193027454 | validation: 0.00583767897938086]
	TIME [epoch: 8.31 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011273700813786501		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.011273700813786501 | validation: 0.00720435815775485]
	TIME [epoch: 8.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00533694455529589		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.00533694455529589 | validation: 0.00932265857744553]
	TIME [epoch: 8.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006625545817425399		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.006625545817425399 | validation: 0.007222516429406563]
	TIME [epoch: 8.35 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006536442350974914		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.006536442350974914 | validation: 0.005323147259449278]
	TIME [epoch: 8.31 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005989855503852154		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.005989855503852154 | validation: 0.009041947491803185]
	TIME [epoch: 8.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006599143519843255		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.006599143519843255 | validation: 0.008290097716983361]
	TIME [epoch: 8.31 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006421963994750257		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.006421963994750257 | validation: 0.005418884179231911]
	TIME [epoch: 8.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006930399913461886		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.006930399913461886 | validation: 0.006934926163590531]
	TIME [epoch: 8.33 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052003524029580715		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.0052003524029580715 | validation: 0.005487808855868105]
	TIME [epoch: 8.32 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007512223840846809		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.007512223840846809 | validation: 0.016412395904094237]
	TIME [epoch: 8.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075484886329184		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.0075484886329184 | validation: 0.006289484659481639]
	TIME [epoch: 8.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005156687308253313		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.005156687308253313 | validation: 0.007420616073710345]
	TIME [epoch: 8.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00588572788638205		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.00588572788638205 | validation: 0.006680201056266057]
	TIME [epoch: 8.31 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00633213584091902		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.00633213584091902 | validation: 0.010521887033848064]
	TIME [epoch: 8.34 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00828069997769488		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.00828069997769488 | validation: 0.005620077432862247]
	TIME [epoch: 8.32 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004535518362735146		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.004535518362735146 | validation: 0.007241126226082192]
	TIME [epoch: 8.31 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008380585714009663		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.008380585714009663 | validation: 0.004705751825384157]
	TIME [epoch: 8.31 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004352747068195981		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.004352747068195981 | validation: 0.00567187452679237]
	TIME [epoch: 8.31 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007742021491138661		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.007742021491138661 | validation: 0.009197675766224123]
	TIME [epoch: 8.32 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005686635708089546		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.005686635708089546 | validation: 0.0064042332860992]
	TIME [epoch: 8.32 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00439471986692115		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.00439471986692115 | validation: 0.004980222343918623]
	TIME [epoch: 8.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00920235997424758		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.00920235997424758 | validation: 0.011464324469930919]
	TIME [epoch: 8.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006316384894363272		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.006316384894363272 | validation: 0.0064631925485694386]
	TIME [epoch: 8.31 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060777317281985755		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.0060777317281985755 | validation: 0.005445105917553578]
	TIME [epoch: 8.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004955307306285659		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.004955307306285659 | validation: 0.0073800166302590104]
	TIME [epoch: 8.35 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006165307339000814		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.006165307339000814 | validation: 0.006118769173882046]
	TIME [epoch: 8.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004301333238639629		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.004301333238639629 | validation: 0.00959954723249136]
	TIME [epoch: 8.31 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00821120580330742		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.00821120580330742 | validation: 0.008114652058709933]
	TIME [epoch: 8.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004558299977037014		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.004558299977037014 | validation: 0.006815051658687479]
	TIME [epoch: 8.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075462196682171846		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0075462196682171846 | validation: 0.00831241588371702]
	TIME [epoch: 8.32 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042361996378932456		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0042361996378932456 | validation: 0.004525698903643311]
	TIME [epoch: 8.32 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005469859195110064		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.005469859195110064 | validation: 0.008830130364201218]
	TIME [epoch: 8.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059745504648899975		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0059745504648899975 | validation: 0.004920505951662067]
	TIME [epoch: 8.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006665367814765946		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.006665367814765946 | validation: 0.007106457906432207]
	TIME [epoch: 8.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005071249881526129		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.005071249881526129 | validation: 0.003983202483212146]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038600649430277634		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0038600649430277634 | validation: 0.008018675604576316]
	TIME [epoch: 8.39 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007307756160648174		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.007307756160648174 | validation: 0.006086851344119763]
	TIME [epoch: 8.32 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005528773934214961		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.005528773934214961 | validation: 0.005694099065580559]
	TIME [epoch: 8.34 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005098796935686256		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.005098796935686256 | validation: 0.005121698958965878]
	TIME [epoch: 8.33 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006085190933147276		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.006085190933147276 | validation: 0.0062913799270210465]
	TIME [epoch: 8.34 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005626536630196432		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.005626536630196432 | validation: 0.004447939272347183]
	TIME [epoch: 8.34 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005103943085732578		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.005103943085732578 | validation: 0.005735130158091359]
	TIME [epoch: 8.39 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005362643711741251		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.005362643711741251 | validation: 0.011466290897456564]
	TIME [epoch: 8.33 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006705060901946694		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.006705060901946694 | validation: 0.00854796205579523]
	TIME [epoch: 8.34 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005086130782745525		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.005086130782745525 | validation: 0.007357276562853657]
	TIME [epoch: 8.34 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006787082525442821		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.006787082525442821 | validation: 0.006309837455246558]
	TIME [epoch: 8.34 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004407348242437312		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.004407348242437312 | validation: 0.005289201965967607]
	TIME [epoch: 8.39 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005351612742353963		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.005351612742353963 | validation: 0.005864104294561771]
	TIME [epoch: 8.33 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004695145824943287		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.004695145824943287 | validation: 0.005227002810121983]
	TIME [epoch: 8.32 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037320784371608655		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.0037320784371608655 | validation: 0.003631374397447151]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055108154464931745		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.0055108154464931745 | validation: 0.011768292549373699]
	TIME [epoch: 8.34 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005794426289469375		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.005794426289469375 | validation: 0.006608473312636985]
	TIME [epoch: 8.33 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005321745337402069		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.005321745337402069 | validation: 0.0061802920094403534]
	TIME [epoch: 8.38 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004308600289137311		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.004308600289137311 | validation: 0.00529772705490404]
	TIME [epoch: 8.33 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004552977372745037		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.004552977372745037 | validation: 0.00974089118247148]
	TIME [epoch: 8.32 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005769536321001665		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.005769536321001665 | validation: 0.005613684740735717]
	TIME [epoch: 8.33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004891811309394565		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.004891811309394565 | validation: 0.006414455452925093]
	TIME [epoch: 8.33 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005006285619848792		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.005006285619848792 | validation: 0.004160356974111632]
	TIME [epoch: 8.37 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004792317293827905		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.004792317293827905 | validation: 0.005588933372607347]
	TIME [epoch: 8.35 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005448479937362366		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.005448479937362366 | validation: 0.005452458922015479]
	TIME [epoch: 8.33 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005163966869388541		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.005163966869388541 | validation: 0.005324268677868712]
	TIME [epoch: 8.31 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004250731731777677		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.004250731731777677 | validation: 0.003908895882569437]
	TIME [epoch: 8.33 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004410639503958589		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.004410639503958589 | validation: 0.01538080517883689]
	TIME [epoch: 8.33 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007959173402330167		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.007959173402330167 | validation: 0.005980498748477897]
	TIME [epoch: 8.37 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004155895782812812		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.004155895782812812 | validation: 0.003979637904853922]
	TIME [epoch: 8.33 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003810052745322449		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.003810052745322449 | validation: 0.005349563882306266]
	TIME [epoch: 8.33 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006740461795836155		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.006740461795836155 | validation: 0.005956076207274138]
	TIME [epoch: 8.31 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00500728070456706		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.00500728070456706 | validation: 0.004041732305095436]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005366650888601409		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.005366650888601409 | validation: 0.004421021906849261]
	TIME [epoch: 8.37 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004972220766605652		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.004972220766605652 | validation: 0.007613422535706843]
	TIME [epoch: 8.34 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005679954413829476		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.005679954413829476 | validation: 0.006717473220542787]
	TIME [epoch: 8.32 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006661454134956633		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.006661454134956633 | validation: 0.011118630298146253]
	TIME [epoch: 8.33 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00477962632232869		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.00477962632232869 | validation: 0.003928188830147798]
	TIME [epoch: 8.31 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006738227891866672		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.006738227891866672 | validation: 0.004692098603542005]
	TIME [epoch: 8.32 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003985635942875817		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.003985635942875817 | validation: 0.005188422414301565]
	TIME [epoch: 8.38 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038203743285409758		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.0038203743285409758 | validation: 0.004609022456291893]
	TIME [epoch: 8.33 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006350212029846514		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.006350212029846514 | validation: 0.01076743493760106]
	TIME [epoch: 8.33 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005631489565796238		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.005631489565796238 | validation: 0.005999081873386648]
	TIME [epoch: 8.32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004069033504722065		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.004069033504722065 | validation: 0.004374246371995871]
	TIME [epoch: 8.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004517824194532191		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.004517824194532191 | validation: 0.008092224748172446]
	TIME [epoch: 8.34 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005026873824874165		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.005026873824874165 | validation: 0.00531911867750938]
	TIME [epoch: 8.36 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003909147503872906		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.003909147503872906 | validation: 0.003676954220126174]
	TIME [epoch: 8.32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004509441845091628		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.004509441845091628 | validation: 0.012368705153488565]
	TIME [epoch: 8.32 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006185542611270198		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.006185542611270198 | validation: 0.0065302737903131436]
	TIME [epoch: 8.31 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004324185508811677		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.004324185508811677 | validation: 0.004899974975861884]
	TIME [epoch: 8.31 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004389443046550052		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.004389443046550052 | validation: 0.01648423332083144]
	TIME [epoch: 8.38 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006993185515607325		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.006993185515607325 | validation: 0.005648269023321993]
	TIME [epoch: 8.33 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038700374157953662		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.0038700374157953662 | validation: 0.006008478297746935]
	TIME [epoch: 8.32 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004447801156822216		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.004447801156822216 | validation: 0.005476365419984926]
	TIME [epoch: 8.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003977096889349137		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.003977096889349137 | validation: 0.005744471783288164]
	TIME [epoch: 8.32 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046718017293353765		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0046718017293353765 | validation: 0.005574079192599918]
	TIME [epoch: 8.31 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004920074246044256		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.004920074246044256 | validation: 0.0068683939686141025]
	TIME [epoch: 8.37 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004752789665838689		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.004752789665838689 | validation: 0.004441289461724007]
	TIME [epoch: 8.33 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004423008003127744		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.004423008003127744 | validation: 0.007039106143169473]
	TIME [epoch: 8.32 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037931644244679433		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.0037931644244679433 | validation: 0.004267996354182899]
	TIME [epoch: 8.32 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034984676697585697		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0034984676697585697 | validation: 0.009664598576505225]
	TIME [epoch: 8.32 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006884325642915717		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.006884325642915717 | validation: 0.004570575286882131]
	TIME [epoch: 8.35 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003519338786199793		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.003519338786199793 | validation: 0.005149936326153275]
	TIME [epoch: 8.34 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004670089021189174		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.004670089021189174 | validation: 0.00400652777904423]
	TIME [epoch: 8.32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004860918443058065		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.004860918443058065 | validation: 0.004451748513238252]
	TIME [epoch: 8.32 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033176741111157074		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.0033176741111157074 | validation: 0.004856549967651603]
	TIME [epoch: 8.32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00403269875840069		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.00403269875840069 | validation: 0.006204478392648115]
	TIME [epoch: 8.32 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004414969996039868		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.004414969996039868 | validation: 0.0045706236052942524]
	TIME [epoch: 8.28 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003751150061968847		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.003751150061968847 | validation: 0.0036972284545890534]
	TIME [epoch: 8.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004433793456619795		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.004433793456619795 | validation: 0.006200305745327993]
	TIME [epoch: 8.33 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004100624004302401		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.004100624004302401 | validation: 0.0035231661083612474]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003430560411119721		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.003430560411119721 | validation: 0.005293477480873417]
	TIME [epoch: 8.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005759208191723463		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.005759208191723463 | validation: 0.007001831638280682]
	TIME [epoch: 8.33 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047539803144509485		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.0047539803144509485 | validation: 0.007068964255830232]
	TIME [epoch: 8.31 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005240287294299091		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.005240287294299091 | validation: 0.004108331768490892]
	TIME [epoch: 8.29 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004411963740859974		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.004411963740859974 | validation: 0.005860996034802588]
	TIME [epoch: 8.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004258224598801252		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.004258224598801252 | validation: 0.0037167132803092597]
	TIME [epoch: 8.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00377114445899805		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.00377114445899805 | validation: 0.004340838494490501]
	TIME [epoch: 8.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004543365866878947		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.004543365866878947 | validation: 0.004231867169771977]
	TIME [epoch: 8.35 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003963407442431323		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.003963407442431323 | validation: 0.004652841001382695]
	TIME [epoch: 8.31 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003376028195851383		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.003376028195851383 | validation: 0.006397377272249252]
	TIME [epoch: 8.29 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036546116557682985		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.0036546116557682985 | validation: 0.0038028786456483304]
	TIME [epoch: 8.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004403738722033273		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.004403738722033273 | validation: 0.006383822199371218]
	TIME [epoch: 8.29 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004046481775171807		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.004046481775171807 | validation: 0.0047893730235161235]
	TIME [epoch: 8.31 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034772874554017926		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0034772874554017926 | validation: 0.004511449950562693]
	TIME [epoch: 8.32 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004130550400444783		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.004130550400444783 | validation: 0.004169816809036918]
	TIME [epoch: 8.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003048100003110273		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.003048100003110273 | validation: 0.005788283094311415]
	TIME [epoch: 8.28 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004517621593879086		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.004517621593879086 | validation: 0.008021381749870335]
	TIME [epoch: 8.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004527388765771445		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.004527388765771445 | validation: 0.004058778721637525]
	TIME [epoch: 8.29 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003369298745765726		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.003369298745765726 | validation: 0.005091418044531245]
	TIME [epoch: 8.34 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003912380815253385		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.003912380815253385 | validation: 0.004503694460120772]
	TIME [epoch: 8.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004201758180612145		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004201758180612145 | validation: 0.005621948896165418]
	TIME [epoch: 8.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003277303052754673		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.003277303052754673 | validation: 0.004021031100664511]
	TIME [epoch: 8.28 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036450107265586142		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.0036450107265586142 | validation: 0.003996343245414623]
	TIME [epoch: 8.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002823911225146163		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.002823911225146163 | validation: 0.003556471518600139]
	TIME [epoch: 8.32 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003745170795632782		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.003745170795632782 | validation: 0.006443189927582546]
	TIME [epoch: 8.33 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005752330985351006		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.005752330985351006 | validation: 0.0050008468788963]
	TIME [epoch: 8.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009931278046150132		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.009931278046150132 | validation: 0.008021565108782125]
	TIME [epoch: 8.29 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004488031345083546		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.004488031345083546 | validation: 0.0036417988529196567]
	TIME [epoch: 8.29 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028707938967800677		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0028707938967800677 | validation: 0.0032029500222343714]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034557471410278954		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.0034557471410278954 | validation: 0.004416795196150929]
	TIME [epoch: 8.31 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00349109236173511		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.00349109236173511 | validation: 0.0033469986359532184]
	TIME [epoch: 8.28 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003484050472059773		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.003484050472059773 | validation: 0.004291375705847262]
	TIME [epoch: 8.26 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036031288689277003		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.0036031288689277003 | validation: 0.004587178945587355]
	TIME [epoch: 8.26 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003121482400957035		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.003121482400957035 | validation: 0.0048052103877196765]
	TIME [epoch: 8.25 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049281465147938425		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0049281465147938425 | validation: 0.00938259968609434]
	TIME [epoch: 8.28 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00431584425313148		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.00431584425313148 | validation: 0.003388920318334973]
	TIME [epoch: 8.31 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027133762059653927		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0027133762059653927 | validation: 0.004246933234597674]
	TIME [epoch: 8.26 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037664251908387757		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.0037664251908387757 | validation: 0.004887100445290434]
	TIME [epoch: 8.27 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003770957045975958		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.003770957045975958 | validation: 0.004225277219576456]
	TIME [epoch: 8.26 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003076668820829301		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.003076668820829301 | validation: 0.0034508895293824046]
	TIME [epoch: 8.26 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005081886763072073		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.005081886763072073 | validation: 0.004265344070182926]
	TIME [epoch: 8.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003482965891411026		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.003482965891411026 | validation: 0.0037703706856653573]
	TIME [epoch: 8.28 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003249259570809754		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.003249259570809754 | validation: 0.004944194345547778]
	TIME [epoch: 8.26 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00396118528427265		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.00396118528427265 | validation: 0.004937350439270098]
	TIME [epoch: 8.27 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004313809062456211		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.004313809062456211 | validation: 0.005165626318451071]
	TIME [epoch: 8.27 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002899098903700345		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.002899098903700345 | validation: 0.003011546365149601]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033555852391638137		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0033555852391638137 | validation: 0.006182128801328332]
	TIME [epoch: 8.32 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004728198271271378		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.004728198271271378 | validation: 0.004003987597550783]
	TIME [epoch: 8.28 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030175517873825743		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.0030175517873825743 | validation: 0.0033180459010269284]
	TIME [epoch: 8.28 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028523400303379865		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0028523400303379865 | validation: 0.004887931721946301]
	TIME [epoch: 8.27 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004608142270755619		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.004608142270755619 | validation: 0.003976789070751878]
	TIME [epoch: 8.27 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00307103788119796		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.00307103788119796 | validation: 0.003262399741733697]
	TIME [epoch: 8.29 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002977240922038846		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.002977240922038846 | validation: 0.003747636814388147]
	TIME [epoch: 8.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003629541903279982		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.003629541903279982 | validation: 0.005008835272317133]
	TIME [epoch: 8.26 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003622500960434299		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.003622500960434299 | validation: 0.0056171519088982795]
	TIME [epoch: 8.27 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033244438719066396		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.0033244438719066396 | validation: 0.0043993974984379465]
	TIME [epoch: 8.27 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00336013854600503		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.00336013854600503 | validation: 0.003594326292374778]
	TIME [epoch: 8.27 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003017966916778677		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.003017966916778677 | validation: 0.004557513717839679]
	TIME [epoch: 8.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034503723189578247		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0034503723189578247 | validation: 0.003386993904376447]
	TIME [epoch: 8.27 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034151476236639125		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0034151476236639125 | validation: 0.004431717858481557]
	TIME [epoch: 8.27 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003868737123829055		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.003868737123829055 | validation: 0.003870610857516126]
	TIME [epoch: 8.27 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003115848735064301		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.003115848735064301 | validation: 0.004724248406338736]
	TIME [epoch: 8.26 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003406813991855957		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.003406813991855957 | validation: 0.004678987439004616]
	TIME [epoch: 8.29 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00285124368118422		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.00285124368118422 | validation: 0.003711577751103691]
	TIME [epoch: 8.29 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033912864637656963		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.0033912864637656963 | validation: 0.003286135844683552]
	TIME [epoch: 8.27 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030175757456162968		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0030175757456162968 | validation: 0.004481602820740703]
	TIME [epoch: 8.27 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002921128404606759		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.002921128404606759 | validation: 0.004368584937548182]
	TIME [epoch: 8.26 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003882458110663777		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.003882458110663777 | validation: 0.007060264302424221]
	TIME [epoch: 8.27 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003961418320027779		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.003961418320027779 | validation: 0.003646874759169815]
	TIME [epoch: 8.31 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034117859954089884		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.0034117859954089884 | validation: 0.008163536057658146]
	TIME [epoch: 8.26 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036185376789572036		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.0036185376789572036 | validation: 0.003164874343640953]
	TIME [epoch: 8.27 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002647618086354295		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.002647618086354295 | validation: 0.003401844890193231]
	TIME [epoch: 8.27 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002748560853841599		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.002748560853841599 | validation: 0.003971654707578659]
	TIME [epoch: 8.26 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037101716472283165		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0037101716472283165 | validation: 0.0037635519595545326]
	TIME [epoch: 8.26 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002837498460023711		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.002837498460023711 | validation: 0.0035063547376333855]
	TIME [epoch: 8.31 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003450985285196353		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.003450985285196353 | validation: 0.003941926436165162]
	TIME [epoch: 8.26 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002897506093350355		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.002897506093350355 | validation: 0.002759788666969375]
	TIME [epoch: 8.28 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031896374036362437		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0031896374036362437 | validation: 0.004416837918874789]
	TIME [epoch: 8.27 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029385824423231307		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0029385824423231307 | validation: 0.003831084177270936]
	TIME [epoch: 8.26 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025304265998375357		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0025304265998375357 | validation: 0.00374255981150998]
	TIME [epoch: 8.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003437052218364234		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.003437052218364234 | validation: 0.0035256233607003093]
	TIME [epoch: 8.28 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033756326176506025		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0033756326176506025 | validation: 0.003165609924129986]
	TIME [epoch: 8.25 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031559572218045487		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.0031559572218045487 | validation: 0.00485947076970396]
	TIME [epoch: 8.27 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031354746352736914		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0031354746352736914 | validation: 0.004025761295059305]
	TIME [epoch: 8.26 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026178211428152082		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0026178211428152082 | validation: 0.00434440207900013]
	TIME [epoch: 8.26 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003982776861930255		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.003982776861930255 | validation: 0.003471492267021717]
	TIME [epoch: 8.31 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003275431356940752		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.003275431356940752 | validation: 0.0031809292626038934]
	TIME [epoch: 8.27 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002529232786927683		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.002529232786927683 | validation: 0.003318773594315156]
	TIME [epoch: 8.25 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026331103645640193		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.0026331103645640193 | validation: 0.0041960372184913376]
	TIME [epoch: 8.26 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032382286608578763		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.0032382286608578763 | validation: 0.004470478630066146]
	TIME [epoch: 8.27 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047626516444150314		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.0047626516444150314 | validation: 0.004579453627080687]
	TIME [epoch: 8.28 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00263152846815271		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.00263152846815271 | validation: 0.0030172569279970267]
	TIME [epoch: 8.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030910638284828897		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0030910638284828897 | validation: 0.0034504657783647396]
	TIME [epoch: 8.26 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026635551933458625		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0026635551933458625 | validation: 0.003916025486045615]
	TIME [epoch: 8.25 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003067937734034797		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.003067937734034797 | validation: 0.004177166305524428]
	TIME [epoch: 8.27 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002576705213898205		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.002576705213898205 | validation: 0.003140176938336869]
	TIME [epoch: 8.27 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002636950560342486		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.002636950560342486 | validation: 0.005228070693021683]
	TIME [epoch: 8.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036538407095926127		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.0036538407095926127 | validation: 0.00343606409982741]
	TIME [epoch: 8.27 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00268108550283402		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.00268108550283402 | validation: 0.003807895421696081]
	TIME [epoch: 8.27 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028398370846075393		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0028398370846075393 | validation: 0.0041535494152123206]
	TIME [epoch: 8.25 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002732760595886625		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.002732760595886625 | validation: 0.003368824593462205]
	TIME [epoch: 8.27 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00285961645656816		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.00285961645656816 | validation: 0.003438477995369937]
	TIME [epoch: 8.28 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029183445268468793		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.0029183445268468793 | validation: 0.003770385153383158]
	TIME [epoch: 8.31 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002988039458326458		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.002988039458326458 | validation: 0.003553900349050348]
	TIME [epoch: 8.26 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003000099668536189		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.003000099668536189 | validation: 0.0041504735016021236]
	TIME [epoch: 8.27 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002999295579638514		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.002999295579638514 | validation: 0.003141859018628016]
	TIME [epoch: 8.25 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002557549245773622		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.002557549245773622 | validation: 0.004253883633648247]
	TIME [epoch: 8.25 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003274800358569162		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.003274800358569162 | validation: 0.002397780267233818]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029582757848109925		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.0029582757848109925 | validation: 0.0038578014904776204]
	TIME [epoch: 8.35 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026398846263361725		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.0026398846263361725 | validation: 0.0030243581673343716]
	TIME [epoch: 8.33 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002413177681844694		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.002413177681844694 | validation: 0.004705417504091365]
	TIME [epoch: 8.33 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027508834261592073		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0027508834261592073 | validation: 0.0028327567385937405]
	TIME [epoch: 8.31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003044058989009558		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.003044058989009558 | validation: 0.0047530899310638105]
	TIME [epoch: 8.33 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034815120594099552		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0034815120594099552 | validation: 0.003945755498943596]
	TIME [epoch: 8.39 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003166658598600387		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.003166658598600387 | validation: 0.0035348923759727507]
	TIME [epoch: 8.33 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026138769752609265		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.0026138769752609265 | validation: 0.0032305795492792557]
	TIME [epoch: 8.33 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002399568606673508		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.002399568606673508 | validation: 0.0068441410672398645]
	TIME [epoch: 8.33 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049704487100897966		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.0049704487100897966 | validation: 0.004523394952772226]
	TIME [epoch: 8.33 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023623224581790954		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0023623224581790954 | validation: 0.0028151446068444275]
	TIME [epoch: 8.36 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027474404922837223		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.0027474404922837223 | validation: 0.004396816803709385]
	TIME [epoch: 8.35 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028351369388707626		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0028351369388707626 | validation: 0.0025917267633048698]
	TIME [epoch: 8.33 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022044581270797272		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0022044581270797272 | validation: 0.0033467584249231427]
	TIME [epoch: 8.33 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028793155022351067		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0028793155022351067 | validation: 0.0035042223128758042]
	TIME [epoch: 8.33 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002296019535652864		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.002296019535652864 | validation: 0.003994728097930946]
	TIME [epoch: 8.32 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002448862331247099		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.002448862331247099 | validation: 0.0030176789923762356]
	TIME [epoch: 8.36 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023490844653092143		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.0023490844653092143 | validation: 0.004632329620260203]
	TIME [epoch: 8.33 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028911917139873232		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.0028911917139873232 | validation: 0.0029544762163150164]
	TIME [epoch: 8.32 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032432366240712955		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0032432366240712955 | validation: 0.0027962886533118418]
	TIME [epoch: 8.32 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002426867151776272		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.002426867151776272 | validation: 0.003099504050713242]
	TIME [epoch: 8.32 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003502571239788163		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.003502571239788163 | validation: 0.003941266229252375]
	TIME [epoch: 8.36 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028243103505308613		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0028243103505308613 | validation: 0.0030627147457038166]
	TIME [epoch: 8.32 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030325951648481906		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0030325951648481906 | validation: 0.003685205033954505]
	TIME [epoch: 8.32 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024396767202841067		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0024396767202841067 | validation: 0.0034828872925913813]
	TIME [epoch: 8.32 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019348287173825687		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0019348287173825687 | validation: 0.0032668379451130398]
	TIME [epoch: 8.31 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002631189963808955		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.002631189963808955 | validation: 0.002370920994806861]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002582194334701502		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.002582194334701502 | validation: 0.003648246552965022]
	TIME [epoch: 8.36 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002740929642909548		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.002740929642909548 | validation: 0.002942122325350899]
	TIME [epoch: 8.29 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002094653423745926		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.002094653423745926 | validation: 0.0033041514028464454]
	TIME [epoch: 8.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002134663764100316		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.002134663764100316 | validation: 0.002931129921240329]
	TIME [epoch: 8.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002743238407278429		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.002743238407278429 | validation: 0.002911612426791094]
	TIME [epoch: 8.29 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023636544091536816		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.0023636544091536816 | validation: 0.003056531341926098]
	TIME [epoch: 8.34 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022031293121019904		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0022031293121019904 | validation: 0.0031625488268568836]
	TIME [epoch: 8.31 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003407083967267967		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.003407083967267967 | validation: 0.003526257952291669]
	TIME [epoch: 8.29 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002606479418161588		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.002606479418161588 | validation: 0.002777320050674148]
	TIME [epoch: 8.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455860164878022		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.002455860164878022 | validation: 0.0027445175564753755]
	TIME [epoch: 8.29 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021752034574676984		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.0021752034574676984 | validation: 0.0029899029482975087]
	TIME [epoch: 8.29 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001971346941349264		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.001971346941349264 | validation: 0.0029151885113921326]
	TIME [epoch: 8.35 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003258416577281899		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.003258416577281899 | validation: 0.004380076290995429]
	TIME [epoch: 8.29 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002696809069618943		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.002696809069618943 | validation: 0.0038945041561984734]
	TIME [epoch: 8.28 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002597141447359733		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.002597141447359733 | validation: 0.0031319312394277247]
	TIME [epoch: 8.29 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00211849853330994		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.00211849853330994 | validation: 0.003242631952339225]
	TIME [epoch: 8.29 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026129911112838383		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.0026129911112838383 | validation: 0.0028932114781359992]
	TIME [epoch: 8.31 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002531619906383595		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.002531619906383595 | validation: 0.002980230795815679]
	TIME [epoch: 8.34 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023743179357426053		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.0023743179357426053 | validation: 0.0032682619241822977]
	TIME [epoch: 8.29 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00229389076803031		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.00229389076803031 | validation: 0.0031486292179775674]
	TIME [epoch: 8.28 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024065128147704004		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.0024065128147704004 | validation: 0.0034366170073526093]
	TIME [epoch: 8.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002599913053187492		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.002599913053187492 | validation: 0.003263112452033562]
	TIME [epoch: 8.29 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002358099199738184		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.002358099199738184 | validation: 0.0035040232824160465]
	TIME [epoch: 8.35 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021752063539376776		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0021752063539376776 | validation: 0.002796765172094863]
	TIME [epoch: 8.31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019149993882363022		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0019149993882363022 | validation: 0.0026928521326757427]
	TIME [epoch: 8.29 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026723849243452816		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0026723849243452816 | validation: 0.004194325750511992]
	TIME [epoch: 8.28 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002897081715623312		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.002897081715623312 | validation: 0.003658276986815235]
	TIME [epoch: 8.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021856382996516305		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0021856382996516305 | validation: 0.0029242303645174187]
	TIME [epoch: 8.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024138394583723345		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.0024138394583723345 | validation: 0.0036138337651102593]
	TIME [epoch: 8.33 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019913958890815553		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0019913958890815553 | validation: 0.0027665541296004285]
	TIME [epoch: 8.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005416718505335734		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.005416718505335734 | validation: 0.005590012244063178]
	TIME [epoch: 8.29 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027707530191479327		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.0027707530191479327 | validation: 0.0035117977149632237]
	TIME [epoch: 8.28 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002080878149092472		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.002080878149092472 | validation: 0.0031107213952153686]
	TIME [epoch: 8.29 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022315735729569568		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0022315735729569568 | validation: 0.003377194007085454]
	TIME [epoch: 8.34 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024290726023030455		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0024290726023030455 | validation: 0.0023865619930572464]
	TIME [epoch: 8.31 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019171049077751242		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0019171049077751242 | validation: 0.002026209492283692]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021709453540775785		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.0021709453540775785 | validation: 0.003442182064377704]
	TIME [epoch: 8.27 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022753890276120225		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0022753890276120225 | validation: 0.0028648519296917627]
	TIME [epoch: 8.25 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023326767850262746		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0023326767850262746 | validation: 0.002920479689853937]
	TIME [epoch: 8.27 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018998851504070527		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0018998851504070527 | validation: 0.0021669509662585115]
	TIME [epoch: 8.31 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002050727065020243		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.002050727065020243 | validation: 0.0031295580777774097]
	TIME [epoch: 8.27 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002341270012398722		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.002341270012398722 | validation: 0.0028451208457968545]
	TIME [epoch: 8.27 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021513813427079414		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0021513813427079414 | validation: 0.0030802896597685505]
	TIME [epoch: 8.27 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022824028921456714		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0022824028921456714 | validation: 0.002496282702540943]
	TIME [epoch: 8.27 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021964958733250326		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0021964958733250326 | validation: 0.0028298845000519625]
	TIME [epoch: 8.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002269790084826055		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.002269790084826055 | validation: 0.003340009323561997]
	TIME [epoch: 8.29 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022601709558574563		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0022601709558574563 | validation: 0.003406703705845484]
	TIME [epoch: 8.27 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00284875493194588		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.00284875493194588 | validation: 0.004722488347856992]
	TIME [epoch: 8.27 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022238094945008467		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0022238094945008467 | validation: 0.002746476515130273]
	TIME [epoch: 8.26 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002238916146576173		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.002238916146576173 | validation: 0.0033285330626299714]
	TIME [epoch: 8.27 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002185562994600641		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.002185562994600641 | validation: 0.0025714001379824724]
	TIME [epoch: 8.31 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017086797971517576		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0017086797971517576 | validation: 0.004367384311586644]
	TIME [epoch: 8.28 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002643942492224294		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.002643942492224294 | validation: 0.002116444428581197]
	TIME [epoch: 8.26 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022190685702516786		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0022190685702516786 | validation: 0.002584728285401113]
	TIME [epoch: 8.27 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019139882428614555		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0019139882428614555 | validation: 0.0026760625917065265]
	TIME [epoch: 8.26 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018266792482780274		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.0018266792482780274 | validation: 0.0030406962602255528]
	TIME [epoch: 8.29 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020335919790160023		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0020335919790160023 | validation: 0.00318709914603426]
	TIME [epoch: 8.29 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002176741703692736		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.002176741703692736 | validation: 0.0027128427392445077]
	TIME [epoch: 8.28 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002419192374191254		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.002419192374191254 | validation: 0.002621912781964639]
	TIME [epoch: 8.26 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022319166437656896		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.0022319166437656896 | validation: 0.0027599017635375313]
	TIME [epoch: 8.26 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020089599274076965		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0020089599274076965 | validation: 0.0030945449269484454]
	TIME [epoch: 8.27 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002011269531813444		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.002011269531813444 | validation: 0.0028370849555919217]
	TIME [epoch: 8.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002276798135484219		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.002276798135484219 | validation: 0.0025885415366016166]
	TIME [epoch: 8.27 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020767744908455005		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0020767744908455005 | validation: 0.002386843759515888]
	TIME [epoch: 8.28 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022380497941233164		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0022380497941233164 | validation: 0.0025848767388106787]
	TIME [epoch: 8.26 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002184038799968193		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.002184038799968193 | validation: 0.0027192861625905235]
	TIME [epoch: 8.27 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019249210575703294		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.0019249210575703294 | validation: 0.0027120011121355603]
	TIME [epoch: 8.27 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019530614102590492		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0019530614102590492 | validation: 0.002415170254719571]
	TIME [epoch: 8.31 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002061106363215814		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.002061106363215814 | validation: 0.0024454400883096527]
	TIME [epoch: 8.25 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018252172095378834		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0018252172095378834 | validation: 0.0024970556219978813]
	TIME [epoch: 8.26 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002522808607646238		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.002522808607646238 | validation: 0.002429516255253571]
	TIME [epoch: 8.27 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017197917474625		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.0017197917474625 | validation: 0.002901546782197609]
	TIME [epoch: 8.27 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018148286872662243		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.0018148286872662243 | validation: 0.0023847126816078862]
	TIME [epoch: 8.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020276514587830295		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0020276514587830295 | validation: 0.0023567106684522603]
	TIME [epoch: 8.28 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018636753566507113		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.0018636753566507113 | validation: 0.0023228844158345164]
	TIME [epoch: 8.27 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019085770022710686		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0019085770022710686 | validation: 0.0026090517058241325]
	TIME [epoch: 8.26 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026730537209800516		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.0026730537209800516 | validation: 0.0031343172663419257]
	TIME [epoch: 8.29 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002147308647076238		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.002147308647076238 | validation: 0.0026890930718347183]
	TIME [epoch: 8.27 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00164369962977867		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.00164369962977867 | validation: 0.0029062921516661744]
	TIME [epoch: 8.31 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018199472705999042		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0018199472705999042 | validation: 0.0028055175237890577]
	TIME [epoch: 8.27 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019649757820173358		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0019649757820173358 | validation: 0.004148019422319703]
	TIME [epoch: 8.27 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002426030585946986		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.002426030585946986 | validation: 0.0029403592605352774]
	TIME [epoch: 8.26 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002095006916282789		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.002095006916282789 | validation: 0.002761606351666311]
	TIME [epoch: 8.27 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018879026433359964		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0018879026433359964 | validation: 0.0028441582483999]
	TIME [epoch: 8.29 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016679070770840943		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.0016679070770840943 | validation: 0.002663719214331688]
	TIME [epoch: 8.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018957632262378405		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.0018957632262378405 | validation: 0.002495622791903182]
	TIME [epoch: 8.27 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019534865769765264		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0019534865769765264 | validation: 0.0029317969383678044]
	TIME [epoch: 8.26 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003630485477048331		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.003630485477048331 | validation: 0.003680076552726139]
	TIME [epoch: 8.25 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019989821044531955		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.0019989821044531955 | validation: 0.0025062228538926526]
	TIME [epoch: 8.27 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023761501740772115		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0023761501740772115 | validation: 0.003208183474228359]
	TIME [epoch: 8.31 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002198951206283071		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.002198951206283071 | validation: 0.002433226106313712]
	TIME [epoch: 8.28 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019425222264835673		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0019425222264835673 | validation: 0.002214375802227086]
	TIME [epoch: 8.27 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016665482811765997		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0016665482811765997 | validation: 0.0019615709895293137]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002104622496461042		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.002104622496461042 | validation: 0.003255433401023344]
	TIME [epoch: 8.25 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023342166274228425		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0023342166274228425 | validation: 0.002555569037594265]
	TIME [epoch: 8.28 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001612664414814591		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.001612664414814591 | validation: 0.0021730499283217643]
	TIME [epoch: 8.31 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002071231802104312		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.002071231802104312 | validation: 0.0026515660323198588]
	TIME [epoch: 8.27 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002147705940137138		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.002147705940137138 | validation: 0.0026749714283816318]
	TIME [epoch: 8.26 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001769341979764969		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.001769341979764969 | validation: 0.001866805450323315]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_911.pth
	Model improved!!!
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001713476162720126		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.001713476162720126 | validation: 0.0026538168104947293]
	TIME [epoch: 8.24 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002059233197857094		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.002059233197857094 | validation: 0.0019245834798010044]
	TIME [epoch: 8.31 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018862389605132253		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0018862389605132253 | validation: 0.0022552161085942794]
	TIME [epoch: 8.28 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018220885361682451		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0018220885361682451 | validation: 0.002147357924318798]
	TIME [epoch: 8.51 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018453097636603093		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0018453097636603093 | validation: 0.0031370629194918767]
	TIME [epoch: 8.21 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00165507654902791		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.00165507654902791 | validation: 0.002717608264148364]
	TIME [epoch: 8.21 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018445261927701065		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0018445261927701065 | validation: 0.00260092407821333]
	TIME [epoch: 8.22 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002189426895883704		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.002189426895883704 | validation: 0.0029476344277589465]
	TIME [epoch: 8.26 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021542217776020214		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0021542217776020214 | validation: 0.00257273837629592]
	TIME [epoch: 8.21 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00209992189694493		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.00209992189694493 | validation: 0.002855004510739707]
	TIME [epoch: 8.21 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020053434797683034		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0020053434797683034 | validation: 0.0024630152792201477]
	TIME [epoch: 8.21 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002363375051120426		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.002363375051120426 | validation: 0.0027757865268358967]
	TIME [epoch: 8.21 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022662388754061315		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0022662388754061315 | validation: 0.0028245147071475508]
	TIME [epoch: 8.25 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016569389145974648		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0016569389145974648 | validation: 0.0021225347746800464]
	TIME [epoch: 8.23 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015622437878185474		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0015622437878185474 | validation: 0.002951750661793909]
	TIME [epoch: 8.21 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015760108723379134		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0015760108723379134 | validation: 0.002816981737699247]
	TIME [epoch: 8.21 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017986382486765275		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0017986382486765275 | validation: 0.0025551540653226262]
	TIME [epoch: 8.21 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018262574272778311		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0018262574272778311 | validation: 0.002265934005705874]
	TIME [epoch: 8.21 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018305224373417468		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0018305224373417468 | validation: 0.002590225163113706]
	TIME [epoch: 8.26 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018086831273618302		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.0018086831273618302 | validation: 0.00259689900287583]
	TIME [epoch: 8.22 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001833558859889568		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.001833558859889568 | validation: 0.0033613812241511686]
	TIME [epoch: 8.21 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018439651007094585		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0018439651007094585 | validation: 0.0032880806017599835]
	TIME [epoch: 8.22 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002162011746374837		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.002162011746374837 | validation: 0.0024432031230020284]
	TIME [epoch: 8.21 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016808798571175125		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0016808798571175125 | validation: 0.002187056800693478]
	TIME [epoch: 8.22 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017247862022052082		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0017247862022052082 | validation: 0.0027457480118992087]
	TIME [epoch: 8.26 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016287170359973885		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.0016287170359973885 | validation: 0.0027644739364351663]
	TIME [epoch: 8.25 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017912587696313556		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0017912587696313556 | validation: 0.002485120797458172]
	TIME [epoch: 8.27 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015833171151901338		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0015833171151901338 | validation: 0.0021376059694116806]
	TIME [epoch: 8.24 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001723371006464075		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.001723371006464075 | validation: 0.0029190110582272884]
	TIME [epoch: 8.22 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001436770950985827		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.001436770950985827 | validation: 0.0018478589528449882]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017076998408176295		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0017076998408176295 | validation: 0.002104168763798408]
	TIME [epoch: 8.23 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017098968111050271		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0017098968111050271 | validation: 0.0023335018003444227]
	TIME [epoch: 8.22 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017423932290285772		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0017423932290285772 | validation: 0.002537481942754402]
	TIME [epoch: 8.22 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017009640901637074		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.0017009640901637074 | validation: 0.0030373453486746685]
	TIME [epoch: 8.22 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014906946365430599		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0014906946365430599 | validation: 0.002228191363868417]
	TIME [epoch: 8.23 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001785060121313207		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.001785060121313207 | validation: 0.002234621832258984]
	TIME [epoch: 8.27 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016215103681976768		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.0016215103681976768 | validation: 0.002454686590733294]
	TIME [epoch: 8.22 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00165497744446185		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.00165497744446185 | validation: 0.002668003819009572]
	TIME [epoch: 8.22 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016078612560544942		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0016078612560544942 | validation: 0.0024865371588352367]
	TIME [epoch: 8.28 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019827180697812043		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0019827180697812043 | validation: 0.002190032245060502]
	TIME [epoch: 8.22 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016973496608767835		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0016973496608767835 | validation: 0.0022754318258565195]
	TIME [epoch: 8.26 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00159971303197051		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.00159971303197051 | validation: 0.0027766075266016533]
	TIME [epoch: 8.23 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017240983666607858		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0017240983666607858 | validation: 0.0020925231574184535]
	TIME [epoch: 8.22 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001652648380684081		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.001652648380684081 | validation: 0.0029588703247644655]
	TIME [epoch: 8.22 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013208904742129039		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0013208904742129039 | validation: 0.0022707555898550036]
	TIME [epoch: 8.22 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016868237722791834		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.0016868237722791834 | validation: 0.0026487358573610404]
	TIME [epoch: 8.22 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014745696039253527		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0014745696039253527 | validation: 0.0016676390407824382]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_958.pth
	Model improved!!!
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016093000639268778		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0016093000639268778 | validation: 0.002858694479337765]
	TIME [epoch: 8.21 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018614770378079415		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.0018614770378079415 | validation: 0.0026811802298394643]
	TIME [epoch: 8.21 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001562420567744703		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.001562420567744703 | validation: 0.0022244974770404217]
	TIME [epoch: 8.21 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014442876556935945		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0014442876556935945 | validation: 0.0023403495964063803]
	TIME [epoch: 8.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001459128297139501		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.001459128297139501 | validation: 0.0021785390681385132]
	TIME [epoch: 8.22 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019052329671555492		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0019052329671555492 | validation: 0.002348179810461777]
	TIME [epoch: 8.25 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001701488585094356		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.001701488585094356 | validation: 0.002346155256147487]
	TIME [epoch: 8.21 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016423129815395813		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0016423129815395813 | validation: 0.0023866370315795554]
	TIME [epoch: 8.21 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014768322766784115		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0014768322766784115 | validation: 0.0024545646598003075]
	TIME [epoch: 8.21 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015112792318682693		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0015112792318682693 | validation: 0.002721608541117761]
	TIME [epoch: 8.21 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018108817529388227		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0018108817529388227 | validation: 0.0019748872852321173]
	TIME [epoch: 8.27 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001837504449200223		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.001837504449200223 | validation: 0.002134395672914712]
	TIME [epoch: 8.24 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015547979281727825		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0015547979281727825 | validation: 0.0031060430248910915]
	TIME [epoch: 8.23 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001854990310969906		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.001854990310969906 | validation: 0.002709566237023891]
	TIME [epoch: 8.23 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013557929721792906		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0013557929721792906 | validation: 0.002641420634321476]
	TIME [epoch: 8.22 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017411567258224037		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0017411567258224037 | validation: 0.0019780471333487393]
	TIME [epoch: 8.23 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001386367275079296		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.001386367275079296 | validation: 0.0021639727332140933]
	TIME [epoch: 8.27 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019242853664097964		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0019242853664097964 | validation: 0.0030489195339330833]
	TIME [epoch: 8.22 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019432218306876616		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0019432218306876616 | validation: 0.0022124590665999018]
	TIME [epoch: 8.23 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014677911656533534		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0014677911656533534 | validation: 0.003808037081716827]
	TIME [epoch: 8.23 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015155580527947967		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0015155580527947967 | validation: 0.0021960057143237295]
	TIME [epoch: 8.23 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001663683750031799		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.001663683750031799 | validation: 0.0019582562934565883]
	TIME [epoch: 8.22 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015279451000646902		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0015279451000646902 | validation: 0.002827712796885126]
	TIME [epoch: 8.23 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015200722895278734		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0015200722895278734 | validation: 0.002391579852465422]
	TIME [epoch: 8.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018405183155340635		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0018405183155340635 | validation: 0.0025370271718240566]
	TIME [epoch: 8.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001334205299209093		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.001334205299209093 | validation: 0.0021532224490709976]
	TIME [epoch: 8.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017324863145168736		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0017324863145168736 | validation: 0.0022293206637159636]
	TIME [epoch: 8.19 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014426217034612058		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.0014426217034612058 | validation: 0.002063145470414179]
	TIME [epoch: 8.24 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015813094606459078		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.0015813094606459078 | validation: 0.0028616383392909197]
	TIME [epoch: 8.21 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001480888620899201		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.001480888620899201 | validation: 0.0022124034590443216]
	TIME [epoch: 8.19 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016338449927873044		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0016338449927873044 | validation: 0.0026834936019690136]
	TIME [epoch: 8.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016269540753377058		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0016269540753377058 | validation: 0.0021037470063179154]
	TIME [epoch: 8.19 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014720407166807785		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0014720407166807785 | validation: 0.00206290774462244]
	TIME [epoch: 8.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001395620532664673		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.001395620532664673 | validation: 0.00164578581728846]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014275221791631416		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0014275221791631416 | validation: 0.0022893820726760366]
	TIME [epoch: 8.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001509314972729491		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.001509314972729491 | validation: 0.003195015717111061]
	TIME [epoch: 8.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017092272327439545		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0017092272327439545 | validation: 0.0020523076075143762]
	TIME [epoch: 8.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001470239337304448		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.001470239337304448 | validation: 0.0020072597665702994]
	TIME [epoch: 8.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001580344174266854		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.001580344174266854 | validation: 0.0020150941339712085]
	TIME [epoch: 8.24 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017554125326115072		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0017554125326115072 | validation: 0.0024300406973691412]
	TIME [epoch: 8.22 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001284892288811304		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.001284892288811304 | validation: 0.001857666282660431]
	TIME [epoch: 8.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017001463227502979		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.0017001463227502979 | validation: 0.0019798959586365098]
	TIME [epoch: 8.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012252186582509068		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0012252186582509068 | validation: 0.0020755876599609186]
	TIME [epoch: 8.19 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014053003757749248		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0014053003757749248 | validation: 0.002462533269608446]
	TIME [epoch: 8.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014649334593885054		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0014649334593885054 | validation: 0.001702635522184531]
	TIME [epoch: 8.24 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016428149696928373		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0016428149696928373 | validation: 0.002277055408411788]
	TIME [epoch: 8.19 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015800734106386218		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0015800734106386218 | validation: 0.001968902359887017]
	TIME [epoch: 8.19 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013572533988830205		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0013572533988830205 | validation: 0.0019214609040530436]
	TIME [epoch: 8.19 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019019217272408491		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0019019217272408491 | validation: 0.0022535569924219062]
	TIME [epoch: 8.19 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015204757860974162		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0015204757860974162 | validation: 0.0022422986429910144]
	TIME [epoch: 8.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013906941165104454		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.0013906941165104454 | validation: 0.0021648696912753155]
	TIME [epoch: 8.22 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011657862370712132		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0011657862370712132 | validation: 0.0024278306010917985]
	TIME [epoch: 8.19 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014907314162632143		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.0014907314162632143 | validation: 0.0018234987002863933]
	TIME [epoch: 8.19 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013827860114824693		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0013827860114824693 | validation: 0.0019680943275811424]
	TIME [epoch: 8.21 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014861142164667204		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0014861142164667204 | validation: 0.0019638546193980035]
	TIME [epoch: 8.19 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014249655566971207		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0014249655566971207 | validation: 0.002115279108820162]
	TIME [epoch: 8.24 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013355358045529899		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0013355358045529899 | validation: 0.001609583286327439]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_1015.pth
	Model improved!!!
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00141434623122691		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.00141434623122691 | validation: 0.0019830198093261]
	TIME [epoch: 8.19 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014561102076894768		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0014561102076894768 | validation: 0.0020598355662577667]
	TIME [epoch: 8.19 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014542983082267724		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0014542983082267724 | validation: 0.0021531324887441184]
	TIME [epoch: 8.19 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011779334617065822		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0011779334617065822 | validation: 0.0018064787650747282]
	TIME [epoch: 8.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012843212063759933		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.0012843212063759933 | validation: 0.002918356793489206]
	TIME [epoch: 8.24 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015262537452153382		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0015262537452153382 | validation: 0.0018304609029710938]
	TIME [epoch: 8.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014532873202744564		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.0014532873202744564 | validation: 0.002702799213755304]
	TIME [epoch: 8.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017268158899273133		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0017268158899273133 | validation: 0.0022162389951103194]
	TIME [epoch: 8.19 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014253302212215357		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0014253302212215357 | validation: 0.002336739722786141]
	TIME [epoch: 8.19 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013246848598319496		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0013246848598319496 | validation: 0.0020080575889963965]
	TIME [epoch: 8.23 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015679345651159412		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.0015679345651159412 | validation: 0.002191314436113615]
	TIME [epoch: 8.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001326129767312327		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.001326129767312327 | validation: 0.001878333623516375]
	TIME [epoch: 8.19 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014409872965454994		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0014409872965454994 | validation: 0.001930536845626559]
	TIME [epoch: 8.19 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014410278386725683		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0014410278386725683 | validation: 0.0018367814856297046]
	TIME [epoch: 8.19 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014066688897979326		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0014066688897979326 | validation: 0.0022051288727043834]
	TIME [epoch: 8.19 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014476847999794634		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0014476847999794634 | validation: 0.0018549242984717625]
	TIME [epoch: 8.24 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001262389780336739		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.001262389780336739 | validation: 0.0021741664397029606]
	TIME [epoch: 8.19 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013546559671608065		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0013546559671608065 | validation: 0.0020806545560046663]
	TIME [epoch: 8.19 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011354569002104877		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.0011354569002104877 | validation: 0.0022320518384172728]
	TIME [epoch: 8.19 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013311169738237576		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0013311169738237576 | validation: 0.0024766081680092063]
	TIME [epoch: 8.19 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015099878329901137		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0015099878329901137 | validation: 0.0020014071754954656]
	TIME [epoch: 8.2 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014539670355257028		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0014539670355257028 | validation: 0.0018079718022893537]
	TIME [epoch: 8.23 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014188630818715712		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0014188630818715712 | validation: 0.0023093947917466025]
	TIME [epoch: 8.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012961845069353163		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0012961845069353163 | validation: 0.0018539114071548521]
	TIME [epoch: 8.19 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013808708470864233		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0013808708470864233 | validation: 0.002094911544728467]
	TIME [epoch: 8.19 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011965383751831977		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0011965383751831977 | validation: 0.0013775273302320966]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_1041.pth
	Model improved!!!
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012251777477154814		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0012251777477154814 | validation: 0.0023196584571861453]
	TIME [epoch: 8.23 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001368480744993327		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.001368480744993327 | validation: 0.0016703012208652624]
	TIME [epoch: 8.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011876141433601792		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0011876141433601792 | validation: 0.0021281987600044036]
	TIME [epoch: 8.19 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012151646315507647		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0012151646315507647 | validation: 0.0020149791356737826]
	TIME [epoch: 8.19 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011221207815618525		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0011221207815618525 | validation: 0.0023100569977490856]
	TIME [epoch: 8.19 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001456029284083265		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.001456029284083265 | validation: 0.002114455668884558]
	TIME [epoch: 8.19 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012790537780092803		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0012790537780092803 | validation: 0.0021780831427991912]
	TIME [epoch: 8.23 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001477361944680606		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.001477361944680606 | validation: 0.0023410961498844516]
	TIME [epoch: 8.2 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011705359406468125		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.0011705359406468125 | validation: 0.002432488155455582]
	TIME [epoch: 8.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015522993589139956		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0015522993589139956 | validation: 0.0020363250178520964]
	TIME [epoch: 8.21 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001146975014619411		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.001146975014619411 | validation: 0.002100478173396931]
	TIME [epoch: 8.19 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001277670103892978		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.001277670103892978 | validation: 0.0018855470179823942]
	TIME [epoch: 8.19 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013216695307558275		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0013216695307558275 | validation: 0.0018857260216505827]
	TIME [epoch: 8.22 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011005501774143634		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0011005501774143634 | validation: 0.001735247865447148]
	TIME [epoch: 8.19 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010588251752024153		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0010588251752024153 | validation: 0.0020723905430202827]
	TIME [epoch: 8.19 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013094565656470965		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0013094565656470965 | validation: 0.0017646645261627939]
	TIME [epoch: 8.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011601347601000026		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0011601347601000026 | validation: 0.0028799267543247516]
	TIME [epoch: 8.19 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015022985859669412		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0015022985859669412 | validation: 0.002559962551263352]
	TIME [epoch: 8.23 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014473909557033778		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0014473909557033778 | validation: 0.0022038797914598752]
	TIME [epoch: 8.21 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001490386819368494		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.001490386819368494 | validation: 0.002346601881802318]
	TIME [epoch: 8.19 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013273233311944543		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.0013273233311944543 | validation: 0.001824131707637087]
	TIME [epoch: 8.19 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013827795244089121		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0013827795244089121 | validation: 0.001646255277413774]
	TIME [epoch: 8.19 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012164408441330211		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0012164408441330211 | validation: 0.002241854965801829]
	TIME [epoch: 8.19 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001333301256989858		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.001333301256989858 | validation: 0.0020613708306056254]
	TIME [epoch: 8.23 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013061492303905258		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0013061492303905258 | validation: 0.002388926054972492]
	TIME [epoch: 8.19 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014841573494908018		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0014841573494908018 | validation: 0.002143590828260878]
	TIME [epoch: 8.19 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012573021132395931		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0012573021132395931 | validation: 0.0018615534650826692]
	TIME [epoch: 8.19 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012919670551308597		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.0012919670551308597 | validation: 0.0022450671757253743]
	TIME [epoch: 8.19 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001200149998573665		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.001200149998573665 | validation: 0.002264809624314418]
	TIME [epoch: 8.21 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014315275834120712		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0014315275834120712 | validation: 0.0018627853829152193]
	TIME [epoch: 8.22 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013152224443860098		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0013152224443860098 | validation: 0.0021927027438368605]
	TIME [epoch: 8.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014289766471684423		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0014289766471684423 | validation: 0.001913397680199406]
	TIME [epoch: 8.19 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012871081393679856		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0012871081393679856 | validation: 0.0019827088018982177]
	TIME [epoch: 8.19 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013462838807370018		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0013462838807370018 | validation: 0.001816245152108687]
	TIME [epoch: 8.19 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015936922967215257		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0015936922967215257 | validation: 0.0021070771127079417]
	TIME [epoch: 8.23 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001392333453590753		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.001392333453590753 | validation: 0.0019572416348417896]
	TIME [epoch: 8.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012073349399688814		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0012073349399688814 | validation: 0.0020525734485371486]
	TIME [epoch: 8.19 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001422687250206556		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.001422687250206556 | validation: 0.0018607263462645093]
	TIME [epoch: 8.19 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001134359034657806		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.001134359034657806 | validation: 0.00217383864996489]
	TIME [epoch: 8.19 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013753286384166835		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.0013753286384166835 | validation: 0.0014325718393835727]
	TIME [epoch: 8.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013379968105203563		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.0013379968105203563 | validation: 0.001574274595488478]
	TIME [epoch: 8.23 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012928408651327803		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0012928408651327803 | validation: 0.0019151065580345081]
	TIME [epoch: 8.19 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014209542275510608		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0014209542275510608 | validation: 0.002220997550422491]
	TIME [epoch: 8.19 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012112530158108741		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.0012112530158108741 | validation: 0.001973938737490813]
	TIME [epoch: 8.19 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011010646633977323		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0011010646633977323 | validation: 0.0018744804318715396]
	TIME [epoch: 8.19 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015276640550612585		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0015276640550612585 | validation: 0.001531098112084915]
	TIME [epoch: 8.21 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001450737944857736		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.001450737944857736 | validation: 0.001955695479880897]
	TIME [epoch: 8.22 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013574685180861913		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0013574685180861913 | validation: 0.0018006097681677255]
	TIME [epoch: 8.19 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013182325137592347		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0013182325137592347 | validation: 0.0026592719269072657]
	TIME [epoch: 8.19 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011830884546646307		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0011830884546646307 | validation: 0.0022771203989095366]
	TIME [epoch: 8.19 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011931203824670192		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0011931203824670192 | validation: 0.0016671912449967216]
	TIME [epoch: 8.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014621217966620777		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0014621217966620777 | validation: 0.0017302577324239082]
	TIME [epoch: 8.24 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009579615768160618		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0009579615768160618 | validation: 0.002049253555616378]
	TIME [epoch: 8.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011751254678625564		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.0011751254678625564 | validation: 0.001917831426407533]
	TIME [epoch: 8.19 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00110720893541698		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.00110720893541698 | validation: 0.0016474189374319696]
	TIME [epoch: 8.19 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011757285828337704		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0011757285828337704 | validation: 0.00240048800325679]
	TIME [epoch: 8.19 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010923858898862586		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0010923858898862586 | validation: 0.002198624939567795]
	TIME [epoch: 8.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013988313444723123		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0013988313444723123 | validation: 0.0020094642451087188]
	TIME [epoch: 8.22 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001148457581933036		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.001148457581933036 | validation: 0.0021233888311647776]
	TIME [epoch: 8.19 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013790486836882305		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0013790486836882305 | validation: 0.001785976713748541]
	TIME [epoch: 8.21 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001108792901350779		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.001108792901350779 | validation: 0.0021679713076983393]
	TIME [epoch: 8.21 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015024707700794352		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0015024707700794352 | validation: 0.001937971485928907]
	TIME [epoch: 8.21 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012690414579453971		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0012690414579453971 | validation: 0.00243466126688218]
	TIME [epoch: 8.26 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011720451660180665		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.0011720451660180665 | validation: 0.002124477125458518]
	TIME [epoch: 8.22 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001331284650996392		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.001331284650996392 | validation: 0.0023072993970398893]
	TIME [epoch: 8.21 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013782198239027349		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0013782198239027349 | validation: 0.0023377052113725903]
	TIME [epoch: 8.21 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012799380381688166		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0012799380381688166 | validation: 0.0017122696481280358]
	TIME [epoch: 8.21 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012553917301323214		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0012553917301323214 | validation: 0.0017114250327654216]
	TIME [epoch: 8.21 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013282945260643967		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0013282945260643967 | validation: 0.0019393173236638512]
	TIME [epoch: 8.26 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001174177877360846		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.001174177877360846 | validation: 0.0014068432771126966]
	TIME [epoch: 8.21 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013220357309270576		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0013220357309270576 | validation: 0.0018232531188075286]
	TIME [epoch: 8.21 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001400790246186663		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.001400790246186663 | validation: 0.001951905398179659]
	TIME [epoch: 8.21 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011140077346274025		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.0011140077346274025 | validation: 0.0022848766714951693]
	TIME [epoch: 8.21 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011530400760847502		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0011530400760847502 | validation: 0.001977032129272094]
	TIME [epoch: 8.23 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012242137926530279		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0012242137926530279 | validation: 0.001837632690025857]
	TIME [epoch: 8.24 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001029925618438872		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.001029925618438872 | validation: 0.001653096451023038]
	TIME [epoch: 8.21 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012042128056250884		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0012042128056250884 | validation: 0.0019369784278846787]
	TIME [epoch: 8.21 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010259439312479285		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0010259439312479285 | validation: 0.0020116787998030565]
	TIME [epoch: 8.21 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012790908177641746		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0012790908177641746 | validation: 0.001794321465608407]
	TIME [epoch: 8.21 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011182347811766844		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0011182347811766844 | validation: 0.0014785525247544137]
	TIME [epoch: 8.26 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012720268086258367		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0012720268086258367 | validation: 0.001867760041358352]
	TIME [epoch: 8.21 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010932002591105053		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0010932002591105053 | validation: 0.0017652915578339262]
	TIME [epoch: 8.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011187261435818407		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0011187261435818407 | validation: 0.002405972797493826]
	TIME [epoch: 8.23 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012989595074681794		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.0012989595074681794 | validation: 0.0017545538500356548]
	TIME [epoch: 8.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011805742264913528		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.0011805742264913528 | validation: 0.002169390667920992]
	TIME [epoch: 8.21 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013030430118258812		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0013030430118258812 | validation: 0.0021188761020309787]
	TIME [epoch: 8.25 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011924807091243633		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0011924807091243633 | validation: 0.001588590319845439]
	TIME [epoch: 8.21 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013365362316125023		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0013365362316125023 | validation: 0.0024278072965249527]
	TIME [epoch: 8.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012754349422642072		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.0012754349422642072 | validation: 0.001688990883983639]
	TIME [epoch: 8.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012288497135858236		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0012288497135858236 | validation: 0.0018282872742994617]
	TIME [epoch: 8.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011858485678462186		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0011858485678462186 | validation: 0.0014379812960461581]
	TIME [epoch: 8.23 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011217815820796014		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0011217815820796014 | validation: 0.001712778618591493]
	TIME [epoch: 8.23 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010734231900848264		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0010734231900848264 | validation: 0.002344289587398648]
	TIME [epoch: 8.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011250015691606416		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0011250015691606416 | validation: 0.0017998827111773918]
	TIME [epoch: 8.2 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010568437968757186		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0010568437968757186 | validation: 0.0016999134116875236]
	TIME [epoch: 8.19 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012860320257798285		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0012860320257798285 | validation: 0.001897194150703866]
	TIME [epoch: 8.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001050159072913787		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.001050159072913787 | validation: 0.0016836920283176955]
	TIME [epoch: 8.25 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000997517700615768		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.000997517700615768 | validation: 0.001602748773406974]
	TIME [epoch: 8.21 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011641094506051265		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0011641094506051265 | validation: 0.002702716035428143]
	TIME [epoch: 8.2 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012515669974650862		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0012515669974650862 | validation: 0.0016257499224853226]
	TIME [epoch: 8.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013559597588970944		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.0013559597588970944 | validation: 0.0017277657118522072]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143655/states/model_phi1_1a_v_mmd1_1142.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 9705.321 seconds.
