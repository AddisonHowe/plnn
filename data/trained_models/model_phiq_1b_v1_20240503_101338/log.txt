Args:
Namespace(name='model_phiq_1b_v1', outdir='out/model_training/model_phiq_1b_v1', training_data='data/training_data/data_phiq_1b/training', validation_data='data/training_data/data_phiq_1b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2931545329

Training model...

Saving initial model state to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 11.642661448575806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.642661448575806 | validation: 12.046166496404709]
	TIME [epoch: 114 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 11.27334617669403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.27334617669403 | validation: 11.372953473378296]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.853351114823155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.853351114823155 | validation: 11.246454136688946]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 11.411883158369484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.411883158369484 | validation: 11.412853087962887]
	TIME [epoch: 6.26 sec]
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.732096764263984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.732096764263984 | validation: 10.81490065436732]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.382263205827607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.382263205827607 | validation: 10.962287476506908]
	TIME [epoch: 6.3 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.812344412802382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.812344412802382 | validation: 10.940183070695198]
	TIME [epoch: 6.28 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.474082855070238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.474082855070238 | validation: 10.62506020767005]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.16806733711538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.16806733711538 | validation: 10.667620940596876]
	TIME [epoch: 6.26 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.048952082263174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.048952082263174 | validation: 10.673811566540707]
	TIME [epoch: 6.26 sec]
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.01403691441312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.01403691441312 | validation: 10.66423824567531]
	TIME [epoch: 6.26 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.866340704650634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.866340704650634 | validation: 10.533172930744723]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.767276171262846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.767276171262846 | validation: 10.467528311356974]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.654638827861707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.654638827861707 | validation: 10.377010697948474]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.567839572742491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.567839572742491 | validation: 10.291581816967621]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.297425952942085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.297425952942085 | validation: 10.148216411020229]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.047568005205983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.047568005205983 | validation: 9.903286532476695]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.915789137722271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.915789137722271 | validation: 10.087187548499704]
	TIME [epoch: 6.3 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.738198090489536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.738198090489536 | validation: 9.787190314567884]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.440097719225088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.440097719225088 | validation: 9.504131161955854]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.197570500528688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.197570500528688 | validation: 9.366502948137953]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.883703953307487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.883703953307487 | validation: 9.26100914533325]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.053708523880672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.053708523880672 | validation: 9.206833223698151]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.7319827285906575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.7319827285906575 | validation: 9.080373599087721]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.447374443810172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.447374443810172 | validation: 9.012439068549984]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.63090212454667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.63090212454667 | validation: 9.32472453527659]
	TIME [epoch: 6.26 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.703745059237015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.703745059237015 | validation: 8.921264331070788]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.445670824632809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.445670824632809 | validation: 8.814072753832715]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.417197288335686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.417197288335686 | validation: 8.662517661819416]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.318780598022622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.318780598022622 | validation: 8.5010615584909]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.109307545230967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.109307545230967 | validation: 8.833223896695795]
	TIME [epoch: 6.26 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.187289592627955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.187289592627955 | validation: 8.24663073077867]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.62021178678402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.62021178678402 | validation: 8.791212239217778]
	TIME [epoch: 6.26 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.861007552939476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.861007552939476 | validation: 8.88719205953737]
	TIME [epoch: 6.28 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.5042786679126126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.5042786679126126 | validation: 8.350942343514486]
	TIME [epoch: 6.28 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.181833836715119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.181833836715119 | validation: 8.281660580995645]
	TIME [epoch: 6.26 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.062522782836409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.062522782836409 | validation: 8.097587006057928]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.047976573424808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.047976573424808 | validation: 7.905750590392001]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.99046326892616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.99046326892616 | validation: 7.484170197002843]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.330924085895937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.330924085895937 | validation: 7.441129260816253]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.28492981106223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.28492981106223 | validation: 7.650528765066403]
	TIME [epoch: 6.26 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.8004194552625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.8004194552625 | validation: 8.114481281079488]
	TIME [epoch: 6.26 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.748228735109192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.748228735109192 | validation: 8.021691690351776]
	TIME [epoch: 6.26 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.282807570480579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.282807570480579 | validation: 8.274063797592436]
	TIME [epoch: 6.25 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.0627897732887845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0627897732887845 | validation: 7.797954929071048]
	TIME [epoch: 6.26 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.924057813566241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.924057813566241 | validation: 7.7557368545364085]
	TIME [epoch: 6.3 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.915974790064469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.915974790064469 | validation: 7.967332412087901]
	TIME [epoch: 6.26 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.836757129907188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.836757129907188 | validation: 7.647065549188015]
	TIME [epoch: 6.26 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.539759963430721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.539759963430721 | validation: 8.948627041964805]
	TIME [epoch: 6.25 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.423849529833646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.423849529833646 | validation: 8.560882381570732]
	TIME [epoch: 6.26 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.478820343565616		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 8.478820343565616 | validation: 8.562115802486739]
	TIME [epoch: 6.25 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.048857597367904		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 8.048857597367904 | validation: 7.997626788236282]
	TIME [epoch: 6.3 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.183441629191878		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 7.183441629191878 | validation: 7.8802161767439625]
	TIME [epoch: 6.26 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.104877147879357		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 7.104877147879357 | validation: 7.7436937401886965]
	TIME [epoch: 6.25 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.036968825789799		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 7.036968825789799 | validation: 7.5831531553634175]
	TIME [epoch: 6.26 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.935174936523504		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 6.935174936523504 | validation: 7.488295941451678]
	TIME [epoch: 6.25 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.891682979818203		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 6.891682979818203 | validation: 7.501192898577405]
	TIME [epoch: 6.25 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.830405849803359		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 6.830405849803359 | validation: 7.288609199349766]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_58.pth
	Model improved!!!
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.767840292288732		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 6.767840292288732 | validation: 7.23783565426154]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.844758114551743		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 6.844758114551743 | validation: 7.24763851842984]
	TIME [epoch: 6.26 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.884476816510304		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 6.884476816510304 | validation: 7.263062137081185]
	TIME [epoch: 6.26 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.775120552789985		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 6.775120552789985 | validation: 7.46062212538021]
	TIME [epoch: 6.26 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.6744110080787795		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 6.6744110080787795 | validation: 7.125602673669558]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_63.pth
	Model improved!!!
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.699713114868306		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 6.699713114868306 | validation: 7.533091914698153]
	TIME [epoch: 6.31 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.79914244387666		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 6.79914244387666 | validation: 7.476999534140864]
	TIME [epoch: 6.26 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.767797384976803		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 6.767797384976803 | validation: 7.454583328218906]
	TIME [epoch: 6.26 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.76003592960654		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 6.76003592960654 | validation: 7.3050665904477]
	TIME [epoch: 6.25 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.972512738332557		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 6.972512738332557 | validation: 7.815734215193348]
	TIME [epoch: 6.26 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.652545954465559		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 7.652545954465559 | validation: 8.082543236356049]
	TIME [epoch: 6.26 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.779454096852397		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 7.779454096852397 | validation: 7.772777750309695]
	TIME [epoch: 6.29 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.517490877997034		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 7.517490877997034 | validation: 7.475103045564247]
	TIME [epoch: 6.26 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.024084892597246		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 7.024084892597246 | validation: 7.1673125257421395]
	TIME [epoch: 6.26 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.813786870764531		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 6.813786870764531 | validation: 7.061892740763674]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_73.pth
	Model improved!!!
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.711183153778037		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 6.711183153778037 | validation: 7.12346017982363]
	TIME [epoch: 6.25 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.69144968194461		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 6.69144968194461 | validation: 7.126635695217435]
	TIME [epoch: 6.27 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.5622038569463506		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 6.5622038569463506 | validation: 6.77617277253716]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_76.pth
	Model improved!!!
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.728165926957047		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 6.728165926957047 | validation: 6.594590053192107]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_77.pth
	Model improved!!!
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.629476234724754		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 6.629476234724754 | validation: 6.748663642058951]
	TIME [epoch: 6.25 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.545186842012091		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 6.545186842012091 | validation: 6.865317729755246]
	TIME [epoch: 6.26 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.498925407603959		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 6.498925407603959 | validation: 6.664925936639042]
	TIME [epoch: 6.25 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.7450670323145125		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 6.7450670323145125 | validation: 7.281036015907123]
	TIME [epoch: 6.28 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.833039770808661		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 6.833039770808661 | validation: 6.800260167056329]
	TIME [epoch: 6.28 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.49476494723499		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 6.49476494723499 | validation: 6.720362624811484]
	TIME [epoch: 6.26 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.524373129750071		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 7.524373129750071 | validation: 7.0211858375159535]
	TIME [epoch: 6.26 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.960386685336132		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 7.960386685336132 | validation: 6.784387801133413]
	TIME [epoch: 6.25 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.737948737925413		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 7.737948737925413 | validation: 6.800335405192881]
	TIME [epoch: 6.25 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.356217654998719		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 7.356217654998719 | validation: 6.958216910198249]
	TIME [epoch: 6.28 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.976765345166443		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 6.976765345166443 | validation: 7.170631992748062]
	TIME [epoch: 6.28 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.740623327388008		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 6.740623327388008 | validation: 7.038650648167263]
	TIME [epoch: 6.26 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.706683247565792		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 6.706683247565792 | validation: 6.925702513284547]
	TIME [epoch: 6.26 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.621104760143036		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 6.621104760143036 | validation: 6.869525266183016]
	TIME [epoch: 6.26 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.529973698915777		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 6.529973698915777 | validation: 6.769649139661795]
	TIME [epoch: 6.25 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.485435686601585		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 6.485435686601585 | validation: 6.751588418423437]
	TIME [epoch: 6.29 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.371976352364534		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 6.371976352364534 | validation: 6.457113228167764]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_94.pth
	Model improved!!!
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.347120896899476		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 6.347120896899476 | validation: 6.693943120367829]
	TIME [epoch: 6.25 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.285544355249242		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 6.285544355249242 | validation: 6.424153909852313]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_96.pth
	Model improved!!!
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.63263307885253		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 6.63263307885253 | validation: 7.382300445337879]
	TIME [epoch: 6.25 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.77337359454865		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 6.77337359454865 | validation: 7.250646612083056]
	TIME [epoch: 6.25 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.678917816797419		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 6.678917816797419 | validation: 7.1814149729067545]
	TIME [epoch: 6.3 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.629549121535445		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 6.629549121535445 | validation: 6.781891037158854]
	TIME [epoch: 6.26 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.394903672969526		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 6.394903672969526 | validation: 6.650758384530224]
	TIME [epoch: 6.25 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.322644672693671		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 6.322644672693671 | validation: 6.637612365415986]
	TIME [epoch: 6.25 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.309992708915786		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 6.309992708915786 | validation: 6.636980218484772]
	TIME [epoch: 6.26 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.368613658646106		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 6.368613658646106 | validation: 6.5110985220228255]
	TIME [epoch: 6.25 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.567388857069033		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 6.567388857069033 | validation: 7.4019923252258355]
	TIME [epoch: 6.29 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.769397772391786		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 6.769397772391786 | validation: 6.853689152447172]
	TIME [epoch: 6.26 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.466397112305275		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 6.466397112305275 | validation: 6.86039254931725]
	TIME [epoch: 6.25 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.188716918248728		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 6.188716918248728 | validation: 6.299241320906537]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_108.pth
	Model improved!!!
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.4006760631522095		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 6.4006760631522095 | validation: 7.220671073368355]
	TIME [epoch: 6.37 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.975414792911534		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 6.975414792911534 | validation: 6.838523251320195]
	TIME [epoch: 6.26 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.720421995865401		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 6.720421995865401 | validation: 6.7648360440272555]
	TIME [epoch: 6.31 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.517224696755014		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 6.517224696755014 | validation: 6.551872984793386]
	TIME [epoch: 6.26 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.471939667300659		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 6.471939667300659 | validation: 6.439910612179851]
	TIME [epoch: 6.25 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.152664323018407		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 6.152664323018407 | validation: 6.326276211803581]
	TIME [epoch: 6.25 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.570674407434327		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 6.570674407434327 | validation: 6.565817772538745]
	TIME [epoch: 6.25 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.4624165705293395		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 6.4624165705293395 | validation: 6.599465793038757]
	TIME [epoch: 6.25 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.437568253343002		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 6.437568253343002 | validation: 6.677669197019777]
	TIME [epoch: 6.31 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.471712461309804		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 6.471712461309804 | validation: 6.6305236685800075]
	TIME [epoch: 6.26 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.948187771080169		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 6.948187771080169 | validation: 7.131570960498717]
	TIME [epoch: 6.25 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.669381158234849		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 7.669381158234849 | validation: 6.552023369780434]
	TIME [epoch: 6.25 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.468597645479993		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 7.468597645479993 | validation: 6.623625573407775]
	TIME [epoch: 6.26 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.285077288300066		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 7.285077288300066 | validation: 6.60987920136586]
	TIME [epoch: 6.26 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.212625550263008		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 7.212625550263008 | validation: 6.493585771340605]
	TIME [epoch: 6.3 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.970922714630264		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 6.970922714630264 | validation: 6.300682391426122]
	TIME [epoch: 6.27 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.311211963488307		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 6.311211963488307 | validation: 6.9229239919193475]
	TIME [epoch: 6.26 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.413445796062423		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 6.413445796062423 | validation: 6.687558569979192]
	TIME [epoch: 6.26 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2200018816368585		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 6.2200018816368585 | validation: 6.5080990288963]
	TIME [epoch: 6.26 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.687104070536248		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 6.687104070536248 | validation: 6.482423314303727]
	TIME [epoch: 6.25 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.733103615922633		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 6.733103615922633 | validation: 6.481278907688537]
	TIME [epoch: 6.3 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.613318191568016		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 6.613318191568016 | validation: 6.881997908689188]
	TIME [epoch: 6.26 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.706541996170055		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 6.706541996170055 | validation: 6.696604621001943]
	TIME [epoch: 6.26 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.262268565946951		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 6.262268565946951 | validation: 6.341044338066696]
	TIME [epoch: 6.25 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.27394106272842		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 6.27394106272842 | validation: 6.4838306110367805]
	TIME [epoch: 6.25 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.433938309626699		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 6.433938309626699 | validation: 6.5208969136910415]
	TIME [epoch: 6.25 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.5102011971589615		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 6.5102011971589615 | validation: 6.314919297464138]
	TIME [epoch: 6.31 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2884464538289615		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 6.2884464538289615 | validation: 6.42926754047807]
	TIME [epoch: 6.25 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.23720701384713		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 6.23720701384713 | validation: 6.201919321020718]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_137.pth
	Model improved!!!
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.246848004907394		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 6.246848004907394 | validation: 6.385836166081148]
	TIME [epoch: 6.25 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.189499029968898		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 6.189499029968898 | validation: 6.344038827655941]
	TIME [epoch: 6.26 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.872829574114164		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 6.872829574114164 | validation: 6.197392204864486]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_140.pth
	Model improved!!!
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.804275746084191		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 6.804275746084191 | validation: 6.23262045054749]
	TIME [epoch: 6.3 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.33733454448412		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 6.33733454448412 | validation: 6.233798413245449]
	TIME [epoch: 6.26 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.141866219533783		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 6.141866219533783 | validation: 6.2483218978228106]
	TIME [epoch: 6.26 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.097780438039924		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 6.097780438039924 | validation: 6.195774764362332]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_144.pth
	Model improved!!!
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.074912664628387		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 6.074912664628387 | validation: 6.2544047060698675]
	TIME [epoch: 6.26 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.387357529420752		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 6.387357529420752 | validation: 7.024938701675865]
	TIME [epoch: 6.28 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.56010630089376		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 6.56010630089376 | validation: 6.411590883400724]
	TIME [epoch: 6.29 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.172458889340313		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 6.172458889340313 | validation: 6.1728131622269835]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_148.pth
	Model improved!!!
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.126135986593463		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 6.126135986593463 | validation: 6.267225524430671]
	TIME [epoch: 6.26 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.038201207296703		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 7.038201207296703 | validation: 6.350571228634056]
	TIME [epoch: 6.25 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.845098802677113		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 6.845098802677113 | validation: 6.284054367144982]
	TIME [epoch: 6.25 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.904667850789931		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 6.904667850789931 | validation: 6.525779379584877]
	TIME [epoch: 6.27 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.2703737331902305		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 7.2703737331902305 | validation: 6.716789305176673]
	TIME [epoch: 6.29 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.500798279347945		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 7.500798279347945 | validation: 7.010661428975567]
	TIME [epoch: 6.26 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.476457577432808		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 7.476457577432808 | validation: 6.2115958823974164]
	TIME [epoch: 6.25 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.753718199298938		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 6.753718199298938 | validation: 6.322700192452077]
	TIME [epoch: 6.25 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.683158018534571		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 6.683158018534571 | validation: 6.228401602997377]
	TIME [epoch: 6.25 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.594820243869298		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 6.594820243869298 | validation: 6.248313811868995]
	TIME [epoch: 6.28 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0929694867050115		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 6.0929694867050115 | validation: 6.078129606700612]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_159.pth
	Model improved!!!
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.986526062753572		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 5.986526062753572 | validation: 5.962121238701605]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_160.pth
	Model improved!!!
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.011884070799269		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 6.011884070799269 | validation: 6.231165547958672]
	TIME [epoch: 6.26 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.954148416679549		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 5.954148416679549 | validation: 6.125050353735144]
	TIME [epoch: 6.26 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.198154356810951		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 6.198154356810951 | validation: 6.261622672135493]
	TIME [epoch: 6.25 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.474306752750786		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 6.474306752750786 | validation: 6.045393219456526]
	TIME [epoch: 6.3 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.421381082918641		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 6.421381082918641 | validation: 6.206649521831967]
	TIME [epoch: 6.26 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.4177569131784065		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 6.4177569131784065 | validation: 6.117168692777112]
	TIME [epoch: 6.26 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2574769975451385		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 6.2574769975451385 | validation: 6.194520828985254]
	TIME [epoch: 6.25 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.007204728455986		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 6.007204728455986 | validation: 6.073880711281751]
	TIME [epoch: 6.26 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.988668553850126		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 5.988668553850126 | validation: 6.275022121309527]
	TIME [epoch: 6.25 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.88179085678547		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 5.88179085678547 | validation: 6.05368139060767]
	TIME [epoch: 6.3 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.102886708499819		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 6.102886708499819 | validation: 6.058623044153487]
	TIME [epoch: 6.26 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.968113849924549		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 5.968113849924549 | validation: 6.272343966944277]
	TIME [epoch: 6.26 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0771187603552335		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 6.0771187603552335 | validation: 6.276108212317671]
	TIME [epoch: 6.26 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.381376071783688		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 6.381376071783688 | validation: 6.682451246814125]
	TIME [epoch: 6.26 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.675154681742474		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 6.675154681742474 | validation: 6.209237711803354]
	TIME [epoch: 6.26 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.536440138088392		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 6.536440138088392 | validation: 6.017231413895546]
	TIME [epoch: 6.3 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.456835883313782		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 6.456835883313782 | validation: 6.037983948603792]
	TIME [epoch: 6.26 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.484833670145216		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 6.484833670145216 | validation: 6.124715466731129]
	TIME [epoch: 6.26 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.179436040479034		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 6.179436040479034 | validation: 6.168411019557368]
	TIME [epoch: 6.24 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.083383226293034		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 6.083383226293034 | validation: 6.10991375815073]
	TIME [epoch: 6.26 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.195487274272516		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 6.195487274272516 | validation: 6.0678453772757255]
	TIME [epoch: 6.25 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0264641935908605		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 6.0264641935908605 | validation: 5.899857579524589]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_182.pth
	Model improved!!!
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.847051676670687		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 5.847051676670687 | validation: 5.906842917840914]
	TIME [epoch: 6.25 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.107958493103547		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 6.107958493103547 | validation: 6.128622960932461]
	TIME [epoch: 6.25 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.701605322602423		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 6.701605322602423 | validation: 6.104598014879715]
	TIME [epoch: 6.24 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.307660076102019		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 6.307660076102019 | validation: 5.9321837761037495]
	TIME [epoch: 6.24 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.4702288385984374		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 6.4702288385984374 | validation: 6.077080416086924]
	TIME [epoch: 6.24 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.6556973456674555		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 6.6556973456674555 | validation: 6.126804697347188]
	TIME [epoch: 6.3 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.573670487305187		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 6.573670487305187 | validation: 6.211767295261367]
	TIME [epoch: 6.25 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.27669094493465		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 6.27669094493465 | validation: 5.904770478196002]
	TIME [epoch: 6.25 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.038754515265888		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 6.038754515265888 | validation: 5.997290609190507]
	TIME [epoch: 6.25 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.126268448466228		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 6.126268448466228 | validation: 5.993870123733336]
	TIME [epoch: 6.25 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0631319943995		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 6.0631319943995 | validation: 6.080894842946389]
	TIME [epoch: 6.25 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.4550612754101095		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 6.4550612754101095 | validation: 6.67954559449087]
	TIME [epoch: 6.29 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.537653019725853		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 6.537653019725853 | validation: 6.238619932914787]
	TIME [epoch: 6.26 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.032125110943852		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 6.032125110943852 | validation: 6.2018522398368265]
	TIME [epoch: 6.25 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.875439367129623		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 5.875439367129623 | validation: 6.150072917390158]
	TIME [epoch: 6.25 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.818612179241016		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 5.818612179241016 | validation: 6.051884437829938]
	TIME [epoch: 6.25 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.859654006196558		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 5.859654006196558 | validation: 6.120574415473889]
	TIME [epoch: 6.24 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.775749996001451		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 5.775749996001451 | validation: 5.880209426408392]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_200.pth
	Model improved!!!
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.886800974000348		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 5.886800974000348 | validation: 6.030753398195953]
	TIME [epoch: 6.25 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.172444451667588		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 6.172444451667588 | validation: 6.057902618393047]
	TIME [epoch: 6.25 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.9533072450848366		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 5.9533072450848366 | validation: 5.94429739651012]
	TIME [epoch: 6.25 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.781358049012121		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 5.781358049012121 | validation: 5.942452866775126]
	TIME [epoch: 6.25 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.753022662488327		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 5.753022662488327 | validation: 6.289131316267625]
	TIME [epoch: 6.25 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.950886158299267		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 5.950886158299267 | validation: 5.958140354973219]
	TIME [epoch: 6.3 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.733127803287637		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 5.733127803287637 | validation: 5.995709293026038]
	TIME [epoch: 6.25 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.5882931935999185		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 5.5882931935999185 | validation: 5.878728397460205]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_208.pth
	Model improved!!!
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.5697154454963265		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 5.5697154454963265 | validation: 5.678290807566005]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_209.pth
	Model improved!!!
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.645837382274777		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 5.645837382274777 | validation: 5.883349453332164]
	TIME [epoch: 6.26 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.70097692708738		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 5.70097692708738 | validation: 6.135271754390941]
	TIME [epoch: 6.27 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.775158494205487		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 5.775158494205487 | validation: 6.418632627134013]
	TIME [epoch: 6.29 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.817563519705715		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 5.817563519705715 | validation: 5.83746525127707]
	TIME [epoch: 6.25 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.518855681480215		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 5.518855681480215 | validation: 5.621954382171905]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_214.pth
	Model improved!!!
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.537438955357027		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 5.537438955357027 | validation: 6.243627168021387]
	TIME [epoch: 6.25 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.685253001415046		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 5.685253001415046 | validation: 5.630085608575711]
	TIME [epoch: 6.26 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.466137781878187		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 5.466137781878187 | validation: 5.713785591878547]
	TIME [epoch: 6.27 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.405242203765922		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 5.405242203765922 | validation: 5.633661781176233]
	TIME [epoch: 6.29 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.427751273890603		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 5.427751273890603 | validation: 5.91305150273805]
	TIME [epoch: 6.25 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.461997695741176		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 5.461997695741176 | validation: 5.62200700308613]
	TIME [epoch: 6.25 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.472838488642907		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 5.472838488642907 | validation: 5.99246387611779]
	TIME [epoch: 6.26 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.645452948096205		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 5.645452948096205 | validation: 5.8801423912449415]
	TIME [epoch: 6.25 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.653856505368651		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 5.653856505368651 | validation: 5.904314459844155]
	TIME [epoch: 6.27 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.442353989228386		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 5.442353989228386 | validation: 5.8786179289879446]
	TIME [epoch: 6.29 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.466992151084026		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 5.466992151084026 | validation: 5.322830134616472]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_225.pth
	Model improved!!!
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.27470298299081		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 5.27470298299081 | validation: 5.534457870965634]
	TIME [epoch: 6.25 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.464205555692367		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 5.464205555692367 | validation: 6.228809555115387]
	TIME [epoch: 6.25 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.789722198004116		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 5.789722198004116 | validation: 6.326370005872958]
	TIME [epoch: 6.24 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.1743972516209995		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 6.1743972516209995 | validation: 6.920679591423703]
	TIME [epoch: 6.28 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.475883828638938		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 6.475883828638938 | validation: 6.8676877872562105]
	TIME [epoch: 6.27 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.37036594794391		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 6.37036594794391 | validation: 6.830805703156175]
	TIME [epoch: 6.26 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.538605356640369		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 6.538605356640369 | validation: 6.755757754102335]
	TIME [epoch: 6.25 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.783539521241428		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 6.783539521241428 | validation: 6.87531464346002]
	TIME [epoch: 6.25 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.112126252849207		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 7.112126252849207 | validation: 6.929134147371975]
	TIME [epoch: 6.25 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.2040371248935084		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 7.2040371248935084 | validation: 6.957282848025887]
	TIME [epoch: 6.28 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.423150069349956		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 7.423150069349956 | validation: 7.083457220574287]
	TIME [epoch: 6.27 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.462925434530057		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 7.462925434530057 | validation: 7.016415957340037]
	TIME [epoch: 6.26 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.861258414496602		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 6.861258414496602 | validation: 6.8046203681971]
	TIME [epoch: 6.24 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.218996659730577		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 6.218996659730577 | validation: 6.6985397093112855]
	TIME [epoch: 6.25 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.096482580462987		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 6.096482580462987 | validation: 6.794118773612657]
	TIME [epoch: 6.25 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.1239964730977885		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 6.1239964730977885 | validation: 6.571965688520082]
	TIME [epoch: 6.28 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.099376785628749		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 6.099376785628749 | validation: 6.573779425224481]
	TIME [epoch: 6.26 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.100417111403158		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 6.100417111403158 | validation: 6.386964783786472]
	TIME [epoch: 6.26 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.828685880673561		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 5.828685880673561 | validation: 6.200742985277863]
	TIME [epoch: 6.25 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.664788922630116		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 5.664788922630116 | validation: 5.483288797142292]
	TIME [epoch: 6.26 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.393219123063172		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 5.393219123063172 | validation: 5.750225241602845]
	TIME [epoch: 6.25 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.602364247284283		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 5.602364247284283 | validation: 6.221370661657058]
	TIME [epoch: 6.28 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.487847204900531		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 6.487847204900531 | validation: 6.625949276172859]
	TIME [epoch: 6.28 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.339569117651438		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 6.339569117651438 | validation: 6.063323931169141]
	TIME [epoch: 6.25 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.139378357003645		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 6.139378357003645 | validation: 6.077141917769271]
	TIME [epoch: 6.25 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.403200729418019		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 6.403200729418019 | validation: 6.069344187071659]
	TIME [epoch: 6.25 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.40184581665252		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 6.40184581665252 | validation: 5.943665357439562]
	TIME [epoch: 6.25 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.373590124039076		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 6.373590124039076 | validation: 5.962379380922197]
	TIME [epoch: 6.28 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.1251830373549305		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 6.1251830373549305 | validation: 5.947654502030873]
	TIME [epoch: 6.28 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.111761033052913		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 6.111761033052913 | validation: 6.178475923061832]
	TIME [epoch: 6.25 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.210902796597713		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 6.210902796597713 | validation: 5.886681437814611]
	TIME [epoch: 6.25 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.877063422726742		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 5.877063422726742 | validation: 5.815139775201694]
	TIME [epoch: 6.25 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.601183657469748		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 5.601183657469748 | validation: 5.820037974695593]
	TIME [epoch: 6.25 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.641687862891013		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 5.641687862891013 | validation: 5.831763101559382]
	TIME [epoch: 6.28 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.39568669754809		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 5.39568669754809 | validation: 5.81989892605507]
	TIME [epoch: 6.27 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.352225011152031		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 5.352225011152031 | validation: 5.778896432609901]
	TIME [epoch: 6.25 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.581021102786693		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 5.581021102786693 | validation: 5.800181003662139]
	TIME [epoch: 6.25 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.34407275636615		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 5.34407275636615 | validation: 5.262869211633479]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_263.pth
	Model improved!!!
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2101618492086095		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 5.2101618492086095 | validation: 5.21695831535912]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_264.pth
	Model improved!!!
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.15361873408038		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 5.15361873408038 | validation: 5.234592380204054]
	TIME [epoch: 6.29 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.179844979471509		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 5.179844979471509 | validation: 5.543999832822594]
	TIME [epoch: 6.26 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.250663335641086		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 5.250663335641086 | validation: 5.341472587359904]
	TIME [epoch: 6.25 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.210847191814693		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 5.210847191814693 | validation: 5.432314853609807]
	TIME [epoch: 6.25 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.16839292117197		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 5.16839292117197 | validation: 5.551118691594629]
	TIME [epoch: 6.25 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2717378999986995		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 5.2717378999986995 | validation: 5.758142162177173]
	TIME [epoch: 6.25 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.3081344350490465		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 5.3081344350490465 | validation: 5.295168900685431]
	TIME [epoch: 6.3 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.3484785275005		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 5.3484785275005 | validation: 5.385765893599061]
	TIME [epoch: 6.26 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2261152895933325		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 5.2261152895933325 | validation: 5.5970601354492455]
	TIME [epoch: 6.25 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.352154101142392		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 5.352154101142392 | validation: 5.472684088639996]
	TIME [epoch: 6.25 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.148798748166577		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 5.148798748166577 | validation: 5.24351329386147]
	TIME [epoch: 6.24 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.12185108822667		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 5.12185108822667 | validation: 5.906973780953939]
	TIME [epoch: 6.25 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.499603441590402		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 5.499603441590402 | validation: 6.439010733107125]
	TIME [epoch: 6.29 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.734524759882609		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 5.734524759882609 | validation: 6.715769862170246]
	TIME [epoch: 6.26 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.906101371825631		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 5.906101371825631 | validation: 6.002143897337355]
	TIME [epoch: 6.25 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.217693486871336		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 5.217693486871336 | validation: 5.502173302240267]
	TIME [epoch: 6.25 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.270255331512895		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 5.270255331512895 | validation: 5.2880757296526735]
	TIME [epoch: 6.26 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.058273908305498		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 5.058273908305498 | validation: 5.429418082176762]
	TIME [epoch: 6.25 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.353700083801467		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 5.353700083801467 | validation: 5.734709052319862]
	TIME [epoch: 6.3 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.298716543957739		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 5.298716543957739 | validation: 5.697898388190723]
	TIME [epoch: 6.26 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.4113557454254		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 5.4113557454254 | validation: 5.969455527550031]
	TIME [epoch: 6.25 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.4896771550372865		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 5.4896771550372865 | validation: 6.213113712351586]
	TIME [epoch: 6.25 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.5683057351931495		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 5.5683057351931495 | validation: 5.883470022746536]
	TIME [epoch: 6.25 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.524697202495221		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 5.524697202495221 | validation: 5.977375860376727]
	TIME [epoch: 6.24 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.389193921947198		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 5.389193921947198 | validation: 6.191694613042725]
	TIME [epoch: 6.3 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.659842838236323		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 5.659842838236323 | validation: 5.856831313641299]
	TIME [epoch: 6.26 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.252122159889713		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 5.252122159889713 | validation: 5.270682128115702]
	TIME [epoch: 6.25 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.014629077272636		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 5.014629077272636 | validation: 5.31282126828534]
	TIME [epoch: 6.25 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0704997322707985		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 5.0704997322707985 | validation: 5.181059974693318]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_293.pth
	Model improved!!!
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.926396924564795		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 4.926396924564795 | validation: 5.027583974181764]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_294.pth
	Model improved!!!
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.983828696504053		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 4.983828696504053 | validation: 5.2815062091187075]
	TIME [epoch: 6.31 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.912573445377781		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 4.912573445377781 | validation: 5.03711895266669]
	TIME [epoch: 6.24 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0058448489408205		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 5.0058448489408205 | validation: 4.949522052332483]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_297.pth
	Model improved!!!
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0524417210478605		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 5.0524417210478605 | validation: 5.989934804567863]
	TIME [epoch: 6.25 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.400547064116264		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 5.400547064116264 | validation: 5.9798037111906766]
	TIME [epoch: 6.25 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2211578424441525		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 5.2211578424441525 | validation: 5.23849448868381]
	TIME [epoch: 6.26 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0174274008522906		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 5.0174274008522906 | validation: 5.175977135371678]
	TIME [epoch: 6.29 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.999544186314526		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 4.999544186314526 | validation: 5.49397255104026]
	TIME [epoch: 6.25 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.194861758071812		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 5.194861758071812 | validation: 5.400714738706318]
	TIME [epoch: 6.25 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.933226800130366		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 4.933226800130366 | validation: 4.896765011337989]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_304.pth
	Model improved!!!
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.928662376333252		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 4.928662376333252 | validation: 4.9007550942848255]
	TIME [epoch: 6.24 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.786632595241127		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 4.786632595241127 | validation: 5.174244560755987]
	TIME [epoch: 6.26 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.046274593359466		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 5.046274593359466 | validation: 5.502739365303971]
	TIME [epoch: 6.27 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.303965789079761		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 5.303965789079761 | validation: 5.480474349633958]
	TIME [epoch: 6.25 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.630010552953946		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 5.630010552953946 | validation: 6.134318084251925]
	TIME [epoch: 6.24 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.799612523694658		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 5.799612523694658 | validation: 5.756640814130751]
	TIME [epoch: 6.24 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.257103052953418		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 5.257103052953418 | validation: 5.539676900187706]
	TIME [epoch: 6.24 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.148137686026589		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 5.148137686026589 | validation: 5.209266937088184]
	TIME [epoch: 6.25 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.915796050549011		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 4.915796050549011 | validation: 4.846411183154913]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_313.pth
	Model improved!!!
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.753560185705878		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 4.753560185705878 | validation: 4.7956640410777]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_314.pth
	Model improved!!!
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.843615373317931		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 4.843615373317931 | validation: 4.881630057456581]
	TIME [epoch: 6.24 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.978844867241287		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 4.978844867241287 | validation: 5.380662616674668]
	TIME [epoch: 6.24 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.975530994037555		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 4.975530994037555 | validation: 5.1831906439200495]
	TIME [epoch: 6.24 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.90592702742641		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 4.90592702742641 | validation: 4.703298728147034]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_318.pth
	Model improved!!!
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.714872353065238		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 4.714872353065238 | validation: 4.639048823561095]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_319.pth
	Model improved!!!
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.743203820600927		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 4.743203820600927 | validation: 4.645336066563207]
	TIME [epoch: 6.24 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7577980473797385		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 4.7577980473797385 | validation: 5.4228462304709595]
	TIME [epoch: 6.24 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.360846455846619		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 5.360846455846619 | validation: 5.315771958765437]
	TIME [epoch: 6.24 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.04437424217355		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 5.04437424217355 | validation: 5.529952260132307]
	TIME [epoch: 6.24 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.966095713776211		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 4.966095713776211 | validation: 4.872494727247]
	TIME [epoch: 6.29 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.708813243622481		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 4.708813243622481 | validation: 4.677764758298617]
	TIME [epoch: 6.25 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.655635503214078		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 4.655635503214078 | validation: 5.115730485818183]
	TIME [epoch: 6.24 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.15945315251844		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 5.15945315251844 | validation: 5.559265702542909]
	TIME [epoch: 6.24 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.873510985911232		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 4.873510985911232 | validation: 5.307159614937279]
	TIME [epoch: 6.24 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.070920136244046		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 5.070920136244046 | validation: 5.013864594271569]
	TIME [epoch: 6.24 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.074026938958864		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 5.074026938958864 | validation: 5.561345189067244]
	TIME [epoch: 6.28 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.118714126342793		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 5.118714126342793 | validation: 5.205338484717455]
	TIME [epoch: 6.25 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.234127450813157		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 5.234127450813157 | validation: 5.402267051384484]
	TIME [epoch: 6.24 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.62066346826996		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 5.62066346826996 | validation: 6.443793952240513]
	TIME [epoch: 6.24 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.042858074302281		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 6.042858074302281 | validation: 6.401572789406744]
	TIME [epoch: 6.24 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.899915852277922		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 5.899915852277922 | validation: 6.437480463793009]
	TIME [epoch: 6.24 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.6487910744944685		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 5.6487910744944685 | validation: 6.037506731742301]
	TIME [epoch: 6.28 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.422647491673539		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 5.422647491673539 | validation: 6.030552942683899]
	TIME [epoch: 7.58 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2674587575758425		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 5.2674587575758425 | validation: 5.056521670041385]
	TIME [epoch: 6.24 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.869173518726191		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 4.869173518726191 | validation: 5.30001160055755]
	TIME [epoch: 6.24 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.119322787917879		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 5.119322787917879 | validation: 5.417146951688123]
	TIME [epoch: 6.25 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.923516429151749		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 4.923516429151749 | validation: 4.8942731069164305]
	TIME [epoch: 6.24 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.791958224751963		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 4.791958224751963 | validation: 4.851733512283824]
	TIME [epoch: 6.28 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.675311985048067		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 4.675311985048067 | validation: 5.198584839663481]
	TIME [epoch: 6.25 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.064295585068086		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 5.064295585068086 | validation: 4.831905090452473]
	TIME [epoch: 6.24 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.760380375648367		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 4.760380375648367 | validation: 4.800609646859038]
	TIME [epoch: 6.24 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.722386790833032		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 4.722386790833032 | validation: 4.950957447246012]
	TIME [epoch: 6.24 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.735480844797159		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 4.735480844797159 | validation: 4.655649949950309]
	TIME [epoch: 6.25 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5833866604064974		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 4.5833866604064974 | validation: 4.526670528447756]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_348.pth
	Model improved!!!
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.627676830000583		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 4.627676830000583 | validation: 5.088129586520411]
	TIME [epoch: 6.25 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.763969300259551		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 4.763969300259551 | validation: 5.036447078678509]
	TIME [epoch: 6.25 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.629723743859809		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 4.629723743859809 | validation: 4.447210279543664]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_351.pth
	Model improved!!!
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.639303977267928		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 4.639303977267928 | validation: 4.774111924545005]
	TIME [epoch: 6.25 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.70482082350315		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 4.70482082350315 | validation: 4.78103649977731]
	TIME [epoch: 6.25 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6303642929110875		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 4.6303642929110875 | validation: 4.703067758276903]
	TIME [epoch: 6.3 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.780659212494646		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 4.780659212494646 | validation: 4.823725340859924]
	TIME [epoch: 6.25 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.645373038975711		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 4.645373038975711 | validation: 4.664799157687124]
	TIME [epoch: 6.25 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.593410616158991		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 4.593410616158991 | validation: 4.277142065834006]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_357.pth
	Model improved!!!
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5152297679193465		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 4.5152297679193465 | validation: 4.795604585400033]
	TIME [epoch: 6.25 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.482898551798422		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 4.482898551798422 | validation: 4.547392801451483]
	TIME [epoch: 6.25 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.650913222678455		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 4.650913222678455 | validation: 4.846382868212533]
	TIME [epoch: 6.3 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.589088711126953		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 4.589088711126953 | validation: 4.623657497268587]
	TIME [epoch: 6.25 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.491539552857569		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 4.491539552857569 | validation: 4.483648523737347]
	TIME [epoch: 6.25 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.402877132141973		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 4.402877132141973 | validation: 4.205796043921883]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_363.pth
	Model improved!!!
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.392186181827411		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 4.392186181827411 | validation: 4.224442705871472]
	TIME [epoch: 6.24 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.434811973672341		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 4.434811973672341 | validation: 4.743647714971605]
	TIME [epoch: 6.25 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.497978229247165		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 4.497978229247165 | validation: 4.4692087669352745]
	TIME [epoch: 6.29 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.38878483673814		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 4.38878483673814 | validation: 4.368630054454249]
	TIME [epoch: 6.24 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3644628746076535		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 4.3644628746076535 | validation: 4.430829166723301]
	TIME [epoch: 6.24 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.75451940831082		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 4.75451940831082 | validation: 5.080574127934941]
	TIME [epoch: 6.24 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.623107983619546		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 4.623107983619546 | validation: 4.552661094208537]
	TIME [epoch: 6.25 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.61001953421507		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 4.61001953421507 | validation: 4.84281528447212]
	TIME [epoch: 6.26 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.568105778104208		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 4.568105778104208 | validation: 4.482129008482207]
	TIME [epoch: 6.27 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.450814916797832		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 4.450814916797832 | validation: 4.696025334684698]
	TIME [epoch: 6.24 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.540133463776172		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 4.540133463776172 | validation: 4.85564609875793]
	TIME [epoch: 6.24 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.47451367320701		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 4.47451367320701 | validation: 4.314535555452088]
	TIME [epoch: 6.24 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.385840623326924		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 4.385840623326924 | validation: 4.708648796275902]
	TIME [epoch: 6.24 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.872885423378877		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 4.872885423378877 | validation: 5.898137368805207]
	TIME [epoch: 6.26 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.056610715785374		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 5.056610715785374 | validation: 5.296065519035702]
	TIME [epoch: 6.27 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.553570942031695		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 4.553570942031695 | validation: 4.587894145376969]
	TIME [epoch: 6.25 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.395376906250036		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 4.395376906250036 | validation: 4.608203284913568]
	TIME [epoch: 6.25 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.43375519617994		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 4.43375519617994 | validation: 4.539529629793385]
	TIME [epoch: 6.24 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.332801354434045		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 4.332801354434045 | validation: 4.414229202534724]
	TIME [epoch: 6.24 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.282472911383289		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 4.282472911383289 | validation: 4.321515280791257]
	TIME [epoch: 6.26 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.385046323063213		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 4.385046323063213 | validation: 4.854864213115794]
	TIME [epoch: 6.28 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.466957662829046		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 4.466957662829046 | validation: 4.434474959120039]
	TIME [epoch: 6.25 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.323613178861222		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 4.323613178861222 | validation: 4.272142780601788]
	TIME [epoch: 6.24 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.271301655877517		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 4.271301655877517 | validation: 4.3070423228100285]
	TIME [epoch: 6.24 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.316954700952825		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 4.316954700952825 | validation: 5.2070434570085675]
	TIME [epoch: 6.24 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.582865610178771		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 4.582865610178771 | validation: 5.065317293469359]
	TIME [epoch: 6.27 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.572838722745194		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 4.572838722745194 | validation: 4.989798710901827]
	TIME [epoch: 6.27 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.495166547708382		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 4.495166547708382 | validation: 4.936517266133926]
	TIME [epoch: 6.24 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.526630709566708		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 4.526630709566708 | validation: 4.335468946997008]
	TIME [epoch: 6.24 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.251966315238286		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 4.251966315238286 | validation: 4.505555055292232]
	TIME [epoch: 6.24 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.224278510275618		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 4.224278510275618 | validation: 4.2801708853772755]
	TIME [epoch: 6.24 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1796294401210385		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 4.1796294401210385 | validation: 4.744823076786208]
	TIME [epoch: 6.26 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.444521265056934		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 4.444521265056934 | validation: 4.632817746047818]
	TIME [epoch: 6.28 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2785926010423125		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 4.2785926010423125 | validation: 4.617996035151267]
	TIME [epoch: 6.24 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.407281764248434		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 4.407281764248434 | validation: 4.908363169758059]
	TIME [epoch: 6.24 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3212708933082435		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 4.3212708933082435 | validation: 4.50458901400939]
	TIME [epoch: 6.24 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.404959630425384		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 4.404959630425384 | validation: 4.580687390934669]
	TIME [epoch: 6.24 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.269516026580822		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 4.269516026580822 | validation: 4.4387075167234835]
	TIME [epoch: 6.26 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.255015912166857		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 4.255015912166857 | validation: 4.653970337695676]
	TIME [epoch: 6.28 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.316136209937555		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 4.316136209937555 | validation: 4.637070988760009]
	TIME [epoch: 6.24 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.306179408698009		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 4.306179408698009 | validation: 4.58404465725077]
	TIME [epoch: 6.25 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.342415996685162		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 4.342415996685162 | validation: 4.340083260440302]
	TIME [epoch: 6.24 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2791168834491184		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 4.2791168834491184 | validation: 4.868603998217578]
	TIME [epoch: 6.24 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.92497833762638		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 4.92497833762638 | validation: 5.8026696570761835]
	TIME [epoch: 6.25 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.011085517779097		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 5.011085517779097 | validation: 4.84680642650536]
	TIME [epoch: 6.28 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.350212760584475		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 4.350212760584475 | validation: 4.379903293000428]
	TIME [epoch: 6.25 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.269997470322047		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 4.269997470322047 | validation: 4.425053870503125]
	TIME [epoch: 6.24 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.248652635051699		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 4.248652635051699 | validation: 4.375047509724685]
	TIME [epoch: 6.24 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.337869794453624		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 4.337869794453624 | validation: 5.068004866502099]
	TIME [epoch: 6.24 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6228382172008455		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 4.6228382172008455 | validation: 4.845167886406028]
	TIME [epoch: 6.26 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.64447049169639		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 4.64447049169639 | validation: 5.6027507537333]
	TIME [epoch: 6.28 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.086114816060011		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 5.086114816060011 | validation: 5.5324248304962484]
	TIME [epoch: 6.25 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.678627927279497		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 4.678627927279497 | validation: 4.690208318333986]
	TIME [epoch: 6.25 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.33851420454612		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 4.33851420454612 | validation: 4.750572951001766]
	TIME [epoch: 6.24 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.312749075870626		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 4.312749075870626 | validation: 4.320328303386464]
	TIME [epoch: 6.25 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.174894519795027		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 4.174894519795027 | validation: 4.487242113958912]
	TIME [epoch: 6.26 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1572996317408295		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 4.1572996317408295 | validation: 4.274731575300161]
	TIME [epoch: 6.28 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.203453901390842		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 4.203453901390842 | validation: 4.236138232157666]
	TIME [epoch: 6.25 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.18053051424329		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 4.18053051424329 | validation: 4.886297454018283]
	TIME [epoch: 6.24 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.737408252941166		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 4.737408252941166 | validation: 5.845746121414613]
	TIME [epoch: 6.25 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.864086870555515		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 4.864086870555515 | validation: 5.727058574835675]
	TIME [epoch: 6.25 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.817605239414679		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 4.817605239414679 | validation: 5.049041443103299]
	TIME [epoch: 6.26 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4014345711352405		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 4.4014345711352405 | validation: 4.201349040604573]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_426.pth
	Model improved!!!
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.147777239737543		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 4.147777239737543 | validation: 4.111849033245225]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_427.pth
	Model improved!!!
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.071864680983124		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 4.071864680983124 | validation: 4.324560732672289]
	TIME [epoch: 6.25 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.270507886238256		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 4.270507886238256 | validation: 4.2530958201663704]
	TIME [epoch: 6.25 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.209261008034332		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 4.209261008034332 | validation: 4.054230338259833]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_430.pth
	Model improved!!!
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.061189187785569		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 4.061189187785569 | validation: 4.140378063311734]
	TIME [epoch: 6.3 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.224797894357756		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 4.224797894357756 | validation: 4.1773437871509485]
	TIME [epoch: 6.26 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.123563664133637		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 4.123563664133637 | validation: 4.226792445029687]
	TIME [epoch: 6.26 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.063669556269003		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 4.063669556269003 | validation: 4.060872723948508]
	TIME [epoch: 6.25 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.021444046327828		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 4.021444046327828 | validation: 4.282755842013253]
	TIME [epoch: 6.25 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0838727298569095		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 4.0838727298569095 | validation: 4.072813111109067]
	TIME [epoch: 6.25 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9955886224415433		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 3.9955886224415433 | validation: 3.9842971020622713]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_437.pth
	Model improved!!!
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.02782767282419		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 4.02782767282419 | validation: 4.117079890674237]
	TIME [epoch: 6.26 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.224054861463871		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 4.224054861463871 | validation: 5.287171113152535]
	TIME [epoch: 6.25 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.622981847503716		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 4.622981847503716 | validation: 5.377147169101978]
	TIME [epoch: 6.25 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.665088549921072		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 4.665088549921072 | validation: 4.53951293369849]
	TIME [epoch: 6.25 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.147450363608756		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 4.147450363608756 | validation: 4.163969734805322]
	TIME [epoch: 6.25 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.003873999871784		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 4.003873999871784 | validation: 4.085825140265326]
	TIME [epoch: 6.3 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0037191190800225		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 4.0037191190800225 | validation: 4.053644893761755]
	TIME [epoch: 6.26 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.039275461696286		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 4.039275461696286 | validation: 4.401049621984443]
	TIME [epoch: 6.25 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.144539072575592		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 4.144539072575592 | validation: 4.1587847378191825]
	TIME [epoch: 6.26 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.04482246734627		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 4.04482246734627 | validation: 4.526891913390031]
	TIME [epoch: 6.25 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.063551222597587		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 4.063551222597587 | validation: 4.747497661147774]
	TIME [epoch: 6.25 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.32698164573076		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 4.32698164573076 | validation: 4.641005535465309]
	TIME [epoch: 6.3 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.08386777117157		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 4.08386777117157 | validation: 4.130414780129508]
	TIME [epoch: 6.26 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9760546387524913		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 3.9760546387524913 | validation: 4.125696584403904]
	TIME [epoch: 6.26 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9735931130945317		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 3.9735931130945317 | validation: 4.104250962544118]
	TIME [epoch: 6.26 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9580749697180924		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 3.9580749697180924 | validation: 4.060794427115034]
	TIME [epoch: 6.25 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9594436699982247		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.9594436699982247 | validation: 4.199609620574662]
	TIME [epoch: 6.25 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9535938698515514		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 3.9535938698515514 | validation: 3.8970656455084054]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_455.pth
	Model improved!!!
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9577792000086105		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 3.9577792000086105 | validation: 4.406089864881083]
	TIME [epoch: 6.26 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0432673488025594		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 4.0432673488025594 | validation: 4.652768092384268]
	TIME [epoch: 6.26 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.111296619398209		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 4.111296619398209 | validation: 4.236263450283041]
	TIME [epoch: 6.25 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9584070444348702		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 3.9584070444348702 | validation: 4.31454924784535]
	TIME [epoch: 6.26 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0400252641619545		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 4.0400252641619545 | validation: 4.1145310485275735]
	TIME [epoch: 6.26 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.042755399638982		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 4.042755399638982 | validation: 4.357066053289838]
	TIME [epoch: 6.31 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.038954553661753		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 4.038954553661753 | validation: 4.395662208410428]
	TIME [epoch: 6.26 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.990354007458378		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 3.990354007458378 | validation: 4.653764877864415]
	TIME [epoch: 6.26 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2610410587128005		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 4.2610410587128005 | validation: 4.182056055502306]
	TIME [epoch: 6.25 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.044772260758978		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 4.044772260758978 | validation: 3.9672411355566046]
	TIME [epoch: 6.25 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9499686597876416		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 3.9499686597876416 | validation: 4.10142643931179]
	TIME [epoch: 6.25 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.921290584039521		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 3.921290584039521 | validation: 4.221936210116961]
	TIME [epoch: 6.3 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.214736561676508		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 4.214736561676508 | validation: 5.191007830028848]
	TIME [epoch: 6.26 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.34084642398591		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 4.34084642398591 | validation: 4.378837148263472]
	TIME [epoch: 6.25 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.043894439090488		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 4.043894439090488 | validation: 4.625430679851499]
	TIME [epoch: 6.25 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.054922617908985		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 4.054922617908985 | validation: 4.019136522829539]
	TIME [epoch: 6.25 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9466452135128334		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 3.9466452135128334 | validation: 3.927884720819752]
	TIME [epoch: 6.25 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.850688142427522		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 3.850688142427522 | validation: 3.9059414546359843]
	TIME [epoch: 6.3 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.899624967394727		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 3.899624967394727 | validation: 4.194887834023149]
	TIME [epoch: 6.26 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.97084822652773		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 3.97084822652773 | validation: 3.9198940756548284]
	TIME [epoch: 6.26 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9084493621147867		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 3.9084493621147867 | validation: 4.115969845678012]
	TIME [epoch: 6.26 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.03834440376817		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 4.03834440376817 | validation: 4.430950692893807]
	TIME [epoch: 6.25 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.971777317461607		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 3.971777317461607 | validation: 3.9175007169661233]
	TIME [epoch: 6.25 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.826927822807274		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 3.826927822807274 | validation: 4.29378058621694]
	TIME [epoch: 6.3 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9713050373109784		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 3.9713050373109784 | validation: 4.187847201128687]
	TIME [epoch: 6.26 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.915809525311979		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 3.915809525311979 | validation: 4.084809048721523]
	TIME [epoch: 6.25 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.856635346132091		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 3.856635346132091 | validation: 3.986556371084549]
	TIME [epoch: 6.26 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8613921520605508		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 3.8613921520605508 | validation: 4.017970191921872]
	TIME [epoch: 6.25 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8344117680896765		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 3.8344117680896765 | validation: 4.163069460673224]
	TIME [epoch: 6.25 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.868093710621511		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 3.868093710621511 | validation: 3.9536947029593454]
	TIME [epoch: 6.3 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8402896729263194		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 3.8402896729263194 | validation: 4.101822999931969]
	TIME [epoch: 6.26 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.891957603104196		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 3.891957603104196 | validation: 4.140754288061626]
	TIME [epoch: 6.25 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9523476048115125		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 3.9523476048115125 | validation: 4.398633772420016]
	TIME [epoch: 6.25 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.126750459932264		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 4.126750459932264 | validation: 4.567533865353624]
	TIME [epoch: 6.25 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.16784900854342		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 4.16784900854342 | validation: 4.1028754833934]
	TIME [epoch: 6.26 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.850363287187417		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 3.850363287187417 | validation: 4.245615680039676]
	TIME [epoch: 6.29 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8171698484434695		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 3.8171698484434695 | validation: 4.186202244555962]
	TIME [epoch: 6.26 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.958164876167979		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 3.958164876167979 | validation: 4.79627798385208]
	TIME [epoch: 6.26 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1505089101550485		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 4.1505089101550485 | validation: 4.401141954319447]
	TIME [epoch: 6.25 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.934327434534385		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 3.934327434534385 | validation: 4.3031925253149845]
	TIME [epoch: 6.25 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.903388726672459		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 3.903388726672459 | validation: 3.9849140465758737]
	TIME [epoch: 6.26 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7881512458117146		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 3.7881512458117146 | validation: 3.8789708056417793]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_497.pth
	Model improved!!!
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7937945838987592		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 3.7937945838987592 | validation: 3.942015661588208]
	TIME [epoch: 6.26 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7772954923740336		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 3.7772954923740336 | validation: 4.038232239905063]
	TIME [epoch: 6.25 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.937249916001439		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 3.937249916001439 | validation: 4.031166383156703]
	TIME [epoch: 6.25 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8874051176278552		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 3.8874051176278552 | validation: 3.928189477113458]
	TIME [epoch: 6.25 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8424862134811923		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 3.8424862134811923 | validation: 4.041762015615558]
	TIME [epoch: 6.27 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8494505121304448		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 3.8494505121304448 | validation: 4.106081396409371]
	TIME [epoch: 6.28 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7881036027579644		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 3.7881036027579644 | validation: 4.110042973676643]
	TIME [epoch: 6.25 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.877362565890302		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 3.877362565890302 | validation: 4.007050180167267]
	TIME [epoch: 6.25 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7779923294084807		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 3.7779923294084807 | validation: 4.031801799605871]
	TIME [epoch: 6.25 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7458355110167973		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 3.7458355110167973 | validation: 4.0063585365934165]
	TIME [epoch: 6.25 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.760285495148531		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 3.760285495148531 | validation: 3.921785363491844]
	TIME [epoch: 6.26 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7869215395459683		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 3.7869215395459683 | validation: 4.2035169204521745]
	TIME [epoch: 6.28 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.853634512412021		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 3.853634512412021 | validation: 4.199633116994761]
	TIME [epoch: 6.26 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.039581234434335		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 4.039581234434335 | validation: 4.780636339507715]
	TIME [epoch: 6.25 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.921798561112892		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 3.921798561112892 | validation: 3.900534146046847]
	TIME [epoch: 6.25 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.795404845792612		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 3.795404845792612 | validation: 3.9379953436170334]
	TIME [epoch: 6.25 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.769868362970161		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 3.769868362970161 | validation: 4.137370296124828]
	TIME [epoch: 6.27 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7379821571356993		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 3.7379821571356993 | validation: 4.123480707191153]
	TIME [epoch: 6.29 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.724090882518541		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 3.724090882518541 | validation: 4.528332076822471]
	TIME [epoch: 6.26 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.80550915322812		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 3.80550915322812 | validation: 4.34811808520341]
	TIME [epoch: 6.25 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.774360552336228		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 3.774360552336228 | validation: 3.905476692739977]
	TIME [epoch: 6.25 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7393099227425552		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 3.7393099227425552 | validation: 4.088845408114205]
	TIME [epoch: 6.25 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.810552980234419		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 3.810552980234419 | validation: 3.9271325054978883]
	TIME [epoch: 6.27 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.724232553338572		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 3.724232553338572 | validation: 3.9345055209405055]
	TIME [epoch: 6.29 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6743540973441546		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 3.6743540973441546 | validation: 3.8774749160294393]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_522.pth
	Model improved!!!
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7165585149055316		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 3.7165585149055316 | validation: 3.8602934775912088]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_523.pth
	Model improved!!!
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7131646958579223		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 3.7131646958579223 | validation: 3.9029485082225617]
	TIME [epoch: 6.25 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7144745805384414		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 3.7144745805384414 | validation: 3.9355148755158726]
	TIME [epoch: 6.25 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8038454630100387		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 3.8038454630100387 | validation: 4.170597331701284]
	TIME [epoch: 6.29 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7910709107107445		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 3.7910709107107445 | validation: 3.8266632444101054]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_527.pth
	Model improved!!!
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.79185992437734		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 3.79185992437734 | validation: 4.59940005587093]
	TIME [epoch: 6.25 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.891896592536995		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 3.891896592536995 | validation: 3.805323147465675]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_529.pth
	Model improved!!!
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7094502468350052		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 3.7094502468350052 | validation: 3.8903903492356253]
	TIME [epoch: 6.25 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7411181972433694		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 3.7411181972433694 | validation: 3.989541862068904]
	TIME [epoch: 6.25 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.724942079616561		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 3.724942079616561 | validation: 3.9402564268809073]
	TIME [epoch: 6.29 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8300744036079335		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 3.8300744036079335 | validation: 4.24184603651104]
	TIME [epoch: 6.26 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.824067229090163		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 3.824067229090163 | validation: 3.797820671631781]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_534.pth
	Model improved!!!
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7592679010529975		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 3.7592679010529975 | validation: 4.183596505086744]
	TIME [epoch: 6.24 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7977980391763912		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 3.7977980391763912 | validation: 3.9692505591299048]
	TIME [epoch: 6.25 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6972470479018775		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 3.6972470479018775 | validation: 3.9820538314400986]
	TIME [epoch: 6.25 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8273877593969243		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 3.8273877593969243 | validation: 4.100269013939262]
	TIME [epoch: 6.29 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7754851404200584		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 3.7754851404200584 | validation: 3.879033975818495]
	TIME [epoch: 6.25 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7122371336517435		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 3.7122371336517435 | validation: 3.9703623461154827]
	TIME [epoch: 6.25 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.834711944341959		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 3.834711944341959 | validation: 4.297155774155602]
	TIME [epoch: 6.25 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.085992785275426		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 4.085992785275426 | validation: 4.124002310652967]
	TIME [epoch: 6.24 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.750325326845712		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 3.750325326845712 | validation: 3.867065500044508]
	TIME [epoch: 6.25 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.771087492407702		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 3.771087492407702 | validation: 4.463632590830794]
	TIME [epoch: 6.31 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1331670733172166		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 4.1331670733172166 | validation: 5.157236759803755]
	TIME [epoch: 6.26 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.221383282480057		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 4.221383282480057 | validation: 4.649768454861757]
	TIME [epoch: 6.26 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.058710741909005		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 4.058710741909005 | validation: 4.527941891603806]
	TIME [epoch: 6.25 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.119878192574963		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 4.119878192574963 | validation: 4.406507188790255]
	TIME [epoch: 6.24 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9457691063659546		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 3.9457691063659546 | validation: 4.052701229383752]
	TIME [epoch: 6.25 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8040848922595276		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 3.8040848922595276 | validation: 4.034897690920033]
	TIME [epoch: 6.29 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.679314729945986		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 3.679314729945986 | validation: 3.86503858493934]
	TIME [epoch: 6.26 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6447843831396964		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 3.6447843831396964 | validation: 3.7899128956856716]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_552.pth
	Model improved!!!
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.656631383426915		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 3.656631383426915 | validation: 3.779029477296815]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_553.pth
	Model improved!!!
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.654979361027485		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 3.654979361027485 | validation: 3.981524965078228]
	TIME [epoch: 6.26 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6537279517319217		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 3.6537279517319217 | validation: 4.010195387549616]
	TIME [epoch: 6.27 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.652877841998885		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 3.652877841998885 | validation: 3.7695285097333286]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_556.pth
	Model improved!!!
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6713303464437206		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 3.6713303464437206 | validation: 4.019584738890481]
	TIME [epoch: 6.26 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6583480894184346		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 3.6583480894184346 | validation: 3.8753084731753633]
	TIME [epoch: 6.25 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.666720672248191		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 3.666720672248191 | validation: 3.7335413524460384]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_559.pth
	Model improved!!!
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6279726713454474		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 3.6279726713454474 | validation: 3.6912892148900758]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_560.pth
	Model improved!!!
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.621011531658184		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 3.621011531658184 | validation: 3.854537687320751]
	TIME [epoch: 6.28 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.728856480925591		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 3.728856480925591 | validation: 4.079448555705966]
	TIME [epoch: 6.28 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.718112577434972		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 3.718112577434972 | validation: 3.896667823705999]
	TIME [epoch: 6.25 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6636935632433705		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 3.6636935632433705 | validation: 3.958553112728351]
	TIME [epoch: 6.25 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.697114384212017		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 3.697114384212017 | validation: 4.003702896011005]
	TIME [epoch: 6.25 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.798684686638829		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 3.798684686638829 | validation: 4.204979662919948]
	TIME [epoch: 6.25 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8280566391758395		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 3.8280566391758395 | validation: 4.289699527284044]
	TIME [epoch: 6.28 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7810846333401997		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 3.7810846333401997 | validation: 3.9350122547045308]
	TIME [epoch: 6.26 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.633844194762771		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 3.633844194762771 | validation: 3.9175915097792484]
	TIME [epoch: 6.25 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.716725795426021		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 3.716725795426021 | validation: 3.718943669201754]
	TIME [epoch: 6.25 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6387045936599733		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 3.6387045936599733 | validation: 4.055143100505733]
	TIME [epoch: 6.25 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7388657197859465		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 3.7388657197859465 | validation: 3.9192034361357835]
	TIME [epoch: 6.25 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.702679069190446		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 3.702679069190446 | validation: 3.8642041061233527]
	TIME [epoch: 6.29 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6693216392842003		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 3.6693216392842003 | validation: 3.976089490997135]
	TIME [epoch: 6.27 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8705312137282544		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 3.8705312137282544 | validation: 4.454263373695831]
	TIME [epoch: 6.25 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8477430283195853		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 3.8477430283195853 | validation: 3.9046257586552384]
	TIME [epoch: 6.25 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.660879260484801		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 3.660879260484801 | validation: 3.9459072622794853]
	TIME [epoch: 6.25 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.801987302741251		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 3.801987302741251 | validation: 4.085124490215112]
	TIME [epoch: 6.25 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7109702050340965		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 3.7109702050340965 | validation: 4.091258223122359]
	TIME [epoch: 6.29 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.770404588083713		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 3.770404588083713 | validation: 4.030361860329293]
	TIME [epoch: 6.26 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7168680477299496		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 3.7168680477299496 | validation: 3.903264711937188]
	TIME [epoch: 6.25 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6372900497149403		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 3.6372900497149403 | validation: 3.7638149231061515]
	TIME [epoch: 6.25 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5831201470273575		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 3.5831201470273575 | validation: 3.6556740451174217]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_583.pth
	Model improved!!!
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.574130615883773		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 3.574130615883773 | validation: 3.7940260755714297]
	TIME [epoch: 6.25 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6628620788273842		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 3.6628620788273842 | validation: 3.97791667455406]
	TIME [epoch: 6.29 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6519330493540183		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 3.6519330493540183 | validation: 3.9575782284887993]
	TIME [epoch: 6.25 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6499274486962094		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 3.6499274486962094 | validation: 4.0359546484687225]
	TIME [epoch: 6.24 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6778215459056405		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 3.6778215459056405 | validation: 3.9186481263123]
	TIME [epoch: 6.25 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6450021374264		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 3.6450021374264 | validation: 3.779554933017442]
	TIME [epoch: 6.25 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6518764659622684		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 3.6518764659622684 | validation: 3.8056455998243566]
	TIME [epoch: 6.25 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6755134695739096		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 3.6755134695739096 | validation: 3.762233992885176]
	TIME [epoch: 6.29 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.675012157412495		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 3.675012157412495 | validation: 3.7051451652980463]
	TIME [epoch: 6.25 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.618401354602586		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 3.618401354602586 | validation: 3.8621970161930594]
	TIME [epoch: 6.25 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6107593600824215		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 3.6107593600824215 | validation: 3.8710625687728815]
	TIME [epoch: 6.25 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7012975163018105		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 3.7012975163018105 | validation: 3.9591828355389422]
	TIME [epoch: 6.25 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6610272140319715		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 3.6610272140319715 | validation: 3.7637526049420873]
	TIME [epoch: 6.24 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6659123114520353		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 3.6659123114520353 | validation: 3.9409502037006634]
	TIME [epoch: 6.29 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6177942835244457		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 3.6177942835244457 | validation: 3.846491265828955]
	TIME [epoch: 6.26 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.613276929120752		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 3.613276929120752 | validation: 3.874293525147136]
	TIME [epoch: 6.25 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7304542702515686		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 3.7304542702515686 | validation: 4.446654975959959]
	TIME [epoch: 6.24 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8322830726492167		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 3.8322830726492167 | validation: 3.8930202560893323]
	TIME [epoch: 6.24 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6262513508131113		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 3.6262513508131113 | validation: 3.911569967297739]
	TIME [epoch: 6.24 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6567793160089734		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 3.6567793160089734 | validation: 3.964905923417737]
	TIME [epoch: 6.29 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7006197089359234		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 3.7006197089359234 | validation: 3.9757952682023374]
	TIME [epoch: 6.25 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6955540713941333		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 3.6955540713941333 | validation: 3.718565979078397]
	TIME [epoch: 6.25 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.669679119949925		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 3.669679119949925 | validation: 3.697410772184691]
	TIME [epoch: 6.25 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.61516170409949		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 3.61516170409949 | validation: 3.6894820818916583]
	TIME [epoch: 6.25 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5662146386529967		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 3.5662146386529967 | validation: 3.6473848134921285]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_608.pth
	Model improved!!!
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.545248448195716		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 3.545248448195716 | validation: 3.8042408588717906]
	TIME [epoch: 6.3 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.557159244451095		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 3.557159244451095 | validation: 3.713033620038618]
	TIME [epoch: 6.25 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.567082410594487		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 3.567082410594487 | validation: 3.6767817703903045]
	TIME [epoch: 6.25 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6327129258787467		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 3.6327129258787467 | validation: 4.230009506957256]
	TIME [epoch: 6.25 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.868317782566256		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 3.868317782566256 | validation: 4.290529626760648]
	TIME [epoch: 6.25 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.697832463121547		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 3.697832463121547 | validation: 3.8430380126659704]
	TIME [epoch: 6.24 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.611143119029019		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 3.611143119029019 | validation: 3.912253393029384]
	TIME [epoch: 6.29 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.599516410451228		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 3.599516410451228 | validation: 3.878460724914944]
	TIME [epoch: 6.25 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.62917981061143		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 3.62917981061143 | validation: 3.851380552840195]
	TIME [epoch: 6.25 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.557479294422565		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 3.557479294422565 | validation: 3.739826698800044]
	TIME [epoch: 6.25 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5722167391633164		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 3.5722167391633164 | validation: 3.932196661598163]
	TIME [epoch: 6.25 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.666963189854232		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 3.666963189854232 | validation: 3.9359993465292886]
	TIME [epoch: 6.25 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6001060630911574		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 3.6001060630911574 | validation: 3.934025839256713]
	TIME [epoch: 6.3 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.624894096653016		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 3.624894096653016 | validation: 3.740625673891155]
	TIME [epoch: 6.26 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5657989328331183		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 3.5657989328331183 | validation: 3.8508333578040617]
	TIME [epoch: 6.25 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.583605508047447		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 3.583605508047447 | validation: 3.766765469470386]
	TIME [epoch: 6.25 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.591585961937138		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 3.591585961937138 | validation: 3.7864519412547333]
	TIME [epoch: 6.25 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6275270695773303		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 3.6275270695773303 | validation: 3.9777464020433424]
	TIME [epoch: 6.25 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.665787363249736		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 3.665787363249736 | validation: 3.9485178207027687]
	TIME [epoch: 6.3 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6076540024989274		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 3.6076540024989274 | validation: 3.756643592848272]
	TIME [epoch: 6.25 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5637582582549356		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 3.5637582582549356 | validation: 3.7050264747959263]
	TIME [epoch: 6.25 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.553028910094432		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 3.553028910094432 | validation: 3.5550061098294377]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_630.pth
	Model improved!!!
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.535355780128499		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 3.535355780128499 | validation: 3.6931722001454883]
	TIME [epoch: 6.25 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.521785810503327		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 3.521785810503327 | validation: 3.6209054679582113]
	TIME [epoch: 6.25 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.525826011159311		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 3.525826011159311 | validation: 3.6540042851829058]
	TIME [epoch: 6.29 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.507142326612934		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 3.507142326612934 | validation: 3.615980574746776]
	TIME [epoch: 6.25 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5283343062545462		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 3.5283343062545462 | validation: 3.6657513838708393]
	TIME [epoch: 6.25 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5422426671588356		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 3.5422426671588356 | validation: 3.64720429664335]
	TIME [epoch: 6.24 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5522207497918203		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 3.5522207497918203 | validation: 3.7416327336298068]
	TIME [epoch: 6.24 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5359400349571146		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 3.5359400349571146 | validation: 3.654990336063017]
	TIME [epoch: 6.25 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5614806429029207		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 3.5614806429029207 | validation: 3.6231771854971395]
	TIME [epoch: 6.28 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5349512213180954		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 3.5349512213180954 | validation: 3.5931452851008228]
	TIME [epoch: 6.24 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.514287022314932		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 3.514287022314932 | validation: 3.6073425525918963]
	TIME [epoch: 6.24 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.518367108495672		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 3.518367108495672 | validation: 3.624379291888318]
	TIME [epoch: 6.24 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5206160345236848		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 3.5206160345236848 | validation: 3.6678742336363976]
	TIME [epoch: 6.24 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.507971197896062		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 3.507971197896062 | validation: 3.578996085236385]
	TIME [epoch: 6.26 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5147866678351605		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 3.5147866678351605 | validation: 3.6447830771889844]
	TIME [epoch: 6.28 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.520993361906952		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 3.520993361906952 | validation: 3.6563592372946374]
	TIME [epoch: 6.24 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5601890144326833		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 3.5601890144326833 | validation: 3.66945801420117]
	TIME [epoch: 6.24 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.529920731500552		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 3.529920731500552 | validation: 3.6255052720645873]
	TIME [epoch: 6.24 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5282749689768758		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 3.5282749689768758 | validation: 3.6263397416271204]
	TIME [epoch: 6.25 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5106275354420164		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 3.5106275354420164 | validation: 3.582581130285085]
	TIME [epoch: 6.26 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5019139850473584		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 3.5019139850473584 | validation: 3.5949168659273125]
	TIME [epoch: 6.28 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5125257462474115		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 3.5125257462474115 | validation: 3.536608325032486]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_652.pth
	Model improved!!!
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5147089577169663		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 3.5147089577169663 | validation: 3.5112582590680876]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_653.pth
	Model improved!!!
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.505980295430007		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 3.505980295430007 | validation: 3.6359171956451237]
	TIME [epoch: 6.24 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5289804981714616		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 3.5289804981714616 | validation: 3.587824808508068]
	TIME [epoch: 6.23 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.494806121203058		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 3.494806121203058 | validation: 3.6213301157369084]
	TIME [epoch: 6.26 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5874636815308145		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 3.5874636815308145 | validation: 3.6023605319312932]
	TIME [epoch: 6.26 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5113950073459823		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 3.5113950073459823 | validation: 3.688964688704748]
	TIME [epoch: 6.24 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.526542007400783		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 3.526542007400783 | validation: 3.6346108658615663]
	TIME [epoch: 6.24 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5022842476371436		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 3.5022842476371436 | validation: 3.6326808534628894]
	TIME [epoch: 6.24 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5321644006536896		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 3.5321644006536896 | validation: 3.6339712899006242]
	TIME [epoch: 6.23 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.491413097005165		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 3.491413097005165 | validation: 3.5701899278844618]
	TIME [epoch: 6.26 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5030177790116417		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 3.5030177790116417 | validation: 3.5215998098669523]
	TIME [epoch: 6.26 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5272002702364125		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 3.5272002702364125 | validation: 3.770894746894431]
	TIME [epoch: 6.24 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5236412680906533		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 3.5236412680906533 | validation: 3.616983992571451]
	TIME [epoch: 6.24 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5225637595319244		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 3.5225637595319244 | validation: 3.5614041317751313]
	TIME [epoch: 6.23 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5375262324497894		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 3.5375262324497894 | validation: 3.6433197834619904]
	TIME [epoch: 6.23 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5719277803882235		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 3.5719277803882235 | validation: 3.820008136605698]
	TIME [epoch: 6.26 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5518108776067536		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 3.5518108776067536 | validation: 3.5411752544071726]
	TIME [epoch: 6.26 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4866801898863162		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 3.4866801898863162 | validation: 3.5856080567517363]
	TIME [epoch: 6.24 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4752024725400656		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 3.4752024725400656 | validation: 3.6156622279199517]
	TIME [epoch: 6.24 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.51358033681767		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 3.51358033681767 | validation: 3.627824180531648]
	TIME [epoch: 6.24 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5503132671063735		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 3.5503132671063735 | validation: 3.700380228539062]
	TIME [epoch: 6.23 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.561503534894511		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 3.561503534894511 | validation: 3.748931887360524]
	TIME [epoch: 6.27 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5769680476817167		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 3.5769680476817167 | validation: 3.685188870773576]
	TIME [epoch: 6.26 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.529794795427567		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 3.529794795427567 | validation: 3.662590733960175]
	TIME [epoch: 6.24 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5099574347656564		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 3.5099574347656564 | validation: 3.5975173664899476]
	TIME [epoch: 6.24 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.560708621125864		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 3.560708621125864 | validation: 3.7009138938737367]
	TIME [epoch: 6.24 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.580490483014268		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 3.580490483014268 | validation: 3.609128754914175]
	TIME [epoch: 6.24 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4882082236228573		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 3.4882082236228573 | validation: 3.515886986339093]
	TIME [epoch: 6.26 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.507793180875813		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 3.507793180875813 | validation: 3.5869243060921754]
	TIME [epoch: 6.26 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.511050569703634		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 3.511050569703634 | validation: 3.6281879745842254]
	TIME [epoch: 6.24 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.491755489148482		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 3.491755489148482 | validation: 3.613021840215911]
	TIME [epoch: 6.24 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5005846095544966		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 3.5005846095544966 | validation: 3.649046746740085]
	TIME [epoch: 6.24 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.50194427648059		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 3.50194427648059 | validation: 3.6253055299200057]
	TIME [epoch: 6.24 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4819611410571407		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 3.4819611410571407 | validation: 3.539812351333424]
	TIME [epoch: 6.27 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.503484538465999		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 3.503484538465999 | validation: 3.5580800488247584]
	TIME [epoch: 6.27 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5083897860364575		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 3.5083897860364575 | validation: 3.582416909769561]
	TIME [epoch: 6.24 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.519628343169141		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 3.519628343169141 | validation: 3.5460351912048194]
	TIME [epoch: 6.24 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.479407071203333		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 3.479407071203333 | validation: 3.5577601656079008]
	TIME [epoch: 6.24 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4773959961227803		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 3.4773959961227803 | validation: 3.618859668520744]
	TIME [epoch: 6.24 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.470236136534821		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 3.470236136534821 | validation: 3.5648467133564052]
	TIME [epoch: 6.27 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4792704360678686		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 3.4792704360678686 | validation: 3.5799581354446772]
	TIME [epoch: 6.26 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.455431526831487		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 3.455431526831487 | validation: 3.531750160471131]
	TIME [epoch: 6.24 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.481608196764272		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 3.481608196764272 | validation: 3.602287430843589]
	TIME [epoch: 6.24 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.503986027946449		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 3.503986027946449 | validation: 3.7213126191932586]
	TIME [epoch: 6.24 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.531653265332613		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 3.531653265332613 | validation: 3.763433733779153]
	TIME [epoch: 6.24 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5639152344922005		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 3.5639152344922005 | validation: 3.7566081341133426]
	TIME [epoch: 6.27 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.513218521324213		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 3.513218521324213 | validation: 3.587782636520922]
	TIME [epoch: 6.26 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5129397145300354		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 3.5129397145300354 | validation: 3.61494651081111]
	TIME [epoch: 6.23 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.511329846275009		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 3.511329846275009 | validation: 3.6671334103271302]
	TIME [epoch: 6.24 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.507552604530634		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 3.507552604530634 | validation: 3.607710947223649]
	TIME [epoch: 6.24 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.498101532434354		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 3.498101532434354 | validation: 3.6353821528441634]
	TIME [epoch: 6.24 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.530542492619451		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 3.530542492619451 | validation: 3.71148294817057]
	TIME [epoch: 6.28 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.501461782684861		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 3.501461782684861 | validation: 3.5638323925630964]
	TIME [epoch: 6.26 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5145416488600203		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 3.5145416488600203 | validation: 3.5975056911558134]
	TIME [epoch: 6.24 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.482588200125668		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 3.482588200125668 | validation: 3.713564242234133]
	TIME [epoch: 6.24 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5241299352800124		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 3.5241299352800124 | validation: 3.6938660172147166]
	TIME [epoch: 6.24 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.523814624530885		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 3.523814624530885 | validation: 3.64045387072946]
	TIME [epoch: 6.24 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.501389226724548		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 3.501389226724548 | validation: 3.620882283084383]
	TIME [epoch: 6.28 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4688237832155737		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 3.4688237832155737 | validation: 3.533491775589079]
	TIME [epoch: 6.25 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4618881419011656		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 3.4618881419011656 | validation: 3.5601797604170358]
	TIME [epoch: 6.24 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4656323789486425		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 3.4656323789486425 | validation: 3.583047981067126]
	TIME [epoch: 6.25 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4746756309135876		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 3.4746756309135876 | validation: 3.589296498483775]
	TIME [epoch: 6.24 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4991145192312647		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 3.4991145192312647 | validation: 3.5657668495032118]
	TIME [epoch: 6.24 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4633808134398523		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 3.4633808134398523 | validation: 3.564081383748474]
	TIME [epoch: 6.28 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4709474988832807		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 3.4709474988832807 | validation: 3.5265254962470927]
	TIME [epoch: 6.26 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4548477732021237		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 3.4548477732021237 | validation: 3.5533133112870194]
	TIME [epoch: 6.25 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4666178567716344		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 3.4666178567716344 | validation: 3.535622661927964]
	TIME [epoch: 6.24 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5052389478919417		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 3.5052389478919417 | validation: 3.573486546149309]
	TIME [epoch: 6.24 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.510516135739728		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 3.510516135739728 | validation: 3.5230313946142537]
	TIME [epoch: 6.24 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.461875226525577		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 3.461875226525577 | validation: 3.539802027883838]
	TIME [epoch: 6.29 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.446980116726293		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 3.446980116726293 | validation: 3.6368269690329402]
	TIME [epoch: 6.25 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.472882034035886		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 3.472882034035886 | validation: 3.5795446402088484]
	TIME [epoch: 6.25 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4533008279785413		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 3.4533008279785413 | validation: 3.5693403028191373]
	TIME [epoch: 6.24 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.450896613418853		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 3.450896613418853 | validation: 3.517796071416389]
	TIME [epoch: 6.23 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4427860071374874		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 3.4427860071374874 | validation: 3.4989442427875233]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_727.pth
	Model improved!!!
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.446588794473837		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 3.446588794473837 | validation: 3.4642134376355944]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_728.pth
	Model improved!!!
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.455506473379433		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 3.455506473379433 | validation: 3.5537510085774198]
	TIME [epoch: 6.26 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.431960388503353		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 3.431960388503353 | validation: 3.580710206860755]
	TIME [epoch: 6.24 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.440613580606617		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 3.440613580606617 | validation: 3.6588004389989184]
	TIME [epoch: 6.24 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4743765596102065		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 3.4743765596102065 | validation: 3.6082872100973047]
	TIME [epoch: 6.25 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.452423444609997		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 3.452423444609997 | validation: 3.665276102400985]
	TIME [epoch: 6.25 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.499709827239548		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 3.499709827239548 | validation: 3.597621085202242]
	TIME [epoch: 6.29 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.456169044596309		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 3.456169044596309 | validation: 3.628648962495376]
	TIME [epoch: 6.25 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.446814935559914		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 3.446814935559914 | validation: 3.609217538875392]
	TIME [epoch: 6.25 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.465477838019759		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 3.465477838019759 | validation: 3.5373748832799636]
	TIME [epoch: 6.25 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4342159173426374		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 3.4342159173426374 | validation: 3.511541675784634]
	TIME [epoch: 6.24 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.436510082775563		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 3.436510082775563 | validation: 3.556083629314271]
	TIME [epoch: 6.25 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4556152618829614		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 3.4556152618829614 | validation: 3.5795407521637284]
	TIME [epoch: 6.29 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4268025108248605		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 3.4268025108248605 | validation: 3.541765771440148]
	TIME [epoch: 6.3 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.456749069837605		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 3.456749069837605 | validation: 3.5071158692825337]
	TIME [epoch: 6.24 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.463529615284313		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 3.463529615284313 | validation: 3.5812527925528506]
	TIME [epoch: 6.25 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4423353799903325		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 3.4423353799903325 | validation: 3.642577168087656]
	TIME [epoch: 6.25 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4614113297626683		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 3.4614113297626683 | validation: 3.5740198971838844]
	TIME [epoch: 6.24 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.481339659342675		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 3.481339659342675 | validation: 3.6234741023303405]
	TIME [epoch: 6.29 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4648241216922346		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 3.4648241216922346 | validation: 3.578086249903915]
	TIME [epoch: 6.25 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.441191042668263		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 3.441191042668263 | validation: 3.5642421381717684]
	TIME [epoch: 6.25 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.432678265208821		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 3.432678265208821 | validation: 3.5116279383634987]
	TIME [epoch: 6.25 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4280346622111764		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 3.4280346622111764 | validation: 3.5473506376141923]
	TIME [epoch: 6.25 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.412892194385679		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 3.412892194385679 | validation: 3.49423122849531]
	TIME [epoch: 6.25 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.433535213661222		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 3.433535213661222 | validation: 3.4958463126524038]
	TIME [epoch: 6.3 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4185365538865895		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 3.4185365538865895 | validation: 3.491687915201659]
	TIME [epoch: 6.25 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.429092636218246		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 3.429092636218246 | validation: 3.521987513209085]
	TIME [epoch: 6.25 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.418244984331784		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 3.418244984331784 | validation: 3.55123938991633]
	TIME [epoch: 6.24 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.43436395738221		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 3.43436395738221 | validation: 3.5327245537652567]
	TIME [epoch: 6.25 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4424105726118563		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 3.4424105726118563 | validation: 3.5860543539239362]
	TIME [epoch: 6.25 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4378055164183965		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 3.4378055164183965 | validation: 3.607936858302889]
	TIME [epoch: 6.3 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.436318350571609		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 3.436318350571609 | validation: 3.667310651589668]
	TIME [epoch: 6.26 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4672771301642227		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 3.4672771301642227 | validation: 3.6098228113443867]
	TIME [epoch: 6.24 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.461997396081531		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 3.461997396081531 | validation: 3.6645808447629453]
	TIME [epoch: 6.25 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4723582324167133		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 3.4723582324167133 | validation: 3.588595975206605]
	TIME [epoch: 6.25 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.429672542061608		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 3.429672542061608 | validation: 3.574723948330819]
	TIME [epoch: 6.25 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.470454301627773		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 3.470454301627773 | validation: 3.5665195421979154]
	TIME [epoch: 6.3 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4396424486162633		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 3.4396424486162633 | validation: 3.50563355634657]
	TIME [epoch: 6.25 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.413326677098059		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 3.413326677098059 | validation: 3.4792373832482015]
	TIME [epoch: 6.24 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4122319357158815		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 3.4122319357158815 | validation: 3.6215701508638096]
	TIME [epoch: 6.25 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.467203517202795		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 3.467203517202795 | validation: 3.587366275883509]
	TIME [epoch: 6.25 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.414769186420062		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 3.414769186420062 | validation: 3.6083364522627166]
	TIME [epoch: 6.25 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4257097945663486		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 3.4257097945663486 | validation: 3.5840592019472948]
	TIME [epoch: 6.3 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4323894182531194		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 3.4323894182531194 | validation: 3.5484174485808726]
	TIME [epoch: 6.25 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4350599222230485		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 3.4350599222230485 | validation: 3.5812744208478]
	TIME [epoch: 6.26 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4173472156772022		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 3.4173472156772022 | validation: 3.5127904220738086]
	TIME [epoch: 6.25 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4338510117616687		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 3.4338510117616687 | validation: 3.5402848846509016]
	TIME [epoch: 6.25 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4312920307697397		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 3.4312920307697397 | validation: 3.5070143925168877]
	TIME [epoch: 6.26 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.419451557339938		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 3.419451557339938 | validation: 3.559791437773254]
	TIME [epoch: 6.29 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4364510976250906		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 3.4364510976250906 | validation: 3.525081792414632]
	TIME [epoch: 6.26 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4190136106584137		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 3.4190136106584137 | validation: 3.5368084588198307]
	TIME [epoch: 6.25 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4427783252039803		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 3.4427783252039803 | validation: 3.6380740751450444]
	TIME [epoch: 6.25 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4583165434114287		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 3.4583165434114287 | validation: 3.62473255691446]
	TIME [epoch: 6.25 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4803981628881466		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 3.4803981628881466 | validation: 3.6593318369134105]
	TIME [epoch: 6.25 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4350690428399724		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 3.4350690428399724 | validation: 3.557555241468564]
	TIME [epoch: 6.29 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.467524289418252		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 3.467524289418252 | validation: 3.5601037042480144]
	TIME [epoch: 6.25 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.433922025106759		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 3.433922025106759 | validation: 3.5174881113195067]
	TIME [epoch: 6.25 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.434345459520815		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 3.434345459520815 | validation: 3.5596737078830034]
	TIME [epoch: 6.25 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4300030682240044		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 3.4300030682240044 | validation: 3.552446705363547]
	TIME [epoch: 6.25 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4316609278358046		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 3.4316609278358046 | validation: 3.5544376890310594]
	TIME [epoch: 6.27 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4265318103178615		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 3.4265318103178615 | validation: 3.4584208057341663]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_788.pth
	Model improved!!!
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4313262112100813		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 3.4313262112100813 | validation: 3.524477762909632]
	TIME [epoch: 6.26 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.410383910410469		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 3.410383910410469 | validation: 3.5693998830735447]
	TIME [epoch: 6.25 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.443035093189624		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 3.443035093189624 | validation: 3.587750643606845]
	TIME [epoch: 6.26 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.472650406412293		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 3.472650406412293 | validation: 3.602705566361475]
	TIME [epoch: 6.25 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4821213345935274		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 3.4821213345935274 | validation: 3.603477745238468]
	TIME [epoch: 6.27 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4688827001781677		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 3.4688827001781677 | validation: 3.600513549106012]
	TIME [epoch: 6.29 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4756267160048697		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 3.4756267160048697 | validation: 3.626501359097352]
	TIME [epoch: 6.26 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.442590102860232		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 3.442590102860232 | validation: 3.5850122159223696]
	TIME [epoch: 6.25 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4424953556566265		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 3.4424953556566265 | validation: 3.6569873234504735]
	TIME [epoch: 6.25 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.450343330198863		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 3.450343330198863 | validation: 3.597167958003218]
	TIME [epoch: 6.25 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.443613499456668		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 3.443613499456668 | validation: 3.545716459555421]
	TIME [epoch: 6.28 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4268730395974467		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 3.4268730395974467 | validation: 3.598510607692722]
	TIME [epoch: 6.29 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4172435725547614		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 3.4172435725547614 | validation: 3.5631147732417943]
	TIME [epoch: 6.26 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.423192513642947		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 3.423192513642947 | validation: 3.5437774837292375]
	TIME [epoch: 6.26 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4406288709656487		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 3.4406288709656487 | validation: 3.5705462748688244]
	TIME [epoch: 6.25 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.438725433950416		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 3.438725433950416 | validation: 3.483599368789889]
	TIME [epoch: 6.25 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.390245941499985		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 3.390245941499985 | validation: 3.508238340502417]
	TIME [epoch: 6.27 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.416402848649342		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 3.416402848649342 | validation: 3.44373490488666]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_806.pth
	Model improved!!!
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.408836803872356		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 3.408836803872356 | validation: 3.5081665907014212]
	TIME [epoch: 6.26 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.401306758447202		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 3.401306758447202 | validation: 3.4835332577469895]
	TIME [epoch: 6.26 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.404361229991344		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 3.404361229991344 | validation: 3.484403431567862]
	TIME [epoch: 6.25 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3909774242341513		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 3.3909774242341513 | validation: 3.478910259068113]
	TIME [epoch: 6.25 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.414029589948193		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 3.414029589948193 | validation: 3.4942050359546233]
	TIME [epoch: 6.28 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.408661903601264		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 3.408661903601264 | validation: 3.52833211242754]
	TIME [epoch: 6.29 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4248110536603833		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 3.4248110536603833 | validation: 3.519724295862334]
	TIME [epoch: 6.26 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3936486152671925		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 3.3936486152671925 | validation: 3.4822953952994333]
	TIME [epoch: 6.26 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3880722940628987		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 3.3880722940628987 | validation: 3.471564713958612]
	TIME [epoch: 6.26 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4038457975898706		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 3.4038457975898706 | validation: 3.456495019434093]
	TIME [epoch: 6.25 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4308871859150445		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 3.4308871859150445 | validation: 3.5201183982116233]
	TIME [epoch: 6.28 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.416535768377816		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 3.416535768377816 | validation: 3.5102040266754644]
	TIME [epoch: 6.28 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4061340771637543		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 3.4061340771637543 | validation: 3.4965614819276407]
	TIME [epoch: 6.25 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4102746974780915		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 3.4102746974780915 | validation: 3.529086316931096]
	TIME [epoch: 6.25 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4259606260807245		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 3.4259606260807245 | validation: 3.532676988087127]
	TIME [epoch: 6.26 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4115500050137415		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 3.4115500050137415 | validation: 3.549140515105285]
	TIME [epoch: 6.27 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4183344974389844		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 3.4183344974389844 | validation: 3.4971835813052943]
	TIME [epoch: 6.29 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4003897109860874		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 3.4003897109860874 | validation: 3.5010771677228525]
	TIME [epoch: 6.27 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4123466012317714		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 3.4123466012317714 | validation: 3.5234798048011307]
	TIME [epoch: 6.25 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4204338136777923		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 3.4204338136777923 | validation: 3.472947087316544]
	TIME [epoch: 6.25 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4102072191667236		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 3.4102072191667236 | validation: 3.504243131918347]
	TIME [epoch: 6.25 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4206058961585755		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 3.4206058961585755 | validation: 3.4606767208969877]
	TIME [epoch: 6.26 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4116238503472207		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 3.4116238503472207 | validation: 3.5166419628365024]
	TIME [epoch: 6.3 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.405655958349418		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 3.405655958349418 | validation: 3.5729080530836814]
	TIME [epoch: 6.26 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.417948402566255		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 3.417948402566255 | validation: 3.5162433793975274]
	TIME [epoch: 6.26 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.407681662856021		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 3.407681662856021 | validation: 3.558212137516782]
	TIME [epoch: 6.25 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.404226018083491		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 3.404226018083491 | validation: 3.5355159055503735]
	TIME [epoch: 6.25 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.400465402280611		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 3.400465402280611 | validation: 3.5292761776912203]
	TIME [epoch: 6.24 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.403927576285626		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 3.403927576285626 | validation: 3.5430189961460368]
	TIME [epoch: 6.31 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4128225538881143		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 3.4128225538881143 | validation: 3.5227169930141966]
	TIME [epoch: 6.26 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4056015625124303		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 3.4056015625124303 | validation: 3.4554956564444863]
	TIME [epoch: 6.25 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4082133389801386		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 3.4082133389801386 | validation: 3.473872917629226]
	TIME [epoch: 6.25 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.414286389227281		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 3.414286389227281 | validation: 3.5288141893471496]
	TIME [epoch: 6.25 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.415471807111615		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 3.415471807111615 | validation: 3.565506911281674]
	TIME [epoch: 6.24 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.408997670563833		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 3.408997670563833 | validation: 3.5873026184878465]
	TIME [epoch: 6.3 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.410345747238544		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 3.410345747238544 | validation: 3.529428516636864]
	TIME [epoch: 6.26 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.400685344155175		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 3.400685344155175 | validation: 3.4978116615843216]
	TIME [epoch: 6.25 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.40655142633993		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 3.40655142633993 | validation: 3.5719305282576577]
	TIME [epoch: 6.24 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4170839068703107		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 3.4170839068703107 | validation: 3.4859768957818704]
	TIME [epoch: 6.25 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.395637020726817		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 3.395637020726817 | validation: 3.515399846784806]
	TIME [epoch: 6.25 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.399957003008698		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 3.399957003008698 | validation: 3.4923561388195177]
	TIME [epoch: 6.29 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3959702181263167		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 3.3959702181263167 | validation: 3.5176196636821997]
	TIME [epoch: 6.26 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4178333747837026		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 3.4178333747837026 | validation: 3.552019452456468]
	TIME [epoch: 6.25 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4000347038817615		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 3.4000347038817615 | validation: 3.5398198918965447]
	TIME [epoch: 6.25 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3917747378992766		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 3.3917747378992766 | validation: 3.547299330443379]
	TIME [epoch: 6.25 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.393798079661382		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 3.393798079661382 | validation: 3.5251458390958765]
	TIME [epoch: 6.25 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3984274773314893		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 3.3984274773314893 | validation: 3.563406465632088]
	TIME [epoch: 6.3 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.378982378008523		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 3.378982378008523 | validation: 3.5281641769968033]
	TIME [epoch: 6.26 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3979739669351194		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 3.3979739669351194 | validation: 3.4971459511797622]
	TIME [epoch: 6.24 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.400158461985609		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 3.400158461985609 | validation: 3.543187673528263]
	TIME [epoch: 6.25 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3937618180277327		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 3.3937618180277327 | validation: 3.4586566651332147]
	TIME [epoch: 6.25 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4000659771562702		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 3.4000659771562702 | validation: 3.5094129138318912]
	TIME [epoch: 6.25 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4080933325547407		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 3.4080933325547407 | validation: 3.5490842683861255]
	TIME [epoch: 6.29 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.397482898479314		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 3.397482898479314 | validation: 3.5407328982706456]
	TIME [epoch: 6.26 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3986096395190044		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 3.3986096395190044 | validation: 3.6188299689668106]
	TIME [epoch: 6.25 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4124094060735146		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 3.4124094060735146 | validation: 3.5504295455478743]
	TIME [epoch: 6.25 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4185225266276817		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 3.4185225266276817 | validation: 3.5212085819850247]
	TIME [epoch: 6.25 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.426538825477862		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 3.426538825477862 | validation: 3.5569349459955895]
	TIME [epoch: 6.25 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.405400462516341		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 3.405400462516341 | validation: 3.4744013679876846]
	TIME [epoch: 6.3 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.403141603522341		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 3.403141603522341 | validation: 3.442064770592337]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_866.pth
	Model improved!!!
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4316098062879776		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 3.4316098062879776 | validation: 3.519245783412307]
	TIME [epoch: 6.24 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3982126416167544		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 3.3982126416167544 | validation: 3.504018003827083]
	TIME [epoch: 6.25 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3949160231201696		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 3.3949160231201696 | validation: 3.524036584629571]
	TIME [epoch: 6.25 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.375682582024119		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 3.375682582024119 | validation: 3.568919433160673]
	TIME [epoch: 6.24 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.402995690911565		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 3.402995690911565 | validation: 3.5473675063668537]
	TIME [epoch: 6.29 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.395849151674425		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 3.395849151674425 | validation: 3.4841537673829777]
	TIME [epoch: 6.24 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.398361617489958		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 3.398361617489958 | validation: 3.5137526668452987]
	TIME [epoch: 6.24 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4031682356052815		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 3.4031682356052815 | validation: 3.527450718040622]
	TIME [epoch: 6.24 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.383185405352203		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 3.383185405352203 | validation: 3.528786647204132]
	TIME [epoch: 6.25 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.385488928516381		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 3.385488928516381 | validation: 3.5182308104361733]
	TIME [epoch: 6.24 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4084719859079735		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 3.4084719859079735 | validation: 3.52203333037716]
	TIME [epoch: 6.3 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.398140136083956		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 3.398140136083956 | validation: 3.4721814460886553]
	TIME [epoch: 6.25 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.395720542458279		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 3.395720542458279 | validation: 3.531393332233092]
	TIME [epoch: 6.24 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.397161853534107		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 3.397161853534107 | validation: 3.617723263708303]
	TIME [epoch: 6.23 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3808867483245444		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 3.3808867483245444 | validation: 3.5087998264358307]
	TIME [epoch: 6.25 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3696571324610214		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 3.3696571324610214 | validation: 3.531084825498194]
	TIME [epoch: 6.24 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3960026873359226		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 3.3960026873359226 | validation: 3.576578784052377]
	TIME [epoch: 6.29 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.397782396212776		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 3.397782396212776 | validation: 3.5780710435030856]
	TIME [epoch: 6.24 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.393765379283338		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 3.393765379283338 | validation: 3.5110997188182926]
	TIME [epoch: 6.25 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.389384508373372		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 3.389384508373372 | validation: 3.504910397657524]
	TIME [epoch: 6.24 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3733329096792057		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 3.3733329096792057 | validation: 3.5393124429778737]
	TIME [epoch: 6.24 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3793268816523785		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 3.3793268816523785 | validation: 3.5806180554094595]
	TIME [epoch: 6.24 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3798385607052377		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 3.3798385607052377 | validation: 3.5447768439157956]
	TIME [epoch: 6.29 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4010456225457735		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 3.4010456225457735 | validation: 3.520130086607248]
	TIME [epoch: 6.24 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3964865465282537		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 3.3964865465282537 | validation: 3.5077997117202906]
	TIME [epoch: 6.24 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.37750439500141		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 3.37750439500141 | validation: 3.5013425671094502]
	TIME [epoch: 6.25 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3958383082620376		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 3.3958383082620376 | validation: 3.5704334259323494]
	TIME [epoch: 6.24 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.377932322420328		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 3.377932322420328 | validation: 3.4903269481992]
	TIME [epoch: 6.25 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3708728588865347		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 3.3708728588865347 | validation: 3.519582611376408]
	TIME [epoch: 6.29 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3797314530160882		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 3.3797314530160882 | validation: 3.5156051571792437]
	TIME [epoch: 6.24 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3820146846869052		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 3.3820146846869052 | validation: 3.496908746611727]
	TIME [epoch: 6.25 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.361336009495242		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 3.361336009495242 | validation: 3.5637591237355135]
	TIME [epoch: 6.24 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.395943535277989		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 3.395943535277989 | validation: 3.5904995828744184]
	TIME [epoch: 6.25 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3867611201246377		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 3.3867611201246377 | validation: 3.5278974826118517]
	TIME [epoch: 6.25 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3878266997968334		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 3.3878266997968334 | validation: 3.513914875762785]
	TIME [epoch: 6.29 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.371652861739864		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 3.371652861739864 | validation: 3.559817150659046]
	TIME [epoch: 6.24 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3861384886143684		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 3.3861384886143684 | validation: 3.5217744849737205]
	TIME [epoch: 6.25 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.393281211676987		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 3.393281211676987 | validation: 3.54734582818878]
	TIME [epoch: 6.25 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.388325488083721		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 3.388325488083721 | validation: 3.5249759439142436]
	TIME [epoch: 6.24 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3943273648596857		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 3.3943273648596857 | validation: 3.6172751652871407]
	TIME [epoch: 6.26 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.409985993459083		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 3.409985993459083 | validation: 3.6154559292388555]
	TIME [epoch: 6.29 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3878284134219623		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 3.3878284134219623 | validation: 3.4927370479455515]
	TIME [epoch: 6.25 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4041017887519565		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 3.4041017887519565 | validation: 3.6068755507108365]
	TIME [epoch: 6.24 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3811291586811216		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 3.3811291586811216 | validation: 3.462090771650141]
	TIME [epoch: 6.25 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.355344237849759		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 3.355344237849759 | validation: 3.496899419711877]
	TIME [epoch: 6.25 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3947010883825617		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 3.3947010883825617 | validation: 3.4717970925655743]
	TIME [epoch: 6.26 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.398018897644036		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 3.398018897644036 | validation: 3.48328615686951]
	TIME [epoch: 6.28 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3723291176941266		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 3.3723291176941266 | validation: 3.4775759845532157]
	TIME [epoch: 6.25 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.372901974107923		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 3.372901974107923 | validation: 3.448405891669603]
	TIME [epoch: 6.24 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4027306303958578		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 3.4027306303958578 | validation: 3.480388818300076]
	TIME [epoch: 6.24 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.379504201594692		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 3.379504201594692 | validation: 3.477503304561954]
	TIME [epoch: 6.24 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3952901948798377		[learning rate: 1.8186e-05]
	Learning Rate: 1.8186e-05
	LOSS [training: 3.3952901948798377 | validation: 3.5342444294524857]
	TIME [epoch: 6.26 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.380171117434452		[learning rate: 1.8054e-05]
	Learning Rate: 1.80542e-05
	LOSS [training: 3.380171117434452 | validation: 3.4931154859298883]
	TIME [epoch: 6.28 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3721684015036653		[learning rate: 1.7923e-05]
	Learning Rate: 1.79234e-05
	LOSS [training: 3.3721684015036653 | validation: 3.4493612171287786]
	TIME [epoch: 6.25 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.373793043353357		[learning rate: 1.7794e-05]
	Learning Rate: 1.77936e-05
	LOSS [training: 3.373793043353357 | validation: 3.513363362238138]
	TIME [epoch: 6.25 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.380106759467096		[learning rate: 1.7665e-05]
	Learning Rate: 1.76647e-05
	LOSS [training: 3.380106759467096 | validation: 3.489356913869691]
	TIME [epoch: 6.25 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3817480157313176		[learning rate: 1.7537e-05]
	Learning Rate: 1.75367e-05
	LOSS [training: 3.3817480157313176 | validation: 3.4934887452400076]
	TIME [epoch: 6.25 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3981315260826572		[learning rate: 1.741e-05]
	Learning Rate: 1.74096e-05
	LOSS [training: 3.3981315260826572 | validation: 3.543404298605467]
	TIME [epoch: 6.28 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.386481912929368		[learning rate: 1.7284e-05]
	Learning Rate: 1.72835e-05
	LOSS [training: 3.386481912929368 | validation: 3.4970349097333293]
	TIME [epoch: 6.28 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.393147679856159		[learning rate: 1.7158e-05]
	Learning Rate: 1.71583e-05
	LOSS [training: 3.393147679856159 | validation: 3.4885198283684375]
	TIME [epoch: 6.25 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.376029780881587		[learning rate: 1.7034e-05]
	Learning Rate: 1.7034e-05
	LOSS [training: 3.376029780881587 | validation: 3.4577670074894673]
	TIME [epoch: 6.25 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3848761836842183		[learning rate: 1.6911e-05]
	Learning Rate: 1.69106e-05
	LOSS [training: 3.3848761836842183 | validation: 3.4889482603637294]
	TIME [epoch: 6.25 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3847009132051946		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 3.3847009132051946 | validation: 3.4472371241921715]
	TIME [epoch: 6.24 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.378036334540192		[learning rate: 1.6666e-05]
	Learning Rate: 1.66664e-05
	LOSS [training: 3.378036334540192 | validation: 3.4416154599642885]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_930.pth
	Model improved!!!
EPOCH 931/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3767121888545675		[learning rate: 1.6546e-05]
	Learning Rate: 1.65457e-05
	LOSS [training: 3.3767121888545675 | validation: 3.526822239626841]
	TIME [epoch: 6.29 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.392421538309563		[learning rate: 1.6426e-05]
	Learning Rate: 1.64258e-05
	LOSS [training: 3.392421538309563 | validation: 3.5264078051162056]
	TIME [epoch: 6.25 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.38080405006375		[learning rate: 1.6307e-05]
	Learning Rate: 1.63068e-05
	LOSS [training: 3.38080405006375 | validation: 3.489210021678792]
	TIME [epoch: 6.25 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.378063886779569		[learning rate: 1.6189e-05]
	Learning Rate: 1.61887e-05
	LOSS [training: 3.378063886779569 | validation: 3.4999136129588675]
	TIME [epoch: 6.25 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3915820633809273		[learning rate: 1.6071e-05]
	Learning Rate: 1.60714e-05
	LOSS [training: 3.3915820633809273 | validation: 3.4720592063189812]
	TIME [epoch: 6.25 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3820534221782985		[learning rate: 1.5955e-05]
	Learning Rate: 1.59549e-05
	LOSS [training: 3.3820534221782985 | validation: 3.515167507471321]
	TIME [epoch: 6.27 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3909992289831488		[learning rate: 1.5839e-05]
	Learning Rate: 1.58393e-05
	LOSS [training: 3.3909992289831488 | validation: 3.4596383409787865]
	TIME [epoch: 6.26 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.415265192598554		[learning rate: 1.5725e-05]
	Learning Rate: 1.57246e-05
	LOSS [training: 3.415265192598554 | validation: 3.508245196149665]
	TIME [epoch: 6.24 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.386232591575416		[learning rate: 1.5611e-05]
	Learning Rate: 1.56107e-05
	LOSS [training: 3.386232591575416 | validation: 3.4843329721198106]
	TIME [epoch: 6.24 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.376218528345782		[learning rate: 1.5498e-05]
	Learning Rate: 1.54976e-05
	LOSS [training: 3.376218528345782 | validation: 3.4884305111518388]
	TIME [epoch: 6.25 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3785740944392564		[learning rate: 1.5385e-05]
	Learning Rate: 1.53853e-05
	LOSS [training: 3.3785740944392564 | validation: 3.5046314941904604]
	TIME [epoch: 6.25 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3674740497384184		[learning rate: 1.5274e-05]
	Learning Rate: 1.52738e-05
	LOSS [training: 3.3674740497384184 | validation: 3.465114700983051]
	TIME [epoch: 6.28 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3750304592544955		[learning rate: 1.5163e-05]
	Learning Rate: 1.51632e-05
	LOSS [training: 3.3750304592544955 | validation: 3.50981434705275]
	TIME [epoch: 6.26 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3918872267616873		[learning rate: 1.5053e-05]
	Learning Rate: 1.50533e-05
	LOSS [training: 3.3918872267616873 | validation: 3.4748090487647225]
	TIME [epoch: 6.24 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4005963840799156		[learning rate: 1.4944e-05]
	Learning Rate: 1.49442e-05
	LOSS [training: 3.4005963840799156 | validation: 3.4647550748519365]
	TIME [epoch: 6.24 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3662069214823425		[learning rate: 1.4836e-05]
	Learning Rate: 1.4836e-05
	LOSS [training: 3.3662069214823425 | validation: 3.4900498085222917]
	TIME [epoch: 6.25 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3797208873802966		[learning rate: 1.4728e-05]
	Learning Rate: 1.47285e-05
	LOSS [training: 3.3797208873802966 | validation: 3.5231666736239333]
	TIME [epoch: 6.25 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.384941801135146		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 3.384941801135146 | validation: 3.4407873596345784]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_948.pth
	Model improved!!!
EPOCH 949/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3835845385224146		[learning rate: 1.4516e-05]
	Learning Rate: 1.45158e-05
	LOSS [training: 3.3835845385224146 | validation: 3.510359673646608]
	TIME [epoch: 6.27 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.368782477752734		[learning rate: 1.4411e-05]
	Learning Rate: 1.44107e-05
	LOSS [training: 3.368782477752734 | validation: 3.495735293430239]
	TIME [epoch: 6.24 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.406232834059863		[learning rate: 1.4306e-05]
	Learning Rate: 1.43063e-05
	LOSS [training: 3.406232834059863 | validation: 3.4565279993130265]
	TIME [epoch: 6.25 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3907733194971934		[learning rate: 1.4203e-05]
	Learning Rate: 1.42026e-05
	LOSS [training: 3.3907733194971934 | validation: 3.546904960469706]
	TIME [epoch: 6.24 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3864352276504692		[learning rate: 1.41e-05]
	Learning Rate: 1.40997e-05
	LOSS [training: 3.3864352276504692 | validation: 3.5144892413638766]
	TIME [epoch: 6.24 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.379051486295755		[learning rate: 1.3998e-05]
	Learning Rate: 1.39976e-05
	LOSS [training: 3.379051486295755 | validation: 3.474378693462021]
	TIME [epoch: 6.29 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3856076909435915		[learning rate: 1.3896e-05]
	Learning Rate: 1.38962e-05
	LOSS [training: 3.3856076909435915 | validation: 3.505548106781843]
	TIME [epoch: 6.26 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.372299014481027		[learning rate: 1.3795e-05]
	Learning Rate: 1.37955e-05
	LOSS [training: 3.372299014481027 | validation: 3.42829273289382]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_956.pth
	Model improved!!!
EPOCH 957/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.362203978077088		[learning rate: 1.3696e-05]
	Learning Rate: 1.36955e-05
	LOSS [training: 3.362203978077088 | validation: 3.4513343785288777]
	TIME [epoch: 6.25 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3797489931870652		[learning rate: 1.3596e-05]
	Learning Rate: 1.35963e-05
	LOSS [training: 3.3797489931870652 | validation: 3.487209295400697]
	TIME [epoch: 6.24 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3846035658625997		[learning rate: 1.3498e-05]
	Learning Rate: 1.34978e-05
	LOSS [training: 3.3846035658625997 | validation: 3.545660948880122]
	TIME [epoch: 6.24 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.36791917686748		[learning rate: 1.34e-05]
	Learning Rate: 1.34e-05
	LOSS [training: 3.36791917686748 | validation: 3.4917724228640834]
	TIME [epoch: 6.29 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.362833118877815		[learning rate: 1.3303e-05]
	Learning Rate: 1.33029e-05
	LOSS [training: 3.362833118877815 | validation: 3.528342388198227]
	TIME [epoch: 6.25 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.381058453032143		[learning rate: 1.3207e-05]
	Learning Rate: 1.32066e-05
	LOSS [training: 3.381058453032143 | validation: 3.4683341869589377]
	TIME [epoch: 6.25 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.396848086719294		[learning rate: 1.3111e-05]
	Learning Rate: 1.31109e-05
	LOSS [training: 3.396848086719294 | validation: 3.4147163362783393]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1b_v1_20240503_101338/states/model_phiq_1b_v1_963.pth
	Model improved!!!
EPOCH 964/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.380846276875776		[learning rate: 1.3016e-05]
	Learning Rate: 1.30159e-05
	LOSS [training: 3.380846276875776 | validation: 3.449819548330418]
	TIME [epoch: 6.25 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3813778378475443		[learning rate: 1.2922e-05]
	Learning Rate: 1.29216e-05
	LOSS [training: 3.3813778378475443 | validation: 3.465596060360866]
	TIME [epoch: 6.24 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3879979812059164		[learning rate: 1.2828e-05]
	Learning Rate: 1.2828e-05
	LOSS [training: 3.3879979812059164 | validation: 3.479724964972526]
	TIME [epoch: 6.31 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.381419862551989		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 3.381419862551989 | validation: 3.4569089047367045]
	TIME [epoch: 6.24 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3936167170814207		[learning rate: 1.2643e-05]
	Learning Rate: 1.26428e-05
	LOSS [training: 3.3936167170814207 | validation: 3.4317390081853025]
	TIME [epoch: 6.25 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.382568447695122		[learning rate: 1.2551e-05]
	Learning Rate: 1.25512e-05
	LOSS [training: 3.382568447695122 | validation: 3.4404435111842737]
	TIME [epoch: 6.24 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3774197327995257		[learning rate: 1.246e-05]
	Learning Rate: 1.24602e-05
	LOSS [training: 3.3774197327995257 | validation: 3.501213501821296]
	TIME [epoch: 6.24 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3704871858041		[learning rate: 1.237e-05]
	Learning Rate: 1.237e-05
	LOSS [training: 3.3704871858041 | validation: 3.5098105361950234]
	TIME [epoch: 6.24 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3562888840598784		[learning rate: 1.228e-05]
	Learning Rate: 1.22803e-05
	LOSS [training: 3.3562888840598784 | validation: 3.4923227007077022]
	TIME [epoch: 6.3 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.367641337949378		[learning rate: 1.2191e-05]
	Learning Rate: 1.21914e-05
	LOSS [training: 3.367641337949378 | validation: 3.435946167787318]
	TIME [epoch: 6.24 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3811099731928467		[learning rate: 1.2103e-05]
	Learning Rate: 1.21031e-05
	LOSS [training: 3.3811099731928467 | validation: 3.536009083161197]
	TIME [epoch: 6.24 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.354806267951832		[learning rate: 1.2015e-05]
	Learning Rate: 1.20154e-05
	LOSS [training: 3.354806267951832 | validation: 3.486788378626999]
	TIME [epoch: 6.25 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3670930812315403		[learning rate: 1.1928e-05]
	Learning Rate: 1.19283e-05
	LOSS [training: 3.3670930812315403 | validation: 3.5308485226015467]
	TIME [epoch: 6.25 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.379144986314311		[learning rate: 1.1842e-05]
	Learning Rate: 1.18419e-05
	LOSS [training: 3.379144986314311 | validation: 3.4566892393538304]
	TIME [epoch: 6.24 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3661995835653014		[learning rate: 1.1756e-05]
	Learning Rate: 1.17561e-05
	LOSS [training: 3.3661995835653014 | validation: 3.4523564597971985]
	TIME [epoch: 6.29 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.373083186834152		[learning rate: 1.1671e-05]
	Learning Rate: 1.16709e-05
	LOSS [training: 3.373083186834152 | validation: 3.4656074555729313]
	TIME [epoch: 6.25 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.382754309403676		[learning rate: 1.1586e-05]
	Learning Rate: 1.15864e-05
	LOSS [training: 3.382754309403676 | validation: 3.4964955568608573]
	TIME [epoch: 6.25 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3759456926680143		[learning rate: 1.1502e-05]
	Learning Rate: 1.15024e-05
	LOSS [training: 3.3759456926680143 | validation: 3.467379153968704]
	TIME [epoch: 6.24 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.350243712536853		[learning rate: 1.1419e-05]
	Learning Rate: 1.14191e-05
	LOSS [training: 3.350243712536853 | validation: 3.4796425756729024]
	TIME [epoch: 6.24 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.368251362723184		[learning rate: 1.1336e-05]
	Learning Rate: 1.13364e-05
	LOSS [training: 3.368251362723184 | validation: 3.4341475543400284]
	TIME [epoch: 6.25 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.365351005460391		[learning rate: 1.1254e-05]
	Learning Rate: 1.12542e-05
	LOSS [training: 3.365351005460391 | validation: 3.4758289575563595]
	TIME [epoch: 6.29 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.34963239488458		[learning rate: 1.1173e-05]
	Learning Rate: 1.11727e-05
	LOSS [training: 3.34963239488458 | validation: 3.446123157036232]
	TIME [epoch: 6.25 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3928369702670738		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 3.3928369702670738 | validation: 3.58160049380764]
	TIME [epoch: 6.24 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3865630531391053		[learning rate: 1.1011e-05]
	Learning Rate: 1.10114e-05
	LOSS [training: 3.3865630531391053 | validation: 3.498259516138286]
	TIME [epoch: 6.24 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.349939174369938		[learning rate: 1.0932e-05]
	Learning Rate: 1.09316e-05
	LOSS [training: 3.349939174369938 | validation: 3.4200756917518613]
	TIME [epoch: 6.24 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.390433902793191		[learning rate: 1.0852e-05]
	Learning Rate: 1.08524e-05
	LOSS [training: 3.390433902793191 | validation: 3.514902727704639]
	TIME [epoch: 6.25 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.372835939481644		[learning rate: 1.0774e-05]
	Learning Rate: 1.07738e-05
	LOSS [training: 3.372835939481644 | validation: 3.475808430317081]
	TIME [epoch: 6.3 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3702746754569635		[learning rate: 1.0696e-05]
	Learning Rate: 1.06957e-05
	LOSS [training: 3.3702746754569635 | validation: 3.457536658260577]
	TIME [epoch: 6.25 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3737112281614525		[learning rate: 1.0618e-05]
	Learning Rate: 1.06182e-05
	LOSS [training: 3.3737112281614525 | validation: 3.4817530081070336]
	TIME [epoch: 6.25 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3662862270225653		[learning rate: 1.0541e-05]
	Learning Rate: 1.05413e-05
	LOSS [training: 3.3662862270225653 | validation: 3.4839409662829586]
	TIME [epoch: 6.25 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.366482989442824		[learning rate: 1.0465e-05]
	Learning Rate: 1.04649e-05
	LOSS [training: 3.366482989442824 | validation: 3.4921526168679513]
	TIME [epoch: 6.24 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3533504784042116		[learning rate: 1.0389e-05]
	Learning Rate: 1.03891e-05
	LOSS [training: 3.3533504784042116 | validation: 3.532399486597509]
	TIME [epoch: 6.25 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3591829939749185		[learning rate: 1.0314e-05]
	Learning Rate: 1.03139e-05
	LOSS [training: 3.3591829939749185 | validation: 3.495968848321734]
	TIME [epoch: 6.29 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3669294176219395		[learning rate: 1.0239e-05]
	Learning Rate: 1.02391e-05
	LOSS [training: 3.3669294176219395 | validation: 3.5946117878517123]
	TIME [epoch: 6.25 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3828757860422494		[learning rate: 1.0165e-05]
	Learning Rate: 1.0165e-05
	LOSS [training: 3.3828757860422494 | validation: 3.4959494074409765]
	TIME [epoch: 6.25 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.361543472369394		[learning rate: 1.0091e-05]
	Learning Rate: 1.00913e-05
	LOSS [training: 3.361543472369394 | validation: 3.526981232206774]
	TIME [epoch: 6.24 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3694316904294217		[learning rate: 1.0018e-05]
	Learning Rate: 1.00182e-05
	LOSS [training: 3.3694316904294217 | validation: 3.5684206020743545]
	TIME [epoch: 6.24 sec]
Finished training in 6445.536 seconds.
