Args:
Namespace(name='model_phiq_1a_v2', outdir='out/model_training/model_phiq_1a_v2', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.75, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2220990764

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.325313003110335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.325313003110335 | validation: 7.218960953185057]
	TIME [epoch: 97.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.536853429472402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.536853429472402 | validation: 6.3832297732698215]
	TIME [epoch: 6.82 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.8952710715911705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8952710715911705 | validation: 6.0390386772335525]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.531764131537573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.531764131537573 | validation: 5.387981701349852]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.469935382444458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.469935382444458 | validation: 5.1549516656839565]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.095323934850646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.095323934850646 | validation: 5.083101238743556]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.8250645992356755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8250645992356755 | validation: 4.882656535372124]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.645570979989598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.645570979989598 | validation: 4.5947160798704285]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.474906205901034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.474906205901034 | validation: 4.766236813206959]
	TIME [epoch: 6.77 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.793785519555966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.793785519555966 | validation: 4.524393175157318]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.36800930462857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.36800930462857 | validation: 4.431251656738997]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.241673725454407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241673725454407 | validation: 4.2041773377220855]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.221265432812835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221265432812835 | validation: 4.1783444071643645]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.137253694952232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137253694952232 | validation: 4.070508017071513]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.042909558996472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.042909558996472 | validation: 4.12755257537547]
	TIME [epoch: 6.76 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9714104690619694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9714104690619694 | validation: 3.782120522153673]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.860146753834167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.860146753834167 | validation: 3.704094394689658]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.729238536547976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.729238536547976 | validation: 3.605801132893235]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6705854464628596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6705854464628596 | validation: 3.339759132643975]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.738023800653049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.738023800653049 | validation: 3.9221329864147285]
	TIME [epoch: 6.77 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6701956382537895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6701956382537895 | validation: 3.4068928336712814]
	TIME [epoch: 6.76 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5078890618218885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5078890618218885 | validation: 3.3730786083927295]
	TIME [epoch: 6.76 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6460723687710996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6460723687710996 | validation: 3.3898735965472726]
	TIME [epoch: 6.8 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.539873319187273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539873319187273 | validation: 3.3881315376820025]
	TIME [epoch: 6.77 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4485103617612305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4485103617612305 | validation: 3.3905193556325166]
	TIME [epoch: 6.76 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4489446089910123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4489446089910123 | validation: 3.30322022145002]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4747563944366004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4747563944366004 | validation: 3.463331450316157]
	TIME [epoch: 6.77 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9769286941725057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9769286941725057 | validation: 4.11924388547266]
	TIME [epoch: 6.76 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.721020637589304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.721020637589304 | validation: 3.8913846984478084]
	TIME [epoch: 6.77 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7271278082097865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7271278082097865 | validation: 3.830569548549481]
	TIME [epoch: 6.81 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4122068160776102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4122068160776102 | validation: 3.230065688155559]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_31.pth
	Model improved!!!
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.41578511414754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.41578511414754 | validation: 3.2006934357596446]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.26253985231671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.26253985231671 | validation: 3.2886490317194874]
	TIME [epoch: 6.76 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.340163961390009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.340163961390009 | validation: 3.0860961693467237]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1417458782255223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1417458782255223 | validation: 2.990000655512425]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0973773092920123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0973773092920123 | validation: 2.962992663384748]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_36.pth
	Model improved!!!
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0318176513285118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0318176513285118 | validation: 2.789562364274972]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1260891445552015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1260891445552015 | validation: 3.4461371990829184]
	TIME [epoch: 6.77 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1420924191269575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1420924191269575 | validation: 2.7939730175865347]
	TIME [epoch: 6.77 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.877822807251883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.877822807251883 | validation: 3.0820502820109557]
	TIME [epoch: 6.76 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.12827503226961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.12827503226961 | validation: 3.312765454652273]
	TIME [epoch: 6.76 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.310536341595908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.310536341595908 | validation: 3.0983995405398446]
	TIME [epoch: 6.76 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.028261179945928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.028261179945928 | validation: 2.884505565869084]
	TIME [epoch: 6.79 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0378626313713295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0378626313713295 | validation: 3.000283617390921]
	TIME [epoch: 6.78 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2730820638965636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2730820638965636 | validation: 3.0132892744312274]
	TIME [epoch: 6.76 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0917241291993998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0917241291993998 | validation: 2.8918888970124677]
	TIME [epoch: 6.76 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.061376300797961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.061376300797961 | validation: 2.947930364673198]
	TIME [epoch: 6.76 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9288340567118656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9288340567118656 | validation: 2.731646636818989]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_48.pth
	Model improved!!!
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7592975676865796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7592975676865796 | validation: 2.8321071906706603]
	TIME [epoch: 6.76 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9924169991978697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9924169991978697 | validation: 6.57899849430399]
	TIME [epoch: 6.81 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.670295632940153		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 6.670295632940153 | validation: 7.028480446945704]
	TIME [epoch: 6.77 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.336285255063261		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 6.336285255063261 | validation: 5.527560546546309]
	TIME [epoch: 6.76 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7695133477677		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 4.7695133477677 | validation: 3.4525288191757553]
	TIME [epoch: 6.76 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4035351001372427		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 3.4035351001372427 | validation: 3.3460704210231986]
	TIME [epoch: 6.76 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.275521934223113		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.275521934223113 | validation: 3.062095063852014]
	TIME [epoch: 6.76 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.098746761298228		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 3.098746761298228 | validation: 2.9059755571340764]
	TIME [epoch: 6.76 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.951097495074303		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 2.951097495074303 | validation: 2.7650093459411753]
	TIME [epoch: 6.8 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7965012045306286		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 2.7965012045306286 | validation: 2.915612983926406]
	TIME [epoch: 6.77 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.76014589481481		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 2.76014589481481 | validation: 2.540592807738178]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6686367238689312		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 2.6686367238689312 | validation: 2.523089938514228]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.080924666786776		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 3.080924666786776 | validation: 3.1421150552960664]
	TIME [epoch: 6.76 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.865482436854807		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 2.865482436854807 | validation: 2.914345910529815]
	TIME [epoch: 6.76 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7088184625965406		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 2.7088184625965406 | validation: 2.618024519820742]
	TIME [epoch: 6.78 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8108953256573432		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 2.8108953256573432 | validation: 3.5524842817783657]
	TIME [epoch: 6.81 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8736454747968696		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 2.8736454747968696 | validation: 2.476987423159996]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.793478189907235		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 2.793478189907235 | validation: 2.8151625402199763]
	TIME [epoch: 6.77 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.67010105802193		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 2.67010105802193 | validation: 2.548639975122903]
	TIME [epoch: 6.76 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.566839073795631		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 2.566839073795631 | validation: 2.4698220203903603]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_68.pth
	Model improved!!!
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.410566888592119		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 2.410566888592119 | validation: 2.8618760177868077]
	TIME [epoch: 6.76 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9298138931982063		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 2.9298138931982063 | validation: 2.824371371643843]
	TIME [epoch: 6.8 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5860697001253143		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 2.5860697001253143 | validation: 2.4016574101181494]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_71.pth
	Model improved!!!
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.590724538903156		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 2.590724538903156 | validation: 3.43964909276678]
	TIME [epoch: 6.77 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9686433393692147		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 2.9686433393692147 | validation: 2.398461969619505]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_73.pth
	Model improved!!!
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.389732593171025		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 2.389732593171025 | validation: 2.2146057479902543]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3217416681476415		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 2.3217416681476415 | validation: 2.5442209031781506]
	TIME [epoch: 6.76 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.349840329106267		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 2.349840329106267 | validation: 2.2007364630919755]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_76.pth
	Model improved!!!
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0926724734161675		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 2.0926724734161675 | validation: 3.90654031482892]
	TIME [epoch: 6.81 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3532254927813065		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 3.3532254927813065 | validation: 2.701218025884219]
	TIME [epoch: 6.77 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5710433106610777		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 2.5710433106610777 | validation: 2.1565617425324337]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_79.pth
	Model improved!!!
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0328537256065036		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 2.0328537256065036 | validation: 3.4513059673319137]
	TIME [epoch: 6.77 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.618248996304363		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 3.618248996304363 | validation: 3.523644500365422]
	TIME [epoch: 6.76 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8474670104065116		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 2.8474670104065116 | validation: 2.5004304616312876]
	TIME [epoch: 6.76 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4793731262236833		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 2.4793731262236833 | validation: 2.171445715051128]
	TIME [epoch: 6.76 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.063972837450249		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 2.063972837450249 | validation: 1.9717443152110552]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_84.pth
	Model improved!!!
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9365610650132496		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.9365610650132496 | validation: 1.9393902074232614]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_85.pth
	Model improved!!!
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.995363404226286		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 1.995363404226286 | validation: 2.1305628266801033]
	TIME [epoch: 6.77 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2264252119168177		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 2.2264252119168177 | validation: 1.8277310467307641]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_87.pth
	Model improved!!!
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0406165438903434		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 2.0406165438903434 | validation: 1.8886345357804608]
	TIME [epoch: 6.77 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9674564395648733		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 1.9674564395648733 | validation: 1.7505692140350546]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_89.pth
	Model improved!!!
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9422824186628003		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.9422824186628003 | validation: 1.8251970460971394]
	TIME [epoch: 6.8 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9939063727672623		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 1.9939063727672623 | validation: 2.5643983680101963]
	TIME [epoch: 6.78 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9275521015932917		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.9275521015932917 | validation: 2.2249703563099943]
	TIME [epoch: 6.77 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7578368439864804		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 2.7578368439864804 | validation: 5.076560357098115]
	TIME [epoch: 6.76 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1383148300036834		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 3.1383148300036834 | validation: 2.069881139167114]
	TIME [epoch: 6.76 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.099337768703575		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 2.099337768703575 | validation: 2.329818874398306]
	TIME [epoch: 6.76 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1633073239199114		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 2.1633073239199114 | validation: 1.887789431499082]
	TIME [epoch: 6.76 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8823073291018337		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 1.8823073291018337 | validation: 1.9744988479772516]
	TIME [epoch: 6.8 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7646410415906508		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 1.7646410415906508 | validation: 1.9720290228245965]
	TIME [epoch: 6.78 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.002358280758399		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 2.002358280758399 | validation: 3.5969976444187086]
	TIME [epoch: 6.77 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3751447145083544		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 3.3751447145083544 | validation: 3.7890265867680153]
	TIME [epoch: 6.76 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9794456696480287		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 3.9794456696480287 | validation: 3.6313417214736368]
	TIME [epoch: 6.76 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7781290896602635		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 2.7781290896602635 | validation: 2.1797246699228126]
	TIME [epoch: 6.76 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0061802919805394		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 2.0061802919805394 | validation: 1.8593393943335736]
	TIME [epoch: 6.76 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.097297687350599		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 2.097297687350599 | validation: 2.0054313166463835]
	TIME [epoch: 6.8 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.919677188167863		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 1.919677188167863 | validation: 1.9525408084536369]
	TIME [epoch: 6.78 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.900200716100673		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 1.900200716100673 | validation: 2.0413002621268594]
	TIME [epoch: 6.77 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.363316963301787		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 2.363316963301787 | validation: 2.0607302639898286]
	TIME [epoch: 6.76 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.004655398585064		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 2.004655398585064 | validation: 2.1054527865979216]
	TIME [epoch: 6.76 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8211505761573725		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 1.8211505761573725 | validation: 2.0698267610330707]
	TIME [epoch: 6.76 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8731593057865061		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 1.8731593057865061 | validation: 1.784048998896119]
	TIME [epoch: 6.77 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.918752659296401		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 1.918752659296401 | validation: 1.7934865616065754]
	TIME [epoch: 6.81 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.689549590214885		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 1.689549590214885 | validation: 1.8265031789434065]
	TIME [epoch: 6.78 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9478085415078397		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 1.9478085415078397 | validation: 1.9275697813683192]
	TIME [epoch: 6.77 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7377551309725934		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 1.7377551309725934 | validation: 1.5551783493851898]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_114.pth
	Model improved!!!
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.589828976235077		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 1.589828976235077 | validation: 2.0305493795285154]
	TIME [epoch: 6.77 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2407944068203163		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 2.2407944068203163 | validation: 1.9741710935946295]
	TIME [epoch: 6.77 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7742973943162457		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 1.7742973943162457 | validation: 1.7479269041918686]
	TIME [epoch: 6.77 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7617182859188416		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 1.7617182859188416 | validation: 1.5883216291667588]
	TIME [epoch: 6.81 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.681025196362156		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 3.681025196362156 | validation: 4.431704061397507]
	TIME [epoch: 6.77 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6379026207559524		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 3.6379026207559524 | validation: 3.1836216009711094]
	TIME [epoch: 6.76 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.900749546359418		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 2.900749546359418 | validation: 3.015968549803377]
	TIME [epoch: 6.76 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.963198161420844		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 2.963198161420844 | validation: 3.124882572579204]
	TIME [epoch: 6.77 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.783945424010615		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 2.783945424010615 | validation: 1.8250334878834966]
	TIME [epoch: 6.77 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0114306467756653		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 2.0114306467756653 | validation: 2.0270678221560834]
	TIME [epoch: 6.78 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7318437662449968		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 1.7318437662449968 | validation: 1.9382064051541659]
	TIME [epoch: 6.81 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9030505391721857		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 1.9030505391721857 | validation: 1.8091264417067179]
	TIME [epoch: 6.77 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3353884421998345		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 2.3353884421998345 | validation: 3.6642188469416457]
	TIME [epoch: 6.77 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6323789527132524		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 2.6323789527132524 | validation: 1.9498164536286415]
	TIME [epoch: 6.76 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0442876313555463		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 2.0442876313555463 | validation: 1.9882320410165435]
	TIME [epoch: 6.76 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4256928220286014		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 2.4256928220286014 | validation: 3.296933342445215]
	TIME [epoch: 6.76 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.202150383146639		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 2.202150383146639 | validation: 1.910627253311846]
	TIME [epoch: 6.78 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7980747078299018		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 1.7980747078299018 | validation: 1.7560729599526508]
	TIME [epoch: 6.8 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6261938982847362		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 1.6261938982847362 | validation: 1.8834374956714806]
	TIME [epoch: 6.77 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6294587111553884		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 1.6294587111553884 | validation: 1.5323907759502524]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_134.pth
	Model improved!!!
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7190720309352083		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 2.7190720309352083 | validation: 3.686912597937747]
	TIME [epoch: 6.76 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1894548612706037		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 3.1894548612706037 | validation: 1.8361465524920157]
	TIME [epoch: 6.76 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7947492255921802		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 1.7947492255921802 | validation: 1.9903542405927075]
	TIME [epoch: 6.76 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7695875015511804		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 1.7695875015511804 | validation: 1.75033429675553]
	TIME [epoch: 6.79 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8129142887172036		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 1.8129142887172036 | validation: 1.9307283852092705]
	TIME [epoch: 6.78 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6991090067883219		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 1.6991090067883219 | validation: 1.580044479347174]
	TIME [epoch: 6.76 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4437680590300257		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 1.4437680590300257 | validation: 1.5090756340698515]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_141.pth
	Model improved!!!
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6378165474374584		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 1.6378165474374584 | validation: 1.6477268612807803]
	TIME [epoch: 6.77 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.401576187445755		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 1.401576187445755 | validation: 3.34847681526512]
	TIME [epoch: 6.77 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8769086942728042		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 1.8769086942728042 | validation: 1.554398840539823]
	TIME [epoch: 6.76 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4739038707002157		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 1.4739038707002157 | validation: 1.4938296482206659]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_145.pth
	Model improved!!!
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9396586062947816		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 1.9396586062947816 | validation: 1.57598532681199]
	TIME [epoch: 6.78 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5773100026610454		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 1.5773100026610454 | validation: 1.5336670900182772]
	TIME [epoch: 6.77 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3131333397334775		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 1.3131333397334775 | validation: 1.8129316459133151]
	TIME [epoch: 6.77 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7697236947103234		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 1.7697236947103234 | validation: 2.489577879613236]
	TIME [epoch: 6.77 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5851196814663349		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 1.5851196814663349 | validation: 1.4706412059549239]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_150.pth
	Model improved!!!
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.412818618220062		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 1.412818618220062 | validation: 1.5437639600168953]
	TIME [epoch: 6.77 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5454982342365369		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 1.5454982342365369 | validation: 2.0862676184283293]
	TIME [epoch: 6.81 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.005189022629869		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 2.005189022629869 | validation: 1.5950765026750313]
	TIME [epoch: 6.77 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4592880834884432		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 1.4592880834884432 | validation: 1.9208201635904898]
	TIME [epoch: 6.76 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.927373788144518		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 1.927373788144518 | validation: 3.252590877026112]
	TIME [epoch: 6.76 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6947955885889017		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 2.6947955885889017 | validation: 3.631899599494571]
	TIME [epoch: 6.76 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0079912428822997		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 2.0079912428822997 | validation: 1.8854520306886857]
	TIME [epoch: 6.76 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3940609167406044		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 1.3940609167406044 | validation: 1.5529705879714644]
	TIME [epoch: 6.76 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5312031176489138		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 1.5312031176489138 | validation: 1.5553460318281966]
	TIME [epoch: 6.81 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.506965820008554		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 1.506965820008554 | validation: 1.4146395616658118]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_160.pth
	Model improved!!!
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2065117984982914		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 1.2065117984982914 | validation: 2.1271742684792914]
	TIME [epoch: 6.77 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5801165004345243		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 1.5801165004345243 | validation: 2.3690163449550736]
	TIME [epoch: 6.76 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8635289695531694		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 1.8635289695531694 | validation: 1.484866125213486]
	TIME [epoch: 6.76 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.437464141799393		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 1.437464141799393 | validation: 1.682531817009278]
	TIME [epoch: 6.76 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4767299754362304		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 1.4767299754362304 | validation: 1.5863080045509297]
	TIME [epoch: 6.77 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6315059421506464		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 1.6315059421506464 | validation: 1.9851691492851584]
	TIME [epoch: 6.8 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7001628707428986		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 1.7001628707428986 | validation: 1.5609529309579433]
	TIME [epoch: 6.77 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3709765166131667		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 1.3709765166131667 | validation: 1.4611385372440528]
	TIME [epoch: 6.76 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6486183775511467		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 1.6486183775511467 | validation: 3.1130555675941327]
	TIME [epoch: 6.76 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6151113319536465		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 2.6151113319536465 | validation: 3.850027045478244]
	TIME [epoch: 6.76 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0606642399428026		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 3.0606642399428026 | validation: 3.5043630127824317]
	TIME [epoch: 6.76 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5254509484631584		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 2.5254509484631584 | validation: 1.7709873148578739]
	TIME [epoch: 6.79 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.610575107851909		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 1.610575107851909 | validation: 1.573083585767062]
	TIME [epoch: 6.79 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.525893831560984		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 1.525893831560984 | validation: 1.4471020780507613]
	TIME [epoch: 6.77 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3623660996655393		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 1.3623660996655393 | validation: 1.4171553878355563]
	TIME [epoch: 6.76 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2976412502218144		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 1.2976412502218144 | validation: 1.917611488772346]
	TIME [epoch: 6.76 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5060569805292985		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 1.5060569805292985 | validation: 1.946742489517305]
	TIME [epoch: 6.76 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8006963183528533		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 1.8006963183528533 | validation: 2.3948081799715695]
	TIME [epoch: 6.77 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.082046288426863		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 2.082046288426863 | validation: 1.5614662525481757]
	TIME [epoch: 6.8 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4707250544939001		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 1.4707250544939001 | validation: 1.821499094518462]
	TIME [epoch: 6.78 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5840064176657218		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 1.5840064176657218 | validation: 1.7432002979538483]
	TIME [epoch: 6.76 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9362976560577232		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 1.9362976560577232 | validation: 2.710973153901768]
	TIME [epoch: 6.76 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2136870706099905		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 2.2136870706099905 | validation: 1.7343516879362895]
	TIME [epoch: 6.77 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.951991412472684		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 1.951991412472684 | validation: 2.437388150768309]
	TIME [epoch: 6.77 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.220079302370615		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 2.220079302370615 | validation: 2.0055677294999406]
	TIME [epoch: 6.77 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7368336358837424		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 1.7368336358837424 | validation: 1.686526967089756]
	TIME [epoch: 6.81 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4322346320407768		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 1.4322346320407768 | validation: 1.5206031703835128]
	TIME [epoch: 6.78 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4712199470075675		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 1.4712199470075675 | validation: 1.453604271701547]
	TIME [epoch: 6.77 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3811271423646367		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 1.3811271423646367 | validation: 1.4720890395021717]
	TIME [epoch: 6.76 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.588633540419297		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 1.588633540419297 | validation: 1.9879224214343516]
	TIME [epoch: 6.76 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4652481868187481		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 1.4652481868187481 | validation: 1.54883020760276]
	TIME [epoch: 6.77 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2686572086366446		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 1.2686572086366446 | validation: 1.4101525870244154]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_192.pth
	Model improved!!!
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9545284638475713		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 1.9545284638475713 | validation: 2.666594874295354]
	TIME [epoch: 6.8 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.065089604841469		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 2.065089604841469 | validation: 1.8448891639084055]
	TIME [epoch: 6.77 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.494441169606731		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 1.494441169606731 | validation: 2.118662310944863]
	TIME [epoch: 6.76 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4411167613330296		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 2.4411167613330296 | validation: 3.1375621966032514]
	TIME [epoch: 6.76 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3528083629818846		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 2.3528083629818846 | validation: 2.047281485606856]
	TIME [epoch: 6.76 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5463357748018858		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 1.5463357748018858 | validation: 1.5210509128313108]
	TIME [epoch: 6.76 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2492763350184743		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 1.2492763350184743 | validation: 1.3825968821574688]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_199.pth
	Model improved!!!
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2221239687533396		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 1.2221239687533396 | validation: 1.5347767269925234]
	TIME [epoch: 6.81 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3391046368433865		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 1.3391046368433865 | validation: 1.4694756259350643]
	TIME [epoch: 6.77 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.81417973085819		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 1.81417973085819 | validation: 1.5642304872248562]
	TIME [epoch: 6.77 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3282441405095196		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 1.3282441405095196 | validation: 1.4826547595235084]
	TIME [epoch: 6.76 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.284683689197906		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 1.284683689197906 | validation: 2.38413685700804]
	TIME [epoch: 6.76 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.444125573501907		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 1.444125573501907 | validation: 3.605689519734481]
	TIME [epoch: 6.77 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0037996088251755		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 3.0037996088251755 | validation: 2.9385244865770983]
	TIME [epoch: 6.77 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7980446344213323		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 2.7980446344213323 | validation: 2.206082341752227]
	TIME [epoch: 6.8 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0722718370326594		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 2.0722718370326594 | validation: 1.8081408606213722]
	TIME [epoch: 6.77 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.52429592131527		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 1.52429592131527 | validation: 1.7661912421415529]
	TIME [epoch: 6.76 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5697131781499212		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 1.5697131781499212 | validation: 1.4801268284324487]
	TIME [epoch: 6.76 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3213572000906317		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 1.3213572000906317 | validation: 1.4110020058729105]
	TIME [epoch: 6.76 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.231064631308767		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 1.231064631308767 | validation: 1.507460212036877]
	TIME [epoch: 6.76 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2377941313869796		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 1.2377941313869796 | validation: 1.6626233902036205]
	TIME [epoch: 6.77 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.382823082408212		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 1.382823082408212 | validation: 1.4852598413401465]
	TIME [epoch: 6.81 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2433667886661537		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 1.2433667886661537 | validation: 1.6047783214778262]
	TIME [epoch: 6.77 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2348600585718748		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 1.2348600585718748 | validation: 1.4977165233024006]
	TIME [epoch: 6.77 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2066192013580443		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 1.2066192013580443 | validation: 1.6784874594230352]
	TIME [epoch: 6.77 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4356799880659832		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 1.4356799880659832 | validation: 4.567700165864517]
	TIME [epoch: 6.76 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.963253557737997		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 3.963253557737997 | validation: 4.246045277693258]
	TIME [epoch: 6.76 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.26645522526868		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 3.26645522526868 | validation: 3.381592042965842]
	TIME [epoch: 6.8 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6753001070407527		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 2.6753001070407527 | validation: 3.1992982176002687]
	TIME [epoch: 6.78 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.612145002648168		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 2.612145002648168 | validation: 3.42870906106866]
	TIME [epoch: 6.77 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.278190221418182		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 3.278190221418182 | validation: 2.2966005409719212]
	TIME [epoch: 6.77 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6256327985233225		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 1.6256327985233225 | validation: 1.51655838825435]
	TIME [epoch: 6.76 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3036100315699448		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 1.3036100315699448 | validation: 1.380028007104261]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_225.pth
	Model improved!!!
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3253966327813504		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 1.3253966327813504 | validation: 1.3116476364145253]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_226.pth
	Model improved!!!
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2989013444346247		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 1.2989013444346247 | validation: 1.4735720905405667]
	TIME [epoch: 6.8 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3441317405878177		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 1.3441317405878177 | validation: 1.3878641039577886]
	TIME [epoch: 6.77 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1495539845492286		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 1.1495539845492286 | validation: 1.6466986160811747]
	TIME [epoch: 6.76 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2088196413110608		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 1.2088196413110608 | validation: 1.2730374783759475]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_230.pth
	Model improved!!!
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1095677030149345		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 1.1095677030149345 | validation: 1.7065017614672602]
	TIME [epoch: 6.76 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2047805270374672		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 1.2047805270374672 | validation: 1.2948406893112403]
	TIME [epoch: 6.77 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6538883649611753		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 1.6538883649611753 | validation: 2.8075159991450525]
	TIME [epoch: 6.77 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.631704706862313		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 2.631704706862313 | validation: 3.140395101021144]
	TIME [epoch: 6.8 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.349867008402641		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 2.349867008402641 | validation: 3.094175307488004]
	TIME [epoch: 6.77 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.381921470090468		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 2.381921470090468 | validation: 2.6826758363578422]
	TIME [epoch: 6.76 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6438159942185093		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 2.6438159942185093 | validation: 2.7409857122338837]
	TIME [epoch: 6.76 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.111427464388944		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 2.111427464388944 | validation: 2.0676223995142813]
	TIME [epoch: 6.76 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6065688934074065		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 1.6065688934074065 | validation: 1.6287897852445405]
	TIME [epoch: 6.76 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2236699765636885		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 1.2236699765636885 | validation: 1.3645540732581285]
	TIME [epoch: 6.76 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2174600796548412		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 1.2174600796548412 | validation: 1.2424675382841754]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_241.pth
	Model improved!!!
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4384384337760467		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 1.4384384337760467 | validation: 1.3568863279019445]
	TIME [epoch: 6.77 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.083283453666918		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 1.083283453666918 | validation: 1.7295953560618627]
	TIME [epoch: 6.76 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4458621758557066		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 1.4458621758557066 | validation: 1.6311740628611733]
	TIME [epoch: 6.77 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2606451833822345		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 1.2606451833822345 | validation: 2.0170635753139923]
	TIME [epoch: 6.77 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2642389259855917		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 1.2642389259855917 | validation: 1.2163098539545194]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_246.pth
	Model improved!!!
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0837409507764393		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 1.0837409507764393 | validation: 1.2849886735687757]
	TIME [epoch: 6.77 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4941445919999894		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 1.4941445919999894 | validation: 1.4132854233672238]
	TIME [epoch: 6.81 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8557658287447687		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 1.8557658287447687 | validation: 1.5294191520818838]
	TIME [epoch: 6.77 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2585487348249218		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 1.2585487348249218 | validation: 1.4134580562092323]
	TIME [epoch: 6.76 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1284741859626952		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 1.1284741859626952 | validation: 1.4058856165621751]
	TIME [epoch: 6.77 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0195803782652457		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 2.0195803782652457 | validation: 1.4600030035055966]
	TIME [epoch: 6.77 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3680156060770468		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 1.3680156060770468 | validation: 1.5771670751403097]
	TIME [epoch: 6.76 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.198667477608284		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 1.198667477608284 | validation: 1.7954302018777597]
	TIME [epoch: 6.79 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2283848845259415		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 1.2283848845259415 | validation: 1.5429785883437614]
	TIME [epoch: 6.8 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3975153921316674		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 1.3975153921316674 | validation: 1.509438659157336]
	TIME [epoch: 6.78 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4631891359527538		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 1.4631891359527538 | validation: 3.097778697023763]
	TIME [epoch: 6.76 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.124122570533604		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 2.124122570533604 | validation: 1.5345165770812013]
	TIME [epoch: 6.77 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3765104834348902		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 1.3765104834348902 | validation: 1.4758520089192795]
	TIME [epoch: 6.76 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2164333049544198		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 1.2164333049544198 | validation: 1.3843026488147183]
	TIME [epoch: 6.77 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1027091254360155		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 1.1027091254360155 | validation: 1.3070168050940087]
	TIME [epoch: 6.8 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2229880379882703		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 1.2229880379882703 | validation: 1.468897356757768]
	TIME [epoch: 6.79 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1516537173990153		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 1.1516537173990153 | validation: 1.2805780128032116]
	TIME [epoch: 6.76 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1536703644625759		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 1.1536703644625759 | validation: 1.4043824786591712]
	TIME [epoch: 6.76 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0163093353548671		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 1.0163093353548671 | validation: 1.7648441744470593]
	TIME [epoch: 6.76 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3995004010873824		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 1.3995004010873824 | validation: 2.244858499013243]
	TIME [epoch: 6.77 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.342998861917565		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 1.342998861917565 | validation: 1.6131572662283293]
	TIME [epoch: 6.77 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2697684935718423		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 1.2697684935718423 | validation: 1.4060756858442056]
	TIME [epoch: 6.8 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1437482454330896		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 1.1437482454330896 | validation: 1.5968052154398051]
	TIME [epoch: 6.78 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.462632165126542		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 1.462632165126542 | validation: 1.7285199519495191]
	TIME [epoch: 6.77 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6117884039635422		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 1.6117884039635422 | validation: 1.5075008188574246]
	TIME [epoch: 6.77 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.130364101255863		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 1.130364101255863 | validation: 1.315445682006231]
	TIME [epoch: 6.76 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1568779264664215		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 1.1568779264664215 | validation: 1.3962014380062102]
	TIME [epoch: 6.77 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1374375041407419		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 1.1374375041407419 | validation: 1.229681303000405]
	TIME [epoch: 6.76 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0733664662227653		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 1.0733664662227653 | validation: 1.562502750822409]
	TIME [epoch: 6.81 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1647389717113346		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 1.1647389717113346 | validation: 1.3417319547819893]
	TIME [epoch: 6.78 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2041052439624327		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 1.2041052439624327 | validation: 1.3694035277900714]
	TIME [epoch: 6.77 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1170283440014002		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 1.1170283440014002 | validation: 1.58620542022823]
	TIME [epoch: 6.76 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2327429376635282		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 1.2327429376635282 | validation: 1.435792300379495]
	TIME [epoch: 6.77 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2253857464470115		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 1.2253857464470115 | validation: 1.253124108270733]
	TIME [epoch: 6.75 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0823732846189185		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 1.0823732846189185 | validation: 1.6815738797422028]
	TIME [epoch: 6.77 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2765848755505291		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 1.2765848755505291 | validation: 1.5241256118071447]
	TIME [epoch: 6.81 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1108732277032347		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 1.1108732277032347 | validation: 1.5038626842221747]
	TIME [epoch: 6.78 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.009040347299709		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 1.009040347299709 | validation: 1.2213749865412722]
	TIME [epoch: 6.76 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1424998443362426		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 1.1424998443362426 | validation: 1.2979962179335471]
	TIME [epoch: 6.77 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0844433325928742		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 1.0844433325928742 | validation: 1.3229583440166715]
	TIME [epoch: 6.76 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1220915193907584		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 1.1220915193907584 | validation: 1.4345811846848122]
	TIME [epoch: 6.76 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0749805162625974		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 1.0749805162625974 | validation: 1.1714766967595212]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_288.pth
	Model improved!!!
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9208963615003101		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.9208963615003101 | validation: 1.2732133061768254]
	TIME [epoch: 6.83 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1336904543890354		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 1.1336904543890354 | validation: 1.1914885536598434]
	TIME [epoch: 6.77 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9225122988819996		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.9225122988819996 | validation: 1.2123048365321711]
	TIME [epoch: 6.76 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1298063255554456		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 1.1298063255554456 | validation: 1.3276185578299873]
	TIME [epoch: 6.76 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0450342485533426		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 1.0450342485533426 | validation: 1.2554403794627582]
	TIME [epoch: 6.76 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.076839194954838		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 1.076839194954838 | validation: 1.5733589000250887]
	TIME [epoch: 6.76 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1142848612976737		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 1.1142848612976737 | validation: 1.3295552094530776]
	TIME [epoch: 6.77 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9706767343599566		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.9706767343599566 | validation: 1.218686582482004]
	TIME [epoch: 6.8 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.067591895214394		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 1.067591895214394 | validation: 1.423183004427273]
	TIME [epoch: 6.76 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9445989376062578		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.9445989376062578 | validation: 1.188425455573907]
	TIME [epoch: 6.76 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.956765548707092		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.956765548707092 | validation: 1.1020427763425156]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_299.pth
	Model improved!!!
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0955529962309543		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 1.0955529962309543 | validation: 1.3003122358204386]
	TIME [epoch: 6.77 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.127816316514447		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 1.127816316514447 | validation: 1.2550092148025338]
	TIME [epoch: 6.76 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.077350792266584		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 1.077350792266584 | validation: 1.2231858929546802]
	TIME [epoch: 6.8 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1262610225174505		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 1.1262610225174505 | validation: 1.1620424525284583]
	TIME [epoch: 6.79 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0646393425698888		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 1.0646393425698888 | validation: 1.4324675401624698]
	TIME [epoch: 6.77 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0942981946975914		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 1.0942981946975914 | validation: 1.199364425009539]
	TIME [epoch: 6.76 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9710570524829871		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.9710570524829871 | validation: 1.2419971838100785]
	TIME [epoch: 6.77 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9943487775383302		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.9943487775383302 | validation: 1.083528039446605]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_307.pth
	Model improved!!!
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9874768759722623		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.9874768759722623 | validation: 1.3721784985012793]
	TIME [epoch: 6.76 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0141472598276766		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 1.0141472598276766 | validation: 1.0813446558313347]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240504_113908/states/model_phiq_1a_v2_309.pth
	Model improved!!!
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8847642384969715		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.8847642384969715 | validation: 1.1953997096988709]
	TIME [epoch: 6.78 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.01166395998834		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 1.01166395998834 | validation: 1.7598142593622312]
	TIME [epoch: 6.76 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.917926051588359		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 1.917926051588359 | validation: 2.7027672056064134]
	TIME [epoch: 6.76 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9379809411214381		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 1.9379809411214381 | validation: 1.9947772450294865]
	TIME [epoch: 6.76 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8916740107024221		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 1.8916740107024221 | validation: 1.8854719646889495]
	TIME [epoch: 6.75 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8676053228447773		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 1.8676053228447773 | validation: 1.8972718575644554]
	TIME [epoch: 6.76 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7848199786528456		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 1.7848199786528456 | validation: 1.9844411241137165]
	TIME [epoch: 6.79 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549865678058093		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 1.8549865678058093 | validation: 1.9658309974345523]
	TIME [epoch: 6.76 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8452419622825724		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 1.8452419622825724 | validation: 2.035804232635879]
	TIME [epoch: 6.76 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.97276934322813		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 1.97276934322813 | validation: 2.036112677717677]
	TIME [epoch: 6.76 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9476383667226551		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 1.9476383667226551 | validation: 2.013424048664258]
	TIME [epoch: 6.76 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1028501824866948		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 2.1028501824866948 | validation: 2.2026964084442753]
	TIME [epoch: 6.75 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.399145099127808		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 2.399145099127808 | validation: 2.6064698857945734]
	TIME [epoch: 6.76 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6745394784188585		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 2.6745394784188585 | validation: 2.495508124647572]
	TIME [epoch: 6.81 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.560252548498622		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 2.560252548498622 | validation: 2.744031684009378]
	TIME [epoch: 6.76 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.606880381866155		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 2.606880381866155 | validation: 2.546325649140833]
	TIME [epoch: 6.76 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.501474179262848		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 2.501474179262848 | validation: 2.975468522145773]
	TIME [epoch: 6.76 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.722609719671851		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 2.722609719671851 | validation: 2.65814036763685]
	TIME [epoch: 6.76 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2555636828178525		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 2.2555636828178525 | validation: 2.191409028134378]
	TIME [epoch: 6.76 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9565143036907835		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 1.9565143036907835 | validation: 2.0160910223142388]
	TIME [epoch: 6.77 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8154912740154527		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 1.8154912740154527 | validation: 2.057146749661143]
	TIME [epoch: 6.81 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.983942093083561		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 1.983942093083561 | validation: 1.9651659584505294]
	TIME [epoch: 6.76 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8349705317035936		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 1.8349705317035936 | validation: 1.7738293598953128]
	TIME [epoch: 6.76 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6898184531268154		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 1.6898184531268154 | validation: 2.009060171202896]
	TIME [epoch: 6.76 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7060208588464099		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 1.7060208588464099 | validation: 2.223317882884944]
	TIME [epoch: 6.76 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8925797728037788		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 1.8925797728037788 | validation: 2.545924371142794]
	TIME [epoch: 6.76 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3670703371350754		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 2.3670703371350754 | validation: 2.74411046664785]
	TIME [epoch: 6.77 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.686069513292733		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 2.686069513292733 | validation: 3.113756145036853]
	TIME [epoch: 6.8 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4499297908866344		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 3.4499297908866344 | validation: 3.9783826181492623]
	TIME [epoch: 6.76 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.383238901544542		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 4.383238901544542 | validation: 4.246854309278096]
	TIME [epoch: 6.76 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.611195092673097		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 4.611195092673097 | validation: 4.557188097085904]
	TIME [epoch: 6.76 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.939673850112723		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 4.939673850112723 | validation: 4.7356238259357974]
	TIME [epoch: 6.76 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.000150376317347		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 5.000150376317347 | validation: 4.714127610345971]
	TIME [epoch: 6.77 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.193713711398464		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 5.193713711398464 | validation: 4.972363035610815]
	TIME [epoch: 6.79 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.3975418282916054		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 5.3975418282916054 | validation: 5.396092970521227]
	TIME [epoch: 6.8 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.425370780352523		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 5.425370780352523 | validation: 4.888333151499387]
	TIME [epoch: 6.77 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.143495797852179		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 5.143495797852179 | validation: 4.377599232023556]
	TIME [epoch: 6.77 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.829912605956135		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 4.829912605956135 | validation: 3.7388383850123565]
	TIME [epoch: 6.77 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.475050736835239		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 4.475050736835239 | validation: 3.798657188668773]
	TIME [epoch: 6.76 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.937220198094672		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 3.937220198094672 | validation: 3.051060772594604]
	TIME [epoch: 6.77 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.247696816363958		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 3.247696816363958 | validation: 2.094056084702331]
	TIME [epoch: 6.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.835266756630186		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 1.835266756630186 | validation: 1.7932850342316815]
	TIME [epoch: 6.78 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5268442311118309		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 1.5268442311118309 | validation: 1.697932121863057]
	TIME [epoch: 6.76 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6920073374094862		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 1.6920073374094862 | validation: 2.0517505380246925]
	TIME [epoch: 6.77 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5736017861777256		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 1.5736017861777256 | validation: 1.9460924407673417]
	TIME [epoch: 6.76 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4570880932309787		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 1.4570880932309787 | validation: 2.0371766723581968]
	TIME [epoch: 6.76 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7362341514660633		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 1.7362341514660633 | validation: 2.430954288195057]
	TIME [epoch: 6.76 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.241186604667369		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 2.241186604667369 | validation: 3.3176235386948463]
	TIME [epoch: 6.81 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.817500355960273		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 2.817500355960273 | validation: 3.6465550866371412]
	TIME [epoch: 6.78 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.023451768518516		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 3.023451768518516 | validation: 3.4724889494193416]
	TIME [epoch: 6.77 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.000799968608346		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 3.000799968608346 | validation: 3.65504311728291]
	TIME [epoch: 6.76 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.194989841590352		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 3.194989841590352 | validation: 3.899045061608895]
	TIME [epoch: 6.76 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.455433234514144		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 3.455433234514144 | validation: 3.996208480031206]
	TIME [epoch: 6.76 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6182882690947595		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 3.6182882690947595 | validation: 4.093129534777198]
	TIME [epoch: 6.76 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.680099243962189		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 3.680099243962189 | validation: 4.261642949789287]
	TIME [epoch: 6.8 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7330637663359827		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 3.7330637663359827 | validation: 4.255712976643686]
	TIME [epoch: 6.77 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.167226021152437		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 4.167226021152437 | validation: 4.905959924204274]
	TIME [epoch: 6.77 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4363658598294435		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 4.4363658598294435 | validation: 4.8601641141779215]
	TIME [epoch: 6.76 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.295324374158599		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 4.295324374158599 | validation: 4.649218793372462]
	TIME [epoch: 6.76 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.203312225295464		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 4.203312225295464 | validation: 4.778044797840886]
	TIME [epoch: 6.76 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4123590258110825		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 4.4123590258110825 | validation: 4.909311276626365]
	TIME [epoch: 6.76 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.450407514628336		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 4.450407514628336 | validation: 4.800522495956217]
	TIME [epoch: 6.8 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.339201273550412		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 4.339201273550412 | validation: 4.717064368838905]
	TIME [epoch: 6.76 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.516778295303035		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 4.516778295303035 | validation: 4.834406854968432]
	TIME [epoch: 6.76 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7406388630941425		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 4.7406388630941425 | validation: 4.994983810778951]
	TIME [epoch: 6.76 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.880776909877168		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 4.880776909877168 | validation: 4.897817403769077]
	TIME [epoch: 6.76 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6761741838516775		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 4.6761741838516775 | validation: 4.824829913556917]
	TIME [epoch: 6.76 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3778296260851475		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 4.3778296260851475 | validation: 4.744310457800742]
	TIME [epoch: 6.76 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.278391873574521		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 4.278391873574521 | validation: 4.113892330784932]
	TIME [epoch: 6.81 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8370650999392732		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 3.8370650999392732 | validation: 4.330830820683822]
	TIME [epoch: 6.77 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9361673959644117		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 3.9361673959644117 | validation: 4.055662625175838]
	TIME [epoch: 6.76 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.673228718486107		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 3.673228718486107 | validation: 3.970680813063839]
	TIME [epoch: 6.76 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.745921506588482		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 3.745921506588482 | validation: 4.197897482775211]
	TIME [epoch: 6.76 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.321033689147962		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 4.321033689147962 | validation: 4.8366799641438964]
	TIME [epoch: 6.76 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.691957898815009		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 4.691957898815009 | validation: 4.780539815818299]
	TIME [epoch: 6.77 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.531866161763048		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 4.531866161763048 | validation: 4.771898923959897]
	TIME [epoch: 6.8 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.429439998476475		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 4.429439998476475 | validation: 4.4061487183764125]
	TIME [epoch: 6.76 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.290148497209618		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 4.290148497209618 | validation: 4.559049541783457]
	TIME [epoch: 6.76 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.26925310507426		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 4.26925310507426 | validation: 4.818866870605094]
	TIME [epoch: 6.77 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.687305738825884		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 4.687305738825884 | validation: 5.051933142988973]
	TIME [epoch: 6.76 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.53970070655828		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 4.53970070655828 | validation: 4.791794495300712]
	TIME [epoch: 6.76 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.597339623375193		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 4.597339623375193 | validation: 4.717258825737228]
	TIME [epoch: 6.77 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.398482429132914		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 4.398482429132914 | validation: 4.364969415484968]
	TIME [epoch: 6.8 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.276936851968729		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 4.276936851968729 | validation: 4.639450086769146]
	TIME [epoch: 6.76 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.643050545437963		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 4.643050545437963 | validation: 4.916069618818078]
	TIME [epoch: 6.76 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.552122905254348		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 4.552122905254348 | validation: 5.153442032198622]
	TIME [epoch: 6.76 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.648795776414392		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 4.648795776414392 | validation: 5.181379833072423]
	TIME [epoch: 6.76 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4793742465244755		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 4.4793742465244755 | validation: 5.028812796261576]
	TIME [epoch: 6.76 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.336847779823419		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 4.336847779823419 | validation: 4.841640664399728]
	TIME [epoch: 6.78 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.219572177600583		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 4.219572177600583 | validation: 4.687527105562154]
	TIME [epoch: 6.79 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.188254479984023		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 4.188254479984023 | validation: 4.606885034319861]
	TIME [epoch: 6.76 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9738690621765276		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 3.9738690621765276 | validation: 4.061379154825094]
	TIME [epoch: 6.76 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7760341757958016		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 3.7760341757958016 | validation: 4.143342051463705]
	TIME [epoch: 6.76 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.872679032743286		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 3.872679032743286 | validation: 4.302047132230408]
	TIME [epoch: 6.76 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.758496776394882		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 3.758496776394882 | validation: 3.835368653023071]
	TIME [epoch: 6.76 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.586481411240638		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 3.586481411240638 | validation: 3.8331618395915967]
	TIME [epoch: 6.79 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.556423539655537		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 3.556423539655537 | validation: 3.9817933339483513]
	TIME [epoch: 6.78 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5330900928277105		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 3.5330900928277105 | validation: 4.499702976419754]
	TIME [epoch: 6.77 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6677710929036227		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 3.6677710929036227 | validation: 4.38159577175938]
	TIME [epoch: 6.76 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.627447630064757		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 3.627447630064757 | validation: 4.077281195830345]
	TIME [epoch: 6.76 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.533352198471465		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 3.533352198471465 | validation: 4.07798028614738]
	TIME [epoch: 6.76 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.669034762432212		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 3.669034762432212 | validation: 4.086990695591526]
	TIME [epoch: 6.76 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7516825564682024		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 3.7516825564682024 | validation: 3.928514878562188]
	TIME [epoch: 6.8 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5472731027150415		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 3.5472731027150415 | validation: 3.6994277509503335]
	TIME [epoch: 6.77 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.435784954736315		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 3.435784954736315 | validation: 3.7881376922079313]
	TIME [epoch: 6.77 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6176216855740067		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 3.6176216855740067 | validation: 4.110651768617542]
	TIME [epoch: 6.76 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.79199715708581		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 3.79199715708581 | validation: 4.382288132107791]
	TIME [epoch: 6.75 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.869963149960452		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 3.869963149960452 | validation: 4.298200948645109]
	TIME [epoch: 6.76 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6662913526777023		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 3.6662913526777023 | validation: 4.266363120861483]
	TIME [epoch: 6.76 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.636190326616276		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 3.636190326616276 | validation: 4.318402974281303]
	TIME [epoch: 6.8 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.695113240192943		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 3.695113240192943 | validation: 4.370437692553538]
	TIME [epoch: 6.77 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6159947687745957		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 3.6159947687745957 | validation: 4.10211422183391]
	TIME [epoch: 6.76 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3920395851548264		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 3.3920395851548264 | validation: 4.119781178746736]
	TIME [epoch: 6.76 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.402691278422945		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 3.402691278422945 | validation: 4.176284019673237]
	TIME [epoch: 6.76 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.335161455933079		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 3.335161455933079 | validation: 4.006128445928339]
	TIME [epoch: 6.76 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.329144007248786		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 3.329144007248786 | validation: 3.9051811706427477]
	TIME [epoch: 6.77 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2321058294852896		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 3.2321058294852896 | validation: 3.8509178218584577]
	TIME [epoch: 6.8 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1377647051892197		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 3.1377647051892197 | validation: 3.7067483931049576]
	TIME [epoch: 6.77 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0544178807268207		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 3.0544178807268207 | validation: 3.696495742687854]
	TIME [epoch: 6.76 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9800712796948376		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 2.9800712796948376 | validation: 3.6967081958754635]
	TIME [epoch: 6.76 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9682870438134237		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 2.9682870438134237 | validation: 3.6238684252477693]
	TIME [epoch: 6.76 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8816694486566385		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 2.8816694486566385 | validation: 3.5126994366653896]
	TIME [epoch: 6.75 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7824555775833693		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 2.7824555775833693 | validation: 3.390618452542996]
	TIME [epoch: 6.76 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6705565288559554		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 2.6705565288559554 | validation: 3.405066199014218]
	TIME [epoch: 6.8 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6624992546961233		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 2.6624992546961233 | validation: 3.3547244058327808]
	TIME [epoch: 6.77 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.685047356498694		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 2.685047356498694 | validation: 3.505839970208233]
	TIME [epoch: 6.76 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.695554665494648		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 2.695554665494648 | validation: 3.4988797161501]
	TIME [epoch: 6.76 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6432991655721736		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 2.6432991655721736 | validation: 3.410893994585846]
	TIME [epoch: 6.76 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6666240981661673		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 2.6666240981661673 | validation: 3.5234617030555935]
	TIME [epoch: 6.76 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.73225018256528		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 2.73225018256528 | validation: 3.4850707119830835]
	TIME [epoch: 6.77 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.852914143476994		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 2.852914143476994 | validation: 4.0246229010944665]
	TIME [epoch: 6.81 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1458516506072893		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 3.1458516506072893 | validation: 4.180760921870548]
	TIME [epoch: 6.77 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2880013635786227		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 3.2880013635786227 | validation: 4.587586006438895]
	TIME [epoch: 6.76 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.450267986068231		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 3.450267986068231 | validation: 4.567157550436152]
	TIME [epoch: 6.75 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4738556375042933		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 3.4738556375042933 | validation: 4.48116443843904]
	TIME [epoch: 6.76 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4630953357653844		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 3.4630953357653844 | validation: 4.712911229550856]
	TIME [epoch: 6.75 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6400443822257578		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 3.6400443822257578 | validation: 4.713682321229148]
	TIME [epoch: 6.77 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5987853170419317		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 3.5987853170419317 | validation: 4.588871997670675]
	TIME [epoch: 6.8 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.488629204136651		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 3.488629204136651 | validation: 4.727775005631289]
	TIME [epoch: 6.76 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5614374406471287		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 3.5614374406471287 | validation: 4.7791686301942224]
	TIME [epoch: 6.76 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.483965393537774		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 3.483965393537774 | validation: 4.514773005231911]
	TIME [epoch: 6.76 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.424743823078116		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 3.424743823078116 | validation: 4.467449239452083]
	TIME [epoch: 6.76 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.308866088410797		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 3.308866088410797 | validation: 4.453635758446134]
	TIME [epoch: 6.76 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3309855613349617		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 3.3309855613349617 | validation: 4.471356324554672]
	TIME [epoch: 6.79 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3077017968562683		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.3077017968562683 | validation: 4.5714100802454]
	TIME [epoch: 6.77 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4535409696277743		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 3.4535409696277743 | validation: 4.508385090830526]
	TIME [epoch: 6.76 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5164008764530097		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 3.5164008764530097 | validation: 4.633336789189659]
	TIME [epoch: 6.75 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4925172811852576		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 3.4925172811852576 | validation: 4.7107996837380375]
	TIME [epoch: 6.75 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.530610048968372		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 3.530610048968372 | validation: 4.754547042400481]
	TIME [epoch: 6.76 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5039620662074054		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 3.5039620662074054 | validation: 4.659268976946666]
	TIME [epoch: 6.76 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4793956017432714		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 3.4793956017432714 | validation: 4.774874190261729]
	TIME [epoch: 6.79 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5865912716785027		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 3.5865912716785027 | validation: 4.6349277183604505]
	TIME [epoch: 6.77 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4445233876781653		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 3.4445233876781653 | validation: 4.580222724536333]
	TIME [epoch: 6.76 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4935495714994973		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 3.4935495714994973 | validation: 4.581432788559491]
	TIME [epoch: 6.76 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.714258186166651		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 3.714258186166651 | validation: 4.734908002849592]
	TIME [epoch: 6.76 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9676368217445357		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 3.9676368217445357 | validation: 4.95298966057816]
	TIME [epoch: 6.76 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.105443255541845		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 4.105443255541845 | validation: 4.968780962302779]
	TIME [epoch: 6.76 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.332270485282968		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 4.332270485282968 | validation: 4.766279258340962]
	TIME [epoch: 6.79 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4445040483907325		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 4.4445040483907325 | validation: 4.999594250414464]
	TIME [epoch: 6.77 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.390021071411898		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 4.390021071411898 | validation: 5.026996560900073]
	TIME [epoch: 6.76 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5282678819178415		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 4.5282678819178415 | validation: 5.184900390869965]
	TIME [epoch: 6.76 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4543182025700645		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 4.4543182025700645 | validation: 5.0687943779135]
	TIME [epoch: 6.75 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.319497807882263		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 4.319497807882263 | validation: 4.806221244069457]
	TIME [epoch: 6.76 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.255149179954538		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 4.255149179954538 | validation: 4.985650166338079]
	TIME [epoch: 6.75 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.387986364944187		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 4.387986364944187 | validation: 5.081000683217946]
	TIME [epoch: 6.8 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.283386059622179		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 4.283386059622179 | validation: 5.188768289965028]
	TIME [epoch: 6.77 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.338160705533164		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 4.338160705533164 | validation: 5.223657940810844]
	TIME [epoch: 6.76 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.475841735950691		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 4.475841735950691 | validation: 5.379772587023414]
	TIME [epoch: 6.76 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.371760007081564		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 4.371760007081564 | validation: 5.364965777634751]
	TIME [epoch: 6.76 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.457909483954523		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 4.457909483954523 | validation: 5.428126832345093]
	TIME [epoch: 6.76 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.513948626884369		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 4.513948626884369 | validation: 5.430389560264328]
	TIME [epoch: 6.76 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6706972884721285		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 4.6706972884721285 | validation: 5.207989780223757]
	TIME [epoch: 6.8 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.537651011751619		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 4.537651011751619 | validation: 4.812741772793906]
	TIME [epoch: 6.76 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3643622023057045		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 4.3643622023057045 | validation: 4.864087227951874]
	TIME [epoch: 6.76 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.608424152095983		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 4.608424152095983 | validation: 5.272103762330445]
	TIME [epoch: 6.76 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.665106327026787		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 4.665106327026787 | validation: 5.265492164684427]
	TIME [epoch: 6.76 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7555793454470345		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 4.7555793454470345 | validation: 5.515746696224947]
	TIME [epoch: 6.76 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7824755634979805		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 4.7824755634979805 | validation: 5.51135303063017]
	TIME [epoch: 6.76 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.766500607366765		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 4.766500607366765 | validation: 5.042949399377185]
	TIME [epoch: 6.81 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.628244214395166		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 4.628244214395166 | validation: 4.818779967747824]
	TIME [epoch: 6.76 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.559056806510457		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 4.559056806510457 | validation: 4.832966312011083]
	TIME [epoch: 6.76 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.513329710317903		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 4.513329710317903 | validation: 4.8076452710329765]
	TIME [epoch: 6.76 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.394770565877823		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 4.394770565877823 | validation: 4.758202629060028]
	TIME [epoch: 6.76 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.357607811391124		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 4.357607811391124 | validation: 4.66869050740701]
	TIME [epoch: 6.76 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.331456660020715		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 4.331456660020715 | validation: 4.700451028037815]
	TIME [epoch: 6.77 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.29842168380267		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 4.29842168380267 | validation: 4.621795311059148]
	TIME [epoch: 6.8 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330772454053637		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 4.330772454053637 | validation: 4.709347790430089]
	TIME [epoch: 6.76 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.39683312727109		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 4.39683312727109 | validation: 4.625840063424576]
	TIME [epoch: 6.76 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.322412392436876		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 4.322412392436876 | validation: 4.608220639838793]
	TIME [epoch: 6.76 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.296409409806246		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 4.296409409806246 | validation: 4.623247102528669]
	TIME [epoch: 6.77 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.171438949363184		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 4.171438949363184 | validation: 4.454639811019851]
	TIME [epoch: 6.76 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.019379574790656		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 4.019379574790656 | validation: 4.377641919064738]
	TIME [epoch: 6.77 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8331435795813436		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 3.8331435795813436 | validation: 4.226573868865521]
	TIME [epoch: 6.79 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.742875818787048		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 3.742875818787048 | validation: 4.324769237040737]
	TIME [epoch: 6.76 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.817080800477102		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 3.817080800477102 | validation: 4.3699312068030025]
	TIME [epoch: 6.76 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9540834698425966		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 3.9540834698425966 | validation: 4.681854462440986]
	TIME [epoch: 6.76 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.901883475981213		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 3.901883475981213 | validation: 4.869011781153514]
	TIME [epoch: 6.76 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.918844645647062		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 3.918844645647062 | validation: 4.756774565199817]
	TIME [epoch: 6.76 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.012376997776133		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 4.012376997776133 | validation: 4.700450381376596]
	TIME [epoch: 6.79 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.028748928019099		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 4.028748928019099 | validation: 4.501185314810453]
	TIME [epoch: 6.78 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.865816443378696		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 3.865816443378696 | validation: 4.510702480107801]
	TIME [epoch: 6.76 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.985700466725694		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 3.985700466725694 | validation: 4.642671421596462]
	TIME [epoch: 6.76 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.099473944632499		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 4.099473944632499 | validation: 4.845698405656499]
	TIME [epoch: 6.76 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0599137740921725		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 4.0599137740921725 | validation: 4.8633237518537715]
	TIME [epoch: 6.76 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.059145888492661		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 4.059145888492661 | validation: 4.786797377445695]
	TIME [epoch: 6.76 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.052235563590446		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 4.052235563590446 | validation: 4.91503124750678]
	TIME [epoch: 6.79 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.064492990225398		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 4.064492990225398 | validation: 4.81593808496778]
	TIME [epoch: 6.78 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9433418248325065		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 3.9433418248325065 | validation: 4.805473309294796]
	TIME [epoch: 6.76 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7790321531313826		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 3.7790321531313826 | validation: 4.65344568699655]
	TIME [epoch: 6.76 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6664457264739374		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 3.6664457264739374 | validation: 4.6925319983991365]
	TIME [epoch: 6.76 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7043439381611463		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 3.7043439381611463 | validation: 4.718783178819039]
	TIME [epoch: 6.76 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.699794337467525		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 3.699794337467525 | validation: 4.638586545113538]
	TIME [epoch: 6.76 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.645738677374128		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 3.645738677374128 | validation: 4.714300316651743]
	TIME [epoch: 6.8 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6328633917175406		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 3.6328633917175406 | validation: 4.531686552242734]
	TIME [epoch: 6.77 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.607307416866569		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 3.607307416866569 | validation: 4.505962555936641]
	TIME [epoch: 6.77 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5809157855199594		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 3.5809157855199594 | validation: 4.498557471690264]
	TIME [epoch: 6.76 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6312712989490734		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 3.6312712989490734 | validation: 4.645619115525988]
	TIME [epoch: 6.76 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7296088603693276		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 3.7296088603693276 | validation: 4.574249411359521]
	TIME [epoch: 6.76 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7298517241608957		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 3.7298517241608957 | validation: 4.599544996432149]
	TIME [epoch: 6.76 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.80603420695354		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 3.80603420695354 | validation: 4.83003000081445]
	TIME [epoch: 6.81 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.756858239991895		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 3.756858239991895 | validation: 4.729800699798989]
	TIME [epoch: 6.78 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.729866733949077		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 3.729866733949077 | validation: 4.617351211379896]
	TIME [epoch: 6.76 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.603591665163812		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 3.603591665163812 | validation: 4.496975328961229]
	TIME [epoch: 6.76 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5536027540089603		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 3.5536027540089603 | validation: 4.744200791857477]
	TIME [epoch: 6.76 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7152593471649746		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 3.7152593471649746 | validation: 4.780031149926554]
	TIME [epoch: 6.76 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.785265221621951		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 3.785265221621951 | validation: 4.826670199458874]
	TIME [epoch: 6.76 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8572091068207115		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 3.8572091068207115 | validation: 4.699610494613047]
	TIME [epoch: 6.8 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8243653969629223		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 3.8243653969629223 | validation: 4.638868735591137]
	TIME [epoch: 6.76 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8766668510061866		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 3.8766668510061866 | validation: 4.617978981411262]
	TIME [epoch: 6.76 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9003838795506196		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 3.9003838795506196 | validation: 4.780540484534213]
	TIME [epoch: 6.77 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.888662508041181		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 3.888662508041181 | validation: 4.5894047052816465]
	TIME [epoch: 6.76 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8677144792447375		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 3.8677144792447375 | validation: 4.695078823504479]
	TIME [epoch: 6.76 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8131403182419494		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 3.8131403182419494 | validation: 4.745784257927458]
	TIME [epoch: 6.77 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.885288951239144		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 3.885288951239144 | validation: 4.616371865585274]
	TIME [epoch: 6.8 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8578166334913555		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 3.8578166334913555 | validation: 4.736141746421476]
	TIME [epoch: 6.77 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.736077307629966		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 3.736077307629966 | validation: 4.807269123010618]
	TIME [epoch: 6.76 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.715957612058655		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 3.715957612058655 | validation: 4.850136120255021]
	TIME [epoch: 6.77 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8743403045523515		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 3.8743403045523515 | validation: 4.947722250396325]
	TIME [epoch: 6.77 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9053886631160184		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 3.9053886631160184 | validation: 4.995474934470766]
	TIME [epoch: 6.77 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.908316412503599		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 3.908316412503599 | validation: 4.978985703708817]
	TIME [epoch: 6.78 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.910640748361626		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 3.910640748361626 | validation: 5.119730525439559]
	TIME [epoch: 6.8 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.053577825246916		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 4.053577825246916 | validation: 5.095968020858605]
	TIME [epoch: 6.77 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.044304354718732		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 4.044304354718732 | validation: 5.084446855940976]
	TIME [epoch: 6.76 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.982304127478028		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 3.982304127478028 | validation: 4.986833317979359]
	TIME [epoch: 6.77 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.975333730916269		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 3.975333730916269 | validation: 4.965407546233237]
	TIME [epoch: 6.77 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9964396593280904		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 3.9964396593280904 | validation: 5.240282609975765]
	TIME [epoch: 6.77 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.093784268484473		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 4.093784268484473 | validation: 5.039930076840518]
	TIME [epoch: 6.8 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.074835282748109		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 4.074835282748109 | validation: 5.0651914569848415]
	TIME [epoch: 6.79 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.034254466791262		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 4.034254466791262 | validation: 5.129104742004323]
	TIME [epoch: 6.77 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.127930573576321		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 4.127930573576321 | validation: 5.231043603975603]
	TIME [epoch: 6.77 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.109004941795064		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 4.109004941795064 | validation: 5.217277130288688]
	TIME [epoch: 6.77 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0576275979426075		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 4.0576275979426075 | validation: 5.194132331936758]
	TIME [epoch: 6.77 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0654693790241305		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 4.0654693790241305 | validation: 5.25135587511576]
	TIME [epoch: 6.76 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.128595918488676		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 4.128595918488676 | validation: 5.335514322719181]
	TIME [epoch: 6.81 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.238266726652945		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 4.238266726652945 | validation: 5.34653395120224]
	TIME [epoch: 6.78 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.22941049408046		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 4.22941049408046 | validation: 5.50688014658442]
	TIME [epoch: 6.77 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.25825392856796		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 4.25825392856796 | validation: 5.368785884183719]
	TIME [epoch: 6.77 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.319676211420028		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 4.319676211420028 | validation: 5.396073434757479]
	TIME [epoch: 6.76 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.281117178318673		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 4.281117178318673 | validation: 5.355963211020796]
	TIME [epoch: 6.76 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.300039321494817		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 4.300039321494817 | validation: 5.390409320983393]
	TIME [epoch: 6.76 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.322367043728569		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 4.322367043728569 | validation: 5.341243762733718]
	TIME [epoch: 6.8 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.328991663726531		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 4.328991663726531 | validation: 5.325370060024158]
	TIME [epoch: 6.78 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.318253124573129		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 4.318253124573129 | validation: 5.265605122369285]
	TIME [epoch: 6.76 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3075506172184035		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 4.3075506172184035 | validation: 5.113667796258805]
	TIME [epoch: 6.76 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.353855138348999		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 4.353855138348999 | validation: 4.8143537837134485]
	TIME [epoch: 6.76 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.265526460929892		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 4.265526460929892 | validation: 4.611479279690235]
	TIME [epoch: 6.76 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.107832796182667		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 4.107832796182667 | validation: 4.596357972911132]
	TIME [epoch: 6.77 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.007183378785896		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 4.007183378785896 | validation: 4.527002861346711]
	TIME [epoch: 6.81 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.922928682753782		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 3.922928682753782 | validation: 4.43863915746544]
	TIME [epoch: 6.78 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9674677326513477		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 3.9674677326513477 | validation: 4.484019446251219]
	TIME [epoch: 6.77 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.026731013268613		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 4.026731013268613 | validation: 4.513528925149149]
	TIME [epoch: 6.76 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.101333562701676		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 4.101333562701676 | validation: 4.55517848785094]
	TIME [epoch: 6.77 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.089792137414868		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 4.089792137414868 | validation: 4.555652396025165]
	TIME [epoch: 6.77 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.017910601826587		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 4.017910601826587 | validation: 4.623178838746506]
	TIME [epoch: 6.78 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.013263271356765		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 4.013263271356765 | validation: 4.624249546740542]
	TIME [epoch: 6.82 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.072086611371621		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 4.072086611371621 | validation: 4.595105008773945]
	TIME [epoch: 6.77 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.064937983013609		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 4.064937983013609 | validation: 4.60773591711372]
	TIME [epoch: 6.78 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9848055761099923		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 3.9848055761099923 | validation: 4.528191269234199]
	TIME [epoch: 6.77 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.988244573658541		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 3.988244573658541 | validation: 4.477508444512562]
	TIME [epoch: 6.77 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9593667958981014		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 3.9593667958981014 | validation: 4.5067877274131884]
	TIME [epoch: 6.77 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9358532924073604		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 3.9358532924073604 | validation: 4.553830979512612]
	TIME [epoch: 6.78 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.032523721059206		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 4.032523721059206 | validation: 4.594838483268843]
	TIME [epoch: 6.81 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1140301012430145		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 4.1140301012430145 | validation: 4.619386308656015]
	TIME [epoch: 6.77 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.224028602080029		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 4.224028602080029 | validation: 4.854221646004264]
	TIME [epoch: 6.77 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.341535294382602		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 4.341535294382602 | validation: 4.9341378742205055]
	TIME [epoch: 6.77 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.333697368429865		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 4.333697368429865 | validation: 4.719787124818009]
	TIME [epoch: 6.77 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.200908270891964		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 4.200908270891964 | validation: 4.641986175714057]
	TIME [epoch: 6.77 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.190864980051951		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 4.190864980051951 | validation: 4.511395128986372]
	TIME [epoch: 6.78 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.067653621805673		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 4.067653621805673 | validation: 4.5240535246877105]
	TIME [epoch: 6.81 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.108299001929089		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 4.108299001929089 | validation: 4.553856411536879]
	TIME [epoch: 6.77 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.092363559504528		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 4.092363559504528 | validation: 4.514347090138622]
	TIME [epoch: 6.77 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.214593011304516		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 4.214593011304516 | validation: 4.703572348664245]
	TIME [epoch: 6.77 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.245208937426453		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 4.245208937426453 | validation: 4.676991679347996]
	TIME [epoch: 6.77 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.219939688340372		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 4.219939688340372 | validation: 4.560504723887845]
	TIME [epoch: 6.77 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.213619512488654		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 4.213619512488654 | validation: 4.62071868270513]
	TIME [epoch: 6.8 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.205807481049142		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 4.205807481049142 | validation: 4.619749891666244]
	TIME [epoch: 6.79 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.150984861216849		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 4.150984861216849 | validation: 4.599293342080216]
	TIME [epoch: 6.78 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.135766489209937		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 4.135766489209937 | validation: 4.571300705226619]
	TIME [epoch: 6.77 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.128174493733565		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 4.128174493733565 | validation: 4.633343892448854]
	TIME [epoch: 6.78 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.144153822891885		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 4.144153822891885 | validation: 4.629784976729817]
	TIME [epoch: 6.76 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.155186717548417		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 4.155186717548417 | validation: 4.667606574754682]
	TIME [epoch: 6.76 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.186187618498382		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 4.186187618498382 | validation: 4.594518213393117]
	TIME [epoch: 6.8 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.15208305993307		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 4.15208305993307 | validation: 4.61947540933195]
	TIME [epoch: 6.79 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.122753265257033		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 4.122753265257033 | validation: 4.51104784297163]
	TIME [epoch: 6.76 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.162211646980586		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 4.162211646980586 | validation: 4.499801527551288]
	TIME [epoch: 6.76 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.185733062225215		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 4.185733062225215 | validation: 4.531541584097777]
	TIME [epoch: 6.76 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.160962682788202		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 4.160962682788202 | validation: 4.530024737352249]
	TIME [epoch: 6.76 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.182881250296353		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 4.182881250296353 | validation: 4.539435789186429]
	TIME [epoch: 6.77 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.250683584475373		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 4.250683584475373 | validation: 4.678497597981028]
	TIME [epoch: 6.8 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.319419509273716		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 4.319419509273716 | validation: 4.773835888909192]
	TIME [epoch: 6.78 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.340838461460152		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 4.340838461460152 | validation: 4.792363834452276]
	TIME [epoch: 6.76 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.355461360139379		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 4.355461360139379 | validation: 5.00806286747274]
	TIME [epoch: 6.77 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3884698292772555		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 4.3884698292772555 | validation: 5.113114410300213]
	TIME [epoch: 6.78 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.317424125722788		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 4.317424125722788 | validation: 5.190168833314586]
	TIME [epoch: 6.77 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3699051835277825		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 4.3699051835277825 | validation: 5.06461365981651]
	TIME [epoch: 6.78 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3805975263440615		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 4.3805975263440615 | validation: 5.124223384701687]
	TIME [epoch: 6.81 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.413557048423247		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 4.413557048423247 | validation: 5.255424403031704]
	TIME [epoch: 6.78 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.362340026784533		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 4.362340026784533 | validation: 5.120820411565223]
	TIME [epoch: 6.77 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.325357859798562		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 4.325357859798562 | validation: 5.175322739682869]
	TIME [epoch: 6.77 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.245127844634517		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 4.245127844634517 | validation: 5.129363332801027]
	TIME [epoch: 6.77 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.215608529083327		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 4.215608529083327 | validation: 5.192030639290909]
	TIME [epoch: 6.77 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.202616731735788		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 4.202616731735788 | validation: 5.1138261233568585]
	TIME [epoch: 6.78 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1947938979448285		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 4.1947938979448285 | validation: 5.105427233519595]
	TIME [epoch: 6.81 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.210162879178074		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 4.210162879178074 | validation: 5.107597709304192]
	TIME [epoch: 6.78 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.198930126276612		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 4.198930126276612 | validation: 5.197607271687703]
	TIME [epoch: 6.77 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.208749517257833		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 4.208749517257833 | validation: 5.274927317907368]
	TIME [epoch: 6.76 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.319758927455153		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 4.319758927455153 | validation: 5.250045421719566]
	TIME [epoch: 6.76 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.29245522951978		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 4.29245522951978 | validation: 5.25005425807133]
	TIME [epoch: 6.77 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.278585662477352		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 4.278585662477352 | validation: 5.240725446916976]
	TIME [epoch: 6.78 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.315546490930155		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 4.315546490930155 | validation: 5.27140993221753]
	TIME [epoch: 6.82 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.322254546950477		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 4.322254546950477 | validation: 5.283619091679938]
	TIME [epoch: 6.78 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.388515485840745		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 4.388515485840745 | validation: 5.205294563393713]
	TIME [epoch: 6.77 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.43724157556245		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 4.43724157556245 | validation: 5.174607228616322]
	TIME [epoch: 6.77 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.429791459731245		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 4.429791459731245 | validation: 5.197731960435984]
	TIME [epoch: 6.78 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.463653624351005		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 4.463653624351005 | validation: 5.305052843332865]
	TIME [epoch: 6.77 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.483949733966478		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 4.483949733966478 | validation: 5.339831867803079]
	TIME [epoch: 6.78 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.469497036928042		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 4.469497036928042 | validation: 5.365333925842037]
	TIME [epoch: 6.81 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.510211634564523		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 4.510211634564523 | validation: 5.4002108045829225]
	TIME [epoch: 6.77 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.531200429435479		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 4.531200429435479 | validation: 5.344019245776034]
	TIME [epoch: 6.77 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.531901481503545		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 4.531901481503545 | validation: 5.413086616379843]
	TIME [epoch: 6.77 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.570929005265872		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 4.570929005265872 | validation: 5.372569956644065]
	TIME [epoch: 6.77 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5372353652959765		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 4.5372353652959765 | validation: 5.3698879533906485]
	TIME [epoch: 6.77 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.541196004016524		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 4.541196004016524 | validation: 5.408556229234836]
	TIME [epoch: 6.79 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5232197890844885		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 4.5232197890844885 | validation: 5.3661353427464755]
	TIME [epoch: 6.79 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.461881047962017		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 4.461881047962017 | validation: 5.3648261846233]
	TIME [epoch: 6.76 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.460646265631124		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 4.460646265631124 | validation: 5.4046371440093965]
	TIME [epoch: 6.76 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.442416074778299		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 4.442416074778299 | validation: 5.337121814084]
	TIME [epoch: 6.77 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.35070749766023		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 4.35070749766023 | validation: 5.398786274916187]
	TIME [epoch: 6.77 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.338873478353698		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 4.338873478353698 | validation: 5.308381335367622]
	TIME [epoch: 6.76 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.291152077065411		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 4.291152077065411 | validation: 5.308748747195275]
	TIME [epoch: 6.8 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.286730129276862		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 4.286730129276862 | validation: 5.378494094002484]
	TIME [epoch: 6.78 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.341242276803414		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 4.341242276803414 | validation: 5.379149101056327]
	TIME [epoch: 6.77 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.373731183430229		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 4.373731183430229 | validation: 5.292915805961794]
	TIME [epoch: 6.76 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.393865356865728		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 4.393865356865728 | validation: 5.3283971799385945]
	TIME [epoch: 6.76 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.451821082600179		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 4.451821082600179 | validation: 5.263718285502016]
	TIME [epoch: 6.76 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.400700972834384		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 4.400700972834384 | validation: 5.309473498620088]
	TIME [epoch: 6.77 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.32707434391951		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 4.32707434391951 | validation: 5.261758773630731]
	TIME [epoch: 6.8 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.23728985422857		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 4.23728985422857 | validation: 5.205513277547496]
	TIME [epoch: 6.78 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.227769843773023		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 4.227769843773023 | validation: 5.198575451933758]
	TIME [epoch: 6.77 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.284001391899994		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 4.284001391899994 | validation: 5.195623671597646]
	TIME [epoch: 6.77 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.308785725508643		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 4.308785725508643 | validation: 5.240045834487683]
	TIME [epoch: 6.77 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.302797700709146		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 4.302797700709146 | validation: 5.270280140446102]
	TIME [epoch: 6.76 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.361865421201404		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 4.361865421201404 | validation: 5.312332932218892]
	TIME [epoch: 6.77 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.352889776198944		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 4.352889776198944 | validation: 5.31449134117258]
	TIME [epoch: 6.81 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.393283343340125		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 4.393283343340125 | validation: 5.316555428910429]
	TIME [epoch: 6.78 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.374508561922108		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 4.374508561922108 | validation: 5.28398574313877]
	TIME [epoch: 6.77 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.325327141659352		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 4.325327141659352 | validation: 5.3072494909323975]
	TIME [epoch: 6.77 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.329572358898613		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 4.329572358898613 | validation: 5.314506473201348]
	TIME [epoch: 6.77 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.324156871026633		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 4.324156871026633 | validation: 5.344579719047239]
	TIME [epoch: 6.77 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330239391786682		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 4.330239391786682 | validation: 5.323378525913735]
	TIME [epoch: 6.77 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.352546378258614		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 4.352546378258614 | validation: 5.330266529578129]
	TIME [epoch: 6.81 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.346266446656984		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 4.346266446656984 | validation: 5.323819659516677]
	TIME [epoch: 6.76 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.318708009342787		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 4.318708009342787 | validation: 5.305477513752214]
	TIME [epoch: 6.77 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2984657780082385		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 4.2984657780082385 | validation: 5.315802991514962]
	TIME [epoch: 6.77 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2977130300196364		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 4.2977130300196364 | validation: 5.36345578932481]
	TIME [epoch: 6.76 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3641529878529415		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 4.3641529878529415 | validation: 5.333767134793898]
	TIME [epoch: 6.77 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.507691219363916		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 4.507691219363916 | validation: 5.4540568935211535]
	TIME [epoch: 6.78 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.561581632931922		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 4.561581632931922 | validation: 5.452563244967356]
	TIME [epoch: 6.81 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.606171010972845		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 4.606171010972845 | validation: 5.452477721125714]
	TIME [epoch: 6.77 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.615518911577955		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 4.615518911577955 | validation: 5.485563700579007]
	TIME [epoch: 6.76 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.643194636958171		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 4.643194636958171 | validation: 5.533396607933402]
	TIME [epoch: 6.77 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.639681707097306		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 4.639681707097306 | validation: 5.42420435655807]
	TIME [epoch: 6.76 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.621205096879869		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 4.621205096879869 | validation: 5.472915953161655]
	TIME [epoch: 6.76 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6403498256985465		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 4.6403498256985465 | validation: 5.505444818733849]
	TIME [epoch: 6.77 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6782540205751015		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 4.6782540205751015 | validation: 5.501191488286141]
	TIME [epoch: 6.81 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6186863972294825		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 4.6186863972294825 | validation: 5.533411057782434]
	TIME [epoch: 6.77 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.675561543893718		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 4.675561543893718 | validation: 5.49547739633894]
	TIME [epoch: 6.77 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.653033124826608		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 4.653033124826608 | validation: 5.4531026048607725]
	TIME [epoch: 6.78 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.628564498409446		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 4.628564498409446 | validation: 5.551865354316867]
	TIME [epoch: 6.77 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.598590393777882		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 4.598590393777882 | validation: 5.516856118601074]
	TIME [epoch: 6.77 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.620824799944225		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 4.620824799944225 | validation: 5.473001283814394]
	TIME [epoch: 6.8 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.574477857988208		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 4.574477857988208 | validation: 5.502989676595657]
	TIME [epoch: 6.78 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.613145260568478		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 4.613145260568478 | validation: 5.4607561166499545]
	TIME [epoch: 6.77 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.581742949501981		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 4.581742949501981 | validation: 5.519689097384446]
	TIME [epoch: 6.77 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6567314253650345		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 4.6567314253650345 | validation: 5.475309493258998]
	TIME [epoch: 6.76 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.622371945270165		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 4.622371945270165 | validation: 5.581980343665946]
	TIME [epoch: 6.77 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.672439007273416		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 4.672439007273416 | validation: 5.556723792007478]
	TIME [epoch: 6.77 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.721299023433784		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 4.721299023433784 | validation: 5.517038152979056]
	TIME [epoch: 6.8 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.692514032081576		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 4.692514032081576 | validation: 5.533092956619182]
	TIME [epoch: 6.78 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.743141812217463		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 4.743141812217463 | validation: 5.52518610805133]
	TIME [epoch: 6.77 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7042594795539685		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 4.7042594795539685 | validation: 5.580688576814824]
	TIME [epoch: 6.77 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.695751033172057		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 4.695751033172057 | validation: 5.5704271054004675]
	TIME [epoch: 6.77 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.699958726187728		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 4.699958726187728 | validation: 5.610682851852781]
	TIME [epoch: 6.77 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.708994753136991		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 4.708994753136991 | validation: 5.627189998964951]
	TIME [epoch: 6.77 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.717827406074132		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 4.717827406074132 | validation: 5.612890092427404]
	TIME [epoch: 6.81 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.690708233632318		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 4.690708233632318 | validation: 5.5884584290528]
	TIME [epoch: 6.78 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.706302859103712		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 4.706302859103712 | validation: 5.63418721000343]
	TIME [epoch: 6.77 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.689733278457282		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 4.689733278457282 | validation: 5.6147823838291595]
	TIME [epoch: 6.78 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.687155305909026		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 4.687155305909026 | validation: 5.588090631127271]
	TIME [epoch: 6.77 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6616731348557705		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 4.6616731348557705 | validation: 5.606576508655927]
	TIME [epoch: 6.77 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.641220284667855		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 4.641220284667855 | validation: 5.6308324837289945]
	TIME [epoch: 6.76 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.60538998309184		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 4.60538998309184 | validation: 5.593066248057231]
	TIME [epoch: 6.8 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.641150406726404		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 4.641150406726404 | validation: 5.620076227427704]
	TIME [epoch: 6.78 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.611806252928988		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 4.611806252928988 | validation: 5.542837770262264]
	TIME [epoch: 6.78 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.579070074062338		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 4.579070074062338 | validation: 5.5494827411929]
	TIME [epoch: 6.77 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.572662558720802		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 4.572662558720802 | validation: 5.459134381448357]
	TIME [epoch: 6.77 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.549103394088856		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 4.549103394088856 | validation: 5.586433916403882]
	TIME [epoch: 6.77 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.551405797242055		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 4.551405797242055 | validation: 5.50601172631014]
	TIME [epoch: 6.77 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.647418821909831		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 4.647418821909831 | validation: 5.449359418992492]
	TIME [epoch: 6.82 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5979346225428595		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 4.5979346225428595 | validation: 5.544856758470216]
	TIME [epoch: 6.77 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.610889353136083		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 4.610889353136083 | validation: 5.508027805244739]
	TIME [epoch: 6.77 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.581759906002646		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 4.581759906002646 | validation: 5.465229361688904]
	TIME [epoch: 6.76 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.577670596232675		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 4.577670596232675 | validation: 5.505463023966279]
	TIME [epoch: 6.77 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.534145375985148		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 4.534145375985148 | validation: 5.547047557732822]
	TIME [epoch: 6.77 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.521145809195683		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 4.521145809195683 | validation: 5.497401445522348]
	TIME [epoch: 6.78 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.520866143979021		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 4.520866143979021 | validation: 5.537312753583412]
	TIME [epoch: 6.81 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.557478103054146		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 4.557478103054146 | validation: 5.5075312033114265]
	TIME [epoch: 6.77 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.559826025777351		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 4.559826025777351 | validation: 5.510016320402494]
	TIME [epoch: 6.77 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5517463339403825		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 4.5517463339403825 | validation: 5.509049159169647]
	TIME [epoch: 6.77 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.564307969860674		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 4.564307969860674 | validation: 5.509827277638735]
	TIME [epoch: 6.77 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.540278906515015		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 4.540278906515015 | validation: 5.502910900722757]
	TIME [epoch: 6.77 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.553750737665425		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 4.553750737665425 | validation: 5.479422004905198]
	TIME [epoch: 6.78 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.573710060087519		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 4.573710060087519 | validation: 5.477238974736899]
	TIME [epoch: 6.81 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.539691201683198		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 4.539691201683198 | validation: 5.491992821847612]
	TIME [epoch: 6.77 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.542620010168117		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 4.542620010168117 | validation: 5.4802431100459685]
	TIME [epoch: 6.77 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.555472342711539		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 4.555472342711539 | validation: 5.493590345631953]
	TIME [epoch: 6.77 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.550448750076626		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 4.550448750076626 | validation: 5.461393893740675]
	TIME [epoch: 6.76 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.519976334284523		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 4.519976334284523 | validation: 5.4975009748539545]
	TIME [epoch: 6.77 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.526589296876183		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 4.526589296876183 | validation: 5.453431620640512]
	TIME [epoch: 6.8 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.555546628280339		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 4.555546628280339 | validation: 5.389744525319072]
	TIME [epoch: 6.78 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.582235673507427		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 4.582235673507427 | validation: 5.488887390931288]
	TIME [epoch: 6.77 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.509071797784979		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 4.509071797784979 | validation: 5.478964528328217]
	TIME [epoch: 6.77 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.506676344203746		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 4.506676344203746 | validation: 5.401094687515361]
	TIME [epoch: 6.77 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4614035831687255		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 4.4614035831687255 | validation: 5.369279877314307]
	TIME [epoch: 6.77 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.430567794146469		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 4.430567794146469 | validation: 5.346081752401834]
	TIME [epoch: 6.77 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3924331601875295		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 4.3924331601875295 | validation: 5.369559612244455]
	TIME [epoch: 6.8 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.430826168522878		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 4.430826168522878 | validation: 5.347477730094939]
	TIME [epoch: 6.78 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.474947526750121		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 4.474947526750121 | validation: 5.386911608302661]
	TIME [epoch: 6.77 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.473965925673909		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 4.473965925673909 | validation: 5.424729110450135]
	TIME [epoch: 6.77 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4833542080475866		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 4.4833542080475866 | validation: 5.359146662829522]
	TIME [epoch: 6.77 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.503748099336347		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 4.503748099336347 | validation: 5.391451502509836]
	TIME [epoch: 6.77 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.546962046507359		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 4.546962046507359 | validation: 5.488814871586899]
	TIME [epoch: 6.77 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.58556028301928		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 4.58556028301928 | validation: 5.436944451259208]
	TIME [epoch: 6.8 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5903264963437325		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 4.5903264963437325 | validation: 5.431442298372984]
	TIME [epoch: 6.78 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.591542842891621		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 4.591542842891621 | validation: 5.461375275301206]
	TIME [epoch: 6.77 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5907600872839005		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 4.5907600872839005 | validation: 5.4400137768276835]
	TIME [epoch: 6.76 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.63616394224923		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 4.63616394224923 | validation: 5.449052350832187]
	TIME [epoch: 6.78 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.639383361418425		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 4.639383361418425 | validation: 5.457598950345077]
	TIME [epoch: 6.76 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.637080366371737		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 4.637080366371737 | validation: 5.435479470840297]
	TIME [epoch: 6.77 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.659732560441294		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 4.659732560441294 | validation: 5.419714967423481]
	TIME [epoch: 6.81 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.673061684316048		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 4.673061684316048 | validation: 5.456214695784902]
	TIME [epoch: 6.78 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.71136850416745		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 4.71136850416745 | validation: 5.4011100399073175]
	TIME [epoch: 6.77 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.689706021377447		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 4.689706021377447 | validation: 5.460191658004021]
	TIME [epoch: 6.77 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.671639417147637		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 4.671639417147637 | validation: 5.425607671883625]
	TIME [epoch: 6.77 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6477513074663825		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 4.6477513074663825 | validation: 5.500594367554845]
	TIME [epoch: 6.77 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.674881021267749		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 4.674881021267749 | validation: 5.474076211023603]
	TIME [epoch: 6.77 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.703998375601105		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 4.703998375601105 | validation: 5.433632964124562]
	TIME [epoch: 6.81 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.695769817222486		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 4.695769817222486 | validation: 5.448526087865444]
	TIME [epoch: 6.77 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.718336662224758		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 4.718336662224758 | validation: 5.48701916097608]
	TIME [epoch: 6.77 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.691758040059775		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 4.691758040059775 | validation: 5.457314546746215]
	TIME [epoch: 6.76 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.65695921014173		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 4.65695921014173 | validation: 5.497707255571246]
	TIME [epoch: 6.77 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.630633457528931		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 4.630633457528931 | validation: 5.425254974250052]
	TIME [epoch: 6.77 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.626818023895858		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 4.626818023895858 | validation: 5.4349840975707]
	TIME [epoch: 6.79 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.64252857488587		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 4.64252857488587 | validation: 5.428010033522439]
	TIME [epoch: 6.8 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.631515766271547		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 4.631515766271547 | validation: 5.444007262600715]
	TIME [epoch: 6.77 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.654598927103067		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 4.654598927103067 | validation: 5.444978961478604]
	TIME [epoch: 6.78 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.632323271978424		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 4.632323271978424 | validation: 5.371557643510775]
	TIME [epoch: 6.77 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.585360141759252		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 4.585360141759252 | validation: 5.433798954057334]
	TIME [epoch: 6.78 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.567439993391925		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 4.567439993391925 | validation: 5.441031108310219]
	TIME [epoch: 6.78 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.531273812376424		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 4.531273812376424 | validation: 5.470960698665706]
	TIME [epoch: 6.78 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.530497350119104		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 4.530497350119104 | validation: 5.413076696726238]
	TIME [epoch: 6.81 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.557116956593298		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 4.557116956593298 | validation: 5.366752019506592]
	TIME [epoch: 6.77 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5304351131069795		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 4.5304351131069795 | validation: 5.417721483177996]
	TIME [epoch: 6.77 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5000668044999035		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 4.5000668044999035 | validation: 5.347322036780714]
	TIME [epoch: 6.77 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.501763739994881		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 4.501763739994881 | validation: 5.419938274372051]
	TIME [epoch: 6.77 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.485867157452175		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 4.485867157452175 | validation: 5.339309801790673]
	TIME [epoch: 6.76 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4658349382463935		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 4.4658349382463935 | validation: 5.374222326479516]
	TIME [epoch: 6.8 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.459583202668956		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 4.459583202668956 | validation: 5.449683386128205]
	TIME [epoch: 6.79 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.500138602880855		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 4.500138602880855 | validation: 5.4305024505562915]
	TIME [epoch: 6.77 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.492896429798086		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 4.492896429798086 | validation: 5.387897014639433]
	TIME [epoch: 6.77 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.476192395584672		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 4.476192395584672 | validation: 5.362283488506852]
	TIME [epoch: 6.77 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.50569855787994		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 4.50569855787994 | validation: 5.4005734492877]
	TIME [epoch: 6.77 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.487146751080028		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 4.487146751080028 | validation: 5.323695171575171]
	TIME [epoch: 6.78 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.501354833688207		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 4.501354833688207 | validation: 5.410943372912353]
	TIME [epoch: 6.81 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5076519019953905		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 4.5076519019953905 | validation: 5.42914241559863]
	TIME [epoch: 6.79 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.501560152525948		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 4.501560152525948 | validation: 5.378261716637801]
	TIME [epoch: 6.78 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.479492734328839		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 4.479492734328839 | validation: 5.415358805807285]
	TIME [epoch: 6.78 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.503555470526855		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 4.503555470526855 | validation: 5.403510168027045]
	TIME [epoch: 6.77 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.476828562034614		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 4.476828562034614 | validation: 5.400294511691543]
	TIME [epoch: 6.77 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.507399384505579		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 4.507399384505579 | validation: 5.336297696518516]
	TIME [epoch: 6.78 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.520321079687886		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 4.520321079687886 | validation: 5.342594102792376]
	TIME [epoch: 6.82 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.519242970888596		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 4.519242970888596 | validation: 5.367019030318328]
	TIME [epoch: 6.79 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.520331520145801		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 4.520331520145801 | validation: 5.408929485096049]
	TIME [epoch: 6.77 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.527268924358772		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 4.527268924358772 | validation: 5.405069086269613]
	TIME [epoch: 6.76 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.522439452728696		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 4.522439452728696 | validation: 5.350656008801492]
	TIME [epoch: 6.77 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.533233468814026		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 4.533233468814026 | validation: 5.359716055549788]
	TIME [epoch: 6.77 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.486594325900344		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 4.486594325900344 | validation: 5.40629107206647]
	TIME [epoch: 6.77 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.476508409406691		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 4.476508409406691 | validation: 5.308573731595247]
	TIME [epoch: 6.82 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4552684681764365		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 4.4552684681764365 | validation: 5.378541803350131]
	TIME [epoch: 6.77 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4720409726201		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 4.4720409726201 | validation: 5.3489842723607115]
	TIME [epoch: 6.77 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4567335032917725		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 4.4567335032917725 | validation: 5.38754353337214]
	TIME [epoch: 6.77 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.461068105352435		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 4.461068105352435 | validation: 5.343589710170554]
	TIME [epoch: 6.77 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.459549682499154		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 4.459549682499154 | validation: 5.369306467431233]
	TIME [epoch: 6.77 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.444998561370956		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 4.444998561370956 | validation: 5.308156631744188]
	TIME [epoch: 6.77 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.426502206555111		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 4.426502206555111 | validation: 5.316957535312504]
	TIME [epoch: 6.81 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.421596345765277		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 4.421596345765277 | validation: 5.3857518990083495]
	TIME [epoch: 6.77 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.419290281258542		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 4.419290281258542 | validation: 5.343642114780401]
	TIME [epoch: 6.76 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.414036720770641		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 4.414036720770641 | validation: 5.348813075578457]
	TIME [epoch: 6.77 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.395263588887477		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 4.395263588887477 | validation: 5.299275585122318]
	TIME [epoch: 6.76 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.412189846775306		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 4.412189846775306 | validation: 5.3842511299165094]
	TIME [epoch: 6.77 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.389604364387134		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 4.389604364387134 | validation: 5.340222987850682]
	TIME [epoch: 6.77 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.373726747265907		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 4.373726747265907 | validation: 5.347595077965986]
	TIME [epoch: 6.81 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.369417490980064		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 4.369417490980064 | validation: 5.29612504618726]
	TIME [epoch: 6.76 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.348674590813323		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 4.348674590813323 | validation: 5.293187609322144]
	TIME [epoch: 6.76 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.360508056454071		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 4.360508056454071 | validation: 5.281551431649442]
	TIME [epoch: 6.76 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.360731756181904		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 4.360731756181904 | validation: 5.295646647298783]
	TIME [epoch: 6.76 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.373605337471247		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 4.373605337471247 | validation: 5.32127867042291]
	TIME [epoch: 6.76 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.328269173768035		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 4.328269173768035 | validation: 5.340998140370884]
	TIME [epoch: 6.78 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3323960653152875		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 4.3323960653152875 | validation: 5.335954976238189]
	TIME [epoch: 6.8 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330770357175884		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 4.330770357175884 | validation: 5.321466070036892]
	TIME [epoch: 6.77 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.335464797934903		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 4.335464797934903 | validation: 5.312853388745353]
	TIME [epoch: 6.77 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.340899677294308		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 4.340899677294308 | validation: 5.369007568593135]
	TIME [epoch: 6.78 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.359481733708284		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 4.359481733708284 | validation: 5.329840074720493]
	TIME [epoch: 6.76 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.317783584306065		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 4.317783584306065 | validation: 5.2567741037813605]
	TIME [epoch: 6.76 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.314867270467374		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 4.314867270467374 | validation: 5.292447737118254]
	TIME [epoch: 6.8 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.345294111151663		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 4.345294111151663 | validation: 5.361466917951469]
	TIME [epoch: 6.79 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.358215463389923		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 4.358215463389923 | validation: 5.3978727589962485]
	TIME [epoch: 6.77 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.36653265635432		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 4.36653265635432 | validation: 5.367555193343842]
	TIME [epoch: 6.77 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.368320417131182		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 4.368320417131182 | validation: 5.3876396588955835]
	TIME [epoch: 6.76 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.372028190988648		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 4.372028190988648 | validation: 5.367747900963678]
	TIME [epoch: 6.77 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.383433529807397		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 4.383433529807397 | validation: 5.361094481173222]
	TIME [epoch: 6.76 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.37967034348177		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 4.37967034348177 | validation: 5.338567389794413]
	TIME [epoch: 6.8 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3601797263846676		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 4.3601797263846676 | validation: 5.352409663460726]
	TIME [epoch: 6.78 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.348028324440711		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 4.348028324440711 | validation: 5.373444769122214]
	TIME [epoch: 6.77 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.347958442310205		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 4.347958442310205 | validation: 5.3091947042634535]
	TIME [epoch: 6.77 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.304325147750481		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 4.304325147750481 | validation: 5.326717266467749]
	TIME [epoch: 6.77 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.296469324091716		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 4.296469324091716 | validation: 5.351164555025909]
	TIME [epoch: 6.77 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3040821515075205		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 4.3040821515075205 | validation: 5.340439062421436]
	TIME [epoch: 6.77 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.340414471455179		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 4.340414471455179 | validation: 5.330212601193797]
	TIME [epoch: 6.81 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.311109645672743		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 4.311109645672743 | validation: 5.298166494934662]
	TIME [epoch: 6.78 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.345277089565501		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 4.345277089565501 | validation: 5.354886773738842]
	TIME [epoch: 6.77 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3239052664966735		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 4.3239052664966735 | validation: 5.338669394853644]
	TIME [epoch: 6.76 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.300739132065149		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 4.300739132065149 | validation: 5.2869901625033435]
	TIME [epoch: 6.76 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.328477654473382		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 4.328477654473382 | validation: 5.327384205459223]
	TIME [epoch: 6.76 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.316416495015262		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 4.316416495015262 | validation: 5.298115218572541]
	TIME [epoch: 6.77 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.346866608977559		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 4.346866608977559 | validation: 5.3155903259332895]
	TIME [epoch: 6.81 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.353363333546611		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 4.353363333546611 | validation: 5.339664908224091]
	TIME [epoch: 6.78 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.339031927268501		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 4.339031927268501 | validation: 5.323763452088816]
	TIME [epoch: 6.77 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.350348645399414		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 4.350348645399414 | validation: 5.298970764104332]
	TIME [epoch: 6.76 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.35518656746507		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 4.35518656746507 | validation: 5.341737426959317]
	TIME [epoch: 6.77 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.352993977311665		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 4.352993977311665 | validation: 5.315148367863042]
	TIME [epoch: 6.77 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.347265429115399		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 4.347265429115399 | validation: 5.346195909034435]
	TIME [epoch: 6.78 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.326702228385246		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 4.326702228385246 | validation: 5.337517280533927]
	TIME [epoch: 6.8 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.350560105084034		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 4.350560105084034 | validation: 5.343382265416405]
	TIME [epoch: 6.77 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.352993549819928		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 4.352993549819928 | validation: 5.346622201875936]
	TIME [epoch: 6.77 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.36760871310476		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 4.36760871310476 | validation: 5.337394626627775]
	TIME [epoch: 6.77 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.362724192680815		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 4.362724192680815 | validation: 5.309957952667244]
	TIME [epoch: 6.77 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.347816368890414		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 4.347816368890414 | validation: 5.306745555813235]
	TIME [epoch: 6.77 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.351491998257558		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 4.351491998257558 | validation: 5.282584464473061]
	TIME [epoch: 6.78 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3222471769526		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 4.3222471769526 | validation: 5.3285753495582515]
	TIME [epoch: 6.81 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.333408431691016		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 4.333408431691016 | validation: 5.31962242548941]
	TIME [epoch: 6.77 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330673325623734		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 4.330673325623734 | validation: 5.331809005762732]
	TIME [epoch: 6.76 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.307857589719944		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 4.307857589719944 | validation: 5.305819216345057]
	TIME [epoch: 6.77 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.309552313906385		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 4.309552313906385 | validation: 5.300199083381208]
	TIME [epoch: 6.77 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.284305255795867		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 4.284305255795867 | validation: 5.275006680115633]
	TIME [epoch: 6.77 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.290833947729846		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 4.290833947729846 | validation: 5.330084183070099]
	TIME [epoch: 6.8 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.316396168755442		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 4.316396168755442 | validation: 5.306024229142202]
	TIME [epoch: 6.78 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.270412439701321		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 4.270412439701321 | validation: 5.303336028873228]
	TIME [epoch: 6.77 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.272368294866783		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 4.272368294866783 | validation: 5.286590779387145]
	TIME [epoch: 6.77 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2811723398125565		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 4.2811723398125565 | validation: 5.29369165991986]
	TIME [epoch: 6.76 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.279413427772529		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 4.279413427772529 | validation: 5.290140670619394]
	TIME [epoch: 6.76 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.275595948111425		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 4.275595948111425 | validation: 5.29972275568492]
	TIME [epoch: 6.76 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.267034591114891		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 4.267034591114891 | validation: 5.245011868731815]
	TIME [epoch: 6.8 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2612041462740375		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 4.2612041462740375 | validation: 5.261210316982739]
	TIME [epoch: 6.78 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.255793448320672		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 4.255793448320672 | validation: 5.298504099015883]
	TIME [epoch: 6.77 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.279360564537081		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 4.279360564537081 | validation: 5.27499110225024]
	TIME [epoch: 6.76 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.290925340550239		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 4.290925340550239 | validation: 5.256540348338643]
	TIME [epoch: 6.77 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.275791853465764		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 4.275791853465764 | validation: 5.30398547613194]
	TIME [epoch: 6.77 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.271670870222334		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 4.271670870222334 | validation: 5.289106898623811]
	TIME [epoch: 6.76 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.259693313134525		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 4.259693313134525 | validation: 5.267255548989113]
	TIME [epoch: 6.8 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.274072337565948		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 4.274072337565948 | validation: 5.252065542082949]
	TIME [epoch: 6.78 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.268227160538391		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 4.268227160538391 | validation: 5.230510302073055]
	TIME [epoch: 6.77 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.261426468807922		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 4.261426468807922 | validation: 5.2001416791719635]
	TIME [epoch: 6.76 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2560783156699635		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 4.2560783156699635 | validation: 5.235564076428735]
	TIME [epoch: 6.77 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.268594277275435		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 4.268594277275435 | validation: 5.2603210891225]
	TIME [epoch: 6.76 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.26628139646648		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 4.26628139646648 | validation: 5.273378735075891]
	TIME [epoch: 6.77 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.258725728860027		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 4.258725728860027 | validation: 5.180304787456983]
	TIME [epoch: 6.8 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.262947344717958		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 4.262947344717958 | validation: 5.22870406696638]
	TIME [epoch: 6.77 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.276096412471212		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 4.276096412471212 | validation: 5.290741708653393]
	TIME [epoch: 6.77 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2763905001940685		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 4.2763905001940685 | validation: 5.198229140355192]
	TIME [epoch: 6.77 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.264758963417421		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 4.264758963417421 | validation: 5.277023684814754]
	TIME [epoch: 6.76 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.288339754404262		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 4.288339754404262 | validation: 5.220215366526487]
	TIME [epoch: 6.76 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.298952244372197		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 4.298952244372197 | validation: 5.288798059348675]
	TIME [epoch: 6.77 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.260009840694727		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 4.260009840694727 | validation: 5.251416961335757]
	TIME [epoch: 6.8 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.325132026976867		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 4.325132026976867 | validation: 5.288618284282203]
	TIME [epoch: 6.77 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.316464433660274		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 4.316464433660274 | validation: 5.2630766027321565]
	TIME [epoch: 6.77 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.295741545599678		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 4.295741545599678 | validation: 5.275749496939548]
	TIME [epoch: 6.77 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.313975262175158		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 4.313975262175158 | validation: 5.321583128294229]
	TIME [epoch: 6.76 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.284158423749761		[learning rate: 1.8186e-05]
	Learning Rate: 1.8186e-05
	LOSS [training: 4.284158423749761 | validation: 5.206703772013151]
	TIME [epoch: 6.76 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.307267496720918		[learning rate: 1.8054e-05]
	Learning Rate: 1.80542e-05
	LOSS [training: 4.307267496720918 | validation: 5.189046395866534]
	TIME [epoch: 6.78 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.25880566943627		[learning rate: 1.7923e-05]
	Learning Rate: 1.79234e-05
	LOSS [training: 4.25880566943627 | validation: 5.210084128362919]
	TIME [epoch: 6.8 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.287551500120998		[learning rate: 1.7794e-05]
	Learning Rate: 1.77936e-05
	LOSS [training: 4.287551500120998 | validation: 5.276081173404311]
	TIME [epoch: 6.77 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.248244522009964		[learning rate: 1.7665e-05]
	Learning Rate: 1.76647e-05
	LOSS [training: 4.248244522009964 | validation: 5.300767164368533]
	TIME [epoch: 6.76 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2762464299521685		[learning rate: 1.7537e-05]
	Learning Rate: 1.75367e-05
	LOSS [training: 4.2762464299521685 | validation: 5.263103377031822]
	TIME [epoch: 6.76 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.278099997030817		[learning rate: 1.741e-05]
	Learning Rate: 1.74096e-05
	LOSS [training: 4.278099997030817 | validation: 5.261274383663428]
	TIME [epoch: 6.76 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.294076383786266		[learning rate: 1.7284e-05]
	Learning Rate: 1.72835e-05
	LOSS [training: 4.294076383786266 | validation: 5.225574880152144]
	TIME [epoch: 6.77 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.302851899575075		[learning rate: 1.7158e-05]
	Learning Rate: 1.71583e-05
	LOSS [training: 4.302851899575075 | validation: 5.260863186389094]
	TIME [epoch: 6.77 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.25643681332941		[learning rate: 1.7034e-05]
	Learning Rate: 1.7034e-05
	LOSS [training: 4.25643681332941 | validation: 5.212588638152885]
	TIME [epoch: 6.81 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.268618570542582		[learning rate: 1.6911e-05]
	Learning Rate: 1.69106e-05
	LOSS [training: 4.268618570542582 | validation: 5.252710510396367]
	TIME [epoch: 6.77 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.289465931219743		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 4.289465931219743 | validation: 5.246052128658411]
	TIME [epoch: 6.77 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.293056640218235		[learning rate: 1.6666e-05]
	Learning Rate: 1.66664e-05
	LOSS [training: 4.293056640218235 | validation: 5.2674027428843155]
	TIME [epoch: 6.76 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.296297744747929		[learning rate: 1.6546e-05]
	Learning Rate: 1.65457e-05
	LOSS [training: 4.296297744747929 | validation: 5.266187481426946]
	TIME [epoch: 6.77 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2854960049850295		[learning rate: 1.6426e-05]
	Learning Rate: 1.64258e-05
	LOSS [training: 4.2854960049850295 | validation: 5.247468487679717]
	TIME [epoch: 6.76 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.288261606293975		[learning rate: 1.6307e-05]
	Learning Rate: 1.63068e-05
	LOSS [training: 4.288261606293975 | validation: 5.240139099877549]
	TIME [epoch: 6.8 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.297255777640638		[learning rate: 1.6189e-05]
	Learning Rate: 1.61887e-05
	LOSS [training: 4.297255777640638 | validation: 5.284074939535831]
	TIME [epoch: 6.79 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.262478117136406		[learning rate: 1.6071e-05]
	Learning Rate: 1.60714e-05
	LOSS [training: 4.262478117136406 | validation: 5.224831347809374]
	TIME [epoch: 6.77 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3019220772461075		[learning rate: 1.5955e-05]
	Learning Rate: 1.59549e-05
	LOSS [training: 4.3019220772461075 | validation: 5.311191603913924]
	TIME [epoch: 6.77 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.288277077363107		[learning rate: 1.5839e-05]
	Learning Rate: 1.58393e-05
	LOSS [training: 4.288277077363107 | validation: 5.283068852221177]
	TIME [epoch: 6.76 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.267314396735149		[learning rate: 1.5725e-05]
	Learning Rate: 1.57246e-05
	LOSS [training: 4.267314396735149 | validation: 5.270442201991185]
	TIME [epoch: 6.77 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.27294338618705		[learning rate: 1.5611e-05]
	Learning Rate: 1.56107e-05
	LOSS [training: 4.27294338618705 | validation: 5.232243114391532]
	TIME [epoch: 6.76 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.294924422323603		[learning rate: 1.5498e-05]
	Learning Rate: 1.54976e-05
	LOSS [training: 4.294924422323603 | validation: 5.216299678587082]
	TIME [epoch: 6.8 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.267549369645293		[learning rate: 1.5385e-05]
	Learning Rate: 1.53853e-05
	LOSS [training: 4.267549369645293 | validation: 5.292680886934784]
	TIME [epoch: 6.78 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.282808488751135		[learning rate: 1.5274e-05]
	Learning Rate: 1.52738e-05
	LOSS [training: 4.282808488751135 | validation: 5.264054583698934]
	TIME [epoch: 6.77 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2902848879803805		[learning rate: 1.5163e-05]
	Learning Rate: 1.51632e-05
	LOSS [training: 4.2902848879803805 | validation: 5.254438198895844]
	TIME [epoch: 6.76 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.273174497778564		[learning rate: 1.5053e-05]
	Learning Rate: 1.50533e-05
	LOSS [training: 4.273174497778564 | validation: 5.206023010210533]
	TIME [epoch: 6.77 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.29466701191222		[learning rate: 1.4944e-05]
	Learning Rate: 1.49442e-05
	LOSS [training: 4.29466701191222 | validation: 5.212367687594515]
	TIME [epoch: 6.77 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.274773122018146		[learning rate: 1.4836e-05]
	Learning Rate: 1.4836e-05
	LOSS [training: 4.274773122018146 | validation: 5.2432862807365215]
	TIME [epoch: 6.78 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.273021974001794		[learning rate: 1.4728e-05]
	Learning Rate: 1.47285e-05
	LOSS [training: 4.273021974001794 | validation: 5.266250895628427]
	TIME [epoch: 6.81 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.280408620497528		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 4.280408620497528 | validation: 5.199598887019277]
	TIME [epoch: 6.77 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.277352191692129		[learning rate: 1.4516e-05]
	Learning Rate: 1.45158e-05
	LOSS [training: 4.277352191692129 | validation: 5.222521330698825]
	TIME [epoch: 6.76 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.318474141606206		[learning rate: 1.4411e-05]
	Learning Rate: 1.44107e-05
	LOSS [training: 4.318474141606206 | validation: 5.2857184308692355]
	TIME [epoch: 6.76 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.266971327894493		[learning rate: 1.4306e-05]
	Learning Rate: 1.43063e-05
	LOSS [training: 4.266971327894493 | validation: 5.249112473050667]
	TIME [epoch: 6.76 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2620635763007355		[learning rate: 1.4203e-05]
	Learning Rate: 1.42026e-05
	LOSS [training: 4.2620635763007355 | validation: 5.2974536861514085]
	TIME [epoch: 6.76 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.281554501880196		[learning rate: 1.41e-05]
	Learning Rate: 1.40997e-05
	LOSS [training: 4.281554501880196 | validation: 5.260838041270102]
	TIME [epoch: 6.78 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.266147332217035		[learning rate: 1.3998e-05]
	Learning Rate: 1.39976e-05
	LOSS [training: 4.266147332217035 | validation: 5.209061683389589]
	TIME [epoch: 6.81 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2963799924360835		[learning rate: 1.3896e-05]
	Learning Rate: 1.38962e-05
	LOSS [training: 4.2963799924360835 | validation: 5.288733821869079]
	TIME [epoch: 6.77 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.281214096200141		[learning rate: 1.3795e-05]
	Learning Rate: 1.37955e-05
	LOSS [training: 4.281214096200141 | validation: 5.282652083966196]
	TIME [epoch: 6.76 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.31244551309181		[learning rate: 1.3696e-05]
	Learning Rate: 1.36955e-05
	LOSS [training: 4.31244551309181 | validation: 5.25640862454945]
	TIME [epoch: 6.76 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.283737448403741		[learning rate: 1.3596e-05]
	Learning Rate: 1.35963e-05
	LOSS [training: 4.283737448403741 | validation: 5.231712799213774]
	TIME [epoch: 6.77 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.300140410992469		[learning rate: 1.3498e-05]
	Learning Rate: 1.34978e-05
	LOSS [training: 4.300140410992469 | validation: 5.287224905270537]
	TIME [epoch: 6.76 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.270593141717157		[learning rate: 1.34e-05]
	Learning Rate: 1.34e-05
	LOSS [training: 4.270593141717157 | validation: 5.250377179955258]
	TIME [epoch: 6.76 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.28576912520174		[learning rate: 1.3303e-05]
	Learning Rate: 1.33029e-05
	LOSS [training: 4.28576912520174 | validation: 5.251603391476107]
	TIME [epoch: 6.82 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.302632325761144		[learning rate: 1.3207e-05]
	Learning Rate: 1.32066e-05
	LOSS [training: 4.302632325761144 | validation: 5.245415279077934]
	TIME [epoch: 6.77 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.273189859911322		[learning rate: 1.3111e-05]
	Learning Rate: 1.31109e-05
	LOSS [training: 4.273189859911322 | validation: 5.273252270231152]
	TIME [epoch: 6.76 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.280845299564528		[learning rate: 1.3016e-05]
	Learning Rate: 1.30159e-05
	LOSS [training: 4.280845299564528 | validation: 5.3056240168365685]
	TIME [epoch: 6.77 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.273690771947128		[learning rate: 1.2922e-05]
	Learning Rate: 1.29216e-05
	LOSS [training: 4.273690771947128 | validation: 5.260607474028729]
	TIME [epoch: 6.77 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.303248287050112		[learning rate: 1.2828e-05]
	Learning Rate: 1.2828e-05
	LOSS [training: 4.303248287050112 | validation: 5.250073396083268]
	TIME [epoch: 6.77 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.289886924129288		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 4.289886924129288 | validation: 5.323826473001735]
	TIME [epoch: 6.78 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.283662192562748		[learning rate: 1.2643e-05]
	Learning Rate: 1.26428e-05
	LOSS [training: 4.283662192562748 | validation: 5.223277583275145]
	TIME [epoch: 6.81 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.304020492407372		[learning rate: 1.2551e-05]
	Learning Rate: 1.25512e-05
	LOSS [training: 4.304020492407372 | validation: 5.298466426118064]
	TIME [epoch: 6.77 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.323940142595931		[learning rate: 1.246e-05]
	Learning Rate: 1.24602e-05
	LOSS [training: 4.323940142595931 | validation: 5.235246040433494]
	TIME [epoch: 6.77 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.302435555067505		[learning rate: 1.237e-05]
	Learning Rate: 1.237e-05
	LOSS [training: 4.302435555067505 | validation: 5.286460414937673]
	TIME [epoch: 6.77 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.305365551269605		[learning rate: 1.228e-05]
	Learning Rate: 1.22803e-05
	LOSS [training: 4.305365551269605 | validation: 5.251433027888384]
	TIME [epoch: 6.76 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2782818812147125		[learning rate: 1.2191e-05]
	Learning Rate: 1.21914e-05
	LOSS [training: 4.2782818812147125 | validation: 5.301215029583859]
	TIME [epoch: 6.76 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2751383056118595		[learning rate: 1.2103e-05]
	Learning Rate: 1.21031e-05
	LOSS [training: 4.2751383056118595 | validation: 5.317834939117045]
	TIME [epoch: 6.78 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.278922798179694		[learning rate: 1.2015e-05]
	Learning Rate: 1.20154e-05
	LOSS [training: 4.278922798179694 | validation: 5.21869725317695]
	TIME [epoch: 6.8 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.271533379413286		[learning rate: 1.1928e-05]
	Learning Rate: 1.19283e-05
	LOSS [training: 4.271533379413286 | validation: 5.315141267265892]
	TIME [epoch: 6.77 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.280515658793542		[learning rate: 1.1842e-05]
	Learning Rate: 1.18419e-05
	LOSS [training: 4.280515658793542 | validation: 5.221991536216969]
	TIME [epoch: 6.77 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2672695550441215		[learning rate: 1.1756e-05]
	Learning Rate: 1.17561e-05
	LOSS [training: 4.2672695550441215 | validation: 5.263042785655373]
	TIME [epoch: 6.77 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.304673271233817		[learning rate: 1.1671e-05]
	Learning Rate: 1.16709e-05
	LOSS [training: 4.304673271233817 | validation: 5.291842986493691]
	TIME [epoch: 6.77 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.288032753149509		[learning rate: 1.1586e-05]
	Learning Rate: 1.15864e-05
	LOSS [training: 4.288032753149509 | validation: 5.348733658201509]
	TIME [epoch: 6.76 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.271603930848679		[learning rate: 1.1502e-05]
	Learning Rate: 1.15024e-05
	LOSS [training: 4.271603930848679 | validation: 5.258088988399735]
	TIME [epoch: 6.8 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.287714182997173		[learning rate: 1.1419e-05]
	Learning Rate: 1.14191e-05
	LOSS [training: 4.287714182997173 | validation: 5.27470835937512]
	TIME [epoch: 6.79 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.300378822768661		[learning rate: 1.1336e-05]
	Learning Rate: 1.13364e-05
	LOSS [training: 4.300378822768661 | validation: 5.301840952537309]
	TIME [epoch: 6.76 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.3202260012567155		[learning rate: 1.1254e-05]
	Learning Rate: 1.12542e-05
	LOSS [training: 4.3202260012567155 | validation: 5.30745878143165]
	TIME [epoch: 6.76 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.283075792708704		[learning rate: 1.1173e-05]
	Learning Rate: 1.11727e-05
	LOSS [training: 4.283075792708704 | validation: 5.3102804693808805]
	TIME [epoch: 6.76 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.27205095721747		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 4.27205095721747 | validation: 5.301394555123941]
	TIME [epoch: 6.76 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.300676086776041		[learning rate: 1.1011e-05]
	Learning Rate: 1.10114e-05
	LOSS [training: 4.300676086776041 | validation: 5.263392928844004]
	TIME [epoch: 6.76 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.307350224991854		[learning rate: 1.0932e-05]
	Learning Rate: 1.09316e-05
	LOSS [training: 4.307350224991854 | validation: 5.288056052362301]
	TIME [epoch: 6.81 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.292800840916056		[learning rate: 1.0852e-05]
	Learning Rate: 1.08524e-05
	LOSS [training: 4.292800840916056 | validation: 5.304585295570455]
	TIME [epoch: 6.77 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.29592624150963		[learning rate: 1.0774e-05]
	Learning Rate: 1.07738e-05
	LOSS [training: 4.29592624150963 | validation: 5.2731878064882105]
	TIME [epoch: 6.76 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.283125641873038		[learning rate: 1.0696e-05]
	Learning Rate: 1.06957e-05
	LOSS [training: 4.283125641873038 | validation: 5.315468932809593]
	TIME [epoch: 6.76 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.271406730148846		[learning rate: 1.0618e-05]
	Learning Rate: 1.06182e-05
	LOSS [training: 4.271406730148846 | validation: 5.317352601190081]
	TIME [epoch: 6.76 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.306983670504575		[learning rate: 1.0541e-05]
	Learning Rate: 1.05413e-05
	LOSS [training: 4.306983670504575 | validation: 5.325390625748483]
	TIME [epoch: 6.76 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.328737344830872		[learning rate: 1.0465e-05]
	Learning Rate: 1.04649e-05
	LOSS [training: 4.328737344830872 | validation: 5.318252481926365]
	TIME [epoch: 6.76 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.322682960293511		[learning rate: 1.0389e-05]
	Learning Rate: 1.03891e-05
	LOSS [training: 4.322682960293511 | validation: 5.254181778488432]
	TIME [epoch: 6.8 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.299088866786236		[learning rate: 1.0314e-05]
	Learning Rate: 1.03139e-05
	LOSS [training: 4.299088866786236 | validation: 5.2749703536594374]
	TIME [epoch: 6.78 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.299658235647524		[learning rate: 1.0239e-05]
	Learning Rate: 1.02391e-05
	LOSS [training: 4.299658235647524 | validation: 5.312420740227527]
	TIME [epoch: 6.77 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.297675002557012		[learning rate: 1.0165e-05]
	Learning Rate: 1.0165e-05
	LOSS [training: 4.297675002557012 | validation: 5.289106938612925]
	TIME [epoch: 6.77 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.295902797686951		[learning rate: 1.0091e-05]
	Learning Rate: 1.00913e-05
	LOSS [training: 4.295902797686951 | validation: 5.236364551541453]
	TIME [epoch: 6.77 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.291080744070307		[learning rate: 1.0018e-05]
	Learning Rate: 1.00182e-05
	LOSS [training: 4.291080744070307 | validation: 5.343326684965636]
	TIME [epoch: 6.76 sec]
Finished training in 6919.884 seconds.
