Args:
Namespace(name='model_phi1_1a_v_mmd1_smallnet', outdir='out/model_training/model_phi1_1a_v_mmd1_smallnet', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[8, 16, 8], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1414798952

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.725143224922595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.725143224922595 | validation: 5.914843107290022]
	TIME [epoch: 111 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.091624755749395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.091624755749395 | validation: 5.570228110591296]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.807075280701062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.807075280701062 | validation: 5.537461878363066]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.477593507879577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.477593507879577 | validation: 4.914268323773491]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8111334796925496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8111334796925496 | validation: 4.3005573489535545]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344313030677451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344313030677451 | validation: 3.826891931068142]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8819358724551303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8819358724551303 | validation: 3.365123891794858]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6450530990065335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6450530990065335 | validation: 3.6334177624237864]
	TIME [epoch: 7.84 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.749634537595786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.749634537595786 | validation: 3.3209979356842876]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5365959963189617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5365959963189617 | validation: 3.0496283351328417]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3024574331939522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3024574331939522 | validation: 3.0990196069867997]
	TIME [epoch: 7.88 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3120420643954223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3120420643954223 | validation: 2.9512481735111304]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1618832034477173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1618832034477173 | validation: 3.019830126316932]
	TIME [epoch: 7.84 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.051442436331949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.051442436331949 | validation: 2.5872094611297607]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9709192276518932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9709192276518932 | validation: 2.54268208871301]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7694388624805368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7694388624805368 | validation: 2.493633377675354]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5866752847569685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5866752847569685 | validation: 2.2714954501289046]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6355721658687032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6355721658687032 | validation: 2.256570513039033]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5769234724694237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5769234724694237 | validation: 2.2953072154477567]
	TIME [epoch: 7.85 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4360612743213432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4360612743213432 | validation: 2.2458647090935258]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5500076443793052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5500076443793052 | validation: 2.3776999550676576]
	TIME [epoch: 7.83 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4841837743800375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4841837743800375 | validation: 2.198874609219229]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4486220428065104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4486220428065104 | validation: 2.360062345500634]
	TIME [epoch: 7.84 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4505139534804805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4505139534804805 | validation: 2.380257232907484]
	TIME [epoch: 7.88 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5431866812854125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5431866812854125 | validation: 2.1391296770266344]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3890531066183245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3890531066183245 | validation: 2.1389587001132444]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3577233790634784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3577233790634784 | validation: 2.070187197366569]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5176333129845896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5176333129845896 | validation: 2.1152103617905915]
	TIME [epoch: 7.88 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.346288142745455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.346288142745455 | validation: 2.0572672702792367]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3267744110566349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3267744110566349 | validation: 2.037606680556091]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3789687225636564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3789687225636564 | validation: 2.3673500176635542]
	TIME [epoch: 7.84 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4538158123079583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4538158123079583 | validation: 2.082450335581208]
	TIME [epoch: 7.83 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2609655444903562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2609655444903562 | validation: 2.1093047813586616]
	TIME [epoch: 7.9 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.278363338045153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.278363338045153 | validation: 2.01279937401044]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3781080368980294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3781080368980294 | validation: 2.0461005074084353]
	TIME [epoch: 7.84 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3720093067079995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3720093067079995 | validation: 1.9888618668056208]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.283732762720478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.283732762720478 | validation: 1.9763249683406792]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2239352879841505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2239352879841505 | validation: 1.988106445617296]
	TIME [epoch: 7.88 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2903296240955033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2903296240955033 | validation: 1.9355276064662479]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.260564580796419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.260564580796419 | validation: 1.9747615522011746]
	TIME [epoch: 7.87 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2128349596962278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2128349596962278 | validation: 1.8990477981190246]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3344218818583555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3344218818583555 | validation: 2.0565239070904493]
	TIME [epoch: 7.88 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4216747752905299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4216747752905299 | validation: 2.0831156559847686]
	TIME [epoch: 7.82 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4912190843156912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4912190843156912 | validation: 1.8057937538021775]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.32300028468537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.32300028468537 | validation: 1.7801416448101501]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.317763720438002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.317763720438002 | validation: 1.7705638461345785]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3050404794028754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3050404794028754 | validation: 1.7611558739182502]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3083946529941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3083946529941 | validation: 1.7686911232685965]
	TIME [epoch: 7.84 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3260201762634447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3260201762634447 | validation: 1.762039409280504]
	TIME [epoch: 7.85 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.324721179415514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.324721179415514 | validation: 1.7595544376634318]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3246511465517778		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.3246511465517778 | validation: 1.721597232655367]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3120068765832573		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.3120068765832573 | validation: 1.719744485678111]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3193831061310681		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.3193831061310681 | validation: 1.728533340809547]
	TIME [epoch: 7.84 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3071946670681158		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.3071946670681158 | validation: 1.6673182297477918]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2954316826948546		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.2954316826948546 | validation: 1.6729084987453997]
	TIME [epoch: 7.83 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2820142361412035		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.2820142361412035 | validation: 1.5682101983885317]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2958905198399775		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.2958905198399775 | validation: 1.5992740607719222]
	TIME [epoch: 7.84 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2623985538766465		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.2623985538766465 | validation: 1.3226383960611057]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0976828781792962		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.0976828781792962 | validation: 1.1994392579707225]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4377973677972578		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.4377973677972578 | validation: 1.398299117543952]
	TIME [epoch: 7.91 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0861087853095237		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.0861087853095237 | validation: 1.2522037029735]
	TIME [epoch: 7.83 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.025459525644406		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.025459525644406 | validation: 0.995054516792371]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9231217183651613		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.9231217183651613 | validation: 1.1138320670166681]
	TIME [epoch: 7.89 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8600315437439312		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.8600315437439312 | validation: 0.7616313833284745]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0320443880116414		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.0320443880116414 | validation: 1.3374816895540786]
	TIME [epoch: 7.87 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9771187035756277		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.9771187035756277 | validation: 1.1417154340837379]
	TIME [epoch: 7.82 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9532483749301883		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.9532483749301883 | validation: 0.8435086607250644]
	TIME [epoch: 7.82 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8391579215033603		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.8391579215033603 | validation: 0.9122287938262714]
	TIME [epoch: 7.82 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.747802267702347		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.747802267702347 | validation: 0.8418594992120375]
	TIME [epoch: 7.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7001194010934684		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7001194010934684 | validation: 0.8628575980603217]
	TIME [epoch: 7.84 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7509462946737046		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.7509462946737046 | validation: 0.6948943038636173]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7323805699722179		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.7323805699722179 | validation: 0.7776450837879747]
	TIME [epoch: 7.84 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.700845864727842		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.700845864727842 | validation: 0.7207029313498048]
	TIME [epoch: 7.83 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6317868029716742		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.6317868029716742 | validation: 1.1028792201865]
	TIME [epoch: 7.87 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7608664659133725		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.7608664659133725 | validation: 0.6426975611586294]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6809578448418104		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.6809578448418104 | validation: 0.6093201409125282]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.638123494902658		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.638123494902658 | validation: 0.5998345947508938]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6045482756410445		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6045482756410445 | validation: 0.6448382844853077]
	TIME [epoch: 7.84 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7279664268265675		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.7279664268265675 | validation: 0.5243379452815916]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.614578367638345		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.614578367638345 | validation: 0.6229727065863742]
	TIME [epoch: 7.84 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6325429725011201		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.6325429725011201 | validation: 0.5395027010862159]
	TIME [epoch: 7.82 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6178692475672258		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.6178692475672258 | validation: 0.633112313682481]
	TIME [epoch: 7.86 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5901282584669363		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5901282584669363 | validation: 0.6086308455134848]
	TIME [epoch: 7.87 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6351587678664619		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.6351587678664619 | validation: 0.5405507124928172]
	TIME [epoch: 7.8 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5893468120868481		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5893468120868481 | validation: 0.5769811095676941]
	TIME [epoch: 7.81 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5598078534012931		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5598078534012931 | validation: 0.4606816855356516]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.501787900717543		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.501787900717543 | validation: 0.4373842439158391]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6756178859992683		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.6756178859992683 | validation: 0.8308016321264158]
	TIME [epoch: 7.89 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6295040685327055		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.6295040685327055 | validation: 0.6083533040869242]
	TIME [epoch: 7.83 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5390888843030941		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.5390888843030941 | validation: 0.45414198793101407]
	TIME [epoch: 7.88 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5481526030066608		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5481526030066608 | validation: 0.4610375385954684]
	TIME [epoch: 7.83 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5200351684092888		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.5200351684092888 | validation: 0.5411492751309572]
	TIME [epoch: 7.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5693346714764779		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.5693346714764779 | validation: 0.5879839742223707]
	TIME [epoch: 7.83 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46000454910333444		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.46000454910333444 | validation: 0.44852653262851905]
	TIME [epoch: 7.82 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.354414352682886		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.354414352682886 | validation: 0.6767382593988347]
	TIME [epoch: 7.81 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4350114151117663		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.4350114151117663 | validation: 0.28871329466782564]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26774896616781557		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.26774896616781557 | validation: 0.46221314532383545]
	TIME [epoch: 7.87 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3419688645566906		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.3419688645566906 | validation: 0.3297875468331025]
	TIME [epoch: 7.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29156219032306363		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.29156219032306363 | validation: 0.31462642323525375]
	TIME [epoch: 7.83 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3244067545688538		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3244067545688538 | validation: 0.3666538503656855]
	TIME [epoch: 7.83 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3039764920229432		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.3039764920229432 | validation: 0.3525429121915127]
	TIME [epoch: 7.83 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3094882183549738		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.3094882183549738 | validation: 0.2791146724949839]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26571438194635394		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.26571438194635394 | validation: 0.2564662782816924]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26759300339281744		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.26759300339281744 | validation: 0.29628556325024014]
	TIME [epoch: 7.83 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2808177624481768		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2808177624481768 | validation: 0.31841444603728264]
	TIME [epoch: 7.78 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25002203999667055		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.25002203999667055 | validation: 0.2936234789297564]
	TIME [epoch: 7.84 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2648705131879015		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2648705131879015 | validation: 0.4728159420231629]
	TIME [epoch: 7.83 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3593286196143244		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3593286196143244 | validation: 0.300973018428512]
	TIME [epoch: 7.8 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26181688270519327		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.26181688270519327 | validation: 0.24256641254291894]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2882226089937985		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2882226089937985 | validation: 0.41405793383647216]
	TIME [epoch: 7.81 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3504255791762274		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3504255791762274 | validation: 0.23652319937360272]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26000580037447907		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.26000580037447907 | validation: 0.26231135988226295]
	TIME [epoch: 7.83 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23603685672756736		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.23603685672756736 | validation: 0.215452966393751]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21945114881687444		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.21945114881687444 | validation: 0.4379928166024544]
	TIME [epoch: 7.85 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33031513143860836		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.33031513143860836 | validation: 0.22699200614473286]
	TIME [epoch: 7.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21832021292545759		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.21832021292545759 | validation: 0.23078190556722156]
	TIME [epoch: 7.8 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28472159341344744		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.28472159341344744 | validation: 0.31324340130033224]
	TIME [epoch: 7.76 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24224099287588374		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.24224099287588374 | validation: 0.26294997209595017]
	TIME [epoch: 7.82 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23943692986789517		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.23943692986789517 | validation: 0.3736161829286951]
	TIME [epoch: 7.83 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25831633642956964		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.25831633642956964 | validation: 0.27121064964352243]
	TIME [epoch: 7.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21406267050473007		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.21406267050473007 | validation: 0.2787233531043302]
	TIME [epoch: 7.86 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24162038265460917		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.24162038265460917 | validation: 0.46110820278582343]
	TIME [epoch: 7.84 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30987385466646733		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.30987385466646733 | validation: 0.3317191988246071]
	TIME [epoch: 7.83 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700599559902594		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.2700599559902594 | validation: 0.4023445599619485]
	TIME [epoch: 7.86 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2381290731645412		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2381290731645412 | validation: 0.2130274097963923]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20271047648282292		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.20271047648282292 | validation: 0.18810866408371132]
	TIME [epoch: 7.91 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25244983303484136		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.25244983303484136 | validation: 0.32630977892785523]
	TIME [epoch: 7.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21172873472733866		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.21172873472733866 | validation: 0.23203131663015797]
	TIME [epoch: 7.84 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2005468089412752		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2005468089412752 | validation: 0.2953309475040732]
	TIME [epoch: 7.83 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31783461460703666		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.31783461460703666 | validation: 0.41879480465470886]
	TIME [epoch: 7.87 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26767200389211154		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.26767200389211154 | validation: 0.2855427572375132]
	TIME [epoch: 7.82 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21880457035764195		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.21880457035764195 | validation: 0.1915493818623996]
	TIME [epoch: 7.83 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2022228227260524		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.2022228227260524 | validation: 0.22790168043315792]
	TIME [epoch: 7.82 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24733645330617607		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.24733645330617607 | validation: 0.3896134111316128]
	TIME [epoch: 7.86 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2775412543562191		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.2775412543562191 | validation: 0.4019808276205561]
	TIME [epoch: 7.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2499889108028915		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2499889108028915 | validation: 0.1822122431787927]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17808436619732923		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.17808436619732923 | validation: 0.1830291530096751]
	TIME [epoch: 7.9 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2160541467017934		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.2160541467017934 | validation: 0.38656503628184713]
	TIME [epoch: 7.84 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3688159047911162		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.3688159047911162 | validation: 0.3014828927460848]
	TIME [epoch: 7.87 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2513221435428923		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2513221435428923 | validation: 0.22838992250265028]
	TIME [epoch: 7.83 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22157364435406027		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.22157364435406027 | validation: 0.19112869567236693]
	TIME [epoch: 7.83 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17543451670275512		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.17543451670275512 | validation: 0.2310710915616206]
	TIME [epoch: 7.84 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1872531171696634		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.1872531171696634 | validation: 0.303268796938427]
	TIME [epoch: 7.85 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24882403847666215		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.24882403847666215 | validation: 0.1849197672282449]
	TIME [epoch: 7.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2004362968058569		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.2004362968058569 | validation: 0.19607257215126564]
	TIME [epoch: 7.83 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23445451621180752		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.23445451621180752 | validation: 0.24245142672827094]
	TIME [epoch: 7.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20039254444077312		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.20039254444077312 | validation: 0.2301025743360688]
	TIME [epoch: 7.86 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21269347736011515		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.21269347736011515 | validation: 0.17977020567000213]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2377381560718401		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2377381560718401 | validation: 0.27437845767676766]
	TIME [epoch: 7.86 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21160974520282366		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.21160974520282366 | validation: 0.21508730686648247]
	TIME [epoch: 7.82 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17589056521413696		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.17589056521413696 | validation: 0.17596073195408216]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23029812262058347		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.23029812262058347 | validation: 0.37059027041387094]
	TIME [epoch: 7.81 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2757181155242631		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.2757181155242631 | validation: 0.2066998197664608]
	TIME [epoch: 7.81 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21065830719390843		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.21065830719390843 | validation: 0.1733504984422039]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17244484479540795		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.17244484479540795 | validation: 0.2081376167995704]
	TIME [epoch: 7.82 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22191371719532588		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.22191371719532588 | validation: 0.31298072013035955]
	TIME [epoch: 7.81 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23990501997829541		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.23990501997829541 | validation: 0.20335406558996194]
	TIME [epoch: 7.8 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19049905745037232		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.19049905745037232 | validation: 0.16611997063244918]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18633964620668872		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.18633964620668872 | validation: 0.18129558807987411]
	TIME [epoch: 7.83 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22800738280347083		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.22800738280347083 | validation: 0.23414759018840722]
	TIME [epoch: 7.82 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.174987573463878		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.174987573463878 | validation: 0.16030875508714137]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16342070049724866		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.16342070049724866 | validation: 0.128223513180824]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28073592919910834		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.28073592919910834 | validation: 0.6037520236782423]
	TIME [epoch: 7.88 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30814812336715264		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.30814812336715264 | validation: 0.18374572220863136]
	TIME [epoch: 7.82 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1594482768289747		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.1594482768289747 | validation: 0.1861151555926021]
	TIME [epoch: 7.82 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1480121445899528		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.1480121445899528 | validation: 0.17513540522377397]
	TIME [epoch: 7.83 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1802949085912631		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.1802949085912631 | validation: 0.20125057769115057]
	TIME [epoch: 7.87 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16095225566692037		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.16095225566692037 | validation: 0.2380628176725098]
	TIME [epoch: 7.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19234688085934068		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.19234688085934068 | validation: 0.2041949749658363]
	TIME [epoch: 7.83 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15255571921818767		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.15255571921818767 | validation: 0.19674968198830162]
	TIME [epoch: 7.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16802071093111748		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.16802071093111748 | validation: 0.27167250993484915]
	TIME [epoch: 7.87 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1800669951669639		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.1800669951669639 | validation: 0.1503837871547154]
	TIME [epoch: 7.88 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1897897824387343		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.1897897824387343 | validation: 0.520733830360544]
	TIME [epoch: 7.83 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3546075908110727		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.3546075908110727 | validation: 0.3109265892175228]
	TIME [epoch: 7.84 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21400409100259243		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.21400409100259243 | validation: 0.19183058273087983]
	TIME [epoch: 7.84 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1671432571811963		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.1671432571811963 | validation: 0.1773053447998837]
	TIME [epoch: 7.82 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14209396753377013		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.14209396753377013 | validation: 0.18397140593903585]
	TIME [epoch: 7.89 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2193664999630504		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.2193664999630504 | validation: 0.21632521406349597]
	TIME [epoch: 7.83 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17586576468232137		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.17586576468232137 | validation: 0.17078538862797943]
	TIME [epoch: 7.82 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1694453199534717		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1694453199534717 | validation: 0.2558638516713691]
	TIME [epoch: 7.84 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14893850000955744		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.14893850000955744 | validation: 0.25583048329430375]
	TIME [epoch: 7.84 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26409999517702737		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.26409999517702737 | validation: 0.2755419499699814]
	TIME [epoch: 7.89 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16788507312918205		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.16788507312918205 | validation: 0.1458701738067708]
	TIME [epoch: 7.88 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20365558917953783		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.20365558917953783 | validation: 0.2754430990503821]
	TIME [epoch: 7.85 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20482664508456316		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.20482664508456316 | validation: 0.23297058848193014]
	TIME [epoch: 7.84 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15983692953002182		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.15983692953002182 | validation: 0.2909056846252408]
	TIME [epoch: 7.87 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19576423076874286		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.19576423076874286 | validation: 0.13255944288746113]
	TIME [epoch: 7.84 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14572165515459967		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.14572165515459967 | validation: 0.17409473867142547]
	TIME [epoch: 7.82 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23271976641947711		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.23271976641947711 | validation: 0.3076364542396227]
	TIME [epoch: 7.82 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605756028263317		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.2605756028263317 | validation: 0.28760265701921073]
	TIME [epoch: 7.83 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2240552129367809		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.2240552129367809 | validation: 0.15770370934516786]
	TIME [epoch: 7.88 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14694528450665026		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.14694528450665026 | validation: 0.21591943523693738]
	TIME [epoch: 7.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17625019859768543		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.17625019859768543 | validation: 0.20894292791922908]
	TIME [epoch: 7.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21490992789656183		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.21490992789656183 | validation: 0.18252993231526998]
	TIME [epoch: 7.84 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18704523985315075		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.18704523985315075 | validation: 0.21484159340111963]
	TIME [epoch: 7.82 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1596366737778218		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1596366737778218 | validation: 0.14881511961724112]
	TIME [epoch: 7.91 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15790754978871885		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.15790754978871885 | validation: 0.1976136453927192]
	TIME [epoch: 7.85 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20190755624513912		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.20190755624513912 | validation: 0.1629321140576437]
	TIME [epoch: 7.87 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.146306901296521		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.146306901296521 | validation: 0.17852972922762103]
	TIME [epoch: 7.86 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1418148679687688		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.1418148679687688 | validation: 0.1390556432648002]
	TIME [epoch: 7.87 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16082969525534152		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.16082969525534152 | validation: 0.21516565338287252]
	TIME [epoch: 7.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14907501828084874		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.14907501828084874 | validation: 0.24879879279962033]
	TIME [epoch: 7.81 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19978483688565596		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.19978483688565596 | validation: 0.16586009563661935]
	TIME [epoch: 7.81 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15944306123836266		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.15944306123836266 | validation: 0.15031916359808073]
	TIME [epoch: 7.82 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13410996116885082		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.13410996116885082 | validation: 0.228018983156706]
	TIME [epoch: 7.86 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16583911200362683		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.16583911200362683 | validation: 0.14239130478915563]
	TIME [epoch: 7.83 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1919530594058534		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.1919530594058534 | validation: 0.17844021781865882]
	TIME [epoch: 7.83 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13557410060214667		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.13557410060214667 | validation: 0.20830241465483212]
	TIME [epoch: 7.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17518984355574424		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.17518984355574424 | validation: 0.1596165148158237]
	TIME [epoch: 7.83 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14495952794408135		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.14495952794408135 | validation: 0.1847244626673038]
	TIME [epoch: 7.88 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1572030836529102		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.1572030836529102 | validation: 0.21780691586795087]
	TIME [epoch: 7.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1664672681787056		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1664672681787056 | validation: 0.1624523524324527]
	TIME [epoch: 7.84 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488387909515117		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.1488387909515117 | validation: 0.1380454355024544]
	TIME [epoch: 7.83 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1418953276370407		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.1418953276370407 | validation: 0.11058366462143035]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17082952126682632		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.17082952126682632 | validation: 0.19533590382734378]
	TIME [epoch: 7.88 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1535841264535998		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.1535841264535998 | validation: 0.17087092019480055]
	TIME [epoch: 7.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1185889203649301		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.1185889203649301 | validation: 0.18486230518273716]
	TIME [epoch: 7.79 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15975863554282227		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.15975863554282227 | validation: 0.12082766379692197]
	TIME [epoch: 7.81 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1335539273054217		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.1335539273054217 | validation: 0.12675323052092655]
	TIME [epoch: 7.83 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13118805732719518		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.13118805732719518 | validation: 0.13174043549695855]
	TIME [epoch: 7.83 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1047670302543165		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.1047670302543165 | validation: 0.1008192906338293]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1583430868524389		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.1583430868524389 | validation: 0.13154267338759484]
	TIME [epoch: 7.81 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18752798038382043		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.18752798038382043 | validation: 0.20231728503686913]
	TIME [epoch: 7.82 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14368449922554222		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.14368449922554222 | validation: 0.12418118433963833]
	TIME [epoch: 7.85 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11261502190473893		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.11261502190473893 | validation: 0.10849090215716548]
	TIME [epoch: 7.81 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12565010128445012		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.12565010128445012 | validation: 0.14313519120438353]
	TIME [epoch: 7.82 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13427740449579254		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.13427740449579254 | validation: 0.1400224744069205]
	TIME [epoch: 7.85 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11003460124908893		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.11003460124908893 | validation: 0.09075397744422169]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12379504036692007		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.12379504036692007 | validation: 0.12289075303354947]
	TIME [epoch: 7.86 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09876576531584376		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.09876576531584376 | validation: 0.10608481876988624]
	TIME [epoch: 7.82 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12427631704459358		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.12427631704459358 | validation: 0.1694216427761321]
	TIME [epoch: 7.82 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14910908983782467		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.14910908983782467 | validation: 0.0919914878080646]
	TIME [epoch: 7.83 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09649202542263315		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.09649202542263315 | validation: 0.10361741309805142]
	TIME [epoch: 7.83 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11443251356507829		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.11443251356507829 | validation: 0.09463985054357016]
	TIME [epoch: 7.86 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10243842468178291		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.10243842468178291 | validation: 0.17262936400000645]
	TIME [epoch: 7.82 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11670905728352758		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.11670905728352758 | validation: 0.08091142996138456]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.104251886286654		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.104251886286654 | validation: 0.21367498847659533]
	TIME [epoch: 7.85 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1738022624825499		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1738022624825499 | validation: 0.09436853780263103]
	TIME [epoch: 7.91 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10539893584720669		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.10539893584720669 | validation: 0.18336653615112322]
	TIME [epoch: 7.85 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12377762468749819		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.12377762468749819 | validation: 0.10624391650775603]
	TIME [epoch: 7.81 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10628621025674503		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.10628621025674503 | validation: 0.13302155938920612]
	TIME [epoch: 7.81 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12040390850125099		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.12040390850125099 | validation: 0.07219508496992819]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11198493877122719		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11198493877122719 | validation: 0.2198234812542801]
	TIME [epoch: 7.82 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13877883784745143		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.13877883784745143 | validation: 0.14471036291797862]
	TIME [epoch: 7.79 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10992154762794451		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.10992154762794451 | validation: 0.10021963467760604]
	TIME [epoch: 7.79 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08033942345220069		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.08033942345220069 | validation: 0.12149812997095165]
	TIME [epoch: 7.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08016543554549077		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.08016543554549077 | validation: 0.05775500330933511]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11034488406904913		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.11034488406904913 | validation: 0.07482420736314205]
	TIME [epoch: 7.87 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11861725043783972		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.11861725043783972 | validation: 0.14039451214719237]
	TIME [epoch: 7.81 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14224104699539702		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.14224104699539702 | validation: 0.07530427723496748]
	TIME [epoch: 7.83 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0813608190721626		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.0813608190721626 | validation: 0.07520119590901689]
	TIME [epoch: 7.81 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09749552446004604		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.09749552446004604 | validation: 0.06178139112429824]
	TIME [epoch: 7.89 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0643377473370305		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.0643377473370305 | validation: 0.11407203063894722]
	TIME [epoch: 7.85 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14894914820377286		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.14894914820377286 | validation: 0.08311993481045488]
	TIME [epoch: 7.81 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08594527535297489		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.08594527535297489 | validation: 0.14169114767690633]
	TIME [epoch: 7.81 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10739185223647083		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.10739185223647083 | validation: 0.12006768459979508]
	TIME [epoch: 7.83 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1105083649591029		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.1105083649591029 | validation: 0.12681767731057034]
	TIME [epoch: 7.85 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10588535441785189		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.10588535441785189 | validation: 0.055882259150906805]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06729951941426124		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.06729951941426124 | validation: 0.06266388806393391]
	TIME [epoch: 7.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07271998195528229		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.07271998195528229 | validation: 0.07405284296373724]
	TIME [epoch: 7.81 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08481506934790631		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.08481506934790631 | validation: 0.17117937866001687]
	TIME [epoch: 7.82 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13662737009852777		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.13662737009852777 | validation: 0.062233459979220654]
	TIME [epoch: 7.87 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11148630725536478		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.11148630725536478 | validation: 0.09559899134335981]
	TIME [epoch: 7.82 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0759345617974755		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.0759345617974755 | validation: 0.05769842830493273]
	TIME [epoch: 7.83 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0592114195413061		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.0592114195413061 | validation: 0.10099796511741257]
	TIME [epoch: 7.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12514437806744408		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.12514437806744408 | validation: 0.06039533108415425]
	TIME [epoch: 7.87 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06706422564768978		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.06706422564768978 | validation: 0.06034719668183494]
	TIME [epoch: 7.86 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07194863642468745		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.07194863642468745 | validation: 0.08382292972862151]
	TIME [epoch: 7.81 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07258939278821071		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.07258939278821071 | validation: 0.10416579767520312]
	TIME [epoch: 7.83 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.071966481118233		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.071966481118233 | validation: 0.0782940920649693]
	TIME [epoch: 7.83 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06372043240964591		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.06372043240964591 | validation: 0.057057775212278936]
	TIME [epoch: 7.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060193987733585835		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.060193987733585835 | validation: 0.14203768138864795]
	TIME [epoch: 7.83 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07514812536252892		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.07514812536252892 | validation: 0.04449853990526245]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10830583807286402		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.10830583807286402 | validation: 0.09072198861292782]
	TIME [epoch: 7.83 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07636052539116903		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.07636052539116903 | validation: 0.0535329230893716]
	TIME [epoch: 7.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07334768865202185		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.07334768865202185 | validation: 0.07440266818324855]
	TIME [epoch: 7.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07920184564062321		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.07920184564062321 | validation: 0.061133544459785374]
	TIME [epoch: 7.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054168924793569335		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.054168924793569335 | validation: 0.05144642034338914]
	TIME [epoch: 7.81 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060125715829194806		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.060125715829194806 | validation: 0.05700581948752995]
	TIME [epoch: 7.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06102266030349006		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.06102266030349006 | validation: 0.066659855126979]
	TIME [epoch: 7.81 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07578186068991677		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.07578186068991677 | validation: 0.05560630171442128]
	TIME [epoch: 7.86 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052361210931354096		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.052361210931354096 | validation: 0.05070697007564166]
	TIME [epoch: 7.82 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04754486442179395		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.04754486442179395 | validation: 0.05966031023195867]
	TIME [epoch: 7.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07513186581537874		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.07513186581537874 | validation: 0.08989642016104152]
	TIME [epoch: 7.76 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05952849673112302		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.05952849673112302 | validation: 0.04920258930295869]
	TIME [epoch: 7.77 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05977693074764227		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.05977693074764227 | validation: 0.052593286370004134]
	TIME [epoch: 7.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06956006117161506		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.06956006117161506 | validation: 0.09766408122565741]
	TIME [epoch: 7.76 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07880847180138652		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.07880847180138652 | validation: 0.0488519146739415]
	TIME [epoch: 7.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05377603099471588		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.05377603099471588 | validation: 0.03751894163195363]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05637229306564644		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.05637229306564644 | validation: 0.12356792358133636]
	TIME [epoch: 7.86 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08029328483795566		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.08029328483795566 | validation: 0.09231455266222527]
	TIME [epoch: 7.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06273731698172208		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.06273731698172208 | validation: 0.04339637437043781]
	TIME [epoch: 7.84 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043882086300894976		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.043882086300894976 | validation: 0.030849552738631547]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03883147202167039		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.03883147202167039 | validation: 0.057539637654253906]
	TIME [epoch: 7.81 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06439424748812204		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.06439424748812204 | validation: 0.03987515642479582]
	TIME [epoch: 7.88 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04424715417036449		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.04424715417036449 | validation: 0.0526410114088886]
	TIME [epoch: 7.82 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0442438473354858		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.0442438473354858 | validation: 0.05037173800123575]
	TIME [epoch: 7.82 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0429429365939582		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.0429429365939582 | validation: 0.06964532782986727]
	TIME [epoch: 7.82 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07383735919489669		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.07383735919489669 | validation: 0.08033925122993889]
	TIME [epoch: 7.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06851519990539301		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.06851519990539301 | validation: 0.032628878781089]
	TIME [epoch: 7.86 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03797671741221465		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.03797671741221465 | validation: 0.02609504506071568]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04271047061352728		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.04271047061352728 | validation: 0.0363637315269144]
	TIME [epoch: 7.85 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06984724969055998		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.06984724969055998 | validation: 0.03300962056167207]
	TIME [epoch: 7.82 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04722375395371607		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.04722375395371607 | validation: 0.08113141139063884]
	TIME [epoch: 7.85 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05409402046443419		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.05409402046443419 | validation: 0.03737680853197713]
	TIME [epoch: 7.83 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042011246919254557		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.042011246919254557 | validation: 0.04441285906921716]
	TIME [epoch: 7.82 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0504361893763062		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.0504361893763062 | validation: 0.033679425740190526]
	TIME [epoch: 7.82 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0408710889549162		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.0408710889549162 | validation: 0.0639569495742506]
	TIME [epoch: 7.81 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052205200942458736		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.052205200942458736 | validation: 0.08912434422135003]
	TIME [epoch: 7.86 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051155570185578134		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.051155570185578134 | validation: 0.03350135868524081]
	TIME [epoch: 7.83 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02768432856650553		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.02768432856650553 | validation: 0.021484482379140885]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04079245568990257		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.04079245568990257 | validation: 0.03514362808663907]
	TIME [epoch: 7.82 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047853963178334985		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.047853963178334985 | validation: 0.1250635003956585]
	TIME [epoch: 7.82 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0990150912745788		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.0990150912745788 | validation: 0.05771433236240271]
	TIME [epoch: 7.87 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04326336529187526		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.04326336529187526 | validation: 0.02622887991782557]
	TIME [epoch: 7.82 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02485197663952969		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.02485197663952969 | validation: 0.030244021004852455]
	TIME [epoch: 7.81 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03490872807636061		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.03490872807636061 | validation: 0.04628267352725192]
	TIME [epoch: 7.82 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05949443343044851		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.05949443343044851 | validation: 0.048921110594312395]
	TIME [epoch: 7.83 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038664624401633284		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.038664624401633284 | validation: 0.032343537926692444]
	TIME [epoch: 7.85 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03008130394270918		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.03008130394270918 | validation: 0.03806737850493573]
	TIME [epoch: 7.83 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042988854384840354		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.042988854384840354 | validation: 0.04744629916847964]
	TIME [epoch: 7.82 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04552822128055098		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.04552822128055098 | validation: 0.04942109074421016]
	TIME [epoch: 7.83 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05306057386612333		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.05306057386612333 | validation: 0.04987824589120693]
	TIME [epoch: 7.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03846158689201218		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.03846158689201218 | validation: 0.05489360333646576]
	TIME [epoch: 7.81 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04153690818634028		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.04153690818634028 | validation: 0.03345053968774818]
	TIME [epoch: 7.82 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0407426286375519		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.0407426286375519 | validation: 0.03000445646244715]
	TIME [epoch: 7.82 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027677913644669084		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.027677913644669084 | validation: 0.021577063279579388]
	TIME [epoch: 7.83 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062149230380335846		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.062149230380335846 | validation: 0.04866616226474235]
	TIME [epoch: 7.87 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0460273728504318		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.0460273728504318 | validation: 0.07616470755348051]
	TIME [epoch: 7.83 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038391625542637184		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.038391625542637184 | validation: 0.030145807591005727]
	TIME [epoch: 7.82 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025934187697705405		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.025934187697705405 | validation: 0.05430397072918443]
	TIME [epoch: 7.83 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04199878979015725		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.04199878979015725 | validation: 0.02328667897918795]
	TIME [epoch: 7.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027379696408236814		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.027379696408236814 | validation: 0.024398394446196882]
	TIME [epoch: 7.89 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05470566886125168		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.05470566886125168 | validation: 0.030125658068979287]
	TIME [epoch: 7.85 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023063954554407864		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.023063954554407864 | validation: 0.01961200782774197]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042655174079077804		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.042655174079077804 | validation: 0.04344951672570112]
	TIME [epoch: 7.84 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03635933327806326		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.03635933327806326 | validation: 0.029065377849297885]
	TIME [epoch: 7.86 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02588861595645553		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.02588861595645553 | validation: 0.029816255629806464]
	TIME [epoch: 7.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02998101995222982		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.02998101995222982 | validation: 0.04337847817368301]
	TIME [epoch: 7.82 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04253429348591716		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.04253429348591716 | validation: 0.03500459851492456]
	TIME [epoch: 7.83 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028057279604954143		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.028057279604954143 | validation: 0.026105593432921757]
	TIME [epoch: 7.82 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02581684986196609		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.02581684986196609 | validation: 0.05216553847819782]
	TIME [epoch: 7.87 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04453630330851233		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.04453630330851233 | validation: 0.01915247939207865]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01879087277180179		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.01879087277180179 | validation: 0.02867390624936149]
	TIME [epoch: 7.82 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04370878078742603		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.04370878078742603 | validation: 0.06250863389327307]
	TIME [epoch: 7.83 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034000888150107406		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.034000888150107406 | validation: 0.03234440143587078]
	TIME [epoch: 7.82 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024659933447049753		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.024659933447049753 | validation: 0.022308291151937706]
	TIME [epoch: 7.87 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031230895544098865		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.031230895544098865 | validation: 0.03756605914207318]
	TIME [epoch: 7.83 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03611956288471009		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.03611956288471009 | validation: 0.027622871545461433]
	TIME [epoch: 7.82 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028903107151423253		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.028903107151423253 | validation: 0.022089060799069457]
	TIME [epoch: 7.83 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03211411010690768		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.03211411010690768 | validation: 0.026472516440384175]
	TIME [epoch: 7.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02445478287114662		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.02445478287114662 | validation: 0.07803800560819343]
	TIME [epoch: 7.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04875893859704021		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.04875893859704021 | validation: 0.07300118002909281]
	TIME [epoch: 7.83 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04408640111022822		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.04408640111022822 | validation: 0.03527924435574388]
	TIME [epoch: 7.82 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026727352912828167		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.026727352912828167 | validation: 0.017432219758831934]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01888703956100446		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.01888703956100446 | validation: 0.020055896740473672]
	TIME [epoch: 7.88 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039491709866952523		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.039491709866952523 | validation: 0.05354571115561916]
	TIME [epoch: 7.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034381114383673306		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.034381114383673306 | validation: 0.02105596719716027]
	TIME [epoch: 7.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040290796487396516		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.040290796487396516 | validation: 0.03532408940034677]
	TIME [epoch: 7.82 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029416172927662687		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.029416172927662687 | validation: 0.030241813898765423]
	TIME [epoch: 7.82 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019132302708510268		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.019132302708510268 | validation: 0.02698787302392184]
	TIME [epoch: 7.87 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027402575947634643		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.027402575947634643 | validation: 0.024977977568072372]
	TIME [epoch: 7.84 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02249160366296174		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.02249160366296174 | validation: 0.03583257467267616]
	TIME [epoch: 7.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033425407706041126		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.033425407706041126 | validation: 0.024339650494999657]
	TIME [epoch: 7.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02886026509901999		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.02886026509901999 | validation: 0.0365564639495152]
	TIME [epoch: 7.84 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02981625494021297		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.02981625494021297 | validation: 0.02073585032981259]
	TIME [epoch: 7.88 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023641422748739163		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.023641422748739163 | validation: 0.02168701058836251]
	TIME [epoch: 7.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021928437608050612		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.021928437608050612 | validation: 0.051667625805182485]
	TIME [epoch: 7.85 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038498892650729755		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.038498892650729755 | validation: 0.017213816303314707]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016593616749584275		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.016593616749584275 | validation: 0.019643744454479275]
	TIME [epoch: 7.87 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013579860507151466		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.013579860507151466 | validation: 0.017640606183997022]
	TIME [epoch: 7.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028373498905311698		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.028373498905311698 | validation: 0.04744473639333776]
	TIME [epoch: 7.84 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03469343249640041		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.03469343249640041 | validation: 0.03187526481238999]
	TIME [epoch: 7.83 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02206445090851973		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.02206445090851973 | validation: 0.0189509504485974]
	TIME [epoch: 7.83 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014491945892830354		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.014491945892830354 | validation: 0.02393135716092047]
	TIME [epoch: 7.88 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035492736676922716		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.035492736676922716 | validation: 0.03056135612742191]
	TIME [epoch: 7.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029043781112149655		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.029043781112149655 | validation: 0.02725502700109451]
	TIME [epoch: 7.83 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018475467295833433		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.018475467295833433 | validation: 0.042586304304333654]
	TIME [epoch: 7.84 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0349362930465036		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.0349362930465036 | validation: 0.018249336847343093]
	TIME [epoch: 7.83 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023488432611540773		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.023488432611540773 | validation: 0.024688207425044507]
	TIME [epoch: 7.88 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02000250679156286		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.02000250679156286 | validation: 0.04714041374632692]
	TIME [epoch: 7.86 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02921471845724		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.02921471845724 | validation: 0.023957848630134608]
	TIME [epoch: 7.86 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01970692939808814		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.01970692939808814 | validation: 0.04246163115895459]
	TIME [epoch: 7.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031769118334161954		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.031769118334161954 | validation: 0.02441658626822009]
	TIME [epoch: 7.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015923034533567235		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.015923034533567235 | validation: 0.017990719041369556]
	TIME [epoch: 7.86 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02826851308877818		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.02826851308877818 | validation: 0.04113195286443587]
	TIME [epoch: 7.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02422937422873996		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.02422937422873996 | validation: 0.026612597608448862]
	TIME [epoch: 7.83 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01816752780289832		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.01816752780289832 | validation: 0.01569113847197924]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014969097260271512		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.014969097260271512 | validation: 0.027424353144713122]
	TIME [epoch: 7.88 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03668411914028698		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.03668411914028698 | validation: 0.01844908203112218]
	TIME [epoch: 7.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011786396899694983		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.011786396899694983 | validation: 0.013433604171503187]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023104917396169943		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.023104917396169943 | validation: 0.028449468336057764]
	TIME [epoch: 7.84 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01916095895440119		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.01916095895440119 | validation: 0.024529735737394608]
	TIME [epoch: 7.83 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030385170628290624		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.030385170628290624 | validation: 0.018220148184176582]
	TIME [epoch: 7.88 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016970310224111846		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.016970310224111846 | validation: 0.03338194612930585]
	TIME [epoch: 7.84 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026062977478740756		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.026062977478740756 | validation: 0.01783038222669276]
	TIME [epoch: 7.85 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013237462350094216		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.013237462350094216 | validation: 0.017223745114942908]
	TIME [epoch: 7.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015253019461615912		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.015253019461615912 | validation: 0.033036317599585904]
	TIME [epoch: 7.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02840668961550172		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.02840668961550172 | validation: 0.01482513579058497]
	TIME [epoch: 7.89 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016915493948078265		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.016915493948078265 | validation: 0.020337788470782974]
	TIME [epoch: 7.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01459287748150205		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.01459287748150205 | validation: 0.02683469740196065]
	TIME [epoch: 7.82 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02805814442653432		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.02805814442653432 | validation: 0.01979716236327434]
	TIME [epoch: 7.86 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012443196098379608		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.012443196098379608 | validation: 0.019979951093992386]
	TIME [epoch: 7.9 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02379380197729849		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.02379380197729849 | validation: 0.016802138494874993]
	TIME [epoch: 7.9 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03598727734249833		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.03598727734249833 | validation: 0.02053262304946417]
	TIME [epoch: 7.85 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014663478365324074		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.014663478365324074 | validation: 0.01933788971605383]
	TIME [epoch: 7.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01703137725713872		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.01703137725713872 | validation: 0.018905659412439435]
	TIME [epoch: 7.84 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022919680147373958		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.022919680147373958 | validation: 0.023373977766565565]
	TIME [epoch: 7.88 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015932047810313803		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.015932047810313803 | validation: 0.018871572128729766]
	TIME [epoch: 7.85 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018658759915012063		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.018658759915012063 | validation: 0.01842148991752745]
	TIME [epoch: 7.82 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015239101443055096		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.015239101443055096 | validation: 0.018544117716203713]
	TIME [epoch: 7.83 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019042657497511876		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.019042657497511876 | validation: 0.024221660322803172]
	TIME [epoch: 7.84 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016360428230433228		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.016360428230433228 | validation: 0.021293931119972243]
	TIME [epoch: 7.89 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014533399680891658		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.014533399680891658 | validation: 0.017017650594652538]
	TIME [epoch: 7.85 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01736074575589347		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.01736074575589347 | validation: 0.03590137773744338]
	TIME [epoch: 7.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033826741770949534		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.033826741770949534 | validation: 0.02686975822462806]
	TIME [epoch: 7.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0143132682654091		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.0143132682654091 | validation: 0.015777275584505753]
	TIME [epoch: 7.83 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010239894652313916		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.010239894652313916 | validation: 0.016267842059804777]
	TIME [epoch: 7.9 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02570468996574382		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.02570468996574382 | validation: 0.019147946261219267]
	TIME [epoch: 7.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012048640154273767		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.012048640154273767 | validation: 0.0147099427516444]
	TIME [epoch: 7.82 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012094551552271926		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.012094551552271926 | validation: 0.02079563336820113]
	TIME [epoch: 7.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020896830323016532		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.020896830323016532 | validation: 0.016059144021065928]
	TIME [epoch: 7.87 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018578770038368685		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.018578770038368685 | validation: 0.016742780497580195]
	TIME [epoch: 7.88 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014614164645222551		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.014614164645222551 | validation: 0.012554576398540641]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009034774477037653		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.009034774477037653 | validation: 0.015023269355939117]
	TIME [epoch: 7.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011643855530405853		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.011643855530405853 | validation: 0.019142657078613207]
	TIME [epoch: 7.88 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029398295953869277		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.029398295953869277 | validation: 0.015318055491718458]
	TIME [epoch: 7.94 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011879138115933769		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.011879138115933769 | validation: 0.01083247416235518]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009620079133910888		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.009620079133910888 | validation: 0.020865249875574893]
	TIME [epoch: 7.84 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011969169932323612		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.011969169932323612 | validation: 0.04134688749634388]
	TIME [epoch: 7.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024158182250272865		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.024158182250272865 | validation: 0.03179420559404172]
	TIME [epoch: 7.86 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015240712841358552		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.015240712841358552 | validation: 0.011714833995633361]
	TIME [epoch: 7.88 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012474551644564923		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.012474551644564923 | validation: 0.026650353708413628]
	TIME [epoch: 7.85 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021857355511850387		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.021857355511850387 | validation: 0.014015184881642526]
	TIME [epoch: 7.85 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016387789760663556		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.016387789760663556 | validation: 0.016412398636246342]
	TIME [epoch: 7.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010773920713560328		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.010773920713560328 | validation: 0.019588009131182388]
	TIME [epoch: 7.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019851762613964125		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.019851762613964125 | validation: 0.01625418614676543]
	TIME [epoch: 7.88 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01091238093964227		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.01091238093964227 | validation: 0.010361594378874378]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009833848116501304		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.009833848116501304 | validation: 0.02306055160351124]
	TIME [epoch: 7.86 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02008941026306221		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.02008941026306221 | validation: 0.015184853523490683]
	TIME [epoch: 7.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024511391945023553		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.024511391945023553 | validation: 0.02084376842398743]
	TIME [epoch: 7.86 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01590497312530025		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.01590497312530025 | validation: 0.016836539109709156]
	TIME [epoch: 7.83 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010148955105637814		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.010148955105637814 | validation: 0.017822859497906966]
	TIME [epoch: 7.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016407617758286107		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.016407617758286107 | validation: 0.01660916590228975]
	TIME [epoch: 7.84 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011694281216309912		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.011694281216309912 | validation: 0.015228549505026757]
	TIME [epoch: 7.82 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00955546958117806		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.00955546958117806 | validation: 0.008976565708031042]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012771168388032094		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.012771168388032094 | validation: 0.0247165463367202]
	TIME [epoch: 7.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02146420247448014		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.02146420247448014 | validation: 0.011278963034630184]
	TIME [epoch: 7.82 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009815606712154371		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.009815606712154371 | validation: 0.016039518007489262]
	TIME [epoch: 7.81 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016681560588097867		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.016681560588097867 | validation: 0.012997521772213888]
	TIME [epoch: 7.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009374057844422261		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.009374057844422261 | validation: 0.011733502616745732]
	TIME [epoch: 7.88 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014121190149494565		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.014121190149494565 | validation: 0.02536602408695009]
	TIME [epoch: 7.81 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013935575407383823		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.013935575407383823 | validation: 0.013336316832885676]
	TIME [epoch: 7.79 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010098736915380956		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.010098736915380956 | validation: 0.011910116284270907]
	TIME [epoch: 7.79 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01586708195799276		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.01586708195799276 | validation: 0.015573438395835745]
	TIME [epoch: 7.82 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017135151964295103		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.017135151964295103 | validation: 0.01411113886154579]
	TIME [epoch: 7.79 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009908337025769378		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.009908337025769378 | validation: 0.012281057959790353]
	TIME [epoch: 7.79 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007095270574093981		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.007095270574093981 | validation: 0.0165597057881359]
	TIME [epoch: 7.78 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028880411756573517		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.028880411756573517 | validation: 0.016476934725359597]
	TIME [epoch: 7.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0103929084682217		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.0103929084682217 | validation: 0.011203576587539739]
	TIME [epoch: 7.84 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008847572725066857		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.008847572725066857 | validation: 0.009741412528638388]
	TIME [epoch: 7.79 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010065303131131404		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.010065303131131404 | validation: 0.014820621394743501]
	TIME [epoch: 7.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018591465802750447		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.018591465802750447 | validation: 0.014200048051596376]
	TIME [epoch: 7.81 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00900900044546224		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.00900900044546224 | validation: 0.010907835359614891]
	TIME [epoch: 7.76 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02965013704430298		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.02965013704430298 | validation: 0.0947400733214655]
	TIME [epoch: 7.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0720501954669108		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.0720501954669108 | validation: 0.03732385072465717]
	TIME [epoch: 7.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01678341887425053		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.01678341887425053 | validation: 0.007362184195256893]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01129929903424463		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.01129929903424463 | validation: 0.039026433704443184]
	TIME [epoch: 7.76 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016620967668325794		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.016620967668325794 | validation: 0.014930393553853566]
	TIME [epoch: 7.77 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008030782980129542		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.008030782980129542 | validation: 0.008760494422989422]
	TIME [epoch: 7.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007266703245443118		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.007266703245443118 | validation: 0.014838580599436952]
	TIME [epoch: 7.76 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0122603575077171		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.0122603575077171 | validation: 0.013661869314693913]
	TIME [epoch: 7.76 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010670075409364922		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.010670075409364922 | validation: 0.017252566425398595]
	TIME [epoch: 7.76 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012462814603478352		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.012462814603478352 | validation: 0.011527161188074909]
	TIME [epoch: 7.81 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009588362210358043		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.009588362210358043 | validation: 0.013449881081098979]
	TIME [epoch: 7.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016783691180702717		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.016783691180702717 | validation: 0.022880510917308736]
	TIME [epoch: 7.77 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011206245427775013		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.011206245427775013 | validation: 0.011240821617695095]
	TIME [epoch: 7.76 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009729631994838913		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.009729631994838913 | validation: 0.011604025489231269]
	TIME [epoch: 7.76 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00740062093159188		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.00740062093159188 | validation: 0.017311495082666405]
	TIME [epoch: 7.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016268864203895502		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.016268864203895502 | validation: 0.0074570840514435925]
	TIME [epoch: 7.77 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006265623034650114		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.006265623034650114 | validation: 0.007923773072321498]
	TIME [epoch: 7.76 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008540986331186766		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.008540986331186766 | validation: 0.01762981971899836]
	TIME [epoch: 7.76 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01566486041471001		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.01566486041471001 | validation: 0.009733193694667911]
	TIME [epoch: 7.76 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008821840727414551		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.008821840727414551 | validation: 0.012125133664924272]
	TIME [epoch: 7.81 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010507921736637657		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.010507921736637657 | validation: 0.008145897078255136]
	TIME [epoch: 7.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01856548955495802		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.01856548955495802 | validation: 0.01330738999723912]
	TIME [epoch: 7.77 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00963311131982927		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.00963311131982927 | validation: 0.017509769707138453]
	TIME [epoch: 7.81 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009884630512205318		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.009884630512205318 | validation: 0.006198430991103636]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007843612189611767		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.007843612189611767 | validation: 0.01457797370842975]
	TIME [epoch: 7.81 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013241820929053506		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.013241820929053506 | validation: 0.00969451152360217]
	TIME [epoch: 7.76 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007130170591944799		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.007130170591944799 | validation: 0.047229978041717156]
	TIME [epoch: 7.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028706686820343105		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.028706686820343105 | validation: 0.012271085291132248]
	TIME [epoch: 7.76 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008392374399001893		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.008392374399001893 | validation: 0.0067496722608906806]
	TIME [epoch: 7.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007152088935344191		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.007152088935344191 | validation: 0.009155630961256]
	TIME [epoch: 7.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007058885247865516		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.007058885247865516 | validation: 0.01691512362932368]
	TIME [epoch: 7.76 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014099128282984147		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.014099128282984147 | validation: 0.010506915215184258]
	TIME [epoch: 7.76 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007155709158340405		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.007155709158340405 | validation: 0.009825079684766539]
	TIME [epoch: 7.76 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007928274537928826		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.007928274537928826 | validation: 0.008150840869146306]
	TIME [epoch: 7.81 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014612790864199195		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.014612790864199195 | validation: 0.01687834280535724]
	TIME [epoch: 7.81 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014121332889236313		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.014121332889236313 | validation: 0.022245258655030298]
	TIME [epoch: 7.79 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010693632774749117		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.010693632774749117 | validation: 0.007042691739723255]
	TIME [epoch: 7.75 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005123008174973477		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.005123008174973477 | validation: 0.007993197815306795]
	TIME [epoch: 7.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012107753502735193		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.012107753502735193 | validation: 0.009367731589946422]
	TIME [epoch: 7.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007946706084546604		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.007946706084546604 | validation: 0.00687207937948956]
	TIME [epoch: 7.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007053337607283841		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.007053337607283841 | validation: 0.00706059823665541]
	TIME [epoch: 7.76 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011771325263250016		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.011771325263250016 | validation: 0.0167139477296833]
	TIME [epoch: 7.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00787431297694863		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.00787431297694863 | validation: 0.008046083179987772]
	TIME [epoch: 7.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006851565723005647		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.006851565723005647 | validation: 0.008917519101856898]
	TIME [epoch: 7.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009236651382901265		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.009236651382901265 | validation: 0.024054370329247704]
	TIME [epoch: 7.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013967533046534716		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.013967533046534716 | validation: 0.008612324452135608]
	TIME [epoch: 7.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00674316591526309		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.00674316591526309 | validation: 0.0062245446833012995]
	TIME [epoch: 7.81 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009356931598297803		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.009356931598297803 | validation: 0.009724327369619378]
	TIME [epoch: 7.83 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008440996094884164		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.008440996094884164 | validation: 0.00919100079582268]
	TIME [epoch: 7.8 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011274515505911624		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.011274515505911624 | validation: 0.02408770162795012]
	TIME [epoch: 7.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012716801822872392		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.012716801822872392 | validation: 0.007492215665013426]
	TIME [epoch: 7.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006196078375797571		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.006196078375797571 | validation: 0.007460594561996681]
	TIME [epoch: 7.76 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007522912636327242		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.007522912636327242 | validation: 0.010468760827618715]
	TIME [epoch: 7.81 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008214943028346234		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.008214943028346234 | validation: 0.00870087443685131]
	TIME [epoch: 7.77 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007371700501418502		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.007371700501418502 | validation: 0.006472229179632614]
	TIME [epoch: 7.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006137493578936721		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.006137493578936721 | validation: 0.01418151359345403]
	TIME [epoch: 7.76 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010853529797656672		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.010853529797656672 | validation: 0.008180778370710191]
	TIME [epoch: 7.75 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008469627146924605		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.008469627146924605 | validation: 0.0070389603311953675]
	TIME [epoch: 7.81 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006834784125667136		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.006834784125667136 | validation: 0.014552155257356985]
	TIME [epoch: 7.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010899653359764064		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.010899653359764064 | validation: 0.006911724885828753]
	TIME [epoch: 7.82 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005370623531860484		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.005370623531860484 | validation: 0.005268011729296149]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01648498281949348		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.01648498281949348 | validation: 0.057384348660506075]
	TIME [epoch: 7.78 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03689564622527544		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.03689564622527544 | validation: 0.006286341847344751]
	TIME [epoch: 7.81 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010182006875305046		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.010182006875305046 | validation: 0.006166546980675949]
	TIME [epoch: 7.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004890657788403431		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.004890657788403431 | validation: 0.004862536342062388]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005269761275862064		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.005269761275862064 | validation: 0.00802397670318541]
	TIME [epoch: 7.76 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011653846537115266		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.011653846537115266 | validation: 0.007013974267396149]
	TIME [epoch: 7.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051892226215930525		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.0051892226215930525 | validation: 0.008263093751486691]
	TIME [epoch: 7.85 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005701904566987713		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.005701904566987713 | validation: 0.014359358664001419]
	TIME [epoch: 7.78 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011934230109998443		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.011934230109998443 | validation: 0.006385479106057112]
	TIME [epoch: 7.81 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004684316370309465		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.004684316370309465 | validation: 0.006749716953107171]
	TIME [epoch: 7.77 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009422197487178947		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.009422197487178947 | validation: 0.007357751697687823]
	TIME [epoch: 7.85 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006496329783942374		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.006496329783942374 | validation: 0.0074809521530525835]
	TIME [epoch: 7.84 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007663367689282934		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.007663367689282934 | validation: 0.011825663196190231]
	TIME [epoch: 7.81 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011011910038166942		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.011011910038166942 | validation: 0.012173890006409872]
	TIME [epoch: 7.77 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006593848937359244		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.006593848937359244 | validation: 0.008950104473576637]
	TIME [epoch: 7.78 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006408776733891536		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.006408776733891536 | validation: 0.010074116024111913]
	TIME [epoch: 7.83 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009787230603354072		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.009787230603354072 | validation: 0.007107813074737247]
	TIME [epoch: 7.78 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004916258370429752		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.004916258370429752 | validation: 0.0071182670995573565]
	TIME [epoch: 7.78 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006930310357148856		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.006930310357148856 | validation: 0.01806413056243484]
	TIME [epoch: 7.77 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011678973145283758		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.011678973145283758 | validation: 0.0078755023733912]
	TIME [epoch: 7.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060073096761644965		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.0060073096761644965 | validation: 0.005426319312301297]
	TIME [epoch: 7.82 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006849599231925612		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.006849599231925612 | validation: 0.010386267948808887]
	TIME [epoch: 7.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007038688587586304		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.007038688587586304 | validation: 0.0075870607488700585]
	TIME [epoch: 7.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009182697866233496		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.009182697866233496 | validation: 0.008077507333426078]
	TIME [epoch: 7.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00548709935934597		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.00548709935934597 | validation: 0.008772453870863381]
	TIME [epoch: 7.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005741214517642224		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.005741214517642224 | validation: 0.008216537788530005]
	TIME [epoch: 7.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009608801968907229		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.009608801968907229 | validation: 0.0064016653842515715]
	TIME [epoch: 7.77 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00485278200357887		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.00485278200357887 | validation: 0.005513743195718877]
	TIME [epoch: 7.75 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00762067469205825		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.00762067469205825 | validation: 0.007523125410266861]
	TIME [epoch: 7.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005490103176994755		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.005490103176994755 | validation: 0.006640708981033654]
	TIME [epoch: 7.78 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007607167296633597		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.007607167296633597 | validation: 0.006387982751987884]
	TIME [epoch: 7.79 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005183442318095197		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.005183442318095197 | validation: 0.004349989621414885]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005429628405608689		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.005429628405608689 | validation: 0.009700547971333389]
	TIME [epoch: 7.76 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009090902388024511		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.009090902388024511 | validation: 0.006340211675939093]
	TIME [epoch: 7.76 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056538628650409095		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.0056538628650409095 | validation: 0.007710035458424492]
	TIME [epoch: 7.81 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006000385819208709		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.006000385819208709 | validation: 0.006203171658092978]
	TIME [epoch: 7.79 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005427667118888131		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.005427667118888131 | validation: 0.013469567004353056]
	TIME [epoch: 7.81 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008039626056183096		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.008039626056183096 | validation: 0.00780548304762208]
	TIME [epoch: 7.76 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006459030247284786		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.006459030247284786 | validation: 0.006113134388942487]
	TIME [epoch: 7.75 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038983419824027594		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.0038983419824027594 | validation: 0.005882799134895074]
	TIME [epoch: 7.81 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011981790112066487		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.011981790112066487 | validation: 0.016463762993435313]
	TIME [epoch: 7.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00844500625905298		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.00844500625905298 | validation: 0.017377089008348622]
	TIME [epoch: 7.75 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008403837752830151		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.008403837752830151 | validation: 0.006600159386448691]
	TIME [epoch: 7.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003206332588892799		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.003206332588892799 | validation: 0.003875366690652399]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005484991947682022		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.005484991947682022 | validation: 0.008685081224676441]
	TIME [epoch: 7.8 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009947235123171198		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.009947235123171198 | validation: 0.006095111825734542]
	TIME [epoch: 7.76 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005823930439260012		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.005823930439260012 | validation: 0.005174094367016126]
	TIME [epoch: 7.76 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004750993475520793		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.004750993475520793 | validation: 0.006629310214378024]
	TIME [epoch: 7.8 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008962160544056054		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.008962160544056054 | validation: 0.014760252489476279]
	TIME [epoch: 7.83 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00589296240999367		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.00589296240999367 | validation: 0.004695290713617446]
	TIME [epoch: 7.87 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004713071279346006		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.004713071279346006 | validation: 0.007785534948318596]
	TIME [epoch: 7.83 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006274507081083062		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.006274507081083062 | validation: 0.004536353611845977]
	TIME [epoch: 7.82 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005476332691845538		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.005476332691845538 | validation: 0.007762207407646937]
	TIME [epoch: 7.82 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007600891054617834		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.007600891054617834 | validation: 0.007276284493698793]
	TIME [epoch: 7.86 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005819107636099508		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.005819107636099508 | validation: 0.00649656921528656]
	TIME [epoch: 7.81 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006288003389950833		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.006288003389950833 | validation: 0.004093414182995911]
	TIME [epoch: 7.82 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003561704869149068		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.003561704869149068 | validation: 0.00410519391754357]
	TIME [epoch: 7.82 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004795985474476964		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.004795985474476964 | validation: 0.012430100202140835]
	TIME [epoch: 7.82 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006598384270036062		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.006598384270036062 | validation: 0.008064737440801849]
	TIME [epoch: 7.88 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007725180187239401		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.007725180187239401 | validation: 0.01168893483489884]
	TIME [epoch: 7.82 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005703533749678783		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.005703533749678783 | validation: 0.00588494907072101]
	TIME [epoch: 7.82 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037749921142675937		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.0037749921142675937 | validation: 0.008214728605827528]
	TIME [epoch: 7.82 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056198073385477065		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.0056198073385477065 | validation: 0.009149133766977853]
	TIME [epoch: 7.83 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007790710132199636		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.007790710132199636 | validation: 0.007644904213548309]
	TIME [epoch: 7.86 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005156595657728666		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.005156595657728666 | validation: 0.006354583122890993]
	TIME [epoch: 7.82 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004208011625177937		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.004208011625177937 | validation: 0.0066315709635982895]
	TIME [epoch: 7.84 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007076150546757771		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.007076150546757771 | validation: 0.005672250400856223]
	TIME [epoch: 7.79 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040730450689171705		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.0040730450689171705 | validation: 0.013557995009273575]
	TIME [epoch: 7.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006390973682802822		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.006390973682802822 | validation: 0.009481197195357068]
	TIME [epoch: 7.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006258246807267876		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.006258246807267876 | validation: 0.007346850776350969]
	TIME [epoch: 7.82 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006328343764059445		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.006328343764059445 | validation: 0.007403745877378206]
	TIME [epoch: 7.82 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005833060397858984		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.005833060397858984 | validation: 0.007790835129639933]
	TIME [epoch: 7.82 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005559522141360525		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.005559522141360525 | validation: 0.003971198938944854]
	TIME [epoch: 7.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00346644910234922		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.00346644910234922 | validation: 0.006727479884992446]
	TIME [epoch: 7.81 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006669554359796917		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.006669554359796917 | validation: 0.005571093019672689]
	TIME [epoch: 7.81 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032528769615688424		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0032528769615688424 | validation: 0.004404806672748826]
	TIME [epoch: 7.81 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004018125812196955		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.004018125812196955 | validation: 0.011061447555363778]
	TIME [epoch: 7.84 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007408729855458564		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.007408729855458564 | validation: 0.011457318332829088]
	TIME [epoch: 7.91 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062180377131245464		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0062180377131245464 | validation: 0.00614809165135553]
	TIME [epoch: 7.81 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008125779563769194		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.008125779563769194 | validation: 0.0276942964314838]
	TIME [epoch: 7.82 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011857095764842616		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.011857095764842616 | validation: 0.004361028574838197]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032486497311699286		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0032486497311699286 | validation: 0.004968386833434329]
	TIME [epoch: 7.83 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003325857019910705		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.003325857019910705 | validation: 0.006185804656571266]
	TIME [epoch: 7.86 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006322829944381725		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.006322829944381725 | validation: 0.004937919777004144]
	TIME [epoch: 7.82 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031459681723789517		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0031459681723789517 | validation: 0.0057199508317664766]
	TIME [epoch: 7.81 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029410921713618745		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.0029410921713618745 | validation: 0.004547906654102089]
	TIME [epoch: 7.78 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008313772930300905		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.008313772930300905 | validation: 0.007190586064105631]
	TIME [epoch: 7.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004471747082190264		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.004471747082190264 | validation: 0.0033899891736338308]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029800183727763155		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.0029800183727763155 | validation: 0.004388480709920281]
	TIME [epoch: 7.82 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006103111659320069		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.006103111659320069 | validation: 0.00740271699873403]
	TIME [epoch: 7.86 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067280185898519074		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.0067280185898519074 | validation: 0.008664485886066935]
	TIME [epoch: 7.82 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004298036663842931		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.004298036663842931 | validation: 0.005175925721139527]
	TIME [epoch: 7.87 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004318867121376442		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.004318867121376442 | validation: 0.006565398948574464]
	TIME [epoch: 7.83 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003739424530733332		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.003739424530733332 | validation: 0.005104213676903697]
	TIME [epoch: 7.82 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003209042795800647		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.003209042795800647 | validation: 0.006402318248311591]
	TIME [epoch: 7.83 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007740817826351224		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.007740817826351224 | validation: 0.004065932013123499]
	TIME [epoch: 7.82 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036035329128949597		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.0036035329128949597 | validation: 0.004609615274948453]
	TIME [epoch: 7.88 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040955641032392655		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.0040955641032392655 | validation: 0.008698976114045971]
	TIME [epoch: 7.83 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004823315397753474		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.004823315397753474 | validation: 0.0028123088255995786]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002236104109757466		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.002236104109757466 | validation: 0.004733718957157309]
	TIME [epoch: 7.83 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007272664573843657		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.007272664573843657 | validation: 0.0051841453219420245]
	TIME [epoch: 7.86 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004199390123516691		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.004199390123516691 | validation: 0.0058016651449073475]
	TIME [epoch: 7.86 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003964063990248054		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.003964063990248054 | validation: 0.0037020400556585214]
	TIME [epoch: 7.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005197581578783096		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.005197581578783096 | validation: 0.004537410659406996]
	TIME [epoch: 7.85 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037635872644593065		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.0037635872644593065 | validation: 0.004614704453065029]
	TIME [epoch: 7.87 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003831817286965209		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.003831817286965209 | validation: 0.005632068974610126]
	TIME [epoch: 7.87 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004445223790134397		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.004445223790134397 | validation: 0.010221498776539533]
	TIME [epoch: 7.84 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007159673279627159		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.007159673279627159 | validation: 0.004956396256842248]
	TIME [epoch: 7.82 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026981581855489824		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0026981581855489824 | validation: 0.00441971810068518]
	TIME [epoch: 7.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002621849029967671		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.002621849029967671 | validation: 0.004623652556371156]
	TIME [epoch: 7.83 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002993363731818582		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.002993363731818582 | validation: 0.01531592147529555]
	TIME [epoch: 7.86 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00679886681699653		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.00679886681699653 | validation: 0.004869127576084413]
	TIME [epoch: 7.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044021818486429005		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0044021818486429005 | validation: 0.013855780816676055]
	TIME [epoch: 7.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007032786897558705		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.007032786897558705 | validation: 0.006492628398526901]
	TIME [epoch: 7.83 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004610031153587196		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.004610031153587196 | validation: 0.004323221537511547]
	TIME [epoch: 7.83 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003334450869048369		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.003334450869048369 | validation: 0.003140001656019753]
	TIME [epoch: 7.89 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002503710084539764		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.002503710084539764 | validation: 0.005143114157403059]
	TIME [epoch: 7.88 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004879509651387132		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.004879509651387132 | validation: 0.003924336580855115]
	TIME [epoch: 7.85 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004361465875796909		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.004361465875796909 | validation: 0.00305196009277301]
	TIME [epoch: 7.86 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032142166877009437		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.0032142166877009437 | validation: 0.005413450693592479]
	TIME [epoch: 7.87 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004818530140545501		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.004818530140545501 | validation: 0.006040933855168188]
	TIME [epoch: 7.88 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004169116523498789		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.004169116523498789 | validation: 0.002561126698902256]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029407290372768934		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0029407290372768934 | validation: 0.002722149012792779]
	TIME [epoch: 7.86 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035781815856769056		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.0035781815856769056 | validation: 0.0031231988304520194]
	TIME [epoch: 7.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004270576351298615		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.004270576351298615 | validation: 0.005854087787939203]
	TIME [epoch: 7.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003901659687395492		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.003901659687395492 | validation: 0.00411552638722228]
	TIME [epoch: 7.83 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003859496769402475		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.003859496769402475 | validation: 0.003638390766488942]
	TIME [epoch: 7.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033663194154792096		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.0033663194154792096 | validation: 0.004031117314364214]
	TIME [epoch: 7.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004157201824649448		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.004157201824649448 | validation: 0.00531334560124668]
	TIME [epoch: 7.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005267887775506854		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.005267887775506854 | validation: 0.0032565692844098945]
	TIME [epoch: 7.92 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027216945348620004		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.0027216945348620004 | validation: 0.003554085283439347]
	TIME [epoch: 7.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034910456776146336		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.0034910456776146336 | validation: 0.0048118607789156154]
	TIME [epoch: 7.86 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003122004379108836		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.003122004379108836 | validation: 0.0031506927483730494]
	TIME [epoch: 7.86 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002423350809419279		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.002423350809419279 | validation: 0.0043669128724904555]
	TIME [epoch: 7.86 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006379900026494674		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.006379900026494674 | validation: 0.0033937205085904196]
	TIME [epoch: 7.89 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002815868482859713		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.002815868482859713 | validation: 0.003166035968445516]
	TIME [epoch: 7.85 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023517892909419866		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0023517892909419866 | validation: 0.00547025505498092]
	TIME [epoch: 7.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004173225001604251		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.004173225001604251 | validation: 0.004392400666505154]
	TIME [epoch: 7.85 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031260862917518954		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.0031260862917518954 | validation: 0.0027895548275597764]
	TIME [epoch: 7.89 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003353002748662447		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.003353002748662447 | validation: 0.004943456479337977]
	TIME [epoch: 7.85 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003121872436110059		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.003121872436110059 | validation: 0.002396327432239123]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003634209668206518		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.003634209668206518 | validation: 0.007830371510313737]
	TIME [epoch: 7.89 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004920811430239934		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.004920811430239934 | validation: 0.003693366631271399]
	TIME [epoch: 7.85 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027942276805160877		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.0027942276805160877 | validation: 0.0036435743151300055]
	TIME [epoch: 7.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026585955064869337		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.0026585955064869337 | validation: 0.005173218346466449]
	TIME [epoch: 7.84 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003866345428961536		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.003866345428961536 | validation: 0.005656757967805976]
	TIME [epoch: 7.84 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004185099333739058		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.004185099333739058 | validation: 0.0031223738925751816]
	TIME [epoch: 7.84 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021878751785199156		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.0021878751785199156 | validation: 0.0031009292764405797]
	TIME [epoch: 7.85 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031751244315461158		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0031751244315461158 | validation: 0.0047546342531190385]
	TIME [epoch: 7.88 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005885126931178463		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.005885126931178463 | validation: 0.002409301712739222]
	TIME [epoch: 7.84 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00269053592640856		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.00269053592640856 | validation: 0.0024805913777731546]
	TIME [epoch: 7.85 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033481429326060355		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0033481429326060355 | validation: 0.0034391363620747215]
	TIME [epoch: 7.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003138566882517335		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.003138566882517335 | validation: 0.004338690606608994]
	TIME [epoch: 7.86 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029494772423498882		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0029494772423498882 | validation: 0.004082263423392577]
	TIME [epoch: 7.88 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025542927069944336		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.0025542927069944336 | validation: 0.0035071935288457914]
	TIME [epoch: 7.85 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030465430922836205		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.0030465430922836205 | validation: 0.0024344413361909274]
	TIME [epoch: 7.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002272416402289512		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.002272416402289512 | validation: 0.0055575858366281265]
	TIME [epoch: 7.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009379906988422712		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.009379906988422712 | validation: 0.00783329771516446]
	TIME [epoch: 7.95 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003468815274138351		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.003468815274138351 | validation: 0.004433905624011122]
	TIME [epoch: 7.86 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025606494708404533		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.0025606494708404533 | validation: 0.003323846411706933]
	TIME [epoch: 7.84 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004537331587070549		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.004537331587070549 | validation: 0.0037335529043343377]
	TIME [epoch: 7.84 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028843995074502127		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.0028843995074502127 | validation: 0.0027569422749609005]
	TIME [epoch: 7.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002670135622670388		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.002670135622670388 | validation: 0.005393112692007001]
	TIME [epoch: 7.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026982689047348137		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.0026982689047348137 | validation: 0.0033308904995187636]
	TIME [epoch: 7.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033636906691606886		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.0033636906691606886 | validation: 0.003168135475750335]
	TIME [epoch: 7.84 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002046844418917613		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.002046844418917613 | validation: 0.0035461069106397453]
	TIME [epoch: 7.85 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027902554548509388		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.0027902554548509388 | validation: 0.004279747844353784]
	TIME [epoch: 7.86 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043678182996321315		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.0043678182996321315 | validation: 0.0028950412471231667]
	TIME [epoch: 7.88 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022882895847710404		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.0022882895847710404 | validation: 0.0040795195854366135]
	TIME [epoch: 7.85 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027637055001494826		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.0027637055001494826 | validation: 0.00751714886753231]
	TIME [epoch: 7.88 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003753657233388891		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.003753657233388891 | validation: 0.0035254757469876906]
	TIME [epoch: 7.87 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00387152622072411		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.00387152622072411 | validation: 0.004242101300631251]
	TIME [epoch: 7.87 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024004775107429425		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.0024004775107429425 | validation: 0.002493817855480857]
	TIME [epoch: 7.86 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002771614408880692		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.002771614408880692 | validation: 0.0035818829019705137]
	TIME [epoch: 7.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002981863794617747		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.002981863794617747 | validation: 0.002386813572004111]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022985956974774847		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.0022985956974774847 | validation: 0.006404945027260415]
	TIME [epoch: 7.85 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037365555744342744		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.0037365555744342744 | validation: 0.00252548994512448]
	TIME [epoch: 7.89 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020245548660865692		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.0020245548660865692 | validation: 0.0048019444780889285]
	TIME [epoch: 7.83 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021688508020323615		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.0021688508020323615 | validation: 0.004368621243134535]
	TIME [epoch: 7.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003516212599517572		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.003516212599517572 | validation: 0.001898675656546973]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002448464232076449		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.002448464232076449 | validation: 0.005253355678018292]
	TIME [epoch: 7.86 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004368375852073835		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.004368375852073835 | validation: 0.002553139950227586]
	TIME [epoch: 7.92 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027796460010884056		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.0027796460010884056 | validation: 0.0021688208176512216]
	TIME [epoch: 7.89 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00274447380831674		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.00274447380831674 | validation: 0.0032010988864882065]
	TIME [epoch: 7.84 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002191244855151107		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.002191244855151107 | validation: 0.007974981090210376]
	TIME [epoch: 7.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004207897940901005		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.004207897940901005 | validation: 0.002113439205721562]
	TIME [epoch: 7.89 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027361527068776284		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0027361527068776284 | validation: 0.004577641815104742]
	TIME [epoch: 7.87 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005311445988111173		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.005311445988111173 | validation: 0.0044653335985022444]
	TIME [epoch: 7.85 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028090608879973453		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0028090608879973453 | validation: 0.0030864502836432895]
	TIME [epoch: 7.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027448077015259807		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.0027448077015259807 | validation: 0.0038889203042529203]
	TIME [epoch: 7.85 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002935606375944066		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.002935606375944066 | validation: 0.0021969167947466775]
	TIME [epoch: 7.91 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018496914348871277		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0018496914348871277 | validation: 0.003337422414631079]
	TIME [epoch: 7.85 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030076144054858546		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0030076144054858546 | validation: 0.0032467048991286305]
	TIME [epoch: 7.86 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025640565676855486		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0025640565676855486 | validation: 0.008645910259452907]
	TIME [epoch: 7.86 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005720397608247954		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.005720397608247954 | validation: 0.0021615090383204833]
	TIME [epoch: 7.89 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001803927330881487		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.001803927330881487 | validation: 0.004031953713359956]
	TIME [epoch: 7.96 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029195927327090804		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.0029195927327090804 | validation: 0.002544907454045734]
	TIME [epoch: 7.86 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019742865635342345		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.0019742865635342345 | validation: 0.0037858803542199114]
	TIME [epoch: 7.86 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002613512277326295		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.002613512277326295 | validation: 0.004626883727716908]
	TIME [epoch: 7.86 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002720178972674709		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.002720178972674709 | validation: 0.003200971721889321]
	TIME [epoch: 7.86 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024978671395605168		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.0024978671395605168 | validation: 0.001694054996741789]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002355593899904043		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.002355593899904043 | validation: 0.003924770242458886]
	TIME [epoch: 7.85 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002630634667867831		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.002630634667867831 | validation: 0.0029689027820404912]
	TIME [epoch: 7.85 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026526110607857827		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.0026526110607857827 | validation: 0.002394811613304128]
	TIME [epoch: 7.84 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018293241420126404		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0018293241420126404 | validation: 0.004508206514889316]
	TIME [epoch: 7.89 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002670725228912838		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.002670725228912838 | validation: 0.002777131848953648]
	TIME [epoch: 7.85 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002128233440676594		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.002128233440676594 | validation: 0.00292102112160884]
	TIME [epoch: 7.85 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003433906523353636		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.003433906523353636 | validation: 0.010400135910166478]
	TIME [epoch: 7.89 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002457055951367297		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.002457055951367297 | validation: 0.004660787349566686]
	TIME [epoch: 7.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002291549736687222		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.002291549736687222 | validation: 0.0028424964244907563]
	TIME [epoch: 7.88 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021880244439515754		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0021880244439515754 | validation: 0.002419183093406031]
	TIME [epoch: 7.86 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021664967710222534		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0021664967710222534 | validation: 0.0020241906171387684]
	TIME [epoch: 7.85 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024619174842800814		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0024619174842800814 | validation: 0.003338077933657411]
	TIME [epoch: 7.85 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022949120981171998		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.0022949120981171998 | validation: 0.0019985761302126195]
	TIME [epoch: 7.87 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015673310571892292		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.0015673310571892292 | validation: 0.0025791702498480415]
	TIME [epoch: 7.88 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022267941300438622		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0022267941300438622 | validation: 0.0022978836881905485]
	TIME [epoch: 7.86 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026476239120501667		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.0026476239120501667 | validation: 0.003474272115112427]
	TIME [epoch: 7.87 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002345097489352451		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.002345097489352451 | validation: 0.0032776500171143876]
	TIME [epoch: 7.86 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019494865329817436		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.0019494865329817436 | validation: 0.0028399528176818204]
	TIME [epoch: 7.88 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026406247645699536		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.0026406247645699536 | validation: 0.0032194537234348213]
	TIME [epoch: 7.88 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020573500355626547		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.0020573500355626547 | validation: 0.002184048582554252]
	TIME [epoch: 7.88 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025408446013627283		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.0025408446013627283 | validation: 0.0045041017272622885]
	TIME [epoch: 7.91 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038582372146844265		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.0038582372146844265 | validation: 0.003578254598275515]
	TIME [epoch: 7.84 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018928151840619505		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.0018928151840619505 | validation: 0.0019055827506304136]
	TIME [epoch: 7.9 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016198328043797435		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.0016198328043797435 | validation: 0.006890950303970847]
	TIME [epoch: 7.86 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028055975861122602		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0028055975861122602 | validation: 0.001332773802046652]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033672118133415033		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.0033672118133415033 | validation: 0.004196307834446184]
	TIME [epoch: 7.85 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002501772181998005		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.002501772181998005 | validation: 0.002495110059454331]
	TIME [epoch: 7.87 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017666203382058852		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.0017666203382058852 | validation: 0.0022936523067172074]
	TIME [epoch: 7.87 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018566163959157288		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0018566163959157288 | validation: 0.0023178005984399357]
	TIME [epoch: 7.86 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024758988349970227		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0024758988349970227 | validation: 0.0036862946032933577]
	TIME [epoch: 7.84 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002154932288216936		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.002154932288216936 | validation: 0.0021623079923580174]
	TIME [epoch: 7.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019694898125246843		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0019694898125246843 | validation: 0.002439660587080182]
	TIME [epoch: 7.87 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018924422287032467		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0018924422287032467 | validation: 0.005001437749084766]
	TIME [epoch: 7.88 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002734129796031734		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.002734129796031734 | validation: 0.002573121593749706]
	TIME [epoch: 7.86 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018158755405577015		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0018158755405577015 | validation: 0.0023280577062892178]
	TIME [epoch: 7.85 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012993722029598764		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0012993722029598764 | validation: 0.0020052795803727186]
	TIME [epoch: 7.86 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020820705594283085		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0020820705594283085 | validation: 0.001830716146216985]
	TIME [epoch: 7.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022251532825784993		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0022251532825784993 | validation: 0.002914886758550356]
	TIME [epoch: 7.87 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004167217075834293		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.004167217075834293 | validation: 0.00455684178777385]
	TIME [epoch: 7.85 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030069241940264254		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.0030069241940264254 | validation: 0.002471467982113901]
	TIME [epoch: 7.86 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014809673875526466		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.0014809673875526466 | validation: 0.0030097215615915093]
	TIME [epoch: 7.91 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002178614501989358		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.002178614501989358 | validation: 0.002888399310048633]
	TIME [epoch: 7.92 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017236508415586114		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.0017236508415586114 | validation: 0.0026239713422389803]
	TIME [epoch: 7.86 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001365763215073799		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.001365763215073799 | validation: 0.0020667752816925225]
	TIME [epoch: 7.85 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002049986462213607		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.002049986462213607 | validation: 0.0021217073072482126]
	TIME [epoch: 7.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027152436960776183		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.0027152436960776183 | validation: 0.0025639953333863336]
	TIME [epoch: 7.86 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022878011722281644		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.0022878011722281644 | validation: 0.002041507706087149]
	TIME [epoch: 7.88 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016291852828667357		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.0016291852828667357 | validation: 0.001817315251366356]
	TIME [epoch: 7.85 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017493962781095348		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.0017493962781095348 | validation: 0.0025309092908979485]
	TIME [epoch: 7.85 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018533374080754936		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0018533374080754936 | validation: 0.0016086730192578723]
	TIME [epoch: 7.85 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022843698711841765		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0022843698711841765 | validation: 0.002708569267432668]
	TIME [epoch: 7.87 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029193278101116056		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0029193278101116056 | validation: 0.004590471969260126]
	TIME [epoch: 7.87 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021078099465208413		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.0021078099465208413 | validation: 0.0024273223429086624]
	TIME [epoch: 7.86 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002210503431227719		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.002210503431227719 | validation: 0.0028399654969586694]
	TIME [epoch: 7.85 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015014790902826195		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0015014790902826195 | validation: 0.0017726972122494723]
	TIME [epoch: 7.85 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014970420252802927		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0014970420252802927 | validation: 0.0009555200116583312]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014165280815763423		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0014165280815763423 | validation: 0.003817306014240608]
	TIME [epoch: 7.91 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003175540399663163		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.003175540399663163 | validation: 0.0029384186714759117]
	TIME [epoch: 7.84 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011837345356782215		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.0011837345356782215 | validation: 0.0034321491918401694]
	TIME [epoch: 7.84 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016808274496992997		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.0016808274496992997 | validation: 0.0015002047485907807]
	TIME [epoch: 7.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002315049498506941		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.002315049498506941 | validation: 0.0014262022331257888]
	TIME [epoch: 7.87 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001426615153261204		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.001426615153261204 | validation: 0.001847606711568403]
	TIME [epoch: 7.82 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020746556966837835		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0020746556966837835 | validation: 0.002124571427606829]
	TIME [epoch: 7.82 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024489575430920916		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.0024489575430920916 | validation: 0.0024950566907812645]
	TIME [epoch: 7.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00174962826330507		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.00174962826330507 | validation: 0.0016788466100894589]
	TIME [epoch: 7.86 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026247614544211363		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0026247614544211363 | validation: 0.009231609249468033]
	TIME [epoch: 7.9 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004910075760936983		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.004910075760936983 | validation: 0.0032978649041826716]
	TIME [epoch: 7.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003086200319359955		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.003086200319359955 | validation: 0.0020755137407395675]
	TIME [epoch: 7.84 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001381923408116492		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.001381923408116492 | validation: 0.005332252769839047]
	TIME [epoch: 7.87 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024534525521117518		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0024534525521117518 | validation: 0.0017341297323203238]
	TIME [epoch: 7.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013534885602389463		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.0013534885602389463 | validation: 0.0023819265401724206]
	TIME [epoch: 7.83 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001414927726700857		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.001414927726700857 | validation: 0.00221385717826735]
	TIME [epoch: 7.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014005761391084454		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0014005761391084454 | validation: 0.003753153297167609]
	TIME [epoch: 7.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026437003127995524		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0026437003127995524 | validation: 0.003557536146642085]
	TIME [epoch: 7.83 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00152702723795015		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.00152702723795015 | validation: 0.0017147151654209791]
	TIME [epoch: 7.87 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012595972769122443		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.0012595972769122443 | validation: 0.0022196474980725658]
	TIME [epoch: 7.85 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018676181322678084		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.0018676181322678084 | validation: 0.0018918683245577154]
	TIME [epoch: 7.82 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015415440317165725		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.0015415440317165725 | validation: 0.0021259525496989022]
	TIME [epoch: 7.82 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015305677525340893		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0015305677525340893 | validation: 0.0030438980234621742]
	TIME [epoch: 7.84 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001753211375775631		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.001753211375775631 | validation: 0.001361434577719103]
	TIME [epoch: 7.86 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015999766548483445		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.0015999766548483445 | validation: 0.0013297815756087665]
	TIME [epoch: 7.85 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013124213764302386		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0013124213764302386 | validation: 0.001713834151251346]
	TIME [epoch: 7.86 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018055348826995408		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0018055348826995408 | validation: 0.002206295861221276]
	TIME [epoch: 7.82 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001883755469237486		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.001883755469237486 | validation: 0.0018951662351335595]
	TIME [epoch: 7.84 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012754174558596		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0012754174558596 | validation: 0.00287997786689241]
	TIME [epoch: 7.85 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020776669467151474		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.0020776669467151474 | validation: 0.0014586338432131338]
	TIME [epoch: 7.82 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011101561789388342		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0011101561789388342 | validation: 0.0010953582850735727]
	TIME [epoch: 7.81 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002089677953477028		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.002089677953477028 | validation: 0.009103797035466314]
	TIME [epoch: 7.83 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017152139346147208		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.0017152139346147208 | validation: 0.0010959076922789812]
	TIME [epoch: 7.88 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012100034136134133		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.0012100034136134133 | validation: 0.002249431052020153]
	TIME [epoch: 7.85 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013849873516387325		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.0013849873516387325 | validation: 0.0008847048319367277]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015788819641558442		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.0015788819641558442 | validation: 0.0663943172409191]
	TIME [epoch: 7.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023595204227339606		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0023595204227339606 | validation: 0.0017058217430073485]
	TIME [epoch: 7.82 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010075434547637749		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.0010075434547637749 | validation: 0.001265784523762611]
	TIME [epoch: 7.86 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00112849986626064		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.00112849986626064 | validation: 0.003866684845846212]
	TIME [epoch: 7.82 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003091557095936608		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.003091557095936608 | validation: 0.0024877946438305143]
	TIME [epoch: 7.81 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016304178854326962		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.0016304178854326962 | validation: 0.0024937947217632895]
	TIME [epoch: 7.82 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001123540941920664		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.001123540941920664 | validation: 0.001084854838298277]
	TIME [epoch: 7.86 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001187666149864727		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.001187666149864727 | validation: 0.0012207694839966106]
	TIME [epoch: 7.89 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020098482362207477		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.0020098482362207477 | validation: 0.002402945119772272]
	TIME [epoch: 7.8 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014796080457992388		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0014796080457992388 | validation: 0.0010266013110931586]
	TIME [epoch: 7.82 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00103579499295196		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.00103579499295196 | validation: 0.0016332328775980729]
	TIME [epoch: 7.81 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001200619544519031		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.001200619544519031 | validation: 0.0029821588980681168]
	TIME [epoch: 7.87 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016356213555475362		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.0016356213555475362 | validation: 0.003274582986881676]
	TIME [epoch: 7.81 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018642458216183136		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.0018642458216183136 | validation: 0.0014984571095098432]
	TIME [epoch: 7.78 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015894066001100851		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0015894066001100851 | validation: 0.0016783001755710024]
	TIME [epoch: 7.76 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012146235739403533		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.0012146235739403533 | validation: 0.0020927010672961164]
	TIME [epoch: 7.79 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001394707819670162		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.001394707819670162 | validation: 0.0012857585666926816]
	TIME [epoch: 7.85 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011610959128967781		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0011610959128967781 | validation: 0.0015289002117687103]
	TIME [epoch: 7.82 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012032625290765292		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0012032625290765292 | validation: 0.0009036923551249787]
	TIME [epoch: 7.83 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000968678582788045		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.000968678582788045 | validation: 0.002338731611750377]
	TIME [epoch: 7.87 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002176115830887749		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.002176115830887749 | validation: 0.0019542546087916394]
	TIME [epoch: 7.85 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008513440128245917		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.0008513440128245917 | validation: 0.0010168461521152406]
	TIME [epoch: 7.86 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001304714617054955		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.001304714617054955 | validation: 0.0011428038297746773]
	TIME [epoch: 7.83 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018659291505829794		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.0018659291505829794 | validation: 0.0024169047477610034]
	TIME [epoch: 7.82 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00103138225343539		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.00103138225343539 | validation: 0.0008260950999549035]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013015634675908132		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.0013015634675908132 | validation: 0.0012981756382523385]
	TIME [epoch: 7.85 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014385135881218934		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.0014385135881218934 | validation: 0.0011504800204034061]
	TIME [epoch: 7.86 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012712488137953154		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0012712488137953154 | validation: 0.0053624669899319105]
	TIME [epoch: 7.83 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008518798566643109		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0008518798566643109 | validation: 0.0013444834961561678]
	TIME [epoch: 7.82 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001763380178553656		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.001763380178553656 | validation: 0.0013860868008458553]
	TIME [epoch: 7.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010935290598911977		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0010935290598911977 | validation: 0.0013533756623861893]
	TIME [epoch: 7.87 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015699498927563113		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.0015699498927563113 | validation: 0.0018732948526500853]
	TIME [epoch: 7.85 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010012619827302447		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0010012619827302447 | validation: 0.0013034560297816153]
	TIME [epoch: 7.89 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007578154522578215		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0007578154522578215 | validation: 0.001504654022854613]
	TIME [epoch: 7.84 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00148656315681134		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.00148656315681134 | validation: 0.0012445373061143066]
	TIME [epoch: 7.82 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012811515201284623		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.0012811515201284623 | validation: 0.0014514610392063112]
	TIME [epoch: 7.88 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008654610979265951		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0008654610979265951 | validation: 0.0008989961039797576]
	TIME [epoch: 7.84 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011819142857958202		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0011819142857958202 | validation: 0.0022865173833438498]
	TIME [epoch: 7.81 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002211051359537457		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.002211051359537457 | validation: 0.0009452014128036579]
	TIME [epoch: 7.81 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010866912552576974		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0010866912552576974 | validation: 0.0005558861349020665]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008523618460298905		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0008523618460298905 | validation: 0.033474480729849515]
	TIME [epoch: 7.88 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011243960667678585		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0011243960667678585 | validation: 0.0018642843196731312]
	TIME [epoch: 7.83 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001442359083025927		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.001442359083025927 | validation: 0.0013640064595632265]
	TIME [epoch: 7.82 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011433895316254577		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0011433895316254577 | validation: 0.0008180607799603594]
	TIME [epoch: 7.82 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010693848199918477		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0010693848199918477 | validation: 0.0011580155379525721]
	TIME [epoch: 7.88 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009023290288243488		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0009023290288243488 | validation: 0.0005996679290499109]
	TIME [epoch: 7.91 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011534930893727902		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0011534930893727902 | validation: 0.004308845074148193]
	TIME [epoch: 7.82 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002034400710827591		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.002034400710827591 | validation: 0.0018845504804321706]
	TIME [epoch: 7.82 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010401310666832563		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0010401310666832563 | validation: 0.002141646386899132]
	TIME [epoch: 7.83 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000892264912601711		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.000892264912601711 | validation: 0.0017488310527228128]
	TIME [epoch: 7.87 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001504543991590135		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.001504543991590135 | validation: 0.002146887329494505]
	TIME [epoch: 7.83 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011816514134259245		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0011816514134259245 | validation: 0.001170339959694534]
	TIME [epoch: 7.83 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007588929146158146		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.0007588929146158146 | validation: 0.0009604863870459521]
	TIME [epoch: 7.83 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011447646711420206		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.0011447646711420206 | validation: 0.0013208447931136896]
	TIME [epoch: 7.83 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012444224846497882		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.0012444224846497882 | validation: 0.0016799125627401628]
	TIME [epoch: 7.88 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007955745871168836		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0007955745871168836 | validation: 0.0011615831669515534]
	TIME [epoch: 7.83 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001538029743965035		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.001538029743965035 | validation: 0.0013045192179039172]
	TIME [epoch: 7.83 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011289469929542622		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0011289469929542622 | validation: 0.0007790177936592926]
	TIME [epoch: 7.81 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012156435008831546		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0012156435008831546 | validation: 0.0010207478900224754]
	TIME [epoch: 7.88 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009716030231225444		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0009716030231225444 | validation: 0.0014325617561203145]
	TIME [epoch: 7.92 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012174319458036038		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0012174319458036038 | validation: 0.0011025518857047177]
	TIME [epoch: 7.82 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001015699329931077		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.001015699329931077 | validation: 0.0010569486159449654]
	TIME [epoch: 7.84 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011087927455563206		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0011087927455563206 | validation: 0.002497470152091765]
	TIME [epoch: 7.83 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010375546097116423		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.0010375546097116423 | validation: 0.0015873148361198722]
	TIME [epoch: 7.86 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007717309471555619		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0007717309471555619 | validation: 0.0011592517461572465]
	TIME [epoch: 7.85 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000791161388720666		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.000791161388720666 | validation: 0.00427425705419118]
	TIME [epoch: 7.83 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002843425463713861		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.002843425463713861 | validation: 0.0035470850726095327]
	TIME [epoch: 7.83 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013449229331285617		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.0013449229331285617 | validation: 0.0026323234963669374]
	TIME [epoch: 7.83 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010906127450011369		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0010906127450011369 | validation: 0.0015087500365891105]
	TIME [epoch: 7.87 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008864792114288158		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.0008864792114288158 | validation: 0.0016377460747190372]
	TIME [epoch: 7.83 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010106323621526757		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0010106323621526757 | validation: 0.0015458464697883276]
	TIME [epoch: 7.86 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020522250285295424		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.0020522250285295424 | validation: 0.0028842963440836407]
	TIME [epoch: 7.88 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002116821526388188		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.002116821526388188 | validation: 0.0013389342939506975]
	TIME [epoch: 7.83 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010511316077584847		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0010511316077584847 | validation: 0.0010468030414827539]
	TIME [epoch: 7.87 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007019137272101029		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0007019137272101029 | validation: 0.0011709771609966948]
	TIME [epoch: 7.83 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008732587889048831		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0008732587889048831 | validation: 0.0016186142697755665]
	TIME [epoch: 7.83 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011979249805300605		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.0011979249805300605 | validation: 0.000732642424827195]
	TIME [epoch: 7.83 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009077029901837097		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0009077029901837097 | validation: 0.0005008049819548717]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001221994315650417		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.001221994315650417 | validation: 0.001436503753727493]
	TIME [epoch: 7.87 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001044722407392392		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.001044722407392392 | validation: 0.001865621012424738]
	TIME [epoch: 7.83 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019209511062819634		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.0019209511062819634 | validation: 0.0016805160236685107]
	TIME [epoch: 7.82 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001313966057179189		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.001313966057179189 | validation: 0.001454850003112445]
	TIME [epoch: 7.84 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010401808122642798		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.0010401808122642798 | validation: 0.001653214246170183]
	TIME [epoch: 7.87 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008500577611816299		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.0008500577611816299 | validation: 0.0010661983554295827]
	TIME [epoch: 7.82 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000817971567039528		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.000817971567039528 | validation: 0.0015757147047261268]
	TIME [epoch: 7.81 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009298887209652395		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0009298887209652395 | validation: 0.0010461439136174144]
	TIME [epoch: 7.86 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011528418180330791		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0011528418180330791 | validation: 0.0009347854562657645]
	TIME [epoch: 7.88 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010944110650019075		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0010944110650019075 | validation: 0.0008973088048567321]
	TIME [epoch: 7.89 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010432964502284474		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0010432964502284474 | validation: 0.0009669532195153581]
	TIME [epoch: 7.83 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008061881145161754		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0008061881145161754 | validation: 0.0026054913279487542]
	TIME [epoch: 7.83 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000987429214674774		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.000987429214674774 | validation: 0.0011933942576276939]
	TIME [epoch: 7.84 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014699825203440783		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0014699825203440783 | validation: 0.0005912052469479496]
	TIME [epoch: 7.85 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008855711559254145		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.0008855711559254145 | validation: 0.0008884769209216907]
	TIME [epoch: 7.87 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007454874110480181		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0007454874110480181 | validation: 0.0014926795369133955]
	TIME [epoch: 7.84 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011206882523166462		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.0011206882523166462 | validation: 0.0007782630902030362]
	TIME [epoch: 7.83 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009820404187920763		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0009820404187920763 | validation: 0.0007749686539945237]
	TIME [epoch: 7.83 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009134927427644677		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0009134927427644677 | validation: 0.0011869863259510315]
	TIME [epoch: 7.87 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010742725546961005		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0010742725546961005 | validation: 0.0017448059655964868]
	TIME [epoch: 7.88 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012118235710435626		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0012118235710435626 | validation: 0.0024303230916246157]
	TIME [epoch: 7.89 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001283493337404977		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.001283493337404977 | validation: 0.0016371821734894017]
	TIME [epoch: 7.84 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010732223079010094		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0010732223079010094 | validation: 0.0012033068667790614]
	TIME [epoch: 7.81 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010369787309335008		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0010369787309335008 | validation: 0.0013975345429262838]
	TIME [epoch: 7.87 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008084667453851629		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0008084667453851629 | validation: 0.0013276993928853526]
	TIME [epoch: 7.83 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009668981344676416		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0009668981344676416 | validation: 0.0007345208045677208]
	TIME [epoch: 7.83 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007763590247979999		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0007763590247979999 | validation: 0.0017001054474086266]
	TIME [epoch: 7.84 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010308191698651433		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.0010308191698651433 | validation: 0.001526919554427355]
	TIME [epoch: 7.83 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008204408992173101		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0008204408992173101 | validation: 0.0012946023231631117]
	TIME [epoch: 7.89 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010817980079907692		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0010817980079907692 | validation: 0.0009838284773317917]
	TIME [epoch: 7.83 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000745658022843744		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.000745658022843744 | validation: 0.001353774369064766]
	TIME [epoch: 7.83 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009026925260794017		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0009026925260794017 | validation: 0.0002444674710627286]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005913385987232995		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0005913385987232995 | validation: 0.0006698054105845701]
	TIME [epoch: 7.88 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007122250786623725		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0007122250786623725 | validation: 0.0016844902371169841]
	TIME [epoch: 7.91 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007706679132020944		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0007706679132020944 | validation: 0.0013708442637718151]
	TIME [epoch: 7.83 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013032057679031811		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.0013032057679031811 | validation: 0.0012764143202595858]
	TIME [epoch: 7.83 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012207794494463224		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.0012207794494463224 | validation: 0.0008074916899801039]
	TIME [epoch: 7.83 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004539186302363898		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0004539186302363898 | validation: 0.0013580510215364452]
	TIME [epoch: 7.87 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001282218386615538		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.001282218386615538 | validation: 0.000922664838697851]
	TIME [epoch: 7.84 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005105415421982336		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0005105415421982336 | validation: 0.012345624070080182]
	TIME [epoch: 7.83 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010383196025991563		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0010383196025991563 | validation: 0.0009611674739935508]
	TIME [epoch: 7.83 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000881842045730235		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.000881842045730235 | validation: 0.0007360765975619145]
	TIME [epoch: 7.84 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000763121048127462		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.000763121048127462 | validation: 0.0019192516908914897]
	TIME [epoch: 7.88 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011888332256589315		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0011888332256589315 | validation: 0.0014366088703933392]
	TIME [epoch: 7.84 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010943199615997016		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0010943199615997016 | validation: 0.0006829890725519752]
	TIME [epoch: 7.86 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012656845765398473		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0012656845765398473 | validation: 0.0020866157379550703]
	TIME [epoch: 7.89 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011921865808971152		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0011921865808971152 | validation: 0.0018271163477683735]
	TIME [epoch: 7.87 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012299336920385106		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0012299336920385106 | validation: 0.0023331580098217516]
	TIME [epoch: 7.88 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010072987197560282		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0010072987197560282 | validation: 0.0015139704722880137]
	TIME [epoch: 7.84 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001254927622214139		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.001254927622214139 | validation: 0.0009706641440288664]
	TIME [epoch: 7.84 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006074923006806534		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0006074923006806534 | validation: 0.001238149579378625]
	TIME [epoch: 7.82 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000843283982137391		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.000843283982137391 | validation: 0.001088054016947087]
	TIME [epoch: 7.83 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001052332325269762		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.001052332325269762 | validation: 0.001820690686295319]
	TIME [epoch: 7.87 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008911734557409927		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0008911734557409927 | validation: 0.0015057695368501881]
	TIME [epoch: 7.84 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007912637042391288		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0007912637042391288 | validation: 0.0016376060082618437]
	TIME [epoch: 7.82 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010403616842555184		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0010403616842555184 | validation: 0.0010382782829670143]
	TIME [epoch: 7.84 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007372421711366072		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0007372421711366072 | validation: 0.003248948445790758]
	TIME [epoch: 7.88 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007861847432625998		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0007861847432625998 | validation: 0.0006718311054762199]
	TIME [epoch: 7.88 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009124301007344857		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0009124301007344857 | validation: 0.00041118208279466105]
	TIME [epoch: 7.84 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000551339157890603		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.000551339157890603 | validation: 0.0008117054541959306]
	TIME [epoch: 7.83 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005839690765459984		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0005839690765459984 | validation: 0.0007952778807266592]
	TIME [epoch: 7.84 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000632935116231621		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.000632935116231621 | validation: 0.0007788472742737627]
	TIME [epoch: 7.88 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008747399651651256		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0008747399651651256 | validation: 0.0010496222921430203]
	TIME [epoch: 7.83 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008282700454369025		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0008282700454369025 | validation: 0.0011936312768627035]
	TIME [epoch: 7.83 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000619192921638446		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.000619192921638446 | validation: 0.0002751625929919985]
	TIME [epoch: 7.82 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010119144822115472		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.0010119144822115472 | validation: 0.0014720414344144193]
	TIME [epoch: 7.81 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000856378858374373		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.000856378858374373 | validation: 0.0009681122844812161]
	TIME [epoch: 7.87 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000982183525826083		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.000982183525826083 | validation: 0.0005279313760103399]
	TIME [epoch: 7.82 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006334898303848149		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0006334898303848149 | validation: 0.001130704202555476]
	TIME [epoch: 7.83 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000976577356384365		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.000976577356384365 | validation: 0.0016421638351091792]
	TIME [epoch: 7.88 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011822783209312568		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0011822783209312568 | validation: 0.001510796295847717]
	TIME [epoch: 7.88 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009283289472815609		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0009283289472815609 | validation: 0.0011766304543951716]
	TIME [epoch: 7.87 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000579962795376928		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.000579962795376928 | validation: 0.0010231195497652462]
	TIME [epoch: 7.83 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006674906878029656		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0006674906878029656 | validation: 0.0005288909977530932]
	TIME [epoch: 7.83 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007818321989211931		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0007818321989211931 | validation: 0.0008158949790062452]
	TIME [epoch: 7.83 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008696602688276101		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0008696602688276101 | validation: 0.000436829367634477]
	TIME [epoch: 7.89 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010288806147505302		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0010288806147505302 | validation: 0.0015384315450703446]
	TIME [epoch: 7.84 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011505539142589692		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0011505539142589692 | validation: 0.0007583989619922545]
	TIME [epoch: 7.83 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006827373288837581		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0006827373288837581 | validation: 0.0007000170959237124]
	TIME [epoch: 7.84 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000543628181343127		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.000543628181343127 | validation: 0.013444271275212454]
	TIME [epoch: 7.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009526134227340487		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0009526134227340487 | validation: 0.0009809805446805671]
	TIME [epoch: 7.89 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006793486551616797		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0006793486551616797 | validation: 0.0007841485713499612]
	TIME [epoch: 7.86 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007353803770376028		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0007353803770376028 | validation: 0.0009402767910734759]
	TIME [epoch: 7.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006656011694780365		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0006656011694780365 | validation: 0.00236482299805401]
	TIME [epoch: 7.84 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002799990692459165		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.002799990692459165 | validation: 0.001286671141263474]
	TIME [epoch: 7.84 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008473923956711709		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0008473923956711709 | validation: 0.0009717104254426339]
	TIME [epoch: 7.88 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005019255538212864		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0005019255538212864 | validation: 0.0037748264856632937]
	TIME [epoch: 7.82 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006641347925480771		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0006641347925480771 | validation: 0.0005662939999397132]
	TIME [epoch: 7.83 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008330864022334012		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0008330864022334012 | validation: 0.006684970246339804]
	TIME [epoch: 7.83 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007719175163558727		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0007719175163558727 | validation: 0.0002617390443177934]
	TIME [epoch: 7.84 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041931741021041045		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.00041931741021041045 | validation: 0.0009071560364467537]
	TIME [epoch: 7.87 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005161606416236919		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.0005161606416236919 | validation: 0.0009763605863869139]
	TIME [epoch: 7.82 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007623054791621243		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0007623054791621243 | validation: 0.0021301357466824997]
	TIME [epoch: 7.84 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00115337373397599		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.00115337373397599 | validation: 0.0010865734451135439]
	TIME [epoch: 7.84 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006085547390733239		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0006085547390733239 | validation: 0.0010368278426748888]
	TIME [epoch: 7.93 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007791863981393464		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0007791863981393464 | validation: 0.0011445218826015102]
	TIME [epoch: 7.86 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009613660537867615		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0009613660537867615 | validation: 0.0007470641440034313]
	TIME [epoch: 7.83 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008525673688922407		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0008525673688922407 | validation: 0.0012426436005149536]
	TIME [epoch: 7.83 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007365909774393437		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.0007365909774393437 | validation: 0.0008012178021273462]
	TIME [epoch: 7.83 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000846541642183624		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.000846541642183624 | validation: 0.00028505813290705985]
	TIME [epoch: 7.87 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006275712480587752		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0006275712480587752 | validation: 0.000978152925049324]
	TIME [epoch: 7.82 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007250267825918608		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0007250267825918608 | validation: 0.0011121700535634239]
	TIME [epoch: 7.82 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006864990282172861		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0006864990282172861 | validation: 0.0015084190868599014]
	TIME [epoch: 7.84 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006554524983371486		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.0006554524983371486 | validation: 0.001045241362067193]
	TIME [epoch: 7.83 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007936774978202068		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.0007936774978202068 | validation: 0.0007932900084278582]
	TIME [epoch: 7.88 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008659358843229705		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0008659358843229705 | validation: 0.0009507286801396058]
	TIME [epoch: 7.83 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005843585002933827		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0005843585002933827 | validation: 0.0007940170555314899]
	TIME [epoch: 7.87 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005073812651603715		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0005073812651603715 | validation: 0.0009316832911954772]
	TIME [epoch: 7.86 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005904837627426138		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0005904837627426138 | validation: 0.0021229323128663076]
	TIME [epoch: 7.86 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004184029466672409		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0004184029466672409 | validation: 0.0009970552155806923]
	TIME [epoch: 7.85 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014012913428931549		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0014012913428931549 | validation: 0.0023274928165627788]
	TIME [epoch: 7.83 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000673894580777173		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.000673894580777173 | validation: 0.0013033619968623508]
	TIME [epoch: 7.83 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035218643080018496		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.00035218643080018496 | validation: 0.0010205750911091815]
	TIME [epoch: 7.83 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010097366779600004		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.0010097366779600004 | validation: 0.0031587781552670693]
	TIME [epoch: 7.87 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005702863856390796		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0005702863856390796 | validation: 0.0006230979213372194]
	TIME [epoch: 7.84 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005031711380648489		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.0005031711380648489 | validation: 0.0010774982149610412]
	TIME [epoch: 7.83 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006852560533462713		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0006852560533462713 | validation: 0.0010680299591042993]
	TIME [epoch: 7.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005082249765754423		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0005082249765754423 | validation: 0.0014435992433990076]
	TIME [epoch: 7.85 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011103884558507986		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0011103884558507986 | validation: 0.0011586922135319196]
	TIME [epoch: 7.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007645690279927965		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0007645690279927965 | validation: 0.001498856442294759]
	TIME [epoch: 7.86 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006275024691115834		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0006275024691115834 | validation: 0.0005558766715925208]
	TIME [epoch: 7.85 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007108044563263039		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0007108044563263039 | validation: 0.0002823250489116944]
	TIME [epoch: 7.84 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004851834574200333		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0004851834574200333 | validation: 0.005590176853325858]
	TIME [epoch: 7.85 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006860687293109259		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0006860687293109259 | validation: 0.0007002971090694662]
	TIME [epoch: 7.88 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007297972354580848		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.0007297972354580848 | validation: 0.0008754978751015763]
	TIME [epoch: 7.84 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000735592938886668		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.000735592938886668 | validation: 0.0008281180592201096]
	TIME [epoch: 7.84 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007619480495209828		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.0007619480495209828 | validation: 0.0010065308037562018]
	TIME [epoch: 7.85 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007654059847910068		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0007654059847910068 | validation: 0.001304869602415452]
	TIME [epoch: 7.91 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007648766981031283		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0007648766981031283 | validation: 0.001022475676710041]
	TIME [epoch: 7.88 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008612431102018719		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0008612431102018719 | validation: 0.0006720756581357552]
	TIME [epoch: 7.83 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000576678461032171		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.000576678461032171 | validation: 0.0004925904885343]
	TIME [epoch: 7.83 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005815843648580048		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0005815843648580048 | validation: 0.0005596504554201482]
	TIME [epoch: 7.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043036449819887953		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.00043036449819887953 | validation: 0.0010185436717254062]
	TIME [epoch: 7.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007752485923851493		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0007752485923851493 | validation: 0.0007553953242376279]
	TIME [epoch: 7.86 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005015886562414629		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0005015886562414629 | validation: 0.0012451830867132225]
	TIME [epoch: 7.84 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005706235812636868		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0005706235812636868 | validation: 0.014706240642893782]
	TIME [epoch: 7.85 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012758966571435595		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0012758966571435595 | validation: 0.0007380199648665951]
	TIME [epoch: 7.84 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008630252805442373		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0008630252805442373 | validation: 0.0007843719119134977]
	TIME [epoch: 7.89 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005194157186288203		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.0005194157186288203 | validation: 0.0007238927675382873]
	TIME [epoch: 7.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004800607256506262		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0004800607256506262 | validation: 0.0011903197975336958]
	TIME [epoch: 7.84 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007276971475702081		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0007276971475702081 | validation: 0.014162263716587953]
	TIME [epoch: 7.84 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008211926087644322		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0008211926087644322 | validation: 0.0006710002852744133]
	TIME [epoch: 7.87 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006714034374550662		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0006714034374550662 | validation: 0.0007724623857994652]
	TIME [epoch: 7.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008913866504655801		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0008913866504655801 | validation: 0.0011877951684850247]
	TIME [epoch: 7.87 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005889506663900486		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0005889506663900486 | validation: 0.0012549190871701831]
	TIME [epoch: 7.82 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005541882503483834		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0005541882503483834 | validation: 0.0007978413282440826]
	TIME [epoch: 7.84 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005693701396683782		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0005693701396683782 | validation: 0.00044929865573295214]
	TIME [epoch: 7.88 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008580722636913876		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0008580722636913876 | validation: 0.0025652800862230388]
	TIME [epoch: 7.85 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003767286565274264		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0003767286565274264 | validation: 0.0009998942071013674]
	TIME [epoch: 7.84 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005639629705747105		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0005639629705747105 | validation: 0.0007082897226390078]
	TIME [epoch: 7.84 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006260467013761286		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0006260467013761286 | validation: 0.0017991544447879572]
	TIME [epoch: 7.85 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008452938228948945		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.0008452938228948945 | validation: 0.0013637589869310266]
	TIME [epoch: 7.89 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005641109314220374		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0005641109314220374 | validation: 0.0006916741833804112]
	TIME [epoch: 7.84 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004820694479840224		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.0004820694479840224 | validation: 0.0007634993710582654]
	TIME [epoch: 7.86 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005846697331188395		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.0005846697331188395 | validation: 0.0005534914545343375]
	TIME [epoch: 7.87 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005418078403610239		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0005418078403610239 | validation: 0.002158944989789897]
	TIME [epoch: 7.88 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006933379438043563		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0006933379438043563 | validation: 0.0004960802733471707]
	TIME [epoch: 7.93 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005724916497416308		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0005724916497416308 | validation: 0.0013612008915435494]
	TIME [epoch: 7.85 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004479028183012639		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0004479028183012639 | validation: 0.0010695262547443748]
	TIME [epoch: 7.83 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005398840186627849		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0005398840186627849 | validation: 0.001427317032646778]
	TIME [epoch: 7.85 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009823952637685277		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0009823952637685277 | validation: 0.00333873796722371]
	TIME [epoch: 7.85 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004817252623640336		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0004817252623640336 | validation: 0.0033757275532394024]
	TIME [epoch: 7.87 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005445878918909651		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0005445878918909651 | validation: 0.0008057940680660965]
	TIME [epoch: 7.85 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000703242365963932		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.000703242365963932 | validation: 0.0006762270010732863]
	TIME [epoch: 7.85 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004585148530431782		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0004585148530431782 | validation: 0.0013082403024547579]
	TIME [epoch: 7.84 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006825651716447161		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.0006825651716447161 | validation: 0.00047027980786922365]
	TIME [epoch: 7.88 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043022500453370686		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.00043022500453370686 | validation: 0.0012238706155554393]
	TIME [epoch: 7.84 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006858234955690237		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0006858234955690237 | validation: 0.0014940912734206276]
	TIME [epoch: 7.83 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007160637375127845		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0007160637375127845 | validation: 0.0009842449400542198]
	TIME [epoch: 7.84 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000664342103241218		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.000664342103241218 | validation: 0.015621002210572242]
	TIME [epoch: 7.84 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005719820801398661		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0005719820801398661 | validation: 0.0008894593432368882]
	TIME [epoch: 7.91 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046954858319539234		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.00046954858319539234 | validation: 0.0016811057645976773]
	TIME [epoch: 7.88 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008614093656322875		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0008614093656322875 | validation: 0.001017651368007038]
	TIME [epoch: 7.83 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041782844274098794		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.00041782844274098794 | validation: 0.0007334178738494135]
	TIME [epoch: 7.83 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007014931893823055		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0007014931893823055 | validation: 0.0010046234832925256]
	TIME [epoch: 7.84 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004400716195844736		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0004400716195844736 | validation: 0.00043242711492448295]
	TIME [epoch: 7.87 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045120316974915615		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.00045120316974915615 | validation: 0.001358203548057121]
	TIME [epoch: 7.84 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046611664244242037		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.00046611664244242037 | validation: 0.002509113572917369]
	TIME [epoch: 7.83 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027943709728546213		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.00027943709728546213 | validation: 0.0014717848432636719]
	TIME [epoch: 7.85 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007276273290021873		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0007276273290021873 | validation: 0.0010988774659021522]
	TIME [epoch: 7.85 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007768848548026228		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0007768848548026228 | validation: 0.0005282329752676658]
	TIME [epoch: 7.88 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004694145077921945		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0004694145077921945 | validation: 0.0006942410657472715]
	TIME [epoch: 7.84 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043883282821793965		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.00043883282821793965 | validation: 0.0009419743937069196]
	TIME [epoch: 7.84 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006494429757954571		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0006494429757954571 | validation: 0.0004677112176405593]
	TIME [epoch: 7.89 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000649379820846732		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.000649379820846732 | validation: 0.0008116742908633183]
	TIME [epoch: 7.93 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008414736971327615		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.0008414736971327615 | validation: 0.0005506089226487569]
	TIME [epoch: 7.86 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007737170370979632		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.0007737170370979632 | validation: 0.002288736638778278]
	TIME [epoch: 7.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000642719792606393		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.000642719792606393 | validation: 0.0008872728811723078]
	TIME [epoch: 7.84 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007245928411126248		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0007245928411126248 | validation: 0.0008629946603783765]
	TIME [epoch: 7.84 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039821935082301875		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.00039821935082301875 | validation: 0.00432822600667726]
	TIME [epoch: 7.89 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005694502522139309		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0005694502522139309 | validation: 0.0006132545036287853]
	TIME [epoch: 7.84 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001683428583268005		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0001683428583268005 | validation: 0.0005404144786671834]
	TIME [epoch: 7.83 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008298953451455228		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0008298953451455228 | validation: 0.0036567199672869065]
	TIME [epoch: 7.83 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009000708675106902		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0009000708675106902 | validation: 0.0006492653506707073]
	TIME [epoch: 7.85 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004162054459148801		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0004162054459148801 | validation: 0.0007191994814227662]
	TIME [epoch: 7.87 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006756446815876132		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0006756446815876132 | validation: 0.001199083204703344]
	TIME [epoch: 7.85 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006541598217521207		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0006541598217521207 | validation: 0.0007250734647006657]
	TIME [epoch: 7.88 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043711364317909867		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.00043711364317909867 | validation: 0.0003931445918940302]
	TIME [epoch: 7.88 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000405631501155626		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.000405631501155626 | validation: 0.002607662686878004]
	TIME [epoch: 7.86 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004917522329834074		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.0004917522329834074 | validation: 0.004308915733746601]
	TIME [epoch: 7.86 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003983938508468108		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.0003983938508468108 | validation: 0.0020421164391903382]
	TIME [epoch: 7.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008531092982529528		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0008531092982529528 | validation: 0.0005480903012373428]
	TIME [epoch: 7.85 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005809985666990039		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0005809985666990039 | validation: 0.0004005448608387718]
	TIME [epoch: 7.84 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006624264912124796		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0006624264912124796 | validation: 0.0007482651505411227]
	TIME [epoch: 7.89 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024350592932731696		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.00024350592932731696 | validation: 0.0008993177497068939]
	TIME [epoch: 7.85 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023086915142818732		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.00023086915142818732 | validation: 0.002162725912960979]
	TIME [epoch: 7.85 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033586529283217547		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.00033586529283217547 | validation: 0.0008952276307897096]
	TIME [epoch: 7.84 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005425949340125646		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0005425949340125646 | validation: 0.0005130496228670491]
	TIME [epoch: 7.83 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004676831809915749		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0004676831809915749 | validation: 0.0028993357858880155]
	TIME [epoch: 7.88 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007454090911797722		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.0007454090911797722 | validation: 0.00041527537083289087]
	TIME [epoch: 7.86 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006485896736775756		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.0006485896736775756 | validation: 0.0006284298418293543]
	TIME [epoch: 7.86 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037981126324512		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.00037981126324512 | validation: 0.0010086408919004403]
	TIME [epoch: 7.83 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005630167999878289		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0005630167999878289 | validation: 0.0008486462018747396]
	TIME [epoch: 7.84 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007359529476547754		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0007359529476547754 | validation: 0.0008288144058894158]
	TIME [epoch: 7.87 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006472938393570361		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0006472938393570361 | validation: 0.0019261866691264453]
	TIME [epoch: 7.83 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004272273006135288		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.0004272273006135288 | validation: 0.0055181326113552176]
	TIME [epoch: 7.83 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003269841610170374		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0003269841610170374 | validation: 0.002079282115624129]
	TIME [epoch: 7.83 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048439345848900106		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.00048439345848900106 | validation: 0.0004416873101743723]
	TIME [epoch: 7.86 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007364286337003796		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.0007364286337003796 | validation: 0.0005140914359723513]
	TIME [epoch: 7.83 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004464560423257229		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0004464560423257229 | validation: 0.0014871136294353278]
	TIME [epoch: 7.83 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005249828482551927		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0005249828482551927 | validation: 0.0013421727708223018]
	TIME [epoch: 7.83 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006256678417179045		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.0006256678417179045 | validation: 0.010794529506781777]
	TIME [epoch: 7.83 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007744933696472573		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0007744933696472573 | validation: 0.0007561944596736598]
	TIME [epoch: 7.86 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036730360646055964		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.00036730360646055964 | validation: 0.0008279083982460103]
	TIME [epoch: 7.83 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047814079426304514		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.00047814079426304514 | validation: 0.00027007814616553994]
	TIME [epoch: 7.84 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033468399782330186		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.00033468399782330186 | validation: 0.0005384497954993384]
	TIME [epoch: 7.86 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004733464169107447		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0004733464169107447 | validation: 0.0012843203445469244]
	TIME [epoch: 7.87 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000689562338079673		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.000689562338079673 | validation: 0.0015636818977942015]
	TIME [epoch: 7.86 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034409592862491236		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.00034409592862491236 | validation: 0.002221488675043739]
	TIME [epoch: 7.82 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046534707998848845		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.00046534707998848845 | validation: 0.0011339021908802103]
	TIME [epoch: 7.83 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031764393623997926		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.00031764393623997926 | validation: 0.0017431224175171963]
	TIME [epoch: 7.83 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002288640937512183		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0002288640937512183 | validation: 0.0007930172509136462]
	TIME [epoch: 7.85 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005684728511859391		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0005684728511859391 | validation: 0.0020947407316193453]
	TIME [epoch: 7.87 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005263253205805912		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0005263253205805912 | validation: 0.0006719226252844513]
	TIME [epoch: 7.84 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046794130002133377		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.00046794130002133377 | validation: 0.0011109659990100724]
	TIME [epoch: 7.81 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041310403444885465		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.00041310403444885465 | validation: 0.0002484886688756713]
	TIME [epoch: 7.81 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005659879065784817		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0005659879065784817 | validation: 0.003483225669971476]
	TIME [epoch: 7.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004180000528609194		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0004180000528609194 | validation: 0.0007859460138792525]
	TIME [epoch: 7.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004215003674251772		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0004215003674251772 | validation: 0.000778239280557611]
	TIME [epoch: 7.76 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004698235453271533		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0004698235453271533 | validation: 0.00229186295929315]
	TIME [epoch: 7.76 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003833219414430566		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0003833219414430566 | validation: 0.0005598759856542373]
	TIME [epoch: 7.76 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005808814329214037		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0005808814329214037 | validation: 0.00039095858323670904]
	TIME [epoch: 7.8 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006262017876727415		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.0006262017876727415 | validation: 0.0023048766678307082]
	TIME [epoch: 7.77 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033063085553379864		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.00033063085553379864 | validation: 0.000966096239673365]
	TIME [epoch: 7.76 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041733617024931506		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.00041733617024931506 | validation: 0.001598380880254087]
	TIME [epoch: 7.76 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004135874065426219		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0004135874065426219 | validation: 0.0011920801596861379]
	TIME [epoch: 7.76 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035082857318915854		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.00035082857318915854 | validation: 0.006463424361465363]
	TIME [epoch: 7.81 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043141269739727165		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.00043141269739727165 | validation: 0.00021394064017203897]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_1143.pth
	Model improved!!!
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004026708147585309		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0004026708147585309 | validation: 0.0008585356576417285]
	TIME [epoch: 7.87 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000408872725362311		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.000408872725362311 | validation: 0.0008187120068313138]
	TIME [epoch: 7.85 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042520465263303604		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.00042520465263303604 | validation: 0.000718224678426818]
	TIME [epoch: 7.83 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047610263567164467		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.00047610263567164467 | validation: 0.0007021022058863453]
	TIME [epoch: 7.81 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045733659073330556		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.00045733659073330556 | validation: 0.0005972619279289227]
	TIME [epoch: 7.77 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037245048910709497		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.00037245048910709497 | validation: 0.0016077960125672997]
	TIME [epoch: 7.78 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006281466101602539		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.0006281466101602539 | validation: 0.0007881399044999826]
	TIME [epoch: 7.77 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008518390421274368		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.0008518390421274368 | validation: 0.0004754915599127054]
	TIME [epoch: 7.82 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005336505371227532		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.0005336505371227532 | validation: 0.0009595141417459727]
	TIME [epoch: 7.79 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046381416061826196		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.00046381416061826196 | validation: 0.0007900414890788534]
	TIME [epoch: 7.78 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003944006894815218		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0003944006894815218 | validation: 0.00026737507098528116]
	TIME [epoch: 7.78 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003787040081852331		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.0003787040081852331 | validation: 0.0018286928027750405]
	TIME [epoch: 7.78 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041705012040757717		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.00041705012040757717 | validation: 0.0005801247843107795]
	TIME [epoch: 7.84 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004758809649903315		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.0004758809649903315 | validation: 0.0003793059332688946]
	TIME [epoch: 7.87 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005205937042498756		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0005205937042498756 | validation: 0.0016519807756579272]
	TIME [epoch: 7.89 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000548963749676199		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.000548963749676199 | validation: 0.0004145272300535749]
	TIME [epoch: 7.85 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035320092845497644		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.00035320092845497644 | validation: 0.000357603418286569]
	TIME [epoch: 7.78 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022617651923122174		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.00022617651923122174 | validation: 0.0012185976887628069]
	TIME [epoch: 7.83 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003633406516438607		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0003633406516438607 | validation: 0.0018065601510267087]
	TIME [epoch: 7.79 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005070547293105776		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0005070547293105776 | validation: 0.0011846286659394564]
	TIME [epoch: 7.79 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007582469580619592		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0007582469580619592 | validation: 0.0002422196719965548]
	TIME [epoch: 7.79 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005300854843894116		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0005300854843894116 | validation: 8.563372620995759e-05]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240520_151317/states/model_phi1_1a_v_mmd1_smallnet_1165.pth
	Model improved!!!
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003926637027902888		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0003926637027902888 | validation: 0.001206843992872524]
	TIME [epoch: 7.84 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038533725816770035		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.00038533725816770035 | validation: 0.002884646715646717]
	TIME [epoch: 7.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005261898775324683		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0005261898775324683 | validation: 0.0011647023282158112]
	TIME [epoch: 7.79 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005817868401525737		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.0005817868401525737 | validation: 0.0017114732116260444]
	TIME [epoch: 7.81 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004507135078769529		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0004507135078769529 | validation: 0.0004476622768569936]
	TIME [epoch: 7.92 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006479439325003685		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.0006479439325003685 | validation: 0.0003633301443145047]
	TIME [epoch: 7.9 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032734303475057927		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.00032734303475057927 | validation: 0.005987933201410526]
	TIME [epoch: 7.86 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042375659010049913		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.00042375659010049913 | validation: 0.0006318980853256023]
	TIME [epoch: 7.79 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005353646415586564		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0005353646415586564 | validation: 0.0008504027072361664]
	TIME [epoch: 7.78 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005150563934398192		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0005150563934398192 | validation: 0.0012012429424711826]
	TIME [epoch: 7.83 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004852598703731243		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.0004852598703731243 | validation: 0.0008463425836348346]
	TIME [epoch: 7.79 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032639237691113967		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.00032639237691113967 | validation: 0.0011561240985938585]
	TIME [epoch: 7.79 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003430417438655984		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0003430417438655984 | validation: 0.006335577154610929]
	TIME [epoch: 7.8 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006309012936019854		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.0006309012936019854 | validation: 0.0005921897633719917]
	TIME [epoch: 7.78 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004587839621101302		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.0004587839621101302 | validation: 0.00211379526982014]
	TIME [epoch: 7.84 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007698286312498349		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.0007698286312498349 | validation: 0.0005307040156196656]
	TIME [epoch: 7.79 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006060091276307376		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0006060091276307376 | validation: 0.00045500623696893426]
	TIME [epoch: 7.79 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028172946318069506		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.00028172946318069506 | validation: 0.0011034586990095452]
	TIME [epoch: 7.88 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007046637852695264		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.0007046637852695264 | validation: 0.002850573883665577]
	TIME [epoch: 7.91 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008577526790866841		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.0008577526790866841 | validation: 0.002087413187713512]
	TIME [epoch: 7.87 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047886445927243276		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.00047886445927243276 | validation: 0.003395906384426592]
	TIME [epoch: 7.78 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005546012568178865		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0005546012568178865 | validation: 0.0006826394132573368]
	TIME [epoch: 7.79 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025631841368691145		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.00025631841368691145 | validation: 0.001973616706828551]
	TIME [epoch: 7.78 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005775720499573819		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.0005775720499573819 | validation: 0.0011522270905291575]
	TIME [epoch: 7.84 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036581417947477955		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.00036581417947477955 | validation: 0.0002765485756770873]
	TIME [epoch: 7.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003628653534567778		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.0003628653534567778 | validation: 0.0007731837852338482]
	TIME [epoch: 7.79 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002570668710819111		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0002570668710819111 | validation: 0.002097398440477895]
	TIME [epoch: 7.78 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004831956417241421		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0004831956417241421 | validation: 0.0010541079875767529]
	TIME [epoch: 7.79 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005474654915975154		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0005474654915975154 | validation: 0.0012064195677160816]
	TIME [epoch: 7.83 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005089245315680877		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.0005089245315680877 | validation: 0.00014757610296863976]
	TIME [epoch: 7.78 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026291207011333405		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.00026291207011333405 | validation: 0.0008172732375079148]
	TIME [epoch: 7.87 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033561500506284526		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.00033561500506284526 | validation: 0.0006676620131876687]
	TIME [epoch: 7.88 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036857518133415846		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.00036857518133415846 | validation: 0.001523283567859819]
	TIME [epoch: 7.86 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004014802490188582		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.0004014802490188582 | validation: 0.001817038428831296]
	TIME [epoch: 7.9 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004369108945761823		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.0004369108945761823 | validation: 0.0012337858009624485]
	TIME [epoch: 7.84 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048365512334765874		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.00048365512334765874 | validation: 0.0006713110085437117]
	TIME [epoch: 7.79 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002201632592648124		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0002201632592648124 | validation: 0.0019310457828626833]
	TIME [epoch: 7.79 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025058293985559543		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.00025058293985559543 | validation: 0.0009184615163828384]
	TIME [epoch: 7.81 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004282985576534977		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0004282985576534977 | validation: 0.0006800958079421324]
	TIME [epoch: 7.81 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003890369506703404		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0003890369506703404 | validation: 0.00127655034755118]
	TIME [epoch: 7.8 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004080071627174365		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.0004080071627174365 | validation: 0.00153894707482021]
	TIME [epoch: 7.79 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030043107543185156		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.00030043107543185156 | validation: 0.0011528333694372775]
	TIME [epoch: 7.79 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042437271324125025		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.00042437271324125025 | validation: 0.0005908528546239537]
	TIME [epoch: 7.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023019908394203293		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.00023019908394203293 | validation: 0.001033286447186188]
	TIME [epoch: 7.84 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008856742773100095		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0008856742773100095 | validation: 0.0011495253291007436]
	TIME [epoch: 7.84 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048425135397531043		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.00048425135397531043 | validation: 0.0014415299734677589]
	TIME [epoch: 7.86 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039245986677383777		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.00039245986677383777 | validation: 0.002398171775679283]
	TIME [epoch: 7.86 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003701297287005571		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.0003701297287005571 | validation: 0.001516497293973944]
	TIME [epoch: 7.9 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004421756425846353		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0004421756425846353 | validation: 0.00028426167556198494]
	TIME [epoch: 7.83 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000560094473903162		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.000560094473903162 | validation: 0.0004863892991953609]
	TIME [epoch: 7.79 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004244325364302839		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.0004244325364302839 | validation: 0.004794807771956952]
	TIME [epoch: 7.79 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041624372660007867		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.00041624372660007867 | validation: 0.0033986810075235826]
	TIME [epoch: 7.82 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026546723262928506		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.00026546723262928506 | validation: 0.0028457773857188708]
	TIME [epoch: 7.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039314334886013304		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.00039314334886013304 | validation: 0.0008350289602667456]
	TIME [epoch: 7.79 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005029094815798348		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.0005029094815798348 | validation: 0.0007317545757699735]
	TIME [epoch: 7.78 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006474120381848413		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.0006474120381848413 | validation: 0.000829639736946243]
	TIME [epoch: 7.78 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004773859850112971		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.0004773859850112971 | validation: 0.0024454325986141548]
	TIME [epoch: 7.84 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044437417898691295		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.00044437417898691295 | validation: 0.0020742485293615094]
	TIME [epoch: 7.8 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004597898671747321		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.0004597898671747321 | validation: 0.0008347366981089577]
	TIME [epoch: 7.77 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038552241548559164		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.00038552241548559164 | validation: 0.002508560294181034]
	TIME [epoch: 7.79 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026201030476898904		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.00026201030476898904 | validation: 0.0009180460550748632]
	TIME [epoch: 7.8 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002589430221674056		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.0002589430221674056 | validation: 0.0021626616388487835]
	TIME [epoch: 7.89 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008799394673485588		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0008799394673485588 | validation: 0.00040417161024799595]
	TIME [epoch: 7.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003239619383899239		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.0003239619383899239 | validation: 0.0005696183966422872]
	TIME [epoch: 7.8 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037130382134644194		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.00037130382134644194 | validation: 0.0005816306919537357]
	TIME [epoch: 7.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034920073470809035		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.00034920073470809035 | validation: 0.0006409305704041009]
	TIME [epoch: 7.82 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004392413403587818		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.0004392413403587818 | validation: 0.0003504998588581412]
	TIME [epoch: 7.86 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006264831268700415		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0006264831268700415 | validation: 0.000939691487640305]
	TIME [epoch: 7.82 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001794081562281602		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.0001794081562281602 | validation: 0.0007544545184168205]
	TIME [epoch: 7.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003475470509041669		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.0003475470509041669 | validation: 0.0025151060836595676]
	TIME [epoch: 7.8 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003508450609870384		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.0003508450609870384 | validation: 0.0028008411953412207]
	TIME [epoch: 7.82 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034458098591951924		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.00034458098591951924 | validation: 0.0009750310134978744]
	TIME [epoch: 7.85 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035011530183725535		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.00035011530183725535 | validation: 0.0009006779609320699]
	TIME [epoch: 7.79 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024066436248184567		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.00024066436248184567 | validation: 0.0010067855714351382]
	TIME [epoch: 7.77 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005594669319059597		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0005594669319059597 | validation: 0.0010660748150580616]
	TIME [epoch: 7.78 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023793128986751455		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.00023793128986751455 | validation: 0.0008726468725467492]
	TIME [epoch: 7.81 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004890527059596781		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0004890527059596781 | validation: 0.00024014871240144413]
	TIME [epoch: 7.82 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006065806663602345		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.0006065806663602345 | validation: 0.0016107962013093902]
	TIME [epoch: 7.78 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003720995425160776		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.0003720995425160776 | validation: 0.0014447482221466039]
	TIME [epoch: 7.78 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000436576945843294		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.000436576945843294 | validation: 0.0011275741357524138]
	TIME [epoch: 7.79 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004195487786891387		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0004195487786891387 | validation: 0.005528930317016321]
	TIME [epoch: 7.84 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048381089653970477		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.00048381089653970477 | validation: 0.0003237739481668225]
	TIME [epoch: 7.8 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003163550572116312		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.0003163550572116312 | validation: 0.002949135784085775]
	TIME [epoch: 7.78 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002774526017497285		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.0002774526017497285 | validation: 0.0015363473588776966]
	TIME [epoch: 7.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039019867658035914		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.00039019867658035914 | validation: 0.0012798346820650187]
	TIME [epoch: 7.83 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030608315960086634		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.00030608315960086634 | validation: 0.0022813793829193287]
	TIME [epoch: 7.92 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039444359998458477		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.00039444359998458477 | validation: 0.0005721095221831352]
	TIME [epoch: 7.84 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005003200649450515		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0005003200649450515 | validation: 0.0038628682398871962]
	TIME [epoch: 7.82 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000454429292980012		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.000454429292980012 | validation: 0.00041187959762997294]
	TIME [epoch: 7.84 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002714216364553133		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.0002714216364553133 | validation: 0.0003216154174492641]
	TIME [epoch: 7.83 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031483542356660797		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.00031483542356660797 | validation: 0.0002772752999188359]
	TIME [epoch: 7.88 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027672097675450756		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.00027672097675450756 | validation: 0.0009892356907475958]
	TIME [epoch: 7.84 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006993998991806645		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.0006993998991806645 | validation: 0.0005626302952130384]
	TIME [epoch: 7.83 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043812505680577686		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.00043812505680577686 | validation: 0.0021199370358084605]
	TIME [epoch: 7.83 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022422122328829053		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.00022422122328829053 | validation: 0.001060581206342139]
	TIME [epoch: 7.85 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002661351229230777		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.0002661351229230777 | validation: 0.0015179580593268085]
	TIME [epoch: 7.87 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003300259181115932		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.0003300259181115932 | validation: 0.0008922977440665436]
	TIME [epoch: 7.84 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019909167940100668		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.00019909167940100668 | validation: 0.003247148538924974]
	TIME [epoch: 7.84 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004077384458940363		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.0004077384458940363 | validation: 0.000375152803487568]
	TIME [epoch: 7.82 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038953014005265363		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.00038953014005265363 | validation: 0.000233517229409026]
	TIME [epoch: 7.88 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023580547385658137		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.00023580547385658137 | validation: 0.001069162454036393]
	TIME [epoch: 7.84 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003141682700621349		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.0003141682700621349 | validation: 0.001340140031444399]
	TIME [epoch: 7.84 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023580825432861798		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.00023580825432861798 | validation: 0.000621774566796498]
	TIME [epoch: 7.88 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005362244898472326		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.0005362244898472326 | validation: 0.002458721885974753]
	TIME [epoch: 7.88 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032832609097489975		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.00032832609097489975 | validation: 0.0006211626278135656]
	TIME [epoch: 7.88 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000423790242945753		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.000423790242945753 | validation: 0.0016906600655955284]
	TIME [epoch: 7.84 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030947145890313665		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.00030947145890313665 | validation: 0.0004354109486875153]
	TIME [epoch: 7.83 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000301193411733572		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.000301193411733572 | validation: 0.001289005223181107]
	TIME [epoch: 7.83 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038219712140277127		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.00038219712140277127 | validation: 0.0005511214214385989]
	TIME [epoch: 7.84 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035448093162097005		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.00035448093162097005 | validation: 0.0007234551993626592]
	TIME [epoch: 7.89 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038658808934138		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.00038658808934138 | validation: 0.0012136254349370186]
	TIME [epoch: 7.83 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000263131894058003		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.000263131894058003 | validation: 0.002096229050628033]
	TIME [epoch: 7.82 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046183657236146836		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.00046183657236146836 | validation: 0.0005764808512947629]
	TIME [epoch: 7.82 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003294740931979652		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.0003294740931979652 | validation: 0.0014297432584503005]
	TIME [epoch: 7.86 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005145289021185158		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0005145289021185158 | validation: 0.0029139953533969766]
	TIME [epoch: 7.86 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029423462654869616		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.00029423462654869616 | validation: 0.0013241227051579135]
	TIME [epoch: 7.83 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004603350764434204		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.0004603350764434204 | validation: 0.0003871117184608197]
	TIME [epoch: 7.84 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022034593758419674		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.00022034593758419674 | validation: 0.0006821843205516522]
	TIME [epoch: 7.84 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002172114459476682		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.0002172114459476682 | validation: 0.0009708822473460428]
	TIME [epoch: 7.88 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004908589140155182		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.0004908589140155182 | validation: 0.0016142869279287117]
	TIME [epoch: 7.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032317280645975807		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.00032317280645975807 | validation: 0.000798987180026554]
	TIME [epoch: 7.88 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005066631531693713		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.0005066631531693713 | validation: 0.0014403406533316128]
	TIME [epoch: 7.84 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006037962491437442		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.0006037962491437442 | validation: 0.0017548294848160834]
	TIME [epoch: 7.82 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041911611293820016		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.00041911611293820016 | validation: 0.0023628310929243446]
	TIME [epoch: 7.88 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047787169740131133		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.00047787169740131133 | validation: 0.0019914709506464786]
	TIME [epoch: 7.82 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005998741298414365		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.0005998741298414365 | validation: 0.0012273591731602202]
	TIME [epoch: 7.82 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036806371626426303		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.00036806371626426303 | validation: 0.0017792030477875153]
	TIME [epoch: 7.82 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004107224843305999		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.0004107224843305999 | validation: 0.002306917290127652]
	TIME [epoch: 7.84 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003365824940809574		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.0003365824940809574 | validation: 0.0009715813222188077]
	TIME [epoch: 7.87 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041094695675143814		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.00041094695675143814 | validation: 0.0004978374205647489]
	TIME [epoch: 7.82 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001771255249788417		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.0001771255249788417 | validation: 0.0007588542605553093]
	TIME [epoch: 7.83 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004931541715976999		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.0004931541715976999 | validation: 0.001845824959075835]
	TIME [epoch: 7.85 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004193134969738064		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.0004193134969738064 | validation: 0.0010684406627124092]
	TIME [epoch: 7.88 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003737396617639869		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.0003737396617639869 | validation: 0.0014539535664729176]
	TIME [epoch: 7.9 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035545628384118437		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.00035545628384118437 | validation: 0.0008746893782939989]
	TIME [epoch: 7.85 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023054586206183036		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.00023054586206183036 | validation: 0.002577582745895355]
	TIME [epoch: 7.82 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037537589433223387		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.00037537589433223387 | validation: 0.0006476804403027998]
	TIME [epoch: 7.82 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003930339729384768		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.0003930339729384768 | validation: 0.0013709227159278744]
	TIME [epoch: 7.88 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002964284065137837		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.0002964284065137837 | validation: 0.002804371576941516]
	TIME [epoch: 7.82 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003669050315213984		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.0003669050315213984 | validation: 0.0005100646807110216]
	TIME [epoch: 7.84 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029562611277712385		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.00029562611277712385 | validation: 0.0013150550128552343]
	TIME [epoch: 7.82 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003002595335206579		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.0003002595335206579 | validation: 0.0023632331539498582]
	TIME [epoch: 7.83 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028797678210358546		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.00028797678210358546 | validation: 0.0007729393719164079]
	TIME [epoch: 7.87 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027033383643760265		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.00027033383643760265 | validation: 0.0020093073086142896]
	TIME [epoch: 7.82 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039479668533097057		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.00039479668533097057 | validation: 0.0010265833201658554]
	TIME [epoch: 7.83 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036003101744307343		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.00036003101744307343 | validation: 0.0007646726575980267]
	TIME [epoch: 7.87 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042803736483639133		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.00042803736483639133 | validation: 0.00046008532949780355]
	TIME [epoch: 7.91 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003036597747059633		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.0003036597747059633 | validation: 0.0008179729884512614]
	TIME [epoch: 7.88 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005162093980894356		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.0005162093980894356 | validation: 0.0014969245231640311]
	TIME [epoch: 7.82 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031578971854078055		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.00031578971854078055 | validation: 0.003684292475625062]
	TIME [epoch: 7.83 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005739998911344151		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.0005739998911344151 | validation: 0.0013477236149729288]
	TIME [epoch: 7.82 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038081285758771835		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.00038081285758771835 | validation: 0.001164825821149009]
	TIME [epoch: 7.88 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004227564566226088		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.0004227564566226088 | validation: 0.0005995358012429418]
	TIME [epoch: 7.83 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032729535260996576		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.00032729535260996576 | validation: 0.000590307010465847]
	TIME [epoch: 7.84 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002998195570558235		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.0002998195570558235 | validation: 0.00033740217873372736]
	TIME [epoch: 7.84 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004678537056749621		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.0004678537056749621 | validation: 0.0007935852424169836]
	TIME [epoch: 7.83 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022068181263487573		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.00022068181263487573 | validation: 0.0007749456225666887]
	TIME [epoch: 7.88 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004174684968986253		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.0004174684968986253 | validation: 0.001002723926808887]
	TIME [epoch: 7.83 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000135721911685319		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.000135721911685319 | validation: 0.000740003960132663]
	TIME [epoch: 7.84 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003720816384660264		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.0003720816384660264 | validation: 0.00016549296984790196]
	TIME [epoch: 7.84 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042109795679629383		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.00042109795679629383 | validation: 0.0007882862337945422]
	TIME [epoch: 7.84 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005310147387697923		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.0005310147387697923 | validation: 0.0007834926211912974]
	TIME [epoch: 7.87 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003366112293043073		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.0003366112293043073 | validation: 0.0014899376404670522]
	TIME [epoch: 7.85 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003532675333835109		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.0003532675333835109 | validation: 0.00023587123885393437]
	TIME [epoch: 7.88 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042117683134414174		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.00042117683134414174 | validation: 0.001589833497041222]
	TIME [epoch: 7.86 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002753010079569493		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.0002753010079569493 | validation: 0.0003839619097213243]
	TIME [epoch: 7.87 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024409821762094052		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.00024409821762094052 | validation: 0.001079159517918379]
	TIME [epoch: 7.87 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003126201734640797		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.0003126201734640797 | validation: 0.00062702898298779]
	TIME [epoch: 7.83 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004109114992212905		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.0004109114992212905 | validation: 0.001689716952945325]
	TIME [epoch: 7.84 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003265161945718411		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.0003265161945718411 | validation: 0.0041091683310153495]
	TIME [epoch: 7.83 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043769056902937046		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.00043769056902937046 | validation: 0.0014780957209316093]
	TIME [epoch: 7.88 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001829016186034893		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.0001829016186034893 | validation: 0.003248018128837053]
	TIME [epoch: 7.84 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004833527620588525		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.0004833527620588525 | validation: 0.0002280384836786391]
	TIME [epoch: 7.84 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001815986336638795		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.0001815986336638795 | validation: 0.0005760943691352329]
	TIME [epoch: 7.85 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032233026996967797		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.00032233026996967797 | validation: 0.004474321783864028]
	TIME [epoch: 7.83 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005413169108097848		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.0005413169108097848 | validation: 0.00038071103159323985]
	TIME [epoch: 7.88 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002983698815453111		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.0002983698815453111 | validation: 0.00048795244175844313]
	TIME [epoch: 7.85 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035755525215666075		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.00035755525215666075 | validation: 0.0016879696121572048]
	TIME [epoch: 7.9 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039753735520073173		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.00039753735520073173 | validation: 0.0017056905738949038]
	TIME [epoch: 7.86 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043748075081125954		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.00043748075081125954 | validation: 0.0010453772903089434]
	TIME [epoch: 7.83 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003418452723269709		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.0003418452723269709 | validation: 0.0004368245449866999]
	TIME [epoch: 7.87 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005966805695467206		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.0005966805695467206 | validation: 0.0004453757968752985]
	TIME [epoch: 7.84 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043073610714329026		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.00043073610714329026 | validation: 0.0008545487372650733]
	TIME [epoch: 7.84 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037639154457114253		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.00037639154457114253 | validation: 0.0022166211992076636]
	TIME [epoch: 7.85 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042741310934916755		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.00042741310934916755 | validation: 0.000968991878735217]
	TIME [epoch: 7.87 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038886772714053365		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.00038886772714053365 | validation: 0.001183163389947774]
	TIME [epoch: 7.85 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004252308401384293		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.0004252308401384293 | validation: 0.002018539412157603]
	TIME [epoch: 7.85 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014649159723706485		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.00014649159723706485 | validation: 0.001264239356238229]
	TIME [epoch: 7.83 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000297116786745979		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.000297116786745979 | validation: 0.0001875830689939706]
	TIME [epoch: 7.83 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005752521983007357		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.0005752521983007357 | validation: 0.0011322375315630984]
	TIME [epoch: 7.92 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002857442091792439		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.0002857442091792439 | validation: 0.00212327481638546]
	TIME [epoch: 7.9 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002759455724832585		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.0002759455724832585 | validation: 0.005545515462480399]
	TIME [epoch: 7.86 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004599506471444954		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.0004599506471444954 | validation: 0.0007318264293282884]
	TIME [epoch: 7.83 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017488039188889035		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.00017488039188889035 | validation: 0.0010773343209880438]
	TIME [epoch: 7.84 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032671983040839493		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.00032671983040839493 | validation: 0.0019396712543744972]
	TIME [epoch: 7.87 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002731786781654915		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.0002731786781654915 | validation: 0.0005089129679191196]
	TIME [epoch: 7.85 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046444630791335473		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.00046444630791335473 | validation: 0.0011145343864011048]
	TIME [epoch: 7.85 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023546962400032535		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.00023546962400032535 | validation: 0.0026237683647913537]
	TIME [epoch: 7.83 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038398308146341437		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.00038398308146341437 | validation: 0.0012431265147179885]
	TIME [epoch: 7.85 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023948779810299014		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.00023948779810299014 | validation: 0.0007847559917300933]
	TIME [epoch: 7.88 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030959407322552715		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.00030959407322552715 | validation: 0.0016589156469808942]
	TIME [epoch: 7.84 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034428702269121423		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.00034428702269121423 | validation: 0.0023071605147183226]
	TIME [epoch: 7.84 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004056999988373673		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.0004056999988373673 | validation: 0.0019316675213962612]
	TIME [epoch: 7.85 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044219277903142444		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.00044219277903142444 | validation: 0.0005963830718006688]
	TIME [epoch: 7.87 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005303675339349538		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.0005303675339349538 | validation: 0.0011502882811355911]
	TIME [epoch: 7.87 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002565914953438186		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.0002565914953438186 | validation: 0.0009306042092232928]
	TIME [epoch: 7.85 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002444229005653267		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.0002444229005653267 | validation: 0.0015273616346178054]
	TIME [epoch: 7.87 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002331758582188528		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.0002331758582188528 | validation: 0.0009130201722146918]
	TIME [epoch: 7.87 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038216450007365354		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.00038216450007365354 | validation: 0.001964642095781067]
	TIME [epoch: 7.89 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018223140002266458		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.00018223140002266458 | validation: 0.001030122258230068]
	TIME [epoch: 7.84 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000403175411870913		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.000403175411870913 | validation: 0.0009380765991452673]
	TIME [epoch: 7.84 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020921695565968902		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.00020921695565968902 | validation: 0.0012669665347070271]
	TIME [epoch: 7.85 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003053573096830826		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.0003053573096830826 | validation: 0.000516747398500125]
	TIME [epoch: 7.83 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037687083538509515		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.00037687083538509515 | validation: 0.0017072031796696854]
	TIME [epoch: 7.89 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003351128166712605		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.0003351128166712605 | validation: 0.0010232858656192994]
	TIME [epoch: 7.84 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033631494367517337		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.00033631494367517337 | validation: 0.0025270125581695045]
	TIME [epoch: 7.83 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028772801282641593		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.00028772801282641593 | validation: 0.001461073371637978]
	TIME [epoch: 7.83 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027487171919472075		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.00027487171919472075 | validation: 0.0022252978109077413]
	TIME [epoch: 7.84 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002657152985551434		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.0002657152985551434 | validation: 0.0005184496983139946]
	TIME [epoch: 7.88 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031968488821204513		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.00031968488821204513 | validation: 0.0007226395984444372]
	TIME [epoch: 7.86 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001774341582790786		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.0001774341582790786 | validation: 0.001793092252284496]
	TIME [epoch: 7.88 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000454701473041931		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.000454701473041931 | validation: 0.0020148957442785464]
	TIME [epoch: 7.9 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040850573601629693		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.00040850573601629693 | validation: 0.0026201011161280335]
	TIME [epoch: 7.89 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034144786156029786		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.00034144786156029786 | validation: 0.0006518501125950724]
	TIME [epoch: 7.84 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002540993531604645		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.0002540993531604645 | validation: 0.002236227832807871]
	TIME [epoch: 7.84 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022947713937955115		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.00022947713937955115 | validation: 0.0016516714774637666]
	TIME [epoch: 7.84 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046672817788441653		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.00046672817788441653 | validation: 0.002335091317430521]
	TIME [epoch: 7.84 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023213432703022806		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.00023213432703022806 | validation: 0.0008259568675490129]
	TIME [epoch: 7.9 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002879962142086288		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.0002879962142086288 | validation: 0.0008616124284179776]
	TIME [epoch: 7.84 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003820085219209564		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.0003820085219209564 | validation: 0.0009588720732920785]
	TIME [epoch: 7.85 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005228084712399015		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.0005228084712399015 | validation: 0.0017198386883196923]
	TIME [epoch: 7.85 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019033914157054555		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.00019033914157054555 | validation: 0.000764380618868854]
	TIME [epoch: 7.84 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040550507479208763		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.00040550507479208763 | validation: 0.0011828327958076006]
	TIME [epoch: 7.88 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004054802506382018		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.0004054802506382018 | validation: 0.0011627766941199554]
	TIME [epoch: 7.84 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033873144758536313		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.00033873144758536313 | validation: 0.0009455283974737698]
	TIME [epoch: 7.84 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005154947409352239		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.0005154947409352239 | validation: 0.00044194141295451764]
	TIME [epoch: 7.84 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022996202025182843		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.00022996202025182843 | validation: 0.0010829385843806759]
	TIME [epoch: 7.89 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026019816554352063		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.00026019816554352063 | validation: 0.000994909340599814]
	TIME [epoch: 7.87 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002478122551758364		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.0002478122551758364 | validation: 0.000695107309874552]
	TIME [epoch: 7.85 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002792324781447544		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.0002792324781447544 | validation: 0.0022062413119108256]
	TIME [epoch: 7.85 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003300407608018339		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.0003300407608018339 | validation: 0.00039140410701504443]
	TIME [epoch: 7.85 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002127942027547418		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.0002127942027547418 | validation: 0.0013057639873539984]
	TIME [epoch: 7.91 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000287893409008021		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.000287893409008021 | validation: 0.0001776143273346591]
	TIME [epoch: 7.89 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000507685665718355		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.000507685665718355 | validation: 0.0008215444669388515]
	TIME [epoch: 7.85 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002436980452966644		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.0002436980452966644 | validation: 0.000884002078716085]
	TIME [epoch: 7.85 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018472602061331788		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.00018472602061331788 | validation: 0.0011928947960941052]
	TIME [epoch: 7.84 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002997911474885073		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.0002997911474885073 | validation: 0.0011506815139304444]
	TIME [epoch: 7.89 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003085109315228849		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.0003085109315228849 | validation: 0.0009304941679704451]
	TIME [epoch: 7.83 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022389540733495726		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.00022389540733495726 | validation: 0.0012724300801550436]
	TIME [epoch: 7.85 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032901278004867816		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.00032901278004867816 | validation: 0.0027576933325934886]
	TIME [epoch: 7.85 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047159811877431086		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.00047159811877431086 | validation: 0.0016479506292545656]
	TIME [epoch: 7.86 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003223348469848979		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.0003223348469848979 | validation: 0.0009287768868250624]
	TIME [epoch: 7.89 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031908583696410966		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.00031908583696410966 | validation: 0.001048142418760219]
	TIME [epoch: 7.84 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035482374457090687		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.00035482374457090687 | validation: 0.002104677900579629]
	TIME [epoch: 7.84 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018167592289782973		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.00018167592289782973 | validation: 0.0010950337246863383]
	TIME [epoch: 7.86 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033859628975396717		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.00033859628975396717 | validation: 0.0011669044261189833]
	TIME [epoch: 7.88 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004220174798527048		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.0004220174798527048 | validation: 0.001568363005039635]
	TIME [epoch: 7.85 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041002585252450624		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.00041002585252450624 | validation: 0.0009285560537633472]
	TIME [epoch: 7.85 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002841112177983374		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.0002841112177983374 | validation: 0.0013837692922064057]
	TIME [epoch: 7.87 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023759743474902707		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.00023759743474902707 | validation: 0.0015899410398579094]
	TIME [epoch: 7.88 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028420351471617324		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.00028420351471617324 | validation: 0.0007125197643050329]
	TIME [epoch: 7.91 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035031052003813755		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.00035031052003813755 | validation: 0.0004396692745184039]
	TIME [epoch: 7.85 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018552647112041366		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.00018552647112041366 | validation: 0.0008043672433621642]
	TIME [epoch: 7.85 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037967451520247637		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.00037967451520247637 | validation: 0.0022814649592958836]
	TIME [epoch: 7.84 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034619999356215605		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.00034619999356215605 | validation: 0.0009120770391521971]
	TIME [epoch: 7.83 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026026853260363315		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.00026026853260363315 | validation: 0.0023187607833650248]
	TIME [epoch: 7.89 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028828410364918524		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.00028828410364918524 | validation: 0.0008041525928511381]
	TIME [epoch: 7.85 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003804605817240538		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.0003804605817240538 | validation: 0.0007017487453722228]
	TIME [epoch: 7.85 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000458313224511387		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.000458313224511387 | validation: 0.0025080655756458043]
	TIME [epoch: 7.85 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005248995556690554		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.0005248995556690554 | validation: 0.001706207623733121]
	TIME [epoch: 7.86 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002609935341908265		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.0002609935341908265 | validation: 0.0007741845559378389]
	TIME [epoch: 7.88 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033486394600069745		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.00033486394600069745 | validation: 0.0011247869742582984]
	TIME [epoch: 7.85 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025300057157210353		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.00025300057157210353 | validation: 0.0006483792667610873]
	TIME [epoch: 7.86 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002449293454953545		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.0002449293454953545 | validation: 0.0011159882974988342]
	TIME [epoch: 7.86 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035550891428031203		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.00035550891428031203 | validation: 0.0017592436724106469]
	TIME [epoch: 7.88 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039139920344938935		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.00039139920344938935 | validation: 0.0010430416711754206]
	TIME [epoch: 7.87 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022465106764612842		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.00022465106764612842 | validation: 0.001225003509270466]
	TIME [epoch: 7.88 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002904162752540813		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.0002904162752540813 | validation: 0.0009307269769277884]
	TIME [epoch: 7.86 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024100580164536646		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.00024100580164536646 | validation: 0.0014015571978942616]
	TIME [epoch: 7.84 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003803251265936658		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.0003803251265936658 | validation: 0.0011221728680634734]
	TIME [epoch: 7.89 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005137931378712974		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.0005137931378712974 | validation: 0.0015657522996505133]
	TIME [epoch: 7.84 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004281171230372816		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.0004281171230372816 | validation: 0.0018522091573903035]
	TIME [epoch: 7.83 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002187304445448446		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.0002187304445448446 | validation: 0.0019082914348883921]
	TIME [epoch: 7.84 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021942680935309225		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.00021942680935309225 | validation: 0.0008064777438048906]
	TIME [epoch: 7.85 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004014638954252756		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.0004014638954252756 | validation: 0.0006714503705948288]
	TIME [epoch: 7.88 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028307784368149355		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.00028307784368149355 | validation: 0.0009403471735393173]
	TIME [epoch: 7.83 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032785697318052495		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.00032785697318052495 | validation: 0.00031922453067372117]
	TIME [epoch: 7.84 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004810536884745904		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.0004810536884745904 | validation: 0.0013194478853150963]
	TIME [epoch: 7.84 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027655870017579407		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.00027655870017579407 | validation: 0.0015930316970683034]
	TIME [epoch: 7.87 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002728063090694151		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.0002728063090694151 | validation: 0.0009016646549073134]
	TIME [epoch: 7.87 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005085770557123932		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.0005085770557123932 | validation: 0.00082903590857432]
	TIME [epoch: 7.84 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030626016088775867		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.00030626016088775867 | validation: 0.002633615839348498]
	TIME [epoch: 7.84 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002767646883928075		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.0002767646883928075 | validation: 0.0009346455917461638]
	TIME [epoch: 7.86 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002909018152138858		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.0002909018152138858 | validation: 0.0012770848940795148]
	TIME [epoch: 7.93 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003911367494298084		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.0003911367494298084 | validation: 0.0023268137686813565]
	TIME [epoch: 7.86 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022184512799697752		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.00022184512799697752 | validation: 0.0008993064167704965]
	TIME [epoch: 7.83 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045804193702896523		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.00045804193702896523 | validation: 0.0007693986544166833]
	TIME [epoch: 7.79 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002658621552165443		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.0002658621552165443 | validation: 0.000529883516943226]
	TIME [epoch: 7.79 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003201559836536116		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.0003201559836536116 | validation: 0.002105328540897367]
	TIME [epoch: 7.87 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003850783871913761		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.0003850783871913761 | validation: 0.0010738949740739052]
	TIME [epoch: 7.83 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021082623689946446		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.00021082623689946446 | validation: 0.0006530664341304178]
	TIME [epoch: 7.82 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047038255849043737		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.00047038255849043737 | validation: 0.0015493904873273888]
	TIME [epoch: 7.84 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002459118200582482		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.0002459118200582482 | validation: 0.0021294104070811023]
	TIME [epoch: 7.85 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000343696477479569		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.000343696477479569 | validation: 0.0016773476676180384]
	TIME [epoch: 7.88 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003781944137652504		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.0003781944137652504 | validation: 0.0027509861796978807]
	TIME [epoch: 7.84 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003803177136806451		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.0003803177136806451 | validation: 0.0006599007462044426]
	TIME [epoch: 7.84 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001860020523035257		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.0001860020523035257 | validation: 0.00107873804144843]
	TIME [epoch: 7.94 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001378576939320715		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.0001378576939320715 | validation: 0.0014880315250968053]
	TIME [epoch: 7.88 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022988898272699476		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.00022988898272699476 | validation: 0.0020883282735925475]
	TIME [epoch: 7.87 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034858845088336633		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.00034858845088336633 | validation: 0.0006261649988604887]
	TIME [epoch: 7.87 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004064005107180879		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.0004064005107180879 | validation: 0.001967767022808208]
	TIME [epoch: 7.85 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033724977794459066		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.00033724977794459066 | validation: 0.0003384687168956147]
	TIME [epoch: 7.84 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004015299820882083		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.0004015299820882083 | validation: 0.0005379273855350127]
	TIME [epoch: 7.88 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035533442320679297		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.00035533442320679297 | validation: 0.0028034390953905453]
	TIME [epoch: 7.84 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038299780854183466		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.00038299780854183466 | validation: 0.002896937012025925]
	TIME [epoch: 7.84 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002551846816366581		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.0002551846816366581 | validation: 0.0010180608891064873]
	TIME [epoch: 7.83 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003250590259402739		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.0003250590259402739 | validation: 0.001341789835607205]
	TIME [epoch: 7.82 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034934454231196304		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.00034934454231196304 | validation: 0.0015724546297583456]
	TIME [epoch: 7.89 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030775400427218334		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.00030775400427218334 | validation: 0.0035690034673522587]
	TIME [epoch: 7.84 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003259961081425666		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.0003259961081425666 | validation: 0.0007327715391145916]
	TIME [epoch: 7.84 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037733451360820626		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.00037733451360820626 | validation: 0.0005375941328258085]
	TIME [epoch: 7.84 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004104937184501285		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.0004104937184501285 | validation: 0.0017487471429562156]
	TIME [epoch: 7.86 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003401337329044818		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.0003401337329044818 | validation: 0.000549756540704494]
	TIME [epoch: 7.88 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034112143363636976		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.00034112143363636976 | validation: 0.002556081231610979]
	TIME [epoch: 7.84 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029005599510555877		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.00029005599510555877 | validation: 0.0004080772063318925]
	TIME [epoch: 7.84 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021630476318264605		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.00021630476318264605 | validation: 0.0019525024239452566]
	TIME [epoch: 7.83 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023130956092514698		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.00023130956092514698 | validation: 0.0017349143238755192]
	TIME [epoch: 7.91 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027839850122895023		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.00027839850122895023 | validation: 0.0010276633159823714]
	TIME [epoch: 7.87 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044030300742151356		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.00044030300742151356 | validation: 0.0010050090859289834]
	TIME [epoch: 7.89 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033048840240297616		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.00033048840240297616 | validation: 0.0008407951438448853]
	TIME [epoch: 7.83 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003772232672442357		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.0003772232672442357 | validation: 0.0008237432300702955]
	TIME [epoch: 7.84 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002759383408154663		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.0002759383408154663 | validation: 0.0025550816993599092]
	TIME [epoch: 7.88 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028554719329492963		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.00028554719329492963 | validation: 0.0014084080849181095]
	TIME [epoch: 7.82 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028027239296435076		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.00028027239296435076 | validation: 0.001637996187880402]
	TIME [epoch: 7.84 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019425145195017037		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.00019425145195017037 | validation: 0.0004974874310204465]
	TIME [epoch: 7.83 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004570522498198426		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.0004570522498198426 | validation: 0.0017926600178329944]
	TIME [epoch: 7.84 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003442639531957321		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.0003442639531957321 | validation: 0.0012296695212418458]
	TIME [epoch: 7.89 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029703737446187925		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.00029703737446187925 | validation: 0.0011046219492245174]
	TIME [epoch: 7.84 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002897256940151647		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.0002897256940151647 | validation: 0.002292943109604]
	TIME [epoch: 7.84 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003000342520417077		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.0003000342520417077 | validation: 0.0023917059413590762]
	TIME [epoch: 7.85 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045421377954096495		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.00045421377954096495 | validation: 0.002768835892147611]
	TIME [epoch: 7.87 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003525913279229478		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.0003525913279229478 | validation: 0.00028526802053252266]
	TIME [epoch: 7.86 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004232037146633965		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.0004232037146633965 | validation: 0.0005791380877593734]
	TIME [epoch: 7.82 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023718466273165806		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.00023718466273165806 | validation: 0.0011993031040289778]
	TIME [epoch: 7.82 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016843886278492735		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.00016843886278492735 | validation: 0.0016365071229397276]
	TIME [epoch: 7.83 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012915606361252219		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.00012915606361252219 | validation: 0.0008630192739989377]
	TIME [epoch: 7.9 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002571770419002486		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.0002571770419002486 | validation: 0.0019112425635545351]
	TIME [epoch: 7.9 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002052180258391576		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.0002052180258391576 | validation: 0.0007894821529569151]
	TIME [epoch: 7.84 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029009886052418324		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.00029009886052418324 | validation: 0.0009662581444663846]
	TIME [epoch: 7.85 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039067079429850124		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.00039067079429850124 | validation: 0.0013800143455676387]
	TIME [epoch: 7.83 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037566260919719575		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.00037566260919719575 | validation: 0.0015114946024804387]
	TIME [epoch: 7.88 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042449402894234665		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.00042449402894234665 | validation: 0.0015475543663392584]
	TIME [epoch: 7.84 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003135364400037468		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.0003135364400037468 | validation: 0.0007116445391125758]
	TIME [epoch: 7.84 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003211261176723541		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.0003211261176723541 | validation: 0.0015449530835019525]
	TIME [epoch: 7.83 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004403040482975518		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.0004403040482975518 | validation: 0.0002643104283048494]
	TIME [epoch: 7.84 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004927387101435769		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.0004927387101435769 | validation: 0.00038347806001729715]
	TIME [epoch: 7.87 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004222785713870645		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.0004222785713870645 | validation: 0.0009970752816509671]
	TIME [epoch: 7.84 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025654410633399775		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.00025654410633399775 | validation: 0.0018146001111880878]
	TIME [epoch: 7.84 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023082282607085158		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.00023082282607085158 | validation: 0.002113634580729445]
	TIME [epoch: 7.84 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002756984684797849		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.0002756984684797849 | validation: 0.0015642572437239571]
	TIME [epoch: 7.86 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002490361128106067		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.0002490361128106067 | validation: 0.0010006037892517261]
	TIME [epoch: 7.85 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004274351733689235		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.0004274351733689235 | validation: 0.0013191103484751052]
	TIME [epoch: 7.84 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036697904234963446		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.00036697904234963446 | validation: 0.000516786066686393]
	TIME [epoch: 7.84 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000357790048179163		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.000357790048179163 | validation: 0.0009676798149805492]
	TIME [epoch: 7.82 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003810599297830797		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.0003810599297830797 | validation: 0.00132249848073913]
	TIME [epoch: 7.87 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002755102014230586		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.0002755102014230586 | validation: 0.0012486402641392962]
	TIME [epoch: 7.84 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025046395474763173		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.00025046395474763173 | validation: 0.001243422836711738]
	TIME [epoch: 7.84 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023528347363560798		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.00023528347363560798 | validation: 0.0021563272677874866]
	TIME [epoch: 7.84 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003816274237315638		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.0003816274237315638 | validation: 0.0010379544910668992]
	TIME [epoch: 7.84 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024010267552607267		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.00024010267552607267 | validation: 0.0016353629909237367]
	TIME [epoch: 7.88 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023665733855083616		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.00023665733855083616 | validation: 0.0012785402411464588]
	TIME [epoch: 7.83 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037881004448048867		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.00037881004448048867 | validation: 0.0009622120082680486]
	TIME [epoch: 7.84 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003341797764217212		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.0003341797764217212 | validation: 0.0009983925137727523]
	TIME [epoch: 7.83 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010572551834844889		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.00010572551834844889 | validation: 0.0013478185836855907]
	TIME [epoch: 7.85 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000315578729049472		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.000315578729049472 | validation: 0.001462311551540136]
	TIME [epoch: 7.87 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003326486078628885		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.0003326486078628885 | validation: 0.0016847522131690528]
	TIME [epoch: 7.84 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031287569911950166		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.00031287569911950166 | validation: 0.000740960081219038]
	TIME [epoch: 7.83 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002890827588119269		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.0002890827588119269 | validation: 0.0015800184532461276]
	TIME [epoch: 7.84 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003470109337422604		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.0003470109337422604 | validation: 0.001262662952031473]
	TIME [epoch: 7.87 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003581013277135605		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.0003581013277135605 | validation: 0.0006512607690023033]
	TIME [epoch: 7.84 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021299491205852328		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.00021299491205852328 | validation: 0.0014325892672508189]
	TIME [epoch: 7.83 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001657855886647208		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.0001657855886647208 | validation: 0.0023710518723307557]
	TIME [epoch: 7.82 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041972954657544336		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.00041972954657544336 | validation: 0.0019670615095823034]
	TIME [epoch: 7.84 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004761612150484989		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.0004761612150484989 | validation: 0.001536926892846516]
	TIME [epoch: 7.87 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037623921938839965		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.00037623921938839965 | validation: 0.0016254889149571801]
	TIME [epoch: 7.84 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022618817637048804		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.00022618817637048804 | validation: 0.0008120804008829757]
	TIME [epoch: 7.84 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021859288744181083		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.00021859288744181083 | validation: 0.0014471446279997382]
	TIME [epoch: 7.84 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002992919409711537		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.0002992919409711537 | validation: 0.0007100656341690987]
	TIME [epoch: 7.85 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021217246596594855		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.00021217246596594855 | validation: 0.0015385383945239017]
	TIME [epoch: 7.89 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003599008949500902		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.0003599008949500902 | validation: 0.001421603222148267]
	TIME [epoch: 7.84 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003052834164605882		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.0003052834164605882 | validation: 0.0011483977666851982]
	TIME [epoch: 7.84 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028260199272167987		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.00028260199272167987 | validation: 0.0010915215916891183]
	TIME [epoch: 7.82 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004011378692244705		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.0004011378692244705 | validation: 0.0007747890706413338]
	TIME [epoch: 7.84 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027589462763507596		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.00027589462763507596 | validation: 0.00047694255898834915]
	TIME [epoch: 7.87 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040681311778410057		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.00040681311778410057 | validation: 0.0008278607383266951]
	TIME [epoch: 7.84 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043336097162385156		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.00043336097162385156 | validation: 0.0013321635237356117]
	TIME [epoch: 7.84 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002781061611025741		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.0002781061611025741 | validation: 0.0010783605507112587]
	TIME [epoch: 7.83 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022822478930496293		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.00022822478930496293 | validation: 0.0015543384459655032]
	TIME [epoch: 7.87 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016964748380414996		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.00016964748380414996 | validation: 0.0015925191274241176]
	TIME [epoch: 7.84 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000263679616051357		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.000263679616051357 | validation: 0.0007921537004107]
	TIME [epoch: 7.82 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000184130270333156		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.000184130270333156 | validation: 0.0006271904662373266]
	TIME [epoch: 7.82 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003728889757540519		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.0003728889757540519 | validation: 0.00299335683709563]
	TIME [epoch: 7.83 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023232628532063692		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.00023232628532063692 | validation: 0.0009542209286456532]
	TIME [epoch: 7.88 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012799744506579282		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.00012799744506579282 | validation: 0.003154918457513781]
	TIME [epoch: 7.84 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014792723578960354		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.00014792723578960354 | validation: 0.00043679511533941626]
	TIME [epoch: 7.82 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003801924662808593		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.0003801924662808593 | validation: 0.0006522659790777281]
	TIME [epoch: 7.83 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015012930927833646		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 0.00015012930927833646 | validation: 0.0015190471211112132]
	TIME [epoch: 7.83 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002885912581490242		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 0.0002885912581490242 | validation: 0.001277049369592101]
	TIME [epoch: 7.87 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031217574423200744		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 0.00031217574423200744 | validation: 0.000888108998921383]
	TIME [epoch: 7.82 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004134778256487885		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 0.0004134778256487885 | validation: 0.0018652294985940765]
	TIME [epoch: 7.82 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000227655836578164		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 0.000227655836578164 | validation: 0.0007340111167975208]
	TIME [epoch: 7.83 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037927419071071445		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 0.00037927419071071445 | validation: 0.0014592572198597882]
	TIME [epoch: 7.86 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030794837941895085		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 0.00030794837941895085 | validation: 0.001698284694505359]
	TIME [epoch: 7.87 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025882486879114894		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 0.00025882486879114894 | validation: 0.0011089307641975897]
	TIME [epoch: 7.83 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002345200983480431		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 0.0002345200983480431 | validation: 0.001703108237225889]
	TIME [epoch: 7.83 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002269281835953745		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.0002269281835953745 | validation: 0.0030331402191582386]
	TIME [epoch: 7.83 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018919017905327107		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 0.00018919017905327107 | validation: 0.0017915474970086854]
	TIME [epoch: 7.87 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004449198056145729		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 0.0004449198056145729 | validation: 0.0016116203515352243]
	TIME [epoch: 7.83 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022048582256468995		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 0.00022048582256468995 | validation: 0.0019241299712458213]
	TIME [epoch: 7.82 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002415347772157277		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 0.0002415347772157277 | validation: 0.0012168838790259607]
	TIME [epoch: 7.81 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003309985650608085		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 0.0003309985650608085 | validation: 0.001138378609630932]
	TIME [epoch: 7.83 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030888899611229183		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 0.00030888899611229183 | validation: 0.002316787747186708]
	TIME [epoch: 7.87 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004338340725186551		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 0.0004338340725186551 | validation: 0.001786842082817323]
	TIME [epoch: 7.83 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015520336834098325		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 0.00015520336834098325 | validation: 0.001176181818522136]
	TIME [epoch: 7.83 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032923131239753745		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 0.00032923131239753745 | validation: 0.0014094395426970906]
	TIME [epoch: 7.82 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.838223021627355e-05		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 6.838223021627355e-05 | validation: 0.0012790789877376448]
	TIME [epoch: 7.83 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002250461867754785		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 0.0002250461867754785 | validation: 0.0010749685416984764]
	TIME [epoch: 7.88 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002022275956148525		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 0.0002022275956148525 | validation: 0.0015646767419022238]
	TIME [epoch: 7.84 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010330399264686906		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 0.00010330399264686906 | validation: 0.0016790843477599983]
	TIME [epoch: 7.86 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031956593655961866		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 0.00031956593655961866 | validation: 0.0013715444763085119]
	TIME [epoch: 7.85 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004516623624724876		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 0.0004516623624724876 | validation: 0.0010307369188873032]
	TIME [epoch: 7.87 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024337014868193243		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 0.00024337014868193243 | validation: 0.0007548292444577278]
	TIME [epoch: 7.86 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023289311711626872		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 0.00023289311711626872 | validation: 0.0013212467367676466]
	TIME [epoch: 7.83 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002179016743821476		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 0.0002179016743821476 | validation: 0.0010943976260816123]
	TIME [epoch: 7.82 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036749545014597043		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 0.00036749545014597043 | validation: 0.000858952541990015]
	TIME [epoch: 7.83 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035323090132770063		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 0.00035323090132770063 | validation: 0.0014293221143579684]
	TIME [epoch: 7.87 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002977427171435536		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 0.0002977427171435536 | validation: 0.0008982295418110447]
	TIME [epoch: 7.83 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032565433977824014		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 0.00032565433977824014 | validation: 0.0006278122777265329]
	TIME [epoch: 7.83 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035778238797810235		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 0.00035778238797810235 | validation: 0.0006915982879999723]
	TIME [epoch: 7.82 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005023621827635927		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 0.0005023621827635927 | validation: 0.0015947004788477866]
	TIME [epoch: 7.82 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030908735840016387		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 0.00030908735840016387 | validation: 0.001477235076025937]
	TIME [epoch: 7.86 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024351695870512778		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 0.00024351695870512778 | validation: 0.0020371422664820747]
	TIME [epoch: 7.83 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017897873185168846		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 0.00017897873185168846 | validation: 0.0010035843572241197]
	TIME [epoch: 7.83 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004191831699514168		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 0.0004191831699514168 | validation: 0.0013988573790089998]
	TIME [epoch: 7.83 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.903723510407044e-05		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 6.903723510407044e-05 | validation: 0.001995738229977448]
	TIME [epoch: 7.85 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002561679199527014		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 0.0002561679199527014 | validation: 0.0017977845717952264]
	TIME [epoch: 7.86 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020913162300157382		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 0.00020913162300157382 | validation: 0.00121529773059122]
	TIME [epoch: 7.82 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030429983281823317		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 0.00030429983281823317 | validation: 0.0010043619190763815]
	TIME [epoch: 7.83 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000264799616240337		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 0.000264799616240337 | validation: 0.000971668892081472]
	TIME [epoch: 7.82 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012790251325727888		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 0.00012790251325727888 | validation: 0.0008504331082065599]
	TIME [epoch: 7.86 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035179321470120927		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 0.00035179321470120927 | validation: 0.0010455820052253399]
	TIME [epoch: 7.84 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025224986858225984		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 0.00025224986858225984 | validation: 0.0014658003452110125]
	TIME [epoch: 7.83 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017214255064749453		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 0.00017214255064749453 | validation: 0.0012737300892455778]
	TIME [epoch: 7.82 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022164188349936477		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 0.00022164188349936477 | validation: 0.0010389180546347952]
	TIME [epoch: 7.83 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003666027467826076		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.0003666027467826076 | validation: 0.0010874619743268372]
	TIME [epoch: 7.87 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003167927542526925		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 0.0003167927542526925 | validation: 0.001570384743580216]
	TIME [epoch: 7.84 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033424633945707807		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 0.00033424633945707807 | validation: 0.0014206711048692293]
	TIME [epoch: 7.83 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034801711634398447		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 0.00034801711634398447 | validation: 0.001010483899255709]
	TIME [epoch: 7.82 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026558191855867544		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 0.00026558191855867544 | validation: 0.001196662633600945]
	TIME [epoch: 7.82 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000441841630299584		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 0.000441841630299584 | validation: 0.0011576973461281794]
	TIME [epoch: 7.86 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016800109337134696		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 0.00016800109337134696 | validation: 0.0014849027315902113]
	TIME [epoch: 7.82 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027062720759749183		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 0.00027062720759749183 | validation: 0.0013643835792794717]
	TIME [epoch: 7.81 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024842677975931424		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 0.00024842677975931424 | validation: 0.001102554097309703]
	TIME [epoch: 7.83 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034111816038882983		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 0.00034111816038882983 | validation: 0.0006448102954428085]
	TIME [epoch: 7.84 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024819211203173745		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 0.00024819211203173745 | validation: 0.0014639075896120913]
	TIME [epoch: 7.86 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000282690496912859		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 0.000282690496912859 | validation: 0.0007756339539266577]
	TIME [epoch: 7.82 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004214731148027351		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 0.0004214731148027351 | validation: 0.0011111797948011217]
	TIME [epoch: 7.82 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025956732011007163		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 0.00025956732011007163 | validation: 0.0013557434482058851]
	TIME [epoch: 7.81 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001954229944984176		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 0.0001954229944984176 | validation: 0.0011772914492504478]
	TIME [epoch: 7.87 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002465264306527013		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 0.0002465264306527013 | validation: 0.0007179951251633937]
	TIME [epoch: 7.84 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021173355039208827		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 0.00021173355039208827 | validation: 0.001165605380752763]
	TIME [epoch: 7.84 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035300677138123395		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 0.00035300677138123395 | validation: 0.0025625054871772656]
	TIME [epoch: 7.83 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020568997665526822		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 0.00020568997665526822 | validation: 0.00046276173288077425]
	TIME [epoch: 7.83 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003389082504633865		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 0.0003389082504633865 | validation: 0.0013753378833323122]
	TIME [epoch: 7.88 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001078481122234378		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 0.0001078481122234378 | validation: 0.0017132003088939252]
	TIME [epoch: 7.82 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021721372085344327		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 0.00021721372085344327 | validation: 0.0009003452017058021]
	TIME [epoch: 7.82 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002674257216842109		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 0.0002674257216842109 | validation: 0.001723589482336876]
	TIME [epoch: 7.81 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000270981211859465		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 0.000270981211859465 | validation: 0.0011288807405587529]
	TIME [epoch: 7.81 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036796820430034675		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 0.00036796820430034675 | validation: 0.001078973779867451]
	TIME [epoch: 7.86 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028011390506334433		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 0.00028011390506334433 | validation: 0.001459939103569015]
	TIME [epoch: 7.82 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004028260780165547		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 0.0004028260780165547 | validation: 0.0018426237639829043]
	TIME [epoch: 7.82 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002581168684041053		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 0.0002581168684041053 | validation: 0.0016036450023093661]
	TIME [epoch: 7.82 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037310442584642005		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 0.00037310442584642005 | validation: 0.0014325349373804067]
	TIME [epoch: 7.84 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00048169662137151814		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 0.00048169662137151814 | validation: 0.0009243257559509424]
	TIME [epoch: 7.87 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023236601221030996		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 0.00023236601221030996 | validation: 0.0007548242158321453]
	TIME [epoch: 7.82 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011489789979883191		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 0.00011489789979883191 | validation: 0.001220311019837589]
	TIME [epoch: 7.84 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022783803975727992		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 0.00022783803975727992 | validation: 0.0011600349972646696]
	TIME [epoch: 7.83 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003183715048418694		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 0.0003183715048418694 | validation: 0.001267777675083985]
	TIME [epoch: 7.87 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003500846932459853		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 0.0003500846932459853 | validation: 0.0010953972921843845]
	TIME [epoch: 7.83 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024022025749502365		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 0.00024022025749502365 | validation: 0.001350759210889839]
	TIME [epoch: 7.84 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025935613573864046		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 0.00025935613573864046 | validation: 0.0018177614543214854]
	TIME [epoch: 7.82 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025649475248925733		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 0.00025649475248925733 | validation: 0.002341768959011568]
	TIME [epoch: 7.82 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026111636708428955		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 0.00026111636708428955 | validation: 0.0013915662042499283]
	TIME [epoch: 7.86 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003609460618521583		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.0003609460618521583 | validation: 0.0016540227164083437]
	TIME [epoch: 7.84 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004129190086519408		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 0.0004129190086519408 | validation: 0.0021323685938503525]
	TIME [epoch: 7.83 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001910387709538879		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 0.0001910387709538879 | validation: 0.0014518436149301647]
	TIME [epoch: 7.83 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024451757072045167		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 0.00024451757072045167 | validation: 0.0009903703092041072]
	TIME [epoch: 7.83 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038262629938163006		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 0.00038262629938163006 | validation: 0.0014327978205139376]
	TIME [epoch: 7.87 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037872466352116584		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 0.00037872466352116584 | validation: 0.0006583285486924284]
	TIME [epoch: 7.81 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004505839362022932		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 0.0004505839362022932 | validation: 0.0009559378736101473]
	TIME [epoch: 7.83 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003055440199189805		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 0.0003055440199189805 | validation: 0.0020572941285044947]
	TIME [epoch: 7.83 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004033206788318801		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 0.0004033206788318801 | validation: 0.0008631700647096609]
	TIME [epoch: 7.85 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023862819411669527		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 0.00023862819411669527 | validation: 0.0010004472396945695]
	TIME [epoch: 7.87 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002075597416657946		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 0.0002075597416657946 | validation: 0.0012632137920359785]
	TIME [epoch: 7.82 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004171639343277356		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 0.0004171639343277356 | validation: 0.0010186803980603658]
	TIME [epoch: 7.82 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028018134221141747		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 0.00028018134221141747 | validation: 0.0016922226818787856]
	TIME [epoch: 7.82 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001694341837520179		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 0.0001694341837520179 | validation: 0.001263489440298767]
	TIME [epoch: 7.88 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.01886671153269e-05		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 9.01886671153269e-05 | validation: 0.00151060897367399]
	TIME [epoch: 7.84 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023355082448533326		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 0.00023355082448533326 | validation: 0.0015450990045190603]
	TIME [epoch: 7.82 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003917871482928039		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 0.0003917871482928039 | validation: 0.0012326783223516706]
	TIME [epoch: 7.83 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036831751096393474		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 0.00036831751096393474 | validation: 0.0019205200052095997]
	TIME [epoch: 7.82 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003128809814474751		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 0.0003128809814474751 | validation: 0.0014008966019990385]
	TIME [epoch: 7.87 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044226156283165905		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 0.00044226156283165905 | validation: 0.0008501358856746961]
	TIME [epoch: 7.83 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003273484420418067		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 0.0003273484420418067 | validation: 0.0007770509051939615]
	TIME [epoch: 7.83 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002442143781027408		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 0.0002442143781027408 | validation: 0.0014610857634632054]
	TIME [epoch: 7.83 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003038479793818569		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 0.0003038479793818569 | validation: 0.0009299723832566693]
	TIME [epoch: 7.83 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003101881382044829		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 0.0003101881382044829 | validation: 0.0027175210979685106]
	TIME [epoch: 7.87 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001838528998118909		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 0.0001838528998118909 | validation: 0.0010145120562212871]
	TIME [epoch: 7.82 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004465139187553928		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 0.0004465139187553928 | validation: 0.0012576809660737123]
	TIME [epoch: 7.84 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004164725949749626		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 0.0004164725949749626 | validation: 0.0013059224866877627]
	TIME [epoch: 7.82 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036683463408491914		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 0.00036683463408491914 | validation: 0.0013615595131070304]
	TIME [epoch: 7.86 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032675500698423467		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 0.00032675500698423467 | validation: 0.0010656826496809471]
	TIME [epoch: 7.86 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020546457327919157		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 0.00020546457327919157 | validation: 0.0014477854522998179]
	TIME [epoch: 7.82 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028781614590541984		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 0.00028781614590541984 | validation: 0.0009955642858858402]
	TIME [epoch: 7.83 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031408295499497376		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 0.00031408295499497376 | validation: 0.0017376365874536477]
	TIME [epoch: 7.83 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036563604220517807		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 0.00036563604220517807 | validation: 0.001273401689847737]
	TIME [epoch: 7.87 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021982585771887188		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 0.00021982585771887188 | validation: 0.0013279688137890072]
	TIME [epoch: 7.83 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020273920070383288		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 0.00020273920070383288 | validation: 0.0016979761236665576]
	TIME [epoch: 7.82 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029802044675078873		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 0.00029802044675078873 | validation: 0.001222230950791637]
	TIME [epoch: 7.84 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032646195402843595		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 0.00032646195402843595 | validation: 0.0013043274703743205]
	TIME [epoch: 7.83 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012838742931634695		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 0.00012838742931634695 | validation: 0.0006999185923038853]
	TIME [epoch: 7.87 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019950771165199588		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 0.00019950771165199588 | validation: 0.0008562796646979259]
	TIME [epoch: 7.82 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038991716594438875		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.00038991716594438875 | validation: 0.002033909624702977]
	TIME [epoch: 7.81 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000275679347858808		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 0.000275679347858808 | validation: 0.001195086841121805]
	TIME [epoch: 7.82 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021467470561376944		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 0.00021467470561376944 | validation: 0.001462384844859404]
	TIME [epoch: 7.83 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026175462643662084		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 0.00026175462643662084 | validation: 0.0013321078541222349]
	TIME [epoch: 7.86 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004008308565103014		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 0.0004008308565103014 | validation: 0.0010761214072333562]
	TIME [epoch: 7.83 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034073102641435837		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 0.00034073102641435837 | validation: 0.0008782060961535106]
	TIME [epoch: 7.81 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001855647707610537		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 0.0001855647707610537 | validation: 0.0015821203890640464]
	TIME [epoch: 7.82 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029438104893537623		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 0.00029438104893537623 | validation: 0.001621259851882786]
	TIME [epoch: 7.86 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001623451477930176		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 0.0001623451477930176 | validation: 0.001558246780382359]
	TIME [epoch: 7.82 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032357981537477416		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 0.00032357981537477416 | validation: 0.0008290615485364894]
	TIME [epoch: 7.82 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029577953190227627		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 0.00029577953190227627 | validation: 0.0011866323859388633]
	TIME [epoch: 7.82 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018468011940913498		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 0.00018468011940913498 | validation: 0.0012648691755308503]
	TIME [epoch: 7.82 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002786082498806686		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 0.0002786082498806686 | validation: 0.001040190120047261]
	TIME [epoch: 7.85 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002855206185730763		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 0.0002855206185730763 | validation: 0.001520991297299348]
	TIME [epoch: 7.82 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002110175301457611		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 0.0002110175301457611 | validation: 0.001414124511027035]
	TIME [epoch: 7.82 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019593063465564643		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 0.00019593063465564643 | validation: 0.0009410793335614178]
	TIME [epoch: 7.82 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018067868560787392		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 0.00018067868560787392 | validation: 0.0006719982752262119]
	TIME [epoch: 7.82 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002070609447675589		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 0.0002070609447675589 | validation: 0.0019560473591285]
	TIME [epoch: 7.85 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003717694024557101		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 0.0003717694024557101 | validation: 0.0011641461686180072]
	TIME [epoch: 7.83 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001200203901337733		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 0.0001200203901337733 | validation: 0.001182666671553058]
	TIME [epoch: 7.81 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024627572442731415		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 0.00024627572442731415 | validation: 0.0013877559312370914]
	TIME [epoch: 7.83 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020884769232346544		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 0.00020884769232346544 | validation: 0.0008740272068773454]
	TIME [epoch: 7.84 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037293969372991875		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 0.00037293969372991875 | validation: 0.001217367035864597]
	TIME [epoch: 7.86 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000361260054720699		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 0.000361260054720699 | validation: 0.0020035196755830544]
	TIME [epoch: 7.83 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001824248646391762		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 0.0001824248646391762 | validation: 0.0020048060826996805]
	TIME [epoch: 7.81 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029871490036042906		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 0.00029871490036042906 | validation: 0.0013552137430123666]
	TIME [epoch: 7.81 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037717902155664726		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 0.00037717902155664726 | validation: 0.001310755637831936]
	TIME [epoch: 7.85 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005691647002204212		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 0.0005691647002204212 | validation: 0.001675120971041527]
	TIME [epoch: 7.82 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024439672566715177		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 0.00024439672566715177 | validation: 0.001559210199251374]
	TIME [epoch: 7.81 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002451279558277169		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 0.0002451279558277169 | validation: 0.0009009723900680697]
	TIME [epoch: 7.83 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024307359274368092		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 0.00024307359274368092 | validation: 0.0013395953582010253]
	TIME [epoch: 7.81 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033746468335610124		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 0.00033746468335610124 | validation: 0.001773578523653356]
	TIME [epoch: 7.88 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002410288700903578		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 0.0002410288700903578 | validation: 0.0012818070242011448]
	TIME [epoch: 7.82 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003251231612315526		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 0.0003251231612315526 | validation: 0.001783097942630671]
	TIME [epoch: 7.83 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002552531866215522		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 0.0002552531866215522 | validation: 0.0017013953484299417]
	TIME [epoch: 7.83 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002992188851420294		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 0.0002992188851420294 | validation: 0.002826043458346607]
	TIME [epoch: 7.83 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001810384481245997		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 0.0001810384481245997 | validation: 0.0007909416193975299]
	TIME [epoch: 7.87 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001545414670042129		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 0.0001545414670042129 | validation: 0.0019235899594849215]
	TIME [epoch: 7.83 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021471290547932643		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 0.00021471290547932643 | validation: 0.0011136097566272055]
	TIME [epoch: 7.83 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016937506882384957		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.00016937506882384957 | validation: 0.0012337857430603468]
	TIME [epoch: 7.81 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022682524925793612		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 0.00022682524925793612 | validation: 0.0010146913991414099]
	TIME [epoch: 7.83 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003949898124863591		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 0.0003949898124863591 | validation: 0.0015279271836528964]
	TIME [epoch: 7.86 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002835628685736862		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 0.0002835628685736862 | validation: 0.0025236185619387213]
	TIME [epoch: 7.81 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013365313417540106		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 0.00013365313417540106 | validation: 0.0009875602981799353]
	TIME [epoch: 7.83 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034716922556955146		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 0.00034716922556955146 | validation: 0.0013849775305056378]
	TIME [epoch: 7.83 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014072735208106523		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 0.00014072735208106523 | validation: 0.00254586640153871]
	TIME [epoch: 7.87 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002511790686494633		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 0.0002511790686494633 | validation: 0.0019156285174041186]
	TIME [epoch: 7.8 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024371644494306313		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 0.00024371644494306313 | validation: 0.0014234807837406533]
	TIME [epoch: 7.82 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003739770183218516		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 0.0003739770183218516 | validation: 0.0010339597161027482]
	TIME [epoch: 7.81 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002086963514214806		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 0.0002086963514214806 | validation: 0.0010898911873157555]
	TIME [epoch: 7.82 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.307860658437783e-05		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 8.307860658437783e-05 | validation: 0.0009564660031015774]
	TIME [epoch: 7.85 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001438979038465802		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 0.0001438979038465802 | validation: 0.0011163261258854771]
	TIME [epoch: 7.83 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021749527525347426		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 0.00021749527525347426 | validation: 0.0015625548095866799]
	TIME [epoch: 7.83 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027069430915729534		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 0.00027069430915729534 | validation: 0.001168713414891175]
	TIME [epoch: 7.82 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023665250534424877		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 0.00023665250534424877 | validation: 0.0011388577002606449]
	TIME [epoch: 7.83 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017359815475683773		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 0.00017359815475683773 | validation: 0.0014595113834349736]
	TIME [epoch: 7.87 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.868707661295857e-05		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 4.868707661295857e-05 | validation: 0.0013035944233703908]
	TIME [epoch: 7.81 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003607683099168051		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 0.0003607683099168051 | validation: 0.0012875428322682689]
	TIME [epoch: 7.82 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034070693476792726		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 0.00034070693476792726 | validation: 0.001984255474913029]
	TIME [epoch: 7.83 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013359853019057844		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 0.00013359853019057844 | validation: 0.0014597076835987269]
	TIME [epoch: 7.82 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002772963443979366		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 0.0002772963443979366 | validation: 0.000851930325646605]
	TIME [epoch: 7.86 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002134858282767289		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 0.0002134858282767289 | validation: 0.0017348664600653168]
	TIME [epoch: 7.83 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026111524532363583		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 0.00026111524532363583 | validation: 0.0012271086635962946]
	TIME [epoch: 7.8 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020963272192163653		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 0.00020963272192163653 | validation: 0.0016378960794655299]
	TIME [epoch: 7.82 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040767822366157946		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 0.00040767822366157946 | validation: 0.0006673528759777465]
	TIME [epoch: 7.87 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031055458830528935		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 0.00031055458830528935 | validation: 0.001590261958223275]
	TIME [epoch: 7.83 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043559417771492907		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 0.00043559417771492907 | validation: 0.001569615428188927]
	TIME [epoch: 7.81 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019744990428983744		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 0.00019744990428983744 | validation: 0.001205360609914649]
	TIME [epoch: 7.82 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012468959296113558		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 0.00012468959296113558 | validation: 0.0010392794884419693]
	TIME [epoch: 7.83 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002911343923662291		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 0.0002911343923662291 | validation: 0.000884809271580565]
	TIME [epoch: 7.86 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003385370965455403		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 0.0003385370965455403 | validation: 0.0014374625979077572]
	TIME [epoch: 7.81 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001771755356997713		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 0.0001771755356997713 | validation: 0.002090087772420107]
	TIME [epoch: 7.84 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005322131813974229		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 0.0005322131813974229 | validation: 0.001305731925053446]
	TIME [epoch: 7.84 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002133466649038367		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 0.0002133466649038367 | validation: 0.0023865772331439486]
	TIME [epoch: 7.83 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025171625352085326		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 0.00025171625352085326 | validation: 0.0015351848865767554]
	TIME [epoch: 7.88 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028083759814982127		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 0.00028083759814982127 | validation: 0.0008478374267214769]
	TIME [epoch: 7.85 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021513882932079053		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 0.00021513882932079053 | validation: 0.0014956356418184358]
	TIME [epoch: 7.84 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038674313569642994		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 0.00038674313569642994 | validation: 0.000960275312050623]
	TIME [epoch: 7.84 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019319860976926262		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 0.00019319860976926262 | validation: 0.001452535920133082]
	TIME [epoch: 7.88 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029731674691771826		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 0.00029731674691771826 | validation: 0.0013308578678408605]
	TIME [epoch: 7.87 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010578878844928275		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 0.00010578878844928275 | validation: 0.0011342804423083326]
	TIME [epoch: 7.84 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020008389853339104		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 0.00020008389853339104 | validation: 0.0018089490520337569]
	TIME [epoch: 7.82 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021054434673689238		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 0.00021054434673689238 | validation: 0.0014381451216424993]
	TIME [epoch: 7.83 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023183324444440713		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 0.00023183324444440713 | validation: 0.0016783501308203811]
	TIME [epoch: 7.86 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003176951291398493		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 0.0003176951291398493 | validation: 0.0009435697120108682]
	TIME [epoch: 7.84 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000211443953698355		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 0.000211443953698355 | validation: 0.0011871276057639664]
	TIME [epoch: 7.85 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022744413346870737		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 0.00022744413346870737 | validation: 0.0017525914921685626]
	TIME [epoch: 7.81 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035869386199304665		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 0.00035869386199304665 | validation: 0.0029637935247973893]
	TIME [epoch: 7.84 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027011887269461646		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 0.00027011887269461646 | validation: 0.0018338897894668786]
	TIME [epoch: 7.89 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000419937327072623		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 0.000419937327072623 | validation: 0.0019836482235291424]
	TIME [epoch: 7.83 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002675916517651711		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 0.0002675916517651711 | validation: 0.0008280153754687562]
	TIME [epoch: 7.85 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003484197981242576		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 0.0003484197981242576 | validation: 0.0011818749660276824]
	TIME [epoch: 7.8 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000203480881001493		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 0.000203480881001493 | validation: 0.0010197087195465758]
	TIME [epoch: 7.81 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003210977893752023		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 0.0003210977893752023 | validation: 0.0019569743982470736]
	TIME [epoch: 7.85 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003061908469208143		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 0.0003061908469208143 | validation: 0.0008980667221652502]
	TIME [epoch: 7.83 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017827330076701254		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 0.00017827330076701254 | validation: 0.0015276552058272632]
	TIME [epoch: 7.83 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023330218675242986		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 0.00023330218675242986 | validation: 0.0011424578149813848]
	TIME [epoch: 7.85 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001810317735423126		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 0.0001810317735423126 | validation: 0.0018714791513338449]
	TIME [epoch: 7.85 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019515721825285825		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 0.00019515721825285825 | validation: 0.000862731268798358]
	TIME [epoch: 7.82 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004226565698052913		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 0.0004226565698052913 | validation: 0.0012736228379823702]
	TIME [epoch: 7.77 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021355942411926243		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 0.00021355942411926243 | validation: 0.0015196680434101193]
	TIME [epoch: 7.81 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003217606541850797		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 0.0003217606541850797 | validation: 0.0016356848663969786]
	TIME [epoch: 7.82 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021433660404866184		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 0.00021433660404866184 | validation: 0.000974144730935377]
	TIME [epoch: 7.86 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002672323993874348		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 0.0002672323993874348 | validation: 0.0017543262025478006]
	TIME [epoch: 7.82 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003212037096086564		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 0.0003212037096086564 | validation: 0.0011437446026909327]
	TIME [epoch: 7.84 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002595934006362961		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 0.0002595934006362961 | validation: 0.0014535810349138993]
	TIME [epoch: 7.82 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021749292503367657		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 0.00021749292503367657 | validation: 0.0009122392639939241]
	TIME [epoch: 7.84 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016882227558539053		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 0.00016882227558539053 | validation: 0.001778873247577068]
	TIME [epoch: 7.88 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020049574377810765		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 0.00020049574377810765 | validation: 0.0017435725081555358]
	TIME [epoch: 7.84 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028857970230223207		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 0.00028857970230223207 | validation: 0.0023513518959286193]
	TIME [epoch: 7.85 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003722896698086435		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 0.0003722896698086435 | validation: 0.0016200255871668717]
	TIME [epoch: 7.83 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005025205602684961		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 0.0005025205602684961 | validation: 0.001215630719126307]
	TIME [epoch: 7.85 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034126901767729545		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 0.00034126901767729545 | validation: 0.0012507301185068762]
	TIME [epoch: 7.88 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003731663042317608		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 0.0003731663042317608 | validation: 0.0015696638952187377]
	TIME [epoch: 7.83 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003292024099165678		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 0.0003292024099165678 | validation: 0.0012311021563693432]
	TIME [epoch: 7.84 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026556227053057066		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 0.00026556227053057066 | validation: 0.0019630093941872353]
	TIME [epoch: 7.83 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027554066291435043		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 0.00027554066291435043 | validation: 0.001663030952874224]
	TIME [epoch: 7.88 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015609535185488066		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.00015609535185488066 | validation: 0.0008496202554206995]
	TIME [epoch: 7.83 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019469857027411174		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 0.00019469857027411174 | validation: 0.0012762170939480955]
	TIME [epoch: 7.84 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033761515511029243		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 0.00033761515511029243 | validation: 0.0014993518442424263]
	TIME [epoch: 7.82 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032206087754334717		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 0.00032206087754334717 | validation: 0.0009185354871944157]
	TIME [epoch: 7.85 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034236478135097405		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 0.00034236478135097405 | validation: 0.0021840748105105075]
	TIME [epoch: 7.87 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001001669812958992		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 0.0001001669812958992 | validation: 0.0015603196761629673]
	TIME [epoch: 7.85 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004418911897086886		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 0.0004418911897086886 | validation: 0.001179681119957582]
	TIME [epoch: 7.83 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003429361501837522		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 0.0003429361501837522 | validation: 0.0013020370697128375]
	TIME [epoch: 7.83 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002032471508122549		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 0.0002032471508122549 | validation: 0.0014194448437738192]
	TIME [epoch: 7.85 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025076246754996513		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 0.00025076246754996513 | validation: 0.0014028730451828846]
	TIME [epoch: 7.89 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016596726241639438		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 0.00016596726241639438 | validation: 0.0012871150397746574]
	TIME [epoch: 7.84 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026558383242645986		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 0.00026558383242645986 | validation: 0.0013834461940253925]
	TIME [epoch: 7.84 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031681092708488423		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 0.00031681092708488423 | validation: 0.0017074901902339754]
	TIME [epoch: 7.85 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003441891007195086		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 0.0003441891007195086 | validation: 0.0010141648337806427]
	TIME [epoch: 7.86 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003941263088274059		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 0.0003941263088274059 | validation: 0.001222089180634157]
	TIME [epoch: 7.87 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003739922115585826		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 0.0003739922115585826 | validation: 0.0019156008285584037]
	TIME [epoch: 7.84 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026489318234353034		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 0.00026489318234353034 | validation: 0.0012171179234535074]
	TIME [epoch: 7.85 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002333676453452127		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 0.0002333676453452127 | validation: 0.0007282377366634698]
	TIME [epoch: 7.84 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029489173332916717		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 0.00029489173332916717 | validation: 0.0004982051155012997]
	TIME [epoch: 7.9 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00042120018602430115		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 0.00042120018602430115 | validation: 0.0009420639650929159]
	TIME [epoch: 7.85 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020444138211978149		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 0.00020444138211978149 | validation: 0.0010548514238727744]
	TIME [epoch: 7.84 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024424771440384105		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 0.00024424771440384105 | validation: 0.0017147716991572422]
	TIME [epoch: 7.83 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037543896440840905		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 0.00037543896440840905 | validation: 0.0014961182636057738]
	TIME [epoch: 7.85 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013682537211479165		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 0.00013682537211479165 | validation: 0.0016078978408395708]
	TIME [epoch: 7.9 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021333507319808986		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 0.00021333507319808986 | validation: 0.0011880743109178474]
	TIME [epoch: 7.84 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022455944664928038		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 0.00022455944664928038 | validation: 0.0024347416555283398]
	TIME [epoch: 7.85 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002839919640472941		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 0.0002839919640472941 | validation: 0.0007261346958775628]
	TIME [epoch: 7.84 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.869167100810311e-05		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 3.869167100810311e-05 | validation: 0.001040044206491488]
	TIME [epoch: 7.87 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023251264289845408		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 0.00023251264289845408 | validation: 0.0011876219836773041]
	TIME [epoch: 7.88 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002070257538303317		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 0.0002070257538303317 | validation: 0.0014852067174578503]
	TIME [epoch: 7.84 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028205120531304994		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 0.00028205120531304994 | validation: 0.001004814312305178]
	TIME [epoch: 7.85 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004045885520314751		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 0.0004045885520314751 | validation: 0.0013936590183352298]
	TIME [epoch: 7.84 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013958217631008106		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 0.00013958217631008106 | validation: 0.0015830267909203305]
	TIME [epoch: 7.88 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022620202997800454		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 0.00022620202997800454 | validation: 0.001428215478828311]
	TIME [epoch: 7.87 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.483615823642184e-05		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 4.483615823642184e-05 | validation: 0.001830045251355938]
	TIME [epoch: 7.84 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033550227780524127		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 0.00033550227780524127 | validation: 0.0010981424732061482]
	TIME [epoch: 7.84 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003294855726635364		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 0.0003294855726635364 | validation: 0.0009361112981516016]
	TIME [epoch: 7.85 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029723338394165613		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 0.00029723338394165613 | validation: 0.00082262947799713]
	TIME [epoch: 7.88 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016411332682864877		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 0.00016411332682864877 | validation: 0.0018869061999462985]
	TIME [epoch: 7.85 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013985913805398664		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 0.00013985913805398664 | validation: 0.0012761470025403688]
	TIME [epoch: 7.85 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012418316540305853		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 0.00012418316540305853 | validation: 0.0020245862023416405]
	TIME [epoch: 7.84 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003368733964547719		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 0.0003368733964547719 | validation: 0.0007536797285145901]
	TIME [epoch: 7.85 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010062323486739057		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 0.00010062323486739057 | validation: 0.0016466330964997784]
	TIME [epoch: 7.9 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028246097992134026		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 0.00028246097992134026 | validation: 0.00148788604819499]
	TIME [epoch: 7.85 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039458840320900945		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 0.00039458840320900945 | validation: 0.0016661872111831235]
	TIME [epoch: 7.84 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027333882236942704		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 0.00027333882236942704 | validation: 0.0013209572403946553]
	TIME [epoch: 7.83 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018338451377770395		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 0.00018338451377770395 | validation: 0.0017300222770991695]
	TIME [epoch: 7.86 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001302860975265765		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 0.0001302860975265765 | validation: 0.0012901967421760504]
	TIME [epoch: 7.87 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031505145170497806		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 0.00031505145170497806 | validation: 0.000757403245954622]
	TIME [epoch: 7.84 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020640906024396366		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 0.00020640906024396366 | validation: 0.0011770252626184786]
	TIME [epoch: 7.84 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022455729734712106		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 0.00022455729734712106 | validation: 0.0016812475290450522]
	TIME [epoch: 7.84 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025528372587375637		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 0.00025528372587375637 | validation: 0.0013121668620057348]
	TIME [epoch: 7.88 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023750051060810674		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 0.00023750051060810674 | validation: 0.0017120888879523415]
	TIME [epoch: 7.84 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043837159909555876		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 0.00043837159909555876 | validation: 0.00186275609416923]
	TIME [epoch: 7.85 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003952084919392857		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 0.0003952084919392857 | validation: 0.0015898738733769076]
	TIME [epoch: 7.84 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003264457595663361		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 0.0003264457595663361 | validation: 0.001353269995231633]
	TIME [epoch: 7.84 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011049604990030937		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 0.00011049604990030937 | validation: 0.0014723378008302355]
	TIME [epoch: 7.89 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004316557436644557		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 0.0004316557436644557 | validation: 0.0014290076033703202]
	TIME [epoch: 7.84 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002574795147961457		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 0.0002574795147961457 | validation: 0.0011578228287131473]
	TIME [epoch: 7.85 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000410690891965241		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 0.000410690891965241 | validation: 0.0013189711398711372]
	TIME [epoch: 7.85 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019969568064298417		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 0.00019969568064298417 | validation: 0.001170202587229956]
	TIME [epoch: 7.84 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002274252324759338		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 0.0002274252324759338 | validation: 0.001302748752723603]
	TIME [epoch: 7.89 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036298565707093707		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 0.00036298565707093707 | validation: 0.002949829051330959]
	TIME [epoch: 7.84 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034177960848405834		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 0.00034177960848405834 | validation: 0.00043420501002462686]
	TIME [epoch: 7.84 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002409900079942846		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 0.0002409900079942846 | validation: 0.0009995690685527824]
	TIME [epoch: 7.84 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002654896597063918		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 0.0002654896597063918 | validation: 0.0024484150122377245]
	TIME [epoch: 7.88 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037227478766472475		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 0.00037227478766472475 | validation: 0.0009527482359824369]
	TIME [epoch: 7.86 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025348958184491586		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 0.00025348958184491586 | validation: 0.0013790127424774656]
	TIME [epoch: 7.82 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002986694926314077		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 0.0002986694926314077 | validation: 0.001847078552958699]
	TIME [epoch: 7.84 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003107712477683003		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 0.0003107712477683003 | validation: 0.0012789243648279234]
	TIME [epoch: 7.82 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018908890950521283		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 0.00018908890950521283 | validation: 0.0014458807390934647]
	TIME [epoch: 7.84 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001767576086155569		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 0.0001767576086155569 | validation: 0.0017544894124360084]
	TIME [epoch: 7.83 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012989477623955682		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 0.00012989477623955682 | validation: 0.0012728448528518338]
	TIME [epoch: 7.82 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025081012779217996		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 0.00025081012779217996 | validation: 0.0017748484338877928]
	TIME [epoch: 7.8 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002741621716678431		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 0.0002741621716678431 | validation: 0.001639671259630446]
	TIME [epoch: 7.8 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002589972542846506		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 0.0002589972542846506 | validation: 0.0014768425805345375]
	TIME [epoch: 7.86 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017171210115918403		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 0.00017171210115918403 | validation: 0.0009851984167227289]
	TIME [epoch: 7.81 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025288278195814543		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 0.00025288278195814543 | validation: 0.0012309000546065533]
	TIME [epoch: 7.79 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003121466807672682		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 0.0003121466807672682 | validation: 0.0013949554722140086]
	TIME [epoch: 7.77 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020987256447708112		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 0.00020987256447708112 | validation: 0.0009538232911683498]
	TIME [epoch: 7.79 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002463219109099859		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 0.0002463219109099859 | validation: 0.0009477588025261641]
	TIME [epoch: 7.87 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002021860077064772		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 0.0002021860077064772 | validation: 0.0007306815266118027]
	TIME [epoch: 7.81 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003240749612988237		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 0.0003240749612988237 | validation: 0.0017545471122446133]
	TIME [epoch: 7.79 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003008891129985527		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 0.0003008891129985527 | validation: 0.001179479063331759]
	TIME [epoch: 7.8 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019241017566415142		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 0.00019241017566415142 | validation: 0.0015052758271551186]
	TIME [epoch: 7.83 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003800941887267599		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 0.0003800941887267599 | validation: 0.0011811925932822067]
	TIME [epoch: 7.84 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018561893537256168		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 0.00018561893537256168 | validation: 0.001974832289309214]
	TIME [epoch: 7.81 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019544199976329236		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 0.00019544199976329236 | validation: 0.0010588016758469542]
	TIME [epoch: 7.79 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002142906814194241		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 0.0002142906814194241 | validation: 0.002659611769942826]
	TIME [epoch: 7.78 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030582004388692344		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 0.00030582004388692344 | validation: 0.0010657509873122973]
	TIME [epoch: 7.83 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003512392636932422		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 0.0003512392636932422 | validation: 0.0008358358760948442]
	TIME [epoch: 7.79 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001867813646723513		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 0.0001867813646723513 | validation: 0.0013813410180043427]
	TIME [epoch: 7.8 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022533366075868624		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 0.00022533366075868624 | validation: 0.0015657763632511879]
	TIME [epoch: 7.8 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022951224587501983		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 0.00022951224587501983 | validation: 0.001358419974446762]
	TIME [epoch: 7.79 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003005034694263124		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 0.0003005034694263124 | validation: 0.0009288041654755864]
	TIME [epoch: 7.83 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040055361738596207		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 0.00040055361738596207 | validation: 0.001739106462667282]
	TIME [epoch: 7.79 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019223508276987534		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 0.00019223508276987534 | validation: 0.0011619266691171477]
	TIME [epoch: 7.79 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034217255512341627		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 0.00034217255512341627 | validation: 0.001020998342007548]
	TIME [epoch: 7.8 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002752712418200609		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 0.0002752712418200609 | validation: 0.0017519410876228952]
	TIME [epoch: 7.8 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022526640192972592		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 0.00022526640192972592 | validation: 0.0012113798837257957]
	TIME [epoch: 7.83 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002258617100551532		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 0.0002258617100551532 | validation: 0.0016082641838694274]
	TIME [epoch: 7.79 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011066598532894666		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 0.00011066598532894666 | validation: 0.0011340787321623873]
	TIME [epoch: 7.79 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003094538154255031		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 0.0003094538154255031 | validation: 0.0016237138323132855]
	TIME [epoch: 7.8 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030893623613922986		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 0.00030893623613922986 | validation: 0.0011891032305322371]
	TIME [epoch: 7.83 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037074210457179444		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 0.00037074210457179444 | validation: 0.0011583397966228742]
	TIME [epoch: 7.79 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002017593010345453		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 0.0002017593010345453 | validation: 0.0011884407885006541]
	TIME [epoch: 7.78 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003320838161095998		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 0.0003320838161095998 | validation: 0.0012618301357555995]
	TIME [epoch: 7.79 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018352228416023197		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 0.00018352228416023197 | validation: 0.0011052686884039993]
	TIME [epoch: 7.8 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002507827369547857		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 0.0002507827369547857 | validation: 0.0011448664894858442]
	TIME [epoch: 7.84 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.041583412760176e-05		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 6.041583412760176e-05 | validation: 0.0014077070649416443]
	TIME [epoch: 7.83 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002779865346482095		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 0.0002779865346482095 | validation: 0.0016346047853900325]
	TIME [epoch: 7.83 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019457651860067715		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 0.00019457651860067715 | validation: 0.0013934911216287845]
	TIME [epoch: 7.83 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026834075897216224		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 0.00026834075897216224 | validation: 0.001182030079212514]
	TIME [epoch: 7.83 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033960652498985324		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 0.00033960652498985324 | validation: 0.001158550651833881]
	TIME [epoch: 7.88 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027654432852905605		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 0.00027654432852905605 | validation: 0.0017605735642380084]
	TIME [epoch: 7.83 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001596666967122755		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 0.0001596666967122755 | validation: 0.0018091946731005172]
	TIME [epoch: 7.85 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002229302849611585		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 0.0002229302849611585 | validation: 0.001093428919586258]
	TIME [epoch: 7.84 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024964459311989454		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 0.00024964459311989454 | validation: 0.0012936322604710994]
	TIME [epoch: 7.84 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002244990535733449		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 0.0002244990535733449 | validation: 0.001886186439947755]
	TIME [epoch: 7.87 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000260722252415275		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 0.000260722252415275 | validation: 0.001110018380530172]
	TIME [epoch: 7.83 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018163432736398132		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 0.00018163432736398132 | validation: 0.0019466564879768687]
	TIME [epoch: 7.83 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022088931087228803		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 0.00022088931087228803 | validation: 0.001277870122334737]
	TIME [epoch: 7.83 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001500247085509061		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 0.0001500247085509061 | validation: 0.001660275231352963]
	TIME [epoch: 7.87 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024614554788372956		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 0.00024614554788372956 | validation: 0.0014339104088122792]
	TIME [epoch: 7.84 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003208873257198224		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 0.0003208873257198224 | validation: 0.0013054100579398178]
	TIME [epoch: 7.83 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023119870561979082		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 0.00023119870561979082 | validation: 0.0012491321905715705]
	TIME [epoch: 7.83 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001280477412333192		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 0.0001280477412333192 | validation: 0.001272424283051942]
	TIME [epoch: 7.83 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016995465956485135		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 0.00016995465956485135 | validation: 0.0016331641044723987]
	TIME [epoch: 7.89 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001402580712820276		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 0.0001402580712820276 | validation: 0.0013311145826538215]
	TIME [epoch: 7.83 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000258270508160944		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 0.000258270508160944 | validation: 0.0014308333943375447]
	TIME [epoch: 7.84 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002175820661924268		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 0.0002175820661924268 | validation: 0.001439686149850788]
	TIME [epoch: 7.83 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015367249160093887		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 0.00015367249160093887 | validation: 0.0012780971695672747]
	TIME [epoch: 7.84 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024161253433627894		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 0.00024161253433627894 | validation: 0.0013584166620557979]
	TIME [epoch: 7.87 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003046336544332782		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 0.0003046336544332782 | validation: 0.0012171994317458179]
	TIME [epoch: 7.83 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003692681803699183		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 0.0003692681803699183 | validation: 0.0011438082526822485]
	TIME [epoch: 7.82 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027446467348498515		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 0.00027446467348498515 | validation: 0.0010848264682776644]
	TIME [epoch: 7.84 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013746413975605478		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 0.00013746413975605478 | validation: 0.0017047778922582228]
	TIME [epoch: 7.84 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020083276965831008		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 0.00020083276965831008 | validation: 0.0013768890856779175]
	TIME [epoch: 7.86 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041589203645980995		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 0.00041589203645980995 | validation: 0.001275081382602628]
	TIME [epoch: 7.82 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002336117114889489		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 0.0002336117114889489 | validation: 0.0017215214877190812]
	TIME [epoch: 7.83 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003292238384123796		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 0.0003292238384123796 | validation: 0.001183672219481668]
	TIME [epoch: 7.82 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003044997923932304		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 0.0003044997923932304 | validation: 0.0010698244446436975]
	TIME [epoch: 7.87 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002682209755782041		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 0.0002682209755782041 | validation: 0.0012132130294452948]
	TIME [epoch: 7.84 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.252870453792348e-05		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 6.252870453792348e-05 | validation: 0.0009096420506782116]
	TIME [epoch: 7.82 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036596039704128703		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 0.00036596039704128703 | validation: 0.0011448715447099635]
	TIME [epoch: 7.84 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003667053650200909		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 0.0003667053650200909 | validation: 0.0010025994329007836]
	TIME [epoch: 7.84 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022311917052477925		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 0.00022311917052477925 | validation: 0.0016422700641972082]
	TIME [epoch: 7.87 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030017462570050866		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 0.00030017462570050866 | validation: 0.0015194144587809104]
	TIME [epoch: 7.83 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035957799339342153		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 0.00035957799339342153 | validation: 0.0013945595835885226]
	TIME [epoch: 7.83 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020766424783272752		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 0.00020766424783272752 | validation: 0.0012889480584844498]
	TIME [epoch: 7.84 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017693990220385224		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 0.00017693990220385224 | validation: 0.0016453283022918584]
	TIME [epoch: 7.85 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022050976229551325		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 0.00022050976229551325 | validation: 0.0018374353283974704]
	TIME [epoch: 7.88 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003644587395892951		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 0.0003644587395892951 | validation: 0.0015290110305782774]
	TIME [epoch: 7.84 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002462265173453353		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 0.0002462265173453353 | validation: 0.0018891764403648138]
	TIME [epoch: 7.83 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039617045072726497		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 0.00039617045072726497 | validation: 0.0016797382922728056]
	TIME [epoch: 7.82 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013383358466381522		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 0.00013383358466381522 | validation: 0.0015776957942681652]
	TIME [epoch: 7.86 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000314510415575781		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 0.000314510415575781 | validation: 0.0019291574200080426]
	TIME [epoch: 7.86 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016061230003569627		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 0.00016061230003569627 | validation: 0.001626508503559486]
	TIME [epoch: 7.83 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002516195435838953		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 0.0002516195435838953 | validation: 0.001381869241696397]
	TIME [epoch: 7.83 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022866876987362005		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 0.00022866876987362005 | validation: 0.0015350580271838582]
	TIME [epoch: 7.84 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002002238014316715		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 0.0002002238014316715 | validation: 0.001520723651104671]
	TIME [epoch: 7.87 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036792935414667815		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 0.00036792935414667815 | validation: 0.0012871110055143796]
	TIME [epoch: 7.83 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003288318930047813		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 0.0003288318930047813 | validation: 0.0014281081070110222]
	TIME [epoch: 7.83 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002900818775022071		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 0.0002900818775022071 | validation: 0.0014585159778292827]
	TIME [epoch: 7.82 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032982699668005356		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 0.00032982699668005356 | validation: 0.00196061340719233]
	TIME [epoch: 7.83 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002912989838986442		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 0.0002912989838986442 | validation: 0.00128131208500903]
	TIME [epoch: 7.87 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024068141102616568		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 0.00024068141102616568 | validation: 0.0014645891542144122]
	TIME [epoch: 7.82 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002587639106947053		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 0.0002587639106947053 | validation: 0.0013646700493087618]
	TIME [epoch: 7.82 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00012102120614360711		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 0.00012102120614360711 | validation: 0.001535445965773106]
	TIME [epoch: 7.82 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026179807440821623		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 0.00026179807440821623 | validation: 0.001478535582366357]
	TIME [epoch: 7.84 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022698136034973328		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 0.00022698136034973328 | validation: 0.0010656590606247097]
	TIME [epoch: 7.87 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003227074389982489		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 0.0003227074389982489 | validation: 0.00127322881084565]
	TIME [epoch: 7.84 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000303815158797609		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 0.000303815158797609 | validation: 0.0017309398766652074]
	TIME [epoch: 7.83 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010530737074370422		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 0.00010530737074370422 | validation: 0.0013418152708590317]
	TIME [epoch: 7.83 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022906707273442705		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 0.00022906707273442705 | validation: 0.0013958993731844115]
	TIME [epoch: 7.87 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011935218384168689		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 0.00011935218384168689 | validation: 0.0013185193215690828]
	TIME [epoch: 7.85 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019340236903130027		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 0.00019340236903130027 | validation: 0.0012405360081392347]
	TIME [epoch: 7.83 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028585297867835213		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 0.00028585297867835213 | validation: 0.0018829645771382185]
	TIME [epoch: 7.82 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041166620423567404		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 0.00041166620423567404 | validation: 0.0017378730663823376]
	TIME [epoch: 7.83 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002448644719348947		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 0.0002448644719348947 | validation: 0.0012884061936615627]
	TIME [epoch: 7.87 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003412772168446443		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 0.0003412772168446443 | validation: 0.0015545597583150608]
	TIME [epoch: 7.84 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002726834910121896		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 0.0002726834910121896 | validation: 0.0019474304115612248]
	TIME [epoch: 7.83 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019811222719502952		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 0.00019811222719502952 | validation: 0.0016977739054622781]
	TIME [epoch: 7.83 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002409400613468287		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 0.0002409400613468287 | validation: 0.001267054964194707]
	TIME [epoch: 7.82 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022765087421086982		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 0.00022765087421086982 | validation: 0.0013391780343852818]
	TIME [epoch: 7.88 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002536549623814328		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 0.0002536549623814328 | validation: 0.001435249025102811]
	TIME [epoch: 7.84 sec]
Finished training in 16550.529 seconds.
