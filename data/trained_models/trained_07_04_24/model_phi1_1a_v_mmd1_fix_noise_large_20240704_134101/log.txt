Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise_large', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise_large', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.5, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3344778260

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.510628891029572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.510628891029572 | validation: 2.322045532893984]
	TIME [epoch: 165 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2668230981438557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2668230981438557 | validation: 2.305989814267236]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1916086113773194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1916086113773194 | validation: 2.302806034656478]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.169467572582972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.169467572582972 | validation: 2.3376297777712525]
	TIME [epoch: 7.53 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.143478020163263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.143478020163263 | validation: 2.5417192038348793]
	TIME [epoch: 7.53 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.204685479469608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.204685479469608 | validation: 2.395162630215988]
	TIME [epoch: 7.52 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1167284518634037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1167284518634037 | validation: 2.344249298221193]
	TIME [epoch: 7.54 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1518801122818134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1518801122818134 | validation: 2.315214251389094]
	TIME [epoch: 7.59 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1932543919932255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1932543919932255 | validation: 2.2526952290395337]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0960343015984453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0960343015984453 | validation: 2.276012495266638]
	TIME [epoch: 7.56 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.069009745613897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.069009745613897 | validation: 2.214811150380479]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0915127475970756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0915127475970756 | validation: 2.2465674517481737]
	TIME [epoch: 7.54 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9050421204210823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9050421204210823 | validation: 1.9104429073426457]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.065389869145935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.065389869145935 | validation: 1.8754723953207573]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7887566798479475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7887566798479475 | validation: 1.7262111193318805]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7323997237042545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7323997237042545 | validation: 1.9801915646208477]
	TIME [epoch: 7.55 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.697890784480747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.697890784480747 | validation: 1.629807675421754]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5770896550722822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5770896550722822 | validation: 1.5749588215958754]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.557762315935923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.557762315935923 | validation: 1.9045469812924516]
	TIME [epoch: 7.55 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7227361929654195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7227361929654195 | validation: 1.5448772438992835]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5187221189759077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5187221189759077 | validation: 1.536542960800034]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5710232684127008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5710232684127008 | validation: 1.493836624634169]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5328654628546046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5328654628546046 | validation: 1.4938104688347829]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.450062848397836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.450062848397836 | validation: 1.4157720200804578]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5439948414009899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5439948414009899 | validation: 1.5531041174049336]
	TIME [epoch: 7.53 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5027975006270462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5027975006270462 | validation: 1.3884043339470011]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3779132291055776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3779132291055776 | validation: 1.478126114060275]
	TIME [epoch: 7.6 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3562756313295687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3562756313295687 | validation: 1.5955308683814402]
	TIME [epoch: 7.56 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5442695500748749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5442695500748749 | validation: 1.2879875950759607]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2684197663948082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2684197663948082 | validation: 1.310617978379816]
	TIME [epoch: 7.55 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.205424501593762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.205424501593762 | validation: 1.106247309334607]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3091410279879807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3091410279879807 | validation: 1.3671219703228688]
	TIME [epoch: 7.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1828675918187384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1828675918187384 | validation: 1.4166504508632545]
	TIME [epoch: 7.55 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3108117088292974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3108117088292974 | validation: 1.174499888303642]
	TIME [epoch: 7.56 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0728244700719292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0728244700719292 | validation: 0.9549282937172265]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9032179060767741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032179060767741 | validation: 0.9659595429620932]
	TIME [epoch: 7.59 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9570994790648188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9570994790648188 | validation: 1.0133635588980445]
	TIME [epoch: 7.58 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6589028787658329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6589028787658329 | validation: 1.5084480189946188]
	TIME [epoch: 7.56 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3488442793667021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3488442793667021 | validation: 1.0427158267540597]
	TIME [epoch: 7.55 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8420076346421206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420076346421206 | validation: 0.750236266791529]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7413360440401321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7413360440401321 | validation: 0.7179233069540889]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7068280203689226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7068280203689226 | validation: 0.8512386420175784]
	TIME [epoch: 7.54 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7528887260929742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528887260929742 | validation: 0.7577686015503508]
	TIME [epoch: 7.51 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6909734177491488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6909734177491488 | validation: 0.8393590317267058]
	TIME [epoch: 7.51 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9875010255720051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9875010255720051 | validation: 1.2379056323574593]
	TIME [epoch: 7.51 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2238820385927331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2238820385927331 | validation: 0.9153929851705692]
	TIME [epoch: 7.54 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.851045333578464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.851045333578464 | validation: 0.6820205756497946]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6737900215291511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6737900215291511 | validation: 0.7443415291949262]
	TIME [epoch: 7.52 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6364013120164749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364013120164749 | validation: 0.7066274042597176]
	TIME [epoch: 7.53 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6333464714671424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6333464714671424 | validation: 0.6208289228309753]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6017174555348719		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.6017174555348719 | validation: 0.5446272118775706]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6038575533997352		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.6038575533997352 | validation: 0.5827929630401891]
	TIME [epoch: 7.58 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8996308114471208		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.8996308114471208 | validation: 1.413833508460903]
	TIME [epoch: 7.55 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9786787199583654		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.9786787199583654 | validation: 0.6193182693474164]
	TIME [epoch: 7.55 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.571314707073429		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.571314707073429 | validation: 0.7399390834648598]
	TIME [epoch: 7.56 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6890228833029834		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.6890228833029834 | validation: 0.515760302656175]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6060786406616954		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.6060786406616954 | validation: 1.0290138632012458]
	TIME [epoch: 7.56 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2695896076950688		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.2695896076950688 | validation: 1.0206809065897473]
	TIME [epoch: 7.55 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8264850486711536		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.8264850486711536 | validation: 0.6027840673914233]
	TIME [epoch: 7.54 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6589518806142413		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.6589518806142413 | validation: 0.5625274669657265]
	TIME [epoch: 7.54 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6124017987968403		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.6124017987968403 | validation: 0.5396209144831576]
	TIME [epoch: 7.57 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6352176930361025		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.6352176930361025 | validation: 0.8033647936364183]
	TIME [epoch: 7.57 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7329535104127496		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.7329535104127496 | validation: 0.5649663369496185]
	TIME [epoch: 7.54 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6534809599884766		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.6534809599884766 | validation: 0.5278886360850058]
	TIME [epoch: 7.54 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5929116572186872		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.5929116572186872 | validation: 0.6003869406491862]
	TIME [epoch: 7.53 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9513299922484444		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.9513299922484444 | validation: 0.6785469262162007]
	TIME [epoch: 7.56 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5762995666269183		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5762995666269183 | validation: 0.47716683865425596]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9008315799261817		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.9008315799261817 | validation: 0.8840842666054776]
	TIME [epoch: 7.54 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6270768135544338		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.6270768135544338 | validation: 0.5931126400723391]
	TIME [epoch: 7.54 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.568977227860904		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.568977227860904 | validation: 0.5458848865364616]
	TIME [epoch: 7.55 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5337837819721792		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.5337837819721792 | validation: 0.500832248385586]
	TIME [epoch: 7.56 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4541528824306278		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.4541528824306278 | validation: 0.46867342509369236]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7124081143821424		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 2.7124081143821424 | validation: 3.4474936196656873]
	TIME [epoch: 7.55 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2674331436994573		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.2674331436994573 | validation: 3.1531860690511673]
	TIME [epoch: 7.55 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8864710608501483		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 2.8864710608501483 | validation: 1.7211718998535068]
	TIME [epoch: 7.54 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.692746225190624		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 1.692746225190624 | validation: 1.1921487833667195]
	TIME [epoch: 7.56 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1851980985471438		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 1.1851980985471438 | validation: 0.9992800419732191]
	TIME [epoch: 7.57 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8911509510665883		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.8911509510665883 | validation: 0.7593800028006017]
	TIME [epoch: 7.54 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9127528946279747		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.9127528946279747 | validation: 0.6159198151539066]
	TIME [epoch: 7.55 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6060087307640294		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.6060087307640294 | validation: 0.5425611456866386]
	TIME [epoch: 7.54 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5330004813513544		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5330004813513544 | validation: 0.4939832697285367]
	TIME [epoch: 7.57 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4590050681145738		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.4590050681145738 | validation: 0.44111227852072865]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5490754467193617		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5490754467193617 | validation: 0.721162812459904]
	TIME [epoch: 7.54 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5038531304979894		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.5038531304979894 | validation: 1.0098519143920592]
	TIME [epoch: 7.55 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8861337451643816		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.8861337451643816 | validation: 0.5041460604187484]
	TIME [epoch: 7.55 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5092887952414837		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5092887952414837 | validation: 0.41421475031642363]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3841159350004715		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.3841159350004715 | validation: 0.37896691382542347]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5297739436280113		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5297739436280113 | validation: 0.4788663297558827]
	TIME [epoch: 7.52 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46052118218456106		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.46052118218456106 | validation: 0.512344576237529]
	TIME [epoch: 7.52 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4393718637016656		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4393718637016656 | validation: 2.859915495048704]
	TIME [epoch: 7.52 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3062091026693323		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 1.3062091026693323 | validation: 0.512496787581259]
	TIME [epoch: 7.55 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42815807195900835		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.42815807195900835 | validation: 0.417196306370365]
	TIME [epoch: 7.56 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3730749372325749		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.3730749372325749 | validation: 0.3580268092810186]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3417016885307578		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.3417016885307578 | validation: 0.339261686060749]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4940141848346249		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.4940141848346249 | validation: 0.5020878398644214]
	TIME [epoch: 7.54 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4990464489950133		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.4990464489950133 | validation: 0.5115443011485303]
	TIME [epoch: 7.56 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4610721901489672		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.4610721901489672 | validation: 0.36475472336179443]
	TIME [epoch: 7.58 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36144735377112275		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.36144735377112275 | validation: 0.29687942700754466]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3733080199401435		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.3733080199401435 | validation: 0.4345772962885247]
	TIME [epoch: 7.54 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37278751146885986		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.37278751146885986 | validation: 0.4758898661014299]
	TIME [epoch: 7.54 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4042727904630296		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.4042727904630296 | validation: 0.4521583985021332]
	TIME [epoch: 7.59 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37140831357621645		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.37140831357621645 | validation: 0.2747403038259939]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4731680432038373		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.4731680432038373 | validation: 0.5481222097364266]
	TIME [epoch: 7.54 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42040702086624604		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.42040702086624604 | validation: 0.3779948345317412]
	TIME [epoch: 7.55 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33924846659125657		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.33924846659125657 | validation: 0.32557418244006486]
	TIME [epoch: 7.55 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.392267388530846		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 1.392267388530846 | validation: 0.8621112378674605]
	TIME [epoch: 7.58 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5697939391695772		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.5697939391695772 | validation: 0.4681465979852846]
	TIME [epoch: 7.57 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3828891099992765		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3828891099992765 | validation: 0.3061677936700181]
	TIME [epoch: 7.55 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28316969423654725		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.28316969423654725 | validation: 0.34198382269990696]
	TIME [epoch: 7.55 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3742241560064557		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.3742241560064557 | validation: 0.44661420600571566]
	TIME [epoch: 7.56 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3472068970809868		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3472068970809868 | validation: 0.2878461180399384]
	TIME [epoch: 7.57 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671169804567884		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.2671169804567884 | validation: 0.43266115239558534]
	TIME [epoch: 7.59 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8688062465735453		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.8688062465735453 | validation: 1.2248507482292184]
	TIME [epoch: 7.55 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9359485870106803		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.9359485870106803 | validation: 0.447180873102569]
	TIME [epoch: 7.55 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3281901484825756		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.3281901484825756 | validation: 0.2910786714425483]
	TIME [epoch: 7.56 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29281860258029846		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.29281860258029846 | validation: 0.28478635351245163]
	TIME [epoch: 7.56 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721771881869706		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.2721771881869706 | validation: 0.6493689436823791]
	TIME [epoch: 7.6 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4356816810688195		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.4356816810688195 | validation: 0.2505427598165243]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26637215500485434		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.26637215500485434 | validation: 0.2845720119637958]
	TIME [epoch: 7.51 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24385237856887326		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.24385237856887326 | validation: 0.25956521884795347]
	TIME [epoch: 7.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31150720561005635		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.31150720561005635 | validation: 0.2801452653979857]
	TIME [epoch: 7.53 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29140804665078335		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.29140804665078335 | validation: 0.5271508048423749]
	TIME [epoch: 7.56 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5719877261231802		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.5719877261231802 | validation: 0.44897351845122657]
	TIME [epoch: 7.51 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.669604671025734		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.669604671025734 | validation: 0.3630920010628522]
	TIME [epoch: 7.51 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2897348060680861		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2897348060680861 | validation: 0.25003783500624394]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42853889829524944		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.42853889829524944 | validation: 0.43865078250543654]
	TIME [epoch: 7.57 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3318919470055995		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3318919470055995 | validation: 0.28089197153382406]
	TIME [epoch: 7.57 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24685867433358813		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.24685867433358813 | validation: 0.25884182955352575]
	TIME [epoch: 7.52 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23160305366800615		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.23160305366800615 | validation: 0.2343927933971649]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2740372066747081		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2740372066747081 | validation: 0.8035954069361628]
	TIME [epoch: 7.53 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37032556067837863		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.37032556067837863 | validation: 0.21160624231162117]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3462788593840589		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.3462788593840589 | validation: 0.1981730035409729]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3220083863656804		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 1.3220083863656804 | validation: 0.3672410587881302]
	TIME [epoch: 7.56 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29355248265888556		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.29355248265888556 | validation: 0.23000395819174907]
	TIME [epoch: 7.54 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32952574035509286		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.32952574035509286 | validation: 0.3076307757449792]
	TIME [epoch: 7.55 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40693911533120664		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.40693911533120664 | validation: 0.3199668074470894]
	TIME [epoch: 7.55 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2791182017528849		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.2791182017528849 | validation: 0.21304063527705971]
	TIME [epoch: 7.59 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.460849742450975		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.460849742450975 | validation: 0.2543705611041355]
	TIME [epoch: 7.55 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2516044932271274		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2516044932271274 | validation: 0.19940615751026192]
	TIME [epoch: 7.55 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2202640428904949		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2202640428904949 | validation: 0.21880009018899024]
	TIME [epoch: 7.55 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23850829135798765		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.23850829135798765 | validation: 0.21685681419583358]
	TIME [epoch: 7.55 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23678312819813246		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.23678312819813246 | validation: 0.2757320094702914]
	TIME [epoch: 7.59 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22275539625609875		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.22275539625609875 | validation: 0.20586322554333036]
	TIME [epoch: 7.54 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33522836071468765		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.33522836071468765 | validation: 0.28359692608220793]
	TIME [epoch: 7.56 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24390129189472573		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.24390129189472573 | validation: 0.18686746801785759]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22203135159972404		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.22203135159972404 | validation: 0.2162123002766027]
	TIME [epoch: 7.56 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613600316653634		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2613600316653634 | validation: 0.47978904202651884]
	TIME [epoch: 7.6 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.028539272046041		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 1.028539272046041 | validation: 3.3775252635626636]
	TIME [epoch: 7.55 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.93369128059488		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 2.93369128059488 | validation: 1.1237275274030225]
	TIME [epoch: 7.55 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.740429459927574		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.740429459927574 | validation: 0.5136839266704514]
	TIME [epoch: 7.57 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4515958092683087		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.4515958092683087 | validation: 0.49209670362795277]
	TIME [epoch: 7.57 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2747770776781267		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.2747770776781267 | validation: 0.19264854655685953]
	TIME [epoch: 7.61 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20255155220789478		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.20255155220789478 | validation: 0.3196269322034839]
	TIME [epoch: 7.57 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.275483939947942		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.275483939947942 | validation: 0.21360405017879242]
	TIME [epoch: 7.57 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18985541757350835		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.18985541757350835 | validation: 0.18881096915350556]
	TIME [epoch: 7.56 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18385856034511933		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.18385856034511933 | validation: 0.17319072556391862]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17135875262906625		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.17135875262906625 | validation: 0.2621767334296629]
	TIME [epoch: 7.59 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24046482388908008		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.24046482388908008 | validation: 0.21058650738123896]
	TIME [epoch: 7.54 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20035504767159487		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.20035504767159487 | validation: 0.19446268074050477]
	TIME [epoch: 7.55 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3360800899945264		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.3360800899945264 | validation: 0.21166509125903965]
	TIME [epoch: 7.55 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18851236739618965		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.18851236739618965 | validation: 0.4255199638432451]
	TIME [epoch: 7.55 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4366833934913703		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.4366833934913703 | validation: 0.23967870773730565]
	TIME [epoch: 7.6 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2515012410768235		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2515012410768235 | validation: 0.1962258049626159]
	TIME [epoch: 7.56 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18334071957130252		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.18334071957130252 | validation: 0.21583340564749554]
	TIME [epoch: 7.54 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3581457199471183		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.3581457199471183 | validation: 0.20370625828029537]
	TIME [epoch: 7.55 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19174263116570994		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.19174263116570994 | validation: 0.1752022778963503]
	TIME [epoch: 7.54 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2214618148142416		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.2214618148142416 | validation: 0.4450936588923311]
	TIME [epoch: 7.6 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39516884543069275		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.39516884543069275 | validation: 0.2217379148517498]
	TIME [epoch: 7.55 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19900801057791714		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.19900801057791714 | validation: 0.16427537961499128]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18229038032655165		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.18229038032655165 | validation: 0.22765968926146346]
	TIME [epoch: 7.55 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24500566196637358		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.24500566196637358 | validation: 0.16535548716699913]
	TIME [epoch: 7.55 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1600010410210534		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.1600010410210534 | validation: 0.22689240720293202]
	TIME [epoch: 7.61 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2574361548045723		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 1.2574361548045723 | validation: 0.5156850506783561]
	TIME [epoch: 7.56 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3269179622839784		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.3269179622839784 | validation: 0.2363314184813231]
	TIME [epoch: 7.55 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2091032596400844		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.2091032596400844 | validation: 0.17378201674921595]
	TIME [epoch: 7.56 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3277557844619866		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.3277557844619866 | validation: 0.6223418791274564]
	TIME [epoch: 7.56 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790374726206341		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.5790374726206341 | validation: 0.35034639852960386]
	TIME [epoch: 7.6 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27181786437252303		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.27181786437252303 | validation: 0.18275077720185545]
	TIME [epoch: 7.55 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1884036294719428		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.1884036294719428 | validation: 0.1602166329473369]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1766291447980743		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1766291447980743 | validation: 0.19490269288183976]
	TIME [epoch: 7.51 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2648843361291134		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.2648843361291134 | validation: 0.22457641942087148]
	TIME [epoch: 7.51 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2389433628043685		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.2389433628043685 | validation: 0.26417715295730937]
	TIME [epoch: 7.56 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18696605465606853		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.18696605465606853 | validation: 0.20212375785269204]
	TIME [epoch: 7.51 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17997489048461499		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.17997489048461499 | validation: 0.17324993897686558]
	TIME [epoch: 7.51 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19037535684988177		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.19037535684988177 | validation: 0.20261983069957956]
	TIME [epoch: 7.51 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1733470482506043		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.1733470482506043 | validation: 0.16067330979632932]
	TIME [epoch: 7.51 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1680883788095395		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.1680883788095395 | validation: 0.1974855230030929]
	TIME [epoch: 7.55 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2027765169848721		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.2027765169848721 | validation: 0.14336737287054832]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20100933003567856		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.20100933003567856 | validation: 0.40253941035765045]
	TIME [epoch: 7.53 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24993379903112634		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.24993379903112634 | validation: 0.17035816444327173]
	TIME [epoch: 7.52 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1720887254201351		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.1720887254201351 | validation: 0.16890723590581527]
	TIME [epoch: 7.52 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1846972325797773		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.1846972325797773 | validation: 0.18532495038068206]
	TIME [epoch: 7.55 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14843564303291973		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.14843564303291973 | validation: 0.14410563037612964]
	TIME [epoch: 7.55 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2701075127093422		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.2701075127093422 | validation: 0.13755744437452325]
	TIME [epoch: 7.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16927402914892453		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.16927402914892453 | validation: 0.18084679734546807]
	TIME [epoch: 7.53 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28522857926794754		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.28522857926794754 | validation: 0.2602378561340907]
	TIME [epoch: 7.54 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19472445510624475		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.19472445510624475 | validation: 0.1652927892715017]
	TIME [epoch: 7.55 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1468627906258069		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.1468627906258069 | validation: 0.1500682836026536]
	TIME [epoch: 7.54 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26335973162478366		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.26335973162478366 | validation: 0.24200154194483764]
	TIME [epoch: 7.52 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2212106344129745		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.2212106344129745 | validation: 0.2035540612967066]
	TIME [epoch: 7.5 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39869128746657806		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.39869128746657806 | validation: 0.25313584156230673]
	TIME [epoch: 120 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23518679857660973		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.23518679857660973 | validation: 0.2046731368790572]
	TIME [epoch: 14.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.213049010609071		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.213049010609071 | validation: 0.44784652891064974]
	TIME [epoch: 14.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3485763552560968		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.3485763552560968 | validation: 0.36221628972992703]
	TIME [epoch: 14.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30542217601130545		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.30542217601130545 | validation: 0.3477153983341974]
	TIME [epoch: 14.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.265260893413155		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.265260893413155 | validation: 0.24524091037089182]
	TIME [epoch: 14.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20521873303450938		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.20521873303450938 | validation: 0.19026866699394823]
	TIME [epoch: 14.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18312616598428857		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.18312616598428857 | validation: 0.1686657199586119]
	TIME [epoch: 14.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18851535804187247		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.18851535804187247 | validation: 0.2628197211331387]
	TIME [epoch: 14.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1989414210854107		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1989414210854107 | validation: 0.25327411014650203]
	TIME [epoch: 14.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20707498731953572		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.20707498731953572 | validation: 0.16845261129036468]
	TIME [epoch: 14.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1825985392603203		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1825985392603203 | validation: 0.18054932605557303]
	TIME [epoch: 14.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2536405076959929		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.2536405076959929 | validation: 0.3105862885734405]
	TIME [epoch: 14.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32529571970939275		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.32529571970939275 | validation: 0.1739761003403319]
	TIME [epoch: 14.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17642843520992857		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.17642843520992857 | validation: 0.2089176214137669]
	TIME [epoch: 14.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5820838342734871		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.5820838342734871 | validation: 0.31145043876796824]
	TIME [epoch: 14.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22249138206610422		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.22249138206610422 | validation: 0.30477985293330667]
	TIME [epoch: 14.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2393205268489715		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.2393205268489715 | validation: 0.2176695638485257]
	TIME [epoch: 14.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17875915666187045		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.17875915666187045 | validation: 0.15299786944120772]
	TIME [epoch: 14.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28288713282344596		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.28288713282344596 | validation: 0.15149438107167879]
	TIME [epoch: 14.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1683385878274239		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.1683385878274239 | validation: 0.23058334394506283]
	TIME [epoch: 14.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5718130580468548		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.5718130580468548 | validation: 1.2012674874239515]
	TIME [epoch: 14.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7287360205817863		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.7287360205817863 | validation: 0.33856690890402336]
	TIME [epoch: 14.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24571032040648474		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.24571032040648474 | validation: 0.15604369856123732]
	TIME [epoch: 14.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1501602844454379		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.1501602844454379 | validation: 0.1780117094798509]
	TIME [epoch: 14.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20920936681360608		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.20920936681360608 | validation: 0.22346207951302305]
	TIME [epoch: 14.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17596996626764144		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.17596996626764144 | validation: 0.18980867571943333]
	TIME [epoch: 14.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15468796376056815		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.15468796376056815 | validation: 0.17323540950628835]
	TIME [epoch: 14.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1498649186857422		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.1498649186857422 | validation: 0.1634953908948643]
	TIME [epoch: 14.8 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15068943432291673		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.15068943432291673 | validation: 0.16580931937291862]
	TIME [epoch: 14.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14653529855324096		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.14653529855324096 | validation: 0.18828804091775672]
	TIME [epoch: 14.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17469929205960705		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.17469929205960705 | validation: 0.16961297102368977]
	TIME [epoch: 14.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1711272406960905		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1711272406960905 | validation: 0.15224933315340214]
	TIME [epoch: 14.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16618387458890227		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.16618387458890227 | validation: 0.17188700470133966]
	TIME [epoch: 14.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18236054973077426		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.18236054973077426 | validation: 0.23545633077702316]
	TIME [epoch: 14.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2758415110042581		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.2758415110042581 | validation: 0.16841540538363012]
	TIME [epoch: 14.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29824125144239017		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.29824125144239017 | validation: 0.2615400111119003]
	TIME [epoch: 14.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21771816713799141		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.21771816713799141 | validation: 0.18765299138772507]
	TIME [epoch: 14.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14734867919694059		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.14734867919694059 | validation: 0.14108851381542192]
	TIME [epoch: 14.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14015205375005563		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.14015205375005563 | validation: 0.20239493994713037]
	TIME [epoch: 14.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1747550831654913		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1747550831654913 | validation: 0.1783793119967999]
	TIME [epoch: 14.7 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16933249877007991		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.16933249877007991 | validation: 0.1699144666474589]
	TIME [epoch: 14.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35828683223772256		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.35828683223772256 | validation: 0.9864596018411365]
	TIME [epoch: 14.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5319518045394813		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.5319518045394813 | validation: 0.19929144538442767]
	TIME [epoch: 14.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5279886687113414		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.5279886687113414 | validation: 0.1991807856668134]
	TIME [epoch: 14.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21809334040631626		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.21809334040631626 | validation: 0.213334879390983]
	TIME [epoch: 14.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.161950386907441		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.161950386907441 | validation: 0.18160305935279655]
	TIME [epoch: 14.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23477463367852944		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.23477463367852944 | validation: 0.15898641888310006]
	TIME [epoch: 14.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14000855910899673		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.14000855910899673 | validation: 0.160131133962019]
	TIME [epoch: 14.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15890212473889245		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.15890212473889245 | validation: 0.1358003873627481]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14174647826656164		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.14174647826656164 | validation: 0.18369066630028819]
	TIME [epoch: 14.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1598497465371705		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.1598497465371705 | validation: 0.13691041436810591]
	TIME [epoch: 14.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28474598319699096		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.28474598319699096 | validation: 1.0083205542022537]
	TIME [epoch: 14.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4074485939996972		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.4074485939996972 | validation: 0.14582925267842095]
	TIME [epoch: 14.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14856629942957955		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.14856629942957955 | validation: 0.147888947321089]
	TIME [epoch: 14.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13394386139219278		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.13394386139219278 | validation: 0.17061634127937325]
	TIME [epoch: 14.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23931493628685513		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.23931493628685513 | validation: 0.7806239445923194]
	TIME [epoch: 14.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48400174817011365		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.48400174817011365 | validation: 0.312859019678871]
	TIME [epoch: 14.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24590768774602487		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.24590768774602487 | validation: 0.2409603924859265]
	TIME [epoch: 14.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20304752633841514		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.20304752633841514 | validation: 0.21991045941671267]
	TIME [epoch: 14.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1563031005730731		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.1563031005730731 | validation: 0.13067196450541574]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12310786237444826		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.12310786237444826 | validation: 0.13735562619933972]
	TIME [epoch: 14.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12965460536912882		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.12965460536912882 | validation: 0.13212087668579497]
	TIME [epoch: 14.9 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12978838547781016		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.12978838547781016 | validation: 0.1542273151413206]
	TIME [epoch: 14.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13684490030893848		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.13684490030893848 | validation: 0.14181780129933155]
	TIME [epoch: 14.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14233065521988103		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.14233065521988103 | validation: 0.13100876353676683]
	TIME [epoch: 14.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3638075850848007		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.3638075850848007 | validation: 0.3192782800767032]
	TIME [epoch: 14.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19844678085207676		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.19844678085207676 | validation: 0.13095692618284588]
	TIME [epoch: 14.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13904065893027762		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.13904065893027762 | validation: 0.1262710384319342]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17365922903877198		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.17365922903877198 | validation: 0.2078946158940611]
	TIME [epoch: 14.7 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18288893062168643		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.18288893062168643 | validation: 0.13123291647028232]
	TIME [epoch: 14.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33858314136286505		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.33858314136286505 | validation: 1.3183196637396049]
	TIME [epoch: 14.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7448188870065484		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.7448188870065484 | validation: 0.29239230945790196]
	TIME [epoch: 14.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20924451901585714		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.20924451901585714 | validation: 0.1544364891798093]
	TIME [epoch: 14.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1496827359704222		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.1496827359704222 | validation: 0.1301248247159709]
	TIME [epoch: 14.7 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12308951282778754		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.12308951282778754 | validation: 0.13501711100275193]
	TIME [epoch: 14.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14306517450199707		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.14306517450199707 | validation: 0.15928416040019258]
	TIME [epoch: 14.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.137464121886799		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.137464121886799 | validation: 0.12756641286475817]
	TIME [epoch: 14.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3899990325240237		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.3899990325240237 | validation: 0.16742325202951625]
	TIME [epoch: 14.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13777473118534134		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.13777473118534134 | validation: 0.1342055571538883]
	TIME [epoch: 14.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14146169827535524		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.14146169827535524 | validation: 0.13436329090428722]
	TIME [epoch: 14.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12795017225468314		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.12795017225468314 | validation: 0.18221224305763592]
	TIME [epoch: 14.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13071437814006875		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.13071437814006875 | validation: 0.1252133919303064]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1322825321340346		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1322825321340346 | validation: 0.17236031497032198]
	TIME [epoch: 14.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.145385693122858		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.145385693122858 | validation: 0.12914699139024965]
	TIME [epoch: 14.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12189730432052379		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.12189730432052379 | validation: 0.20889608017843747]
	TIME [epoch: 14.7 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16669236128648635		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.16669236128648635 | validation: 0.12204586029644231]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14142242031485888		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.14142242031485888 | validation: 0.13525339345148304]
	TIME [epoch: 14.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14355475806055057		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.14355475806055057 | validation: 0.14335030030834583]
	TIME [epoch: 14.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15240489745816582		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.15240489745816582 | validation: 0.12572323218155596]
	TIME [epoch: 14.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1582529064984899		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.1582529064984899 | validation: 0.3126355531951919]
	TIME [epoch: 14.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22827344637090663		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.22827344637090663 | validation: 0.17815080781351084]
	TIME [epoch: 14.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13300956223811325		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.13300956223811325 | validation: 0.23949866891224642]
	TIME [epoch: 14.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16828140058261545		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.16828140058261545 | validation: 0.12380697985530119]
	TIME [epoch: 14.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13063082791738856		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.13063082791738856 | validation: 0.11644867750806578]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12155503692283465		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.12155503692283465 | validation: 0.12070994537978777]
	TIME [epoch: 14.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13018518499553128		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.13018518499553128 | validation: 0.1211536552294055]
	TIME [epoch: 14.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12744979653350244		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.12744979653350244 | validation: 0.13255036889721322]
	TIME [epoch: 14.9 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11234157359745914		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.11234157359745914 | validation: 0.4653874292417342]
	TIME [epoch: 14.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3821659521844607		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.3821659521844607 | validation: 0.3329492130787194]
	TIME [epoch: 14.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23767332467840985		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.23767332467840985 | validation: 0.26256979180943385]
	TIME [epoch: 14.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20718582874110597		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.20718582874110597 | validation: 0.1987109931542078]
	TIME [epoch: 14.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4808378225311424		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.4808378225311424 | validation: 0.19025046466607237]
	TIME [epoch: 14.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1979911733442855		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.1979911733442855 | validation: 0.15385796463330392]
	TIME [epoch: 14.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13914429148249413		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.13914429148249413 | validation: 0.1279475706814116]
	TIME [epoch: 14.8 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12978164926819927		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.12978164926819927 | validation: 0.1347994827883605]
	TIME [epoch: 14.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1163351289273346		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.1163351289273346 | validation: 0.12772968703055715]
	TIME [epoch: 14.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12488265969113745		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.12488265969113745 | validation: 0.1510949511416241]
	TIME [epoch: 14.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12478278758278671		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.12478278758278671 | validation: 0.12926044331363037]
	TIME [epoch: 14.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1270650913827017		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.1270650913827017 | validation: 0.12517748924826738]
	TIME [epoch: 14.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12872204952532726		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.12872204952532726 | validation: 0.12086557673249329]
	TIME [epoch: 14.9 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11916431100029692		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.11916431100029692 | validation: 0.12385606848781228]
	TIME [epoch: 14.7 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12062011046132472		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.12062011046132472 | validation: 0.13720602322764341]
	TIME [epoch: 14.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12688509836211398		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.12688509836211398 | validation: 0.12284191299476083]
	TIME [epoch: 14.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11777811239775239		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.11777811239775239 | validation: 0.12074394523231641]
	TIME [epoch: 14.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12922461281742298		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.12922461281742298 | validation: 0.1326924778369774]
	TIME [epoch: 14.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11613282133414765		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.11613282133414765 | validation: 0.16126535675705325]
	TIME [epoch: 14.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14047696527909506		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.14047696527909506 | validation: 0.13225367306801794]
	TIME [epoch: 14.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13998368444288195		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.13998368444288195 | validation: 0.1632072664386141]
	TIME [epoch: 14.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16063441546114077		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.16063441546114077 | validation: 0.18722093455554867]
	TIME [epoch: 14.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2427482409891316		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.2427482409891316 | validation: 0.1461288926516964]
	TIME [epoch: 14.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15423201714630758		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.15423201714630758 | validation: 0.11576234430426124]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11406872828008692		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.11406872828008692 | validation: 0.14509932730771596]
	TIME [epoch: 14.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12762072731858143		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.12762072731858143 | validation: 0.1695637641722756]
	TIME [epoch: 14.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13488209014904778		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.13488209014904778 | validation: 0.11458610248346407]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10690424087799308		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.10690424087799308 | validation: 0.1199453966880697]
	TIME [epoch: 14.7 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11793319937262414		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.11793319937262414 | validation: 0.12113611847287364]
	TIME [epoch: 14.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13406361464840816		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.13406361464840816 | validation: 0.1396206460798629]
	TIME [epoch: 14.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12572127595189048		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.12572127595189048 | validation: 0.13069948986807325]
	TIME [epoch: 14.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11357082188994747		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.11357082188994747 | validation: 0.12103157713921722]
	TIME [epoch: 14.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11906803615913666		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.11906803615913666 | validation: 0.15596946817376772]
	TIME [epoch: 14.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13622166593817508		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.13622166593817508 | validation: 0.1230846506103884]
	TIME [epoch: 14.8 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2178939937848628		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.2178939937848628 | validation: 0.2851071952846991]
	TIME [epoch: 14.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1814523050779271		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.1814523050779271 | validation: 0.4668940216168753]
	TIME [epoch: 14.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38086160624674653		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.38086160624674653 | validation: 0.32584959004348407]
	TIME [epoch: 14.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23868558144199986		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.23868558144199986 | validation: 0.24713277393108063]
	TIME [epoch: 14.7 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19108299616151214		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.19108299616151214 | validation: 0.1772696490976213]
	TIME [epoch: 14.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15354144927903418		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.15354144927903418 | validation: 0.14845694358379002]
	TIME [epoch: 14.9 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13399023622085032		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.13399023622085032 | validation: 0.11164624673677528]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13875398921216758		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.13875398921216758 | validation: 0.1434379354870851]
	TIME [epoch: 14.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1197576297929775		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.1197576297929775 | validation: 0.11543080864922375]
	TIME [epoch: 14.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1317761703629519		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.1317761703629519 | validation: 0.11745228258952742]
	TIME [epoch: 14.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12072245475764203		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.12072245475764203 | validation: 0.13624934412422007]
	TIME [epoch: 14.9 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12141095227311884		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.12141095227311884 | validation: 0.11928182260505452]
	TIME [epoch: 14.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1152975751081415		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.1152975751081415 | validation: 0.25637816378248623]
	TIME [epoch: 14.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2050513171887874		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.2050513171887874 | validation: 0.10214358155045383]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11679062702374556		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.11679062702374556 | validation: 0.10916303496536521]
	TIME [epoch: 14.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14534674007760418		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.14534674007760418 | validation: 0.15147688636583825]
	TIME [epoch: 14.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12388618980140678		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.12388618980140678 | validation: 0.11943789932294832]
	TIME [epoch: 14.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10793570971596057		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.10793570971596057 | validation: 0.12149869570604488]
	TIME [epoch: 14.8 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10933646595199333		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.10933646595199333 | validation: 0.15130993868675796]
	TIME [epoch: 14.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13080059105778272		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.13080059105778272 | validation: 0.14661293153596974]
	TIME [epoch: 14.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1209896508128518		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.1209896508128518 | validation: 0.1766600262776607]
	TIME [epoch: 14.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12623693606119174		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.12623693606119174 | validation: 0.20780374392795198]
	TIME [epoch: 14.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16326392549110974		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.16326392549110974 | validation: 0.1807953850047378]
	TIME [epoch: 14.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1548841894161146		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.1548841894161146 | validation: 0.1400484105779155]
	TIME [epoch: 14.9 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19630905930075376		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.19630905930075376 | validation: 0.14538103369998326]
	TIME [epoch: 14.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12155289204555736		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.12155289204555736 | validation: 0.1098745290637381]
	TIME [epoch: 14.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10589936154488987		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.10589936154488987 | validation: 0.12548292415697188]
	TIME [epoch: 14.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12008191406283318		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.12008191406283318 | validation: 0.14155050835165633]
	TIME [epoch: 14.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13111721626917083		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.13111721626917083 | validation: 0.1266008803373297]
	TIME [epoch: 14.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16728648674729796		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.16728648674729796 | validation: 0.18150332881819478]
	TIME [epoch: 14.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14209618898214138		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.14209618898214138 | validation: 0.10018422167677957]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10435187752505869		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.10435187752505869 | validation: 0.11767367355285188]
	TIME [epoch: 14.9 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11439600570988462		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.11439600570988462 | validation: 0.2117993315485892]
	TIME [epoch: 14.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.163004363479598		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.163004363479598 | validation: 0.13325115942773752]
	TIME [epoch: 14.7 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12601994332017866		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.12601994332017866 | validation: 0.13012486823539848]
	TIME [epoch: 14.9 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1315601488972616		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.1315601488972616 | validation: 0.17033892990567148]
	TIME [epoch: 14.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23227680054823852		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.23227680054823852 | validation: 0.30986754950200535]
	TIME [epoch: 14.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18800491000741976		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.18800491000741976 | validation: 0.11516442956639975]
	TIME [epoch: 14.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11131467110935275		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.11131467110935275 | validation: 0.10982942611465918]
	TIME [epoch: 14.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1810183647814789		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.1810183647814789 | validation: 0.24011029968989306]
	TIME [epoch: 14.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16129931103546263		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.16129931103546263 | validation: 0.13126850144111157]
	TIME [epoch: 14.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13405355714736691		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.13405355714736691 | validation: 0.13339064247479193]
	TIME [epoch: 14.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12557577795889738		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.12557577795889738 | validation: 0.10683738030551118]
	TIME [epoch: 14.9 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10445170680701371		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.10445170680701371 | validation: 0.10681048802951204]
	TIME [epoch: 14.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11688573909958813		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.11688573909958813 | validation: 0.12412013251830584]
	TIME [epoch: 14.8 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12127873875271891		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.12127873875271891 | validation: 0.12007170464166353]
	TIME [epoch: 14.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10551631040509298		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.10551631040509298 | validation: 0.11336815951318371]
	TIME [epoch: 14.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10816649585152849		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.10816649585152849 | validation: 0.10584768643536419]
	TIME [epoch: 14.9 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11762065703124994		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.11762065703124994 | validation: 0.12050822358124022]
	TIME [epoch: 14.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12577002784520297		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.12577002784520297 | validation: 0.1563225235308519]
	TIME [epoch: 14.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1327452635099052		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.1327452635099052 | validation: 0.12411383619864483]
	TIME [epoch: 14.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11038978368819835		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.11038978368819835 | validation: 0.11763319977216402]
	TIME [epoch: 14.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17626113229599782		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.17626113229599782 | validation: 0.14746315841261587]
	TIME [epoch: 14.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1281128466870842		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.1281128466870842 | validation: 0.14460296216824275]
	TIME [epoch: 14.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12354152921609851		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.12354152921609851 | validation: 0.13815308578378663]
	TIME [epoch: 14.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34220542522591374		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.34220542522591374 | validation: 0.3265064780835365]
	TIME [epoch: 14.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21112353207485302		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.21112353207485302 | validation: 0.11898160026649454]
	TIME [epoch: 14.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11490188791249567		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.11490188791249567 | validation: 0.10359373068221739]
	TIME [epoch: 14.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11548239665779225		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.11548239665779225 | validation: 0.1255413941002952]
	TIME [epoch: 14.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11173444041437465		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.11173444041437465 | validation: 0.12532311515383437]
	TIME [epoch: 14.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1047647850018521		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.1047647850018521 | validation: 0.11542626365116718]
	TIME [epoch: 14.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10520309692811095		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.10520309692811095 | validation: 0.1342735542645177]
	TIME [epoch: 14.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11104874503320475		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.11104874503320475 | validation: 0.1139841949563403]
	TIME [epoch: 14.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11632316078990218		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.11632316078990218 | validation: 0.11420563373981764]
	TIME [epoch: 14.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11830069743043126		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.11830069743043126 | validation: 0.11848210888369043]
	TIME [epoch: 14.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11177222434971609		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.11177222434971609 | validation: 0.17880295948255004]
	TIME [epoch: 14.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13422564959501465		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.13422564959501465 | validation: 0.10552472814514416]
	TIME [epoch: 14.9 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115296206151257		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.10115296206151257 | validation: 0.11452996771254545]
	TIME [epoch: 14.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11404363983277602		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.11404363983277602 | validation: 0.09585074695422345]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09876282107738832		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.09876282107738832 | validation: 0.10639854330564985]
	TIME [epoch: 14.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1135462079298816		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.1135462079298816 | validation: 0.12444883571505626]
	TIME [epoch: 14.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11626908706868316		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.11626908706868316 | validation: 0.0974926498402108]
	TIME [epoch: 14.9 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11078922921917951		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.11078922921917951 | validation: 0.1037948967736437]
	TIME [epoch: 14.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1383417476822411		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.1383417476822411 | validation: 0.13768110662469923]
	TIME [epoch: 14.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12167313149512349		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.12167313149512349 | validation: 0.14665030211884175]
	TIME [epoch: 14.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12878421264303194		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.12878421264303194 | validation: 0.14501403120734851]
	TIME [epoch: 14.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11379747908845339		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.11379747908845339 | validation: 0.10239659738026355]
	TIME [epoch: 14.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10495924813945567		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.10495924813945567 | validation: 0.10669965457637459]
	TIME [epoch: 14.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1116769804275802		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.1116769804275802 | validation: 0.32072702631524275]
	TIME [epoch: 14.8 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24970459264674416		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.24970459264674416 | validation: 0.2100237510334252]
	TIME [epoch: 14.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15256043093256763		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.15256043093256763 | validation: 0.13512971797787487]
	TIME [epoch: 14.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13699019945969096		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.13699019945969096 | validation: 0.10678355599383259]
	TIME [epoch: 14.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11108016507631843		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.11108016507631843 | validation: 0.10092543687079693]
	TIME [epoch: 14.9 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10125762712693709		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.10125762712693709 | validation: 0.10048426044226509]
	TIME [epoch: 14.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10101256546196599		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.10101256546196599 | validation: 0.09461490166018374]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09772625825316045		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.09772625825316045 | validation: 0.12083394347009914]
	TIME [epoch: 14.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1027008484724049		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.1027008484724049 | validation: 0.10542556285003629]
	TIME [epoch: 14.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10998751142342314		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.10998751142342314 | validation: 0.10385226754737947]
	TIME [epoch: 14.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20341456616901138		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.20341456616901138 | validation: 0.17230642074785268]
	TIME [epoch: 14.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14579159978501172		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.14579159978501172 | validation: 0.20091720782703126]
	TIME [epoch: 14.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11719669489951563		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.11719669489951563 | validation: 0.11279624428432271]
	TIME [epoch: 14.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09796980597564277		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.09796980597564277 | validation: 0.10445858328648344]
	TIME [epoch: 14.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10793213135641008		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.10793213135641008 | validation: 0.14389521711232867]
	TIME [epoch: 14.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15759680134340254		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.15759680134340254 | validation: 0.20322055874403244]
	TIME [epoch: 14.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1445960472369385		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.1445960472369385 | validation: 0.12751414268697658]
	TIME [epoch: 14.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10702337986208828		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.10702337986208828 | validation: 0.13022416674926005]
	TIME [epoch: 14.9 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11023682627623702		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.11023682627623702 | validation: 0.13025466139474218]
	TIME [epoch: 14.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1274729563008152		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.1274729563008152 | validation: 0.09909987075625612]
	TIME [epoch: 14.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10376915100450469		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.10376915100450469 | validation: 0.11467582493464624]
	TIME [epoch: 14.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10223009864926694		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.10223009864926694 | validation: 0.2128772673891997]
	TIME [epoch: 14.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16228551164873944		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.16228551164873944 | validation: 0.09360891975095485]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10058956156776191		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.10058956156776191 | validation: 0.10730408615278049]
	TIME [epoch: 14.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10692103108140352		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.10692103108140352 | validation: 0.11092415888018074]
	TIME [epoch: 14.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.095853497196901		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.095853497196901 | validation: 0.10055537135353887]
	TIME [epoch: 14.9 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15830595715702134		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.15830595715702134 | validation: 0.09108515101481199]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10243051706434594		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.10243051706434594 | validation: 0.09992404078147465]
	TIME [epoch: 14.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12216577740190222		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.12216577740190222 | validation: 0.09846570191539701]
	TIME [epoch: 14.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09806615892913602		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.09806615892913602 | validation: 0.09414789675251539]
	TIME [epoch: 14.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10849830184007003		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.10849830184007003 | validation: 0.09366816767463787]
	TIME [epoch: 14.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09762547855203114		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.09762547855203114 | validation: 0.10543389714201917]
	TIME [epoch: 14.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11651403264206393		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.11651403264206393 | validation: 0.1048825750991728]
	TIME [epoch: 14.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10362737302409314		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.10362737302409314 | validation: 0.1057795444724145]
	TIME [epoch: 14.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09958959864788167		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.09958959864788167 | validation: 0.09828737234305124]
	TIME [epoch: 14.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10555481375101768		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.10555481375101768 | validation: 0.10821357469438604]
	TIME [epoch: 14.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640787745809993		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.2640787745809993 | validation: 0.2885761235523864]
	TIME [epoch: 14.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21440751237936959		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.21440751237936959 | validation: 0.23568131072192622]
	TIME [epoch: 14.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16670141426826973		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.16670141426826973 | validation: 0.1455482585228609]
	TIME [epoch: 14.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12559645221565147		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.12559645221565147 | validation: 0.1346733337589645]
	TIME [epoch: 14.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10899795451422638		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.10899795451422638 | validation: 0.11067170719160219]
	TIME [epoch: 14.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10407214798627408		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.10407214798627408 | validation: 0.11144410377720147]
	TIME [epoch: 14.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15269502974795784		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.15269502974795784 | validation: 0.12524725984658927]
	TIME [epoch: 14.9 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09865604339087428		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.09865604339087428 | validation: 0.09780636307374849]
	TIME [epoch: 14.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10377425159206599		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.10377425159206599 | validation: 0.23980141922375603]
	TIME [epoch: 14.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21942297776104472		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.21942297776104472 | validation: 0.17304029698007653]
	TIME [epoch: 14.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1350293065093426		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.1350293065093426 | validation: 0.11372571530446526]
	TIME [epoch: 14.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10510871424532059		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.10510871424532059 | validation: 0.11131228970292192]
	TIME [epoch: 14.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09905984211145996		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.09905984211145996 | validation: 0.12453679922253533]
	TIME [epoch: 14.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10549739038410628		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.10549739038410628 | validation: 0.11517628518016187]
	TIME [epoch: 14.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10518425814484429		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.10518425814484429 | validation: 0.11480680464128162]
	TIME [epoch: 14.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10004268427372127		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.10004268427372127 | validation: 0.11105180631456849]
	TIME [epoch: 14.9 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09951267213092106		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.09951267213092106 | validation: 0.11857049364575514]
	TIME [epoch: 14.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10120993362914987		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.10120993362914987 | validation: 0.10775919507875528]
	TIME [epoch: 14.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0955195579577476		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.0955195579577476 | validation: 0.1021583651165699]
	TIME [epoch: 14.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12189669955993285		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.12189669955993285 | validation: 0.10545217120175404]
	TIME [epoch: 14.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10798569870081198		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.10798569870081198 | validation: 0.09472629613301833]
	TIME [epoch: 14.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09186668338845676		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.09186668338845676 | validation: 0.09682187577600514]
	TIME [epoch: 14.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10529543028847048		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.10529543028847048 | validation: 0.0991021905377473]
	TIME [epoch: 14.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11161479585685974		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.11161479585685974 | validation: 0.12053808820632717]
	TIME [epoch: 14.9 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10510472836543314		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.10510472836543314 | validation: 0.11990213565987659]
	TIME [epoch: 14.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0966845268997019		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.0966845268997019 | validation: 0.0971164471960605]
	TIME [epoch: 14.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09454415440389954		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.09454415440389954 | validation: 0.10331392399100787]
	TIME [epoch: 14.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11696036052189765		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.11696036052189765 | validation: 0.10079086292217633]
	TIME [epoch: 14.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09570888456640998		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.09570888456640998 | validation: 0.28907824536773247]
	TIME [epoch: 14.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27469603936593184		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.27469603936593184 | validation: 0.28710961214210756]
	TIME [epoch: 14.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1566919786236281		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.1566919786236281 | validation: 0.12053654421226322]
	TIME [epoch: 14.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09727274656517367		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.09727274656517367 | validation: 0.10689032328529152]
	TIME [epoch: 14.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09591563515624374		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.09591563515624374 | validation: 0.09728141015638461]
	TIME [epoch: 14.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09516693906629253		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.09516693906629253 | validation: 0.11929919271222997]
	TIME [epoch: 14.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09667094130510845		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.09667094130510845 | validation: 0.1253435672824028]
	TIME [epoch: 14.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1149679041843654		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.1149679041843654 | validation: 0.11845283681959826]
	TIME [epoch: 14.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09997792485031587		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.09997792485031587 | validation: 0.10925400163028365]
	TIME [epoch: 14.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10752526633291326		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.10752526633291326 | validation: 0.10755171785504673]
	TIME [epoch: 14.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10478552395422443		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.10478552395422443 | validation: 0.11706977212314296]
	TIME [epoch: 14.9 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09898536746345335		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.09898536746345335 | validation: 0.10571022021864072]
	TIME [epoch: 14.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1082095251130185		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.1082095251130185 | validation: 0.09812389031439421]
	TIME [epoch: 14.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10586226848745263		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.10586226848745263 | validation: 0.11985591082713151]
	TIME [epoch: 14.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1077090842947894		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.1077090842947894 | validation: 0.11678037755347079]
	TIME [epoch: 14.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10252996953873664		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.10252996953873664 | validation: 0.1299230465186063]
	TIME [epoch: 14.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10234376287555498		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.10234376287555498 | validation: 0.0886448685803929]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09036539115938311		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.09036539115938311 | validation: 0.11538477427102642]
	TIME [epoch: 14.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09784064928146145		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.09784064928146145 | validation: 0.10934134788965444]
	TIME [epoch: 14.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09682366773458537		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.09682366773458537 | validation: 0.10753384044813266]
	TIME [epoch: 14.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09498217143561474		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.09498217143561474 | validation: 0.09436232147784443]
	TIME [epoch: 14.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09152576163547314		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.09152576163547314 | validation: 0.09465843664506829]
	TIME [epoch: 14.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10234096915757224		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.10234096915757224 | validation: 0.1438905252159716]
	TIME [epoch: 14.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13093864918610987		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.13093864918610987 | validation: 0.10668154457724577]
	TIME [epoch: 14.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0956666309168438		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0956666309168438 | validation: 0.10459059722944666]
	TIME [epoch: 14.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09979036581754516		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.09979036581754516 | validation: 0.09495270144822933]
	TIME [epoch: 14.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1579191428854306		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.1579191428854306 | validation: 0.18199285830931608]
	TIME [epoch: 138 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14469322506278529		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.14469322506278529 | validation: 0.13176078383752118]
	TIME [epoch: 32.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12520475396575645		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.12520475396575645 | validation: 0.15030539297975581]
	TIME [epoch: 32.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13676551519892385		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.13676551519892385 | validation: 0.13540884422786204]
	TIME [epoch: 32.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12404267500056337		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.12404267500056337 | validation: 0.14303917816239062]
	TIME [epoch: 32.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13036860755389426		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.13036860755389426 | validation: 0.12715084230437937]
	TIME [epoch: 32.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11901886596215572		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.11901886596215572 | validation: 0.11676194104707752]
	TIME [epoch: 32.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12014105487364948		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.12014105487364948 | validation: 0.12360814115319484]
	TIME [epoch: 32.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12079504547182485		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.12079504547182485 | validation: 0.12862373322011675]
	TIME [epoch: 32.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11783927452769888		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.11783927452769888 | validation: 0.12755820480887772]
	TIME [epoch: 32.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.115036584599897		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.115036584599897 | validation: 0.11676414261179519]
	TIME [epoch: 32.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1419983461710343		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.1419983461710343 | validation: 0.17481591489946194]
	TIME [epoch: 32.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13552363211379623		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.13552363211379623 | validation: 0.1185475420932528]
	TIME [epoch: 32.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11236471510302265		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.11236471510302265 | validation: 0.11548924765943225]
	TIME [epoch: 32.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11223036407826464		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.11223036407826464 | validation: 0.1138151478271862]
	TIME [epoch: 32.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10931507051531528		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.10931507051531528 | validation: 0.11875663315189071]
	TIME [epoch: 32 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11816576052850428		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.11816576052850428 | validation: 0.12470692194334045]
	TIME [epoch: 32.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13923715141842385		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.13923715141842385 | validation: 0.2688544341764292]
	TIME [epoch: 32.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17355393147373163		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.17355393147373163 | validation: 0.14309206011707232]
	TIME [epoch: 32.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11727641540753239		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.11727641540753239 | validation: 0.1184635397127294]
	TIME [epoch: 32 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.113558307247876		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.113558307247876 | validation: 0.12401992052182531]
	TIME [epoch: 32.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10902808224398816		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.10902808224398816 | validation: 0.12071554976057546]
	TIME [epoch: 32.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11755102923186327		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.11755102923186327 | validation: 0.12438028603930577]
	TIME [epoch: 32.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10629681441338798		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.10629681441338798 | validation: 0.10631386950537944]
	TIME [epoch: 32.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10263000120595205		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.10263000120595205 | validation: 0.1170901381240542]
	TIME [epoch: 32.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12891238501897911		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.12891238501897911 | validation: 0.12944143407458064]
	TIME [epoch: 32.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11594132464500018		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.11594132464500018 | validation: 0.11390690197897432]
	TIME [epoch: 32.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10198012736781781		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.10198012736781781 | validation: 0.10855696962079539]
	TIME [epoch: 32.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1059602485220674		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.1059602485220674 | validation: 0.10408737717428171]
	TIME [epoch: 32 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10740155667885817		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.10740155667885817 | validation: 0.0924358168959272]
	TIME [epoch: 32.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09837096796538752		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.09837096796538752 | validation: 0.10672419972337362]
	TIME [epoch: 32.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10244306743147351		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.10244306743147351 | validation: 0.14697035856633206]
	TIME [epoch: 32.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11631896059356446		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.11631896059356446 | validation: 0.11310100765371048]
	TIME [epoch: 32.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09977572641505242		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.09977572641505242 | validation: 0.10230917168957099]
	TIME [epoch: 32 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1326368589611091		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.1326368589611091 | validation: 0.1271377256488751]
	TIME [epoch: 32.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11065930919343832		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.11065930919343832 | validation: 0.12646033083136385]
	TIME [epoch: 32.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10537537348123693		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.10537537348123693 | validation: 0.10623598266620964]
	TIME [epoch: 32.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09910961295235854		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.09910961295235854 | validation: 0.09971774034986722]
	TIME [epoch: 32 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488414582772632		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.1488414582772632 | validation: 0.10908181103296816]
	TIME [epoch: 32.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10664285505197187		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.10664285505197187 | validation: 0.09520841319226508]
	TIME [epoch: 32.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11014144702312341		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.11014144702312341 | validation: 0.16637652226538838]
	TIME [epoch: 32.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14325386405349258		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.14325386405349258 | validation: 0.11576449877601419]
	TIME [epoch: 32.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1077851350044857		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.1077851350044857 | validation: 0.10026036623991466]
	TIME [epoch: 32 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09911865331609976		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.09911865331609976 | validation: 0.10124351036709946]
	TIME [epoch: 32.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09770018891890207		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.09770018891890207 | validation: 0.10185969557547328]
	TIME [epoch: 32.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09985508241933229		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.09985508241933229 | validation: 0.09639432668962622]
	TIME [epoch: 32 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10021375704044466		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.10021375704044466 | validation: 0.1202800518524265]
	TIME [epoch: 32.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10303121462310019		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.10303121462310019 | validation: 0.10365912348492186]
	TIME [epoch: 32.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10450831723791712		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.10450831723791712 | validation: 0.10940908418157425]
	TIME [epoch: 32.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10164133571244344		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.10164133571244344 | validation: 0.09415211024696943]
	TIME [epoch: 32.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10201142891926249		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.10201142891926249 | validation: 0.11635531772773348]
	TIME [epoch: 32 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1036424228668473		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.1036424228668473 | validation: 0.1177187359429267]
	TIME [epoch: 32 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10039342737400474		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.10039342737400474 | validation: 0.10673136994478497]
	TIME [epoch: 32.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13340813555079026		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.13340813555079026 | validation: 0.2873122074652876]
	TIME [epoch: 32.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2049882689989366		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.2049882689989366 | validation: 0.14774027706282994]
	TIME [epoch: 32.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12386899419614797		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.12386899419614797 | validation: 0.12503680266239067]
	TIME [epoch: 32 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10848441186430384		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.10848441186430384 | validation: 0.11389877129812868]
	TIME [epoch: 32.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10229608386808482		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.10229608386808482 | validation: 0.11922971490724832]
	TIME [epoch: 32.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10409549743849536		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.10409549743849536 | validation: 0.10458891148358206]
	TIME [epoch: 32.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09715496447290964		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.09715496447290964 | validation: 0.09643625747243349]
	TIME [epoch: 32.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10330454424535077		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.10330454424535077 | validation: 0.09633247933175336]
	TIME [epoch: 32.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0968809293611399		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0968809293611399 | validation: 0.09709765516405125]
	TIME [epoch: 32.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10492726608565575		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.10492726608565575 | validation: 0.10453497076508858]
	TIME [epoch: 32.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09558251387588339		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.09558251387588339 | validation: 0.0974730026149597]
	TIME [epoch: 32.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09755640611201088		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.09755640611201088 | validation: 0.1126787229113102]
	TIME [epoch: 32 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1049287550805991		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.1049287550805991 | validation: 0.11177276153760436]
	TIME [epoch: 32 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0964196497717903		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.0964196497717903 | validation: 0.11769760669039836]
	TIME [epoch: 32.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09999850727507789		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.09999850727507789 | validation: 0.09630685601412656]
	TIME [epoch: 32.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10461829120860404		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.10461829120860404 | validation: 0.10685386462062954]
	TIME [epoch: 32.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14043302268902008		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.14043302268902008 | validation: 0.12042809571729501]
	TIME [epoch: 32.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10203152114954081		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.10203152114954081 | validation: 0.09296550324021173]
	TIME [epoch: 32.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0965037513111782		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.0965037513111782 | validation: 0.1043120469965905]
	TIME [epoch: 32.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09142571481812309		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.09142571481812309 | validation: 0.1097291830960341]
	TIME [epoch: 32.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10442040638899537		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.10442040638899537 | validation: 0.10194217325213217]
	TIME [epoch: 32.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09714309860449119		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.09714309860449119 | validation: 0.1045569116508881]
	TIME [epoch: 32.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09801653715479566		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.09801653715479566 | validation: 0.10506957721410959]
	TIME [epoch: 32.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0996307047455173		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.0996307047455173 | validation: 0.10146123603137305]
	TIME [epoch: 32.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12112055615660444		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.12112055615660444 | validation: 0.1219594851682265]
	TIME [epoch: 32.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1438515230210772		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.1438515230210772 | validation: 0.13083333929489627]
	TIME [epoch: 32.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1156426008401244		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.1156426008401244 | validation: 0.10275242705327697]
	TIME [epoch: 32.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10534075329873632		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.10534075329873632 | validation: 0.09439701544628788]
	TIME [epoch: 32.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09204612382397863		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.09204612382397863 | validation: 0.106274530545301]
	TIME [epoch: 32.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10375155293777666		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.10375155293777666 | validation: 0.08792692472996273]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09464932736583132		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.09464932736583132 | validation: 0.09607351958984564]
	TIME [epoch: 32.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09576857865124382		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.09576857865124382 | validation: 0.09831266473035902]
	TIME [epoch: 32.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0948862324181026		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.0948862324181026 | validation: 0.09459208568271493]
	TIME [epoch: 32.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09626000126944581		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.09626000126944581 | validation: 0.09167156475520916]
	TIME [epoch: 32.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10274886482756183		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.10274886482756183 | validation: 0.0958862604502777]
	TIME [epoch: 32.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10000466231663366		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.10000466231663366 | validation: 0.09725656758798804]
	TIME [epoch: 32.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09156313952243422		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.09156313952243422 | validation: 0.09524609703487391]
	TIME [epoch: 32.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09691340197866574		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.09691340197866574 | validation: 0.10039218651951325]
	TIME [epoch: 32.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10092950931088975		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.10092950931088975 | validation: 0.10457563871685242]
	TIME [epoch: 32.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09991903526666107		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.09991903526666107 | validation: 0.12996863052632981]
	TIME [epoch: 32.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09255552235033121		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.09255552235033121 | validation: 0.10451363069061628]
	TIME [epoch: 32.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1049148902406029		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.1049148902406029 | validation: 0.09929428567905078]
	TIME [epoch: 32.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09916957710339261		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.09916957710339261 | validation: 0.10678746149883109]
	TIME [epoch: 32.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11390643829194116		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.11390643829194116 | validation: 0.09942938574277113]
	TIME [epoch: 32.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09206446453452599		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.09206446453452599 | validation: 0.09120050539397404]
	TIME [epoch: 32.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09348238407936203		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.09348238407936203 | validation: 0.09570151177024663]
	TIME [epoch: 32.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10376041282868487		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.10376041282868487 | validation: 0.09849416188569105]
	TIME [epoch: 32.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09027072231871901		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.09027072231871901 | validation: 0.09706968362386795]
	TIME [epoch: 32.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09844421445909397		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.09844421445909397 | validation: 0.0936243338486478]
	TIME [epoch: 32.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09665734172919642		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.09665734172919642 | validation: 0.09382045414536615]
	TIME [epoch: 32.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1112523906378902		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.1112523906378902 | validation: 0.10232031706008209]
	TIME [epoch: 32.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10719048620675206		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.10719048620675206 | validation: 0.12105316352327503]
	TIME [epoch: 32.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09789344164348646		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.09789344164348646 | validation: 0.12634530262223687]
	TIME [epoch: 32.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0974695334416916		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0974695334416916 | validation: 0.1109417389098648]
	TIME [epoch: 32.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12912317132390233		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.12912317132390233 | validation: 0.1150023207309927]
	TIME [epoch: 32.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10041967040637127		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.10041967040637127 | validation: 0.10799447095441272]
	TIME [epoch: 32.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11747015962034607		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.11747015962034607 | validation: 0.11508795637370552]
	TIME [epoch: 32.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09746484064544789		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.09746484064544789 | validation: 0.1132125347772468]
	TIME [epoch: 32.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12627373602685654		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.12627373602685654 | validation: 0.1252872842284086]
	TIME [epoch: 32.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13827587040783587		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.13827587040783587 | validation: 0.10191973070257207]
	TIME [epoch: 32.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09789296495166655		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.09789296495166655 | validation: 0.10537617759496754]
	TIME [epoch: 32.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09408451216128284		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.09408451216128284 | validation: 0.14258299926200346]
	TIME [epoch: 32.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11130468182146021		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.11130468182146021 | validation: 0.11108042689655047]
	TIME [epoch: 32.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09753441749220058		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.09753441749220058 | validation: 0.10205749839885972]
	TIME [epoch: 32.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09379324094494666		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.09379324094494666 | validation: 0.09916642571919355]
	TIME [epoch: 32.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09128267689941222		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.09128267689941222 | validation: 0.09042437918902371]
	TIME [epoch: 32.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10036855407017409		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.10036855407017409 | validation: 0.1064886764036459]
	TIME [epoch: 32.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09325556978449773		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.09325556978449773 | validation: 0.33823435952575515]
	TIME [epoch: 32.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24973160208155415		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.24973160208155415 | validation: 0.11604664807951548]
	TIME [epoch: 32.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10735487707246538		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.10735487707246538 | validation: 0.12297978564723937]
	TIME [epoch: 32.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1066998190043626		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.1066998190043626 | validation: 0.08279951629886335]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08976733937189976		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.08976733937189976 | validation: 0.08852051337146857]
	TIME [epoch: 32.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09303020986903168		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.09303020986903168 | validation: 0.09985757095597761]
	TIME [epoch: 32.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08948517843148376		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.08948517843148376 | validation: 0.09459188667128626]
	TIME [epoch: 32.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18417978634899881		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.18417978634899881 | validation: 0.11830335354459529]
	TIME [epoch: 32.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11002673406450184		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.11002673406450184 | validation: 0.0834653883248618]
	TIME [epoch: 32.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08802369501606029		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.08802369501606029 | validation: 0.09733363631742248]
	TIME [epoch: 32.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09043880621691369		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.09043880621691369 | validation: 0.08711060254825168]
	TIME [epoch: 32.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0903138272379461		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.0903138272379461 | validation: 0.09137104575479169]
	TIME [epoch: 32.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15949494033094616		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.15949494033094616 | validation: 0.3468024690752385]
	TIME [epoch: 32.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20043232661405241		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.20043232661405241 | validation: 0.19831577440182024]
	TIME [epoch: 32.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11931251582984885		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.11931251582984885 | validation: 0.0959840156268636]
	TIME [epoch: 32.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09073762834132351		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.09073762834132351 | validation: 0.09616122867439493]
	TIME [epoch: 32.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08870531169533383		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.08870531169533383 | validation: 0.09070962298824112]
	TIME [epoch: 32.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0904624960783244		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0904624960783244 | validation: 0.11294429607355458]
	TIME [epoch: 32.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09501177325434447		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.09501177325434447 | validation: 0.09853390861763461]
	TIME [epoch: 32.1 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09226230182065139		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.09226230182065139 | validation: 0.10188397689928932]
	TIME [epoch: 32.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08996606316684205		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.08996606316684205 | validation: 0.09005302750950345]
	TIME [epoch: 32.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09508377666204096		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.09508377666204096 | validation: 0.09971988761742603]
	TIME [epoch: 32.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09371082956685597		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.09371082956685597 | validation: 0.09987290586525586]
	TIME [epoch: 32.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10848487705929682		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.10848487705929682 | validation: 0.1295857899243827]
	TIME [epoch: 32.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1288906254549808		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.1288906254549808 | validation: 0.11240857683041083]
	TIME [epoch: 32.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12229177951346018		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.12229177951346018 | validation: 0.10049740400108635]
	TIME [epoch: 32.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15098476034424166		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.15098476034424166 | validation: 0.18775454328368374]
	TIME [epoch: 32.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15656045824216913		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.15656045824216913 | validation: 0.10740683683203242]
	TIME [epoch: 32.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10504978579423283		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.10504978579423283 | validation: 0.1006650302512831]
	TIME [epoch: 32.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09536926978866297		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.09536926978866297 | validation: 0.0967932177109188]
	TIME [epoch: 32.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09304641523330624		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.09304641523330624 | validation: 0.09976641585221202]
	TIME [epoch: 32.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09308843295256626		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.09308843295256626 | validation: 0.1038303714144363]
	TIME [epoch: 32.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09134669029536513		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.09134669029536513 | validation: 0.10059995457364329]
	TIME [epoch: 32.1 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13374724123922535		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.13374724123922535 | validation: 0.17115291987781361]
	TIME [epoch: 32.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13641021075767285		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.13641021075767285 | validation: 0.12594054975830538]
	TIME [epoch: 32.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11103311488051497		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.11103311488051497 | validation: 0.11930896155918627]
	TIME [epoch: 32.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1057953670596461		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.1057953670596461 | validation: 0.11035129368072016]
	TIME [epoch: 32.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10232595489022864		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.10232595489022864 | validation: 0.10561310142829033]
	TIME [epoch: 32.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09059991340527988		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.09059991340527988 | validation: 0.09137321782360325]
	TIME [epoch: 32.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0919757935504974		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.0919757935504974 | validation: 0.0990515444170594]
	TIME [epoch: 32.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0913584475504527		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0913584475504527 | validation: 0.09209407958757636]
	TIME [epoch: 32.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09555492851785188		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.09555492851785188 | validation: 0.12814714668792626]
	TIME [epoch: 32.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10303995157050722		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.10303995157050722 | validation: 0.11836026366541014]
	TIME [epoch: 32.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10308297491001923		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.10308297491001923 | validation: 0.1353667564832386]
	TIME [epoch: 32.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11078347660183124		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.11078347660183124 | validation: 0.11013460573062095]
	TIME [epoch: 32.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09051995858883392		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.09051995858883392 | validation: 0.09384390828111353]
	TIME [epoch: 32.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0892794092485946		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0892794092485946 | validation: 0.09056457413694188]
	TIME [epoch: 32.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09175553288612154		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.09175553288612154 | validation: 0.09755761824580747]
	TIME [epoch: 32.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09876053052882118		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.09876053052882118 | validation: 0.1013201242334047]
	TIME [epoch: 32.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0979256170845822		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.0979256170845822 | validation: 0.10402830104610862]
	TIME [epoch: 32.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09408085420878463		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.09408085420878463 | validation: 0.09303151507661964]
	TIME [epoch: 32.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08845239366110602		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.08845239366110602 | validation: 0.09623996457910641]
	TIME [epoch: 32.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09362198894005871		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.09362198894005871 | validation: 0.09262681289765748]
	TIME [epoch: 32.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.090868005147441		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.090868005147441 | validation: 0.08999678891594456]
	TIME [epoch: 32.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10184303971130668		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.10184303971130668 | validation: 0.113522998650203]
	TIME [epoch: 32.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09813288985814314		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.09813288985814314 | validation: 0.10730324180151236]
	TIME [epoch: 32.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09180107457191009		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.09180107457191009 | validation: 0.10449227611265216]
	TIME [epoch: 32.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10457669655783548		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.10457669655783548 | validation: 0.10215914543179264]
	TIME [epoch: 32.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09582654493202813		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.09582654493202813 | validation: 0.0984669532154478]
	TIME [epoch: 32.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0897104637147737		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.0897104637147737 | validation: 0.09100056204749842]
	TIME [epoch: 32.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0940589463850589		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.0940589463850589 | validation: 0.09244759405379083]
	TIME [epoch: 32.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08751378518013471		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.08751378518013471 | validation: 0.08834239846062372]
	TIME [epoch: 32.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09439971129126287		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.09439971129126287 | validation: 0.09546540424221164]
	TIME [epoch: 32.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09600327294055497		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.09600327294055497 | validation: 0.09649720403732262]
	TIME [epoch: 32.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09715685078870305		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.09715685078870305 | validation: 0.09903949610668206]
	TIME [epoch: 32.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09324281123091661		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.09324281123091661 | validation: 0.09009091629215431]
	TIME [epoch: 32.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08921598150624016		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.08921598150624016 | validation: 0.0950619434589409]
	TIME [epoch: 32.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10526060786470143		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.10526060786470143 | validation: 0.11726514416437045]
	TIME [epoch: 32.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09262864070273812		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.09262864070273812 | validation: 0.09157607272944113]
	TIME [epoch: 32.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0870485368206566		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.0870485368206566 | validation: 0.08919298864544123]
	TIME [epoch: 32.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08830321944449923		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.08830321944449923 | validation: 0.110273787918784]
	TIME [epoch: 32.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09604722189327161		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.09604722189327161 | validation: 0.09773627736944362]
	TIME [epoch: 32.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09572965412910361		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.09572965412910361 | validation: 0.09499646294892888]
	TIME [epoch: 32.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09787379980843067		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.09787379980843067 | validation: 0.10377170147534469]
	TIME [epoch: 32.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.099075823087875		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.099075823087875 | validation: 0.09872025532403675]
	TIME [epoch: 32.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10446035429706602		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.10446035429706602 | validation: 0.11926881094931541]
	TIME [epoch: 32.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10614726231917491		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.10614726231917491 | validation: 0.10200256293214095]
	TIME [epoch: 32.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09047452004980244		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.09047452004980244 | validation: 0.0913800300852948]
	TIME [epoch: 32.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08876468629310011		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.08876468629310011 | validation: 0.09051426833781545]
	TIME [epoch: 32.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13696895844525672		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.13696895844525672 | validation: 0.11462075905456948]
	TIME [epoch: 32.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10041152562026402		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.10041152562026402 | validation: 0.08994379682910963]
	TIME [epoch: 32.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08953506874442006		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.08953506874442006 | validation: 0.08625572781931357]
	TIME [epoch: 32.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08586347916794261		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.08586347916794261 | validation: 0.08875460057655032]
	TIME [epoch: 32.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08885147701813609		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.08885147701813609 | validation: 0.09415990046900945]
	TIME [epoch: 32.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08859692833143622		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.08859692833143622 | validation: 0.09417102792197558]
	TIME [epoch: 32.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0892643673235957		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0892643673235957 | validation: 0.09112026709292348]
	TIME [epoch: 32.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0991494630124438		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.0991494630124438 | validation: 0.08515781350075868]
	TIME [epoch: 32.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08874712098653771		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.08874712098653771 | validation: 0.0899240819254237]
	TIME [epoch: 32.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0885589208762433		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0885589208762433 | validation: 0.09583579401065281]
	TIME [epoch: 32.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08809264615897228		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.08809264615897228 | validation: 0.100338649460532]
	TIME [epoch: 32.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08965487610715718		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.08965487610715718 | validation: 0.09277728412031291]
	TIME [epoch: 32.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09148553405848246		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.09148553405848246 | validation: 0.09376943364123053]
	TIME [epoch: 32.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10782177167388086		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.10782177167388086 | validation: 0.10105591626359559]
	TIME [epoch: 32.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10194947128691047		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.10194947128691047 | validation: 0.1033466673027188]
	TIME [epoch: 32.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09051123007340248		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.09051123007340248 | validation: 0.09248740170491755]
	TIME [epoch: 32.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09187738716506731		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.09187738716506731 | validation: 0.09427752382645121]
	TIME [epoch: 32.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08928425184517713		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.08928425184517713 | validation: 0.09851430404945721]
	TIME [epoch: 32.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09223387671980406		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.09223387671980406 | validation: 0.17135674164161538]
	TIME [epoch: 32.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10701395097415933		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.10701395097415933 | validation: 0.099305549010429]
	TIME [epoch: 32.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08810758345888556		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.08810758345888556 | validation: 0.09364636212647819]
	TIME [epoch: 32.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09047046008343013		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.09047046008343013 | validation: 0.09194094886851203]
	TIME [epoch: 32.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0869528311449018		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.0869528311449018 | validation: 0.11028688581749947]
	TIME [epoch: 32.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09320815535456227		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.09320815535456227 | validation: 0.12588088991360805]
	TIME [epoch: 32.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09986407029020825		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.09986407029020825 | validation: 0.10317294615787419]
	TIME [epoch: 32.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0915715498122796		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.0915715498122796 | validation: 0.09701836508192435]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240704_134101/states/model_phi1_1a_v_mmd1_fix_noise_large_725.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 13620.532 seconds.
