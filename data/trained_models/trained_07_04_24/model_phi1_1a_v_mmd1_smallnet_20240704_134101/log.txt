Args:
Namespace(name='model_phi1_1a_v_mmd1_smallnet', outdir='out/model_training/model_phi1_1a_v_mmd1_smallnet', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[8, 16, 8], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3550534501

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.038545499030635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.038545499030635 | validation: 6.150009655527109]
	TIME [epoch: 115 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.366097466073397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.366097466073397 | validation: 6.053830377090493]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.175522566022352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.175522566022352 | validation: 5.967477012726116]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.047157730714503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.047157730714503 | validation: 5.882864572359967]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.960127791591308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.960127791591308 | validation: 5.769610700344192]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.856564551106367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.856564551106367 | validation: 5.699189294720839]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.775474882275349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.775474882275349 | validation: 5.615009909976592]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.672550907431063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.672550907431063 | validation: 5.523347916351738]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.559897281795852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.559897281795852 | validation: 5.356133939695891]
	TIME [epoch: 5.07 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.289699646218184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.289699646218184 | validation: 4.327889493051259]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3665229634180287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3665229634180287 | validation: 3.6654961815394365]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8461691207878848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8461691207878848 | validation: 3.4995209769448437]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6310867862763225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6310867862763225 | validation: 3.146067831233045]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4398119650678405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4398119650678405 | validation: 3.0629287255871986]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.341004543352328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.341004543352328 | validation: 2.981914251533769]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4169035412287494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4169035412287494 | validation: 2.9164685170580373]
	TIME [epoch: 5.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231537560220448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.231537560220448 | validation: 2.78640652494814]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.12832283071135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.12832283071135 | validation: 2.832801588492481]
	TIME [epoch: 5.01 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1204989460116153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1204989460116153 | validation: 2.6647310760877514]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.074107555819607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.074107555819607 | validation: 2.750928153159796]
	TIME [epoch: 5.04 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0716170692984006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0716170692984006 | validation: 2.6041980412910544]
	TIME [epoch: 5.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.054307469402424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.054307469402424 | validation: 2.531652565478859]
	TIME [epoch: 5.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9039396656137832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9039396656137832 | validation: 2.4322170181098586]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.128126709615059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.128126709615059 | validation: 2.4780694427733048]
	TIME [epoch: 5.04 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.300715748483768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.300715748483768 | validation: 2.4127852447000127]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9771691187086247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9771691187086247 | validation: 2.452847823896928]
	TIME [epoch: 5.02 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536311000128332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8536311000128332 | validation: 2.3095580493527703]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.878279534727212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.878279534727212 | validation: 2.397101230167083]
	TIME [epoch: 5.03 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.824724419100761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.824724419100761 | validation: 2.2954047477553523]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7993192494964314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7993192494964314 | validation: 2.4163536650941415]
	TIME [epoch: 5.04 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.810640510610345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.810640510610345 | validation: 2.246901348048933]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7986882295481927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7986882295481927 | validation: 2.3126894625389847]
	TIME [epoch: 5.02 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.765588259961673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.765588259961673 | validation: 2.4073060604589713]
	TIME [epoch: 5.01 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7414750322394303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7414750322394303 | validation: 2.249464687246869]
	TIME [epoch: 5.01 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7221937145756345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7221937145756345 | validation: 2.339726223999896]
	TIME [epoch: 5.01 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7459317970010573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7459317970010573 | validation: 2.2337330933252257]
	TIME [epoch: 5.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7046612947212294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7046612947212294 | validation: 2.334116862818285]
	TIME [epoch: 5.04 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6909362958447964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6909362958447964 | validation: 2.267499217760574]
	TIME [epoch: 5.03 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6727873508700573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6727873508700573 | validation: 2.2142462510503975]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6930515248169584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6930515248169584 | validation: 2.310933980160766]
	TIME [epoch: 5.02 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6354428467360664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6354428467360664 | validation: 2.273386606176145]
	TIME [epoch: 5.01 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6580123080422764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6580123080422764 | validation: 2.2024703315070595]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6358027582878272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6358027582878272 | validation: 2.172224538374552]
	TIME [epoch: 5.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6426008997476935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6426008997476935 | validation: 2.135969099846962]
	TIME [epoch: 5.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6186460480193656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6186460480193656 | validation: 2.1471498872958]
	TIME [epoch: 5.02 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6015228634139493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6015228634139493 | validation: 2.2309944765499603]
	TIME [epoch: 5.02 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.587992590746195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.587992590746195 | validation: 2.3675190132015653]
	TIME [epoch: 5.02 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.580561012347082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.580561012347082 | validation: 2.385303843894424]
	TIME [epoch: 5.03 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.587259586276316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.587259586276316 | validation: 2.206422383772755]
	TIME [epoch: 5.02 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6321032291926032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6321032291926032 | validation: 2.1341946330522874]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.532940561781797		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.532940561781797 | validation: 2.216980889532742]
	TIME [epoch: 5.05 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5737057471901799		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.5737057471901799 | validation: 2.1300300490776483]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.45877168349741		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.45877168349741 | validation: 2.4061427198686545]
	TIME [epoch: 5.01 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5635821529354572		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.5635821529354572 | validation: 2.2295816141074676]
	TIME [epoch: 4.99 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5556971943416587		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.5556971943416587 | validation: 2.1686901571395607]
	TIME [epoch: 5 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4912582519478077		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.4912582519478077 | validation: 2.2273710602333905]
	TIME [epoch: 5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.518025528472835		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.518025528472835 | validation: 2.183015268651209]
	TIME [epoch: 4.99 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4857559721040745		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.4857559721040745 | validation: 2.072187953302607]
	TIME [epoch: 5.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3964149225684532		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.3964149225684532 | validation: 2.069418541735815]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5764430921677446		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.5764430921677446 | validation: 2.085498172783513]
	TIME [epoch: 5 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4700246600814482		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.4700246600814482 | validation: 2.0668854084673702]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4871749056141346		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.4871749056141346 | validation: 2.1116680662907257]
	TIME [epoch: 5 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5106543605696547		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.5106543605696547 | validation: 2.081707775480831]
	TIME [epoch: 5 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4291779613863838		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.4291779613863838 | validation: 2.1280139537262652]
	TIME [epoch: 5 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4033053218447171		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.4033053218447171 | validation: 2.2521498058025964]
	TIME [epoch: 5.05 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.511662388841986		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 1.511662388841986 | validation: 2.1389386428580224]
	TIME [epoch: 5.01 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.490840942601937		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.490840942601937 | validation: 2.0681562009234917]
	TIME [epoch: 5.01 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4144851595587729		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.4144851595587729 | validation: 2.110151127602964]
	TIME [epoch: 5.02 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3820199362552876		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.3820199362552876 | validation: 2.255002033445734]
	TIME [epoch: 5.01 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5506014056794186		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 1.5506014056794186 | validation: 2.105085262721688]
	TIME [epoch: 5.01 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4173215544074478		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 1.4173215544074478 | validation: 2.0703916536984064]
	TIME [epoch: 5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3700349153908		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 1.3700349153908 | validation: 2.2431087049746297]
	TIME [epoch: 5.04 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5182503821046893		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.5182503821046893 | validation: 2.0567933483238754]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.429143164644925		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 1.429143164644925 | validation: 2.0331629910325795]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4023392363948572		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 1.4023392363948572 | validation: 2.0787747196103714]
	TIME [epoch: 5 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4200314685615103		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 1.4200314685615103 | validation: 2.039960357712677]
	TIME [epoch: 5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.432704232532191		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 1.432704232532191 | validation: 2.1228347655183444]
	TIME [epoch: 5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3961726572334356		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 1.3961726572334356 | validation: 2.078952159858395]
	TIME [epoch: 5 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3947351884164798		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.3947351884164798 | validation: 2.0195228783293206]
	TIME [epoch: 5.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4156914529363023		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 1.4156914529363023 | validation: 2.071246483333354]
	TIME [epoch: 5.02 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3706897348383227		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 1.3706897348383227 | validation: 2.154960470395187]
	TIME [epoch: 4.99 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4248617411505344		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 1.4248617411505344 | validation: 2.0208068254769116]
	TIME [epoch: 4.99 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3218118291151213		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 1.3218118291151213 | validation: 2.017376320184722]
	TIME [epoch: 4.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3377984205702176		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 1.3377984205702176 | validation: 2.0304176750461087]
	TIME [epoch: 4.99 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3815505071222152		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 1.3815505071222152 | validation: 2.015151245993529]
	TIME [epoch: 4.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3460983080277038		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 1.3460983080277038 | validation: 2.080289649761209]
	TIME [epoch: 4.99 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.36565891237486		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 1.36565891237486 | validation: 2.037672611061624]
	TIME [epoch: 5.02 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.418974730952015		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 1.418974730952015 | validation: 1.9977359545997477]
	TIME [epoch: 4.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.318127077261476		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 1.318127077261476 | validation: 1.999971775031956]
	TIME [epoch: 5.01 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.294124441063702		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 1.294124441063702 | validation: 2.002831524631678]
	TIME [epoch: 5.01 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3059856464479316		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 1.3059856464479316 | validation: 2.0687844335918015]
	TIME [epoch: 5.01 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3833658992948805		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 1.3833658992948805 | validation: 1.971654346130105]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3326791948598296		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 1.3326791948598296 | validation: 1.9770079077588463]
	TIME [epoch: 5.01 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3084791080914457		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 1.3084791080914457 | validation: 1.9733090857013957]
	TIME [epoch: 5.05 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2721350372817266		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 1.2721350372817266 | validation: 1.9861003030312165]
	TIME [epoch: 5.01 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.260453879726809		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 1.260453879726809 | validation: 1.9487367133619045]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3103715187333036		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 1.3103715187333036 | validation: 1.9366760742938904]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2128185268345741		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 1.2128185268345741 | validation: 1.8949221820000415]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2335563883855694		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.2335563883855694 | validation: 1.8349718747446047]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2064689579039534		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 1.2064689579039534 | validation: 1.866607140313846]
	TIME [epoch: 4.99 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2732804704709686		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 1.2732804704709686 | validation: 1.7660661070662376]
	TIME [epoch: 5.03 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1688824286075659		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 1.1688824286075659 | validation: 1.8245821908873756]
	TIME [epoch: 5.02 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1067686453460763		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 1.1067686453460763 | validation: 1.5476522367131529]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1192934462363726		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 1.1192934462363726 | validation: 1.4437601682361216]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.032596307792552		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 1.032596307792552 | validation: 1.1233763173193672]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7553082476114246		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.7553082476114246 | validation: 0.6956082400362649]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5948118988296396		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.5948118988296396 | validation: 0.44655667954535583]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46188340280119217		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.46188340280119217 | validation: 0.44904348663015786]
	TIME [epoch: 5.02 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299081136448972		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3299081136448972 | validation: 0.2915213738283994]
	TIME [epoch: 4.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958698386633276		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2958698386633276 | validation: 0.42584442835570446]
	TIME [epoch: 4.99 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3445080951400147		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3445080951400147 | validation: 0.3634911696492986]
	TIME [epoch: 4.98 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27998449588531993		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.27998449588531993 | validation: 0.2215902356014487]
	TIME [epoch: 4.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32801826858306105		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.32801826858306105 | validation: 0.4545550368708272]
	TIME [epoch: 4.98 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35694077108784805		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.35694077108784805 | validation: 0.3363897542050559]
	TIME [epoch: 4.99 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2792246283469714		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2792246283469714 | validation: 0.35307847553214133]
	TIME [epoch: 5.01 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32141590782821		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.32141590782821 | validation: 0.2904422769042534]
	TIME [epoch: 4.98 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2816737421255556		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.2816737421255556 | validation: 0.30182869864499945]
	TIME [epoch: 4.99 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2413858134878985		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.2413858134878985 | validation: 0.21554749799710893]
	TIME [epoch: 5.27 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2543944624162502		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.2543944624162502 | validation: 0.27065134287638903]
	TIME [epoch: 5.01 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32555726405303553		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.32555726405303553 | validation: 0.4350142243245019]
	TIME [epoch: 5.01 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2943152500731451		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2943152500731451 | validation: 0.2088983805108404]
	TIME [epoch: 5.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3146659860355372		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3146659860355372 | validation: 0.3169039137451165]
	TIME [epoch: 5.05 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2501715858333067		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.2501715858333067 | validation: 0.43844550360526746]
	TIME [epoch: 4.99 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29571682626178547		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.29571682626178547 | validation: 0.21249415475933509]
	TIME [epoch: 4.99 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26651525372395635		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.26651525372395635 | validation: 0.23054405509149672]
	TIME [epoch: 4.99 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23060635494577392		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.23060635494577392 | validation: 0.37119457032585634]
	TIME [epoch: 4.99 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803392725604954		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2803392725604954 | validation: 0.3353687609937837]
	TIME [epoch: 4.99 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26835810761343065		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.26835810761343065 | validation: 0.21182463152434566]
	TIME [epoch: 4.99 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3165462239628726		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.3165462239628726 | validation: 0.2288560642276054]
	TIME [epoch: 5.02 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23661763383142753		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.23661763383142753 | validation: 0.40663988406142604]
	TIME [epoch: 5.01 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2911774410703946		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.2911774410703946 | validation: 0.23025249743711368]
	TIME [epoch: 4.99 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21615102881902779		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.21615102881902779 | validation: 0.19302105134175887]
	TIME [epoch: 4.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23583351279877993		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.23583351279877993 | validation: 0.25933529953057066]
	TIME [epoch: 5.01 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22624531840094989		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.22624531840094989 | validation: 0.3365717121583731]
	TIME [epoch: 5 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20071391093417645		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.20071391093417645 | validation: 0.33674384207822916]
	TIME [epoch: 5.01 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30186489071424843		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.30186489071424843 | validation: 0.33544478855562043]
	TIME [epoch: 5.01 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.252929399182438		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.252929399182438 | validation: 0.2448618909146186]
	TIME [epoch: 5.04 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2121521089277777		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.2121521089277777 | validation: 0.28751117252542935]
	TIME [epoch: 5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19522305865236822		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.19522305865236822 | validation: 0.24380668232721417]
	TIME [epoch: 5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28420811983265293		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.28420811983265293 | validation: 0.3414711109754687]
	TIME [epoch: 5 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19946253266456496		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.19946253266456496 | validation: 0.2008930499592232]
	TIME [epoch: 5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25378180418995294		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.25378180418995294 | validation: 0.40338136109436085]
	TIME [epoch: 5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2379358196031012		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.2379358196031012 | validation: 0.2203363418036646]
	TIME [epoch: 5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2053900322614786		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2053900322614786 | validation: 0.31424277878082363]
	TIME [epoch: 5.03 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24670729916775272		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.24670729916775272 | validation: 0.2642831583801702]
	TIME [epoch: 5.02 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20713190380924162		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.20713190380924162 | validation: 0.20481884529024938]
	TIME [epoch: 5.01 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18699163710822653		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.18699163710822653 | validation: 0.3002261371652016]
	TIME [epoch: 5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24457440737530295		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.24457440737530295 | validation: 0.1816520162113738]
	TIME [epoch: 5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18304082076996564		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.18304082076996564 | validation: 0.2059311498773181]
	TIME [epoch: 5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24323627603640044		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.24323627603640044 | validation: 0.19803946475475318]
	TIME [epoch: 4.99 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2569023232059603		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2569023232059603 | validation: 0.209451715612254]
	TIME [epoch: 5 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26043820847418486		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.26043820847418486 | validation: 0.23856115540253514]
	TIME [epoch: 5.01 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19416710645523405		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.19416710645523405 | validation: 0.21494652338335823]
	TIME [epoch: 4.99 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21579683132821004		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.21579683132821004 | validation: 0.19002367968627576]
	TIME [epoch: 4.98 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16493699473974763		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.16493699473974763 | validation: 0.20054094424327717]
	TIME [epoch: 4.98 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23742434306763213		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.23742434306763213 | validation: 0.2768738194301307]
	TIME [epoch: 4.98 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2203307343273548		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.2203307343273548 | validation: 0.2032659679411501]
	TIME [epoch: 4.98 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18230112789019454		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.18230112789019454 | validation: 0.16958443041587148]
	TIME [epoch: 4.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16240025639350059		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.16240025639350059 | validation: 0.19747339057065502]
	TIME [epoch: 5.03 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610827327304665		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2610827327304665 | validation: 0.191717295317752]
	TIME [epoch: 4.98 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18759073894241826		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.18759073894241826 | validation: 0.17646425556764317]
	TIME [epoch: 4.95 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1712450646027499		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.1712450646027499 | validation: 0.16181051385886377]
	TIME [epoch: 4.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16319014941671547		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.16319014941671547 | validation: 0.17774785776620297]
	TIME [epoch: 4.96 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1629460213173211		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.1629460213173211 | validation: 0.2898618611125888]
	TIME [epoch: 4.95 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17262013638708062		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.17262013638708062 | validation: 0.18766254164321677]
	TIME [epoch: 4.95 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20087272207616919		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.20087272207616919 | validation: 0.17912315370829784]
	TIME [epoch: 4.98 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19099305299809896		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.19099305299809896 | validation: 0.2385174315288508]
	TIME [epoch: 4.97 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15090552581949582		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.15090552581949582 | validation: 0.2373719122914027]
	TIME [epoch: 4.95 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16643444600931773		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.16643444600931773 | validation: 0.20745404553801228]
	TIME [epoch: 4.95 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1767935012208614		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1767935012208614 | validation: 0.199052768802419]
	TIME [epoch: 4.95 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24214003637159004		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.24214003637159004 | validation: 0.20638845466563477]
	TIME [epoch: 4.95 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2149552230905193		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.2149552230905193 | validation: 0.17341632150621913]
	TIME [epoch: 4.95 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17124916933931902		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.17124916933931902 | validation: 0.20769704938212685]
	TIME [epoch: 4.95 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16551044479494745		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.16551044479494745 | validation: 0.17730383794250548]
	TIME [epoch: 5 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19368090210081113		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.19368090210081113 | validation: 0.16250191843435413]
	TIME [epoch: 4.96 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17653955140022753		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.17653955140022753 | validation: 0.24633115989227972]
	TIME [epoch: 4.95 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14811479308671985		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.14811479308671985 | validation: 0.16308886006292395]
	TIME [epoch: 4.96 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14549883781336725		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.14549883781336725 | validation: 0.18484580456870242]
	TIME [epoch: 4.96 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21671401266655202		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.21671401266655202 | validation: 0.2212260141168657]
	TIME [epoch: 4.95 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1705094508650343		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1705094508650343 | validation: 0.17521003631842946]
	TIME [epoch: 4.96 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17367164666110246		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.17367164666110246 | validation: 0.18176436120371126]
	TIME [epoch: 4.97 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18617824326946258		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.18617824326946258 | validation: 0.19565775955993714]
	TIME [epoch: 4.98 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13686151955162493		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.13686151955162493 | validation: 0.14684942005733392]
	TIME [epoch: 4.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14851043659626767		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.14851043659626767 | validation: 0.23963303771800232]
	TIME [epoch: 4.96 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25089384637418066		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.25089384637418066 | validation: 0.2098713092358132]
	TIME [epoch: 4.95 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16510410748143028		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.16510410748143028 | validation: 0.1740583133029544]
	TIME [epoch: 4.95 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13789135051999682		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.13789135051999682 | validation: 0.2055654898160223]
	TIME [epoch: 4.95 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15124420266792088		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.15124420266792088 | validation: 0.17988605955553139]
	TIME [epoch: 4.95 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13880580745083168		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.13880580745083168 | validation: 0.14962490736211642]
	TIME [epoch: 4.99 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16847200619226563		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.16847200619226563 | validation: 0.2378216502435505]
	TIME [epoch: 4.95 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17725367221102103		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.17725367221102103 | validation: 0.21821691673264965]
	TIME [epoch: 4.95 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17342957374739892		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.17342957374739892 | validation: 0.24620680871956163]
	TIME [epoch: 4.95 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1548731228400022		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.1548731228400022 | validation: 0.19525755300187908]
	TIME [epoch: 4.95 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16138374104433237		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.16138374104433237 | validation: 0.14886512808571015]
	TIME [epoch: 4.95 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1752015739343748		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.1752015739343748 | validation: 0.17410648627214065]
	TIME [epoch: 4.95 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1440041845000893		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1440041845000893 | validation: 0.17241331552203243]
	TIME [epoch: 4.98 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15004502459615143		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.15004502459615143 | validation: 0.16293849753694445]
	TIME [epoch: 4.99 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1829488943487492		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.1829488943487492 | validation: 0.16758745860713864]
	TIME [epoch: 4.96 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1238268624833754		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.1238268624833754 | validation: 0.19394499194140963]
	TIME [epoch: 4.96 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15940334470403222		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.15940334470403222 | validation: 0.21589777690451623]
	TIME [epoch: 4.95 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15384960194178013		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.15384960194178013 | validation: 0.16263155923375067]
	TIME [epoch: 115 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11841367785495414		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.11841367785495414 | validation: 0.19740292078556093]
	TIME [epoch: 9.82 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2149669184041837		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.2149669184041837 | validation: 0.1538527667648723]
	TIME [epoch: 9.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13452480941790584		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.13452480941790584 | validation: 0.3126394702844204]
	TIME [epoch: 9.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2044088251923523		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.2044088251923523 | validation: 0.17109575995266404]
	TIME [epoch: 9.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13182497813903113		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.13182497813903113 | validation: 0.21973194096494192]
	TIME [epoch: 9.77 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16824258006251952		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.16824258006251952 | validation: 0.16943650619543574]
	TIME [epoch: 9.82 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12635821129377578		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.12635821129377578 | validation: 0.15296194829651005]
	TIME [epoch: 9.78 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11204372446514829		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.11204372446514829 | validation: 0.17250245402256087]
	TIME [epoch: 9.76 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13632888702754486		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.13632888702754486 | validation: 0.1617971893568234]
	TIME [epoch: 9.75 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17288870958603705		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.17288870958603705 | validation: 0.29567835867498615]
	TIME [epoch: 9.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1404940752652602		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1404940752652602 | validation: 0.1888672612831599]
	TIME [epoch: 9.78 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1285870016871689		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.1285870016871689 | validation: 0.20413229829832463]
	TIME [epoch: 9.75 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13631112076366625		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.13631112076366625 | validation: 0.15595766647154838]
	TIME [epoch: 9.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14128345006655527		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.14128345006655527 | validation: 0.25225375779166936]
	TIME [epoch: 9.81 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17061020648725397		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.17061020648725397 | validation: 0.1401110316662705]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11566208009275655		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.11566208009275655 | validation: 0.17877230165483227]
	TIME [epoch: 9.75 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12906941525933036		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.12906941525933036 | validation: 0.16744240917183542]
	TIME [epoch: 9.76 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12841903483912803		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.12841903483912803 | validation: 0.22215881385397818]
	TIME [epoch: 9.81 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1707462352601664		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.1707462352601664 | validation: 0.14322054178627314]
	TIME [epoch: 9.76 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12904612377321173		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.12904612377321173 | validation: 0.13766565483626222]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11478532809186345		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.11478532809186345 | validation: 0.17993393423134083]
	TIME [epoch: 9.73 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10936523529616245		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.10936523529616245 | validation: 0.1310326333640272]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16286805084189976		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.16286805084189976 | validation: 0.21046593383017245]
	TIME [epoch: 9.75 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15180670100994256		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.15180670100994256 | validation: 0.13926674650624563]
	TIME [epoch: 9.75 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12030699436050929		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.12030699436050929 | validation: 0.15539181642533356]
	TIME [epoch: 9.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12394391207491932		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.12394391207491932 | validation: 0.21310896604729404]
	TIME [epoch: 9.81 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1598711054868274		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.1598711054868274 | validation: 0.15200398319309155]
	TIME [epoch: 9.75 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10982498015423715		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.10982498015423715 | validation: 0.18048464198926573]
	TIME [epoch: 9.75 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11418434268125487		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.11418434268125487 | validation: 0.13222203314330222]
	TIME [epoch: 9.73 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10183830343070499		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.10183830343070499 | validation: 0.1347397322406981]
	TIME [epoch: 9.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11818658636928281		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.11818658636928281 | validation: 0.21115395497714473]
	TIME [epoch: 9.73 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1563991426087041		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1563991426087041 | validation: 0.12047376244637859]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13375094772802498		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.13375094772802498 | validation: 0.13373923318276498]
	TIME [epoch: 9.75 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13456051339270084		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.13456051339270084 | validation: 0.13173344834330347]
	TIME [epoch: 9.76 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09906110004925993		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.09906110004925993 | validation: 0.12190115372721325]
	TIME [epoch: 9.77 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10349282314182946		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.10349282314182946 | validation: 0.17321584760997466]
	TIME [epoch: 9.75 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20418133998886925		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.20418133998886925 | validation: 0.2102347949735593]
	TIME [epoch: 9.74 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14888691809839466		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.14888691809839466 | validation: 0.16215979295469563]
	TIME [epoch: 9.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11326286191007048		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.11326286191007048 | validation: 0.12955247853623886]
	TIME [epoch: 9.78 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10955995834218145		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.10955995834218145 | validation: 0.13663174557281357]
	TIME [epoch: 9.76 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1264872105115295		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.1264872105115295 | validation: 0.1235094885904586]
	TIME [epoch: 9.77 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12411485780615836		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.12411485780615836 | validation: 0.13262698420908156]
	TIME [epoch: 9.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11122660908097652		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.11122660908097652 | validation: 0.1511333110390024]
	TIME [epoch: 9.81 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1188723221872636		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.1188723221872636 | validation: 0.14381153468591015]
	TIME [epoch: 9.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10505241568619314		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.10505241568619314 | validation: 0.16925886890116928]
	TIME [epoch: 9.75 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1258085577877786		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.1258085577877786 | validation: 0.1597627919554624]
	TIME [epoch: 9.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11595923055192559		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.11595923055192559 | validation: 0.1521306090023698]
	TIME [epoch: 9.82 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11965670604835121		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.11965670604835121 | validation: 0.17895531835253392]
	TIME [epoch: 9.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1353375168774491		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.1353375168774491 | validation: 0.12361251663012934]
	TIME [epoch: 9.76 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11405421561237641		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.11405421561237641 | validation: 0.1417838327948398]
	TIME [epoch: 9.76 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09705837235846268		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.09705837235846268 | validation: 0.15538951345646695]
	TIME [epoch: 9.81 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12560925881931193		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.12560925881931193 | validation: 0.11865523821200673]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11381269281330039		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.11381269281330039 | validation: 0.1410852693749503]
	TIME [epoch: 9.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11572236947959062		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.11572236947959062 | validation: 0.17086809554989124]
	TIME [epoch: 9.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14121610894419398		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.14121610894419398 | validation: 0.12179277549990629]
	TIME [epoch: 9.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11673765765656961		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.11673765765656961 | validation: 0.13086258960429314]
	TIME [epoch: 9.77 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09805389997957925		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.09805389997957925 | validation: 0.13186920553608783]
	TIME [epoch: 9.73 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1029017321338829		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.1029017321338829 | validation: 0.13960736998839868]
	TIME [epoch: 9.72 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11037899949279731		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.11037899949279731 | validation: 0.1565303028623024]
	TIME [epoch: 9.73 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11062730806272855		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.11062730806272855 | validation: 0.1269189360787444]
	TIME [epoch: 9.77 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1018336819224098		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.1018336819224098 | validation: 0.1651128990591185]
	TIME [epoch: 9.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13523145044025567		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.13523145044025567 | validation: 0.1272918707688795]
	TIME [epoch: 9.73 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10916001891938895		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.10916001891938895 | validation: 0.11735647634380977]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09556593699996149		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.09556593699996149 | validation: 0.12556096047774865]
	TIME [epoch: 9.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14136882089729286		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.14136882089729286 | validation: 0.13420972684602178]
	TIME [epoch: 9.73 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09470303069304088		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.09470303069304088 | validation: 0.11963673697532062]
	TIME [epoch: 9.72 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09155645198061022		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.09155645198061022 | validation: 0.13173911044943418]
	TIME [epoch: 9.72 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10889927802959742		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.10889927802959742 | validation: 0.21605717473758806]
	TIME [epoch: 9.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20500186169971313		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.20500186169971313 | validation: 0.17440085956899382]
	TIME [epoch: 9.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1425858623162674		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.1425858623162674 | validation: 0.12639500891879485]
	TIME [epoch: 9.72 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09617264033488071		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.09617264033488071 | validation: 0.11794974087480933]
	TIME [epoch: 9.73 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09065351763158838		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.09065351763158838 | validation: 0.13256318708857387]
	TIME [epoch: 9.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12062331313408897		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.12062331313408897 | validation: 0.10199332091184055]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08323318646605918		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.08323318646605918 | validation: 0.14004133618704523]
	TIME [epoch: 9.75 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09404350384893345		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.09404350384893345 | validation: 0.14844549543952812]
	TIME [epoch: 9.73 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10307489723033425		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.10307489723033425 | validation: 0.0998279510161397]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08950476718127141		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.08950476718127141 | validation: 0.1364101486017102]
	TIME [epoch: 9.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11801113444865746		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11801113444865746 | validation: 0.11257237298184687]
	TIME [epoch: 9.75 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09084972568020716		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.09084972568020716 | validation: 0.12303212442894007]
	TIME [epoch: 9.75 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11160952565729987		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.11160952565729987 | validation: 0.1281467032231132]
	TIME [epoch: 9.76 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08770598892122874		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.08770598892122874 | validation: 0.10144357165800057]
	TIME [epoch: 9.81 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10162552560959404		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.10162552560959404 | validation: 0.136818161939657]
	TIME [epoch: 9.76 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13065004222231885		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.13065004222231885 | validation: 0.17928195658218943]
	TIME [epoch: 9.76 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17008489094990753		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.17008489094990753 | validation: 0.09621269347399086]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10056683976470585		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.10056683976470585 | validation: 0.1353324124311654]
	TIME [epoch: 9.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0905041600063362		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.0905041600063362 | validation: 0.10736469464207413]
	TIME [epoch: 9.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08345917283879364		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.08345917283879364 | validation: 0.09274346635775516]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07470388896670839		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.07470388896670839 | validation: 0.09644139514862403]
	TIME [epoch: 9.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08584633970493602		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.08584633970493602 | validation: 0.1254395707931614]
	TIME [epoch: 9.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11925495752569351		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.11925495752569351 | validation: 0.14148975788541673]
	TIME [epoch: 9.73 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09637177306771554		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.09637177306771554 | validation: 0.1257461764107915]
	TIME [epoch: 9.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09613561938326701		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.09613561938326701 | validation: 0.10161881836523956]
	TIME [epoch: 9.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09581197053938506		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.09581197053938506 | validation: 0.09471068911959683]
	TIME [epoch: 9.77 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08083180762913555		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.08083180762913555 | validation: 0.1006778429646808]
	TIME [epoch: 9.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07498114065888598		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.07498114065888598 | validation: 0.08766134314792608]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09475782592242162		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.09475782592242162 | validation: 0.15535112435633702]
	TIME [epoch: 9.72 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09733632029303957		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.09733632029303957 | validation: 0.10439097753476034]
	TIME [epoch: 9.74 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10113269744859588		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.10113269744859588 | validation: 0.0994288258490215]
	TIME [epoch: 9.77 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11590902545560347		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.11590902545560347 | validation: 0.09978135448397657]
	TIME [epoch: 9.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07250901983233958		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.07250901983233958 | validation: 0.10622406667563897]
	TIME [epoch: 9.74 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0867056309831547		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.0867056309831547 | validation: 0.08994581936528699]
	TIME [epoch: 9.81 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07829903090358528		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.07829903090358528 | validation: 0.10721844433260246]
	TIME [epoch: 9.86 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09916447998479976		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.09916447998479976 | validation: 0.10620409171113378]
	TIME [epoch: 9.81 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07297123953308406		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.07297123953308406 | validation: 0.10273109675141745]
	TIME [epoch: 9.8 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07383364529324773		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.07383364529324773 | validation: 0.10408823011138923]
	TIME [epoch: 9.81 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09525877857740345		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.09525877857740345 | validation: 0.09108923475881706]
	TIME [epoch: 9.86 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07346674491670316		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.07346674491670316 | validation: 0.09657749380770939]
	TIME [epoch: 9.81 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09087080429142923		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.09087080429142923 | validation: 0.08589671896132703]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0732310033836088		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.0732310033836088 | validation: 0.09759826632446258]
	TIME [epoch: 9.83 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0873437812394311		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.0873437812394311 | validation: 0.1062394077360696]
	TIME [epoch: 9.88 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08243935940595729		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.08243935940595729 | validation: 0.10011456548199921]
	TIME [epoch: 9.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07819384692004325		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.07819384692004325 | validation: 0.09441274499881562]
	TIME [epoch: 9.81 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07366115744404879		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.07366115744404879 | validation: 0.09474026065295611]
	TIME [epoch: 9.81 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06345483618426824		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.06345483618426824 | validation: 0.09567471851346357]
	TIME [epoch: 9.84 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08301466421527248		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.08301466421527248 | validation: 0.12382006592633857]
	TIME [epoch: 9.82 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07903164374385192		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.07903164374385192 | validation: 0.09391048869908442]
	TIME [epoch: 9.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06977531643665714		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.06977531643665714 | validation: 0.08648650997383547]
	TIME [epoch: 9.81 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05984463434281673		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.05984463434281673 | validation: 0.0815825059007203]
	TIME [epoch: 9.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07431095069319103		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.07431095069319103 | validation: 0.10978270205476406]
	TIME [epoch: 9.86 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08907994378260767		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.08907994378260767 | validation: 0.08729987434816658]
	TIME [epoch: 9.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07625292045927597		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07625292045927597 | validation: 0.08919460313277128]
	TIME [epoch: 9.82 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06970531658959915		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.06970531658959915 | validation: 0.09141839287766529]
	TIME [epoch: 9.82 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06394659782984458		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.06394659782984458 | validation: 0.12145605756913291]
	TIME [epoch: 9.86 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08833497523395903		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.08833497523395903 | validation: 0.0925560736714682]
	TIME [epoch: 9.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0723100272617113		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.0723100272617113 | validation: 0.08110290044396132]
	TIME [epoch: 9.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0719171968439449		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.0719171968439449 | validation: 0.10289888159846128]
	TIME [epoch: 9.82 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.070038705487545		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.070038705487545 | validation: 0.08296591675800483]
	TIME [epoch: 9.83 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06431001967066988		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.06431001967066988 | validation: 0.08424254057882369]
	TIME [epoch: 9.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07631621814126127		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.07631621814126127 | validation: 0.08094019457221216]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186160942758333		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.06186160942758333 | validation: 0.07633155211767861]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0716625134343836		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.0716625134343836 | validation: 0.10686498306135425]
	TIME [epoch: 9.86 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06966916751648929		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.06966916751648929 | validation: 0.0871539398482191]
	TIME [epoch: 9.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0631976117980824		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.0631976117980824 | validation: 0.09693458330587695]
	TIME [epoch: 9.73 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05964274052987725		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.05964274052987725 | validation: 0.07938137899444343]
	TIME [epoch: 9.75 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08438331421861195		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.08438331421861195 | validation: 0.07250095537463339]
	TIME [epoch: 9.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07313107279562166		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.07313107279562166 | validation: 0.09274164817852544]
	TIME [epoch: 9.82 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06204010558657371		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.06204010558657371 | validation: 0.08023739828354386]
	TIME [epoch: 9.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05568041916128498		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.05568041916128498 | validation: 0.07622641160043502]
	TIME [epoch: 9.85 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058031689738968065		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.058031689738968065 | validation: 0.07964659655748199]
	TIME [epoch: 9.88 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08361258549291083		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.08361258549291083 | validation: 0.07101427975893385]
	TIME [epoch: 9.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09850726477507668		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.09850726477507668 | validation: 0.0753643707066404]
	TIME [epoch: 9.81 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07407143566238837		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.07407143566238837 | validation: 0.07371052535546453]
	TIME [epoch: 9.81 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0646695459749193		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.0646695459749193 | validation: 0.07246429214698645]
	TIME [epoch: 9.82 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06682842658016312		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.06682842658016312 | validation: 0.07513484991987888]
	TIME [epoch: 9.73 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05347666050796181		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.05347666050796181 | validation: 0.08163922603747184]
	TIME [epoch: 9.74 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057859538639357076		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.057859538639357076 | validation: 0.07075628070215784]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053095789094261436		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.053095789094261436 | validation: 0.07212609605415399]
	TIME [epoch: 9.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06441647310758787		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.06441647310758787 | validation: 0.08139093919098564]
	TIME [epoch: 9.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06299123643642617		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.06299123643642617 | validation: 0.07275560218381748]
	TIME [epoch: 9.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0673503232734341		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.0673503232734341 | validation: 0.07037913895405062]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0565151560384851		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.0565151560384851 | validation: 0.0715325448840939]
	TIME [epoch: 9.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055395453897352176		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.055395453897352176 | validation: 0.07508686770458947]
	TIME [epoch: 9.74 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07477805207309028		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.07477805207309028 | validation: 0.098222414923894]
	TIME [epoch: 9.74 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07439284587084476		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.07439284587084476 | validation: 0.08226556071183942]
	TIME [epoch: 9.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0559327293992125		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.0559327293992125 | validation: 0.0684729872764184]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057459105960033355		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.057459105960033355 | validation: 0.07084541379793655]
	TIME [epoch: 9.73 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06449844800140962		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.06449844800140962 | validation: 0.06470648700495935]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05709741463429438		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.05709741463429438 | validation: 0.07195931304288622]
	TIME [epoch: 9.76 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05626133221293146		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.05626133221293146 | validation: 0.06986096229542943]
	TIME [epoch: 9.74 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06257451717116229		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.06257451717116229 | validation: 0.07846707960144064]
	TIME [epoch: 9.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05143998537484924		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.05143998537484924 | validation: 0.0871820222113854]
	TIME [epoch: 9.77 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06125846639311682		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.06125846639311682 | validation: 0.07541408680136283]
	TIME [epoch: 9.82 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06184828351419109		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.06184828351419109 | validation: 0.07632084490530247]
	TIME [epoch: 9.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04983348313838268		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.04983348313838268 | validation: 0.06876472185140466]
	TIME [epoch: 9.81 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058336777587990815		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.058336777587990815 | validation: 0.07498216950402048]
	TIME [epoch: 9.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06609753275859466		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.06609753275859466 | validation: 0.08042502114110814]
	TIME [epoch: 9.82 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0556653322558735		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.0556653322558735 | validation: 0.07108477058271809]
	TIME [epoch: 9.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05756395090133831		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.05756395090133831 | validation: 0.06061519069919929]
	TIME [epoch: 9.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048909833517675555		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.048909833517675555 | validation: 0.06471038739436964]
	TIME [epoch: 9.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07934993372379669		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.07934993372379669 | validation: 0.09242748616673528]
	TIME [epoch: 9.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07205906228676462		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.07205906228676462 | validation: 0.07851509463273607]
	TIME [epoch: 9.83 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05180067806675998		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.05180067806675998 | validation: 0.0652424123889897]
	TIME [epoch: 9.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059389831510560695		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.059389831510560695 | validation: 0.07394998145707997]
	TIME [epoch: 9.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056153097523865464		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.056153097523865464 | validation: 0.0702162760103468]
	TIME [epoch: 9.81 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05307709575571956		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.05307709575571956 | validation: 0.07731994001706814]
	TIME [epoch: 9.85 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06790647791464416		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.06790647791464416 | validation: 0.08942196623332865]
	TIME [epoch: 9.79 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051878826539859466		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.051878826539859466 | validation: 0.062198830512508387]
	TIME [epoch: 9.81 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04632864863327811		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.04632864863327811 | validation: 0.0606650675191266]
	TIME [epoch: 9.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05300581885302401		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.05300581885302401 | validation: 0.0655471490523326]
	TIME [epoch: 9.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052702891573085636		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.052702891573085636 | validation: 0.07278445284416887]
	TIME [epoch: 9.79 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046707679275548766		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.046707679275548766 | validation: 0.07964989302763237]
	TIME [epoch: 9.79 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06380967871140554		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.06380967871140554 | validation: 0.06253043205454313]
	TIME [epoch: 9.81 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08493846796174044		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.08493846796174044 | validation: 0.07692123462152346]
	TIME [epoch: 9.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06322277563297052		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.06322277563297052 | validation: 0.05802322436664081]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042476635179050884		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.042476635179050884 | validation: 0.07171220278813756]
	TIME [epoch: 9.79 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04585809457814435		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.04585809457814435 | validation: 0.06301720965052843]
	TIME [epoch: 9.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04224506631661683		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.04224506631661683 | validation: 0.0572140150527119]
	TIME [epoch: 9.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05245210218943029		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.05245210218943029 | validation: 0.06863772627279437]
	TIME [epoch: 9.78 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045838274130295925		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.045838274130295925 | validation: 0.07128804198023415]
	TIME [epoch: 9.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05227074880244342		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.05227074880244342 | validation: 0.08095457587465704]
	TIME [epoch: 9.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051121626344758225		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.051121626344758225 | validation: 0.06412526378618626]
	TIME [epoch: 9.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04438673398135927		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.04438673398135927 | validation: 0.061576551867002526]
	TIME [epoch: 9.79 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0480149282598447		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.0480149282598447 | validation: 0.059009788335453464]
	TIME [epoch: 9.78 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047773645246326574		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.047773645246326574 | validation: 0.06456342945240451]
	TIME [epoch: 9.81 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049024040940910105		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.049024040940910105 | validation: 0.08072083081238843]
	TIME [epoch: 9.82 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04873279677302412		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04873279677302412 | validation: 0.06968689818003895]
	TIME [epoch: 9.78 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04651164182787648		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.04651164182787648 | validation: 0.060275066737069186]
	TIME [epoch: 9.78 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043899927320012716		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.043899927320012716 | validation: 0.06223386209983171]
	TIME [epoch: 9.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051251421927695856		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.051251421927695856 | validation: 0.09140047158819617]
	TIME [epoch: 9.82 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04998956220770734		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.04998956220770734 | validation: 0.061154086301817394]
	TIME [epoch: 9.82 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046144076986045494		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.046144076986045494 | validation: 0.07226964828212804]
	TIME [epoch: 9.81 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04846140634330273		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.04846140634330273 | validation: 0.07899812371932224]
	TIME [epoch: 9.82 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045557196768407734		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.045557196768407734 | validation: 0.07152977065009963]
	TIME [epoch: 9.85 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04334344108367026		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.04334344108367026 | validation: 0.053676984622944346]
	TIME [epoch: 9.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04864574079286614		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.04864574079286614 | validation: 0.06839831753989453]
	TIME [epoch: 9.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058025882297898373		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.058025882297898373 | validation: 0.05841914009419748]
	TIME [epoch: 9.86 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04936326622554768		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.04936326622554768 | validation: 0.06784489379717357]
	TIME [epoch: 9.87 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041612001007844734		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.041612001007844734 | validation: 0.06093565786336723]
	TIME [epoch: 9.84 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04188068155600913		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.04188068155600913 | validation: 0.05991747031815273]
	TIME [epoch: 9.84 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051010220695910685		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.051010220695910685 | validation: 0.1378918876087828]
	TIME [epoch: 9.86 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06866596482246254		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.06866596482246254 | validation: 0.052077068531701735]
	TIME [epoch: 9.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043826574009029805		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.043826574009029805 | validation: 0.052700251249572845]
	TIME [epoch: 9.82 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0378297599384268		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.0378297599384268 | validation: 0.05536525547604683]
	TIME [epoch: 9.81 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930914897800417		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.05930914897800417 | validation: 0.05656920183649004]
	TIME [epoch: 9.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043841461486248		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.043841461486248 | validation: 0.05074651028332052]
	TIME [epoch: 9.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040792077981963534		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.040792077981963534 | validation: 0.0546862173638611]
	TIME [epoch: 9.8 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04585715235931494		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.04585715235931494 | validation: 0.06284149879203309]
	TIME [epoch: 9.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04903788539504652		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.04903788539504652 | validation: 0.06417369148614495]
	TIME [epoch: 9.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04405285216397781		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04405285216397781 | validation: 0.05958075802393398]
	TIME [epoch: 9.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04432878667471789		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.04432878667471789 | validation: 0.057290405548685364]
	TIME [epoch: 9.81 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03876297830800644		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.03876297830800644 | validation: 0.05621986685964682]
	TIME [epoch: 9.82 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04243781015011203		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.04243781015011203 | validation: 0.07633855873551043]
	TIME [epoch: 9.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04356919719079689		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.04356919719079689 | validation: 0.06524606962412868]
	TIME [epoch: 9.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05061355512244787		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.05061355512244787 | validation: 0.04752950005366714]
	TIME [epoch: 9.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03864919364517817		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.03864919364517817 | validation: 0.0669608939601304]
	TIME [epoch: 9.82 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04046227705365297		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.04046227705365297 | validation: 0.06564497093021314]
	TIME [epoch: 9.83 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040233461637932		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.040233461637932 | validation: 0.054852082707181]
	TIME [epoch: 9.82 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04146201018649825		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.04146201018649825 | validation: 0.04735452658011159]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04132883899989928		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.04132883899989928 | validation: 0.05037971777422757]
	TIME [epoch: 9.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04038293352363353		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.04038293352363353 | validation: 0.05338871679255197]
	TIME [epoch: 9.83 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03792348662203843		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.03792348662203843 | validation: 0.05336210599075144]
	TIME [epoch: 9.82 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041078750549002875		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.041078750549002875 | validation: 0.053350467544949334]
	TIME [epoch: 9.81 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04002023879773494		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.04002023879773494 | validation: 0.05103657272668961]
	TIME [epoch: 9.79 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0353139912631728		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.0353139912631728 | validation: 0.05685573462191254]
	TIME [epoch: 9.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03928455863816718		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.03928455863816718 | validation: 0.07719395944281315]
	TIME [epoch: 9.83 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04350048157380627		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.04350048157380627 | validation: 0.06535762933237177]
	TIME [epoch: 9.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04697342803274422		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.04697342803274422 | validation: 0.05582676870428685]
	TIME [epoch: 9.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03736172870726935		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.03736172870726935 | validation: 0.04954219317071113]
	TIME [epoch: 9.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03769277330710544		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.03769277330710544 | validation: 0.08786640888876104]
	TIME [epoch: 9.83 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043735956914935946		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.043735956914935946 | validation: 0.04898813359350615]
	TIME [epoch: 9.81 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05728955584430429		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.05728955584430429 | validation: 0.06819652668112478]
	TIME [epoch: 9.82 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04525041406896864		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.04525041406896864 | validation: 0.04299509800719059]
	TIME [epoch: 9.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035433588683911214		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.035433588683911214 | validation: 0.04535879026581133]
	TIME [epoch: 9.82 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04405252595225525		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.04405252595225525 | validation: 0.045015993549048505]
	TIME [epoch: 9.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04336978899524027		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04336978899524027 | validation: 0.051490041239701405]
	TIME [epoch: 9.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03568422143301459		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.03568422143301459 | validation: 0.048705166260212394]
	TIME [epoch: 9.83 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0370904042281056		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0370904042281056 | validation: 0.05292033164995778]
	TIME [epoch: 9.81 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043102286004595855		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.043102286004595855 | validation: 0.05149443515982431]
	TIME [epoch: 9.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03534499525345444		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.03534499525345444 | validation: 0.05457086225756257]
	TIME [epoch: 9.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038985201861472994		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.038985201861472994 | validation: 0.05125240320509148]
	TIME [epoch: 9.83 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03407875961884796		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.03407875961884796 | validation: 0.04453437493869383]
	TIME [epoch: 9.81 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03720903924471746		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03720903924471746 | validation: 0.0525889887934]
	TIME [epoch: 9.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040136218971014936		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.040136218971014936 | validation: 0.05202034628098673]
	TIME [epoch: 9.79 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03757624029984392		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.03757624029984392 | validation: 0.05265055169993499]
	TIME [epoch: 9.82 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03857328078425347		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.03857328078425347 | validation: 0.04801198054077152]
	TIME [epoch: 9.82 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03386384531730506		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.03386384531730506 | validation: 0.04570952444631761]
	TIME [epoch: 9.79 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03572160633244928		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.03572160633244928 | validation: 0.05957068909191442]
	TIME [epoch: 9.79 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04109023270872624		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.04109023270872624 | validation: 0.04993083991999895]
	TIME [epoch: 9.82 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06439572748090167		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.06439572748090167 | validation: 0.050942215348220646]
	TIME [epoch: 9.77 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04157697966842615		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.04157697966842615 | validation: 0.054611164783761365]
	TIME [epoch: 9.71 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03583597869393232		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.03583597869393232 | validation: 0.045467609266946374]
	TIME [epoch: 9.72 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03440380909213962		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.03440380909213962 | validation: 0.046443093582942865]
	TIME [epoch: 9.74 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03559010385055612		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.03559010385055612 | validation: 0.04455876600338422]
	TIME [epoch: 9.75 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031473555870793465		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.031473555870793465 | validation: 0.04537912680396257]
	TIME [epoch: 9.71 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03467978777250478		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.03467978777250478 | validation: 0.0510568558556063]
	TIME [epoch: 9.72 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045271213113431255		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.045271213113431255 | validation: 0.05139274032444331]
	TIME [epoch: 9.72 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03474231598622353		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.03474231598622353 | validation: 0.041376606627570305]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03057432816781664		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.03057432816781664 | validation: 0.05121228251697829]
	TIME [epoch: 9.72 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03333606464288236		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.03333606464288236 | validation: 0.044586651050559464]
	TIME [epoch: 9.72 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03271224892554858		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.03271224892554858 | validation: 0.05127468499119165]
	TIME [epoch: 9.74 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06488490354600077		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.06488490354600077 | validation: 0.04483607378859225]
	TIME [epoch: 9.77 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04098485940930006		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.04098485940930006 | validation: 0.04030931097382452]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034790289282842515		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.034790289282842515 | validation: 0.0434218387050969]
	TIME [epoch: 9.72 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03631172392790897		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.03631172392790897 | validation: 0.04607331019751637]
	TIME [epoch: 9.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03202122469882279		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.03202122469882279 | validation: 0.04162069333869517]
	TIME [epoch: 9.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031028915453148183		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.031028915453148183 | validation: 0.046838755002962945]
	TIME [epoch: 9.7 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035627016557922705		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.035627016557922705 | validation: 0.05176825979091158]
	TIME [epoch: 9.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040336969000683204		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.040336969000683204 | validation: 0.046819459950032914]
	TIME [epoch: 9.71 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032387575170244175		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.032387575170244175 | validation: 0.052839906818910425]
	TIME [epoch: 9.75 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03735655491332581		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03735655491332581 | validation: 0.043929415440552184]
	TIME [epoch: 9.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033241516256878954		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.033241516256878954 | validation: 0.05960944733206627]
	TIME [epoch: 9.7 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03681154515808837		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.03681154515808837 | validation: 0.045057911930493175]
	TIME [epoch: 9.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035432133049046105		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.035432133049046105 | validation: 0.04889998242478792]
	TIME [epoch: 9.74 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03157554470939203		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.03157554470939203 | validation: 0.04701186562668834]
	TIME [epoch: 9.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03548404044709194		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.03548404044709194 | validation: 0.04324483975322552]
	TIME [epoch: 9.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030173058331348025		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.030173058331348025 | validation: 0.04052389094732488]
	TIME [epoch: 9.7 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03072960282593439		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.03072960282593439 | validation: 0.041404177909043924]
	TIME [epoch: 9.74 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03439105002818918		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03439105002818918 | validation: 0.0405309310564942]
	TIME [epoch: 9.7 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030944685143207154		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.030944685143207154 | validation: 0.0406751987670055]
	TIME [epoch: 9.7 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03986286633006978		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.03986286633006978 | validation: 0.043632756139271436]
	TIME [epoch: 9.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03116906210986073		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.03116906210986073 | validation: 0.047844574108656536]
	TIME [epoch: 9.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034895657001733874		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.034895657001733874 | validation: 0.040300709756649525]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030481368464731345		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.030481368464731345 | validation: 0.05376248536987786]
	TIME [epoch: 9.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03830664663671899		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.03830664663671899 | validation: 0.05160995704535501]
	TIME [epoch: 9.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03147598641524599		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.03147598641524599 | validation: 0.04431940867579676]
	TIME [epoch: 9.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03555002428625929		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.03555002428625929 | validation: 0.05066225283937558]
	TIME [epoch: 9.71 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03571260741329804		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.03571260741329804 | validation: 0.03771686096011075]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02844455858083295		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.02844455858083295 | validation: 0.03700588472204678]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029020344407147937		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.029020344407147937 | validation: 0.04629687042354492]
	TIME [epoch: 9.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032033382243315055		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.032033382243315055 | validation: 0.04472266596709033]
	TIME [epoch: 129 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03176191789446574		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.03176191789446574 | validation: 0.03805414387746055]
	TIME [epoch: 22.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03479195204241683		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03479195204241683 | validation: 0.03954827056807146]
	TIME [epoch: 22 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03695871339255397		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.03695871339255397 | validation: 0.041947835885538534]
	TIME [epoch: 22 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03263903927240307		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.03263903927240307 | validation: 0.04037765145663188]
	TIME [epoch: 21.9 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031056311089828776		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.031056311089828776 | validation: 0.03844127223491342]
	TIME [epoch: 22 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03346349504821511		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.03346349504821511 | validation: 0.056720715861097115]
	TIME [epoch: 21.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03878753398827284		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.03878753398827284 | validation: 0.058263236424856944]
	TIME [epoch: 22 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04029477018341551		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.04029477018341551 | validation: 0.03375554933889057]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07550232520012753		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.07550232520012753 | validation: 0.05415441404820169]
	TIME [epoch: 22 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03920585767759162		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.03920585767759162 | validation: 0.04025383881903358]
	TIME [epoch: 22 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032592457195700766		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.032592457195700766 | validation: 0.0419416556821338]
	TIME [epoch: 22 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028517291134174297		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.028517291134174297 | validation: 0.038355757921412985]
	TIME [epoch: 22 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03210011014643085		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.03210011014643085 | validation: 0.03837689713464193]
	TIME [epoch: 21.9 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03040918166377815		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.03040918166377815 | validation: 0.034661263083488186]
	TIME [epoch: 22 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027917478152910437		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.027917478152910437 | validation: 0.039370829340231506]
	TIME [epoch: 21.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031581094963632414		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.031581094963632414 | validation: 0.03990168297238332]
	TIME [epoch: 22 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028278580237874006		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.028278580237874006 | validation: 0.03838343331292997]
	TIME [epoch: 21.9 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03262241334580357		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.03262241334580357 | validation: 0.03809112312656203]
	TIME [epoch: 21.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03786153612984403		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.03786153612984403 | validation: 0.04249950236291365]
	TIME [epoch: 21.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031540178658837605		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.031540178658837605 | validation: 0.03574675324847516]
	TIME [epoch: 21.8 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026703001720194297		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.026703001720194297 | validation: 0.040953485070858456]
	TIME [epoch: 21.8 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028518418492918883		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.028518418492918883 | validation: 0.034406996365488426]
	TIME [epoch: 21.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02699949370499862		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.02699949370499862 | validation: 0.04355918917926499]
	TIME [epoch: 21.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032572444303995154		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.032572444303995154 | validation: 0.04738407606147174]
	TIME [epoch: 21.8 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032916451212115366		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.032916451212115366 | validation: 0.03387682468953007]
	TIME [epoch: 21.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027100507686479064		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.027100507686479064 | validation: 0.036546932471750004]
	TIME [epoch: 21.8 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029242156676542095		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.029242156676542095 | validation: 0.03735115967911444]
	TIME [epoch: 21.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04108111729317197		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.04108111729317197 | validation: 0.04060188283181604]
	TIME [epoch: 21.8 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028363322185511214		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.028363322185511214 | validation: 0.03496447953206824]
	TIME [epoch: 21.8 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026643962745243607		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.026643962745243607 | validation: 0.03878813781436316]
	TIME [epoch: 21.8 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026929075652587963		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.026929075652587963 | validation: 0.035432780092695716]
	TIME [epoch: 21.8 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02988968759696904		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.02988968759696904 | validation: 0.0420199589524083]
	TIME [epoch: 21.8 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028835590157393197		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.028835590157393197 | validation: 0.04233480115346348]
	TIME [epoch: 21.8 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026812779068728716		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.026812779068728716 | validation: 0.037177529938679]
	TIME [epoch: 21.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02987061082802847		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.02987061082802847 | validation: 0.036028359690147824]
	TIME [epoch: 21.8 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028204171628611222		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.028204171628611222 | validation: 0.03813360100322574]
	TIME [epoch: 21.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03062892547902438		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.03062892547902438 | validation: 0.03627150257099543]
	TIME [epoch: 21.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028215753187496244		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.028215753187496244 | validation: 0.034973578840982164]
	TIME [epoch: 21.8 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025346905923752907		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.025346905923752907 | validation: 0.035642800722225886]
	TIME [epoch: 21.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0264251483803122		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.0264251483803122 | validation: 0.033083645410206836]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0313635228923871		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.0313635228923871 | validation: 0.03690074522477728]
	TIME [epoch: 21.8 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030451549577922035		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.030451549577922035 | validation: 0.08790571016712759]
	TIME [epoch: 21.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06563693142585836		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.06563693142585836 | validation: 0.06124302990449846]
	TIME [epoch: 21.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03244886553358333		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.03244886553358333 | validation: 0.035396296938099876]
	TIME [epoch: 21.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02586998430999289		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.02586998430999289 | validation: 0.03395119676435023]
	TIME [epoch: 21.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023747023220032838		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.023747023220032838 | validation: 0.033070256500748185]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02505651137400668		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.02505651137400668 | validation: 0.03642073325693754]
	TIME [epoch: 22 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027284739004457752		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.027284739004457752 | validation: 0.034936078632578776]
	TIME [epoch: 22 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024486717193135894		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.024486717193135894 | validation: 0.03245969026754994]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05744081093331402		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.05744081093331402 | validation: 0.0473849565957839]
	TIME [epoch: 21.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03152161011863616		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.03152161011863616 | validation: 0.040128827386431046]
	TIME [epoch: 22 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026944405949477324		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.026944405949477324 | validation: 0.0324390025484563]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025238337895756142		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.025238337895756142 | validation: 0.03317460425502255]
	TIME [epoch: 22 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023852524955720683		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.023852524955720683 | validation: 0.032987273861074355]
	TIME [epoch: 21.9 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024792600459940157		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.024792600459940157 | validation: 0.03237696435309567]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025503327554353486		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.025503327554353486 | validation: 0.036079441194833264]
	TIME [epoch: 22 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025317766684782618		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.025317766684782618 | validation: 0.03354382152660766]
	TIME [epoch: 22 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026778695452893707		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.026778695452893707 | validation: 0.03787832900969004]
	TIME [epoch: 22 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02909017456130756		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.02909017456130756 | validation: 0.03260587009802114]
	TIME [epoch: 21.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024361998348128674		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.024361998348128674 | validation: 0.030289314170144868]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0246153911318526		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0246153911318526 | validation: 0.03114868974884904]
	TIME [epoch: 21.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02530677045339247		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.02530677045339247 | validation: 0.030188109336070433]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027911098238981007		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.027911098238981007 | validation: 0.03468197628029379]
	TIME [epoch: 21.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026546062810156472		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.026546062810156472 | validation: 0.03812610910387418]
	TIME [epoch: 21.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0269871057237941		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.0269871057237941 | validation: 0.03864376914135141]
	TIME [epoch: 21.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03244466701869101		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.03244466701869101 | validation: 0.029769521337819623]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022786820218252877		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.022786820218252877 | validation: 0.029564101783479187]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023355446697356612		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.023355446697356612 | validation: 0.03070242081160436]
	TIME [epoch: 21.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027370536208086958		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.027370536208086958 | validation: 0.0329273318487079]
	TIME [epoch: 21.8 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025859180380103996		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.025859180380103996 | validation: 0.03065609405733432]
	TIME [epoch: 21.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025140521572217765		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.025140521572217765 | validation: 0.029289305531496612]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029319715305128367		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.029319715305128367 | validation: 0.03254577447855126]
	TIME [epoch: 21.9 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026944111411834136		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.026944111411834136 | validation: 0.03337260820236017]
	TIME [epoch: 21.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023215219894697894		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.023215219894697894 | validation: 0.03483406574670211]
	TIME [epoch: 21.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023851281710241513		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.023851281710241513 | validation: 0.03728002782743475]
	TIME [epoch: 21.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024751729847386078		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.024751729847386078 | validation: 0.03229038374395417]
	TIME [epoch: 21.8 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022308737293556966		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.022308737293556966 | validation: 0.046122321958423786]
	TIME [epoch: 21.9 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025949842674128615		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.025949842674128615 | validation: 0.03482375288672883]
	TIME [epoch: 21.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025257492374149853		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.025257492374149853 | validation: 0.02965762068449796]
	TIME [epoch: 21.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024871314646649777		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.024871314646649777 | validation: 0.030150725439924413]
	TIME [epoch: 21.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022957718320013858		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.022957718320013858 | validation: 0.03582810541832665]
	TIME [epoch: 21.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02472344691712145		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.02472344691712145 | validation: 0.030914142100487943]
	TIME [epoch: 21.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02528092736477186		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.02528092736477186 | validation: 0.031064447091749957]
	TIME [epoch: 21.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02510598711106949		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.02510598711106949 | validation: 0.03097112822144292]
	TIME [epoch: 21.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024871974953603444		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.024871974953603444 | validation: 0.02754857573929024]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022072969598477134		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.022072969598477134 | validation: 0.029774182094650267]
	TIME [epoch: 21.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023157948808620602		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.023157948808620602 | validation: 0.032134317097292225]
	TIME [epoch: 21.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023011789620233918		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.023011789620233918 | validation: 0.029098077809998506]
	TIME [epoch: 21.9 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023617772830427396		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.023617772830427396 | validation: 0.028008572041827023]
	TIME [epoch: 21.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023656623953649585		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.023656623953649585 | validation: 0.030265240016563347]
	TIME [epoch: 21.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026004840060183613		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.026004840060183613 | validation: 0.03313106740639318]
	TIME [epoch: 21.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02355510461745968		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.02355510461745968 | validation: 0.032410095974704026]
	TIME [epoch: 21.8 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026274921839059103		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.026274921839059103 | validation: 0.029122848734687994]
	TIME [epoch: 21.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022398630958739653		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.022398630958739653 | validation: 0.0309626712544092]
	TIME [epoch: 21.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023170001930138198		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.023170001930138198 | validation: 0.03316723865428746]
	TIME [epoch: 21.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021960658823305897		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.021960658823305897 | validation: 0.029158387311678503]
	TIME [epoch: 21.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026037506405856115		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.026037506405856115 | validation: 0.03202749111296383]
	TIME [epoch: 21.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024959273847084063		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.024959273847084063 | validation: 0.031361254384354345]
	TIME [epoch: 21.8 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02325494566125091		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.02325494566125091 | validation: 0.02980330144913847]
	TIME [epoch: 21.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020479033632423822		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.020479033632423822 | validation: 0.029728712540904877]
	TIME [epoch: 21.8 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020617783250237866		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.020617783250237866 | validation: 0.030377297842473013]
	TIME [epoch: 21.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02251163335368905		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.02251163335368905 | validation: 0.02582149542666858]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02562098390338062		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.02562098390338062 | validation: 0.03312300868254737]
	TIME [epoch: 21.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022756208109727817		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.022756208109727817 | validation: 0.0299457155833269]
	TIME [epoch: 21.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025802724535102475		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.025802724535102475 | validation: 0.0275324748209518]
	TIME [epoch: 21.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023163566664761125		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.023163566664761125 | validation: 0.027788335990954316]
	TIME [epoch: 21.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020342658100946747		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.020342658100946747 | validation: 0.029923868356179724]
	TIME [epoch: 21.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021763247000782172		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.021763247000782172 | validation: 0.031614474433257864]
	TIME [epoch: 21.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022783192371178818		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.022783192371178818 | validation: 0.02807220798016538]
	TIME [epoch: 21.8 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02085111840055954		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.02085111840055954 | validation: 0.03361772699387134]
	TIME [epoch: 21.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02242393747796894		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.02242393747796894 | validation: 0.02663424995355731]
	TIME [epoch: 21.8 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020468941765896344		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.020468941765896344 | validation: 0.024766113276201124]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02236836701783605		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.02236836701783605 | validation: 0.03925833900449642]
	TIME [epoch: 21.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02475757065373425		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.02475757065373425 | validation: 0.02871567520100831]
	TIME [epoch: 21.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021141947392059372		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.021141947392059372 | validation: 0.03221281429315408]
	TIME [epoch: 21.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022091587057541755		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.022091587057541755 | validation: 0.028344078104664272]
	TIME [epoch: 21.9 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02534115269459078		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.02534115269459078 | validation: 0.027685429828100162]
	TIME [epoch: 21.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022764168241207746		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.022764168241207746 | validation: 0.026608523705501097]
	TIME [epoch: 21.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02187349339002768		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.02187349339002768 | validation: 0.029990310589879356]
	TIME [epoch: 21.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023109177579700095		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.023109177579700095 | validation: 0.03204604060474734]
	TIME [epoch: 21.8 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02372941994649164		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.02372941994649164 | validation: 0.027515934559721685]
	TIME [epoch: 21.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020533557072428467		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.020533557072428467 | validation: 0.02719505892756894]
	TIME [epoch: 21.8 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020489523581540926		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.020489523581540926 | validation: 0.027841068628030263]
	TIME [epoch: 21.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020331265870638167		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.020331265870638167 | validation: 0.026888410561243026]
	TIME [epoch: 21.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021947259017082818		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.021947259017082818 | validation: 0.02909413719695538]
	TIME [epoch: 21.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027320221010078322		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.027320221010078322 | validation: 0.03134990394514758]
	TIME [epoch: 21.8 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023078423868447072		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.023078423868447072 | validation: 0.022074432740165642]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020444677451771615		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.020444677451771615 | validation: 0.02321834439684659]
	TIME [epoch: 21.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02032988473838213		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.02032988473838213 | validation: 0.030646656568399114]
	TIME [epoch: 21.8 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021279135261558793		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.021279135261558793 | validation: 0.025177558902372242]
	TIME [epoch: 21.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019229213760835126		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.019229213760835126 | validation: 0.023519484932420717]
	TIME [epoch: 21.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02154719176423492		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.02154719176423492 | validation: 0.02783008525252472]
	TIME [epoch: 21.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020779876456881386		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.020779876456881386 | validation: 0.026381165742316123]
	TIME [epoch: 21.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020490102435936495		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.020490102435936495 | validation: 0.03044035532163982]
	TIME [epoch: 21.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021792685967712187		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.021792685967712187 | validation: 0.024272156302289252]
	TIME [epoch: 21.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02116395179910024		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.02116395179910024 | validation: 0.027037751347283763]
	TIME [epoch: 21.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02213697439235194		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.02213697439235194 | validation: 0.02396145357420581]
	TIME [epoch: 21.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025733849806350812		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.025733849806350812 | validation: 0.026115258583881654]
	TIME [epoch: 21.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020914078161499445		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.020914078161499445 | validation: 0.02421769583701055]
	TIME [epoch: 21.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019734613809152694		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.019734613809152694 | validation: 0.02567054623075255]
	TIME [epoch: 21.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02284095436841205		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.02284095436841205 | validation: 0.027117041918517375]
	TIME [epoch: 21.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02103997783361361		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.02103997783361361 | validation: 0.02398514937277352]
	TIME [epoch: 21.8 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019857149396512314		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.019857149396512314 | validation: 0.02480108011705693]
	TIME [epoch: 21.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01939760443025216		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.01939760443025216 | validation: 0.023768082532175105]
	TIME [epoch: 21.8 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01980274878955934		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.01980274878955934 | validation: 0.023701738401258783]
	TIME [epoch: 21.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018780646504456988		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.018780646504456988 | validation: 0.025868951549326048]
	TIME [epoch: 21.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021840064568100094		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.021840064568100094 | validation: 0.04839630841196661]
	TIME [epoch: 21.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02854376860889156		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.02854376860889156 | validation: 0.02563136444341059]
	TIME [epoch: 21.8 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019911774131721748		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.019911774131721748 | validation: 0.025206476726594373]
	TIME [epoch: 21.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018898853696598092		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.018898853696598092 | validation: 0.024343305155458753]
	TIME [epoch: 21.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017849678836570675		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.017849678836570675 | validation: 0.027432603652572003]
	TIME [epoch: 21.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018943814849465588		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.018943814849465588 | validation: 0.02542476047580562]
	TIME [epoch: 21.7 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019482480384033236		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.019482480384033236 | validation: 0.03192579860770177]
	TIME [epoch: 21.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020920024700242353		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.020920024700242353 | validation: 0.023224006650865298]
	TIME [epoch: 21.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019279407542093386		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.019279407542093386 | validation: 0.023855926217105902]
	TIME [epoch: 21.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018260866733513525		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.018260866733513525 | validation: 0.02315042419141017]
	TIME [epoch: 21.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020674701784434316		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.020674701784434316 | validation: 0.021935756730914142]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021263398426106417		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.021263398426106417 | validation: 0.023208516625422036]
	TIME [epoch: 21.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017753708499000077		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.017753708499000077 | validation: 0.022065237848918475]
	TIME [epoch: 21.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02156704149849427		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.02156704149849427 | validation: 0.023224603230576964]
	TIME [epoch: 21.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01965769280388425		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.01965769280388425 | validation: 0.02262719893338406]
	TIME [epoch: 21.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017683511999263733		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.017683511999263733 | validation: 0.022037638090661425]
	TIME [epoch: 21.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018325415264863157		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.018325415264863157 | validation: 0.022429491786715272]
	TIME [epoch: 21.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01869007923250872		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.01869007923250872 | validation: 0.020908472686199973]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01887132991025155		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.01887132991025155 | validation: 0.02381683426523542]
	TIME [epoch: 21.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01974278652159133		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.01974278652159133 | validation: 0.026350425351765957]
	TIME [epoch: 21.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02184558288402268		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.02184558288402268 | validation: 0.024556537432006503]
	TIME [epoch: 22 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022437023665444165		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.022437023665444165 | validation: 0.027174112752907598]
	TIME [epoch: 21.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017482370896037148		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.017482370896037148 | validation: 0.022817026629001577]
	TIME [epoch: 21.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018080688448802635		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.018080688448802635 | validation: 0.025720335265144917]
	TIME [epoch: 21.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01797659657394242		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.01797659657394242 | validation: 0.02133026626779251]
	TIME [epoch: 21.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019232859609860355		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.019232859609860355 | validation: 0.02239630854141791]
	TIME [epoch: 21.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018350121859613457		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.018350121859613457 | validation: 0.02483215613773685]
	TIME [epoch: 22 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019560335609979076		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.019560335609979076 | validation: 0.02043804988069229]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018992948800739907		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.018992948800739907 | validation: 0.025991024668546032]
	TIME [epoch: 22 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02311640815415066		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.02311640815415066 | validation: 0.021952271488104927]
	TIME [epoch: 21.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01700425751535998		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.01700425751535998 | validation: 0.019918696407719694]
	TIME [epoch: 22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018358203226518252		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.018358203226518252 | validation: 0.022428325773601085]
	TIME [epoch: 21.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017871652726023098		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.017871652726023098 | validation: 0.023066579377545936]
	TIME [epoch: 21.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017903089424112603		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.017903089424112603 | validation: 0.02286463249261257]
	TIME [epoch: 21.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017982890664071577		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.017982890664071577 | validation: 0.02414725118660369]
	TIME [epoch: 21.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017600711267893164		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.017600711267893164 | validation: 0.019774026973412077]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018089128978300985		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.018089128978300985 | validation: 0.020729781850889452]
	TIME [epoch: 21.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01748045821239444		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.01748045821239444 | validation: 0.021689569287787043]
	TIME [epoch: 21.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017355127299222277		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.017355127299222277 | validation: 0.021787565402955754]
	TIME [epoch: 21.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018358025615090957		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.018358025615090957 | validation: 0.021679896650605213]
	TIME [epoch: 22 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018028981459862568		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.018028981459862568 | validation: 0.027479303455349546]
	TIME [epoch: 21.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01799800210094152		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.01799800210094152 | validation: 0.02120194160782702]
	TIME [epoch: 21.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017505491974304208		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.017505491974304208 | validation: 0.01883146905948031]
	TIME [epoch: 21.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016853936383871806		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.016853936383871806 | validation: 0.021540638488348085]
	TIME [epoch: 21.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01842566864775288		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.01842566864775288 | validation: 0.02697984443423013]
	TIME [epoch: 21.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01840875639825923		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.01840875639825923 | validation: 0.02005208194437803]
	TIME [epoch: 21.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016251634209890474		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.016251634209890474 | validation: 0.02422990304460263]
	TIME [epoch: 21.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020141860666233642		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.020141860666233642 | validation: 0.021347100947128557]
	TIME [epoch: 21.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018633432570882617		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.018633432570882617 | validation: 0.021498103649008088]
	TIME [epoch: 21.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01663626628290714		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.01663626628290714 | validation: 0.02245185940375713]
	TIME [epoch: 21.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01725644465032452		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.01725644465032452 | validation: 0.023618032700653102]
	TIME [epoch: 21.7 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017846623136859637		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.017846623136859637 | validation: 0.023516062260951934]
	TIME [epoch: 21.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017318245050365347		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.017318245050365347 | validation: 0.020003523761187192]
	TIME [epoch: 21.7 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01727413551459857		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.01727413551459857 | validation: 0.022209938968733217]
	TIME [epoch: 21.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01711633574758476		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.01711633574758476 | validation: 0.022867922059768703]
	TIME [epoch: 21.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017808742850589383		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.017808742850589383 | validation: 0.023230992163341457]
	TIME [epoch: 21.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016960710769360128		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.016960710769360128 | validation: 0.01987738068671396]
	TIME [epoch: 21.7 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016889393131541024		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.016889393131541024 | validation: 0.02180195852980068]
	TIME [epoch: 21.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01700651649850156		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.01700651649850156 | validation: 0.021911852970028133]
	TIME [epoch: 21.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01803998353525797		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.01803998353525797 | validation: 0.02259596623629733]
	TIME [epoch: 21.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017765585239294796		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.017765585239294796 | validation: 0.02205977614963884]
	TIME [epoch: 21.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016438473391046152		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.016438473391046152 | validation: 0.01934440130958409]
	TIME [epoch: 21.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015720646983580083		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.015720646983580083 | validation: 0.019910449858391056]
	TIME [epoch: 21.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016645615029387228		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.016645615029387228 | validation: 0.018514403262366697]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017917244729350103		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.017917244729350103 | validation: 0.018446834830916484]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016803341015301305		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.016803341015301305 | validation: 0.021012744688609405]
	TIME [epoch: 21.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016444387908091447		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.016444387908091447 | validation: 0.02377484298810213]
	TIME [epoch: 21.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017735492261757923		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.017735492261757923 | validation: 0.02333477324214327]
	TIME [epoch: 21.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01821846065949845		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.01821846065949845 | validation: 0.02054338961647799]
	TIME [epoch: 21.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01652247896027305		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.01652247896027305 | validation: 0.021102670555339487]
	TIME [epoch: 21.7 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01597603414000394		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.01597603414000394 | validation: 0.018961379338712378]
	TIME [epoch: 21.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0168365922879391		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0168365922879391 | validation: 0.018569862513331682]
	TIME [epoch: 21.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017273326700720584		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.017273326700720584 | validation: 0.017985255090454774]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016277284712382052		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.016277284712382052 | validation: 0.022742492766932514]
	TIME [epoch: 21.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016475566617731852		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.016475566617731852 | validation: 0.024711750470219514]
	TIME [epoch: 21.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01735769437413922		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.01735769437413922 | validation: 0.01771176190054703]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015573022254999931		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.015573022254999931 | validation: 0.019496478728567246]
	TIME [epoch: 21.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016508088632862114		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.016508088632862114 | validation: 0.017823179409721834]
	TIME [epoch: 21.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016234096720326597		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.016234096720326597 | validation: 0.01869931062205282]
	TIME [epoch: 21.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015814726463145946		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.015814726463145946 | validation: 0.020725532627242853]
	TIME [epoch: 21.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016580720316764094		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.016580720316764094 | validation: 0.020090784446557812]
	TIME [epoch: 21.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01862373271100503		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.01862373271100503 | validation: 0.020817105385643395]
	TIME [epoch: 21.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01622710215640726		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.01622710215640726 | validation: 0.02110256510382435]
	TIME [epoch: 21.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017783399133316734		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.017783399133316734 | validation: 0.021512351657090898]
	TIME [epoch: 21.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015636206714246682		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.015636206714246682 | validation: 0.019242493203535445]
	TIME [epoch: 21.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016294983930685288		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.016294983930685288 | validation: 0.021003409292332555]
	TIME [epoch: 21.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01671694660142955		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.01671694660142955 | validation: 0.01823287092422989]
	TIME [epoch: 21.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016150887563802566		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.016150887563802566 | validation: 0.01756697999057382]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015536373831317288		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.015536373831317288 | validation: 0.019973484879241617]
	TIME [epoch: 21.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015576047004377435		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.015576047004377435 | validation: 0.020470206871959166]
	TIME [epoch: 21.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01582340458927191		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.01582340458927191 | validation: 0.023796425391354827]
	TIME [epoch: 21.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015585043365063458		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.015585043365063458 | validation: 0.020442755171949992]
	TIME [epoch: 21.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01618915998254641		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.01618915998254641 | validation: 0.021185275774715965]
	TIME [epoch: 21.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015743836805762905		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.015743836805762905 | validation: 0.01791988034510849]
	TIME [epoch: 21.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015247932597690046		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.015247932597690046 | validation: 0.019182317599880407]
	TIME [epoch: 21.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01566961862793095		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.01566961862793095 | validation: 0.01846900874678262]
	TIME [epoch: 21.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01615516343251018		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.01615516343251018 | validation: 0.019690738538218608]
	TIME [epoch: 21.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015040799210613773		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.015040799210613773 | validation: 0.018215176317066952]
	TIME [epoch: 21.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016362638044763577		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.016362638044763577 | validation: 0.017763638130049265]
	TIME [epoch: 21.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014960424766289657		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.014960424766289657 | validation: 0.01966337990293316]
	TIME [epoch: 21.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015025166429654981		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.015025166429654981 | validation: 0.020291854419370682]
	TIME [epoch: 21.8 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01541758357312257		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.01541758357312257 | validation: 0.01740602089049499]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015740232511362074		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.015740232511362074 | validation: 0.017574927244587705]
	TIME [epoch: 21.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014347310341687732		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.014347310341687732 | validation: 0.0177064913655296]
	TIME [epoch: 21.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01474567145294398		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.01474567145294398 | validation: 0.018289677258592197]
	TIME [epoch: 21.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015547671723602302		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.015547671723602302 | validation: 0.01712847556638387]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015587306903138923		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.015587306903138923 | validation: 0.020474687225522228]
	TIME [epoch: 21.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016619056109085845		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.016619056109085845 | validation: 0.018003492298474547]
	TIME [epoch: 21.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015015568452724466		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.015015568452724466 | validation: 0.017543388925329757]
	TIME [epoch: 21.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01438861071863708		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.01438861071863708 | validation: 0.020215122281429198]
	TIME [epoch: 21.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015776038484923996		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.015776038484923996 | validation: 0.019699992427164147]
	TIME [epoch: 21.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0146510623927629		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0146510623927629 | validation: 0.021108482060791276]
	TIME [epoch: 21.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015658235837507833		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.015658235837507833 | validation: 0.018030049823967925]
	TIME [epoch: 21.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01399504198010573		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.01399504198010573 | validation: 0.01870876809713918]
	TIME [epoch: 21.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015714782174355194		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.015714782174355194 | validation: 0.016619300068112172]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01916248357862397		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.01916248357862397 | validation: 0.022167322296978834]
	TIME [epoch: 21.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017055896454088924		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.017055896454088924 | validation: 0.018043189823674446]
	TIME [epoch: 21.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014366983792470561		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.014366983792470561 | validation: 0.01683169555885242]
	TIME [epoch: 21.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014424540051148974		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.014424540051148974 | validation: 0.01713890349597439]
	TIME [epoch: 21.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014289712275077297		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.014289712275077297 | validation: 0.017427821991689702]
	TIME [epoch: 21.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015231782830875938		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.015231782830875938 | validation: 0.015758503479234784]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014929403451900254		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.014929403451900254 | validation: 0.01649218297619678]
	TIME [epoch: 21.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014247206796503292		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.014247206796503292 | validation: 0.01687812134952125]
	TIME [epoch: 21.7 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014583304905093599		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.014583304905093599 | validation: 0.018282612525317363]
	TIME [epoch: 21.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014606004767119977		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.014606004767119977 | validation: 0.019083935576201486]
	TIME [epoch: 21.7 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014799903665678172		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.014799903665678172 | validation: 0.017245226055708517]
	TIME [epoch: 21.8 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014466935937910937		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.014466935937910937 | validation: 0.017445877794951343]
	TIME [epoch: 21.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014836096017454658		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.014836096017454658 | validation: 0.017549890627628177]
	TIME [epoch: 21.8 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016027382049270247		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.016027382049270247 | validation: 0.017151951327334054]
	TIME [epoch: 21.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014080622491089198		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.014080622491089198 | validation: 0.01629611533723481]
	TIME [epoch: 21.8 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014520245797511731		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.014520245797511731 | validation: 0.020648079911242866]
	TIME [epoch: 21.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016091198237998417		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.016091198237998417 | validation: 0.018650137938582147]
	TIME [epoch: 21.8 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015610229046311107		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.015610229046311107 | validation: 0.019524134154886998]
	TIME [epoch: 21.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014371985757272342		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.014371985757272342 | validation: 0.016341696874545415]
	TIME [epoch: 21.8 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013544781018068027		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.013544781018068027 | validation: 0.0173634510842866]
	TIME [epoch: 21.8 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014000042850012334		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.014000042850012334 | validation: 0.018139911660530098]
	TIME [epoch: 21.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013941409065697874		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.013941409065697874 | validation: 0.017708728579200507]
	TIME [epoch: 21.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014965814406937956		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.014965814406937956 | validation: 0.01628865888618837]
	TIME [epoch: 21.8 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014349018322352303		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.014349018322352303 | validation: 0.015474049428112199]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01426963075150674		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.01426963075150674 | validation: 0.016301342525605673]
	TIME [epoch: 21.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013579316658656821		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.013579316658656821 | validation: 0.017191575500484717]
	TIME [epoch: 21.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014694431838200229		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.014694431838200229 | validation: 0.016249505535635724]
	TIME [epoch: 21.8 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014204115658965916		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.014204115658965916 | validation: 0.01722423622068607]
	TIME [epoch: 21.8 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01430166471166205		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.01430166471166205 | validation: 0.016573803914946332]
	TIME [epoch: 21.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013102342739672524		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.013102342739672524 | validation: 0.017913940640818176]
	TIME [epoch: 21.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015129873212795632		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.015129873212795632 | validation: 0.016234527117165058]
	TIME [epoch: 21.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014563169964470084		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.014563169964470084 | validation: 0.015526061463247295]
	TIME [epoch: 21.8 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014205613367334858		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.014205613367334858 | validation: 0.01575895614853127]
	TIME [epoch: 21.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015601439106344154		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.015601439106344154 | validation: 0.024756169138962746]
	TIME [epoch: 21.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015443163289704038		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.015443163289704038 | validation: 0.017861274434328923]
	TIME [epoch: 21.7 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014715234597630484		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.014715234597630484 | validation: 0.017324933947915927]
	TIME [epoch: 21.8 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013768785956081145		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.013768785956081145 | validation: 0.01484791438059388]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013872447042989174		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.013872447042989174 | validation: 0.016543803180623085]
	TIME [epoch: 21.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013687572381928269		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.013687572381928269 | validation: 0.0162832537121842]
	TIME [epoch: 21.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01410905747603855		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.01410905747603855 | validation: 0.017687038891437655]
	TIME [epoch: 21.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013954670778633198		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.013954670778633198 | validation: 0.01912729793673057]
	TIME [epoch: 21.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014642877844826683		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.014642877844826683 | validation: 0.015617251172807548]
	TIME [epoch: 21.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0136465167623582		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0136465167623582 | validation: 0.014906176280753934]
	TIME [epoch: 21.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013841586632829302		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.013841586632829302 | validation: 0.017639682845946435]
	TIME [epoch: 21.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014049514701382453		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.014049514701382453 | validation: 0.01619194949423595]
	TIME [epoch: 21.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014453320267916996		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.014453320267916996 | validation: 0.015634561349929986]
	TIME [epoch: 21.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014519583114114602		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.014519583114114602 | validation: 0.015625349935417376]
	TIME [epoch: 21.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013426512506508656		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.013426512506508656 | validation: 0.016772860551259358]
	TIME [epoch: 21.8 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014164452473225841		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.014164452473225841 | validation: 0.014805465419942277]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014313778133288116		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.014313778133288116 | validation: 0.01653695252573476]
	TIME [epoch: 21.8 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013628763350881926		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.013628763350881926 | validation: 0.015817799138820976]
	TIME [epoch: 21.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013547076982203596		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.013547076982203596 | validation: 0.015402958920828241]
	TIME [epoch: 21.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015064908736462513		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.015064908736462513 | validation: 0.019329575025997463]
	TIME [epoch: 21.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014518836395465267		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.014518836395465267 | validation: 0.01620957103367931]
	TIME [epoch: 21.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013938278458702796		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.013938278458702796 | validation: 0.01475915189514539]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013140682562229174		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.013140682562229174 | validation: 0.01571663968887651]
	TIME [epoch: 21.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013458600002707453		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.013458600002707453 | validation: 0.015870231289610237]
	TIME [epoch: 21.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013823654938344785		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.013823654938344785 | validation: 0.01728368711447375]
	TIME [epoch: 21.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014165417771484747		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.014165417771484747 | validation: 0.015652486028597687]
	TIME [epoch: 21.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012975009134301822		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.012975009134301822 | validation: 0.017122006992891535]
	TIME [epoch: 21.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014413785806083903		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.014413785806083903 | validation: 0.015954600713492133]
	TIME [epoch: 21.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013388169805981237		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.013388169805981237 | validation: 0.01576389503354389]
	TIME [epoch: 21.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013555256087754838		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.013555256087754838 | validation: 0.015412466366624007]
	TIME [epoch: 21.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012947773102304253		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.012947773102304253 | validation: 0.014123618706355642]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013264548478218884		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.013264548478218884 | validation: 0.015698808771518462]
	TIME [epoch: 21.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014083983399555284		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.014083983399555284 | validation: 0.01756571536784663]
	TIME [epoch: 21.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013614565886041407		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.013614565886041407 | validation: 0.015206616474946921]
	TIME [epoch: 21.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013343250256063324		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.013343250256063324 | validation: 0.015078497664289392]
	TIME [epoch: 21.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013886672045911146		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.013886672045911146 | validation: 0.01670984209550197]
	TIME [epoch: 21.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013331197229275378		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.013331197229275378 | validation: 0.016174515152178555]
	TIME [epoch: 21.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01224516327384383		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.01224516327384383 | validation: 0.015507038788457295]
	TIME [epoch: 21.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013759001419976553		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.013759001419976553 | validation: 0.015870852386642407]
	TIME [epoch: 21.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013435981694209906		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.013435981694209906 | validation: 0.017595230746435514]
	TIME [epoch: 21.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013442312667884818		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.013442312667884818 | validation: 0.014756237270581913]
	TIME [epoch: 21.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01312520224890817		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.01312520224890817 | validation: 0.014072971750883206]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013291199258312902		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.013291199258312902 | validation: 0.015032034784606218]
	TIME [epoch: 21.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012908129434514054		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.012908129434514054 | validation: 0.015528022201644787]
	TIME [epoch: 21.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012233544132090539		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.012233544132090539 | validation: 0.016672452541544692]
	TIME [epoch: 21.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01294140726080364		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.01294140726080364 | validation: 0.014665259458512915]
	TIME [epoch: 21.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012264307422162694		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.012264307422162694 | validation: 0.014185302788257782]
	TIME [epoch: 21.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013450537051513504		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.013450537051513504 | validation: 0.014995657057846282]
	TIME [epoch: 21.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013050287767715883		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.013050287767715883 | validation: 0.015916357202133732]
	TIME [epoch: 21.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013169644888691809		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.013169644888691809 | validation: 0.015585426143520298]
	TIME [epoch: 21.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012076656758537146		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.012076656758537146 | validation: 0.01534326888110455]
	TIME [epoch: 21.7 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01255975253108447		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.01255975253108447 | validation: 0.014664785277562566]
	TIME [epoch: 21.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012665045247832948		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.012665045247832948 | validation: 0.01975847871109052]
	TIME [epoch: 21.7 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013857598582844528		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.013857598582844528 | validation: 0.016501600234093368]
	TIME [epoch: 21.7 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013295303099951295		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.013295303099951295 | validation: 0.01553762897235278]
	TIME [epoch: 21.7 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013213480453096156		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.013213480453096156 | validation: 0.014848766612641014]
	TIME [epoch: 21.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01217195958736		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.01217195958736 | validation: 0.01317512502895541]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012731184647111659		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.012731184647111659 | validation: 0.013084464692067995]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012766247757164804		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.012766247757164804 | validation: 0.01568169740514748]
	TIME [epoch: 21.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01289302714085611		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.01289302714085611 | validation: 0.014239919310107395]
	TIME [epoch: 21.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012965267756256848		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.012965267756256848 | validation: 0.014533100250705066]
	TIME [epoch: 21.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01325744414884063		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.01325744414884063 | validation: 0.014458552028597686]
	TIME [epoch: 21.7 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012274274825318511		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.012274274825318511 | validation: 0.016891164320149085]
	TIME [epoch: 21.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012986091711514505		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.012986091711514505 | validation: 0.015702751325240932]
	TIME [epoch: 21.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012244533993959413		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.012244533993959413 | validation: 0.014976608850104128]
	TIME [epoch: 21.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011933005987766986		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.011933005987766986 | validation: 0.015372606460513779]
	TIME [epoch: 21.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012680356106939324		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.012680356106939324 | validation: 0.0148855970866774]
	TIME [epoch: 21.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011737078312321741		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.011737078312321741 | validation: 0.01467783679179663]
	TIME [epoch: 21.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012679956895567156		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.012679956895567156 | validation: 0.015560337871806664]
	TIME [epoch: 21.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01328176191012568		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.01328176191012568 | validation: 0.016502774004596876]
	TIME [epoch: 21.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012853100794331197		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.012853100794331197 | validation: 0.014507385173718312]
	TIME [epoch: 21.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013101137162854262		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.013101137162854262 | validation: 0.013999771359893082]
	TIME [epoch: 21.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012668817592632038		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.012668817592632038 | validation: 0.016582315849684386]
	TIME [epoch: 21.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012555351086133724		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.012555351086133724 | validation: 0.013419320372723484]
	TIME [epoch: 21.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012886814055883898		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.012886814055883898 | validation: 0.013550974023951563]
	TIME [epoch: 21.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012201647890401601		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.012201647890401601 | validation: 0.01509681685757414]
	TIME [epoch: 21.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011843964876646388		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.011843964876646388 | validation: 0.013780766060843278]
	TIME [epoch: 21.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011657220397599068		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.011657220397599068 | validation: 0.015440148346012931]
	TIME [epoch: 21.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012508870068029584		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.012508870068029584 | validation: 0.013995849569925015]
	TIME [epoch: 21.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011866373317721592		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.011866373317721592 | validation: 0.013606198219213914]
	TIME [epoch: 21.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012472517440459909		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.012472517440459909 | validation: 0.015293251797341208]
	TIME [epoch: 21.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011829338033735167		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.011829338033735167 | validation: 0.013759000189525965]
	TIME [epoch: 21.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012709387519111838		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.012709387519111838 | validation: 0.013024889955232129]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011729227600508168		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.011729227600508168 | validation: 0.013489610116974318]
	TIME [epoch: 21.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012298334943928574		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.012298334943928574 | validation: 0.01571663427252664]
	TIME [epoch: 21.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012116513205504418		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.012116513205504418 | validation: 0.013098725294626834]
	TIME [epoch: 21.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012374103680132148		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.012374103680132148 | validation: 0.013784476088270142]
	TIME [epoch: 21.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012916007122923432		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.012916007122923432 | validation: 0.012548215696519737]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012704149792789957		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.012704149792789957 | validation: 0.012968362018761479]
	TIME [epoch: 21.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012811736092457372		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.012811736092457372 | validation: 0.013504178097144087]
	TIME [epoch: 21.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011817407169715998		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.011817407169715998 | validation: 0.01455565327474101]
	TIME [epoch: 21.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011804679028370688		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.011804679028370688 | validation: 0.014011796619640905]
	TIME [epoch: 21.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0120224882579794		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0120224882579794 | validation: 0.01343085736476527]
	TIME [epoch: 21.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012074810852609296		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.012074810852609296 | validation: 0.014269663106896788]
	TIME [epoch: 21.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012073388377394724		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.012073388377394724 | validation: 0.014346214890124163]
	TIME [epoch: 21.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012461432346630481		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.012461432346630481 | validation: 0.015750558279989585]
	TIME [epoch: 21.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012683048040934628		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.012683048040934628 | validation: 0.014055857007841979]
	TIME [epoch: 21.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012241133308671406		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.012241133308671406 | validation: 0.012158395791280717]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011975470419414791		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.011975470419414791 | validation: 0.014274145982508216]
	TIME [epoch: 21.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011995896707294918		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.011995896707294918 | validation: 0.01449525417137933]
	TIME [epoch: 21.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011579204833218395		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.011579204833218395 | validation: 0.015648047344336294]
	TIME [epoch: 21.7 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011758182092098933		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.011758182092098933 | validation: 0.014563038506693573]
	TIME [epoch: 21.7 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011877171327895446		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.011877171327895446 | validation: 0.014546358915416151]
	TIME [epoch: 21.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012119203576746177		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.012119203576746177 | validation: 0.01371344480828103]
	TIME [epoch: 21.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011789825470382107		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.011789825470382107 | validation: 0.01361527820558469]
	TIME [epoch: 21.7 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011552150800630597		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.011552150800630597 | validation: 0.013062473279739261]
	TIME [epoch: 21.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011793642022480587		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.011793642022480587 | validation: 0.016004711338821927]
	TIME [epoch: 21.7 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011960960321767692		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.011960960321767692 | validation: 0.016518153162722513]
	TIME [epoch: 21.8 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01233455566032478		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.01233455566032478 | validation: 0.014331193386087113]
	TIME [epoch: 21.8 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011889679624679208		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.011889679624679208 | validation: 0.013283396988934645]
	TIME [epoch: 21.8 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0124374979483747		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0124374979483747 | validation: 0.012658419641718136]
	TIME [epoch: 21.7 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01200392027103574		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.01200392027103574 | validation: 0.013833703177401159]
	TIME [epoch: 21.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011658902398352518		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.011658902398352518 | validation: 0.01397200039072248]
	TIME [epoch: 21.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011804425481308011		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.011804425481308011 | validation: 0.012337590099584204]
	TIME [epoch: 21.8 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011537035882947896		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.011537035882947896 | validation: 0.012697077996806897]
	TIME [epoch: 21.8 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011396996699081655		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.011396996699081655 | validation: 0.013714401710365998]
	TIME [epoch: 21.8 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011476478495500103		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.011476478495500103 | validation: 0.013271726281902242]
	TIME [epoch: 21.7 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011933747048809373		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.011933747048809373 | validation: 0.012888457911195505]
	TIME [epoch: 21.8 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012080178259850359		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.012080178259850359 | validation: 0.014078862904335361]
	TIME [epoch: 21.8 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01181920813814873		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.01181920813814873 | validation: 0.01196933240451393]
	TIME [epoch: 21.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_915.pth
	Model improved!!!
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011326722300015258		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.011326722300015258 | validation: 0.012723537752431695]
	TIME [epoch: 21.8 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012309038519706241		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.012309038519706241 | validation: 0.015845966925164063]
	TIME [epoch: 21.8 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012632028859018403		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.012632028859018403 | validation: 0.01240227532065935]
	TIME [epoch: 21.7 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011299520609877007		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.011299520609877007 | validation: 0.013311909293168776]
	TIME [epoch: 21.8 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012425584704335708		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.012425584704335708 | validation: 0.012531786895431397]
	TIME [epoch: 21.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011407662284305758		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.011407662284305758 | validation: 0.013783832157680968]
	TIME [epoch: 21.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011179815321145684		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.011179815321145684 | validation: 0.013141719104680516]
	TIME [epoch: 21.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012231681714832226		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.012231681714832226 | validation: 0.013879321184376639]
	TIME [epoch: 21.8 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011519267866868615		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.011519267866868615 | validation: 0.013665928474567907]
	TIME [epoch: 21.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01178217834945088		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.01178217834945088 | validation: 0.012337556006757178]
	TIME [epoch: 21.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0117116588900007		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0117116588900007 | validation: 0.012777279547931734]
	TIME [epoch: 21.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011233764892042173		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.011233764892042173 | validation: 0.015147678469059215]
	TIME [epoch: 21.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011775951236407414		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.011775951236407414 | validation: 0.013267985093908715]
	TIME [epoch: 21.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011530885385732144		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.011530885385732144 | validation: 0.013218362458084925]
	TIME [epoch: 21.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010777564640725148		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.010777564640725148 | validation: 0.011980006549493525]
	TIME [epoch: 21.8 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011717551354005436		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.011717551354005436 | validation: 0.012861042677658741]
	TIME [epoch: 21.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011061403977179468		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.011061403977179468 | validation: 0.012742913929096576]
	TIME [epoch: 21.8 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010998199072329766		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.010998199072329766 | validation: 0.014411046169112384]
	TIME [epoch: 21.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011892058551587384		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.011892058551587384 | validation: 0.014140285264488907]
	TIME [epoch: 21.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011486396103208693		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.011486396103208693 | validation: 0.012631602510769611]
	TIME [epoch: 21.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012196873032682659		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.012196873032682659 | validation: 0.013433723060084207]
	TIME [epoch: 21.8 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011800500341666211		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.011800500341666211 | validation: 0.01373731366438454]
	TIME [epoch: 21.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011127012303087673		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.011127012303087673 | validation: 0.014612576066025498]
	TIME [epoch: 21.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01095306603847491		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.01095306603847491 | validation: 0.014948867987950285]
	TIME [epoch: 21.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011655456357796416		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.011655456357796416 | validation: 0.013747044250817691]
	TIME [epoch: 21.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010903035311343926		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.010903035311343926 | validation: 0.013330584786972379]
	TIME [epoch: 21.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011360115677196796		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.011360115677196796 | validation: 0.012740977796249074]
	TIME [epoch: 21.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010826761790165201		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.010826761790165201 | validation: 0.012251413256198255]
	TIME [epoch: 21.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011736889631721089		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.011736889631721089 | validation: 0.013907538623120507]
	TIME [epoch: 21.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011505111152333389		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.011505111152333389 | validation: 0.012281761803997211]
	TIME [epoch: 21.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011274620773366266		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.011274620773366266 | validation: 0.013081147494441713]
	TIME [epoch: 21.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011579510447441241		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.011579510447441241 | validation: 0.01152827672411124]
	TIME [epoch: 21.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_947.pth
	Model improved!!!
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010626888026197934		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.010626888026197934 | validation: 0.013194707699040888]
	TIME [epoch: 21.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011352151824469687		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.011352151824469687 | validation: 0.012165731878810718]
	TIME [epoch: 21.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010852635434254396		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.010852635434254396 | validation: 0.013800605562182263]
	TIME [epoch: 21.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01062303100693596		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.01062303100693596 | validation: 0.012898514899200512]
	TIME [epoch: 21.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011178407049557115		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.011178407049557115 | validation: 0.012656694220130464]
	TIME [epoch: 21.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01204934048266838		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.01204934048266838 | validation: 0.014356847899401209]
	TIME [epoch: 21.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011406114758703609		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.011406114758703609 | validation: 0.01184720245331768]
	TIME [epoch: 21.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011497163499404963		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.011497163499404963 | validation: 0.012546669797530555]
	TIME [epoch: 21.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010926330214284262		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.010926330214284262 | validation: 0.012942649812578439]
	TIME [epoch: 21.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011484810807086208		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.011484810807086208 | validation: 0.013515593166636433]
	TIME [epoch: 21.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010456303162238879		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.010456303162238879 | validation: 0.012860536811566025]
	TIME [epoch: 21.8 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011235695175205682		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.011235695175205682 | validation: 0.012638534171331359]
	TIME [epoch: 21.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012454845449628483		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.012454845449628483 | validation: 0.013798743332894235]
	TIME [epoch: 21.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01159017261594917		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.01159017261594917 | validation: 0.013482447854363208]
	TIME [epoch: 21.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011143011440845965		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.011143011440845965 | validation: 0.011982362972971894]
	TIME [epoch: 21.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010609428190674265		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.010609428190674265 | validation: 0.011411895727515296]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01105481845067958		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.01105481845067958 | validation: 0.011537475224315277]
	TIME [epoch: 21.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010962273127829391		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.010962273127829391 | validation: 0.012331285437223493]
	TIME [epoch: 21.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010785202757902568		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.010785202757902568 | validation: 0.013510992172113854]
	TIME [epoch: 21.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010393137341272105		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.010393137341272105 | validation: 0.011507729113136612]
	TIME [epoch: 21.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011195783396371112		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.011195783396371112 | validation: 0.014368383835052451]
	TIME [epoch: 21.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011233285915860558		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.011233285915860558 | validation: 0.012117213151390374]
	TIME [epoch: 21.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010512249296230286		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.010512249296230286 | validation: 0.012911525508174964]
	TIME [epoch: 21.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010877352298869554		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.010877352298869554 | validation: 0.01289090647002528]
	TIME [epoch: 21.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010637213172201034		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.010637213172201034 | validation: 0.01277479792055232]
	TIME [epoch: 21.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011263990416646818		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.011263990416646818 | validation: 0.013026360613061054]
	TIME [epoch: 21.7 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010698744844721352		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.010698744844721352 | validation: 0.012705667059661932]
	TIME [epoch: 21.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010358205812580307		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.010358205812580307 | validation: 0.012143816168836894]
	TIME [epoch: 21.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0112801986219958		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0112801986219958 | validation: 0.011385090156133783]
	TIME [epoch: 21.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010829813996137479		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.010829813996137479 | validation: 0.011392326928626451]
	TIME [epoch: 21.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011390707533526303		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.011390707533526303 | validation: 0.012717161321548308]
	TIME [epoch: 21.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011307331335120487		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.011307331335120487 | validation: 0.011942710010846475]
	TIME [epoch: 21.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010730386390661425		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.010730386390661425 | validation: 0.013566628214433309]
	TIME [epoch: 21.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011038885275561603		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.011038885275561603 | validation: 0.013024310397246386]
	TIME [epoch: 21.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010843958002264373		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.010843958002264373 | validation: 0.013296634976059844]
	TIME [epoch: 21.7 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010856447481421434		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.010856447481421434 | validation: 0.012247172545010537]
	TIME [epoch: 21.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011069278416352106		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.011069278416352106 | validation: 0.011690688684910978]
	TIME [epoch: 21.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01092637718103611		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.01092637718103611 | validation: 0.015499157815442111]
	TIME [epoch: 21.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01414239243251212		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.01414239243251212 | validation: 0.013233556094728025]
	TIME [epoch: 21.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010496742448740042		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.010496742448740042 | validation: 0.013324094458703804]
	TIME [epoch: 21.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011059263233170897		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.011059263233170897 | validation: 0.012686152361202605]
	TIME [epoch: 21.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0107890086103039		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0107890086103039 | validation: 0.012272473611574155]
	TIME [epoch: 21.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010609517391374366		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.010609517391374366 | validation: 0.011899779892285125]
	TIME [epoch: 21.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010245422298597567		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.010245422298597567 | validation: 0.013256973771812012]
	TIME [epoch: 21.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010967956909550497		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.010967956909550497 | validation: 0.012875231608010369]
	TIME [epoch: 21.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010727966839987068		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.010727966839987068 | validation: 0.012110144177109376]
	TIME [epoch: 21.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010918621354879312		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.010918621354879312 | validation: 0.012560886004128895]
	TIME [epoch: 21.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010347061138164866		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.010347061138164866 | validation: 0.012000718695631466]
	TIME [epoch: 21.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010978207580014445		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.010978207580014445 | validation: 0.011840836637088303]
	TIME [epoch: 21.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01038824622883066		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.01038824622883066 | validation: 0.013181121018507224]
	TIME [epoch: 21.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01068838851063629		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.01068838851063629 | validation: 0.012407372077940713]
	TIME [epoch: 21.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009752093117217833		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.009752093117217833 | validation: 0.012283880775199072]
	TIME [epoch: 21.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010876037283257409		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.010876037283257409 | validation: 0.01199277835607587]
	TIME [epoch: 21.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011018324837175611		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.011018324837175611 | validation: 0.012611796505715641]
	TIME [epoch: 152 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01091777070225361		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.01091777070225361 | validation: 0.011197910305058026]
	TIME [epoch: 47.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011873973278904501		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.011873973278904501 | validation: 0.012412185941473603]
	TIME [epoch: 47.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010939983965430763		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.010939983965430763 | validation: 0.012718464993099027]
	TIME [epoch: 47.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010609749411069656		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.010609749411069656 | validation: 0.013993938815773932]
	TIME [epoch: 47.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011607598503145682		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.011607598503145682 | validation: 0.012551066010821012]
	TIME [epoch: 47.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01038744055217382		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.01038744055217382 | validation: 0.012007546384317329]
	TIME [epoch: 47.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010293156699167279		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.010293156699167279 | validation: 0.011737465423120073]
	TIME [epoch: 47.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010168271544505864		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.010168271544505864 | validation: 0.012623664027560147]
	TIME [epoch: 47.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010939342403731301		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.010939342403731301 | validation: 0.012540563264079495]
	TIME [epoch: 47.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010436625234668196		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.010436625234668196 | validation: 0.011877614546876837]
	TIME [epoch: 47.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010735074039405282		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.010735074039405282 | validation: 0.012393131394668432]
	TIME [epoch: 47.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010752428369616188		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.010752428369616188 | validation: 0.011846051022781842]
	TIME [epoch: 47.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01033547410060803		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.01033547410060803 | validation: 0.0132502992058309]
	TIME [epoch: 47.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009966368923086893		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.009966368923086893 | validation: 0.011975525793628747]
	TIME [epoch: 47.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010721985309011504		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.010721985309011504 | validation: 0.011418535935746989]
	TIME [epoch: 47.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011152103479459725		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.011152103479459725 | validation: 0.011293695312096272]
	TIME [epoch: 47.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010396702328749832		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.010396702328749832 | validation: 0.012867196869605708]
	TIME [epoch: 47.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010442384618315955		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.010442384618315955 | validation: 0.011673319205498793]
	TIME [epoch: 47.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010699411055720486		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.010699411055720486 | validation: 0.01324353486673737]
	TIME [epoch: 47.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010451447822503377		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.010451447822503377 | validation: 0.011828144844802324]
	TIME [epoch: 47.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010644795826467372		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.010644795826467372 | validation: 0.011641696191509666]
	TIME [epoch: 47.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010361539226300174		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.010361539226300174 | validation: 0.011733431430436153]
	TIME [epoch: 47.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01016582894370654		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.01016582894370654 | validation: 0.011607394028099117]
	TIME [epoch: 47.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010876885191932781		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.010876885191932781 | validation: 0.012564383479007762]
	TIME [epoch: 47.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010927703206933655		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.010927703206933655 | validation: 0.012529875371100504]
	TIME [epoch: 47.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010643298962866407		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.010643298962866407 | validation: 0.011275543641187213]
	TIME [epoch: 47.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010289700015625158		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.010289700015625158 | validation: 0.012467968258843646]
	TIME [epoch: 47.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010761656384294285		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.010761656384294285 | validation: 0.012967779284274422]
	TIME [epoch: 47.6 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010393119098253942		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.010393119098253942 | validation: 0.012644830940367818]
	TIME [epoch: 47.6 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010166092713163464		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.010166092713163464 | validation: 0.010908601980198648]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010640508177509417		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.010640508177509417 | validation: 0.011594157006478947]
	TIME [epoch: 47.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010593581916243629		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.010593581916243629 | validation: 0.011258656478035253]
	TIME [epoch: 47.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01041314957184732		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.01041314957184732 | validation: 0.010754215382709785]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010514712785975362		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.010514712785975362 | validation: 0.010680674414900754]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1035.pth
	Model improved!!!
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010934013371014817		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.010934013371014817 | validation: 0.014680877588714954]
	TIME [epoch: 47.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009911150256070006		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.009911150256070006 | validation: 0.011290019723835585]
	TIME [epoch: 47.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009887003317924638		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.009887003317924638 | validation: 0.011517558507183879]
	TIME [epoch: 47.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010214221013745414		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.010214221013745414 | validation: 0.012327472956431397]
	TIME [epoch: 47.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009871688395725477		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.009871688395725477 | validation: 0.011605012584027553]
	TIME [epoch: 47.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010178888850519803		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.010178888850519803 | validation: 0.01143025899445791]
	TIME [epoch: 47.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010214288152058479		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.010214288152058479 | validation: 0.010628605052240434]
	TIME [epoch: 47.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1042.pth
	Model improved!!!
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010283737819287556		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.010283737819287556 | validation: 0.011505089804644944]
	TIME [epoch: 47.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010372951712791267		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.010372951712791267 | validation: 0.011054569118397481]
	TIME [epoch: 47.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0100970806446543		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0100970806446543 | validation: 0.01059915962009609]
	TIME [epoch: 47.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1045.pth
	Model improved!!!
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00977344683258705		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.00977344683258705 | validation: 0.011942251961084385]
	TIME [epoch: 47.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01016953864567805		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.01016953864567805 | validation: 0.012018387363114804]
	TIME [epoch: 47.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011402454286567087		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.011402454286567087 | validation: 0.014261864947570247]
	TIME [epoch: 47.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010108861978698362		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.010108861978698362 | validation: 0.011800659701242014]
	TIME [epoch: 47.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010408361347131634		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.010408361347131634 | validation: 0.012221456350970033]
	TIME [epoch: 47.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01009401265772112		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.01009401265772112 | validation: 0.010584662337993645]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010457442007695831		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.010457442007695831 | validation: 0.012122436596578036]
	TIME [epoch: 47.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009580335778421366		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.009580335778421366 | validation: 0.011907827306613893]
	TIME [epoch: 47.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010381570832502145		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.010381570832502145 | validation: 0.011182392359888316]
	TIME [epoch: 47.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010088075077809186		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.010088075077809186 | validation: 0.012789083203558854]
	TIME [epoch: 47.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009696907150115083		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.009696907150115083 | validation: 0.01130469231779767]
	TIME [epoch: 47.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010663859570456643		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.010663859570456643 | validation: 0.01232753793751221]
	TIME [epoch: 47.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010201905415066466		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.010201905415066466 | validation: 0.011949087688572963]
	TIME [epoch: 47.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010104395729831473		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.010104395729831473 | validation: 0.01010463500870087]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009874212585945036		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.009874212585945036 | validation: 0.011158199446491223]
	TIME [epoch: 47.6 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010350560979585937		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.010350560979585937 | validation: 0.011418854597415654]
	TIME [epoch: 47.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00983077516032485		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.00983077516032485 | validation: 0.011617848651620247]
	TIME [epoch: 47.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00995057914164384		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.00995057914164384 | validation: 0.010781295211324864]
	TIME [epoch: 47.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010052590190297373		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.010052590190297373 | validation: 0.011118034436212891]
	TIME [epoch: 47.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010152047147679517		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.010152047147679517 | validation: 0.013183329337744732]
	TIME [epoch: 47.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011401363644118385		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.011401363644118385 | validation: 0.012212012099656569]
	TIME [epoch: 47.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010298827970981531		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.010298827970981531 | validation: 0.011019928878502129]
	TIME [epoch: 47.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0100634731419643		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0100634731419643 | validation: 0.010505124562311171]
	TIME [epoch: 47.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010386957167660589		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.010386957167660589 | validation: 0.011539624090672773]
	TIME [epoch: 47.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00970253898247894		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.00970253898247894 | validation: 0.012324789899000058]
	TIME [epoch: 47.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010469113157890838		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.010469113157890838 | validation: 0.011605667581734144]
	TIME [epoch: 47.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010902130773325107		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.010902130773325107 | validation: 0.011059674363190837]
	TIME [epoch: 47.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010149458889541543		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.010149458889541543 | validation: 0.011011440846226486]
	TIME [epoch: 47.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009721470838927068		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.009721470838927068 | validation: 0.011347516698468566]
	TIME [epoch: 47.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010102845664936022		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.010102845664936022 | validation: 0.009997130099082287]
	TIME [epoch: 47.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1075.pth
	Model improved!!!
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009998104144696192		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.009998104144696192 | validation: 0.011451793531914074]
	TIME [epoch: 47.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009887698248907896		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.009887698248907896 | validation: 0.010755398568940817]
	TIME [epoch: 47.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010222822971466199		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.010222822971466199 | validation: 0.011182452607501212]
	TIME [epoch: 47.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010184162494801028		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.010184162494801028 | validation: 0.012207896480285883]
	TIME [epoch: 47.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010255567337841612		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.010255567337841612 | validation: 0.010226952587031077]
	TIME [epoch: 47.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009967807991042634		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.009967807991042634 | validation: 0.010417350037708401]
	TIME [epoch: 47.6 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009407850912368279		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.009407850912368279 | validation: 0.01036606526027357]
	TIME [epoch: 47.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010161413829400508		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.010161413829400508 | validation: 0.011014350920690547]
	TIME [epoch: 47.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01010369615054293		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.01010369615054293 | validation: 0.009931315856317215]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01037433845718941		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.01037433845718941 | validation: 0.011421424191034245]
	TIME [epoch: 47.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010097935519922902		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.010097935519922902 | validation: 0.011518259208251004]
	TIME [epoch: 47.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009945062968739316		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.009945062968739316 | validation: 0.01072856301012605]
	TIME [epoch: 47.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01000141157485173		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.01000141157485173 | validation: 0.010843079002667693]
	TIME [epoch: 47.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00994847449405834		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.00994847449405834 | validation: 0.011598176523642973]
	TIME [epoch: 47.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010000044296432493		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.010000044296432493 | validation: 0.01113955794387807]
	TIME [epoch: 47.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0093860046642931		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0093860046642931 | validation: 0.010580096996134842]
	TIME [epoch: 47.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009859445062097207		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.009859445062097207 | validation: 0.011442215735401372]
	TIME [epoch: 47.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010212811309373418		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.010212811309373418 | validation: 0.010908851817725933]
	TIME [epoch: 47.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009774378132801395		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.009774378132801395 | validation: 0.011616201807013955]
	TIME [epoch: 47.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009459279310769793		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.009459279310769793 | validation: 0.010627386020572522]
	TIME [epoch: 47.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010223949325250973		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.010223949325250973 | validation: 0.012244470674372565]
	TIME [epoch: 47.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009473870417828403		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.009473870417828403 | validation: 0.012029854100008524]
	TIME [epoch: 47.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00967797574600645		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.00967797574600645 | validation: 0.011321277716413756]
	TIME [epoch: 47.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009695367485427374		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.009695367485427374 | validation: 0.01084779373211309]
	TIME [epoch: 47.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009345157823561762		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.009345157823561762 | validation: 0.01127214936802633]
	TIME [epoch: 47.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009699558829931364		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.009699558829931364 | validation: 0.011683788607769655]
	TIME [epoch: 47.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009966840233377683		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.009966840233377683 | validation: 0.011809624654790313]
	TIME [epoch: 47.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01026934765349948		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.01026934765349948 | validation: 0.011135209403399186]
	TIME [epoch: 47.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009839808177100963		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.009839808177100963 | validation: 0.010067242692184215]
	TIME [epoch: 47.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009643023194212763		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.009643023194212763 | validation: 0.010278501809604011]
	TIME [epoch: 47.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009662271312873163		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.009662271312873163 | validation: 0.011266988404847703]
	TIME [epoch: 47.6 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00930074715904054		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.00930074715904054 | validation: 0.011760204789670681]
	TIME [epoch: 47.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01023841541976651		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.01023841541976651 | validation: 0.010552777556201242]
	TIME [epoch: 47.6 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009266537157229278		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.009266537157229278 | validation: 0.010025353635461313]
	TIME [epoch: 47.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00975464451746207		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.00975464451746207 | validation: 0.011006659876417925]
	TIME [epoch: 47.6 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009933735964643784		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.009933735964643784 | validation: 0.011044025396691333]
	TIME [epoch: 47.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00931835101758576		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.00931835101758576 | validation: 0.010748679462233322]
	TIME [epoch: 47.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009476207900727326		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.009476207900727326 | validation: 0.010883089097992918]
	TIME [epoch: 47.6 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009875706542132105		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.009875706542132105 | validation: 0.011090402321633271]
	TIME [epoch: 47.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009364796291256278		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.009364796291256278 | validation: 0.010035483661601527]
	TIME [epoch: 47.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009673743000076453		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.009673743000076453 | validation: 0.011341932251860422]
	TIME [epoch: 47.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009996207548317888		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.009996207548317888 | validation: 0.010376555982881579]
	TIME [epoch: 47.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01002773366410234		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.01002773366410234 | validation: 0.011750445807141248]
	TIME [epoch: 47.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010050507965861216		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.010050507965861216 | validation: 0.011408459499576397]
	TIME [epoch: 47.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009743607063443672		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.009743607063443672 | validation: 0.010277904395530646]
	TIME [epoch: 47.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01009320021608511		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.01009320021608511 | validation: 0.01043712834235333]
	TIME [epoch: 47.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009647398200022781		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.009647398200022781 | validation: 0.010815624849987134]
	TIME [epoch: 47.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010040562509922916		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.010040562509922916 | validation: 0.011254497532343723]
	TIME [epoch: 47.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009955364742005843		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.009955364742005843 | validation: 0.012455801496320015]
	TIME [epoch: 47.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010027372215361346		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.010027372215361346 | validation: 0.012429382143375061]
	TIME [epoch: 47.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009775700454496432		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.009775700454496432 | validation: 0.011562220474691739]
	TIME [epoch: 47.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009905211853630984		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.009905211853630984 | validation: 0.011180694445437435]
	TIME [epoch: 47.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00962290773611488		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.00962290773611488 | validation: 0.01056060895445867]
	TIME [epoch: 47.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009710971601996278		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.009710971601996278 | validation: 0.011180507203174865]
	TIME [epoch: 47.6 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009934261814143025		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.009934261814143025 | validation: 0.011572851701914761]
	TIME [epoch: 47.6 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009685675310212714		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.009685675310212714 | validation: 0.011557775271779829]
	TIME [epoch: 47.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00956264495706777		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.00956264495706777 | validation: 0.010731046874421574]
	TIME [epoch: 47.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009398667425454124		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.009398667425454124 | validation: 0.010778787758529687]
	TIME [epoch: 47.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009148958069573509		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.009148958069573509 | validation: 0.01230858733343737]
	TIME [epoch: 47.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009302874034291113		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.009302874034291113 | validation: 0.009748322864444135]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1135.pth
	Model improved!!!
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009583537582161563		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.009583537582161563 | validation: 0.009739621391407468]
	TIME [epoch: 47.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1136.pth
	Model improved!!!
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009841819006740103		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.009841819006740103 | validation: 0.011846789573377887]
	TIME [epoch: 47.6 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009795968311918072		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.009795968311918072 | validation: 0.010572265169659539]
	TIME [epoch: 47.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00967088789514763		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.00967088789514763 | validation: 0.010739798724212251]
	TIME [epoch: 47.6 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009275190703893954		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.009275190703893954 | validation: 0.010357636896821511]
	TIME [epoch: 47.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009695427527099837		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.009695427527099837 | validation: 0.011357129813513828]
	TIME [epoch: 47.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009563093517713566		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.009563093517713566 | validation: 0.011146543471027403]
	TIME [epoch: 47.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009601648901672541		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.009601648901672541 | validation: 0.010035241344773292]
	TIME [epoch: 47.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009958737606478935		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.009958737606478935 | validation: 0.010003848649457673]
	TIME [epoch: 47.6 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009160599696856793		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.009160599696856793 | validation: 0.011560712157729419]
	TIME [epoch: 47.6 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009779583066093424		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.009779583066093424 | validation: 0.010634669317536398]
	TIME [epoch: 47.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009881871853533826		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.009881871853533826 | validation: 0.010793726408892592]
	TIME [epoch: 47.6 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009060254022296578		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.009060254022296578 | validation: 0.011978950852124998]
	TIME [epoch: 47.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009853312142375408		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.009853312142375408 | validation: 0.009715442665060744]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00930913694116588		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.00930913694116588 | validation: 0.012580195340259006]
	TIME [epoch: 47.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009430183512843561		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.009430183512843561 | validation: 0.010829118211607437]
	TIME [epoch: 47.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009677853847969129		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.009677853847969129 | validation: 0.011215099951086143]
	TIME [epoch: 47.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009494431625127977		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.009494431625127977 | validation: 0.01165980573441406]
	TIME [epoch: 47.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009268254930995291		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.009268254930995291 | validation: 0.010787936263837427]
	TIME [epoch: 47.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00949117011211485		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.00949117011211485 | validation: 0.011028442426244026]
	TIME [epoch: 47.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00912495450368109		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.00912495450368109 | validation: 0.010079362829262434]
	TIME [epoch: 47.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009260649308051223		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.009260649308051223 | validation: 0.010615005124416833]
	TIME [epoch: 47.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009756944716510256		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.009756944716510256 | validation: 0.010358450078707254]
	TIME [epoch: 47.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009605667256196942		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.009605667256196942 | validation: 0.010895961226233932]
	TIME [epoch: 47.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01003438943072518		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.01003438943072518 | validation: 0.010457178143327445]
	TIME [epoch: 47.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00982713224020718		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.00982713224020718 | validation: 0.012914862216099342]
	TIME [epoch: 47.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009360545943737912		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.009360545943737912 | validation: 0.012170003529753837]
	TIME [epoch: 47.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01021653819392911		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.01021653819392911 | validation: 0.010628782929592982]
	TIME [epoch: 47.6 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009528977506333768		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.009528977506333768 | validation: 0.010627703600396953]
	TIME [epoch: 47.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009535394543534019		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.009535394543534019 | validation: 0.010586480252479642]
	TIME [epoch: 47.6 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009560976211086062		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.009560976211086062 | validation: 0.009754975743420428]
	TIME [epoch: 47.6 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009617273378817175		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.009617273378817175 | validation: 0.010633626998862935]
	TIME [epoch: 47.6 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009613821620214305		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.009613821620214305 | validation: 0.013186224217917134]
	TIME [epoch: 47.6 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009744206168419027		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.009744206168419027 | validation: 0.010674147323200819]
	TIME [epoch: 47.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009318055156895374		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.009318055156895374 | validation: 0.00972642424843143]
	TIME [epoch: 47.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009381383831707822		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.009381383831707822 | validation: 0.010439420338592392]
	TIME [epoch: 47.6 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009780249176262855		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.009780249176262855 | validation: 0.01117800271226831]
	TIME [epoch: 47.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009277966095508482		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.009277966095508482 | validation: 0.010381385197329843]
	TIME [epoch: 47.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009418200522340674		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.009418200522340674 | validation: 0.010658815212419341]
	TIME [epoch: 47.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009174485374708111		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.009174485374708111 | validation: 0.011462702662068884]
	TIME [epoch: 47.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009323403928214046		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.009323403928214046 | validation: 0.011192266724103733]
	TIME [epoch: 47.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009525687846215066		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.009525687846215066 | validation: 0.011173204484651485]
	TIME [epoch: 47.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009300672438310873		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.009300672438310873 | validation: 0.009997735196827819]
	TIME [epoch: 47.6 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009628821188864455		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.009628821188864455 | validation: 0.01148084825978269]
	TIME [epoch: 47.6 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009504791127622073		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.009504791127622073 | validation: 0.010392026935665896]
	TIME [epoch: 47.6 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009322259759577222		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.009322259759577222 | validation: 0.009939375324493595]
	TIME [epoch: 47.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00956438409191258		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.00956438409191258 | validation: 0.009377896713449841]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00946883189231217		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.00946883189231217 | validation: 0.010132324594594826]
	TIME [epoch: 47.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009747037539912608		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.009747037539912608 | validation: 0.010495097846864084]
	TIME [epoch: 47.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009304103480363217		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.009304103480363217 | validation: 0.0095539181942512]
	TIME [epoch: 47.6 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009199091338577065		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.009199091338577065 | validation: 0.011606797279603679]
	TIME [epoch: 47.6 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009184361879259388		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.009184361879259388 | validation: 0.010602932882876726]
	TIME [epoch: 47.6 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009234395099962326		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.009234395099962326 | validation: 0.01144486475303412]
	TIME [epoch: 47.6 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008906023007261791		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.008906023007261791 | validation: 0.010833208085099588]
	TIME [epoch: 47.6 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009462987046199927		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.009462987046199927 | validation: 0.010565950807449244]
	TIME [epoch: 47.6 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009391601870455768		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.009391601870455768 | validation: 0.010260122332288547]
	TIME [epoch: 47.6 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009521215951832895		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.009521215951832895 | validation: 0.010567758024352912]
	TIME [epoch: 47.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009537455087413793		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.009537455087413793 | validation: 0.010472861788310122]
	TIME [epoch: 47.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009191394136261334		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.009191394136261334 | validation: 0.010699133073086856]
	TIME [epoch: 47.6 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009190278924069392		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.009190278924069392 | validation: 0.011002821463708007]
	TIME [epoch: 47.6 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009932328360348165		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.009932328360348165 | validation: 0.010739327850644225]
	TIME [epoch: 47.6 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009836013764778846		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.009836013764778846 | validation: 0.011917401481484194]
	TIME [epoch: 47.6 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009421063930803508		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.009421063930803508 | validation: 0.010713658611365596]
	TIME [epoch: 47.7 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009624821365079814		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.009624821365079814 | validation: 0.011088281623766634]
	TIME [epoch: 47.6 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009347575941195285		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.009347575941195285 | validation: 0.011578225959076493]
	TIME [epoch: 47.6 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009167690516379196		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.009167690516379196 | validation: 0.010488093753695723]
	TIME [epoch: 47.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009363910686265348		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.009363910686265348 | validation: 0.01059644454485241]
	TIME [epoch: 47.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009501380745916534		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.009501380745916534 | validation: 0.010061912691206036]
	TIME [epoch: 47.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008863549062118607		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.008863549062118607 | validation: 0.01046266368708352]
	TIME [epoch: 47.6 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009133296610436274		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.009133296610436274 | validation: 0.011526962137567265]
	TIME [epoch: 47.6 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009211944567043693		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.009211944567043693 | validation: 0.010404377315955317]
	TIME [epoch: 47.6 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009521805511578947		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.009521805511578947 | validation: 0.010905349953609506]
	TIME [epoch: 47.6 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00931650948515082		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.00931650948515082 | validation: 0.010085021689140428]
	TIME [epoch: 47.6 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009038917798061624		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.009038917798061624 | validation: 0.009742450002921146]
	TIME [epoch: 47.6 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009889559649953125		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.009889559649953125 | validation: 0.010330213776905111]
	TIME [epoch: 47.6 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008946210294733135		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.008946210294733135 | validation: 0.010584452794616173]
	TIME [epoch: 47.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009794864306744588		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.009794864306744588 | validation: 0.010832093548415413]
	TIME [epoch: 47.6 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009138330650113071		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.009138330650113071 | validation: 0.010947974229797566]
	TIME [epoch: 47.6 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008682503820986209		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.008682503820986209 | validation: 0.010491181954485012]
	TIME [epoch: 47.6 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009445439585460728		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.009445439585460728 | validation: 0.01079964808922863]
	TIME [epoch: 47.6 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009741118086017617		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.009741118086017617 | validation: 0.01055574293548345]
	TIME [epoch: 47.6 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009325178322342781		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.009325178322342781 | validation: 0.010300228453227655]
	TIME [epoch: 47.6 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009422376147235059		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.009422376147235059 | validation: 0.009350603540054233]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1218.pth
	Model improved!!!
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009271072545480472		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.009271072545480472 | validation: 0.010184934931064727]
	TIME [epoch: 47.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00883793738961894		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.00883793738961894 | validation: 0.010692734216871475]
	TIME [epoch: 47.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008976475924671505		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.008976475924671505 | validation: 0.00944950873041401]
	TIME [epoch: 47.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009451895258494006		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.009451895258494006 | validation: 0.00944534138490683]
	TIME [epoch: 47.6 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008850827911717596		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.008850827911717596 | validation: 0.010792708807349553]
	TIME [epoch: 47.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009223482295310722		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.009223482295310722 | validation: 0.010093748244101943]
	TIME [epoch: 47.6 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00914839865359704		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.00914839865359704 | validation: 0.009584078305588859]
	TIME [epoch: 47.6 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009448326311938355		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.009448326311938355 | validation: 0.009888475849077047]
	TIME [epoch: 47.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008922848825655378		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.008922848825655378 | validation: 0.01042525735259522]
	TIME [epoch: 47.6 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009049713769434978		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.009049713769434978 | validation: 0.01062722011450629]
	TIME [epoch: 47.6 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009625146875604916		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.009625146875604916 | validation: 0.0098565423935553]
	TIME [epoch: 47.6 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008987439982132745		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.008987439982132745 | validation: 0.010782492389048569]
	TIME [epoch: 47.6 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009157480526179223		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.009157480526179223 | validation: 0.011472110102203718]
	TIME [epoch: 47.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009156695330483152		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.009156695330483152 | validation: 0.009922575459616115]
	TIME [epoch: 47.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009016320323583883		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.009016320323583883 | validation: 0.01097987128086451]
	TIME [epoch: 47.6 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009252252458385077		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.009252252458385077 | validation: 0.01098381377612491]
	TIME [epoch: 47.6 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009204498870073116		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.009204498870073116 | validation: 0.00996159576746575]
	TIME [epoch: 47.6 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008962291503667496		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.008962291503667496 | validation: 0.010503568615792925]
	TIME [epoch: 47.7 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009205505074579816		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.009205505074579816 | validation: 0.010278226535100299]
	TIME [epoch: 47.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087674812600981		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.0087674812600981 | validation: 0.009758257258573284]
	TIME [epoch: 47.6 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009405516749755299		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.009405516749755299 | validation: 0.01055372989425811]
	TIME [epoch: 47.6 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009254322537189366		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.009254322537189366 | validation: 0.009825990190936178]
	TIME [epoch: 47.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009320741642093993		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.009320741642093993 | validation: 0.010433447116066898]
	TIME [epoch: 47.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009286756442828333		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.009286756442828333 | validation: 0.009363792342454462]
	TIME [epoch: 47.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009124146450458313		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.009124146450458313 | validation: 0.009626815111668525]
	TIME [epoch: 47.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008710937984214462		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.008710937984214462 | validation: 0.010488637499847317]
	TIME [epoch: 47.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009370356976807692		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.009370356976807692 | validation: 0.011160470816995088]
	TIME [epoch: 47.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009055172909902844		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.009055172909902844 | validation: 0.009601681293936766]
	TIME [epoch: 47.7 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009048988414393567		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.009048988414393567 | validation: 0.010266100325612445]
	TIME [epoch: 47.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00903925776732208		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.00903925776732208 | validation: 0.009736546964910416]
	TIME [epoch: 47.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009059697063214614		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.009059697063214614 | validation: 0.010198577544147103]
	TIME [epoch: 47.7 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009102658848619745		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.009102658848619745 | validation: 0.009638347097062839]
	TIME [epoch: 47.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009639388845744053		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.009639388845744053 | validation: 0.010113251623783242]
	TIME [epoch: 47.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008874053542745206		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.008874053542745206 | validation: 0.00869037254980745]
	TIME [epoch: 47.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1252.pth
	Model improved!!!
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009168057122295403		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.009168057122295403 | validation: 0.01003371707828213]
	TIME [epoch: 47.6 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009390344486533028		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.009390344486533028 | validation: 0.009919666750083244]
	TIME [epoch: 47.6 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009236907974894574		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.009236907974894574 | validation: 0.009735726588937106]
	TIME [epoch: 47.6 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00924700323168091		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.00924700323168091 | validation: 0.011341434351690471]
	TIME [epoch: 47.6 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008845915222471433		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.008845915222471433 | validation: 0.010611126466178563]
	TIME [epoch: 47.6 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009536362718141574		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.009536362718141574 | validation: 0.009493120172389685]
	TIME [epoch: 47.6 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009132700707024112		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.009132700707024112 | validation: 0.009647296252136292]
	TIME [epoch: 47.6 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008859353271277187		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.008859353271277187 | validation: 0.01053166055627536]
	TIME [epoch: 47.6 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009228765957504945		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.009228765957504945 | validation: 0.011126884737144096]
	TIME [epoch: 47.6 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008522719160552805		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.008522719160552805 | validation: 0.009423415992411761]
	TIME [epoch: 47.6 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008832775884973809		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.008832775884973809 | validation: 0.010384425418362682]
	TIME [epoch: 47.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009055548332269939		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.009055548332269939 | validation: 0.011520520341372756]
	TIME [epoch: 47.6 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009076623525406158		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.009076623525406158 | validation: 0.010539695532926902]
	TIME [epoch: 47.6 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009390278298506295		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.009390278298506295 | validation: 0.010657154487540291]
	TIME [epoch: 47.6 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009117667629581919		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.009117667629581919 | validation: 0.010866347206423925]
	TIME [epoch: 47.6 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008823075706596625		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.008823075706596625 | validation: 0.009439512690093767]
	TIME [epoch: 47.6 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008614876676717653		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.008614876676717653 | validation: 0.01086767520646764]
	TIME [epoch: 47.6 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008944107290955915		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.008944107290955915 | validation: 0.010164982238809375]
	TIME [epoch: 47.6 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009116948666595622		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.009116948666595622 | validation: 0.009798586049218298]
	TIME [epoch: 47.6 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008927163335853115		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.008927163335853115 | validation: 0.011289557952014398]
	TIME [epoch: 47.6 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009161131142124726		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.009161131142124726 | validation: 0.01027136918819187]
	TIME [epoch: 47.6 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009355646349816022		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.009355646349816022 | validation: 0.009909575966345643]
	TIME [epoch: 47.6 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008945794170634808		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.008945794170634808 | validation: 0.009966775808626599]
	TIME [epoch: 47.6 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008784978150871623		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.008784978150871623 | validation: 0.009782305096716052]
	TIME [epoch: 47.6 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009485966598241		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.009485966598241 | validation: 0.009374568841163455]
	TIME [epoch: 47.6 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008514566689991124		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.008514566689991124 | validation: 0.009581064009073282]
	TIME [epoch: 47.6 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008580023659943797		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.008580023659943797 | validation: 0.009852641907658374]
	TIME [epoch: 47.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009021502557306124		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.009021502557306124 | validation: 0.009344174561261326]
	TIME [epoch: 47.6 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009192909509056909		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.009192909509056909 | validation: 0.00962193749571118]
	TIME [epoch: 47.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009074293921370932		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.009074293921370932 | validation: 0.010979600216240339]
	TIME [epoch: 47.6 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00890174290972194		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.00890174290972194 | validation: 0.009796474595528806]
	TIME [epoch: 47.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008978911377214911		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.008978911377214911 | validation: 0.009845821763305281]
	TIME [epoch: 47.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008834793657296454		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.008834793657296454 | validation: 0.009150873725781142]
	TIME [epoch: 47.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008772328925677274		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.008772328925677274 | validation: 0.010784212695910393]
	TIME [epoch: 47.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009176347479588738		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.009176347479588738 | validation: 0.009645445316505515]
	TIME [epoch: 47.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008753740126626386		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.008753740126626386 | validation: 0.009473026270847448]
	TIME [epoch: 47.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009033259668791791		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.009033259668791791 | validation: 0.009975859514062433]
	TIME [epoch: 47.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0088053824903403		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.0088053824903403 | validation: 0.009467918776945871]
	TIME [epoch: 47.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008669242758228201		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.008669242758228201 | validation: 0.010348467225417282]
	TIME [epoch: 47.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009043936259148035		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.009043936259148035 | validation: 0.010510332454343485]
	TIME [epoch: 47.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008912085370369445		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.008912085370369445 | validation: 0.009512188069965765]
	TIME [epoch: 47.6 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009155857019405323		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.009155857019405323 | validation: 0.009429569346543574]
	TIME [epoch: 47.6 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008989277436905398		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.008989277436905398 | validation: 0.011354175342281978]
	TIME [epoch: 47.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008974005819769364		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.008974005819769364 | validation: 0.010405532630350028]
	TIME [epoch: 47.6 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008817298068253428		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.008817298068253428 | validation: 0.009230804112677855]
	TIME [epoch: 47.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008447959213304067		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.008447959213304067 | validation: 0.009257749972051858]
	TIME [epoch: 47.6 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008434444341648374		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.008434444341648374 | validation: 0.009719638288716114]
	TIME [epoch: 47.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00903507599927881		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.00903507599927881 | validation: 0.009120226879938778]
	TIME [epoch: 47.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008519262749359777		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.008519262749359777 | validation: 0.009244922213008933]
	TIME [epoch: 47.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00883881510060808		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.00883881510060808 | validation: 0.010777896453953129]
	TIME [epoch: 47.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008510092274871978		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.008510092274871978 | validation: 0.009432608004496234]
	TIME [epoch: 47.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008948740926863943		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.008948740926863943 | validation: 0.010753589095945787]
	TIME [epoch: 47.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008580304052562271		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.008580304052562271 | validation: 0.009315959607394427]
	TIME [epoch: 47.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008663068089557358		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.008663068089557358 | validation: 0.009714203955826874]
	TIME [epoch: 47.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00898689586157012		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.00898689586157012 | validation: 0.009629893542343898]
	TIME [epoch: 47.5 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008396645315543832		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.008396645315543832 | validation: 0.010128382795289022]
	TIME [epoch: 47.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00871458325333848		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.00871458325333848 | validation: 0.00952238306950018]
	TIME [epoch: 47.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008996120094682116		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.008996120094682116 | validation: 0.010058211085837382]
	TIME [epoch: 47.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009252202126528004		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.009252202126528004 | validation: 0.008777487922749337]
	TIME [epoch: 47.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008794403196135085		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.008794403196135085 | validation: 0.009802045297884004]
	TIME [epoch: 47.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009091784851307714		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.009091784851307714 | validation: 0.010326143124220847]
	TIME [epoch: 47.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008901958837948476		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.008901958837948476 | validation: 0.009800885330942307]
	TIME [epoch: 47.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009207099730836209		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.009207099730836209 | validation: 0.008862904623564243]
	TIME [epoch: 47.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008963278164867871		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.008963278164867871 | validation: 0.009629901210917518]
	TIME [epoch: 47.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009144133344113806		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.009144133344113806 | validation: 0.010408535554436514]
	TIME [epoch: 47.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009211459611890465		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.009211459611890465 | validation: 0.009558773374444834]
	TIME [epoch: 47.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008969952482057043		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.008969952482057043 | validation: 0.010580425383485009]
	TIME [epoch: 47.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00896799998029096		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.00896799998029096 | validation: 0.010340929736375773]
	TIME [epoch: 47.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008675168698314192		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.008675168698314192 | validation: 0.01016737059447634]
	TIME [epoch: 47.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008876249929401996		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.008876249929401996 | validation: 0.009681968425733392]
	TIME [epoch: 47.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009006911901195033		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.009006911901195033 | validation: 0.008924210088125844]
	TIME [epoch: 47.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009213700091652238		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.009213700091652238 | validation: 0.010063858911227519]
	TIME [epoch: 47.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008737146077187277		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.008737146077187277 | validation: 0.00994287768794391]
	TIME [epoch: 47.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008501952758830129		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.008501952758830129 | validation: 0.009619886606693074]
	TIME [epoch: 47.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008757324065303346		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.008757324065303346 | validation: 0.00941296027402719]
	TIME [epoch: 47.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008919762138940862		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.008919762138940862 | validation: 0.009419710449501567]
	TIME [epoch: 47.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008923498541453209		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.008923498541453209 | validation: 0.010442294189684547]
	TIME [epoch: 47.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008411551975400295		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.008411551975400295 | validation: 0.009598541409899712]
	TIME [epoch: 47.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009452448343936179		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.009452448343936179 | validation: 0.009848643141509933]
	TIME [epoch: 47.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008588701009792903		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.008588701009792903 | validation: 0.009561006585233164]
	TIME [epoch: 47.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008826336614202845		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.008826336614202845 | validation: 0.010011775906856575]
	TIME [epoch: 47.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008794491301220526		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.008794491301220526 | validation: 0.011068470054680065]
	TIME [epoch: 47.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008924237119144056		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.008924237119144056 | validation: 0.010221649159877957]
	TIME [epoch: 47.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008887320511249944		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.008887320511249944 | validation: 0.008922516327881932]
	TIME [epoch: 47.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008713463124952001		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.008713463124952001 | validation: 0.009711023412318656]
	TIME [epoch: 47.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008811493662699454		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.008811493662699454 | validation: 0.008639009304434689]
	TIME [epoch: 47.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1338.pth
	Model improved!!!
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00858396751104545		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.00858396751104545 | validation: 0.009143284334419903]
	TIME [epoch: 47.6 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008943270456226966		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.008943270456226966 | validation: 0.009899419859846735]
	TIME [epoch: 47.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008866823624798225		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.008866823624798225 | validation: 0.009884275536278249]
	TIME [epoch: 47.6 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008507926962650377		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.008507926962650377 | validation: 0.008662810632374953]
	TIME [epoch: 47.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009065138547133504		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.009065138547133504 | validation: 0.009680929754169919]
	TIME [epoch: 47.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009051473837820713		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.009051473837820713 | validation: 0.009388209652962578]
	TIME [epoch: 47.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00901758164414107		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.00901758164414107 | validation: 0.010266586394371332]
	TIME [epoch: 47.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008866155797798819		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.008866155797798819 | validation: 0.010773543928070648]
	TIME [epoch: 47.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008853674822386222		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.008853674822386222 | validation: 0.0094125350395373]
	TIME [epoch: 47.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008662048827145505		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.008662048827145505 | validation: 0.009337762768624719]
	TIME [epoch: 47.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00851812134015897		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.00851812134015897 | validation: 0.008631998716765399]
	TIME [epoch: 47.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1349.pth
	Model improved!!!
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008543889633296758		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.008543889633296758 | validation: 0.010422592775591499]
	TIME [epoch: 47.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009050681388738244		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.009050681388738244 | validation: 0.008523571373086208]
	TIME [epoch: 47.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1351.pth
	Model improved!!!
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008463179662341173		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.008463179662341173 | validation: 0.008930830162078785]
	TIME [epoch: 47.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008837594333223052		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.008837594333223052 | validation: 0.009481567472004973]
	TIME [epoch: 47.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009020941599604686		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.009020941599604686 | validation: 0.010588823151039609]
	TIME [epoch: 47.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008901615066421205		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.008901615066421205 | validation: 0.010681308807786475]
	TIME [epoch: 47.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00837647010344403		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.00837647010344403 | validation: 0.009501447110376662]
	TIME [epoch: 47.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008634962352084849		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.008634962352084849 | validation: 0.008959282521577128]
	TIME [epoch: 47.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008861025303909387		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.008861025303909387 | validation: 0.009414935824887978]
	TIME [epoch: 47.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008762135581309642		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.008762135581309642 | validation: 0.009562533402900461]
	TIME [epoch: 47.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00893522875131226		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.00893522875131226 | validation: 0.009349311902700606]
	TIME [epoch: 47.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008962568978792065		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.008962568978792065 | validation: 0.00962721918559584]
	TIME [epoch: 47.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008600056112205823		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.008600056112205823 | validation: 0.010662846575876413]
	TIME [epoch: 47.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00905492397518083		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.00905492397518083 | validation: 0.01162192334724684]
	TIME [epoch: 47.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008885133843082225		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.008885133843082225 | validation: 0.010297222094905197]
	TIME [epoch: 47.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009038991670137726		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.009038991670137726 | validation: 0.010103728108006358]
	TIME [epoch: 47.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008435110495081565		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.008435110495081565 | validation: 0.01044082512823534]
	TIME [epoch: 47.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008667617551049393		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.008667617551049393 | validation: 0.010441728166620692]
	TIME [epoch: 47.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009084265246905917		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.009084265246905917 | validation: 0.008312378135695159]
	TIME [epoch: 47.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1368.pth
	Model improved!!!
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008405272861844705		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.008405272861844705 | validation: 0.009213550710637754]
	TIME [epoch: 47.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008711385848411385		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.008711385848411385 | validation: 0.010168940053914569]
	TIME [epoch: 47.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008663469061626468		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.008663469061626468 | validation: 0.009175136572637018]
	TIME [epoch: 47.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008379274088525027		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.008379274088525027 | validation: 0.009439606719203263]
	TIME [epoch: 47.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008865360022596146		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.008865360022596146 | validation: 0.009963631834837144]
	TIME [epoch: 47.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008745450345563293		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.008745450345563293 | validation: 0.009467145021127922]
	TIME [epoch: 47.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008820421572705562		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.008820421572705562 | validation: 0.009102525159598716]
	TIME [epoch: 47.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00872288172030261		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.00872288172030261 | validation: 0.009325772883050917]
	TIME [epoch: 47.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008732873956464438		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.008732873956464438 | validation: 0.00943927926437392]
	TIME [epoch: 47.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008864216853137381		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.008864216853137381 | validation: 0.009973957007709465]
	TIME [epoch: 47.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00859832776646126		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.00859832776646126 | validation: 0.010541313533192756]
	TIME [epoch: 47.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008343721844556297		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.008343721844556297 | validation: 0.009350502126751625]
	TIME [epoch: 47.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009000589169525078		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.009000589169525078 | validation: 0.009061189076876757]
	TIME [epoch: 47.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008300052247409989		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.008300052247409989 | validation: 0.009790113087085656]
	TIME [epoch: 47.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008436882553240856		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.008436882553240856 | validation: 0.009369090630467092]
	TIME [epoch: 47.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008556684847129613		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.008556684847129613 | validation: 0.009798252834088417]
	TIME [epoch: 47.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008536766166492817		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.008536766166492817 | validation: 0.009481059063205743]
	TIME [epoch: 47.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008633767196088646		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.008633767196088646 | validation: 0.009230270838938731]
	TIME [epoch: 47.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008852896631600653		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.008852896631600653 | validation: 0.009518495602564231]
	TIME [epoch: 47.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00871522989429544		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.00871522989429544 | validation: 0.010132583463004118]
	TIME [epoch: 47.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008484448037272085		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.008484448037272085 | validation: 0.009980565534428287]
	TIME [epoch: 47.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008199169825253358		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.008199169825253358 | validation: 0.0097052481237481]
	TIME [epoch: 47.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008787358118174693		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.008787358118174693 | validation: 0.009118994362187232]
	TIME [epoch: 47.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008413397457686867		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.008413397457686867 | validation: 0.009383142496615869]
	TIME [epoch: 47.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008879163898660598		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.008879163898660598 | validation: 0.009077178552057745]
	TIME [epoch: 47.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008647969948901725		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.008647969948901725 | validation: 0.009503376090851582]
	TIME [epoch: 47.5 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008607034300106524		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.008607034300106524 | validation: 0.010623525738054123]
	TIME [epoch: 47.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008780182783102426		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.008780182783102426 | validation: 0.009084076969035376]
	TIME [epoch: 47.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00888590108142333		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.00888590108142333 | validation: 0.009468239826120009]
	TIME [epoch: 47.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008896072003079975		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.008896072003079975 | validation: 0.009680837304063192]
	TIME [epoch: 47.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00861083959545457		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.00861083959545457 | validation: 0.010215092151406138]
	TIME [epoch: 47.5 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008414899786110513		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.008414899786110513 | validation: 0.008571203540278]
	TIME [epoch: 47.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00870500748742542		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.00870500748742542 | validation: 0.009388393458120006]
	TIME [epoch: 47.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009032626350165562		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.009032626350165562 | validation: 0.009671680503861706]
	TIME [epoch: 47.6 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008556275167875754		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.008556275167875754 | validation: 0.009785362020389866]
	TIME [epoch: 47.6 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008451347278465547		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.008451347278465547 | validation: 0.010682309233876223]
	TIME [epoch: 47.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008299678897914805		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.008299678897914805 | validation: 0.011850528852458773]
	TIME [epoch: 47.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00868798052194439		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.00868798052194439 | validation: 0.010038123593770671]
	TIME [epoch: 47.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008724564147810115		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.008724564147810115 | validation: 0.010226243269869258]
	TIME [epoch: 47.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00851441055024961		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.00851441055024961 | validation: 0.009797314374149078]
	TIME [epoch: 47.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00868524165535967		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.00868524165535967 | validation: 0.009222597155806419]
	TIME [epoch: 47.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008644449444359516		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.008644449444359516 | validation: 0.009214264001767511]
	TIME [epoch: 47.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00866305087690144		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.00866305087690144 | validation: 0.009876082789921903]
	TIME [epoch: 47.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008248631585741927		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.008248631585741927 | validation: 0.009311078652489016]
	TIME [epoch: 47.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008703261720011402		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.008703261720011402 | validation: 0.01015553291572092]
	TIME [epoch: 47.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008485520601542727		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.008485520601542727 | validation: 0.010198374384574885]
	TIME [epoch: 47.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008779247310737644		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.008779247310737644 | validation: 0.009779954979771997]
	TIME [epoch: 47.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008735490962658486		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.008735490962658486 | validation: 0.009924722488562864]
	TIME [epoch: 47.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008452900536493864		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.008452900536493864 | validation: 0.009551881910199015]
	TIME [epoch: 47.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008755835158964883		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.008755835158964883 | validation: 0.008184195632790662]
	TIME [epoch: 47.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1418.pth
	Model improved!!!
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008865144806015685		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.008865144806015685 | validation: 0.008927167718045]
	TIME [epoch: 47.7 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009082355215370371		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.009082355215370371 | validation: 0.010272869708146774]
	TIME [epoch: 47.7 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00850607575814991		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.00850607575814991 | validation: 0.009127084722635466]
	TIME [epoch: 47.7 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009174288381514122		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.009174288381514122 | validation: 0.009683253738085967]
	TIME [epoch: 47.6 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008740734368421724		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.008740734368421724 | validation: 0.009831459107613558]
	TIME [epoch: 47.7 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008337740767823688		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.008337740767823688 | validation: 0.009565289105827587]
	TIME [epoch: 47.7 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008714190007769249		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.008714190007769249 | validation: 0.009872180482924996]
	TIME [epoch: 47.6 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009000678511329213		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.009000678511329213 | validation: 0.009150965954527374]
	TIME [epoch: 47.6 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008896773686696328		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.008896773686696328 | validation: 0.009106702332303034]
	TIME [epoch: 47.6 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008690750084916987		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.008690750084916987 | validation: 0.0101136770261255]
	TIME [epoch: 47.6 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008219076026425715		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.008219076026425715 | validation: 0.009689976238611758]
	TIME [epoch: 47.6 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008301799406298103		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.008301799406298103 | validation: 0.009163581560787365]
	TIME [epoch: 47.6 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009233717378566396		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.009233717378566396 | validation: 0.010691222903626143]
	TIME [epoch: 47.7 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00827686436748563		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.00827686436748563 | validation: 0.009268404083689882]
	TIME [epoch: 47.6 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008963987211820987		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.008963987211820987 | validation: 0.009685029199847515]
	TIME [epoch: 47.6 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008395306399201007		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.008395306399201007 | validation: 0.008713264733837316]
	TIME [epoch: 47.6 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008456820061159656		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.008456820061159656 | validation: 0.008376919504387442]
	TIME [epoch: 47.6 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008534953184881539		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.008534953184881539 | validation: 0.009787003390991443]
	TIME [epoch: 47.6 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008802330243067247		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.008802330243067247 | validation: 0.01050843772173655]
	TIME [epoch: 47.6 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008539336594667195		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.008539336594667195 | validation: 0.009535066335968925]
	TIME [epoch: 47.6 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008252558371733663		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.008252558371733663 | validation: 0.009159263882737564]
	TIME [epoch: 47.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008441926827652857		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.008441926827652857 | validation: 0.009377151059588354]
	TIME [epoch: 47.6 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008703072317668837		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.008703072317668837 | validation: 0.009178982040668712]
	TIME [epoch: 47.6 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008519226760696774		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.008519226760696774 | validation: 0.00945414962586565]
	TIME [epoch: 47.6 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00855875782845497		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.00855875782845497 | validation: 0.009147523837098027]
	TIME [epoch: 47.6 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008451310149732558		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.008451310149732558 | validation: 0.009940383522149578]
	TIME [epoch: 47.6 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008736819549869067		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.008736819549869067 | validation: 0.00936815384055164]
	TIME [epoch: 47.6 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0083237787694588		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.0083237787694588 | validation: 0.009027694980846056]
	TIME [epoch: 47.6 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008885650051986112		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.008885650051986112 | validation: 0.009114388244827784]
	TIME [epoch: 47.6 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008525359591264062		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.008525359591264062 | validation: 0.009842771336140088]
	TIME [epoch: 47.6 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008423580384427821		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.008423580384427821 | validation: 0.009534022138785245]
	TIME [epoch: 47.6 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008188086789188838		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.008188086789188838 | validation: 0.009702944655320133]
	TIME [epoch: 47.6 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0083694597761431		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.0083694597761431 | validation: 0.009186656355881898]
	TIME [epoch: 47.6 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008773632514791024		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.008773632514791024 | validation: 0.009382979408822323]
	TIME [epoch: 47.6 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008466960230302634		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.008466960230302634 | validation: 0.008911108463764528]
	TIME [epoch: 47.6 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008626542034367968		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.008626542034367968 | validation: 0.010093946911617822]
	TIME [epoch: 47.6 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008312947353833566		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.008312947353833566 | validation: 0.008759134019973195]
	TIME [epoch: 47.6 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008718981039320558		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.008718981039320558 | validation: 0.008467015200815017]
	TIME [epoch: 47.6 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008598555026425122		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.008598555026425122 | validation: 0.009590188365394822]
	TIME [epoch: 47.6 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008026060949038027		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.008026060949038027 | validation: 0.009459452872902917]
	TIME [epoch: 47.6 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008581434660551165		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.008581434660551165 | validation: 0.009855045697477678]
	TIME [epoch: 47.6 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008371603778959315		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.008371603778959315 | validation: 0.010529850805070528]
	TIME [epoch: 47.6 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008319007749465184		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.008319007749465184 | validation: 0.009358871277591744]
	TIME [epoch: 47.6 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00801281589379343		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.00801281589379343 | validation: 0.008552614597970942]
	TIME [epoch: 47.6 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008563212524497044		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.008563212524497044 | validation: 0.009493764593420141]
	TIME [epoch: 47.6 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008378245886112919		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.008378245886112919 | validation: 0.009682160185303646]
	TIME [epoch: 47.6 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008602123265500541		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.008602123265500541 | validation: 0.010344031840485816]
	TIME [epoch: 47.6 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008582432541928427		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.008582432541928427 | validation: 0.008976407303190376]
	TIME [epoch: 47.6 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008576736026026464		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.008576736026026464 | validation: 0.00884014232885615]
	TIME [epoch: 47.6 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008286528828520611		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.008286528828520611 | validation: 0.00891247674229214]
	TIME [epoch: 47.6 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008622781690203585		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.008622781690203585 | validation: 0.00957947666589271]
	TIME [epoch: 47.6 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008515407238898508		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.008515407238898508 | validation: 0.010363839152468895]
	TIME [epoch: 47.6 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008441326838953635		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.008441326838953635 | validation: 0.009639634181129522]
	TIME [epoch: 47.6 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008451115289157155		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.008451115289157155 | validation: 0.010311596320068601]
	TIME [epoch: 47.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008430006466683014		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.008430006466683014 | validation: 0.00901988736855305]
	TIME [epoch: 47.6 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008738001116743764		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.008738001116743764 | validation: 0.009312396178455924]
	TIME [epoch: 47.6 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008631570673758422		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.008631570673758422 | validation: 0.010674587530468984]
	TIME [epoch: 47.6 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008461159592946306		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.008461159592946306 | validation: 0.009538333012349265]
	TIME [epoch: 47.6 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008352538321869211		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.008352538321869211 | validation: 0.008762928220952138]
	TIME [epoch: 47.6 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008757157164326446		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.008757157164326446 | validation: 0.00933890108926954]
	TIME [epoch: 47.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008439979299507929		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.008439979299507929 | validation: 0.009133721646825375]
	TIME [epoch: 47.6 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008271835465407532		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.008271835465407532 | validation: 0.009760460908354699]
	TIME [epoch: 47.6 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008496554074219044		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.008496554074219044 | validation: 0.008299791067216124]
	TIME [epoch: 47.6 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008466816514754278		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.008466816514754278 | validation: 0.01064609855338245]
	TIME [epoch: 47.6 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008536103495281763		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.008536103495281763 | validation: 0.010807384505124008]
	TIME [epoch: 47.6 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008611929924951427		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.008611929924951427 | validation: 0.009008536664400178]
	TIME [epoch: 47.6 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008324712104625605		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.008324712104625605 | validation: 0.010914825624189108]
	TIME [epoch: 47.6 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087213737166906		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.0087213737166906 | validation: 0.009775079241857635]
	TIME [epoch: 47.6 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008553539160567423		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.008553539160567423 | validation: 0.009371305455182843]
	TIME [epoch: 47.6 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008344045637251656		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.008344045637251656 | validation: 0.009167536602980253]
	TIME [epoch: 47.6 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008314700090483589		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.008314700090483589 | validation: 0.009432433262371143]
	TIME [epoch: 47.6 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008346033504859659		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.008346033504859659 | validation: 0.00938011850141824]
	TIME [epoch: 47.6 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008357918500972018		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.008357918500972018 | validation: 0.010089742315128169]
	TIME [epoch: 47.6 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00845584290790521		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.00845584290790521 | validation: 0.010415088659840322]
	TIME [epoch: 47.6 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008539231961190606		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.008539231961190606 | validation: 0.010196803092702988]
	TIME [epoch: 47.6 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008165887996927072		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.008165887996927072 | validation: 0.010846014355222403]
	TIME [epoch: 47.6 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008493456747267002		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.008493456747267002 | validation: 0.009840935112690349]
	TIME [epoch: 47.6 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008470199903273846		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.008470199903273846 | validation: 0.009565334897415654]
	TIME [epoch: 47.6 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008566151791963052		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.008566151791963052 | validation: 0.00832253221467288]
	TIME [epoch: 47.6 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00817793934069265		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.00817793934069265 | validation: 0.009174172043049995]
	TIME [epoch: 47.6 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008458901146209463		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.008458901146209463 | validation: 0.009714965990899636]
	TIME [epoch: 47.6 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008276437426281903		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.008276437426281903 | validation: 0.010809869228904402]
	TIME [epoch: 47.6 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008044415709807063		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.008044415709807063 | validation: 0.009701703136788052]
	TIME [epoch: 47.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008489135518347062		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.008489135518347062 | validation: 0.009373771936127758]
	TIME [epoch: 47.7 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008785690769449558		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.008785690769449558 | validation: 0.008891253778291745]
	TIME [epoch: 47.7 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008189880385953117		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.008189880385953117 | validation: 0.00909188619849568]
	TIME [epoch: 47.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008416337393426319		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.008416337393426319 | validation: 0.008434992114328115]
	TIME [epoch: 47.8 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008306484436732424		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.008306484436732424 | validation: 0.009678336682466522]
	TIME [epoch: 47.7 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008448650990812679		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.008448650990812679 | validation: 0.008933162260532625]
	TIME [epoch: 47.8 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008342014376189718		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.008342014376189718 | validation: 0.00968684925984267]
	TIME [epoch: 47.7 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00846298304159221		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.00846298304159221 | validation: 0.009205747339208674]
	TIME [epoch: 47.7 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008523595814553846		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.008523595814553846 | validation: 0.01004776406114123]
	TIME [epoch: 47.7 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008496453512565273		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.008496453512565273 | validation: 0.009547590346128542]
	TIME [epoch: 47.7 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00851273284749112		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.00851273284749112 | validation: 0.010326430741700848]
	TIME [epoch: 47.7 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008482316554363925		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.008482316554363925 | validation: 0.008814207546339499]
	TIME [epoch: 47.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008545005099740877		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.008545005099740877 | validation: 0.009074412615979376]
	TIME [epoch: 47.7 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008574193796530679		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.008574193796530679 | validation: 0.009204591172003014]
	TIME [epoch: 47.7 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009169126517149101		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.009169126517149101 | validation: 0.009182165230410072]
	TIME [epoch: 47.7 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008638662534896122		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.008638662534896122 | validation: 0.009249642104772929]
	TIME [epoch: 47.7 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008082738377483696		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.008082738377483696 | validation: 0.009298555467748567]
	TIME [epoch: 47.7 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087242312099382		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.0087242312099382 | validation: 0.00983144494328886]
	TIME [epoch: 47.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240704_134101/states/model_phi1_1a_v_mmd1_smallnet_1519.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 40159.825 seconds.
