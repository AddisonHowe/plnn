Args:
Namespace(name='model_phi1_1a_v_mmd6', outdir='out/model_training/model_phi1_1a_v_mmd6', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.001, nepochs_warmup=20, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.01, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2087049345

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.0088184943302325		[learning rate: 0.0013375]
	Learning Rate: 0.0013375
	LOSS [training: 6.0088184943302325 | validation: 5.728235913448698]
	TIME [epoch: 100 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.760593852982158		[learning rate: 0.0017875]
	Learning Rate: 0.0017875
	LOSS [training: 5.760593852982158 | validation: 5.555533984056011]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.540974153778854		[learning rate: 0.0022375]
	Learning Rate: 0.0022375
	LOSS [training: 5.540974153778854 | validation: 5.3456835795679565]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.375902332426764		[learning rate: 0.0026875]
	Learning Rate: 0.0026875
	LOSS [training: 5.375902332426764 | validation: 5.202921198920097]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2106047120549475		[learning rate: 0.0031375]
	Learning Rate: 0.0031375
	LOSS [training: 5.2106047120549475 | validation: 5.062729285827121]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.969834298618919		[learning rate: 0.0035875]
	Learning Rate: 0.0035875
	LOSS [training: 4.969834298618919 | validation: 4.719880086798437]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.892493564281172		[learning rate: 0.0040375]
	Learning Rate: 0.0040375
	LOSS [training: 4.892493564281172 | validation: 4.455244610597683]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.625004035502691		[learning rate: 0.0044875]
	Learning Rate: 0.0044875
	LOSS [training: 4.625004035502691 | validation: 5.238965277457979]
	TIME [epoch: 9.7 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.036474884225619		[learning rate: 0.0049375]
	Learning Rate: 0.0049375
	LOSS [training: 5.036474884225619 | validation: 5.114923183959379]
	TIME [epoch: 9.71 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.47187646741523		[learning rate: 0.0053875]
	Learning Rate: 0.0053875
	LOSS [training: 4.47187646741523 | validation: 4.121694174873652]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123691345643895		[learning rate: 0.0058375]
	Learning Rate: 0.0058375
	LOSS [training: 4.123691345643895 | validation: 4.511874623598593]
	TIME [epoch: 9.68 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2345087433519915		[learning rate: 0.0062875]
	Learning Rate: 0.0062875
	LOSS [training: 4.2345087433519915 | validation: 4.089505264012502]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.95811372325864		[learning rate: 0.0067375]
	Learning Rate: 0.0067375
	LOSS [training: 3.95811372325864 | validation: 3.7267315673329393]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.758705740400713		[learning rate: 0.0071875]
	Learning Rate: 0.0071875
	LOSS [training: 3.758705740400713 | validation: 3.559103341553949]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6563535596409955		[learning rate: 0.0076375]
	Learning Rate: 0.0076375
	LOSS [training: 3.6563535596409955 | validation: 4.342488025056962]
	TIME [epoch: 9.7 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6994307520667182		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 3.6994307520667182 | validation: 3.5560189593373948]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271425990305093		[learning rate: 0.0085375]
	Learning Rate: 0.0085375
	LOSS [training: 3.271425990305093 | validation: 3.1985946141063306]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2475850222852163		[learning rate: 0.0089875]
	Learning Rate: 0.0089875
	LOSS [training: 3.2475850222852163 | validation: 3.274106201217548]
	TIME [epoch: 9.73 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0442250732171034		[learning rate: 0.0094375]
	Learning Rate: 0.0094375
	LOSS [training: 3.0442250732171034 | validation: 3.271197351703473]
	TIME [epoch: 9.69 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0522152302184704		[learning rate: 0.0098875]
	Learning Rate: 0.0098875
	LOSS [training: 3.0522152302184704 | validation: 3.015813718149914]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.877008041099902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.877008041099902 | validation: 3.1648917672022945]
	TIME [epoch: 9.69 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7866441900498424		[learning rate: 0.01]
	Learning Rate: 0.00999998
	LOSS [training: 2.7866441900498424 | validation: 3.2319963225816277]
	TIME [epoch: 9.69 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4072228635725414		[learning rate: 0.01]
	Learning Rate: 0.00999995
	LOSS [training: 3.4072228635725414 | validation: 3.003780457381006]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.724272710557591		[learning rate: 0.0099999]
	Learning Rate: 0.00999991
	LOSS [training: 2.724272710557591 | validation: 3.030709344281901]
	TIME [epoch: 9.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6671996950614605		[learning rate: 0.0099999]
	Learning Rate: 0.00999986
	LOSS [training: 2.6671996950614605 | validation: 2.764529997623539]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5521668735413074		[learning rate: 0.0099998]
	Learning Rate: 0.00999979
	LOSS [training: 2.5521668735413074 | validation: 2.9059769675116947]
	TIME [epoch: 9.68 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.463338171881428		[learning rate: 0.0099997]
	Learning Rate: 0.00999971
	LOSS [training: 2.463338171881428 | validation: 2.780917914457926]
	TIME [epoch: 9.68 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4356652828870855		[learning rate: 0.0099996]
	Learning Rate: 0.00999961
	LOSS [training: 2.4356652828870855 | validation: 3.0047094533832084]
	TIME [epoch: 9.72 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3430382867784023		[learning rate: 0.0099995]
	Learning Rate: 0.00999951
	LOSS [training: 2.3430382867784023 | validation: 3.120212539828744]
	TIME [epoch: 9.68 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4218690697690466		[learning rate: 0.0099994]
	Learning Rate: 0.00999939
	LOSS [training: 2.4218690697690466 | validation: 2.7018003881579875]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5122031307990325		[learning rate: 0.0099993]
	Learning Rate: 0.00999926
	LOSS [training: 2.5122031307990325 | validation: 2.564561808562077]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6792824535269038		[learning rate: 0.0099991]
	Learning Rate: 0.00999911
	LOSS [training: 2.6792824535269038 | validation: 2.786692983002502]
	TIME [epoch: 9.72 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4172104160641386		[learning rate: 0.009999]
	Learning Rate: 0.00999896
	LOSS [training: 2.4172104160641386 | validation: 2.602927131717901]
	TIME [epoch: 9.69 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1305084176550406		[learning rate: 0.0099988]
	Learning Rate: 0.00999879
	LOSS [training: 2.1305084176550406 | validation: 2.830850118471304]
	TIME [epoch: 9.68 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.38739586911269		[learning rate: 0.0099986]
	Learning Rate: 0.0099986
	LOSS [training: 2.38739586911269 | validation: 2.591735812222264]
	TIME [epoch: 9.68 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0232095583065655		[learning rate: 0.0099984]
	Learning Rate: 0.00999841
	LOSS [training: 2.0232095583065655 | validation: 3.58903876976892]
	TIME [epoch: 9.69 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5638907388083663		[learning rate: 0.0099982]
	Learning Rate: 0.0099982
	LOSS [training: 2.5638907388083663 | validation: 2.505542798780475]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0513821226070132		[learning rate: 0.009998]
	Learning Rate: 0.00999798
	LOSS [training: 2.0513821226070132 | validation: 2.5395281191973784]
	TIME [epoch: 9.7 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.027899983807934		[learning rate: 0.0099977]
	Learning Rate: 0.00999774
	LOSS [training: 2.027899983807934 | validation: 2.5286234172947846]
	TIME [epoch: 9.68 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.179524470719934		[learning rate: 0.0099975]
	Learning Rate: 0.0099975
	LOSS [training: 2.179524470719934 | validation: 2.649597064318345]
	TIME [epoch: 9.68 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0090815685820917		[learning rate: 0.0099972]
	Learning Rate: 0.00999724
	LOSS [training: 2.0090815685820917 | validation: 2.841275931430845]
	TIME [epoch: 9.69 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0492042817311833		[learning rate: 0.009997]
	Learning Rate: 0.00999696
	LOSS [training: 2.0492042817311833 | validation: 2.5766867901939383]
	TIME [epoch: 9.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9326742015471092		[learning rate: 0.0099967]
	Learning Rate: 0.00999668
	LOSS [training: 1.9326742015471092 | validation: 2.6713332963031946]
	TIME [epoch: 9.68 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.908099567117494		[learning rate: 0.0099964]
	Learning Rate: 0.00999638
	LOSS [training: 1.908099567117494 | validation: 2.677766424128631]
	TIME [epoch: 9.68 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9196872532524805		[learning rate: 0.0099961]
	Learning Rate: 0.00999607
	LOSS [training: 1.9196872532524805 | validation: 2.436944233012232]
	TIME [epoch: 9.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8876063936407825		[learning rate: 0.0099957]
	Learning Rate: 0.00999575
	LOSS [training: 1.8876063936407825 | validation: 2.435990959761277]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.828304468011423		[learning rate: 0.0099954]
	Learning Rate: 0.00999541
	LOSS [training: 1.828304468011423 | validation: 2.358547712039806]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7938023433606904		[learning rate: 0.0099951]
	Learning Rate: 0.00999506
	LOSS [training: 1.7938023433606904 | validation: 2.257747298827713]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6381756698742678		[learning rate: 0.0099947]
	Learning Rate: 0.0099947
	LOSS [training: 1.6381756698742678 | validation: 2.219266010068293]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8820587252352028		[learning rate: 0.0099943]
	Learning Rate: 0.00999432
	LOSS [training: 1.8820587252352028 | validation: 2.1294355852518603]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6502819168710077		[learning rate: 0.0099939]
	Learning Rate: 0.00999393
	LOSS [training: 1.6502819168710077 | validation: 2.1560826914705316]
	TIME [epoch: 9.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6307729788027474		[learning rate: 0.0099935]
	Learning Rate: 0.00999353
	LOSS [training: 1.6307729788027474 | validation: 2.1406519169856475]
	TIME [epoch: 9.69 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5267067271516348		[learning rate: 0.0099931]
	Learning Rate: 0.00999312
	LOSS [training: 1.5267067271516348 | validation: 2.1286695124764647]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8296436793088917		[learning rate: 0.0099927]
	Learning Rate: 0.00999269
	LOSS [training: 1.8296436793088917 | validation: 2.4469985389289857]
	TIME [epoch: 9.7 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.83336035332189		[learning rate: 0.0099923]
	Learning Rate: 0.00999225
	LOSS [training: 1.83336035332189 | validation: 2.084479097085842]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4754291309348195		[learning rate: 0.0099918]
	Learning Rate: 0.0099918
	LOSS [training: 1.4754291309348195 | validation: 2.1482745969473203]
	TIME [epoch: 9.73 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4876909861836936		[learning rate: 0.0099913]
	Learning Rate: 0.00999134
	LOSS [training: 1.4876909861836936 | validation: 2.1524392279881575]
	TIME [epoch: 9.68 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5723330367155333		[learning rate: 0.0099909]
	Learning Rate: 0.00999086
	LOSS [training: 1.5723330367155333 | validation: 2.100303635925864]
	TIME [epoch: 9.68 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4570239842375603		[learning rate: 0.0099904]
	Learning Rate: 0.00999037
	LOSS [training: 1.4570239842375603 | validation: 2.2600676256532064]
	TIME [epoch: 9.68 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.601118328908537		[learning rate: 0.0099899]
	Learning Rate: 0.00998987
	LOSS [training: 1.601118328908537 | validation: 2.1012301940388753]
	TIME [epoch: 9.7 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.509473749292666		[learning rate: 0.0099893]
	Learning Rate: 0.00998935
	LOSS [training: 1.509473749292666 | validation: 2.1336402327482222]
	TIME [epoch: 9.71 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.436188899884963		[learning rate: 0.0099888]
	Learning Rate: 0.00998882
	LOSS [training: 1.436188899884963 | validation: 2.1369420547218247]
	TIME [epoch: 9.68 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7633049196267017		[learning rate: 0.0099883]
	Learning Rate: 0.00998828
	LOSS [training: 1.7633049196267017 | validation: 2.545424657738029]
	TIME [epoch: 9.68 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7093194366979383		[learning rate: 0.0099877]
	Learning Rate: 0.00998772
	LOSS [training: 1.7093194366979383 | validation: 2.1160111637576]
	TIME [epoch: 9.68 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.441491308006727		[learning rate: 0.0099872]
	Learning Rate: 0.00998716
	LOSS [training: 1.441491308006727 | validation: 2.0646972450804624]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4259199862321812		[learning rate: 0.0099866]
	Learning Rate: 0.00998658
	LOSS [training: 1.4259199862321812 | validation: 2.128786985839821]
	TIME [epoch: 9.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4265341472957485		[learning rate: 0.009986]
	Learning Rate: 0.00998598
	LOSS [training: 1.4265341472957485 | validation: 2.2302669863369493]
	TIME [epoch: 9.72 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5735811214629774		[learning rate: 0.0099854]
	Learning Rate: 0.00998538
	LOSS [training: 2.5735811214629774 | validation: 2.32111611889464]
	TIME [epoch: 9.72 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369701348824151		[learning rate: 0.0099848]
	Learning Rate: 0.00998476
	LOSS [training: 2.369701348824151 | validation: 2.763594095669845]
	TIME [epoch: 9.72 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8524620334478406		[learning rate: 0.0099841]
	Learning Rate: 0.00998413
	LOSS [training: 2.8524620334478406 | validation: 3.058458873363295]
	TIME [epoch: 9.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8911325503855174		[learning rate: 0.0099835]
	Learning Rate: 0.00998348
	LOSS [training: 2.8911325503855174 | validation: 2.966217891200468]
	TIME [epoch: 9.73 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8640762804157838		[learning rate: 0.0099828]
	Learning Rate: 0.00998283
	LOSS [training: 2.8640762804157838 | validation: 2.883134459656564]
	TIME [epoch: 9.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9136658133877673		[learning rate: 0.0099822]
	Learning Rate: 0.00998216
	LOSS [training: 2.9136658133877673 | validation: 3.089592637540049]
	TIME [epoch: 9.72 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.059703228821233		[learning rate: 0.0099815]
	Learning Rate: 0.00998147
	LOSS [training: 3.059703228821233 | validation: 2.9966146684420183]
	TIME [epoch: 9.73 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0166069845139516		[learning rate: 0.0099808]
	Learning Rate: 0.00998078
	LOSS [training: 3.0166069845139516 | validation: 2.9454711483419107]
	TIME [epoch: 9.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.926403673275356		[learning rate: 0.0099801]
	Learning Rate: 0.00998007
	LOSS [training: 2.926403673275356 | validation: 3.207353785459358]
	TIME [epoch: 9.72 sec]
EPOCH 77/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 3.577472605562325		[learning rate: 0.0099793]
	Learning Rate: 0.00997935
	LOSS [training: 3.577472605562325 | validation: 3.664312583555936]
	TIME [epoch: 102 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.621997129110843		[learning rate: 0.0099786]
	Learning Rate: 0.00997862
	LOSS [training: 3.621997129110843 | validation: 3.684317582351161]
	TIME [epoch: 9.79 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7164874420377534		[learning rate: 0.0099779]
	Learning Rate: 0.00997787
	LOSS [training: 3.7164874420377534 | validation: 3.9441632994676867]
	TIME [epoch: 9.7 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.310389941646491		[learning rate: 0.0099771]
	Learning Rate: 0.00997711
	LOSS [training: 4.310389941646491 | validation: 4.754672217572619]
	TIME [epoch: 9.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.637665709002246		[learning rate: 0.0099763]
	Learning Rate: 0.00997634
	LOSS [training: 4.637665709002246 | validation: 4.614450223557139]
	TIME [epoch: 9.7 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.720964342789225		[learning rate: 0.0099756]
	Learning Rate: 0.00997555
	LOSS [training: 4.720964342789225 | validation: 4.929751136319472]
	TIME [epoch: 9.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0014170434129195		[learning rate: 0.0099748]
	Learning Rate: 0.00997476
	LOSS [training: 5.0014170434129195 | validation: 5.022416313795716]
	TIME [epoch: 9.75 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.7045060116044235		[learning rate: 0.0099739]
	Learning Rate: 0.00997395
	LOSS [training: 5.7045060116044235 | validation: 5.296180415184864]
	TIME [epoch: 9.71 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.072507766366508		[learning rate: 0.0099731]
	Learning Rate: 0.00997312
	LOSS [training: 5.072507766366508 | validation: 4.589708228954297]
	TIME [epoch: 9.71 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.487456276257109		[learning rate: 0.0099723]
	Learning Rate: 0.00997229
	LOSS [training: 6.487456276257109 | validation: 7.09537286687741]
	TIME [epoch: 9.71 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.517925340326464		[learning rate: 0.0099714]
	Learning Rate: 0.00997144
	LOSS [training: 5.517925340326464 | validation: 3.6522896383759234]
	TIME [epoch: 9.72 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.00892421424865		[learning rate: 0.0099706]
	Learning Rate: 0.00997058
	LOSS [training: 4.00892421424865 | validation: 4.544751663626876]
	TIME [epoch: 9.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.316681649467874		[learning rate: 0.0099697]
	Learning Rate: 0.0099697
	LOSS [training: 4.316681649467874 | validation: 7.454074471402]
	TIME [epoch: 9.71 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.448068700896255		[learning rate: 0.0099688]
	Learning Rate: 0.00996882
	LOSS [training: 7.448068700896255 | validation: 7.314788724821977]
	TIME [epoch: 9.7 sec]
EPOCH 91/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.0010000000000000002
		[batch 4/4] avg loss: 5.899680034417884		[learning rate: 0.0099679]
	Learning Rate: 0.00996792
	LOSS [training: 5.899680034417884 | validation: 7.019882785766349]
	TIME [epoch: 103 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.384914474436498		[learning rate: 0.009967]
	Learning Rate: 0.009967
	LOSS [training: 6.384914474436498 | validation: 7.006614865042309]
	TIME [epoch: 9.78 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.512122319091912		[learning rate: 0.0099661]
	Learning Rate: 0.00996608
	LOSS [training: 6.512122319091912 | validation: 5.046910406767948]
	TIME [epoch: 9.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.459949324817175		[learning rate: 0.0099651]
	Learning Rate: 0.00996514
	LOSS [training: 5.459949324817175 | validation: 5.299774461728152]
	TIME [epoch: 9.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.959632964470234		[learning rate: 0.0099642]
	Learning Rate: 0.00996419
	LOSS [training: 5.959632964470234 | validation: 7.092047268584151]
	TIME [epoch: 9.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.882861558342121		[learning rate: 0.0099632]
	Learning Rate: 0.00996323
	LOSS [training: 5.882861558342121 | validation: 5.9248994010027545]
	TIME [epoch: 9.72 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.009158931462926		[learning rate: 0.0099623]
	Learning Rate: 0.00996225
	LOSS [training: 6.009158931462926 | validation: 6.953024311143648]
	TIME [epoch: 9.71 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.813777315785321		[learning rate: 0.0099613]
	Learning Rate: 0.00996126
	LOSS [training: 6.813777315785321 | validation: 5.969611375830937]
	TIME [epoch: 9.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.48729327293684		[learning rate: 0.0099603]
	Learning Rate: 0.00996026
	LOSS [training: 5.48729327293684 | validation: 4.617491100678911]
	TIME [epoch: 9.69 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.026823514570981		[learning rate: 0.0099592]
	Learning Rate: 0.00995925
	LOSS [training: 5.026823514570981 | validation: 5.2988238504384535]
	TIME [epoch: 9.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.643329605079915		[learning rate: 0.0099582]
	Learning Rate: 0.00995822
	LOSS [training: 5.643329605079915 | validation: 5.594568586118994]
	TIME [epoch: 9.72 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.489242568116705		[learning rate: 0.0099572]
	Learning Rate: 0.00995718
	LOSS [training: 5.489242568116705 | validation: 4.21633117009044]
	TIME [epoch: 9.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.594192609643061		[learning rate: 0.0099561]
	Learning Rate: 0.00995613
	LOSS [training: 5.594192609643061 | validation: 4.672847174534791]
	TIME [epoch: 9.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.106292212923087		[learning rate: 0.0099551]
	Learning Rate: 0.00995506
	LOSS [training: 5.106292212923087 | validation: 4.130681439302531]
	TIME [epoch: 9.69 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.523110545489118		[learning rate: 0.009954]
	Learning Rate: 0.00995398
	LOSS [training: 4.523110545489118 | validation: 3.916338694631733]
	TIME [epoch: 9.69 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837601997906521		[learning rate: 0.0099529]
	Learning Rate: 0.00995289
	LOSS [training: 4.837601997906521 | validation: 4.735534370471999]
	TIME [epoch: 9.71 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.639468351630075		[learning rate: 0.0099518]
	Learning Rate: 0.00995179
	LOSS [training: 4.639468351630075 | validation: 3.890753092175097]
	TIME [epoch: 9.73 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.070110766271208		[learning rate: 0.0099507]
	Learning Rate: 0.00995067
	LOSS [training: 4.070110766271208 | validation: 3.468332966365682]
	TIME [epoch: 9.7 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9409657240136644		[learning rate: 0.0099495]
	Learning Rate: 0.00994955
	LOSS [training: 3.9409657240136644 | validation: 3.803410820799769]
	TIME [epoch: 9.69 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.900950395410129		[learning rate: 0.0099484]
	Learning Rate: 0.0099484
	LOSS [training: 3.900950395410129 | validation: 3.4142275563242546]
	TIME [epoch: 9.7 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767834205492366		[learning rate: 0.0099473]
	Learning Rate: 0.00994725
	LOSS [training: 3.767834205492366 | validation: 3.488213913757221]
	TIME [epoch: 9.7 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7671868157782984		[learning rate: 0.0099461]
	Learning Rate: 0.00994608
	LOSS [training: 3.7671868157782984 | validation: 3.444996537570309]
	TIME [epoch: 9.74 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.770574218718574		[learning rate: 0.0099449]
	Learning Rate: 0.0099449
	LOSS [training: 3.770574218718574 | validation: 3.451184373845212]
	TIME [epoch: 9.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6891606147348526		[learning rate: 0.0099437]
	Learning Rate: 0.00994371
	LOSS [training: 3.6891606147348526 | validation: 3.6642461107438242]
	TIME [epoch: 9.7 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.482586785026946		[learning rate: 0.0099425]
	Learning Rate: 0.00994251
	LOSS [training: 4.482586785026946 | validation: 5.79313446131525]
	TIME [epoch: 9.71 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6805488994349		[learning rate: 0.0099413]
	Learning Rate: 0.00994129
	LOSS [training: 4.6805488994349 | validation: 3.3843216201104163]
	TIME [epoch: 9.7 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5343283178844676		[learning rate: 0.0099401]
	Learning Rate: 0.00994006
	LOSS [training: 3.5343283178844676 | validation: 3.2595205790190045]
	TIME [epoch: 9.73 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.634892363568595		[learning rate: 0.0099388]
	Learning Rate: 0.00993882
	LOSS [training: 3.634892363568595 | validation: 5.786167066978706]
	TIME [epoch: 9.72 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.842366965627675		[learning rate: 0.0099376]
	Learning Rate: 0.00993756
	LOSS [training: 5.842366965627675 | validation: 5.525345838299064]
	TIME [epoch: 9.69 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.540403922771795		[learning rate: 0.0099363]
	Learning Rate: 0.00993629
	LOSS [training: 5.540403922771795 | validation: 5.785132498184875]
	TIME [epoch: 9.69 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.547007352919199		[learning rate: 0.009935]
	Learning Rate: 0.00993501
	LOSS [training: 5.547007352919199 | validation: 4.261306770053441]
	TIME [epoch: 9.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9069911541915565		[learning rate: 0.0099337]
	Learning Rate: 0.00993372
	LOSS [training: 3.9069911541915565 | validation: 3.303562971517529]
	TIME [epoch: 9.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7335226405380526		[learning rate: 0.0099324]
	Learning Rate: 0.00993241
	LOSS [training: 3.7335226405380526 | validation: 4.1295605456530975]
	TIME [epoch: 9.72 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.926909497367048		[learning rate: 0.0099311]
	Learning Rate: 0.00993109
	LOSS [training: 3.926909497367048 | validation: 3.3761950153784275]
	TIME [epoch: 9.69 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6618620129672426		[learning rate: 0.0099298]
	Learning Rate: 0.00992976
	LOSS [training: 3.6618620129672426 | validation: 3.343251837991824]
	TIME [epoch: 9.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5770676664513514		[learning rate: 0.0099284]
	Learning Rate: 0.00992842
	LOSS [training: 3.5770676664513514 | validation: 3.310718572681255]
	TIME [epoch: 9.69 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.523607632943392		[learning rate: 0.0099271]
	Learning Rate: 0.00992706
	LOSS [training: 3.523607632943392 | validation: 3.2251619439427426]
	TIME [epoch: 9.7 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.728865830149734		[learning rate: 0.0099257]
nan encountered in epoch 128 (validation loss).
	Learning Rate: 0.00992569
	LOSS [training: 3.728865830149734 | validation: nan]
	TIME [epoch: 9.74 sec]
EPOCH 129/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.00010000000000000003
		[batch 4/4] avg loss: 3.8189664017955596		[learning rate: 0.0099243]
	Learning Rate: 0.00992431
	LOSS [training: 3.8189664017955596 | validation: 4.103952170056418]
	TIME [epoch: 103 sec]
EPOCH 130/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 1.0000000000000004e-05
		[batch 4/4] avg loss: 3.9129277517131964		[learning rate: 0.0099229]
	Learning Rate: 0.00992291
	LOSS [training: 3.9129277517131964 | validation: 3.7622356597574047]
	TIME [epoch: 103 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7444188294044625		[learning rate: 0.0099215]
	Learning Rate: 0.00992151
	LOSS [training: 3.7444188294044625 | validation: 3.7624820995865056]
	TIME [epoch: 9.78 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.756078393514832		[learning rate: 0.0099201]
	Learning Rate: 0.00992008
	LOSS [training: 3.756078393514832 | validation: 3.7706517762267753]
	TIME [epoch: 9.7 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7512818405106354		[learning rate: 0.0099187]
	Learning Rate: 0.00991865
	LOSS [training: 3.7512818405106354 | validation: 3.7193164207890135]
	TIME [epoch: 9.69 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7426139855587013		[learning rate: 0.0099172]
	Learning Rate: 0.00991721
	LOSS [training: 3.7426139855587013 | validation: 3.7598759868702016]
	TIME [epoch: 9.69 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.742638101928163		[learning rate: 0.0099157]
	Learning Rate: 0.00991575
	LOSS [training: 3.742638101928163 | validation: 3.7684834159180634]
	TIME [epoch: 9.7 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748415797256419		[learning rate: 0.0099143]
	Learning Rate: 0.00991428
	LOSS [training: 3.748415797256419 | validation: 3.76799377878348]
	TIME [epoch: 9.73 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7593642060166568		[learning rate: 0.0099128]
	Learning Rate: 0.0099128
	LOSS [training: 3.7593642060166568 | validation: 3.772699301852764]
	TIME [epoch: 9.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7569922418617803		[learning rate: 0.0099113]
	Learning Rate: 0.0099113
	LOSS [training: 3.7569922418617803 | validation: 3.764217253529246]
	TIME [epoch: 9.7 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.754402643526329		[learning rate: 0.0099098]
	Learning Rate: 0.00990979
	LOSS [training: 3.754402643526329 | validation: 4.039777303879143]
	TIME [epoch: 9.69 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.813242985311441		[learning rate: 0.0099083]
	Learning Rate: 0.00990827
	LOSS [training: 3.813242985311441 | validation: 3.7362108490904333]
	TIME [epoch: 9.7 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7282057404264357		[learning rate: 0.0099067]
	Learning Rate: 0.00990674
	LOSS [training: 3.7282057404264357 | validation: 3.6364942433138823]
	TIME [epoch: 9.74 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8349831316065393		[learning rate: 0.0099052]
	Learning Rate: 0.00990519
	LOSS [training: 3.8349831316065393 | validation: 3.8956098827036656]
	TIME [epoch: 9.7 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.055805371272325		[learning rate: 0.0099036]
	Learning Rate: 0.00990363
	LOSS [training: 4.055805371272325 | validation: 4.189779475225446]
	TIME [epoch: 9.7 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.198969857283513		[learning rate: 0.0099021]
	Learning Rate: 0.00990206
	LOSS [training: 4.198969857283513 | validation: 4.015428034626643]
	TIME [epoch: 9.69 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.999761430532044		[learning rate: 0.0099005]
	Learning Rate: 0.00990048
	LOSS [training: 3.999761430532044 | validation: 4.021763797339059]
	TIME [epoch: 9.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.095744347403499		[learning rate: 0.0098989]
	Learning Rate: 0.00989888
	LOSS [training: 4.095744347403499 | validation: 4.026807910663235]
	TIME [epoch: 9.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.028558949251429		[learning rate: 0.0098973]
	Learning Rate: 0.00989727
	LOSS [training: 4.028558949251429 | validation: 3.9508801533172377]
	TIME [epoch: 9.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9203084139547686		[learning rate: 0.0098956]
	Learning Rate: 0.00989565
	LOSS [training: 3.9203084139547686 | validation: 3.5152859797399785]
	TIME [epoch: 9.68 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9941754627274677		[learning rate: 0.009894]
	Learning Rate: 0.00989401
	LOSS [training: 3.9941754627274677 | validation: 3.973307520243175]
	TIME [epoch: 9.68 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.939383429083189		[learning rate: 0.0098924]
	Learning Rate: 0.00989237
	LOSS [training: 3.939383429083189 | validation: 3.9026915787528607]
	TIME [epoch: 9.68 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8803122456446033		[learning rate: 0.0098907]
	Learning Rate: 0.00989071
	LOSS [training: 3.8803122456446033 | validation: 3.869933340015406]
	TIME [epoch: 9.72 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.85592400668593		[learning rate: 0.009889]
	Learning Rate: 0.00988904
	LOSS [training: 3.85592400668593 | validation: 4.646432688305101]
	TIME [epoch: 9.71 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.044964533580727		[learning rate: 0.0098874]
	Learning Rate: 0.00988735
	LOSS [training: 4.044964533580727 | validation: 3.5355953085802305]
	TIME [epoch: 9.68 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5763159185776012		[learning rate: 0.0098857]
	Learning Rate: 0.00988565
	LOSS [training: 3.5763159185776012 | validation: 3.600037381277314]
	TIME [epoch: 9.68 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0569670708205745		[learning rate: 0.0098839]
	Learning Rate: 0.00988395
	LOSS [training: 4.0569670708205745 | validation: 3.8969423780423567]
	TIME [epoch: 9.68 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225585788013659		[learning rate: 0.0098822]
	Learning Rate: 0.00988222
	LOSS [training: 4.225585788013659 | validation: 3.5720259743792777]
	TIME [epoch: 9.71 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804032606507915		[learning rate: 0.0098805]
	Learning Rate: 0.00988049
	LOSS [training: 3.804032606507915 | validation: 3.3008901906217067]
	TIME [epoch: 9.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5472595175826807		[learning rate: 0.0098787]
	Learning Rate: 0.00987874
	LOSS [training: 3.5472595175826807 | validation: 3.2317116247619118]
	TIME [epoch: 9.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375308413816704		[learning rate: 0.009877]
	Learning Rate: 0.00987698
	LOSS [training: 3.375308413816704 | validation: 3.16199596149612]
	TIME [epoch: 9.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3578024893508083		[learning rate: 0.0098752]
	Learning Rate: 0.00987521
	LOSS [training: 3.3578024893508083 | validation: 3.1582921249588995]
	TIME [epoch: 9.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3940986978268577		[learning rate: 0.0098734]
	Learning Rate: 0.00987343
	LOSS [training: 3.3940986978268577 | validation: 3.1526768584383085]
	TIME [epoch: 9.7 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.471869556431379		[learning rate: 0.0098716]
	Learning Rate: 0.00987163
	LOSS [training: 3.471869556431379 | validation: 3.6925960369331756]
	TIME [epoch: 9.73 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.723549746096122		[learning rate: 0.0098698]
	Learning Rate: 0.00986982
	LOSS [training: 3.723549746096122 | validation: 3.8146258119074803]
	TIME [epoch: 9.69 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8770450945397164		[learning rate: 0.009868]
	Learning Rate: 0.009868
	LOSS [training: 3.8770450945397164 | validation: 3.882011580247894]
	TIME [epoch: 9.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9161117187161767		[learning rate: 0.0098662]
	Learning Rate: 0.00986616
	LOSS [training: 3.9161117187161767 | validation: 3.876908081422674]
	TIME [epoch: 9.68 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8849694491548044		[learning rate: 0.0098643]
	Learning Rate: 0.00986431
	LOSS [training: 3.8849694491548044 | validation: 3.877949181270071]
	TIME [epoch: 9.69 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.882930753502568		[learning rate: 0.0098625]
	Learning Rate: 0.00986245
	LOSS [training: 3.882930753502568 | validation: 3.7279141314210333]
	TIME [epoch: 9.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.82350579973328		[learning rate: 0.0098606]
	Learning Rate: 0.00986058
	LOSS [training: 3.82350579973328 | validation: 3.7387568827487563]
	TIME [epoch: 9.68 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7445824460030703		[learning rate: 0.0098587]
	Learning Rate: 0.0098587
	LOSS [training: 3.7445824460030703 | validation: 3.7692778136841563]
	TIME [epoch: 9.68 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8650287378942014		[learning rate: 0.0098568]
	Learning Rate: 0.0098568
	LOSS [training: 3.8650287378942014 | validation: 3.9317919445490714]
	TIME [epoch: 9.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.969015775138642		[learning rate: 0.0098549]
	Learning Rate: 0.00985489
	LOSS [training: 3.969015775138642 | validation: 3.9236686113213657]
	TIME [epoch: 9.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8979810220865296		[learning rate: 0.009853]
	Learning Rate: 0.00985297
	LOSS [training: 3.8979810220865296 | validation: 3.8360073909791184]
	TIME [epoch: 9.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.867235380091233		[learning rate: 0.009851]
	Learning Rate: 0.00985103
	LOSS [training: 3.867235380091233 | validation: 3.799257567688403]
	TIME [epoch: 9.69 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8522765159984247		[learning rate: 0.0098491]
	Learning Rate: 0.00984909
	LOSS [training: 3.8522765159984247 | validation: 3.8033487283010086]
	TIME [epoch: 9.68 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.881048295134918		[learning rate: 0.0098471]
	Learning Rate: 0.00984713
	LOSS [training: 3.881048295134918 | validation: 3.9717784048850566]
	TIME [epoch: 9.68 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9642544283438585		[learning rate: 0.0098452]
	Learning Rate: 0.00984516
	LOSS [training: 3.9642544283438585 | validation: 3.9413236004302394]
	TIME [epoch: 9.69 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8998437630659892		[learning rate: 0.0098432]
	Learning Rate: 0.00984317
	LOSS [training: 3.8998437630659892 | validation: 3.8845713849495684]
	TIME [epoch: 9.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8586947184846627		[learning rate: 0.0098412]
	Learning Rate: 0.00984118
	LOSS [training: 3.8586947184846627 | validation: 3.874250486210869]
	TIME [epoch: 9.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8711797909461385		[learning rate: 0.0098392]
	Learning Rate: 0.00983917
	LOSS [training: 3.8711797909461385 | validation: 3.8990238926843306]
	TIME [epoch: 9.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9074658701975524		[learning rate: 0.0098371]
	Learning Rate: 0.00983714
	LOSS [training: 3.9074658701975524 | validation: 3.897001419026238]
	TIME [epoch: 9.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9019493775413854		[learning rate: 0.0098351]
	Learning Rate: 0.00983511
	LOSS [training: 3.9019493775413854 | validation: 3.8936690498870368]
	TIME [epoch: 9.7 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9165892939208833		[learning rate: 0.0098331]
	Learning Rate: 0.00983306
	LOSS [training: 3.9165892939208833 | validation: 3.922495240528413]
	TIME [epoch: 9.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.934581308902343		[learning rate: 0.009831]
	Learning Rate: 0.00983101
	LOSS [training: 3.934581308902343 | validation: 3.913870935463067]
	TIME [epoch: 9.73 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9228416570259186		[learning rate: 0.0098289]
	Learning Rate: 0.00982894
	LOSS [training: 3.9228416570259186 | validation: 3.9023893641709435]
	TIME [epoch: 9.7 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9062742617157005		[learning rate: 0.0098269]
	Learning Rate: 0.00982685
	LOSS [training: 3.9062742617157005 | validation: 3.917538733283946]
	TIME [epoch: 9.7 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.950871370221855		[learning rate: 0.0098248]
	Learning Rate: 0.00982476
	LOSS [training: 3.950871370221855 | validation: 4.069090050819774]
	TIME [epoch: 9.69 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.050388623167651		[learning rate: 0.0098226]
	Learning Rate: 0.00982265
	LOSS [training: 4.050388623167651 | validation: 4.151718941970927]
	TIME [epoch: 9.69 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1758594432587985		[learning rate: 0.0098205]
	Learning Rate: 0.00982053
	LOSS [training: 4.1758594432587985 | validation: 4.18738071580059]
	TIME [epoch: 9.74 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153359804009778		[learning rate: 0.0098184]
	Learning Rate: 0.00981839
	LOSS [training: 4.153359804009778 | validation: 4.130506372555713]
	TIME [epoch: 9.69 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.092035351055174		[learning rate: 0.0098162]
	Learning Rate: 0.00981625
	LOSS [training: 4.092035351055174 | validation: 4.061876506853397]
	TIME [epoch: 9.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.034244082189661		[learning rate: 0.0098141]
	Learning Rate: 0.00981409
	LOSS [training: 4.034244082189661 | validation: 4.0015177718194685]
	TIME [epoch: 9.7 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.965511338145121		[learning rate: 0.0098119]
	Learning Rate: 0.00981192
	LOSS [training: 3.965511338145121 | validation: 3.9193101900753042]
	TIME [epoch: 9.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8979607961317213		[learning rate: 0.0098097]
	Learning Rate: 0.00980974
	LOSS [training: 3.8979607961317213 | validation: 3.8747526454511187]
	TIME [epoch: 9.74 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8589174268809416		[learning rate: 0.0098075]
	Learning Rate: 0.00980754
	LOSS [training: 3.8589174268809416 | validation: 3.8443515617740833]
	TIME [epoch: 9.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8207259726830776		[learning rate: 0.0098053]
	Learning Rate: 0.00980534
	LOSS [training: 3.8207259726830776 | validation: 3.7888662754835822]
	TIME [epoch: 9.7 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.294076666933005		[learning rate: 0.0098031]
	Learning Rate: 0.00980312
	LOSS [training: 4.294076666933005 | validation: 4.325125389783504]
	TIME [epoch: 9.69 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8718253448504836		[learning rate: 0.0098009]
	Learning Rate: 0.00980088
	LOSS [training: 3.8718253448504836 | validation: 3.574615666899308]
	TIME [epoch: 9.71 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.623912579069923		[learning rate: 0.0097986]
	Learning Rate: 0.00979864
	LOSS [training: 3.623912579069923 | validation: 3.5537759983373505]
	TIME [epoch: 9.75 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195698872360786		[learning rate: 0.0097964]
	Learning Rate: 0.00979638
	LOSS [training: 4.195698872360786 | validation: 4.791038333294146]
	TIME [epoch: 9.71 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.227606889015506		[learning rate: 0.0097941]
	Learning Rate: 0.00979412
	LOSS [training: 4.227606889015506 | validation: 4.780840487121985]
	TIME [epoch: 9.71 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.69912092294042		[learning rate: 0.0097918]
	Learning Rate: 0.00979183
	LOSS [training: 4.69912092294042 | validation: 4.339952134440654]
	TIME [epoch: 110 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.036211073995102		[learning rate: 0.0097895]
	Learning Rate: 0.00978954
	LOSS [training: 4.036211073995102 | validation: 4.459258151096844]
	TIME [epoch: 19.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.27481431081497		[learning rate: 0.0097872]
	Learning Rate: 0.00978723
	LOSS [training: 4.27481431081497 | validation: 4.562249188643255]
	TIME [epoch: 19.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2535537466978734		[learning rate: 0.0097849]
	Learning Rate: 0.00978492
	LOSS [training: 4.2535537466978734 | validation: 3.7633855814093344]
	TIME [epoch: 19.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8065053066223484		[learning rate: 0.0097826]
	Learning Rate: 0.00978259
	LOSS [training: 3.8065053066223484 | validation: 3.8394372312301277]
	TIME [epoch: 19.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8126010376375135		[learning rate: 0.0097802]
	Learning Rate: 0.00978024
	LOSS [training: 3.8126010376375135 | validation: 3.829639533510858]
	TIME [epoch: 19.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7615756652942953		[learning rate: 0.0097779]
	Learning Rate: 0.00977789
	LOSS [training: 3.7615756652942953 | validation: 3.6662471531585736]
	TIME [epoch: 19.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6042580750173006		[learning rate: 0.0097755]
	Learning Rate: 0.00977552
	LOSS [training: 3.6042580750173006 | validation: 3.5557208353833873]
	TIME [epoch: 19.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.665753502232172		[learning rate: 0.0097731]
	Learning Rate: 0.00977314
	LOSS [training: 3.665753502232172 | validation: 3.7549306247581318]
	TIME [epoch: 19.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7448634243440893		[learning rate: 0.0097708]
	Learning Rate: 0.00977075
	LOSS [training: 3.7448634243440893 | validation: 3.8085474308386695]
	TIME [epoch: 19.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.793482570293249		[learning rate: 0.0097683]
	Learning Rate: 0.00976835
	LOSS [training: 3.793482570293249 | validation: 3.836948546819454]
	TIME [epoch: 19.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8366141745161784		[learning rate: 0.0097659]
	Learning Rate: 0.00976593
	LOSS [training: 3.8366141745161784 | validation: 3.847534611933724]
	TIME [epoch: 19.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804222050016379		[learning rate: 0.0097635]
	Learning Rate: 0.0097635
	LOSS [training: 3.804222050016379 | validation: 3.8291454801006424]
	TIME [epoch: 19.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.818959213023664		[learning rate: 0.0097611]
	Learning Rate: 0.00976106
	LOSS [training: 3.818959213023664 | validation: 3.8609700139157788]
	TIME [epoch: 19.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8723380127914746		[learning rate: 0.0097586]
	Learning Rate: 0.00975861
	LOSS [training: 3.8723380127914746 | validation: 3.950292932065743]
	TIME [epoch: 19.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9344454182340924		[learning rate: 0.0097561]
	Learning Rate: 0.00975615
	LOSS [training: 3.9344454182340924 | validation: 4.034529677032513]
	TIME [epoch: 19.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.078473539304779		[learning rate: 0.0097537]
	Learning Rate: 0.00975367
	LOSS [training: 4.078473539304779 | validation: 3.9552718246517227]
	TIME [epoch: 19.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842063826821497		[learning rate: 0.0097512]
	Learning Rate: 0.00975118
	LOSS [training: 3.842063826821497 | validation: 3.7878808115556897]
	TIME [epoch: 19.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7596489231578807		[learning rate: 0.0097487]
	Learning Rate: 0.00974868
	LOSS [training: 3.7596489231578807 | validation: 3.770723168331844]
	TIME [epoch: 19.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7510213494404576		[learning rate: 0.0097462]
	Learning Rate: 0.00974616
	LOSS [training: 3.7510213494404576 | validation: 3.769795384472174]
	TIME [epoch: 19.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.751015391075153		[learning rate: 0.0097436]
	Learning Rate: 0.00974364
	LOSS [training: 3.751015391075153 | validation: 3.769573840447395]
	TIME [epoch: 19.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7426100667209443		[learning rate: 0.0097411]
	Learning Rate: 0.0097411
	LOSS [training: 3.7426100667209443 | validation: 3.758725731609977]
	TIME [epoch: 19.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.738183668867541		[learning rate: 0.0097385]
	Learning Rate: 0.00973855
	LOSS [training: 3.738183668867541 | validation: 3.755274567685798]
	TIME [epoch: 19.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.742809136192241		[learning rate: 0.009736]
	Learning Rate: 0.00973599
	LOSS [training: 3.742809136192241 | validation: 3.7621538998128137]
	TIME [epoch: 19.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7435928107599548		[learning rate: 0.0097334]
	Learning Rate: 0.00973341
	LOSS [training: 3.7435928107599548 | validation: 3.7664958784592697]
	TIME [epoch: 19.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7462060596339013		[learning rate: 0.0097308]
	Learning Rate: 0.00973083
	LOSS [training: 3.7462060596339013 | validation: 3.761352431159385]
	TIME [epoch: 19.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7471715193398785		[learning rate: 0.0097282]
	Learning Rate: 0.00972823
	LOSS [training: 3.7471715193398785 | validation: 3.781916030003737]
	TIME [epoch: 19.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7661942402644213		[learning rate: 0.0097256]
	Learning Rate: 0.00972562
	LOSS [training: 3.7661942402644213 | validation: 3.810493004998434]
	TIME [epoch: 19.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838707256193575		[learning rate: 0.009723]
	Learning Rate: 0.00972299
	LOSS [training: 3.838707256193575 | validation: 3.9009892236890664]
	TIME [epoch: 19.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.851811317040267		[learning rate: 0.0097204]
	Learning Rate: 0.00972036
	LOSS [training: 3.851811317040267 | validation: 3.857123081420755]
	TIME [epoch: 19.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795089248064916		[learning rate: 0.0097177]
	Learning Rate: 0.00971771
	LOSS [training: 3.795089248064916 | validation: 3.772577013073862]
	TIME [epoch: 19.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.739785928396921		[learning rate: 0.0097151]
	Learning Rate: 0.00971505
	LOSS [training: 3.739785928396921 | validation: 3.7206307507572562]
	TIME [epoch: 19.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.705282560520746		[learning rate: 0.0097124]
	Learning Rate: 0.00971238
	LOSS [training: 3.705282560520746 | validation: 3.708486548749888]
	TIME [epoch: 19.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7120461121892356		[learning rate: 0.0097097]
	Learning Rate: 0.0097097
	LOSS [training: 3.7120461121892356 | validation: 3.7337739098506737]
	TIME [epoch: 19.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7310243369658798		[learning rate: 0.009707]
	Learning Rate: 0.009707
	LOSS [training: 3.7310243369658798 | validation: 3.7585596155277954]
	TIME [epoch: 19.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.75540318307496		[learning rate: 0.0097043]
	Learning Rate: 0.00970429
	LOSS [training: 3.75540318307496 | validation: 3.7847068112568034]
	TIME [epoch: 19.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7719888404965256		[learning rate: 0.0097016]
	Learning Rate: 0.00970157
	LOSS [training: 3.7719888404965256 | validation: 3.77962716005028]
	TIME [epoch: 19.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7666903867930595		[learning rate: 0.0096988]
	Learning Rate: 0.00969884
	LOSS [training: 3.7666903867930595 | validation: 3.7701335480956417]
	TIME [epoch: 19.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7649519365726514		[learning rate: 0.0096961]
	Learning Rate: 0.0096961
	LOSS [training: 3.7649519365726514 | validation: 3.774622151915096]
	TIME [epoch: 19.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.754422774324772		[learning rate: 0.0096933]
	Learning Rate: 0.00969334
	LOSS [training: 3.754422774324772 | validation: 3.7648349709949755]
	TIME [epoch: 19.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7500539428366437		[learning rate: 0.0096906]
	Learning Rate: 0.00969057
	LOSS [training: 3.7500539428366437 | validation: 3.747497581282264]
	TIME [epoch: 19.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.742591999182141		[learning rate: 0.0096878]
	Learning Rate: 0.00968779
	LOSS [training: 3.742591999182141 | validation: 3.7537618736252103]
	TIME [epoch: 19.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.743088962633149		[learning rate: 0.009685]
	Learning Rate: 0.009685
	LOSS [training: 3.743088962633149 | validation: 3.7738704987676077]
	TIME [epoch: 19.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7673057067933016		[learning rate: 0.0096822]
	Learning Rate: 0.0096822
	LOSS [training: 3.7673057067933016 | validation: 3.7852568231558674]
	TIME [epoch: 19.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767132932240361		[learning rate: 0.0096794]
	Learning Rate: 0.00967938
	LOSS [training: 3.767132932240361 | validation: 3.7716528392311917]
	TIME [epoch: 19.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.753638254529041		[learning rate: 0.0096766]
	Learning Rate: 0.00967655
	LOSS [training: 3.753638254529041 | validation: 3.762655415840269]
	TIME [epoch: 19.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7462333301988506		[learning rate: 0.0096737]
	Learning Rate: 0.00967371
	LOSS [training: 3.7462333301988506 | validation: 3.7580465770807683]
	TIME [epoch: 19.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8687061620705183		[learning rate: 0.0096709]
	Learning Rate: 0.00967086
	LOSS [training: 3.8687061620705183 | validation: 4.0967957532552735]
	TIME [epoch: 19.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8946198686157043		[learning rate: 0.009668]
	Learning Rate: 0.009668
	LOSS [training: 3.8946198686157043 | validation: 3.74992571538552]
	TIME [epoch: 19.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.737347724072873		[learning rate: 0.0096651]
	Learning Rate: 0.00966512
	LOSS [training: 3.737347724072873 | validation: 3.7520254391943393]
	TIME [epoch: 19.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.068332634446479		[learning rate: 0.0096622]
	Learning Rate: 0.00966223
	LOSS [training: 4.068332634446479 | validation: 4.375957136710316]
	TIME [epoch: 19.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285188103510808		[learning rate: 0.0096593]
	Learning Rate: 0.00965933
	LOSS [training: 4.285188103510808 | validation: 4.0863184602887355]
	TIME [epoch: 19.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.072595220289321		[learning rate: 0.0096564]
	Learning Rate: 0.00965642
	LOSS [training: 4.072595220289321 | validation: 4.146519634582875]
	TIME [epoch: 19.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9312279071945477		[learning rate: 0.0096535]
	Learning Rate: 0.00965349
	LOSS [training: 3.9312279071945477 | validation: 3.7484541672740894]
	TIME [epoch: 19.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.023952178418671		[learning rate: 0.0096506]
	Learning Rate: 0.00965056
	LOSS [training: 4.023952178418671 | validation: 4.34969484745681]
	TIME [epoch: 19.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.292267262112363		[learning rate: 0.0096476]
	Learning Rate: 0.00964761
	LOSS [training: 4.292267262112363 | validation: 4.209678580812662]
	TIME [epoch: 19.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.103261144892702		[learning rate: 0.0096447]
	Learning Rate: 0.00964465
	LOSS [training: 4.103261144892702 | validation: 3.8400759185264963]
	TIME [epoch: 19.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7580630352189237		[learning rate: 0.0096417]
	Learning Rate: 0.00964168
	LOSS [training: 3.7580630352189237 | validation: 3.754703070920179]
	TIME [epoch: 19.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7399619827930204		[learning rate: 0.0096387]
	Learning Rate: 0.0096387
	LOSS [training: 3.7399619827930204 | validation: 3.7589188706734094]
	TIME [epoch: 19.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74426607551433		[learning rate: 0.0096357]
	Learning Rate: 0.0096357
	LOSS [training: 3.74426607551433 | validation: 3.7638155834566303]
	TIME [epoch: 19.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7481275879020552		[learning rate: 0.0096327]
	Learning Rate: 0.00963269
	LOSS [training: 3.7481275879020552 | validation: 3.7656476416225964]
	TIME [epoch: 19.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748656292588527		[learning rate: 0.0096297]
	Learning Rate: 0.00962967
	LOSS [training: 3.748656292588527 | validation: 3.7670615767014977]
	TIME [epoch: 19.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7494773943622848		[learning rate: 0.0096266]
	Learning Rate: 0.00962664
	LOSS [training: 3.7494773943622848 | validation: 3.764489602921513]
	TIME [epoch: 19.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746804105519455		[learning rate: 0.0096236]
	Learning Rate: 0.0096236
	LOSS [training: 3.746804105519455 | validation: 3.762653147202024]
	TIME [epoch: 19.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.745973286546338		[learning rate: 0.0096205]
	Learning Rate: 0.00962054
	LOSS [training: 3.745973286546338 | validation: 3.763753758651897]
	TIME [epoch: 19.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746204683121392		[learning rate: 0.0096175]
	Learning Rate: 0.00961748
	LOSS [training: 3.746204683121392 | validation: 3.762073780933551]
	TIME [epoch: 19.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7447333807647776		[learning rate: 0.0096144]
	Learning Rate: 0.0096144
	LOSS [training: 3.7447333807647776 | validation: 3.760922383510392]
	TIME [epoch: 19.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.744134845152744		[learning rate: 0.0096113]
	Learning Rate: 0.00961131
	LOSS [training: 3.744134845152744 | validation: 3.760937627036106]
	TIME [epoch: 19.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.743497211748811		[learning rate: 0.0096082]
	Learning Rate: 0.00960821
	LOSS [training: 3.743497211748811 | validation: 3.760588599015386]
	TIME [epoch: 19.2 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.745535379372091		[learning rate: 0.0096051]
	Learning Rate: 0.00960509
	LOSS [training: 3.745535379372091 | validation: 3.7646466697101104]
	TIME [epoch: 19.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74731276983394		[learning rate: 0.009602]
	Learning Rate: 0.00960197
	LOSS [training: 3.74731276983394 | validation: 3.763248747768983]
	TIME [epoch: 19.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746266932327036		[learning rate: 0.0095988]
	Learning Rate: 0.00959883
	LOSS [training: 3.746266932327036 | validation: 3.7621652639901226]
	TIME [epoch: 19.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746500120813063		[learning rate: 0.0095957]
	Learning Rate: 0.00959568
	LOSS [training: 3.746500120813063 | validation: 3.7631862158281795]
	TIME [epoch: 19.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7453401400351813		[learning rate: 0.0095925]
	Learning Rate: 0.00959252
	LOSS [training: 3.7453401400351813 | validation: 3.7615010792216195]
	TIME [epoch: 19.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7450554181027718		[learning rate: 0.0095893]
	Learning Rate: 0.00958934
	LOSS [training: 3.7450554181027718 | validation: 3.761946754838958]
	TIME [epoch: 19.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746324348391505		[learning rate: 0.0095862]
	Learning Rate: 0.00958616
	LOSS [training: 3.746324348391505 | validation: 3.7622472180268867]
	TIME [epoch: 19.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.747167214945512		[learning rate: 0.009583]
	Learning Rate: 0.00958296
	LOSS [training: 3.747167214945512 | validation: 3.7612865770263313]
	TIME [epoch: 19.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7446524422692504		[learning rate: 0.0095798]
	Learning Rate: 0.00957975
	LOSS [training: 3.7446524422692504 | validation: 3.76200750315055]
	TIME [epoch: 19.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7476497813524694		[learning rate: 0.0095765]
	Learning Rate: 0.00957653
	LOSS [training: 3.7476497813524694 | validation: 3.7655427372330488]
	TIME [epoch: 19.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7489426409831843		[learning rate: 0.0095733]
	Learning Rate: 0.0095733
	LOSS [training: 3.7489426409831843 | validation: 3.7660352868587537]
	TIME [epoch: 19.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7493954082926315		[learning rate: 0.0095701]
	Learning Rate: 0.00957006
	LOSS [training: 3.7493954082926315 | validation: 3.766856319848344]
	TIME [epoch: 19.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.749300668898453		[learning rate: 0.0095668]
	Learning Rate: 0.0095668
	LOSS [training: 3.749300668898453 | validation: 3.7662147627523956]
	TIME [epoch: 19.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7490576070757613		[learning rate: 0.0095635]
	Learning Rate: 0.00956353
	LOSS [training: 3.7490576070757613 | validation: 3.7630313395339705]
	TIME [epoch: 19.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.750258490661019		[learning rate: 0.0095603]
	Learning Rate: 0.00956026
	LOSS [training: 3.750258490661019 | validation: 3.76241149382735]
	TIME [epoch: 19.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.741722964457649		[learning rate: 0.009557]
	Learning Rate: 0.00955696
	LOSS [training: 3.741722964457649 | validation: 3.7579662026758633]
	TIME [epoch: 19.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.739166647946182		[learning rate: 0.0095537]
	Learning Rate: 0.00955366
	LOSS [training: 3.739166647946182 | validation: 3.7548096567630944]
	TIME [epoch: 19.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7408304549280618		[learning rate: 0.0095503]
	Learning Rate: 0.00955035
	LOSS [training: 3.7408304549280618 | validation: 3.7649242472814293]
	TIME [epoch: 19.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748084079957635		[learning rate: 0.009547]
	Learning Rate: 0.00954702
	LOSS [training: 3.748084079957635 | validation: 3.7633920333289814]
	TIME [epoch: 19.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7461363709453748		[learning rate: 0.0095437]
	Learning Rate: 0.00954369
	LOSS [training: 3.7461363709453748 | validation: 3.7627145511609266]
	TIME [epoch: 19.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7453371340805584		[learning rate: 0.0095403]
	Learning Rate: 0.00954034
	LOSS [training: 3.7453371340805584 | validation: 3.765706844521268]
	TIME [epoch: 19.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7473413914129097		[learning rate: 0.009537]
	Learning Rate: 0.00953698
	LOSS [training: 3.7473413914129097 | validation: 3.7673533679255913]
	TIME [epoch: 19.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74601584636673		[learning rate: 0.0095336]
	Learning Rate: 0.0095336
	LOSS [training: 3.74601584636673 | validation: 3.7603217084772105]
	TIME [epoch: 19.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7732007654533337		[learning rate: 0.0095302]
	Learning Rate: 0.00953022
	LOSS [training: 3.7732007654533337 | validation: 3.768395556709276]
	TIME [epoch: 19.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.749742416843373		[learning rate: 0.0095268]
	Learning Rate: 0.00952682
	LOSS [training: 3.749742416843373 | validation: 3.7812742492684706]
	TIME [epoch: 19.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763739480261794		[learning rate: 0.0095234]
	Learning Rate: 0.00952342
	LOSS [training: 3.763739480261794 | validation: 3.7851532542430695]
	TIME [epoch: 19.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765815882005886		[learning rate: 0.00952]
	Learning Rate: 0.00952
	LOSS [training: 3.765815882005886 | validation: 3.7766438128634894]
	TIME [epoch: 19.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.75841919816724		[learning rate: 0.0095166]
	Learning Rate: 0.00951657
	LOSS [training: 3.75841919816724 | validation: 3.772540368813156]
	TIME [epoch: 19.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7519665662824457		[learning rate: 0.0095131]
	Learning Rate: 0.00951313
	LOSS [training: 3.7519665662824457 | validation: 3.762739614876814]
	TIME [epoch: 19.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.750810625158347		[learning rate: 0.0095097]
	Learning Rate: 0.00950967
	LOSS [training: 3.750810625158347 | validation: 3.7668529139987164]
	TIME [epoch: 19.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.739649175856274		[learning rate: 0.0095062]
	Learning Rate: 0.00950621
	LOSS [training: 3.739649175856274 | validation: 3.7520493484871373]
	TIME [epoch: 19.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.734897889521494		[learning rate: 0.0095027]
	Learning Rate: 0.00950273
	LOSS [training: 3.734897889521494 | validation: 3.7553321870067755]
	TIME [epoch: 19.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.743399196426484		[learning rate: 0.0094992]
	Learning Rate: 0.00949924
	LOSS [training: 3.743399196426484 | validation: 3.7639036054392268]
	TIME [epoch: 19.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746596127940836		[learning rate: 0.0094957]
	Learning Rate: 0.00949574
	LOSS [training: 3.746596127940836 | validation: 3.7642140431314983]
	TIME [epoch: 19.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746900901493098		[learning rate: 0.0094922]
	Learning Rate: 0.00949223
	LOSS [training: 3.746900901493098 | validation: 3.7629572022140856]
	TIME [epoch: 19.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.745295415045236		[learning rate: 0.0094887]
	Learning Rate: 0.00948871
	LOSS [training: 3.745295415045236 | validation: 3.764134668358264]
	TIME [epoch: 19.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7479826081517085		[learning rate: 0.0094852]
	Learning Rate: 0.00948517
	LOSS [training: 3.7479826081517085 | validation: 3.7667128074226444]
	TIME [epoch: 19.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7499993201900335		[learning rate: 0.0094816]
	Learning Rate: 0.00948163
	LOSS [training: 3.7499993201900335 | validation: 3.7678000911569947]
	TIME [epoch: 19.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748918810519756		[learning rate: 0.0094781]
	Learning Rate: 0.00947807
	LOSS [training: 3.748918810519756 | validation: 3.764321860826364]
	TIME [epoch: 19.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7473903080306474		[learning rate: 0.0094745]
	Learning Rate: 0.0094745
	LOSS [training: 3.7473903080306474 | validation: 3.762639848218099]
	TIME [epoch: 19.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7442756816252354		[learning rate: 0.0094709]
	Learning Rate: 0.00947092
	LOSS [training: 3.7442756816252354 | validation: 3.756122768776729]
	TIME [epoch: 19.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7382280693896313		[learning rate: 0.0094673]
	Learning Rate: 0.00946733
	LOSS [training: 3.7382280693896313 | validation: 3.7578240224988235]
	TIME [epoch: 19.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7407636208351294		[learning rate: 0.0094637]
	Learning Rate: 0.00946373
	LOSS [training: 3.7407636208351294 | validation: 3.7558764732995895]
	TIME [epoch: 19.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.737994977310785		[learning rate: 0.0094601]
	Learning Rate: 0.00946011
	LOSS [training: 3.737994977310785 | validation: 3.7481763011615667]
	TIME [epoch: 19.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.733530404524008		[learning rate: 0.0094565]
	Learning Rate: 0.00945649
	LOSS [training: 3.733530404524008 | validation: 3.7557561957032206]
	TIME [epoch: 19.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7418530990654735		[learning rate: 0.0094528]
	Learning Rate: 0.00945285
	LOSS [training: 3.7418530990654735 | validation: 3.761558079435293]
	TIME [epoch: 19.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.744993569840771		[learning rate: 0.0094492]
	Learning Rate: 0.0094492
	LOSS [training: 3.744993569840771 | validation: 3.761647136050761]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd6_20240705_132548/states/model_phi1_1a_v_mmd6_316.pth
Halted early. No improvement in validation loss for 250 epochs.
Finished training in 4749.640 seconds.
