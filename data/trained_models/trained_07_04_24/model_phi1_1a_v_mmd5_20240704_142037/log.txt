Args:
Namespace(name='model_phi1_1a_v_mmd5', outdir='out/model_training/model_phi1_1a_v_mmd5', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.1, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 320982211

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3734319389018586		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 5.3734319389018586 | validation: 5.54598510343328]
	TIME [epoch: 122 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984465512456937		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 4.984465512456937 | validation: 4.835081349273688]
	TIME [epoch: 9.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.780315146968926		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 4.780315146968926 | validation: 5.202459604286412]
	TIME [epoch: 8.91 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.659209792411312		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 4.659209792411312 | validation: 4.699639147020325]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.47628197381689		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 4.47628197381689 | validation: 4.732380485269912]
	TIME [epoch: 8.93 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.362560244840283		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 4.362560244840283 | validation: 4.477853383659187]
	TIME [epoch: 8.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.295929377619528		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 4.295929377619528 | validation: 4.202302676488573]
	TIME [epoch: 8.95 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.052099631946395		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 4.052099631946395 | validation: 4.191807229713749]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161135961097266		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 4.161135961097266 | validation: 4.09887534553312]
	TIME [epoch: 8.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9863271039373456		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 3.9863271039373456 | validation: 4.030158283525864]
	TIME [epoch: 8.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.87856021967127		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 3.87856021967127 | validation: 3.90335659884031]
	TIME [epoch: 8.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7846181816638422		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 3.7846181816638422 | validation: 3.66064964648875]
	TIME [epoch: 8.95 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.583220016246587		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 3.583220016246587 | validation: 3.663605462608061]
	TIME [epoch: 8.98 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3863787405545223		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 3.3863787405545223 | validation: 3.3056879554653107]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1649956719016545		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 3.1649956719016545 | validation: 3.183442390279131]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0966981887958704		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 3.0966981887958704 | validation: 3.2741686329530455]
	TIME [epoch: 8.95 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0593375324179966		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 3.0593375324179966 | validation: 2.9185741091091817]
	TIME [epoch: 8.91 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.019723067894192		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 3.019723067894192 | validation: 2.903915493069957]
	TIME [epoch: 8.93 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.714117240592321		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 2.714117240592321 | validation: 2.90315632344894]
	TIME [epoch: 8.95 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7898040176665884		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 2.7898040176665884 | validation: 2.6526426263956333]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5124523531711733		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 2.5124523531711733 | validation: 2.678786298213041]
	TIME [epoch: 8.92 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5991803756145475		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 2.5991803756145475 | validation: 2.362487270347524]
	TIME [epoch: 8.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3373094787782636		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 2.3373094787782636 | validation: 2.258494615064404]
	TIME [epoch: 8.97 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1603883985050185		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 2.1603883985050185 | validation: 2.1516025055999712]
	TIME [epoch: 8.97 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2182376414276805		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 2.2182376414276805 | validation: 2.1838681364873667]
	TIME [epoch: 8.95 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.002633084892419		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 2.002633084892419 | validation: 2.0733884723395826]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9679027904398498		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 1.9679027904398498 | validation: 2.113189064704527]
	TIME [epoch: 8.92 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9017406778861816		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 1.9017406778861816 | validation: 2.0030128320198255]
	TIME [epoch: 8.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8148077936388238		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 1.8148077936388238 | validation: 2.099697917905689]
	TIME [epoch: 8.89 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5507191013588761		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 1.5507191013588761 | validation: 1.937574311365183]
	TIME [epoch: 8.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2049776133632935		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 2.2049776133632935 | validation: 1.4989149311313974]
	TIME [epoch: 8.93 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4972375987875255		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 1.4972375987875255 | validation: 1.714229453503179]
	TIME [epoch: 8.93 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.647503476970121		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 1.647503476970121 | validation: 1.386613730473751]
	TIME [epoch: 8.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.367461630895641		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 1.367461630895641 | validation: 1.5203465652719497]
	TIME [epoch: 8.93 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8713149395778543		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 1.8713149395778543 | validation: 1.1998193643636477]
	TIME [epoch: 8.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4757688374862545		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 1.4757688374862545 | validation: 1.2037578420738506]
	TIME [epoch: 8.96 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.159672022253654		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 1.159672022253654 | validation: 1.7401842916694639]
	TIME [epoch: 8.94 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3702221724531085		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 1.3702221724531085 | validation: 1.3698669681782447]
	TIME [epoch: 8.93 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1724013134206317		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 1.1724013134206317 | validation: 1.419900889734454]
	TIME [epoch: 8.94 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.026520011511559		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 1.026520011511559 | validation: 1.356549862489081]
	TIME [epoch: 9 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9163185834989449		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 0.9163185834989449 | validation: 1.2645498994547544]
	TIME [epoch: 8.96 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1691727803620855		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 1.1691727803620855 | validation: 0.9534120092752709]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8610103824865005		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 0.8610103824865005 | validation: 1.0630198854405242]
	TIME [epoch: 8.99 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0069772219002306		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 1.0069772219002306 | validation: 1.0270254527235845]
	TIME [epoch: 9.05 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7704712115988459		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 0.7704712115988459 | validation: 1.1159391456599956]
	TIME [epoch: 8.97 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8260486674032489		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 0.8260486674032489 | validation: 0.7098708864481855]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.157116810376215		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 1.157116810376215 | validation: 0.8271214811462331]
	TIME [epoch: 8.96 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6289319141796329		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 0.6289319141796329 | validation: 0.7976712967249644]
	TIME [epoch: 9.02 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.847987940927515		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 0.847987940927515 | validation: 0.7429734040893876]
	TIME [epoch: 8.96 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6606956422295773		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 0.6606956422295773 | validation: 1.113371204634747]
	TIME [epoch: 8.97 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7894646197715177		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 0.7894646197715177 | validation: 0.6550505819887803]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7405716170717446		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 0.7405716170717446 | validation: 0.7508135730706986]
	TIME [epoch: 8.95 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6985023725585681		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 0.6985023725585681 | validation: 0.661042713885958]
	TIME [epoch: 8.88 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6206465273178148		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 0.6206465273178148 | validation: 1.3113750395204906]
	TIME [epoch: 8.87 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8788256978168696		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 0.8788256978168696 | validation: 0.6858544353297221]
	TIME [epoch: 8.88 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7223417163759741		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 0.7223417163759741 | validation: 0.5995572234314848]
	TIME [epoch: 8.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6930161454983548		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 0.6930161454983548 | validation: 0.691807670646214]
	TIME [epoch: 8.9 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465348438187196		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 0.5465348438187196 | validation: 0.6418420268611993]
	TIME [epoch: 8.88 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6759637892540974		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 0.6759637892540974 | validation: 0.584503924179026]
	TIME [epoch: 8.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6640646651449816		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 0.6640646651449816 | validation: 0.6599680551820155]
	TIME [epoch: 8.93 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6470153496967901		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 0.6470153496967901 | validation: 0.5460107713577921]
	TIME [epoch: 8.91 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5424321480778609		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 0.5424321480778609 | validation: 0.6340448201340716]
	TIME [epoch: 8.92 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5604823740268791		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 0.5604823740268791 | validation: 0.891802083323591]
	TIME [epoch: 8.91 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7142763076796067		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 0.7142763076796067 | validation: 0.7284874424518484]
	TIME [epoch: 8.94 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5792544315395441		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 0.5792544315395441 | validation: 0.6862603794924569]
	TIME [epoch: 8.93 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5126281250885939		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 0.5126281250885939 | validation: 0.6245032893156152]
	TIME [epoch: 8.92 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8688258985420436		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 0.8688258985420436 | validation: 0.6514098020494327]
	TIME [epoch: 8.92 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5940334880383764		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 0.5940334880383764 | validation: 0.5872647850391247]
	TIME [epoch: 8.93 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47436329635474733		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 0.47436329635474733 | validation: 0.5380605295018548]
	TIME [epoch: 8.95 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44946145803513027		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 0.44946145803513027 | validation: 0.6799405021376952]
	TIME [epoch: 8.91 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5594047461774698		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 0.5594047461774698 | validation: 0.518461638751024]
	TIME [epoch: 8.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.57154568948722		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 0.57154568948722 | validation: 0.652884751840995]
	TIME [epoch: 8.95 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5286621136274448		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 0.5286621136274448 | validation: 0.49847357808579573]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4965281142191328		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 0.4965281142191328 | validation: 0.5815133584007847]
	TIME [epoch: 8.94 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5056525986424938		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 0.5056525986424938 | validation: 0.5616509213859338]
	TIME [epoch: 8.97 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4635695283055199		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 0.4635695283055199 | validation: 0.5093307748496115]
	TIME [epoch: 8.98 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5687834310615917		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 0.5687834310615917 | validation: 0.5763242689246735]
	TIME [epoch: 9.01 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5799527819742177		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 0.5799527819742177 | validation: 0.58143648483362]
	TIME [epoch: 8.96 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4934327972908049		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 0.4934327972908049 | validation: 0.5290071105210833]
	TIME [epoch: 8.96 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44736740926067875		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 0.44736740926067875 | validation: 0.6003433829187056]
	TIME [epoch: 8.96 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4676373273271425		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 0.4676373273271425 | validation: 0.5478377241612893]
	TIME [epoch: 9.02 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5302170639772281		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 0.5302170639772281 | validation: 1.1756364735786784]
	TIME [epoch: 8.97 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7188555443983783		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 0.7188555443983783 | validation: 0.66736548429962]
	TIME [epoch: 8.96 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4754781839687912		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 0.4754781839687912 | validation: 0.4723711093037825]
	TIME [epoch: 8.97 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42510322569777026		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 0.42510322569777026 | validation: 0.49638492023527314]
	TIME [epoch: 9.04 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4007792382089912		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 0.4007792382089912 | validation: 0.46203544474082703]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44045905631390225		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 0.44045905631390225 | validation: 0.5569145571452913]
	TIME [epoch: 8.88 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5591997834761607		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 0.5591997834761607 | validation: 0.7490571637618317]
	TIME [epoch: 8.87 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5470828784761161		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 0.5470828784761161 | validation: 0.5218269161146379]
	TIME [epoch: 8.93 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46402076590064434		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 0.46402076590064434 | validation: 0.5614845466941017]
	TIME [epoch: 8.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5021992135140783		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 0.5021992135140783 | validation: 0.5029973684013936]
	TIME [epoch: 8.89 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4550499547919666		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 0.4550499547919666 | validation: 0.7046857107233736]
	TIME [epoch: 8.87 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6446478984318194		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 0.6446478984318194 | validation: 0.5070155460290113]
	TIME [epoch: 8.91 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46388746151939536		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 0.46388746151939536 | validation: 0.5082664823869839]
	TIME [epoch: 8.9 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48736873790721214		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 0.48736873790721214 | validation: 0.46917269995593813]
	TIME [epoch: 8.88 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4220557581085863		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 0.4220557581085863 | validation: 0.5690550362908828]
	TIME [epoch: 8.9 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4640290194067239		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 0.4640290194067239 | validation: 0.5654928579524633]
	TIME [epoch: 8.9 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4509475696167081		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 0.4509475696167081 | validation: 0.5134537949849908]
	TIME [epoch: 8.92 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4554830985393991		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 0.4554830985393991 | validation: 0.5088864015717935]
	TIME [epoch: 8.88 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47621681896176765		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 0.47621681896176765 | validation: 0.5304713419022793]
	TIME [epoch: 8.9 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4252899396148568		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 0.4252899396148568 | validation: 0.6950423377875958]
	TIME [epoch: 8.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.432011763334531		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 0.432011763334531 | validation: 0.7507249162512737]
	TIME [epoch: 8.95 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5585766648064555		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 0.5585766648064555 | validation: 0.5217239817286616]
	TIME [epoch: 8.91 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43237194852899485		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 0.43237194852899485 | validation: 0.428074957088055]
	TIME [epoch: 8.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4416391890179135		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 0.4416391890179135 | validation: 0.5538390903065868]
	TIME [epoch: 8.93 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4768633574908443		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 0.4768633574908443 | validation: 0.38503818725963684]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34875249222998544		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 0.34875249222998544 | validation: 0.40013286242151885]
	TIME [epoch: 8.93 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3972276078390821		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 0.3972276078390821 | validation: 0.7961889353524403]
	TIME [epoch: 8.93 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4479831152411265		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 0.4479831152411265 | validation: 0.5047184980367945]
	TIME [epoch: 8.94 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48616518315931323		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 0.48616518315931323 | validation: 0.7576484673714268]
	TIME [epoch: 8.99 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5737620862533068		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 0.5737620862533068 | validation: 0.5391240245080138]
	TIME [epoch: 8.94 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3858120226949291		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 0.3858120226949291 | validation: 0.41710412997306956]
	TIME [epoch: 8.93 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3943139021646461		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 0.3943139021646461 | validation: 0.47157986243087574]
	TIME [epoch: 8.94 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3734739003434878		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 0.3734739003434878 | validation: 0.33706468692773955]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3803021843263943		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 0.3803021843263943 | validation: 0.4131317425972235]
	TIME [epoch: 8.96 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4188398827357901		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 0.4188398827357901 | validation: 0.4615935349239387]
	TIME [epoch: 8.96 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39259187480114666		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 0.39259187480114666 | validation: 0.4847966651037069]
	TIME [epoch: 8.94 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4340781181318173		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 0.4340781181318173 | validation: 0.4722536435615909]
	TIME [epoch: 9.03 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38635254628471144		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 0.38635254628471144 | validation: 0.4104044146972211]
	TIME [epoch: 8.99 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3801575323145619		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 0.3801575323145619 | validation: 0.5781670866546887]
	TIME [epoch: 8.98 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43049877956899807		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 0.43049877956899807 | validation: 0.3924891055226024]
	TIME [epoch: 8.99 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43568906622154857		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 0.43568906622154857 | validation: 0.4033532427843688]
	TIME [epoch: 9.01 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3142108859865219		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 0.3142108859865219 | validation: 0.3415421861358957]
	TIME [epoch: 9 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3696646793211911		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 0.3696646793211911 | validation: 0.5022407338037411]
	TIME [epoch: 8.97 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4612458057604286		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 0.4612458057604286 | validation: 0.868945922648424]
	TIME [epoch: 8.98 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7267400078145462		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 0.7267400078145462 | validation: 0.45040159990599093]
	TIME [epoch: 8.98 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35356064351812855		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 0.35356064351812855 | validation: 0.5268930335159355]
	TIME [epoch: 9.03 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39212496141683195		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 0.39212496141683195 | validation: 0.3215798845540073]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3058866271558813		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 0.3058866271558813 | validation: 0.5179151248475944]
	TIME [epoch: 8.96 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4240398346525863		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 0.4240398346525863 | validation: 0.5055668012448045]
	TIME [epoch: 8.98 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3821064411400857		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 0.3821064411400857 | validation: 0.3086647755342827]
	TIME [epoch: 9.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.342141955501511		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 0.342141955501511 | validation: 0.4024042003998437]
	TIME [epoch: 8.97 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4833896345887201		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 0.4833896345887201 | validation: 0.49502649863139087]
	TIME [epoch: 8.99 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43522693455427053		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 0.43522693455427053 | validation: 0.39704564080410787]
	TIME [epoch: 8.98 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3447728848544619		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 0.3447728848544619 | validation: 0.39086662892850277]
	TIME [epoch: 9.03 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3904618854370847		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 0.3904618854370847 | validation: 0.3897469310525465]
	TIME [epoch: 8.96 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3490804596366292		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 0.3490804596366292 | validation: 0.3954946067215865]
	TIME [epoch: 8.95 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40231022762130786		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 0.40231022762130786 | validation: 0.7127527164982804]
	TIME [epoch: 8.96 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49496264819683117		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 0.49496264819683117 | validation: 0.5744336410684006]
	TIME [epoch: 9.03 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42615847693476794		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 0.42615847693476794 | validation: 0.41347239813839615]
	TIME [epoch: 8.97 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3708917900937258		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 0.3708917900937258 | validation: 0.3718250634947826]
	TIME [epoch: 8.95 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1449711429200218		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 1.1449711429200218 | validation: 0.8753245875787257]
	TIME [epoch: 8.97 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47411145496462365		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 0.47411145496462365 | validation: 0.3651516446191]
	TIME [epoch: 9.03 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37148979244858127		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 0.37148979244858127 | validation: 0.4659068223804325]
	TIME [epoch: 8.99 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37511015900957717		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 0.37511015900957717 | validation: 0.5958628145942757]
	TIME [epoch: 8.97 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38614784816483905		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 0.38614784816483905 | validation: 0.35718494370167714]
	TIME [epoch: 8.97 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36185562504864877		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 0.36185562504864877 | validation: 0.3378276944905395]
	TIME [epoch: 9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3395438556843199		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 0.3395438556843199 | validation: 0.3549025787312977]
	TIME [epoch: 8.99 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3670068495815119		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 0.3670068495815119 | validation: 0.4372241117716445]
	TIME [epoch: 8.98 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37438163407252495		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 0.37438163407252495 | validation: 0.49714397178951764]
	TIME [epoch: 8.97 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4149430424820488		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 0.4149430424820488 | validation: 0.3384082389099029]
	TIME [epoch: 8.99 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37231878003444624		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 0.37231878003444624 | validation: 0.4012750469830392]
	TIME [epoch: 9 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3610704196090523		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 0.3610704196090523 | validation: 0.3806645334090866]
	TIME [epoch: 8.99 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4035425491788928		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 0.4035425491788928 | validation: 0.4383404451628837]
	TIME [epoch: 8.98 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4142103394733173		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 0.4142103394733173 | validation: 0.4003217539671904]
	TIME [epoch: 8.97 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3826713660071554		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 0.3826713660071554 | validation: 0.43551987646211593]
	TIME [epoch: 9.05 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33403975629985144		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 0.33403975629985144 | validation: 0.450243324132663]
	TIME [epoch: 8.98 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3696127540379205		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 0.3696127540379205 | validation: 0.33336552237303063]
	TIME [epoch: 8.99 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4907749243648233		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 0.4907749243648233 | validation: 0.442576369457509]
	TIME [epoch: 9 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.383307834265175		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 0.383307834265175 | validation: 0.36651541377747676]
	TIME [epoch: 9.04 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3738564565553719		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 0.3738564565553719 | validation: 0.46849033708338306]
	TIME [epoch: 8.98 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3730171473058387		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 0.3730171473058387 | validation: 0.49083921122456553]
	TIME [epoch: 8.98 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37303830080649353		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 0.37303830080649353 | validation: 0.42150897353022987]
	TIME [epoch: 8.97 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3640000826398318		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 0.3640000826398318 | validation: 0.5463198323216216]
	TIME [epoch: 9.05 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3871474831732114		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 0.3871474831732114 | validation: 0.4324543939970439]
	TIME [epoch: 8.96 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3385033908453934		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 0.3385033908453934 | validation: 0.3530609979774352]
	TIME [epoch: 8.97 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4692636041593545		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 0.4692636041593545 | validation: 0.4481862200437149]
	TIME [epoch: 8.95 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4379047653220747		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 0.4379047653220747 | validation: 0.3755756682744279]
	TIME [epoch: 9 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377689082666516		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 0.3377689082666516 | validation: 0.5044164646276029]
	TIME [epoch: 8.99 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3722469328397127		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 0.3722469328397127 | validation: 0.3806347259947659]
	TIME [epoch: 8.95 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34285468861816		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 0.34285468861816 | validation: 0.3777694813521644]
	TIME [epoch: 8.97 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36265889714303223		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 0.36265889714303223 | validation: 0.3776182772281939]
	TIME [epoch: 9.02 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34767124283578865		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 0.34767124283578865 | validation: 0.4051899032511352]
	TIME [epoch: 9 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35131498395571503		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 0.35131498395571503 | validation: 0.3979936714870692]
	TIME [epoch: 8.96 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3750601190118466		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 0.3750601190118466 | validation: 0.5025355674066091]
	TIME [epoch: 8.96 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.451060844917529		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 0.451060844917529 | validation: 0.7065018850939804]
	TIME [epoch: 8.99 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40527958696500166		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 0.40527958696500166 | validation: 0.3426622347724604]
	TIME [epoch: 9.03 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35827123059417626		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 0.35827123059417626 | validation: 0.3750418150673579]
	TIME [epoch: 8.99 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35805404126955526		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 0.35805404126955526 | validation: 0.3324616758964296]
	TIME [epoch: 8.95 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37734893310991563		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 0.37734893310991563 | validation: 0.9922728593018739]
	TIME [epoch: 8.96 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164471625801509		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 0.5164471625801509 | validation: 0.47155302201471877]
	TIME [epoch: 9 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6469541524269595		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 2.6469541524269595 | validation: 5.471094007880152]
	TIME [epoch: 8.97 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765531772300176		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 3.765531772300176 | validation: 4.168932589160592]
	TIME [epoch: 8.98 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9079428404352186		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 3.9079428404352186 | validation: 4.579718953390163]
	TIME [epoch: 8.96 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.545733864509078		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 3.545733864509078 | validation: 4.4932337867487675]
	TIME [epoch: 9.01 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.470884601043748		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 3.470884601043748 | validation: 2.4981407929837394]
	TIME [epoch: 8.96 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7434386634117596		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 2.7434386634117596 | validation: 3.454110722085631]
	TIME [epoch: 8.96 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6384670832253976		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 2.6384670832253976 | validation: 2.6757952713988065]
	TIME [epoch: 8.98 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.168366955981343		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 2.168366955981343 | validation: 1.8222262703512402]
	TIME [epoch: 9.02 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.035923143789439		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 2.035923143789439 | validation: 2.3085892844769016]
	TIME [epoch: 8.96 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.299966403501501		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 2.299966403501501 | validation: 2.1460933770800423]
	TIME [epoch: 8.98 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4989256350205662		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 1.4989256350205662 | validation: 1.6993330243762639]
	TIME [epoch: 8.98 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0149933069586365		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 1.0149933069586365 | validation: 2.683875071475751]
	TIME [epoch: 9.01 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8662032472918932		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 3.8662032472918932 | validation: 3.731892853119162]
	TIME [epoch: 8.98 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4987965049734306		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 2.4987965049734306 | validation: 3.792525160633932]
	TIME [epoch: 8.97 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.044624511507237		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 4.044624511507237 | validation: 3.998195625667618]
	TIME [epoch: 8.97 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3023048065863		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 4.3023048065863 | validation: 4.456670067817496]
	TIME [epoch: 9.01 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.115122535379594		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 4.115122535379594 | validation: 4.000852895013476]
	TIME [epoch: 9.01 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5222453611875912		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 3.5222453611875912 | validation: 2.9055861214239407]
	TIME [epoch: 8.97 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.279337911418902		[learning rate: 0.019695]
nan encountered in epoch 200 (validation loss).
	Learning Rate: 0.0196952
	LOSS [training: 3.279337911418902 | validation: nan]
	TIME [epoch: 8.97 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.266715127509697		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 4.266715127509697 | validation: 4.902344467862807]
	TIME [epoch: 132 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18913969056732		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 4.18913969056732 | validation: 4.323636454345122]
	TIME [epoch: 17.6 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9381508611294707		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 3.9381508611294707 | validation: 3.9182091704348734]
	TIME [epoch: 17.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.534032471694216		[learning rate: 0.019679]
	Learning Rate: 0.0196788
	LOSS [training: 3.534032471694216 | validation: 3.4005765773273247]
	TIME [epoch: 17.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.141736416929592		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 3.141736416929592 | validation: 3.040524874246268]
	TIME [epoch: 17.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.902078750593974		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 2.902078750593974 | validation: 2.9815022212457993]
	TIME [epoch: 17.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.868913799407719		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 2.868913799407719 | validation: 2.8490742745743267]
	TIME [epoch: 17.6 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.765028457061258		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 2.765028457061258 | validation: 2.8681830041748695]
	TIME [epoch: 17.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.708145929844244		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 2.708145929844244 | validation: 2.627882068889314]
	TIME [epoch: 17.6 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4891144191613		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 2.4891144191613 | validation: 2.142408863063412]
	TIME [epoch: 17.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9762105814461957		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 1.9762105814461957 | validation: 1.8396743541791623]
	TIME [epoch: 17.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6987547614173426		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 1.6987547614173426 | validation: 2.043083860866246]
	TIME [epoch: 17.6 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2313957383511607		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 2.2313957383511607 | validation: 2.333737552722001]
	TIME [epoch: 17.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0736637501781505		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 2.0736637501781505 | validation: 2.1614219673607966]
	TIME [epoch: 17.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8146522095801432		[learning rate: 0.019631]
	Learning Rate: 0.0196314
	LOSS [training: 1.8146522095801432 | validation: 2.0358735688566982]
	TIME [epoch: 17.6 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6190434460830798		[learning rate: 0.019627]
	Learning Rate: 0.019627
	LOSS [training: 1.6190434460830798 | validation: 1.7850838633505286]
	TIME [epoch: 17.6 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4687348409512688		[learning rate: 0.019622]
	Learning Rate: 0.0196225
	LOSS [training: 1.4687348409512688 | validation: 1.5597938320371034]
	TIME [epoch: 17.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2752087339975826		[learning rate: 0.019618]
	Learning Rate: 0.019618
	LOSS [training: 1.2752087339975826 | validation: 1.1114199181680329]
	TIME [epoch: 17.5 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0627511541582848		[learning rate: 0.019613]
	Learning Rate: 0.0196134
	LOSS [training: 1.0627511541582848 | validation: 1.030919134598256]
	TIME [epoch: 17.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0068668346394931		[learning rate: 0.019609]
	Learning Rate: 0.0196089
	LOSS [training: 1.0068668346394931 | validation: 1.0320054507835563]
	TIME [epoch: 17.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9400839580156393		[learning rate: 0.019604]
	Learning Rate: 0.0196043
	LOSS [training: 0.9400839580156393 | validation: 1.2344349423139727]
	TIME [epoch: 17.6 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9324028067047252		[learning rate: 0.0196]
	Learning Rate: 0.0195997
	LOSS [training: 0.9324028067047252 | validation: 0.787772530670301]
	TIME [epoch: 17.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7986136920029966		[learning rate: 0.019595]
	Learning Rate: 0.019595
	LOSS [training: 0.7986136920029966 | validation: 0.8604291732055418]
	TIME [epoch: 17.6 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8057205260127124		[learning rate: 0.01959]
	Learning Rate: 0.0195904
	LOSS [training: 0.8057205260127124 | validation: 0.8281396126502731]
	TIME [epoch: 17.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7039994888833445		[learning rate: 0.019586]
	Learning Rate: 0.0195857
	LOSS [training: 0.7039994888833445 | validation: 0.7325959287211059]
	TIME [epoch: 17.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.750647555937012		[learning rate: 0.019581]
	Learning Rate: 0.0195809
	LOSS [training: 0.750647555937012 | validation: 0.9166578447874596]
	TIME [epoch: 17.6 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7351676652408978		[learning rate: 0.019576]
	Learning Rate: 0.0195762
	LOSS [training: 0.7351676652408978 | validation: 0.6421465191534181]
	TIME [epoch: 17.6 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7046301254177159		[learning rate: 0.019571]
	Learning Rate: 0.0195714
	LOSS [training: 0.7046301254177159 | validation: 0.716664291234733]
	TIME [epoch: 17.5 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6198784305102314		[learning rate: 0.019567]
	Learning Rate: 0.0195666
	LOSS [training: 0.6198784305102314 | validation: 0.6899595298822178]
	TIME [epoch: 17.6 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6559590203012392		[learning rate: 0.019562]
	Learning Rate: 0.0195618
	LOSS [training: 0.6559590203012392 | validation: 0.7843752265555455]
	TIME [epoch: 17.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6052878947715357		[learning rate: 0.019557]
	Learning Rate: 0.0195569
	LOSS [training: 0.6052878947715357 | validation: 0.736437466946035]
	TIME [epoch: 17.6 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6403704637818748		[learning rate: 0.019552]
	Learning Rate: 0.0195521
	LOSS [training: 0.6403704637818748 | validation: 0.6079921717314157]
	TIME [epoch: 17.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6572162161840148		[learning rate: 0.019547]
	Learning Rate: 0.0195472
	LOSS [training: 0.6572162161840148 | validation: 0.9020277625711887]
	TIME [epoch: 17.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.639988365983086		[learning rate: 0.019542]
	Learning Rate: 0.0195422
	LOSS [training: 0.639988365983086 | validation: 0.8948033364132526]
	TIME [epoch: 17.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6411214301926675		[learning rate: 0.019537]
	Learning Rate: 0.0195373
	LOSS [training: 0.6411214301926675 | validation: 0.7089358037773632]
	TIME [epoch: 17.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5778414720711758		[learning rate: 0.019532]
	Learning Rate: 0.0195323
	LOSS [training: 0.5778414720711758 | validation: 0.6715656222260029]
	TIME [epoch: 17.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9889551061891424		[learning rate: 0.019527]
	Learning Rate: 0.0195273
	LOSS [training: 0.9889551061891424 | validation: 3.5577756291519824]
	TIME [epoch: 17.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.618646816677828		[learning rate: 0.019522]
	Learning Rate: 0.0195222
	LOSS [training: 4.618646816677828 | validation: 4.746627613623408]
	TIME [epoch: 17.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891122165572238		[learning rate: 0.019517]
	Learning Rate: 0.0195172
	LOSS [training: 4.891122165572238 | validation: 5.534637652907272]
	TIME [epoch: 17.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02872220013245		[learning rate: 0.019512]
	Learning Rate: 0.0195121
	LOSS [training: 5.02872220013245 | validation: 4.823403396433647]
	TIME [epoch: 17.6 sec]
EPOCH 241/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.905322211864718		[learning rate: 0.019507]
	Learning Rate: 0.019507
	LOSS [training: 4.905322211864718 | validation: 4.4821470797543554]
	TIME [epoch: 136 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.234913223691528		[learning rate: 0.019502]
	Learning Rate: 0.0195018
	LOSS [training: 5.234913223691528 | validation: 4.965358314353665]
	TIME [epoch: 17.7 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.920569371928085		[learning rate: 0.019497]
	Learning Rate: 0.0194967
	LOSS [training: 4.920569371928085 | validation: 4.918255472556257]
	TIME [epoch: 17.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.306307030936057		[learning rate: 0.019491]
	Learning Rate: 0.0194915
	LOSS [training: 5.306307030936057 | validation: 4.538030659782718]
	TIME [epoch: 17.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.807364210690192		[learning rate: 0.019486]
	Learning Rate: 0.0194863
	LOSS [training: 4.807364210690192 | validation: 4.826040862251645]
	TIME [epoch: 17.7 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.943819420509637		[learning rate: 0.019481]
	Learning Rate: 0.019481
	LOSS [training: 4.943819420509637 | validation: 4.511123532093238]
	TIME [epoch: 17.6 sec]
EPOCH 247/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.0010000000000000002
		[batch 4/4] avg loss: 4.813650297607625		[learning rate: 0.019476]
	Learning Rate: 0.0194757
	LOSS [training: 4.813650297607625 | validation: 4.494178100737347]
	TIME [epoch: 136 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.712953257362437		[learning rate: 0.01947]
	Learning Rate: 0.0194705
	LOSS [training: 5.712953257362437 | validation: 4.8609498163977864]
	TIME [epoch: 17.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.4692387433845235		[learning rate: 0.019465]
	Learning Rate: 0.0194651
	LOSS [training: 5.4692387433845235 | validation: 7.092476484294188]
	TIME [epoch: 17.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.659098130416732		[learning rate: 0.01946]
	Learning Rate: 0.0194598
	LOSS [training: 6.659098130416732 | validation: 5.264587463510835]
	TIME [epoch: 17.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.538354037522861		[learning rate: 0.019454]
	Learning Rate: 0.0194544
	LOSS [training: 5.538354037522861 | validation: 5.0865253383546145]
	TIME [epoch: 17.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.2734877198002925		[learning rate: 0.019449]
	Learning Rate: 0.019449
	LOSS [training: 6.2734877198002925 | validation: 7.351070354420823]
	TIME [epoch: 17.7 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.205950875771597		[learning rate: 0.019444]
	Learning Rate: 0.0194436
	LOSS [training: 7.205950875771597 | validation: 7.172702975711693]
	TIME [epoch: 17.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.140208799709756		[learning rate: 0.019438]
	Learning Rate: 0.0194381
	LOSS [training: 7.140208799709756 | validation: 7.5041437212416575]
	TIME [epoch: 17.7 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.428011107424623		[learning rate: 0.019433]
	Learning Rate: 0.0194327
	LOSS [training: 7.428011107424623 | validation: 7.426152441241344]
	TIME [epoch: 17.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.383350509286577		[learning rate: 0.019427]
	Learning Rate: 0.0194272
	LOSS [training: 7.383350509286577 | validation: 7.440412445843014]
	TIME [epoch: 17.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.321783351494726		[learning rate: 0.019422]
	Learning Rate: 0.0194216
	LOSS [training: 7.321783351494726 | validation: 7.395622199699067]
	TIME [epoch: 17.7 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.235039224847861		[learning rate: 0.019416]
	Learning Rate: 0.0194161
	LOSS [training: 7.235039224847861 | validation: 6.805773323917013]
	TIME [epoch: 17.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.393449476940329		[learning rate: 0.019411]
	Learning Rate: 0.0194105
	LOSS [training: 6.393449476940329 | validation: 6.959650352659757]
	TIME [epoch: 17.7 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.803989364026334		[learning rate: 0.019405]
	Learning Rate: 0.0194049
	LOSS [training: 6.803989364026334 | validation: 6.889098358084059]
	TIME [epoch: 17.6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.336892100152769		[learning rate: 0.019399]
	Learning Rate: 0.0193993
	LOSS [training: 6.336892100152769 | validation: 5.880751964905496]
	TIME [epoch: 17.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.010065861894454		[learning rate: 0.019394]
	Learning Rate: 0.0193936
	LOSS [training: 6.010065861894454 | validation: 6.460520503543561]
	TIME [epoch: 17.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.175592373617982		[learning rate: 0.019388]
	Learning Rate: 0.0193879
	LOSS [training: 6.175592373617982 | validation: 6.644175897854218]
	TIME [epoch: 17.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.040659813599216		[learning rate: 0.019382]
	Learning Rate: 0.0193822
	LOSS [training: 6.040659813599216 | validation: 5.944883619131383]
	TIME [epoch: 17.6 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.7685250042411464		[learning rate: 0.019376]
	Learning Rate: 0.0193765
	LOSS [training: 5.7685250042411464 | validation: 6.7558888526607035]
	TIME [epoch: 17.6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.932342367073801		[learning rate: 0.019371]
	Learning Rate: 0.0193707
	LOSS [training: 6.932342367073801 | validation: 7.275334779308533]
	TIME [epoch: 17.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.196369540244112		[learning rate: 0.019365]
	Learning Rate: 0.0193649
	LOSS [training: 7.196369540244112 | validation: 7.2180068182466055]
	TIME [epoch: 17.6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.075682105736821		[learning rate: 0.019359]
	Learning Rate: 0.0193591
	LOSS [training: 7.075682105736821 | validation: 7.038795183144687]
	TIME [epoch: 17.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.884816922412521		[learning rate: 0.019353]
	Learning Rate: 0.0193533
	LOSS [training: 6.884816922412521 | validation: 6.78622908520592]
	TIME [epoch: 17.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.361174257243847		[learning rate: 0.019347]
	Learning Rate: 0.0193474
	LOSS [training: 6.361174257243847 | validation: 6.752649014689029]
	TIME [epoch: 17.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.5419050356063835		[learning rate: 0.019342]
	Learning Rate: 0.0193416
	LOSS [training: 6.5419050356063835 | validation: 5.797707258742648]
	TIME [epoch: 17.7 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.430842381237598		[learning rate: 0.019336]
	Learning Rate: 0.0193356
	LOSS [training: 5.430842381237598 | validation: 5.768340827669936]
	TIME [epoch: 17.6 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9997458808748405		[learning rate: 0.01933]
	Learning Rate: 0.0193297
	LOSS [training: 4.9997458808748405 | validation: 3.832819358776071]
	TIME [epoch: 17.6 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.077101004472745		[learning rate: 0.019324]
	Learning Rate: 0.0193237
	LOSS [training: 3.077101004472745 | validation: 3.418243359094304]
	TIME [epoch: 17.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3004280463293973		[learning rate: 0.019318]
	Learning Rate: 0.0193178
	LOSS [training: 3.3004280463293973 | validation: 3.9257752818162936]
	TIME [epoch: 17.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30793775390853		[learning rate: 0.019312]
	Learning Rate: 0.0193117
	LOSS [training: 3.30793775390853 | validation: 3.1592965915552176]
	TIME [epoch: 17.7 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.803670238082897		[learning rate: 0.019306]
	Learning Rate: 0.0193057
	LOSS [training: 2.803670238082897 | validation: 2.754705778012375]
	TIME [epoch: 17.6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.703749800005525		[learning rate: 0.0193]
	Learning Rate: 0.0192996
	LOSS [training: 2.703749800005525 | validation: 3.0191306035098733]
	TIME [epoch: 17.6 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6184366626895508		[learning rate: 0.019294]
	Learning Rate: 0.0192935
	LOSS [training: 2.6184366626895508 | validation: 2.701106063140609]
	TIME [epoch: 17.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6600479331744453		[learning rate: 0.019287]
	Learning Rate: 0.0192874
	LOSS [training: 2.6600479331744453 | validation: 2.696986025388046]
	TIME [epoch: 17.6 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.435366454993509		[learning rate: 0.019281]
	Learning Rate: 0.0192813
	LOSS [training: 2.435366454993509 | validation: 2.2186036188627627]
	TIME [epoch: 17.6 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.250306028293452		[learning rate: 0.019275]
	Learning Rate: 0.0192751
	LOSS [training: 2.250306028293452 | validation: 1.9968789004072742]
	TIME [epoch: 17.6 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1344822954070652		[learning rate: 0.019269]
	Learning Rate: 0.0192689
	LOSS [training: 2.1344822954070652 | validation: 1.7343161182329243]
	TIME [epoch: 17.7 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.028420315093628		[learning rate: 0.019263]
	Learning Rate: 0.0192627
	LOSS [training: 2.028420315093628 | validation: 1.8309610127263038]
	TIME [epoch: 17.6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0369931746803718		[learning rate: 0.019256]
	Learning Rate: 0.0192565
	LOSS [training: 2.0369931746803718 | validation: 1.9246193895347075]
	TIME [epoch: 17.6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.010402237883088		[learning rate: 0.01925]
	Learning Rate: 0.0192502
	LOSS [training: 2.010402237883088 | validation: 1.8418188719148392]
	TIME [epoch: 17.6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.022609618156957		[learning rate: 0.019244]
	Learning Rate: 0.0192439
	LOSS [training: 2.022609618156957 | validation: 1.656958675344399]
	TIME [epoch: 17.6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.977746683006488		[learning rate: 0.019238]
	Learning Rate: 0.0192376
	LOSS [training: 1.977746683006488 | validation: 2.082261580847566]
	TIME [epoch: 17.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8442301381986335		[learning rate: 0.019231]
	Learning Rate: 0.0192313
	LOSS [training: 1.8442301381986335 | validation: 1.4040498759399944]
	TIME [epoch: 17.6 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6928744307846344		[learning rate: 0.019225]
	Learning Rate: 0.0192249
	LOSS [training: 1.6928744307846344 | validation: 1.6147744402649293]
	TIME [epoch: 17.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.712691790399861		[learning rate: 0.019218]
	Learning Rate: 0.0192185
	LOSS [training: 1.712691790399861 | validation: 1.2712010899300938]
	TIME [epoch: 17.6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6205065868958926		[learning rate: 0.019212]
	Learning Rate: 0.0192121
	LOSS [training: 1.6205065868958926 | validation: 1.2207871302977247]
	TIME [epoch: 17.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6311653650522173		[learning rate: 0.019206]
	Learning Rate: 0.0192056
	LOSS [training: 1.6311653650522173 | validation: 1.0999382498742065]
	TIME [epoch: 17.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.492808097945403		[learning rate: 0.019199]
	Learning Rate: 0.0191992
	LOSS [training: 1.492808097945403 | validation: 1.3028911993080814]
	TIME [epoch: 17.6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5604898782620853		[learning rate: 0.019193]
	Learning Rate: 0.0191927
	LOSS [training: 1.5604898782620853 | validation: 1.0392032152518276]
	TIME [epoch: 17.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4684100780963283		[learning rate: 0.019186]
	Learning Rate: 0.0191861
	LOSS [training: 1.4684100780963283 | validation: 1.0534222957908006]
	TIME [epoch: 17.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4188307460046852		[learning rate: 0.01918]
	Learning Rate: 0.0191796
	LOSS [training: 1.4188307460046852 | validation: 0.9664212401892047]
	TIME [epoch: 17.6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3233176400971747		[learning rate: 0.019173]
	Learning Rate: 0.019173
	LOSS [training: 1.3233176400971747 | validation: 0.9529300363866033]
	TIME [epoch: 17.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3078942968370513		[learning rate: 0.019166]
	Learning Rate: 0.0191664
	LOSS [training: 1.3078942968370513 | validation: 1.3795966871268506]
	TIME [epoch: 17.6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3924744035794157		[learning rate: 0.01916]
	Learning Rate: 0.0191598
	LOSS [training: 1.3924744035794157 | validation: 1.0804677854697997]
	TIME [epoch: 17.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.330348388338092		[learning rate: 0.019153]
	Learning Rate: 0.0191532
	LOSS [training: 1.330348388338092 | validation: 1.1126189579076198]
	TIME [epoch: 17.6 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2666431303252508		[learning rate: 0.019146]
	Learning Rate: 0.0191465
	LOSS [training: 1.2666431303252508 | validation: 1.8227228909823574]
	TIME [epoch: 17.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6325413337074575		[learning rate: 0.01914]
	Learning Rate: 0.0191398
	LOSS [training: 1.6325413337074575 | validation: 1.077322502393265]
	TIME [epoch: 17.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1643331335151967		[learning rate: 0.019133]
	Learning Rate: 0.0191331
	LOSS [training: 1.1643331335151967 | validation: 1.1881877842214157]
	TIME [epoch: 17.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1815223189513093		[learning rate: 0.019126]
	Learning Rate: 0.0191263
	LOSS [training: 1.1815223189513093 | validation: 0.9903758790379419]
	TIME [epoch: 17.6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1602281984258973		[learning rate: 0.01912]
	Learning Rate: 0.0191196
	LOSS [training: 1.1602281984258973 | validation: 1.3739485927831168]
	TIME [epoch: 17.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1957777343339278		[learning rate: 0.019113]
	Learning Rate: 0.0191128
	LOSS [training: 1.1957777343339278 | validation: 0.940190962558313]
	TIME [epoch: 17.6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1996957792239795		[learning rate: 0.019106]
	Learning Rate: 0.019106
	LOSS [training: 1.1996957792239795 | validation: 0.9742341844477741]
	TIME [epoch: 17.6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2099288906294356		[learning rate: 0.019099]
	Learning Rate: 0.0190991
	LOSS [training: 1.2099288906294356 | validation: 0.9085150533351414]
	TIME [epoch: 17.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1618100061853802		[learning rate: 0.019092]
	Learning Rate: 0.0190922
	LOSS [training: 1.1618100061853802 | validation: 1.1909824534461153]
	TIME [epoch: 17.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6627363956980683		[learning rate: 0.019085]
	Learning Rate: 0.0190853
	LOSS [training: 1.6627363956980683 | validation: 2.075229785977961]
	TIME [epoch: 17.6 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.73822751319821		[learning rate: 0.019078]
	Learning Rate: 0.0190784
	LOSS [training: 1.73822751319821 | validation: 2.5347043690919713]
	TIME [epoch: 17.6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4097540804220285		[learning rate: 0.019071]
	Learning Rate: 0.0190715
	LOSS [training: 2.4097540804220285 | validation: 3.1584192053012066]
	TIME [epoch: 17.6 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.749714908028114		[learning rate: 0.019065]
	Learning Rate: 0.0190645
	LOSS [training: 2.749714908028114 | validation: 2.9218289118152176]
	TIME [epoch: 17.7 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5717155180648104		[learning rate: 0.019058]
	Learning Rate: 0.0190575
	LOSS [training: 2.5717155180648104 | validation: 2.5855294355899683]
	TIME [epoch: 17.6 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0657959156923296		[learning rate: 0.019051]
	Learning Rate: 0.0190505
	LOSS [training: 2.0657959156923296 | validation: 1.6365503675012307]
	TIME [epoch: 17.6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.516440630253265		[learning rate: 0.019043]
	Learning Rate: 0.0190435
	LOSS [training: 1.516440630253265 | validation: 2.510788129283464]
	TIME [epoch: 17.6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.105678871704468		[learning rate: 0.019036]
	Learning Rate: 0.0190364
	LOSS [training: 2.105678871704468 | validation: 3.0151504098329225]
	TIME [epoch: 17.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3497082088277006		[learning rate: 0.019029]
	Learning Rate: 0.0190293
	LOSS [training: 2.3497082088277006 | validation: 2.9176891739854884]
	TIME [epoch: 17.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9762448756621995		[learning rate: 0.019022]
	Learning Rate: 0.0190222
	LOSS [training: 1.9762448756621995 | validation: 1.9027863410380679]
	TIME [epoch: 17.6 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0456190037728694		[learning rate: 0.019015]
	Learning Rate: 0.019015
	LOSS [training: 2.0456190037728694 | validation: 2.292991427650573]
	TIME [epoch: 17.7 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533482925851672		[learning rate: 0.019008]
	Learning Rate: 0.0190079
	LOSS [training: 1.8533482925851672 | validation: 1.7190536938863008]
	TIME [epoch: 17.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6588229854348915		[learning rate: 0.019001]
	Learning Rate: 0.0190007
	LOSS [training: 1.6588229854348915 | validation: 2.1232570516814486]
	TIME [epoch: 17.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1657420820039		[learning rate: 0.018993]
	Learning Rate: 0.0189935
	LOSS [training: 2.1657420820039 | validation: 1.7971680446290428]
	TIME [epoch: 17.6 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.682648049503849		[learning rate: 0.018986]
	Learning Rate: 0.0189862
	LOSS [training: 1.682648049503849 | validation: 1.6041366010522795]
	TIME [epoch: 17.6 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.517266889305594		[learning rate: 0.018979]
	Learning Rate: 0.018979
	LOSS [training: 1.517266889305594 | validation: 1.471592318035396]
	TIME [epoch: 17.6 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4010971779238046		[learning rate: 0.018972]
	Learning Rate: 0.0189717
	LOSS [training: 1.4010971779238046 | validation: 1.3633147379479997]
	TIME [epoch: 17.6 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3134246103774128		[learning rate: 0.018964]
	Learning Rate: 0.0189644
	LOSS [training: 1.3134246103774128 | validation: 1.2934783509635852]
	TIME [epoch: 17.6 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2536553575641027		[learning rate: 0.018957]
	Learning Rate: 0.018957
	LOSS [training: 1.2536553575641027 | validation: 1.2454751764920315]
	TIME [epoch: 17.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.203529607656303		[learning rate: 0.01895]
	Learning Rate: 0.0189497
	LOSS [training: 1.203529607656303 | validation: 1.2461018437758198]
	TIME [epoch: 17.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1801215552181392		[learning rate: 0.018942]
	Learning Rate: 0.0189423
	LOSS [training: 1.1801215552181392 | validation: 1.2139615884481139]
	TIME [epoch: 17.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1611115618160741		[learning rate: 0.018935]
	Learning Rate: 0.0189349
	LOSS [training: 1.1611115618160741 | validation: 1.0533438478257904]
	TIME [epoch: 17.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1032096693337055		[learning rate: 0.018927]
	Learning Rate: 0.0189274
	LOSS [training: 1.1032096693337055 | validation: 1.048713911089889]
	TIME [epoch: 17.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0930144835384534		[learning rate: 0.01892]
	Learning Rate: 0.01892
	LOSS [training: 1.0930144835384534 | validation: 1.0492304962448924]
	TIME [epoch: 17.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1001188084758886		[learning rate: 0.018912]
	Learning Rate: 0.0189125
	LOSS [training: 1.1001188084758886 | validation: 1.0455874995409136]
	TIME [epoch: 17.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.085953376736025		[learning rate: 0.018905]
	Learning Rate: 0.018905
	LOSS [training: 1.085953376736025 | validation: 1.0911849403112002]
	TIME [epoch: 17.6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.09688555576391		[learning rate: 0.018897]
	Learning Rate: 0.0188974
	LOSS [training: 1.09688555576391 | validation: 1.0183574777352105]
	TIME [epoch: 17.6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0733793600167982		[learning rate: 0.01889]
	Learning Rate: 0.0188899
	LOSS [training: 1.0733793600167982 | validation: 1.0328744405675023]
	TIME [epoch: 17.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0879153773088759		[learning rate: 0.018882]
	Learning Rate: 0.0188823
	LOSS [training: 1.0879153773088759 | validation: 0.9895890852075998]
	TIME [epoch: 17.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.088236414264268		[learning rate: 0.018875]
	Learning Rate: 0.0188747
	LOSS [training: 1.088236414264268 | validation: 0.975653772589022]
	TIME [epoch: 17.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0761170113775689		[learning rate: 0.018867]
	Learning Rate: 0.0188671
	LOSS [training: 1.0761170113775689 | validation: 1.067872706132019]
	TIME [epoch: 17.6 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1039048768219373		[learning rate: 0.018859]
	Learning Rate: 0.0188594
	LOSS [training: 1.1039048768219373 | validation: 0.997389321445739]
	TIME [epoch: 17.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0626344893870665		[learning rate: 0.018852]
	Learning Rate: 0.0188517
	LOSS [training: 1.0626344893870665 | validation: 1.060269041751973]
	TIME [epoch: 17.6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0781704808089039		[learning rate: 0.018844]
	Learning Rate: 0.018844
	LOSS [training: 1.0781704808089039 | validation: 1.0500767398271187]
	TIME [epoch: 17.6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0581253564309159		[learning rate: 0.018836]
	Learning Rate: 0.0188363
	LOSS [training: 1.0581253564309159 | validation: 1.0025539085129913]
	TIME [epoch: 17.6 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0693996724969486		[learning rate: 0.018829]
	Learning Rate: 0.0188286
	LOSS [training: 1.0693996724969486 | validation: 1.0311455335631738]
	TIME [epoch: 17.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.075991192741447		[learning rate: 0.018821]
	Learning Rate: 0.0188208
	LOSS [training: 1.075991192741447 | validation: 1.0261880468388576]
	TIME [epoch: 17.7 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.072579138263801		[learning rate: 0.018813]
	Learning Rate: 0.018813
	LOSS [training: 1.072579138263801 | validation: 0.9543799648015253]
	TIME [epoch: 17.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.050286309972281		[learning rate: 0.018805]
	Learning Rate: 0.0188052
	LOSS [training: 1.050286309972281 | validation: 1.0022293531375797]
	TIME [epoch: 17.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0309902282656704		[learning rate: 0.018797]
	Learning Rate: 0.0187973
	LOSS [training: 1.0309902282656704 | validation: 1.0676861049019388]
	TIME [epoch: 17.6 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.053821470323566		[learning rate: 0.018789]
	Learning Rate: 0.0187894
	LOSS [training: 1.053821470323566 | validation: 1.01384975500719]
	TIME [epoch: 17.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0367037255766258		[learning rate: 0.018782]
	Learning Rate: 0.0187815
	LOSS [training: 1.0367037255766258 | validation: 1.0457574211670069]
	TIME [epoch: 17.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0507032463533106		[learning rate: 0.018774]
	Learning Rate: 0.0187736
	LOSS [training: 1.0507032463533106 | validation: 1.0281257600220701]
	TIME [epoch: 17.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0255523169943406		[learning rate: 0.018766]
	Learning Rate: 0.0187657
	LOSS [training: 1.0255523169943406 | validation: 1.0423953232228285]
	TIME [epoch: 17.6 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0019277015768369		[learning rate: 0.018758]
	Learning Rate: 0.0187577
	LOSS [training: 1.0019277015768369 | validation: 0.9944040904696279]
	TIME [epoch: 17.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0137680637755138		[learning rate: 0.01875]
	Learning Rate: 0.0187497
	LOSS [training: 1.0137680637755138 | validation: 1.0367773509116203]
	TIME [epoch: 17.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.061247050781356		[learning rate: 0.018742]
	Learning Rate: 0.0187417
	LOSS [training: 1.061247050781356 | validation: 0.9559055064497748]
	TIME [epoch: 17.7 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9651191430742747		[learning rate: 0.018734]
	Learning Rate: 0.0187337
	LOSS [training: 0.9651191430742747 | validation: 0.9671019387342681]
	TIME [epoch: 17.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9816716225600166		[learning rate: 0.018726]
	Learning Rate: 0.0187256
	LOSS [training: 0.9816716225600166 | validation: 1.128206576709704]
	TIME [epoch: 17.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2980206833890084		[learning rate: 0.018718]
	Learning Rate: 0.0187175
	LOSS [training: 1.2980206833890084 | validation: 2.203452272069152]
	TIME [epoch: 17.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.600450488639288		[learning rate: 0.018709]
	Learning Rate: 0.0187094
	LOSS [training: 2.600450488639288 | validation: 2.7064840121324796]
	TIME [epoch: 17.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.040781273643936		[learning rate: 0.018701]
	Learning Rate: 0.0187013
	LOSS [training: 3.040781273643936 | validation: 2.995244553018193]
	TIME [epoch: 17.6 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.12896597711021		[learning rate: 0.018693]
	Learning Rate: 0.0186931
	LOSS [training: 3.12896597711021 | validation: 2.9118250087620705]
	TIME [epoch: 17.6 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.58989495106242		[learning rate: 0.018685]
	Learning Rate: 0.0186849
	LOSS [training: 3.58989495106242 | validation: 4.042310567689224]
	TIME [epoch: 17.7 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6738089638385434		[learning rate: 0.018677]
	Learning Rate: 0.0186767
	LOSS [training: 3.6738089638385434 | validation: 3.1093597064096103]
	TIME [epoch: 17.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.068486839987416		[learning rate: 0.018668]
	Learning Rate: 0.0186685
	LOSS [training: 3.068486839987416 | validation: 2.9146200886120717]
	TIME [epoch: 17.7 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4342728164247145		[learning rate: 0.01866]
	Learning Rate: 0.0186602
	LOSS [training: 3.4342728164247145 | validation: 3.3208289363570844]
	TIME [epoch: 17.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5781912392752124		[learning rate: 0.018652]
	Learning Rate: 0.018652
	LOSS [training: 3.5781912392752124 | validation: 3.718852472239102]
	TIME [epoch: 17.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.026095158095887		[learning rate: 0.018644]
nan encountered in epoch 369 (validation loss).
	Learning Rate: 0.0186437
	LOSS [training: 4.026095158095887 | validation: nan]
	TIME [epoch: 17.7 sec]
EPOCH 370/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.00010000000000000003
		[batch 4/4] avg loss: 4.327063881910604		[learning rate: 0.018635]
	Learning Rate: 0.0186353
	LOSS [training: 4.327063881910604 | validation: 3.762904255562282]
	TIME [epoch: 136 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.65072531880026		[learning rate: 0.018627]
	Learning Rate: 0.018627
	LOSS [training: 3.65072531880026 | validation: 4.141337102508862]
	TIME [epoch: 17.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0686573147062255		[learning rate: 0.018619]
	Learning Rate: 0.0186186
	LOSS [training: 4.0686573147062255 | validation: 3.5237387780155753]
	TIME [epoch: 17.6 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819242068947288		[learning rate: 0.01861]
	Learning Rate: 0.0186102
	LOSS [training: 3.819242068947288 | validation: 3.7746054747412656]
	TIME [epoch: 17.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7411911285406143		[learning rate: 0.018602]
	Learning Rate: 0.0186018
	LOSS [training: 3.7411911285406143 | validation: 3.610378887084785]
	TIME [epoch: 17.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.565532076160544		[learning rate: 0.018593]
	Learning Rate: 0.0185934
	LOSS [training: 3.565532076160544 | validation: 4.034693318310072]
	TIME [epoch: 17.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6132348844175475		[learning rate: 0.018585]
	Learning Rate: 0.0185849
	LOSS [training: 3.6132348844175475 | validation: 3.685069804416939]
	TIME [epoch: 17.7 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.510853125207029		[learning rate: 0.018576]
	Learning Rate: 0.0185764
	LOSS [training: 3.510853125207029 | validation: 3.2779962624093777]
	TIME [epoch: 17.6 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3458167340707696		[learning rate: 0.018568]
	Learning Rate: 0.0185679
	LOSS [training: 3.3458167340707696 | validation: 3.365326360451035]
	TIME [epoch: 17.6 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5987343216106096		[learning rate: 0.018559]
	Learning Rate: 0.0185594
	LOSS [training: 3.5987343216106096 | validation: 3.704941965707368]
	TIME [epoch: 17.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.902727669720454		[learning rate: 0.018551]
	Learning Rate: 0.0185508
	LOSS [training: 3.902727669720454 | validation: 4.4354560631830475]
	TIME [epoch: 17.6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.646720120460643		[learning rate: 0.018542]
	Learning Rate: 0.0185422
	LOSS [training: 3.646720120460643 | validation: 3.996058812261192]
	TIME [epoch: 17.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.486633305230842		[learning rate: 0.018534]
	Learning Rate: 0.0185336
	LOSS [training: 3.486633305230842 | validation: 3.9733874389758714]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd5_20240704_142037/states/model_phi1_1a_v_mmd5_382.pth
Halted early. No improvement in validation loss for 250 epochs.
Finished training in 5622.611 seconds.
