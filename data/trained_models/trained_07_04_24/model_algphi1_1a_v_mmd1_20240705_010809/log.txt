Args:
Namespace(name='model_algphi1_1a_v_mmd1', outdir='out/model_training/model_algphi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='binary_choice', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3885988157

Training model...

Saving initial model state to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9912966491052346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9912966491052346 | validation: 4.051070693335857]
	TIME [epoch: 102 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 4.010118582504824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.010118582504824 | validation: 4.057168462259962]
	TIME [epoch: 4.3 sec]
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 4.012690220506545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.012690220506545 | validation: 4.0472992235739]
	TIME [epoch: 4.26 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 3.99526158899767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.99526158899767 | validation: 4.019686887646408]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 3.990091083763644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.990091083763644 | validation: 4.036177870030116]
	TIME [epoch: 4.22 sec]
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 4.008012294725757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008012294725757 | validation: 4.063111899465909]
	TIME [epoch: 4.22 sec]
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 4.031365517012388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.031365517012388 | validation: 4.078157699768745]
	TIME [epoch: 4.22 sec]
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 4.040789360477838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040789360477838 | validation: 4.0586102331467435]
	TIME [epoch: 4.22 sec]
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 4.01336225905915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01336225905915 | validation: 4.070424011193383]
	TIME [epoch: 4.22 sec]
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 4.0165568141690535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0165568141690535 | validation: 4.059169692530895]
	TIME [epoch: 4.21 sec]
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 4.017958413048064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.017958413048064 | validation: 4.070674713520836]
	TIME [epoch: 4.22 sec]
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 4.01968003144233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01968003144233 | validation: 4.064937215143293]
	TIME [epoch: 4.22 sec]
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9974622168313463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9974622168313463 | validation: 4.029755429460499]
	TIME [epoch: 4.22 sec]
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9799099455829436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9799099455829436 | validation: 4.00703977983813]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9403381017155206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9403381017155206 | validation: 3.987948874936631]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9215226870548725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9215226870548725 | validation: 3.9884123923267816]
	TIME [epoch: 4.23 sec]
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9340708361946124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9340708361946124 | validation: 3.9870192662153565]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 3.92449232559336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.92449232559336 | validation: 3.981183819088277]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 3.926720654282905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.926720654282905 | validation: 3.9749772029109733]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 3.903332631716393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.903332631716393 | validation: 3.9657370698666146]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9005794652384713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9005794652384713 | validation: 3.978308340815669]
	TIME [epoch: 4.21 sec]
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 3.915800951066992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.915800951066992 | validation: 3.9571224010287516]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 3.891355432277343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.891355432277343 | validation: 3.9372666322135563]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 3.877986792274877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.877986792274877 | validation: 3.940308277786285]
	TIME [epoch: 4.25 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8747914196313333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8747914196313333 | validation: 3.9212366823058353]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 3.85013206804006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.85013206804006 | validation: 3.9251019342958076]
	TIME [epoch: 4.21 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 3.853242400332695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.853242400332695 | validation: 3.9102181364422295]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8410619658138354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8410619658138354 | validation: 3.8861737720231195]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8232977173584755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8232977173584755 | validation: 3.8892939064618126]
	TIME [epoch: 4.22 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8247863810549476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8247863810549476 | validation: 3.8891218866158397]
	TIME [epoch: 4.22 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8183152767279633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8183152767279633 | validation: 3.8737104869864707]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7912041275951776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7912041275951776 | validation: 3.838034289493736]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7578430728465246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7578430728465246 | validation: 3.8295629552463053]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7511008042026046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7511008042026046 | validation: 3.820240027450861]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7462040753915007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7462040753915007 | validation: 3.8141998168253695]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 3.740687030052454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.740687030052454 | validation: 3.8363791046362334]
	TIME [epoch: 4.21 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 3.767646540555673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.767646540555673 | validation: 3.8378341658568456]
	TIME [epoch: 4.21 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 3.770352596406552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.770352596406552 | validation: 3.8433695980822646]
	TIME [epoch: 4.21 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 3.779024688270054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.779024688270054 | validation: 3.8463482574197116]
	TIME [epoch: 4.21 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7820055124466654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7820055124466654 | validation: 3.844368929496842]
	TIME [epoch: 4.21 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7792247272458783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7792247272458783 | validation: 3.8282915290317785]
	TIME [epoch: 4.21 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7524823037374055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7524823037374055 | validation: 3.804063392487464]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7386873171005965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7386873171005965 | validation: 3.7998231161406717]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7136244666166967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7136244666166967 | validation: 3.754686367918824]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6964067775466445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6964067775466445 | validation: 3.7741713148136076]
	TIME [epoch: 4.23 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6940233093741055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6940233093741055 | validation: 3.7475868849680705]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 3.659612957298165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.659612957298165 | validation: 3.738735535846197]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 3.655758832980995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.655758832980995 | validation: 3.724851483937349]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6422923664561204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6422923664561204 | validation: 3.705075022355058]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 3.629988523701148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.629988523701148 | validation: 3.704214944295618]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 4.0672195145828		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 4.0672195145828 | validation: 4.204094868746958]
	TIME [epoch: 104 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 4.062512558751712		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 4.062512558751712 | validation: 4.202523842169427]
	TIME [epoch: 8.35 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 4.051547879856975		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 4.051547879856975 | validation: 4.18564922663792]
	TIME [epoch: 8.24 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 4.034098236432828		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 4.034098236432828 | validation: 4.17266068546311]
	TIME [epoch: 8.26 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 4.022322003501044		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 4.022322003501044 | validation: 4.15225965220018]
	TIME [epoch: 8.27 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 4.004089259504341		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 4.004089259504341 | validation: 4.130817109933823]
	TIME [epoch: 8.24 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 3.982330693931193		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 3.982330693931193 | validation: 4.107009206365298]
	TIME [epoch: 8.24 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9621486359230342		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 3.9621486359230342 | validation: 4.083816146431313]
	TIME [epoch: 8.24 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9355700481206792		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 3.9355700481206792 | validation: 4.055910847485804]
	TIME [epoch: 8.24 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9083162096291533		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 3.9083162096291533 | validation: 4.022701319348416]
	TIME [epoch: 8.26 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8805926692703974		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 3.8805926692703974 | validation: 3.9947777151296346]
	TIME [epoch: 8.28 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 3.846827813147473		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 3.846827813147473 | validation: 3.951672330423899]
	TIME [epoch: 8.24 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8115012141069773		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 3.8115012141069773 | validation: 3.904099054921663]
	TIME [epoch: 8.24 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7673794625297		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 3.7673794625297 | validation: 3.8585867999804515]
	TIME [epoch: 8.24 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 3.723772197662606		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 3.723772197662606 | validation: 3.80166667405133]
	TIME [epoch: 8.24 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 3.670148533259433		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 3.670148533259433 | validation: 3.722262604644258]
	TIME [epoch: 8.27 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 3.597266882707445		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 3.597266882707445 | validation: 3.6230837921552217]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4808900359524966		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 3.4808900359524966 | validation: 3.3946028608871957]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2450363738388424		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 3.2450363738388424 | validation: 2.9575044310247627]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 2.807809348450455		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 2.807809348450455 | validation: 2.5217032655791054]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 2.476361577253495		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 2.476361577253495 | validation: 2.271200950592264]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 2.164857428728687		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 2.164857428728687 | validation: 1.9270699448714095]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 1.853689956049213		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 1.853689956049213 | validation: 1.6751669862349385]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6232958710918308		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 1.6232958710918308 | validation: 1.430927549866869]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4017257745669631		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 1.4017257745669631 | validation: 1.237680781935823]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1805380049533165		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 1.1805380049533165 | validation: 1.0285830685357773]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9875045400224549		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.9875045400224549 | validation: 0.8602973202600688]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.81869892612821		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.81869892612821 | validation: 0.7133862138805775]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.673023955140363		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.673023955140363 | validation: 0.5906546565469936]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5535976447205464		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.5535976447205464 | validation: 0.48540473336930096]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4342899378593739		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.4342899378593739 | validation: 0.3704097712495598]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3446455122446876		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.3446455122446876 | validation: 0.29272360461308894]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2687942831625199		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.2687942831625199 | validation: 0.22815100484468032]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21088444091527594		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.21088444091527594 | validation: 0.17380788672669675]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17055706488103775		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.17055706488103775 | validation: 0.139319711607508]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13830687736350183		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.13830687736350183 | validation: 0.11136675514286587]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11319264578551916		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.11319264578551916 | validation: 0.09332234848596738]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09203095658811376		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.09203095658811376 | validation: 0.07264375834941963]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07658639978820342		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.07658639978820342 | validation: 0.05804074004979426]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06388963839724233		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.06388963839724233 | validation: 0.049720180095464486]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05347983254855848		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.05347983254855848 | validation: 0.03911650549947999]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04304366291448681		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.04304366291448681 | validation: 0.03181086160298338]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03440956110293029		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.03440956110293029 | validation: 0.02367072330986905]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.027241907418319923		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.027241907418319923 | validation: 0.019202334416038733]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.020176782540626693		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.020176782540626693 | validation: 0.01593498278613588]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.016643648026802405		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.016643648026802405 | validation: 0.010208389634911372]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011871795297124517		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.011871795297124517 | validation: 0.009178559895584058]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008897891897233959		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.008897891897233959 | validation: 0.006316347261819373]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006610751429125235		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.006610751429125235 | validation: 0.005299613993450525]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004684272110345898		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.004684272110345898 | validation: 0.003564397329005501]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003392769571575311		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.003392769571575311 | validation: 0.003657687538641512]
	TIME [epoch: 115 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002200877556400787		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.002200877556400787 | validation: 0.0017186346138869464]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019768453006714275		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.0019768453006714275 | validation: 0.0014049662134765486]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012092169679185083		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.0012092169679185083 | validation: 0.0014926102478034328]
	TIME [epoch: 18.8 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008525349659357805		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.0008525349659357805 | validation: 0.0003794768673906015]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005047102853475536		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.0005047102853475536 | validation: 0.0008040511505372852]
	TIME [epoch: 18.7 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023112434241106584		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.00023112434241106584 | validation: 0.00018508478324253373]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020197172183838763		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.00020197172183838763 | validation: -8.413418301863285e-05]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: -4.8650685006168545e-05		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: -4.8650685006168545e-05 | validation: 0.00015865954092670088]
	TIME [epoch: 18.8 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: -7.546791865084882e-06		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: -7.546791865084882e-06 | validation: 1.0516929523087802e-05]
	TIME [epoch: 18.7 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00019980225575506693		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: -0.00019980225575506693 | validation: 7.764212428220497e-05]
	TIME [epoch: 18.8 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00025300329030534185		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: -0.00025300329030534185 | validation: -0.00029628846732410977]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: -5.4588227141349146e-05		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: -5.4588227141349146e-05 | validation: -0.0002730518116106415]
	TIME [epoch: 18.7 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001618506447862662		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: -0.0001618506447862662 | validation: -0.0003974890095292514]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013313158029475413		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: -0.00013313158029475413 | validation: -0.00018379780916069778]
	TIME [epoch: 18.7 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023549437339084168		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: -0.00023549437339084168 | validation: 0.0002505128864966651]
	TIME [epoch: 18.7 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00016556833018316808		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: -0.00016556833018316808 | validation: -0.00016109539845372023]
	TIME [epoch: 18.7 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: -7.545752823897577e-05		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: -7.545752823897577e-05 | validation: -0.0003481330352338068]
	TIME [epoch: 18.7 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000195796465405393		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: -0.000195796465405393 | validation: -0.00036824586134847693]
	TIME [epoch: 18.7 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002214287541185367		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: -0.0002214287541185367 | validation: -0.0003095377273673847]
	TIME [epoch: 18.7 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002234264548184648		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: -0.0002234264548184648 | validation: -0.00018533321531278535]
	TIME [epoch: 18.7 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002017700557442457		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: -0.0002017700557442457 | validation: 6.152142184131826e-05]
	TIME [epoch: 18.7 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001810704602064648		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: -0.0001810704602064648 | validation: 0.00013428833057033796]
	TIME [epoch: 18.7 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00015955904759466024		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: -0.00015955904759466024 | validation: -3.2145355880363276e-05]
	TIME [epoch: 18.7 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002764744981245326		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: -0.0002764744981245326 | validation: -3.5726927200299134e-05]
	TIME [epoch: 18.7 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003678710347611838		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: -0.0003678710347611838 | validation: -0.00044421998681296325]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003473050677417071		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: -0.0003473050677417071 | validation: -6.339530553940343e-05]
	TIME [epoch: 18.8 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023899195407116647		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: -0.00023899195407116647 | validation: -2.2092877879028807e-05]
	TIME [epoch: 18.7 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00020139070924649573		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: -0.00020139070924649573 | validation: -9.640144016657357e-06]
	TIME [epoch: 18.7 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00034768660930994667		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: -0.00034768660930994667 | validation: -7.072627513884821e-05]
	TIME [epoch: 18.7 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00022558573656847992		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: -0.00022558573656847992 | validation: 2.207141453645445e-05]
	TIME [epoch: 18.7 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003368498216378155		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: -0.0003368498216378155 | validation: 0.0001237627710572422]
	TIME [epoch: 18.7 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028244446430314603		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: -0.00028244446430314603 | validation: -0.00042965860073053804]
	TIME [epoch: 18.7 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00016075415531079896		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: -0.00016075415531079896 | validation: 2.0168905358396218e-05]
	TIME [epoch: 18.7 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00015808369568091774		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: -0.00015808369568091774 | validation: -0.00032023596124885144]
	TIME [epoch: 18.7 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021268264391924551		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: -0.00021268264391924551 | validation: -0.0004237600279731355]
	TIME [epoch: 18.7 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021270614999639628		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: -0.00021270614999639628 | validation: -0.0002055313177526861]
	TIME [epoch: 18.7 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00035480548790719753		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: -0.00035480548790719753 | validation: -9.902426894347282e-05]
	TIME [epoch: 18.7 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: -9.285244954516102e-05		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: -9.285244954516102e-05 | validation: -0.00028604910836274393]
	TIME [epoch: 18.7 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023255708079603486		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: -0.00023255708079603486 | validation: 0.00013642663336325622]
	TIME [epoch: 18.7 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002657604680363552		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: -0.0002657604680363552 | validation: -7.356473486050863e-05]
	TIME [epoch: 18.7 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002094488131433101		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: -0.0002094488131433101 | validation: -0.0003542261219341514]
	TIME [epoch: 18.7 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00011101597973210285		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: -0.00011101597973210285 | validation: 8.997373280908772e-05]
	TIME [epoch: 18.7 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00025955024035872776		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: -0.00025955024035872776 | validation: -0.00012125507765794907]
	TIME [epoch: 18.7 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00039529942619539327		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: -0.00039529942619539327 | validation: 9.799815916604438e-05]
	TIME [epoch: 18.7 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0004387581945023951		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: -0.0004387581945023951 | validation: -8.81366595028723e-05]
	TIME [epoch: 18.7 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: -9.714154097228712e-05		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: -9.714154097228712e-05 | validation: -0.00025911353083574797]
	TIME [epoch: 18.7 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00041987987719731957		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: -0.00041987987719731957 | validation: -0.0001653528338026682]
	TIME [epoch: 18.7 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00024127472907863702		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: -0.00024127472907863702 | validation: -0.00029318496535914206]
	TIME [epoch: 18.7 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003454331773936787		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: -0.0003454331773936787 | validation: -0.0003900835730306449]
	TIME [epoch: 18.7 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00032838143561126216		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: -0.00032838143561126216 | validation: -0.00025650155204721693]
	TIME [epoch: 18.7 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00020723338839105242		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: -0.00020723338839105242 | validation: -4.8268875672823294e-06]
	TIME [epoch: 18.7 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002835506795603753		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: -0.0002835506795603753 | validation: -0.0001508048881396551]
	TIME [epoch: 18.7 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001864588762614314		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: -0.0001864588762614314 | validation: 0.00011582564876402301]
	TIME [epoch: 18.7 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00042587386743253726		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: -0.00042587386743253726 | validation: -0.0004390959573782522]
	TIME [epoch: 18.7 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021655638366384423		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: -0.00021655638366384423 | validation: 6.702996181881619e-06]
	TIME [epoch: 18.7 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00025269673120505013		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: -0.00025269673120505013 | validation: 4.4410930229170186e-05]
	TIME [epoch: 18.7 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021425879906297743		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: -0.00021425879906297743 | validation: -0.0004233906246522601]
	TIME [epoch: 18.7 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: -6.959005971894317e-05		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: -6.959005971894317e-05 | validation: -0.000513120878078622]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00020111665075532326		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: -0.00020111665075532326 | validation: -0.0003154599512046898]
	TIME [epoch: 18.7 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003180768794618269		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: -0.0003180768794618269 | validation: -0.0002580269058873563]
	TIME [epoch: 18.7 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002594800290869117		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: -0.0002594800290869117 | validation: -5.4669599395259285e-05]
	TIME [epoch: 18.6 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023454485386028057		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: -0.00023454485386028057 | validation: -0.0002366992532997321]
	TIME [epoch: 18.7 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013511380911983205		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: -0.00013511380911983205 | validation: 0.0005186754138635158]
	TIME [epoch: 18.6 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002532812835286986		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: -0.0002532812835286986 | validation: -0.0002968599356486212]
	TIME [epoch: 18.7 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00014149999005520943		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: -0.00014149999005520943 | validation: 1.734228698861084e-05]
	TIME [epoch: 18.7 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00019774490589410498		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: -0.00019774490589410498 | validation: -0.0002453226569515614]
	TIME [epoch: 18.7 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00024544576763984937		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: -0.00024544576763984937 | validation: 7.300122865317653e-08]
	TIME [epoch: 18.7 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028050208498724415		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: -0.00028050208498724415 | validation: -7.843675341571643e-05]
	TIME [epoch: 18.7 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00039217711858934305		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: -0.00039217711858934305 | validation: -0.00024358800019295756]
	TIME [epoch: 18.7 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002585854112287636		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: -0.0002585854112287636 | validation: -0.00032906972973677814]
	TIME [epoch: 18.7 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00015593955751613776		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: -0.00015593955751613776 | validation: -0.0001706853514147788]
	TIME [epoch: 18.7 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003276452364680342		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: -0.0003276452364680342 | validation: 2.1943106924252834e-05]
	TIME [epoch: 18.7 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021656360735179605		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: -0.00021656360735179605 | validation: -0.0004613392273633679]
	TIME [epoch: 18.7 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00011252445544817792		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: -0.00011252445544817792 | validation: -0.0003782876144653775]
	TIME [epoch: 18.7 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000309297132966917		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: -0.000309297132966917 | validation: -0.00030795427209708937]
	TIME [epoch: 18.7 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028308691308105894		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: -0.00028308691308105894 | validation: -0.00013247066434553823]
	TIME [epoch: 18.7 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0004433421782112055		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: -0.0004433421782112055 | validation: -0.00020141669052995502]
	TIME [epoch: 18.7 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00026747118887270906		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: -0.00026747118887270906 | validation: -0.0003149265075920429]
	TIME [epoch: 18.7 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00024428627121957525		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: -0.00024428627121957525 | validation: 0.00033473878601571454]
	TIME [epoch: 18.7 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001406711254021795		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: -0.0001406711254021795 | validation: -0.00033835613462656423]
	TIME [epoch: 18.7 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00037988530376153974		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: -0.00037988530376153974 | validation: 5.101188945113755e-05]
	TIME [epoch: 18.7 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000175039439089135		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: -0.000175039439089135 | validation: -0.00037605149351027656]
	TIME [epoch: 18.7 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00022246868551849605		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: -0.00022246868551849605 | validation: 0.00017455305911986494]
	TIME [epoch: 18.7 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003501103033762221		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: -0.0003501103033762221 | validation: -5.4475835619117056e-05]
	TIME [epoch: 18.7 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013671349833581026		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: -0.00013671349833581026 | validation: -0.0004221715864943167]
	TIME [epoch: 18.7 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00038385609746238063		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: -0.00038385609746238063 | validation: -0.00032415019763260846]
	TIME [epoch: 18.7 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00018901016609626953		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: -0.00018901016609626953 | validation: -9.56446062644769e-05]
	TIME [epoch: 18.7 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001989423952667728		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: -0.0001989423952667728 | validation: -0.0005929334967214812]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00017071940640958516		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: -0.00017071940640958516 | validation: -0.00042277063324423823]
	TIME [epoch: 18.7 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001261651602615559		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: -0.0001261651602615559 | validation: -0.0005308460939506486]
	TIME [epoch: 18.7 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003550366739043294		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: -0.0003550366739043294 | validation: -9.432095184824263e-05]
	TIME [epoch: 18.7 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028295959441774923		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: -0.00028295959441774923 | validation: -0.0003735827596469683]
	TIME [epoch: 18.7 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00022506696513469306		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: -0.00022506696513469306 | validation: -0.00014404061848816686]
	TIME [epoch: 18.7 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003866194320146383		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: -0.0003866194320146383 | validation: -0.00039154798968690584]
	TIME [epoch: 18.7 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002880867379490397		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: -0.0002880867379490397 | validation: -4.940144129473722e-05]
	TIME [epoch: 18.7 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00034631729095306054		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: -0.00034631729095306054 | validation: -0.00022239438936412536]
	TIME [epoch: 18.7 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002913841612449364		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: -0.0002913841612449364 | validation: -8.457532987960993e-05]
	TIME [epoch: 18.6 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002892113194974766		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: -0.0002892113194974766 | validation: 5.042538977718003e-06]
	TIME [epoch: 18.7 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00029976714687171113		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: -0.00029976714687171113 | validation: -0.00022192132836160638]
	TIME [epoch: 18.7 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002048088233744858		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: -0.0002048088233744858 | validation: -6.50895626790362e-05]
	TIME [epoch: 18.7 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: -4.732631336016602e-05		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: -4.732631336016602e-05 | validation: -0.0004892017695021176]
	TIME [epoch: 18.7 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00022460296993669207		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: -0.00022460296993669207 | validation: -0.0001820457977589438]
	TIME [epoch: 18.7 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002796484028863231		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: -0.0002796484028863231 | validation: -0.00019184581406236044]
	TIME [epoch: 18.7 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00043735262198823914		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: -0.00043735262198823914 | validation: -3.956647842155546e-06]
	TIME [epoch: 18.7 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000257384898311811		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: -0.000257384898311811 | validation: -0.00013725887376323962]
	TIME [epoch: 18.7 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000345546688229623		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: -0.000345546688229623 | validation: -5.322581406037319e-05]
	TIME [epoch: 18.7 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00019475402839105584		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: -0.00019475402839105584 | validation: -0.000208415714064496]
	TIME [epoch: 18.7 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00034075112977252853		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: -0.00034075112977252853 | validation: -2.695359583644841e-05]
	TIME [epoch: 18.7 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00017444603032690286		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: -0.00017444603032690286 | validation: -0.00037508019900214595]
	TIME [epoch: 18.7 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002888605982491004		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: -0.0002888605982491004 | validation: -0.00019921316561765325]
	TIME [epoch: 18.7 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002040581247364306		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: -0.0002040581247364306 | validation: -0.00039146553574248436]
	TIME [epoch: 18.7 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003422321862630602		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: -0.0003422321862630602 | validation: 2.6103538527952887e-05]
	TIME [epoch: 18.7 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002502623996267797		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: -0.0002502623996267797 | validation: -0.0004721157513352638]
	TIME [epoch: 18.7 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021857128039719662		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: -0.00021857128039719662 | validation: -0.00033046374615507633]
	TIME [epoch: 18.7 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00021837846700640818		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: -0.00021837846700640818 | validation: -0.0003911113550969168]
	TIME [epoch: 18.7 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00031657373084347483		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: -0.00031657373084347483 | validation: -4.722313402095502e-05]
	TIME [epoch: 18.7 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001833557272275912		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: -0.0001833557272275912 | validation: 0.0002970386648212013]
	TIME [epoch: 18.8 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003273893792927181		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: -0.0003273893792927181 | validation: 9.138555047398267e-05]
	TIME [epoch: 18.7 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003157893753081862		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: -0.0003157893753081862 | validation: 3.8964636302300646e-05]
	TIME [epoch: 18.7 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00020915462222639803		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: -0.00020915462222639803 | validation: -0.00024651764952847357]
	TIME [epoch: 18.7 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001234943642005153		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: -0.0001234943642005153 | validation: -6.791613274914443e-05]
	TIME [epoch: 18.7 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00024664941795878167		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: -0.00024664941795878167 | validation: 5.908939081715925e-05]
	TIME [epoch: 18.7 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00014263426953860803		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: -0.00014263426953860803 | validation: -0.00020196216283520217]
	TIME [epoch: 18.7 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003274006336549533		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: -0.0003274006336549533 | validation: -0.0002723110967826479]
	TIME [epoch: 18.7 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023593930409263254		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: -0.00023593930409263254 | validation: -0.00022946520993427378]
	TIME [epoch: 18.7 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003821833064499784		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: -0.0003821833064499784 | validation: -0.00024220620954040582]
	TIME [epoch: 18.7 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0004140455804053736		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: -0.0004140455804053736 | validation: -0.00044317602375684294]
	TIME [epoch: 18.7 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000437632273086451		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: -0.000437632273086451 | validation: -0.00047152285234566545]
	TIME [epoch: 18.7 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00032171286813527985		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: -0.00032171286813527985 | validation: -0.0003820465034246876]
	TIME [epoch: 18.7 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00025028904444633615		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: -0.00025028904444633615 | validation: -3.800237249969296e-05]
	TIME [epoch: 18.7 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00024598490486316105		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: -0.00024598490486316105 | validation: -0.0001104345678363261]
	TIME [epoch: 18.8 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002374512752386122		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: -0.0002374512752386122 | validation: -0.0002850821165538573]
	TIME [epoch: 18.7 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002644900053472523		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: -0.0002644900053472523 | validation: -0.00023867007028745404]
	TIME [epoch: 18.7 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013072736737495806		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: -0.00013072736737495806 | validation: -0.0004239311358199638]
	TIME [epoch: 18.8 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00023026348537580967		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: -0.00023026348537580967 | validation: -4.103284683156262e-05]
	TIME [epoch: 18.7 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00032931427942766225		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: -0.00032931427942766225 | validation: 7.111326383971316e-05]
	TIME [epoch: 18.7 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00037400971679526585		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: -0.00037400971679526585 | validation: -0.00012866427963612194]
	TIME [epoch: 18.7 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00010361751837489662		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: -0.00010361751837489662 | validation: -0.00047126412020045017]
	TIME [epoch: 18.7 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003148826806278096		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: -0.0003148826806278096 | validation: -0.00026972909193890437]
	TIME [epoch: 18.7 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00035072815837363237		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: -0.00035072815837363237 | validation: -0.0002591497647265344]
	TIME [epoch: 18.7 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002339436240887758		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: -0.0002339436240887758 | validation: -1.4098494024914035e-05]
	TIME [epoch: 18.7 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003110544443190031		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: -0.0003110544443190031 | validation: -7.781409818966889e-05]
	TIME [epoch: 18.7 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0004229623939868359		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: -0.0004229623939868359 | validation: -0.0004006872807356317]
	TIME [epoch: 18.7 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: -5.7553692370843515e-05		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: -5.7553692370843515e-05 | validation: 3.095942426377363e-05]
	TIME [epoch: 18.8 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: -0.000355257495487489		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: -0.000355257495487489 | validation: 0.00010068679795080017]
	TIME [epoch: 18.8 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002457863090729886		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: -0.0002457863090729886 | validation: -0.00020594218777413164]
	TIME [epoch: 18.8 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003808747449827872		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: -0.0003808747449827872 | validation: -0.00025280543974965665]
	TIME [epoch: 18.8 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001776934378596935		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: -0.0001776934378596935 | validation: -0.00020006128156598545]
	TIME [epoch: 18.8 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002072379679497163		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: -0.0002072379679497163 | validation: -0.0004071457798604499]
	TIME [epoch: 18.7 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002497099995634964		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: -0.0002497099995634964 | validation: 1.2969617337807546e-05]
	TIME [epoch: 135 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00026861425444933375		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: -0.00026861425444933375 | validation: -0.0002840714420247208]
	TIME [epoch: 41.7 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00027498246463798906		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: -0.00027498246463798906 | validation: -0.0003603500291555335]
	TIME [epoch: 41.5 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001739758473027606		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: -0.0001739758473027606 | validation: -4.905758013539163e-05]
	TIME [epoch: 41.6 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003405732949202154		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: -0.0003405732949202154 | validation: -0.00013035479418461906]
	TIME [epoch: 41.7 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013785616449357875		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: -0.00013785616449357875 | validation: -0.000335088936215211]
	TIME [epoch: 41.6 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001630597266338507		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: -0.0001630597266338507 | validation: -8.330967570400462e-05]
	TIME [epoch: 41.6 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028859594337416584		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: -0.00028859594337416584 | validation: -0.0003685341806901672]
	TIME [epoch: 41.6 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028280028718833614		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: -0.00028280028718833614 | validation: -4.007213882721939e-05]
	TIME [epoch: 41.5 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001950033536359901		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: -0.0001950033536359901 | validation: -0.00032410977536597677]
	TIME [epoch: 41.5 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: -6.265081726538857e-05		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: -6.265081726538857e-05 | validation: -0.0003647051038124536]
	TIME [epoch: 41.6 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00013888931850834996		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: -0.00013888931850834996 | validation: -0.00023831757005369524]
	TIME [epoch: 41.5 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003129652535305283		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: -0.0003129652535305283 | validation: -0.00019330158320405476]
	TIME [epoch: 41.5 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003243103207683724		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: -0.0003243103207683724 | validation: 0.00017565471797676313]
	TIME [epoch: 41.5 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002726583497861725		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: -0.0002726583497861725 | validation: -0.0004288929081325623]
	TIME [epoch: 41.5 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00033593418506178314		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: -0.00033593418506178314 | validation: -0.0002709115015532335]
	TIME [epoch: 41.6 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00018153835645830306		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: -0.00018153835645830306 | validation: 7.601121002485822e-05]
	TIME [epoch: 41.6 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028863156533849723		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: -0.00028863156533849723 | validation: -0.00027896348434628586]
	TIME [epoch: 41.6 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00018197088011903852		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: -0.00018197088011903852 | validation: -0.0001500940972418041]
	TIME [epoch: 41.6 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003156280892542795		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: -0.0003156280892542795 | validation: -0.0003541138335414029]
	TIME [epoch: 41.6 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00030082375447522197		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: -0.00030082375447522197 | validation: -0.0003386896629556437]
	TIME [epoch: 41.6 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003586937260146868		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: -0.0003586937260146868 | validation: -1.8773236770496917e-05]
	TIME [epoch: 41.6 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00010925964310677427		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: -0.00010925964310677427 | validation: -0.0002640902170380759]
	TIME [epoch: 41.6 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002493090364003121		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: -0.0002493090364003121 | validation: -0.00019119386779812775]
	TIME [epoch: 41.6 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00025642187668270175		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: -0.00025642187668270175 | validation: -0.00022253767435737083]
	TIME [epoch: 41.5 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003764913586742884		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: -0.0003764913586742884 | validation: -0.00014897071500899093]
	TIME [epoch: 41.6 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00018713546608788633		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: -0.00018713546608788633 | validation: -0.00022239614784121998]
	TIME [epoch: 41.6 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001828880417105787		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: -0.0001828880417105787 | validation: -0.00028868815965141345]
	TIME [epoch: 41.6 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003783443403645772		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: -0.0003783443403645772 | validation: -9.30664866605122e-05]
	TIME [epoch: 41.6 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003471567265404267		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: -0.0003471567265404267 | validation: -0.00016432978495369976]
	TIME [epoch: 41.5 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003314421232917637		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: -0.0003314421232917637 | validation: -0.00029007581201535523]
	TIME [epoch: 41.6 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00036960296898114306		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: -0.00036960296898114306 | validation: -0.0001578022549699263]
	TIME [epoch: 41.6 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00028685371020952964		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: -0.00028685371020952964 | validation: -0.00028913124234523305]
	TIME [epoch: 41.6 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00022569067269894805		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: -0.00022569067269894805 | validation: -0.0002832083031846775]
	TIME [epoch: 41.6 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001430242153967678		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: -0.0001430242153967678 | validation: -0.00035138347075236085]
	TIME [epoch: 41.6 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003204426561044456		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: -0.0003204426561044456 | validation: -0.00013343102104594086]
	TIME [epoch: 41.6 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00031595133339041225		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: -0.00031595133339041225 | validation: 1.1843999267814987e-05]
	TIME [epoch: 41.5 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003681067610258033		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: -0.0003681067610258033 | validation: -0.0002540272275420348]
	TIME [epoch: 41.6 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003117059739571788		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: -0.0003117059739571788 | validation: -0.0004034934579749567]
	TIME [epoch: 41.5 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00043103685609443643		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: -0.00043103685609443643 | validation: -0.00014136563165920537]
	TIME [epoch: 41.6 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240705_010809/states/model_algphi1_1a_v_mmd1_290.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5533.098 seconds.
