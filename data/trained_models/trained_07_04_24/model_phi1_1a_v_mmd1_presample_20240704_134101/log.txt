Args:
Namespace(name='model_phi1_1a_v_mmd1_presample', outdir='out/model_training/model_phi1_1a_v_mmd1_presample', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=200, model_do_sample=False, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1185503597

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.152677824474265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.152677824474265 | validation: 4.784328224372723]
	TIME [epoch: 28.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814860288347047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.814860288347047 | validation: 3.7667275841822017]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810244787664054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.810244787664054 | validation: 3.877005782894516]
	TIME [epoch: 8.51 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5649995777318084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5649995777318084 | validation: 3.4692697236758874]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280730103827816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.280730103827816 | validation: 3.274176985314836]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2547153162698628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2547153162698628 | validation: 3.5292960939723175]
	TIME [epoch: 8.52 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.124342402260588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.124342402260588 | validation: 3.220427565809227]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9458058870595876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9458058870595876 | validation: 3.14813306978001]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9553757705534043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9553757705534043 | validation: 3.066551019191309]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8916045066904195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8916045066904195 | validation: 3.120465272499506]
	TIME [epoch: 8.5 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7549048425660327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7549048425660327 | validation: 3.0280020975029536]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7776009225103397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7776009225103397 | validation: 2.924916607036911]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.715122483194667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.715122483194667 | validation: 2.8486532283634314]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.601652144590611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.601652144590611 | validation: 2.833829051740337]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5334898892901596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5334898892901596 | validation: 2.7540415188089358]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4169635032579504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4169635032579504 | validation: 2.7938177242495983]
	TIME [epoch: 8.51 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5117355160487347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5117355160487347 | validation: 2.6058415987627814]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2933376300715578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2933376300715578 | validation: 2.490936382192897]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.228627812349769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.228627812349769 | validation: 2.4659891118196784]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2834840742065423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2834840742065423 | validation: 2.2936881336850607]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9835787958628872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9835787958628872 | validation: 2.2435481474790677]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8832081783497048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8832081783497048 | validation: 2.2913157931543546]
	TIME [epoch: 8.51 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9103925878569794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9103925878569794 | validation: 2.1214153369418023]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7469466964545943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7469466964545943 | validation: 1.9872748137944192]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8301697211099222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8301697211099222 | validation: 2.1954808413582305]
	TIME [epoch: 8.52 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.700258824097183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.700258824097183 | validation: 1.9533291720831318]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5574692925982663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5574692925982663 | validation: 1.850088719683884]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4834947095866908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4834947095866908 | validation: 1.8233496461863807]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8707156061667258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8707156061667258 | validation: 1.9440461916333875]
	TIME [epoch: 8.53 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5142229488437329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5142229488437329 | validation: 1.8337132244179477]
	TIME [epoch: 8.49 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4028657346807578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4028657346807578 | validation: 1.8109566883589645]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.433181415463519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.433181415463519 | validation: 2.075994252091636]
	TIME [epoch: 8.53 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6932184997888504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6932184997888504 | validation: 1.9548171842012412]
	TIME [epoch: 8.53 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3891068388382979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3891068388382979 | validation: 1.77992791459839]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3839623689853673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3839623689853673 | validation: 1.7815193919557784]
	TIME [epoch: 8.51 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3619202855742718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3619202855742718 | validation: 1.784036668478544]
	TIME [epoch: 8.52 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3260996339226343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3260996339226343 | validation: 1.780954995005429]
	TIME [epoch: 8.53 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4439760099766352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4439760099766352 | validation: 2.182597460596992]
	TIME [epoch: 8.51 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4780827918122292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4780827918122292 | validation: 1.8056194506266063]
	TIME [epoch: 8.52 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.265884393511523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.265884393511523 | validation: 1.7534708587898513]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2723998276127533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2723998276127533 | validation: 1.9753373351386565]
	TIME [epoch: 8.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.467847988413116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.467847988413116 | validation: 1.9507423434184705]
	TIME [epoch: 8.53 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4257498771836548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4257498771836548 | validation: 1.8376074889224312]
	TIME [epoch: 8.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3188344025892293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3188344025892293 | validation: 1.7661365364271298]
	TIME [epoch: 8.5 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2226940624392233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2226940624392233 | validation: 1.6997201090210237]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3772496001059924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3772496001059924 | validation: 1.981084271233813]
	TIME [epoch: 8.52 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3448518678077508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3448518678077508 | validation: 1.7963923555199774]
	TIME [epoch: 8.51 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3082085751904495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3082085751904495 | validation: 1.7657992163993192]
	TIME [epoch: 8.5 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2872818630893874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2872818630893874 | validation: 1.9065324180469827]
	TIME [epoch: 8.52 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.32507822074861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.32507822074861 | validation: 1.7768481660903332]
	TIME [epoch: 8.51 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2993586993518405		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.2993586993518405 | validation: 1.7961210449793665]
	TIME [epoch: 8.5 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.252882790545929		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.252882790545929 | validation: 1.694291135985041]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3788685685794062		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.3788685685794062 | validation: 1.80478841891704]
	TIME [epoch: 8.51 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.404584756404184		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.404584756404184 | validation: 1.7198385413194082]
	TIME [epoch: 8.51 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2788967631643144		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.2788967631643144 | validation: 1.6827577589217895]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2506768282342524		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.2506768282342524 | validation: 1.6873784401293648]
	TIME [epoch: 8.5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2667171350648394		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.2667171350648394 | validation: 1.6051231083903073]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2404238198352213		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.2404238198352213 | validation: 1.4750480066332134]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4123343513750355		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.4123343513750355 | validation: 1.4951683098586057]
	TIME [epoch: 8.52 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1156091542210036		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.1156091542210036 | validation: 1.8046416876946019]
	TIME [epoch: 8.51 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4027361523841735		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.4027361523841735 | validation: 1.024717642034928]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8219339511587884		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.8219339511587884 | validation: 0.9373095136323036]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7423789376674104		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.7423789376674104 | validation: 0.8678282150081271]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.731780917606217		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.731780917606217 | validation: 0.7489558533375886]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6056872208114437		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.6056872208114437 | validation: 0.8869080004181258]
	TIME [epoch: 8.5 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8611119811131688		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.8611119811131688 | validation: 0.8517707183001755]
	TIME [epoch: 8.51 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7354367072983363		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.7354367072983363 | validation: 0.6591720483837975]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6545203442386958		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.6545203442386958 | validation: 0.523477160524176]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6125173732096305		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.6125173732096305 | validation: 0.6648037167728794]
	TIME [epoch: 8.51 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6972454056730799		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.6972454056730799 | validation: 0.682738100427204]
	TIME [epoch: 8.53 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6373869348365939		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.6373869348365939 | validation: 0.7123395656310677]
	TIME [epoch: 8.52 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6277635248856708		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6277635248856708 | validation: 0.4990548390210793]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465060164189351		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5465060164189351 | validation: 0.48977504708772834]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5098949280748304		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5098949280748304 | validation: 0.774610045276686]
	TIME [epoch: 8.52 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6609951726710389		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6609951726710389 | validation: 0.4637460983525769]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4233053034086058		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.4233053034086058 | validation: 0.4217592385041731]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5729593693681292		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5729593693681292 | validation: 0.6119002427764286]
	TIME [epoch: 8.5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5331603515863098		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5331603515863098 | validation: 0.5150454015194436]
	TIME [epoch: 8.51 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40902123487342806		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.40902123487342806 | validation: 0.4113935169021449]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4414355047275315		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.4414355047275315 | validation: 0.4261416607839415]
	TIME [epoch: 8.49 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4026494561048946		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.4026494561048946 | validation: 0.4306105208321545]
	TIME [epoch: 8.47 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38050732566592904		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.38050732566592904 | validation: 0.8115349429495797]
	TIME [epoch: 8.5 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5722657769116313		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5722657769116313 | validation: 0.5612405414401167]
	TIME [epoch: 8.48 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4265614739536097		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.4265614739536097 | validation: 0.3485466399299527]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4548487415011799		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4548487415011799 | validation: 0.4819233901102629]
	TIME [epoch: 8.5 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5609466817851101		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5609466817851101 | validation: 0.45517141836399294]
	TIME [epoch: 8.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36253826957263774		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.36253826957263774 | validation: 0.3350789749817361]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.417049905844473		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.417049905844473 | validation: 0.3334962329019868]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4491075770335595		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.4491075770335595 | validation: 0.3333763640299544]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4548019794343495		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4548019794343495 | validation: 0.3309746782017249]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41014316089980335		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.41014316089980335 | validation: 0.371223539079643]
	TIME [epoch: 8.52 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3477457806713416		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.3477457806713416 | validation: 0.43494107204875126]
	TIME [epoch: 8.51 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4278740387736588		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.4278740387736588 | validation: 0.39444268232035806]
	TIME [epoch: 8.5 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37130904772121553		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.37130904772121553 | validation: 0.40363343534351936]
	TIME [epoch: 8.49 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39561635531278005		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.39561635531278005 | validation: 0.4227184757186259]
	TIME [epoch: 8.51 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35288531499040976		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.35288531499040976 | validation: 0.399727578970689]
	TIME [epoch: 8.51 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.314385734938475		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.314385734938475 | validation: 0.48084325907361536]
	TIME [epoch: 8.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38137939213774896		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.38137939213774896 | validation: 0.3162055684841425]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3549052237433731		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.3549052237433731 | validation: 0.4537756173146331]
	TIME [epoch: 8.5 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312356921139503		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3312356921139503 | validation: 0.2543165068202864]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33948661587752377		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.33948661587752377 | validation: 0.5010763841506399]
	TIME [epoch: 8.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4110552782773752		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4110552782773752 | validation: 0.32436138431290673]
	TIME [epoch: 8.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3704017972271425		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.3704017972271425 | validation: 0.3989555465376561]
	TIME [epoch: 8.51 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3274682403137848		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.3274682403137848 | validation: 0.34183764956013896]
	TIME [epoch: 8.52 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919053801860805		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2919053801860805 | validation: 0.6444377447991615]
	TIME [epoch: 8.51 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37887792893493594		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.37887792893493594 | validation: 0.3296521770609967]
	TIME [epoch: 8.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2675980837176983		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2675980837176983 | validation: 0.3397467181927446]
	TIME [epoch: 8.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4002583559413084		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.4002583559413084 | validation: 0.5701458874561126]
	TIME [epoch: 8.52 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3848049951170942		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3848049951170942 | validation: 0.4410763191546894]
	TIME [epoch: 8.51 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28265497438555826		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.28265497438555826 | validation: 0.23209116578008804]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34683094406334314		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.34683094406334314 | validation: 0.4060745105700966]
	TIME [epoch: 8.49 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3251969507171327		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.3251969507171327 | validation: 0.36742717077268106]
	TIME [epoch: 8.51 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36689362891182975		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.36689362891182975 | validation: 0.43325425279798796]
	TIME [epoch: 8.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3738742296237561		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.3738742296237561 | validation: 0.2470565545837578]
	TIME [epoch: 8.49 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21909414689539408		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.21909414689539408 | validation: 0.2648869366603092]
	TIME [epoch: 8.49 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26304625211739086		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.26304625211739086 | validation: 0.621811716225634]
	TIME [epoch: 8.51 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43112770525408534		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.43112770525408534 | validation: 0.41353562579178615]
	TIME [epoch: 8.49 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3253960320821616		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.3253960320821616 | validation: 0.36634777775157845]
	TIME [epoch: 8.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127113094571726		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.3127113094571726 | validation: 0.33725790047055926]
	TIME [epoch: 8.49 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3681351722513083		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.3681351722513083 | validation: 0.32241911610798535]
	TIME [epoch: 8.49 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2983834297429311		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2983834297429311 | validation: 0.270164166732139]
	TIME [epoch: 8.5 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608364752993836		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2608364752993836 | validation: 0.227776708489901]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23312605485318022		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.23312605485318022 | validation: 0.4346539334121545]
	TIME [epoch: 8.5 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818692259895118		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.2818692259895118 | validation: 0.22028773990349343]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24860444451707525		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.24860444451707525 | validation: 0.2812811958380915]
	TIME [epoch: 8.51 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29492814131628386		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.29492814131628386 | validation: 0.24368933770994844]
	TIME [epoch: 8.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23848730920600775		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.23848730920600775 | validation: 0.2735749980448412]
	TIME [epoch: 8.49 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24097364639767244		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.24097364639767244 | validation: 0.21295498058110474]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26523769598548824		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.26523769598548824 | validation: 0.3836851335484277]
	TIME [epoch: 8.51 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27512170945163406		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.27512170945163406 | validation: 0.21035835943958442]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21487738685425783		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.21487738685425783 | validation: 0.20805992193960446]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23925945432920484		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.23925945432920484 | validation: 0.28217426995621187]
	TIME [epoch: 8.51 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2306343718196481		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.2306343718196481 | validation: 0.3255878583412415]
	TIME [epoch: 8.51 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2177500035996333		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.2177500035996333 | validation: 0.16768304617225882]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23436180836939874		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.23436180836939874 | validation: 0.2594489344815002]
	TIME [epoch: 8.49 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25731638055082473		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.25731638055082473 | validation: 0.27705358917226164]
	TIME [epoch: 8.5 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3211880033450937		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.3211880033450937 | validation: 0.24732197014972246]
	TIME [epoch: 8.53 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20292024699465688		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.20292024699465688 | validation: 0.19523023661877287]
	TIME [epoch: 8.52 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18076460956263934		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.18076460956263934 | validation: 0.17591994292551708]
	TIME [epoch: 8.51 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21106694761328462		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.21106694761328462 | validation: 0.22715282491103556]
	TIME [epoch: 8.51 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17594291940436269		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.17594291940436269 | validation: 0.23925702455113518]
	TIME [epoch: 8.52 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15776186349267332		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.15776186349267332 | validation: 0.250948239373639]
	TIME [epoch: 8.53 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3114159524305922		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.3114159524305922 | validation: 0.21436539620566203]
	TIME [epoch: 8.51 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16138914014125239		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.16138914014125239 | validation: 0.20814606716803985]
	TIME [epoch: 8.51 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31096430226610094		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.31096430226610094 | validation: 0.3487170473153305]
	TIME [epoch: 8.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22187845350537516		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.22187845350537516 | validation: 0.15555363668692201]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14252524089611313		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.14252524089611313 | validation: 0.1381228977641823]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15603743354679517		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.15603743354679517 | validation: 0.26854482043340566]
	TIME [epoch: 8.5 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19057915946128928		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.19057915946128928 | validation: 0.20346160217580844]
	TIME [epoch: 8.49 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16687266623165		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.16687266623165 | validation: 0.19690388612006762]
	TIME [epoch: 8.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20839935484814429		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.20839935484814429 | validation: 0.2115518429026469]
	TIME [epoch: 8.52 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17776252176807664		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.17776252176807664 | validation: 0.11934479175343891]
	TIME [epoch: 8.48 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20967513690956327		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.20967513690956327 | validation: 0.23000742526134463]
	TIME [epoch: 8.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19634591955261144		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.19634591955261144 | validation: 0.14415810204152035]
	TIME [epoch: 8.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1611089806101869		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.1611089806101869 | validation: 0.12319995090250338]
	TIME [epoch: 8.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18691361478577784		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.18691361478577784 | validation: 0.2451982855104325]
	TIME [epoch: 8.5 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22091229964423487		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.22091229964423487 | validation: 0.2530646343519585]
	TIME [epoch: 8.49 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17414544393083867		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.17414544393083867 | validation: 0.10336031559615047]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15266215641404307		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.15266215641404307 | validation: 0.15632096453903366]
	TIME [epoch: 8.51 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1393974828590044		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.1393974828590044 | validation: 0.10902732247237562]
	TIME [epoch: 8.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1193120364355883		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.1193120364355883 | validation: 0.1284483105811916]
	TIME [epoch: 8.48 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17132798004097138		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.17132798004097138 | validation: 0.14998862607531674]
	TIME [epoch: 8.49 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1824731250606764		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.1824731250606764 | validation: 0.18594726979226278]
	TIME [epoch: 8.51 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19108420211728122		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.19108420211728122 | validation: 0.10494213922851552]
	TIME [epoch: 8.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1474258898757394		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.1474258898757394 | validation: 0.10506084228019938]
	TIME [epoch: 8.49 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1714176011114865		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.1714176011114865 | validation: 0.14059136848688988]
	TIME [epoch: 8.49 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13971688944479616		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.13971688944479616 | validation: 0.15160402742383494]
	TIME [epoch: 8.5 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16972361603621214		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.16972361603621214 | validation: 0.2532146862851714]
	TIME [epoch: 8.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1816070322727149		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.1816070322727149 | validation: 0.21731491423385518]
	TIME [epoch: 8.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1715972059033711		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1715972059033711 | validation: 0.20709983462442993]
	TIME [epoch: 8.5 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20006667818797047		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.20006667818797047 | validation: 0.14000660011672245]
	TIME [epoch: 8.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1310277097141571		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.1310277097141571 | validation: 0.09956689539722334]
	TIME [epoch: 8.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15539830843509891		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.15539830843509891 | validation: 0.1605091075268299]
	TIME [epoch: 8.5 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14654655623344542		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.14654655623344542 | validation: 0.09790122223725836]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10329802969268384		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.10329802969268384 | validation: 0.15748795454230136]
	TIME [epoch: 8.49 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1575811277250878		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.1575811277250878 | validation: 0.13556369899669204]
	TIME [epoch: 8.51 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14436685327853		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.14436685327853 | validation: 0.0908331351198477]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09657625078893743		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.09657625078893743 | validation: 0.07304760823035479]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18396499681682127		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.18396499681682127 | validation: 0.24045612125957527]
	TIME [epoch: 8.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17155749610930535		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.17155749610930535 | validation: 0.09039684339806606]
	TIME [epoch: 8.51 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12712235158844964		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.12712235158844964 | validation: 0.09142540726995302]
	TIME [epoch: 8.51 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11493313492716968		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.11493313492716968 | validation: 0.09532727260725041]
	TIME [epoch: 8.51 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10211792294744433		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.10211792294744433 | validation: 0.07648021935696568]
	TIME [epoch: 8.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11272866933414075		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.11272866933414075 | validation: 0.13740149340372548]
	TIME [epoch: 8.51 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1456480090041838		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.1456480090041838 | validation: 0.17141977648125403]
	TIME [epoch: 8.51 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12887879444371944		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.12887879444371944 | validation: 0.09034029981720412]
	TIME [epoch: 8.52 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11374998904742128		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.11374998904742128 | validation: 0.08958990859962579]
	TIME [epoch: 8.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0946541074966758		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.0946541074966758 | validation: 0.24393815826608223]
	TIME [epoch: 8.51 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14511005159118068		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.14511005159118068 | validation: 0.10011705561383397]
	TIME [epoch: 8.52 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13310106535257274		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.13310106535257274 | validation: 0.17264703913748713]
	TIME [epoch: 8.52 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12239459549716278		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.12239459549716278 | validation: 0.14732611204630397]
	TIME [epoch: 8.51 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09840580526811045		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.09840580526811045 | validation: 0.08556491661499202]
	TIME [epoch: 8.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1359639157830526		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.1359639157830526 | validation: 0.08439630890931105]
	TIME [epoch: 8.52 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10437212883342506		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.10437212883342506 | validation: 0.15192208779450786]
	TIME [epoch: 8.51 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18358395190649224		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.18358395190649224 | validation: 0.2550541649796979]
	TIME [epoch: 8.51 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1691432076023407		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1691432076023407 | validation: 0.09699013164633206]
	TIME [epoch: 8.51 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11585651892821988		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.11585651892821988 | validation: 0.19974063933634983]
	TIME [epoch: 8.52 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13614892516517965		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.13614892516517965 | validation: 0.07529345955022587]
	TIME [epoch: 8.51 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08134510402605588		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.08134510402605588 | validation: 0.06609117427735348]
	TIME [epoch: 8.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0911551180281705		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.0911551180281705 | validation: 0.1062366417763386]
	TIME [epoch: 8.51 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12637866470266687		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.12637866470266687 | validation: 0.08052340870591906]
	TIME [epoch: 32.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06750604445789496		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.06750604445789496 | validation: 0.07229489366253795]
	TIME [epoch: 16 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11785652115861704		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.11785652115861704 | validation: 0.0650708410302091]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07911855230301511		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.07911855230301511 | validation: 0.16518066426904837]
	TIME [epoch: 15.9 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17088231673274998		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.17088231673274998 | validation: 0.18999572515159355]
	TIME [epoch: 16 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13796064412694134		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.13796064412694134 | validation: 0.05556862015972984]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07359398063205913		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.07359398063205913 | validation: 0.0794347644507103]
	TIME [epoch: 16 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08985146553343029		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.08985146553343029 | validation: 0.11992927884739463]
	TIME [epoch: 15.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11939806271347697		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.11939806271347697 | validation: 0.0795581126991225]
	TIME [epoch: 15.9 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09220178719046448		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.09220178719046448 | validation: 0.05967987864180421]
	TIME [epoch: 15.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07797942207895721		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.07797942207895721 | validation: 0.08489508792278673]
	TIME [epoch: 15.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06526512623065947		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.06526512623065947 | validation: 0.08197530646766235]
	TIME [epoch: 15.9 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10887561128256054		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.10887561128256054 | validation: 0.10230206641180861]
	TIME [epoch: 15.9 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10839502042303942		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.10839502042303942 | validation: 0.06002783672227254]
	TIME [epoch: 15.9 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06540306837470924		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.06540306837470924 | validation: 0.05532419343958316]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07683226956834		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.07683226956834 | validation: 0.07091295254326792]
	TIME [epoch: 15.9 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06518123210164195		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.06518123210164195 | validation: 0.08323127760379284]
	TIME [epoch: 15.9 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.155816715224875		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.155816715224875 | validation: 0.07330071119903088]
	TIME [epoch: 15.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10608919466339034		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.10608919466339034 | validation: 0.07852621211444313]
	TIME [epoch: 15.9 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07365413966469622		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.07365413966469622 | validation: 0.07914086266820211]
	TIME [epoch: 15.9 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05550705570282269		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.05550705570282269 | validation: 0.0753103813744387]
	TIME [epoch: 15.9 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0753398687322398		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.0753398687322398 | validation: 0.05017487139052265]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08584435078354652		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.08584435078354652 | validation: 0.0720004493181251]
	TIME [epoch: 15.9 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06963358684721097		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.06963358684721097 | validation: 0.08175062334078227]
	TIME [epoch: 15.9 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06662983864188696		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.06662983864188696 | validation: 0.09079806713529695]
	TIME [epoch: 15.9 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09475934006988421		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.09475934006988421 | validation: 0.06489931139118679]
	TIME [epoch: 15.9 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05211677774886066		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.05211677774886066 | validation: 0.04002647528025864]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15737992947073495		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.15737992947073495 | validation: 0.20767272229918177]
	TIME [epoch: 15.9 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14688784349679473		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.14688784349679473 | validation: 0.050609818042304446]
	TIME [epoch: 15.9 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051743338530940305		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.051743338530940305 | validation: 0.046533865423348864]
	TIME [epoch: 15.9 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07333052039381538		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.07333052039381538 | validation: 0.07047223720568398]
	TIME [epoch: 15.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10642089016500977		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.10642089016500977 | validation: 0.0730137080572459]
	TIME [epoch: 15.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06716369267399178		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.06716369267399178 | validation: 0.043546820558465524]
	TIME [epoch: 15.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04545075258732334		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.04545075258732334 | validation: 0.08918507587546605]
	TIME [epoch: 15.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06642641129917987		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.06642641129917987 | validation: 0.04913198401064322]
	TIME [epoch: 15.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04628939301312715		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.04628939301312715 | validation: 0.14090491605746935]
	TIME [epoch: 15.9 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11613865617186613		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.11613865617186613 | validation: 0.03688869356501784]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06097422713240809		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.06097422713240809 | validation: 0.06965506880795902]
	TIME [epoch: 15.9 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055498653731069034		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.055498653731069034 | validation: 0.04944134593087783]
	TIME [epoch: 15.9 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06292708009444582		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.06292708009444582 | validation: 0.07072750047751739]
	TIME [epoch: 15.9 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0846527520702354		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.0846527520702354 | validation: 0.08266637502597166]
	TIME [epoch: 15.9 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062457950892746014		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.062457950892746014 | validation: 0.045269995470121636]
	TIME [epoch: 15.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06320496539043853		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.06320496539043853 | validation: 0.04151129172109394]
	TIME [epoch: 15.9 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05459402789770599		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.05459402789770599 | validation: 0.036588139275112945]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053184464482392314		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.053184464482392314 | validation: 0.057719958367211985]
	TIME [epoch: 15.9 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04902741606433788		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.04902741606433788 | validation: 0.03854184088698956]
	TIME [epoch: 15.9 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05618535405253179		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.05618535405253179 | validation: 0.05253376027962314]
	TIME [epoch: 15.9 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05842557385538795		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.05842557385538795 | validation: 0.04873672342382157]
	TIME [epoch: 15.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06696932351352355		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.06696932351352355 | validation: 0.044985678525380865]
	TIME [epoch: 15.9 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046824940665493545		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.046824940665493545 | validation: 0.12995843108185712]
	TIME [epoch: 15.9 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05953169641301985		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.05953169641301985 | validation: 0.038354069517273465]
	TIME [epoch: 15.9 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06227565852448197		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.06227565852448197 | validation: 0.04736462647335502]
	TIME [epoch: 15.9 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05540301206608065		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.05540301206608065 | validation: 0.033824675965790175]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05881099607988976		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.05881099607988976 | validation: 0.05995359416079131]
	TIME [epoch: 15.9 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04755983911352223		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.04755983911352223 | validation: 0.027803933941691405]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059384560661346134		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.059384560661346134 | validation: 0.0972108713763278]
	TIME [epoch: 15.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060708777205871		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.06060708777205871 | validation: 0.0339052921816197]
	TIME [epoch: 15.9 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03700360637035302		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.03700360637035302 | validation: 0.023465053403349034]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04395949780520031		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.04395949780520031 | validation: 0.04738010922411344]
	TIME [epoch: 16 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041757246412688166		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.041757246412688166 | validation: 0.028279752227564098]
	TIME [epoch: 15.9 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03271177695772841		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.03271177695772841 | validation: 0.06885615852257806]
	TIME [epoch: 16 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0952473590341856		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.0952473590341856 | validation: 0.08783912521594063]
	TIME [epoch: 16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640876857797216		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.06640876857797216 | validation: 0.04863991600593055]
	TIME [epoch: 15.9 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0398237686206569		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.0398237686206569 | validation: 0.05554902822693431]
	TIME [epoch: 15.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04107724980745425		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.04107724980745425 | validation: 0.031782478235574826]
	TIME [epoch: 16 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050409940327083666		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.050409940327083666 | validation: 0.032766409984260454]
	TIME [epoch: 16 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02602154381242762		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.02602154381242762 | validation: 0.039551577140139606]
	TIME [epoch: 15.9 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04073611034929911		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.04073611034929911 | validation: 0.05412897635762104]
	TIME [epoch: 16 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0734043477349852		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.0734043477349852 | validation: 0.0494992198090987]
	TIME [epoch: 15.9 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02852994594935222		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.02852994594935222 | validation: 0.04423980095949062]
	TIME [epoch: 16 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044189721778809816		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.044189721778809816 | validation: 0.02971077535813056]
	TIME [epoch: 15.9 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039269451893864056		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.039269451893864056 | validation: 0.07535658385622554]
	TIME [epoch: 16 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0529346122167451		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.0529346122167451 | validation: 0.024262570845607845]
	TIME [epoch: 16 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028205412236200114		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.028205412236200114 | validation: 0.04171111365037579]
	TIME [epoch: 15.9 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0416646789737361		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.0416646789737361 | validation: 0.02217311957337209]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0361001583248134		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.0361001583248134 | validation: 0.05355425123804877]
	TIME [epoch: 15.9 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05762122062844162		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.05762122062844162 | validation: 0.023090440210627984]
	TIME [epoch: 16 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023453807718786305		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.023453807718786305 | validation: 0.023993791329571808]
	TIME [epoch: 15.9 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059903172606688204		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.059903172606688204 | validation: 0.04558193043037799]
	TIME [epoch: 16 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04807054355903999		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.04807054355903999 | validation: 0.0472336965390049]
	TIME [epoch: 15.9 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025671705463391394		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.025671705463391394 | validation: 0.03278581855036638]
	TIME [epoch: 16 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044291233344505444		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.044291233344505444 | validation: 0.039967814062938326]
	TIME [epoch: 15.9 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022822295240111964		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.022822295240111964 | validation: 0.025874080692462914]
	TIME [epoch: 15.9 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048130056697244225		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.048130056697244225 | validation: 0.04253407094672783]
	TIME [epoch: 15.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027322341772213		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.027322341772213 | validation: 0.03300791780583909]
	TIME [epoch: 15.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04695713253696736		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.04695713253696736 | validation: 0.029414999550088288]
	TIME [epoch: 15.9 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031135768924606348		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.031135768924606348 | validation: 0.02107335610868189]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023557187788918243		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.023557187788918243 | validation: 0.10121338534391455]
	TIME [epoch: 15.9 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06471884771158057		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.06471884771158057 | validation: 0.04457529717202713]
	TIME [epoch: 15.9 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026464522421381156		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.026464522421381156 | validation: 0.019765065604689363]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035693742471812485		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.035693742471812485 | validation: 0.03846530931807643]
	TIME [epoch: 15.9 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030322547334202917		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.030322547334202917 | validation: 0.02238847367161236]
	TIME [epoch: 15.9 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0270476144437496		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.0270476144437496 | validation: 0.03195268516504238]
	TIME [epoch: 16 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05052467882344095		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.05052467882344095 | validation: 0.030357427976354292]
	TIME [epoch: 15.9 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02621246862851558		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.02621246862851558 | validation: 0.031829202085288597]
	TIME [epoch: 15.9 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03424611204400104		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.03424611204400104 | validation: 0.026298661799252526]
	TIME [epoch: 15.9 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030634683953489207		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.030634683953489207 | validation: 0.07040234884478726]
	TIME [epoch: 15.9 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04757308992621221		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.04757308992621221 | validation: 0.02802430457951719]
	TIME [epoch: 15.9 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039278041766891		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.039278041766891 | validation: 0.044836560823455496]
	TIME [epoch: 15.9 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031775944819883356		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.031775944819883356 | validation: 0.03515711196700189]
	TIME [epoch: 15.9 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02875872901571981		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.02875872901571981 | validation: 0.06250403817294847]
	TIME [epoch: 15.9 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04315310728379883		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.04315310728379883 | validation: 0.028809404261891977]
	TIME [epoch: 15.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024883504579856438		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.024883504579856438 | validation: 0.025154461688939003]
	TIME [epoch: 15.9 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024799247738262878		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.024799247738262878 | validation: 0.04330300628264737]
	TIME [epoch: 15.9 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04538381033283072		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.04538381033283072 | validation: 0.024006936866613347]
	TIME [epoch: 15.9 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02084936922884179		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.02084936922884179 | validation: 0.036574653657076364]
	TIME [epoch: 15.9 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032246327659732614		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.032246327659732614 | validation: 0.02168052303692762]
	TIME [epoch: 15.9 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017931977628782642		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.017931977628782642 | validation: 0.020995395821551032]
	TIME [epoch: 15.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03797594829411739		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03797594829411739 | validation: 0.016680893458811068]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027368886015602396		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.027368886015602396 | validation: 0.024357886757463518]
	TIME [epoch: 15.9 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041725404710249214		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.041725404710249214 | validation: 0.04591396262349602]
	TIME [epoch: 15.9 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02538218686110257		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.02538218686110257 | validation: 0.023647727733548965]
	TIME [epoch: 15.9 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027055627104754183		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.027055627104754183 | validation: 0.07387082081694188]
	TIME [epoch: 15.9 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04412196636620355		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.04412196636620355 | validation: 0.021457359958528273]
	TIME [epoch: 15.9 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023942210258484427		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.023942210258484427 | validation: 0.03231935225669703]
	TIME [epoch: 15.9 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024355877743352537		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.024355877743352537 | validation: 0.02093859697295774]
	TIME [epoch: 15.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03190324815271816		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.03190324815271816 | validation: 0.03987235866852636]
	TIME [epoch: 15.9 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024749519776914815		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.024749519776914815 | validation: 0.0165106786880383]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02349420322223964		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.02349420322223964 | validation: 0.04368432077453789]
	TIME [epoch: 15.9 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03934012915433349		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.03934012915433349 | validation: 0.022051504818195608]
	TIME [epoch: 15.9 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018817288179217077		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.018817288179217077 | validation: 0.034531223822921606]
	TIME [epoch: 15.9 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030672282310895815		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.030672282310895815 | validation: 0.021573398683606297]
	TIME [epoch: 15.9 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020572663546865714		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.020572663546865714 | validation: 0.0222420304283314]
	TIME [epoch: 15.9 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039783205150478666		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.039783205150478666 | validation: 0.0287695111481884]
	TIME [epoch: 15.9 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023193756999904558		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.023193756999904558 | validation: 0.025191682352150052]
	TIME [epoch: 15.9 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028971508584228006		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.028971508584228006 | validation: 0.02047962782682588]
	TIME [epoch: 15.9 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025981831677494845		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.025981831677494845 | validation: 0.015694211773530073]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023313491699517918		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.023313491699517918 | validation: 0.02815557480342867]
	TIME [epoch: 15.9 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02228615704593988		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.02228615704593988 | validation: 0.015566605398710975]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012992078726962484		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.012992078726962484 | validation: 0.014174295077412587]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030693355623429366		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.030693355623429366 | validation: 0.0976211636422879]
	TIME [epoch: 15.9 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037330049235229146		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.037330049235229146 | validation: 0.03134418366801471]
	TIME [epoch: 15.9 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03817793931175613		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.03817793931175613 | validation: 0.016050506189760984]
	TIME [epoch: 15.9 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01497191036855092		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.01497191036855092 | validation: 0.017193195986797843]
	TIME [epoch: 15.9 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018879649785610103		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.018879649785610103 | validation: 0.0172094061626168]
	TIME [epoch: 15.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03432637935511446		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.03432637935511446 | validation: 0.01576791063695962]
	TIME [epoch: 15.9 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01803531603244976		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.01803531603244976 | validation: 0.03806387312093224]
	TIME [epoch: 15.9 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024412990661831033		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.024412990661831033 | validation: 0.02951527900705543]
	TIME [epoch: 15.9 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027613369247169856		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.027613369247169856 | validation: 0.02225628407848173]
	TIME [epoch: 15.9 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017769828604864535		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.017769828604864535 | validation: 0.010892295851391664]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0187563338074082		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.0187563338074082 | validation: 0.051238228254677075]
	TIME [epoch: 15.9 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027134907867094667		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.027134907867094667 | validation: 0.01970383823110001]
	TIME [epoch: 15.9 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01829622418237612		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.01829622418237612 | validation: 0.02443755858303908]
	TIME [epoch: 15.9 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03490907057122999		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.03490907057122999 | validation: 0.019279363406717744]
	TIME [epoch: 15.9 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01944395400593791		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.01944395400593791 | validation: 0.016940543787837513]
	TIME [epoch: 15.9 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014275267007220711		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.014275267007220711 | validation: 0.030822879944739492]
	TIME [epoch: 15.9 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033687788023580086		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.033687788023580086 | validation: 0.03764956066091641]
	TIME [epoch: 15.9 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020904727379713307		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.020904727379713307 | validation: 0.01740168661154531]
	TIME [epoch: 15.9 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02134711306191084		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.02134711306191084 | validation: 0.029407259657244944]
	TIME [epoch: 15.9 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023078574267450396		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.023078574267450396 | validation: 0.01644402218557312]
	TIME [epoch: 15.9 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014267940084637682		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.014267940084637682 | validation: 0.02438602486588455]
	TIME [epoch: 15.9 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03990782528321363		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.03990782528321363 | validation: 0.014593884633705237]
	TIME [epoch: 15.9 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0223434005177981		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.0223434005177981 | validation: 0.01825154071990559]
	TIME [epoch: 15.9 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02120210926125942		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.02120210926125942 | validation: 0.026855620027914827]
	TIME [epoch: 15.9 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019297078971590238		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.019297078971590238 | validation: 0.032106428951636665]
	TIME [epoch: 15.9 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01913613047651257		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.01913613047651257 | validation: 0.018734801769406968]
	TIME [epoch: 15.9 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01574045706677824		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.01574045706677824 | validation: 0.030190995568161332]
	TIME [epoch: 15.9 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04198947658497859		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.04198947658497859 | validation: 0.018725374386489285]
	TIME [epoch: 15.9 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017987468940469566		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.017987468940469566 | validation: 0.014294985707220063]
	TIME [epoch: 15.9 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014259439431291786		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.014259439431291786 | validation: 0.03724871200699563]
	TIME [epoch: 15.9 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025088140570995898		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.025088140570995898 | validation: 0.014571284428260289]
	TIME [epoch: 15.9 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010062304761891846		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.010062304761891846 | validation: 0.01817560327053513]
	TIME [epoch: 15.9 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02425272541029258		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.02425272541029258 | validation: 0.013785884110618343]
	TIME [epoch: 15.9 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021097635663413952		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.021097635663413952 | validation: 0.021124756968025474]
	TIME [epoch: 15.9 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02803984991754396		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.02803984991754396 | validation: 0.018864803009456466]
	TIME [epoch: 15.9 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013571362531322434		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.013571362531322434 | validation: 0.014260269089786247]
	TIME [epoch: 15.9 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012617428157440214		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.012617428157440214 | validation: 0.016289678820693095]
	TIME [epoch: 15.9 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032333570518907215		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.032333570518907215 | validation: 0.010418579643556277]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028361522402972458		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.028361522402972458 | validation: 0.017611213294381953]
	TIME [epoch: 15.9 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014678176545564352		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.014678176545564352 | validation: 0.011992626257833067]
	TIME [epoch: 15.9 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032838647447550794		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.032838647447550794 | validation: 0.01937851676084147]
	TIME [epoch: 15.9 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022800220320055357		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.022800220320055357 | validation: 0.026254425187191198]
	TIME [epoch: 15.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01393721415335512		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.01393721415335512 | validation: 0.010184812362751434]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017357724939655882		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.017357724939655882 | validation: 0.023492480056442406]
	TIME [epoch: 15.9 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01711220899857093		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.01711220899857093 | validation: 0.01738677217623603]
	TIME [epoch: 15.9 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014903341369445936		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.014903341369445936 | validation: 0.013695077849489685]
	TIME [epoch: 15.9 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024946573353523113		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.024946573353523113 | validation: 0.022234407699562038]
	TIME [epoch: 15.9 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01405794619675803		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.01405794619675803 | validation: 0.011623386946784428]
	TIME [epoch: 15.9 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01520036385800033		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.01520036385800033 | validation: 0.028187577005870438]
	TIME [epoch: 15.9 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023583659955651377		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.023583659955651377 | validation: 0.02569459355759273]
	TIME [epoch: 15.9 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02320747310363202		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.02320747310363202 | validation: 0.03521147598434252]
	TIME [epoch: 15.9 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02709651795797772		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.02709651795797772 | validation: 0.024788416363240702]
	TIME [epoch: 15.9 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023310377802576953		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.023310377802576953 | validation: 0.023324755463065134]
	TIME [epoch: 15.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015728669511432897		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.015728669511432897 | validation: 0.020525584491238973]
	TIME [epoch: 15.9 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020597546203521343		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.020597546203521343 | validation: 0.017745356750866942]
	TIME [epoch: 15.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017482521056577816		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.017482521056577816 | validation: 0.017816557469033805]
	TIME [epoch: 15.9 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01831689790360998		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.01831689790360998 | validation: 0.025722063037398973]
	TIME [epoch: 15.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013841233119924029		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.013841233119924029 | validation: 0.016526672352222674]
	TIME [epoch: 15.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014964145043174514		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.014964145043174514 | validation: 0.035349961183109235]
	TIME [epoch: 15.9 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024291347041439516		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.024291347041439516 | validation: 0.010213114827395358]
	TIME [epoch: 15.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015149469625157768		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.015149469625157768 | validation: 0.020836129035410187]
	TIME [epoch: 15.9 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01887275415632078		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.01887275415632078 | validation: 0.027666068249918012]
	TIME [epoch: 15.9 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015127638396038113		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.015127638396038113 | validation: 0.008863664187352363]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01416146176300188		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.01416146176300188 | validation: 0.038047745789214424]
	TIME [epoch: 15.9 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024502981286194513		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.024502981286194513 | validation: 0.01456443138550179]
	TIME [epoch: 15.9 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00910131326073049		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.00910131326073049 | validation: 0.012629359491271981]
	TIME [epoch: 15.9 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018123002266493504		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.018123002266493504 | validation: 0.013450064509641028]
	TIME [epoch: 15.9 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017805965977238828		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.017805965977238828 | validation: 0.026299973546077268]
	TIME [epoch: 15.9 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014421646245176438		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.014421646245176438 | validation: 0.014006141305751355]
	TIME [epoch: 15.9 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013269221762213255		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.013269221762213255 | validation: 0.013072295607140825]
	TIME [epoch: 15.9 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01906041961817131		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.01906041961817131 | validation: 0.042090355128129864]
	TIME [epoch: 15.9 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023836270893162703		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.023836270893162703 | validation: 0.015507560108373543]
	TIME [epoch: 15.9 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011283403870842382		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.011283403870842382 | validation: 0.015620908725135024]
	TIME [epoch: 15.9 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020311728197261472		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.020311728197261472 | validation: 0.02519768546690027]
	TIME [epoch: 15.9 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013078924450311183		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.013078924450311183 | validation: 0.014067537152955233]
	TIME [epoch: 15.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011508093024229074		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.011508093024229074 | validation: 0.027551733735489907]
	TIME [epoch: 15.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02186652521245055		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.02186652521245055 | validation: 0.011451776725243293]
	TIME [epoch: 15.9 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012486702138686221		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.012486702138686221 | validation: 0.011937135083314225]
	TIME [epoch: 15.9 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013281961681204886		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.013281961681204886 | validation: 0.015281113114697185]
	TIME [epoch: 15.9 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015922398530842034		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.015922398530842034 | validation: 0.017375935273902146]
	TIME [epoch: 15.9 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017377418320284718		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.017377418320284718 | validation: 0.013235777770994616]
	TIME [epoch: 15.9 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015409330053381047		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.015409330053381047 | validation: 0.016245897205165755]
	TIME [epoch: 15.9 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02059017616089285		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.02059017616089285 | validation: 0.011092986189913383]
	TIME [epoch: 15.9 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008437619381092295		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.008437619381092295 | validation: 0.010521463816346895]
	TIME [epoch: 15.9 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01570775336712342		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.01570775336712342 | validation: 0.014525062719480744]
	TIME [epoch: 15.9 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013200262797506515		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.013200262797506515 | validation: 0.029553388495120052]
	TIME [epoch: 16.6 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01736238643857904		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.01736238643857904 | validation: 0.019295800564712133]
	TIME [epoch: 15.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011583967132436316		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.011583967132436316 | validation: 0.014723328367597939]
	TIME [epoch: 15.9 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01611735897539773		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.01611735897539773 | validation: 0.03275196554228926]
	TIME [epoch: 15.9 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023509580919281178		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.023509580919281178 | validation: 0.018131260880683814]
	TIME [epoch: 15.9 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020512572757244815		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.020512572757244815 | validation: 0.012806132837042378]
	TIME [epoch: 15.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009271507170331695		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.009271507170331695 | validation: 0.009586424512514893]
	TIME [epoch: 15.9 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015052626909776873		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.015052626909776873 | validation: 0.022362342264781083]
	TIME [epoch: 15.9 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013621064199192312		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.013621064199192312 | validation: 0.018403565616768976]
	TIME [epoch: 15.9 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010975631058875591		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.010975631058875591 | validation: 0.01220836444858534]
	TIME [epoch: 15.9 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016693322528260727		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.016693322528260727 | validation: 0.016643176133830165]
	TIME [epoch: 15.9 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012099457109228832		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.012099457109228832 | validation: 0.013738691282890698]
	TIME [epoch: 15.9 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021539013643643218		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.021539013643643218 | validation: 0.013142018831677083]
	TIME [epoch: 15.9 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010024664308347003		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.010024664308347003 | validation: 0.009498334429652137]
	TIME [epoch: 15.9 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012098637893211325		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.012098637893211325 | validation: 0.02467282962911175]
	TIME [epoch: 15.9 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015431270874055745		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.015431270874055745 | validation: 0.00792954953780241]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009032951597532148		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.009032951597532148 | validation: 0.01629081217266882]
	TIME [epoch: 15.9 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017085101319099735		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.017085101319099735 | validation: 0.01619213805319225]
	TIME [epoch: 15.9 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012018437899438258		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.012018437899438258 | validation: 0.011247870918869418]
	TIME [epoch: 15.9 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014334519702695347		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.014334519702695347 | validation: 0.023784861248749994]
	TIME [epoch: 15.9 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0121037667960748		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.0121037667960748 | validation: 0.008172724630803338]
	TIME [epoch: 15.9 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012410295919309955		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.012410295919309955 | validation: 0.011941316630650485]
	TIME [epoch: 15.9 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013271473553490058		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.013271473553490058 | validation: 0.032381345762422464]
	TIME [epoch: 15.9 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014046056663199715		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.014046056663199715 | validation: 0.007867961668072663]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015489317265873812		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.015489317265873812 | validation: 0.008138105944683113]
	TIME [epoch: 15.9 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011598700393338079		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.011598700393338079 | validation: 0.016555815525005346]
	TIME [epoch: 15.9 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01714965202418707		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.01714965202418707 | validation: 0.023201646463229857]
	TIME [epoch: 15.9 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012036739480173158		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.012036739480173158 | validation: 0.010242655889462772]
	TIME [epoch: 15.9 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009308799886816897		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.009308799886816897 | validation: 0.02627075958144553]
	TIME [epoch: 15.9 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016778005669681857		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.016778005669681857 | validation: 0.012066835008996284]
	TIME [epoch: 15.9 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012388460349792803		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.012388460349792803 | validation: 0.01202403740606169]
	TIME [epoch: 15.9 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016462373086894463		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.016462373086894463 | validation: 0.010953339682344992]
	TIME [epoch: 15.9 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008232770566359494		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.008232770566359494 | validation: 0.016788761666484388]
	TIME [epoch: 15.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01165038388607167		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.01165038388607167 | validation: 0.010096593233746162]
	TIME [epoch: 15.9 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010953586790066064		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.010953586790066064 | validation: 0.016128106827837007]
	TIME [epoch: 15.9 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015812110503886777		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.015812110503886777 | validation: 0.013158730934234333]
	TIME [epoch: 15.9 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00890006077142231		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.00890006077142231 | validation: 0.015249716426581838]
	TIME [epoch: 15.9 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011780190367029491		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.011780190367029491 | validation: 0.014333528254956842]
	TIME [epoch: 15.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016862359446370472		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.016862359446370472 | validation: 0.021064766734860657]
	TIME [epoch: 15.9 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012449483037299803		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.012449483037299803 | validation: 0.00926445808961526]
	TIME [epoch: 15.9 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008648941778414123		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.008648941778414123 | validation: 0.019166772121898185]
	TIME [epoch: 15.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012397089330848947		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.012397089330848947 | validation: 0.020907834281387257]
	TIME [epoch: 15.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016234696254718895		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.016234696254718895 | validation: 0.016626169676015448]
	TIME [epoch: 15.9 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010884487523925415		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.010884487523925415 | validation: 0.008584856036162535]
	TIME [epoch: 15.9 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013439000564840736		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.013439000564840736 | validation: 0.012660200001799314]
	TIME [epoch: 15.9 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009705167081905755		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.009705167081905755 | validation: 0.0205100780628001]
	TIME [epoch: 15.9 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017266135547184116		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.017266135547184116 | validation: 0.032743732406377624]
	TIME [epoch: 15.9 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01507413726888974		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.01507413726888974 | validation: 0.00781985246096953]
	TIME [epoch: 15.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007648100757529995		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.007648100757529995 | validation: 0.011640394410730146]
	TIME [epoch: 15.9 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007307913948723552		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.007307913948723552 | validation: 0.008602367407444116]
	TIME [epoch: 15.9 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012548961893700847		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.012548961893700847 | validation: 0.019853715268340316]
	TIME [epoch: 15.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017009619930149225		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.017009619930149225 | validation: 0.00931168602203459]
	TIME [epoch: 15.9 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0093973452473189		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.0093973452473189 | validation: 0.009437499257372528]
	TIME [epoch: 15.9 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068872128472431265		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.0068872128472431265 | validation: 0.010588615370425006]
	TIME [epoch: 15.9 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014351386107498209		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.014351386107498209 | validation: 0.013584582553022913]
	TIME [epoch: 15.9 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010663040886885567		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.010663040886885567 | validation: 0.009979023993082856]
	TIME [epoch: 15.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008639878974288098		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.008639878974288098 | validation: 0.00813784264217891]
	TIME [epoch: 15.9 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014367230114778182		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.014367230114778182 | validation: 0.012963995491358888]
	TIME [epoch: 15.9 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020404458679319164		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.020404458679319164 | validation: 0.016468302196775444]
	TIME [epoch: 15.9 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009535825701668246		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.009535825701668246 | validation: 0.008411297338309021]
	TIME [epoch: 15.9 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006385249845924981		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.006385249845924981 | validation: 0.00968364245939091]
	TIME [epoch: 15.9 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01122282279476252		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.01122282279476252 | validation: 0.016525558599493954]
	TIME [epoch: 15.9 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013827036577000787		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.013827036577000787 | validation: 0.0069364690808877685]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00876891289572932		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.00876891289572932 | validation: 0.008190955806405241]
	TIME [epoch: 15.9 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011575104672209149		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.011575104672209149 | validation: 0.012499650514266072]
	TIME [epoch: 15.9 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015367303067226577		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.015367303067226577 | validation: 0.008802812273913201]
	TIME [epoch: 15.9 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008477639300001566		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.008477639300001566 | validation: 0.014076591101757685]
	TIME [epoch: 15.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011167619758944392		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.011167619758944392 | validation: 0.010061178638118716]
	TIME [epoch: 15.9 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009204121405783604		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.009204121405783604 | validation: 0.01513859454485499]
	TIME [epoch: 15.9 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010909378826483327		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.010909378826483327 | validation: 0.009406831712023042]
	TIME [epoch: 15.9 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007144689490500308		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.007144689490500308 | validation: 0.012769795303895408]
	TIME [epoch: 15.9 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01550974693789943		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.01550974693789943 | validation: 0.01258671812602781]
	TIME [epoch: 15.9 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009328669695741332		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.009328669695741332 | validation: 0.012423263740930023]
	TIME [epoch: 15.9 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007954648468988338		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.007954648468988338 | validation: 0.008591136831329902]
	TIME [epoch: 15.9 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01035913567441219		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.01035913567441219 | validation: 0.011545470325079625]
	TIME [epoch: 15.9 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00986557554766111		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.00986557554766111 | validation: 0.012725571637584927]
	TIME [epoch: 15.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010221452025393278		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.010221452025393278 | validation: 0.02139046938062593]
	TIME [epoch: 15.9 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013506271407584175		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.013506271407584175 | validation: 0.007666111380738443]
	TIME [epoch: 15.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007187976782235642		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.007187976782235642 | validation: 0.009378539001947937]
	TIME [epoch: 15.9 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008084524302891008		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.008084524302891008 | validation: 0.012063869631538136]
	TIME [epoch: 15.9 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01471066257944037		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.01471066257944037 | validation: 0.010423714398338236]
	TIME [epoch: 15.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007804025618480304		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.007804025618480304 | validation: 0.007024447754524246]
	TIME [epoch: 15.9 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012806877088773438		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.012806877088773438 | validation: 0.013748473044776686]
	TIME [epoch: 15.9 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008618325206170993		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.008618325206170993 | validation: 0.0073396723018152345]
	TIME [epoch: 15.9 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006625005107169923		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.006625005107169923 | validation: 0.008431610105585944]
	TIME [epoch: 15.9 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008236951653623568		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.008236951653623568 | validation: 0.015850757210546025]
	TIME [epoch: 49.8 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01455642450314528		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.01455642450314528 | validation: 0.009193006912073972]
	TIME [epoch: 33.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007103491355674686		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.007103491355674686 | validation: 0.0082827659668122]
	TIME [epoch: 33.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008484086575169471		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.008484086575169471 | validation: 0.011329443451455722]
	TIME [epoch: 33.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009483737917950881		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.009483737917950881 | validation: 0.01503174882739651]
	TIME [epoch: 33.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010855919684178036		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.010855919684178036 | validation: 0.00852133105769097]
	TIME [epoch: 33.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009182180381398189		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.009182180381398189 | validation: 0.009797185438318146]
	TIME [epoch: 33.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007416624262806906		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.007416624262806906 | validation: 0.011842996523175723]
	TIME [epoch: 33.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012283723824264469		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.012283723824264469 | validation: 0.014704860663759398]
	TIME [epoch: 33.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008706354778501466		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.008706354778501466 | validation: 0.008691341146240893]
	TIME [epoch: 33.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01071084111901316		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.01071084111901316 | validation: 0.011901697478983658]
	TIME [epoch: 33.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067742947454578345		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.0067742947454578345 | validation: 0.007722284286854519]
	TIME [epoch: 33.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074019248608272175		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.0074019248608272175 | validation: 0.01180643863645046]
	TIME [epoch: 33.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01167920780209191		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.01167920780209191 | validation: 0.012306559734230487]
	TIME [epoch: 33.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009299171414538477		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.009299171414538477 | validation: 0.005705779418487139]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005963760170383258		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.005963760170383258 | validation: 0.00783842982113633]
	TIME [epoch: 33.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016068714685975587		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.016068714685975587 | validation: 0.00732119791745346]
	TIME [epoch: 33.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007988885463541883		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.007988885463541883 | validation: 0.006843591017742095]
	TIME [epoch: 33.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007144537181875775		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.007144537181875775 | validation: 0.008422987580743856]
	TIME [epoch: 33.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007048268547573169		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.007048268547573169 | validation: 0.007063375823281256]
	TIME [epoch: 33.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008228322121945655		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.008228322121945655 | validation: 0.01733613929511023]
	TIME [epoch: 33.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00960224908326301		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.00960224908326301 | validation: 0.007626893319360726]
	TIME [epoch: 33.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005565636237239791		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.005565636237239791 | validation: 0.006458268332570979]
	TIME [epoch: 33.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007688321863342869		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.007688321863342869 | validation: 0.012154224664038454]
	TIME [epoch: 33.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013609911895430914		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.013609911895430914 | validation: 0.009249165740326644]
	TIME [epoch: 33.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006499833539628723		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.006499833539628723 | validation: 0.006440311031043422]
	TIME [epoch: 33.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008038359326725836		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.008038359326725836 | validation: 0.00836773441984931]
	TIME [epoch: 33.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008068660474763194		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.008068660474763194 | validation: 0.007752092957944651]
	TIME [epoch: 33.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006726186768387747		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.006726186768387747 | validation: 0.014132175338569742]
	TIME [epoch: 33.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012342570738660363		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.012342570738660363 | validation: 0.010841078430432187]
	TIME [epoch: 33.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008049325478759944		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.008049325478759944 | validation: 0.007347855425689566]
	TIME [epoch: 33.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006147036182910222		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.006147036182910222 | validation: 0.00959852448498182]
	TIME [epoch: 33.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008873035686779888		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.008873035686779888 | validation: 0.006523589601923842]
	TIME [epoch: 33.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006783728040986736		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.006783728040986736 | validation: 0.010772598540453957]
	TIME [epoch: 33.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007238973198732222		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.007238973198732222 | validation: 0.014614654191000673]
	TIME [epoch: 33.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012799972771985475		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.012799972771985475 | validation: 0.005733924091174784]
	TIME [epoch: 33.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007021233372496969		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.007021233372496969 | validation: 0.007071920191687923]
	TIME [epoch: 33.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064573150068728945		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.0064573150068728945 | validation: 0.008003838803799821]
	TIME [epoch: 33.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007442021141947612		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.007442021141947612 | validation: 0.0158900566543458]
	TIME [epoch: 33.7 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012700864030250062		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.012700864030250062 | validation: 0.006098120418382815]
	TIME [epoch: 33.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006427807862796867		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.006427807862796867 | validation: 0.006659777674302058]
	TIME [epoch: 33.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006658804572342733		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.006658804572342733 | validation: 0.006022300710868039]
	TIME [epoch: 33.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004801186939095153		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.004801186939095153 | validation: 0.010840449761644166]
	TIME [epoch: 33.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010670900195410362		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.010670900195410362 | validation: 0.007000180577679236]
	TIME [epoch: 33.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008407628043722536		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.008407628043722536 | validation: 0.007750267430884179]
	TIME [epoch: 33.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007828500063648396		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.007828500063648396 | validation: 0.009568690070890092]
	TIME [epoch: 33.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007554272412881192		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.007554272412881192 | validation: 0.011688187013095881]
	TIME [epoch: 33.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007455937925099063		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.007455937925099063 | validation: 0.007313973233777267]
	TIME [epoch: 33.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006064384980356571		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.006064384980356571 | validation: 0.007947235945274885]
	TIME [epoch: 33.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007174987031624048		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.007174987031624048 | validation: 0.007631300516388948]
	TIME [epoch: 33.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006078588094426046		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.006078588094426046 | validation: 0.008233030189736338]
	TIME [epoch: 33.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011688814481067851		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.011688814481067851 | validation: 0.007483842514285879]
	TIME [epoch: 33.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008422721794509973		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.008422721794509973 | validation: 0.007985703684576583]
	TIME [epoch: 33.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005823714279500813		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.005823714279500813 | validation: 0.005797680286914234]
	TIME [epoch: 33.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006515933578203362		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.006515933578203362 | validation: 0.009505290750424302]
	TIME [epoch: 33.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010206842187643039		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.010206842187643039 | validation: 0.007537207298908645]
	TIME [epoch: 33.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00814887825119098		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.00814887825119098 | validation: 0.006921961982142295]
	TIME [epoch: 33.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005883803610290835		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.005883803610290835 | validation: 0.015323471881364157]
	TIME [epoch: 33.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007582059158836355		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.007582059158836355 | validation: 0.006985096636799899]
	TIME [epoch: 33.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058011773600710565		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.0058011773600710565 | validation: 0.007258120215026483]
	TIME [epoch: 33.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006382677871327072		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.006382677871327072 | validation: 0.010181409901622224]
	TIME [epoch: 33.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009546647062338348		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.009546647062338348 | validation: 0.01798362232006042]
	TIME [epoch: 33.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007474788378072676		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.007474788378072676 | validation: 0.006558627243397648]
	TIME [epoch: 33.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005432453670688958		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.005432453670688958 | validation: 0.007834685784754285]
	TIME [epoch: 33.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076103411861685715		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.0076103411861685715 | validation: 0.011209963880763755]
	TIME [epoch: 33.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01031764761788193		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.01031764761788193 | validation: 0.0066966994698713315]
	TIME [epoch: 33.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007334583806200475		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.007334583806200475 | validation: 0.006332704509481589]
	TIME [epoch: 33.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005877740089583519		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.005877740089583519 | validation: 0.013109816462465356]
	TIME [epoch: 33.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008342804865331366		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.008342804865331366 | validation: 0.006681070988617477]
	TIME [epoch: 33.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012263137367616911		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.012263137367616911 | validation: 0.008762187021422735]
	TIME [epoch: 33.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006080328106303207		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.006080328106303207 | validation: 0.009294826410126284]
	TIME [epoch: 33.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005889483326214843		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.005889483326214843 | validation: 0.0067308411928392806]
	TIME [epoch: 33.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040481451111478405		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.0040481451111478405 | validation: 0.008499254802176011]
	TIME [epoch: 33.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008542495561612132		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.008542495561612132 | validation: 0.006653642225309098]
	TIME [epoch: 33.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005526519114970522		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.005526519114970522 | validation: 0.00856989708694282]
	TIME [epoch: 33.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00804012662215495		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.00804012662215495 | validation: 0.006304576816229422]
	TIME [epoch: 33.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006292967952344393		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.006292967952344393 | validation: 0.007014910642103005]
	TIME [epoch: 33.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006989546782361421		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.006989546782361421 | validation: 0.007306644072884805]
	TIME [epoch: 33.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007229619766492843		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.007229619766492843 | validation: 0.005233372712239716]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005717224265887066		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.005717224265887066 | validation: 0.009850636151757176]
	TIME [epoch: 33.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060087655745199794		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.0060087655745199794 | validation: 0.011571932921740757]
	TIME [epoch: 33.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005658568219225073		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.005658568219225073 | validation: 0.005328742217108488]
	TIME [epoch: 33.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006901290795365338		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.006901290795365338 | validation: 0.011592853687001742]
	TIME [epoch: 33.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006777798879431137		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.006777798879431137 | validation: 0.00847905630943709]
	TIME [epoch: 33.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006518003402290253		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.006518003402290253 | validation: 0.005696657616357943]
	TIME [epoch: 33.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005300779276700381		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.005300779276700381 | validation: 0.011116689081306742]
	TIME [epoch: 33.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006711305581826313		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.006711305581826313 | validation: 0.006037883044653538]
	TIME [epoch: 33.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007193678219893799		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.007193678219893799 | validation: 0.009763964987558736]
	TIME [epoch: 33.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006442003835906232		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.006442003835906232 | validation: 0.006318599446732323]
	TIME [epoch: 33.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007119769500652948		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.007119769500652948 | validation: 0.00965580145045659]
	TIME [epoch: 33.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007490240969854065		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.007490240969854065 | validation: 0.005921111045510511]
	TIME [epoch: 33.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005692982437676665		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.005692982437676665 | validation: 0.007065897651093552]
	TIME [epoch: 33.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065249616399985725		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.0065249616399985725 | validation: 0.007319475746801975]
	TIME [epoch: 33.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007205990636709754		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.007205990636709754 | validation: 0.006420265966357018]
	TIME [epoch: 33.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004731146554318265		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.004731146554318265 | validation: 0.008008363650629714]
	TIME [epoch: 33.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007783689864861572		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.007783689864861572 | validation: 0.00535068738399537]
	TIME [epoch: 33.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004248154922362896		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.004248154922362896 | validation: 0.00833751067694297]
	TIME [epoch: 33.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006822902741583321		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.006822902741583321 | validation: 0.006461263469412578]
	TIME [epoch: 33.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006205395812499948		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.006205395812499948 | validation: 0.009746524651856686]
	TIME [epoch: 33.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054138542749756725		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.0054138542749756725 | validation: 0.006047121351951541]
	TIME [epoch: 33.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007964898921171993		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.007964898921171993 | validation: 0.005631558328325219]
	TIME [epoch: 33.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062701131709542186		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0062701131709542186 | validation: 0.010341969466504859]
	TIME [epoch: 33.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007578176651525381		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.007578176651525381 | validation: 0.004881727599871953]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005183773845293935		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.005183773845293935 | validation: 0.006689940057319203]
	TIME [epoch: 33.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004997294453056496		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.004997294453056496 | validation: 0.005225447105560185]
	TIME [epoch: 33.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004625279030488469		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.004625279030488469 | validation: 0.01138405057626659]
	TIME [epoch: 33.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008609489574321774		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.008609489574321774 | validation: 0.008512736210392087]
	TIME [epoch: 33.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004763648016997385		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.004763648016997385 | validation: 0.005565984089861627]
	TIME [epoch: 33.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004444400758358634		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.004444400758358634 | validation: 0.007293260935803252]
	TIME [epoch: 33.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006951883611248827		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.006951883611248827 | validation: 0.006023221128995753]
	TIME [epoch: 33.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010619713826583336		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.010619713826583336 | validation: 0.006726134455154755]
	TIME [epoch: 33.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004358742930439816		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.004358742930439816 | validation: 0.0038235831927961278]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038069376342717296		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.0038069376342717296 | validation: 0.007677508336314901]
	TIME [epoch: 33.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006864100159779498		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.006864100159779498 | validation: 0.0057882323259745225]
	TIME [epoch: 33.7 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054622368524960075		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.0054622368524960075 | validation: 0.005876377338363037]
	TIME [epoch: 33.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004773563664932138		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.004773563664932138 | validation: 0.00548318116710397]
	TIME [epoch: 33.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008060635747918694		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.008060635747918694 | validation: 0.006515263762640386]
	TIME [epoch: 33.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049710096352221875		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.0049710096352221875 | validation: 0.004352141291131572]
	TIME [epoch: 33.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004699678905852214		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.004699678905852214 | validation: 0.006436983662017396]
	TIME [epoch: 33.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004559252563097557		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.004559252563097557 | validation: 0.010909731254646247]
	TIME [epoch: 33.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006961023627896515		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.006961023627896515 | validation: 0.005599285787760346]
	TIME [epoch: 33.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004424898893067599		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.004424898893067599 | validation: 0.0076183545606476]
	TIME [epoch: 33.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006040752720109361		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.006040752720109361 | validation: 0.00494818532129008]
	TIME [epoch: 33.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005797392363708718		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.005797392363708718 | validation: 0.013096990123458677]
	TIME [epoch: 33.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005573230310150994		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.005573230310150994 | validation: 0.00899968201079068]
	TIME [epoch: 33.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005721860919879563		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.005721860919879563 | validation: 0.008998248967997382]
	TIME [epoch: 33.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00590424921952813		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.00590424921952813 | validation: 0.005818378874759927]
	TIME [epoch: 33.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005161916644288756		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.005161916644288756 | validation: 0.008097542387220272]
	TIME [epoch: 33.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006129589675320872		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.006129589675320872 | validation: 0.004384294562642129]
	TIME [epoch: 33.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003781484278695239		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.003781484278695239 | validation: 0.005106713382914984]
	TIME [epoch: 33.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00478390505035625		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.00478390505035625 | validation: 0.008851335026892379]
	TIME [epoch: 33.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006160278284953009		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.006160278284953009 | validation: 0.0050183056133779845]
	TIME [epoch: 33.7 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004975077405535513		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.004975077405535513 | validation: 0.008171725387701136]
	TIME [epoch: 33.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00576087495643426		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.00576087495643426 | validation: 0.004978780905942249]
	TIME [epoch: 33.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004743954286335498		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.004743954286335498 | validation: 0.006514080857794974]
	TIME [epoch: 33.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006282210732726705		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.006282210732726705 | validation: 0.005342907257183925]
	TIME [epoch: 33.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004694537031640158		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.004694537031640158 | validation: 0.006770359290144022]
	TIME [epoch: 33.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005172445524312065		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.005172445524312065 | validation: 0.0035094042779715146]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004132716866224417		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.004132716866224417 | validation: 0.006093224032884604]
	TIME [epoch: 33.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046879106601478366		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.0046879106601478366 | validation: 0.014064479187625382]
	TIME [epoch: 33.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006683414529180073		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.006683414529180073 | validation: 0.007655289787201122]
	TIME [epoch: 33.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006496729620936361		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.006496729620936361 | validation: 0.004918058327901209]
	TIME [epoch: 33.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038310608406881552		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.0038310608406881552 | validation: 0.004476856740780468]
	TIME [epoch: 33.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004385149811003298		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.004385149811003298 | validation: 0.003899589055565678]
	TIME [epoch: 33.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004308547727403354		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.004308547727403354 | validation: 0.004356810933874595]
	TIME [epoch: 33.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057864780431635555		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.0057864780431635555 | validation: 0.006620392196204278]
	TIME [epoch: 33.7 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004473945039888795		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.004473945039888795 | validation: 0.008650026200930683]
	TIME [epoch: 33.7 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005034522808086264		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.005034522808086264 | validation: 0.006582558417883359]
	TIME [epoch: 33.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004438731529481932		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.004438731529481932 | validation: 0.006291071280450899]
	TIME [epoch: 33.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004466059670192819		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.004466059670192819 | validation: 0.004196595837252412]
	TIME [epoch: 33.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006257348600112463		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.006257348600112463 | validation: 0.006585058711173725]
	TIME [epoch: 33.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037642350991257357		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.0037642350991257357 | validation: 0.004782813794722458]
	TIME [epoch: 33.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004114728799833548		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.004114728799833548 | validation: 0.0046402696921023995]
	TIME [epoch: 33.7 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00487316220442356		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.00487316220442356 | validation: 0.009820133551050447]
	TIME [epoch: 33.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006807143005775759		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.006807143005775759 | validation: 0.007120391111970904]
	TIME [epoch: 33.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004368030994064655		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.004368030994064655 | validation: 0.004057239525098291]
	TIME [epoch: 33.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041681477910589715		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.0041681477910589715 | validation: 0.006152258419272829]
	TIME [epoch: 33.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043323355860722616		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.0043323355860722616 | validation: 0.005642849366109996]
	TIME [epoch: 33.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005448616029436134		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.005448616029436134 | validation: 0.005553396787171386]
	TIME [epoch: 33.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003595767851251717		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.003595767851251717 | validation: 0.004185991665939926]
	TIME [epoch: 33.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004258761086016904		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.004258761086016904 | validation: 0.004966456714171133]
	TIME [epoch: 33.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00570095160122556		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.00570095160122556 | validation: 0.00543693285820431]
	TIME [epoch: 33.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004082728563970653		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.004082728563970653 | validation: 0.005114911044686717]
	TIME [epoch: 33.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036869545817645663		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.0036869545817645663 | validation: 0.0065333723449912725]
	TIME [epoch: 33.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004003577534749868		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.004003577534749868 | validation: 0.007436559336533406]
	TIME [epoch: 33.7 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004771016635067631		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.004771016635067631 | validation: 0.0055650407587294675]
	TIME [epoch: 33.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030977010769455545		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0030977010769455545 | validation: 0.0034307114100876096]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006283403313901605		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.006283403313901605 | validation: 0.00920823846963778]
	TIME [epoch: 33.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004814540850370838		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.004814540850370838 | validation: 0.004523723832724204]
	TIME [epoch: 33.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003970346361836534		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.003970346361836534 | validation: 0.005389243659774622]
	TIME [epoch: 33.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004189324192504677		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.004189324192504677 | validation: 0.004021000391372727]
	TIME [epoch: 33.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003527382417285249		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.003527382417285249 | validation: 0.0067119423614393735]
	TIME [epoch: 33.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00470508891264268		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.00470508891264268 | validation: 0.004132928584237708]
	TIME [epoch: 33.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004703118634055218		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.004703118634055218 | validation: 0.005750298434355824]
	TIME [epoch: 33.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004597354374015665		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.004597354374015665 | validation: 0.006263985271426671]
	TIME [epoch: 33.7 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042189351055492985		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.0042189351055492985 | validation: 0.005135597111000553]
	TIME [epoch: 33.7 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004256215700376342		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.004256215700376342 | validation: 0.005820898848684692]
	TIME [epoch: 33.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004128654329270253		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.004128654329270253 | validation: 0.004146430369514824]
	TIME [epoch: 33.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003353312033574026		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.003353312033574026 | validation: 0.004180143046054857]
	TIME [epoch: 33.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003006727667099468		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.003006727667099468 | validation: 0.004414609190740835]
	TIME [epoch: 33.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067810034130529415		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.0067810034130529415 | validation: 0.004284979383866658]
	TIME [epoch: 33.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003510165783137016		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.003510165783137016 | validation: 0.005364742587045787]
	TIME [epoch: 33.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042840488505684554		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.0042840488505684554 | validation: 0.004283512771620918]
	TIME [epoch: 33.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033448801705192486		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0033448801705192486 | validation: 0.007101075812961821]
	TIME [epoch: 33.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005273717211832292		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.005273717211832292 | validation: 0.005689174981470974]
	TIME [epoch: 33.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00371004271256791		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.00371004271256791 | validation: 0.00479359884462765]
	TIME [epoch: 33.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003946800153305091		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.003946800153305091 | validation: 0.004721819657704239]
	TIME [epoch: 33.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004991865891068313		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.004991865891068313 | validation: 0.003237126288305966]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003299882815207211		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.003299882815207211 | validation: 0.006748562321075029]
	TIME [epoch: 33.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004356714584490907		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.004356714584490907 | validation: 0.003996939050760789]
	TIME [epoch: 33.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004706158907084291		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.004706158907084291 | validation: 0.007658220398793259]
	TIME [epoch: 33.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004461246236159481		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.004461246236159481 | validation: 0.005861048489314556]
	TIME [epoch: 33.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035946435480101032		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.0035946435480101032 | validation: 0.004781185368448567]
	TIME [epoch: 33.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033475783408277206		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.0033475783408277206 | validation: 0.0038270641141961263]
	TIME [epoch: 33.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003480313730978595		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.003480313730978595 | validation: 0.003496054215000661]
	TIME [epoch: 33.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003454834160877589		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.003454834160877589 | validation: 0.004783671024936877]
	TIME [epoch: 33.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005359192741770738		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.005359192741770738 | validation: 0.0037878176744304357]
	TIME [epoch: 33.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002798107995989885		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.002798107995989885 | validation: 0.00434899845663543]
	TIME [epoch: 33.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004931653688539312		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.004931653688539312 | validation: 0.004682197918548665]
	TIME [epoch: 33.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004026586156979773		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.004026586156979773 | validation: 0.0034502616453169132]
	TIME [epoch: 33.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003428043258004874		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.003428043258004874 | validation: 0.0028344988999793996]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031346835720921676		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.0031346835720921676 | validation: 0.0053487399952091095]
	TIME [epoch: 33.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00511600794640159		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.00511600794640159 | validation: 0.006952367301821185]
	TIME [epoch: 33.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003950304737501521		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.003950304737501521 | validation: 0.0031560869577435115]
	TIME [epoch: 33.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027057497839508273		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.0027057497839508273 | validation: 0.0037021120522416]
	TIME [epoch: 33.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002402282409256719		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.002402282409256719 | validation: 0.004536097307300098]
	TIME [epoch: 33.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004578678478389684		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.004578678478389684 | validation: 0.004477676821668757]
	TIME [epoch: 33.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003667763855515006		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.003667763855515006 | validation: 0.004260647400077339]
	TIME [epoch: 33.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002894465910540236		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.002894465910540236 | validation: 0.00910207631971655]
	TIME [epoch: 33.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053453289708981075		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.0053453289708981075 | validation: 0.002974112349481482]
	TIME [epoch: 33.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003010952661570931		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.003010952661570931 | validation: 0.0040888502524451245]
	TIME [epoch: 33.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028293539426064163		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0028293539426064163 | validation: 0.004846687680853428]
	TIME [epoch: 33.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005044962943345085		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.005044962943345085 | validation: 0.004173139011546837]
	TIME [epoch: 33.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044505790040567355		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0044505790040567355 | validation: 0.0035113588437679574]
	TIME [epoch: 33.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033931857866021406		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.0033931857866021406 | validation: 0.0035292194855680724]
	TIME [epoch: 33.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002963381795906431		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.002963381795906431 | validation: 0.004477147874873363]
	TIME [epoch: 33.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032127266702187776		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0032127266702187776 | validation: 0.004561951830768453]
	TIME [epoch: 33.7 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032634900038845537		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0032634900038845537 | validation: 0.006194710976175612]
	TIME [epoch: 33.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036336296009515196		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0036336296009515196 | validation: 0.0030094996227010293]
	TIME [epoch: 33.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034820929773319888		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.0034820929773319888 | validation: 0.0031121867830918876]
	TIME [epoch: 33.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004374574897380858		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.004374574897380858 | validation: 0.004899175982478138]
	TIME [epoch: 33.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003359918997468622		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.003359918997468622 | validation: 0.003809606069352634]
	TIME [epoch: 33.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024673883820511924		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.0024673883820511924 | validation: 0.004198476181309379]
	TIME [epoch: 33.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027429097711022263		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0027429097711022263 | validation: 0.002761420641670816]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035405420435567735		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.0035405420435567735 | validation: 0.0052160936020795405]
	TIME [epoch: 33.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004649452606940344		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.004649452606940344 | validation: 0.007388653397394893]
	TIME [epoch: 33.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004519668361232911		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.004519668361232911 | validation: 0.004319800736344307]
	TIME [epoch: 33.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023291819976903557		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.0023291819976903557 | validation: 0.003091991203430661]
	TIME [epoch: 33.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004571943187584485		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.004571943187584485 | validation: 0.0045354931435411475]
	TIME [epoch: 33.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034415409736771466		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0034415409736771466 | validation: 0.00407733746670322]
	TIME [epoch: 34.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035066377655711303		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.0035066377655711303 | validation: 0.004435446720570237]
	TIME [epoch: 33.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030204787554635668		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.0030204787554635668 | validation: 0.002855410462697966]
	TIME [epoch: 33.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028293356416595793		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.0028293356416595793 | validation: 0.005699272125700356]
	TIME [epoch: 33.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003270597922524992		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.003270597922524992 | validation: 0.004723567936098355]
	TIME [epoch: 33.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035092461683676676		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.0035092461683676676 | validation: 0.003384668520305433]
	TIME [epoch: 33.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002882248166833289		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.002882248166833289 | validation: 0.0034475911342029056]
	TIME [epoch: 33.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002693277802902603		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.002693277802902603 | validation: 0.005654004690899654]
	TIME [epoch: 33.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037743155194105574		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0037743155194105574 | validation: 0.005704600515250632]
	TIME [epoch: 33.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003919017945074169		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.003919017945074169 | validation: 0.004611407749010066]
	TIME [epoch: 33.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034566556860496506		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.0034566556860496506 | validation: 0.005259190689307949]
	TIME [epoch: 33.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003650148023072758		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.003650148023072758 | validation: 0.0042832850214516854]
	TIME [epoch: 33.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002733659178010133		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.002733659178010133 | validation: 0.00432257899718064]
	TIME [epoch: 33.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002686561111807271		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.002686561111807271 | validation: 0.003461776198751803]
	TIME [epoch: 33.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030582534915946456		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.0030582534915946456 | validation: 0.004618479731652884]
	TIME [epoch: 33.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033260548885229615		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.0033260548885229615 | validation: 0.005797740789566484]
	TIME [epoch: 33.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003103618040679464		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.003103618040679464 | validation: 0.002972007502926498]
	TIME [epoch: 33.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032131055680665504		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.0032131055680665504 | validation: 0.0034472367748616026]
	TIME [epoch: 33.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003569862331838416		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.003569862331838416 | validation: 0.0038461610344004537]
	TIME [epoch: 33.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002086218947802908		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.002086218947802908 | validation: 0.002490181020851785]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027084730615284398		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.0027084730615284398 | validation: 0.00452639072334304]
	TIME [epoch: 33.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038996617008001887		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0038996617008001887 | validation: 0.0035039133343363416]
	TIME [epoch: 33.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025355463287956748		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.0025355463287956748 | validation: 0.004813262407343036]
	TIME [epoch: 33.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029224997758264593		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0029224997758264593 | validation: 0.003183168505674532]
	TIME [epoch: 33.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031414559900084307		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.0031414559900084307 | validation: 0.003931542018425673]
	TIME [epoch: 33.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026878646822560076		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0026878646822560076 | validation: 0.003191197788407825]
	TIME [epoch: 33.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002385744594747008		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.002385744594747008 | validation: 0.003371405736328874]
	TIME [epoch: 33.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032622652982520362		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0032622652982520362 | validation: 0.0031537506823919774]
	TIME [epoch: 33.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002868790544695073		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.002868790544695073 | validation: 0.003586107099383484]
	TIME [epoch: 33.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002879817742270086		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.002879817742270086 | validation: 0.002854690982124145]
	TIME [epoch: 33.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002567988604092792		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.002567988604092792 | validation: 0.0033707472655598245]
	TIME [epoch: 33.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00271101755726183		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.00271101755726183 | validation: 0.003669862472683685]
	TIME [epoch: 33.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035401189978565622		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0035401189978565622 | validation: 0.005544666741180004]
	TIME [epoch: 33.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030362419313674695		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0030362419313674695 | validation: 0.0039617380993013105]
	TIME [epoch: 33.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026083139812768923		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0026083139812768923 | validation: 0.0025718177137619754]
	TIME [epoch: 33.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002654432614785667		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.002654432614785667 | validation: 0.002782778094495084]
	TIME [epoch: 33.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028370399247497646		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.0028370399247497646 | validation: 0.0033469670528100694]
	TIME [epoch: 33.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038348355251382393		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.0038348355251382393 | validation: 0.0030493251769841915]
	TIME [epoch: 33.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022723417906081772		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.0022723417906081772 | validation: 0.004123815898939468]
	TIME [epoch: 33.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002538891346544993		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.002538891346544993 | validation: 0.003793003904954672]
	TIME [epoch: 33.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029126546556100183		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0029126546556100183 | validation: 0.004047142912284839]
	TIME [epoch: 33.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027996947974890754		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0027996947974890754 | validation: 0.004489989979258922]
	TIME [epoch: 33.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002517243805125237		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.002517243805125237 | validation: 0.0025993264575791706]
	TIME [epoch: 33.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024136611978211155		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.0024136611978211155 | validation: 0.00459408917446736]
	TIME [epoch: 33.7 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032543045129661222		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.0032543045129661222 | validation: 0.0032407244029922827]
	TIME [epoch: 33.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024310290905820695		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.0024310290905820695 | validation: 0.0022484183767534738]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024885424902333737		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0024885424902333737 | validation: 0.004096436357337073]
	TIME [epoch: 33.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033396309636279675		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0033396309636279675 | validation: 0.002599990559887527]
	TIME [epoch: 33.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019379280214369818		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0019379280214369818 | validation: 0.0026047555507014205]
	TIME [epoch: 33.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022472569723586007		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.0022472569723586007 | validation: 0.0028948804456568437]
	TIME [epoch: 33.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023792058196921663		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.0023792058196921663 | validation: 0.004932418325105551]
	TIME [epoch: 33.7 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035306399038201215		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0035306399038201215 | validation: 0.0024515165984493013]
	TIME [epoch: 33.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002141005322045437		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.002141005322045437 | validation: 0.002291562481199767]
	TIME [epoch: 33.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023788005228227143		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0023788005228227143 | validation: 0.0029641609265828937]
	TIME [epoch: 33.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025475092633180877		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0025475092633180877 | validation: 0.002236815002026229]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002697545926409555		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.002697545926409555 | validation: 0.0050708628765897155]
	TIME [epoch: 33.7 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002817722977987785		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.002817722977987785 | validation: 0.004390702118971113]
	TIME [epoch: 33.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003542669039452141		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.003542669039452141 | validation: 0.002732066966236711]
	TIME [epoch: 33.7 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001942081316753459		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.001942081316753459 | validation: 0.0017126516808385964]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032609698663038896		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0032609698663038896 | validation: 0.0030281944246595407]
	TIME [epoch: 33.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026777465060441505		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.0026777465060441505 | validation: 0.0033206752958122553]
	TIME [epoch: 33.7 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022946562261877733		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0022946562261877733 | validation: 0.0022678814989468485]
	TIME [epoch: 33.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027507790116239866		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0027507790116239866 | validation: 0.002361554022178785]
	TIME [epoch: 33.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002453141900207977		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.002453141900207977 | validation: 0.0035029856654605774]
	TIME [epoch: 33.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002577957802164296		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.002577957802164296 | validation: 0.0021010351563322837]
	TIME [epoch: 33.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021657181109538083		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.0021657181109538083 | validation: 0.0037158620991827153]
	TIME [epoch: 33.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002943936415402199		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.002943936415402199 | validation: 0.0031699676471929374]
	TIME [epoch: 33.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002327341161459667		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.002327341161459667 | validation: 0.002441778761077943]
	TIME [epoch: 33.7 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019524073244818498		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0019524073244818498 | validation: 0.0021319455902906004]
	TIME [epoch: 33.7 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018101970251558296		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0018101970251558296 | validation: 0.0028383230771448793]
	TIME [epoch: 33.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029831663722364055		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0029831663722364055 | validation: 0.002714646454610197]
	TIME [epoch: 33.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019828603226329376		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.0019828603226329376 | validation: 0.0045201042484516365]
	TIME [epoch: 33.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002368123572609503		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.002368123572609503 | validation: 0.003957991896689126]
	TIME [epoch: 33.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002462704059958733		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.002462704059958733 | validation: 0.003324940846494491]
	TIME [epoch: 33.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002290715583885999		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.002290715583885999 | validation: 0.0039525743576924325]
	TIME [epoch: 33.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022320933416070518		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0022320933416070518 | validation: 0.003051362536987794]
	TIME [epoch: 33.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017813174736770816		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.0017813174736770816 | validation: 0.002764018932073129]
	TIME [epoch: 33.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025205239403440706		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.0025205239403440706 | validation: 0.002245329768813317]
	TIME [epoch: 33.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00217099044057252		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.00217099044057252 | validation: 0.0020651975414013642]
	TIME [epoch: 33.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002758816251492722		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.002758816251492722 | validation: 0.00208621413185399]
	TIME [epoch: 33.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029819824710852773		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0029819824710852773 | validation: 0.0024962241864637663]
	TIME [epoch: 33.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022065357696165154		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0022065357696165154 | validation: 0.003011240892579956]
	TIME [epoch: 33.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023848221386590057		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.0023848221386590057 | validation: 0.002863108283887896]
	TIME [epoch: 33.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018172770373070977		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0018172770373070977 | validation: 0.0034555100629346733]
	TIME [epoch: 33.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002197504387360359		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.002197504387360359 | validation: 0.004016438283164785]
	TIME [epoch: 33.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022064240245594554		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.0022064240245594554 | validation: 0.002840992695455429]
	TIME [epoch: 33.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016834284890201272		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.0016834284890201272 | validation: 0.004447406913297775]
	TIME [epoch: 33.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002446442080203684		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.002446442080203684 | validation: 0.0020714632506741585]
	TIME [epoch: 33.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002023841779449643		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.002023841779449643 | validation: 0.002802718063701465]
	TIME [epoch: 33.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020152644940433975		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0020152644940433975 | validation: 0.002249037487225172]
	TIME [epoch: 33.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028011493349059777		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.0028011493349059777 | validation: 0.002966890712896613]
	TIME [epoch: 33.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002213104297405307		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.002213104297405307 | validation: 0.0016345912774454164]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002353259003744562		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.002353259003744562 | validation: 0.0028796669360998985]
	TIME [epoch: 33.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048696756726351485		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.0048696756726351485 | validation: 0.0042540169454548145]
	TIME [epoch: 33.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003024405643062376		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.003024405643062376 | validation: 0.002019103089939183]
	TIME [epoch: 33.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021887785010684183		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.0021887785010684183 | validation: 0.002391194984466524]
	TIME [epoch: 33.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021019238498695757		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.0021019238498695757 | validation: 0.0024076900363991308]
	TIME [epoch: 33.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018207029470698269		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0018207029470698269 | validation: 0.002101120617557684]
	TIME [epoch: 33.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016783852379516086		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.0016783852379516086 | validation: 0.003348402662417523]
	TIME [epoch: 33.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002254773330400605		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.002254773330400605 | validation: 0.0018021121445335525]
	TIME [epoch: 33.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016864750989927027		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.0016864750989927027 | validation: 0.003995233367669321]
	TIME [epoch: 33.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024965801399987417		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.0024965801399987417 | validation: 0.002154837621731527]
	TIME [epoch: 33.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021118887212326284		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0021118887212326284 | validation: 0.0020280314704186095]
	TIME [epoch: 33.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018316012550516373		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.0018316012550516373 | validation: 0.0019625507609737705]
	TIME [epoch: 33.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001588721157601655		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.001588721157601655 | validation: 0.001701990072176206]
	TIME [epoch: 33.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019152958940041446		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0019152958940041446 | validation: 0.003056527045181923]
	TIME [epoch: 33.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017733063756308377		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0017733063756308377 | validation: 0.0028944876700404596]
	TIME [epoch: 33.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022824953701112436		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0022824953701112436 | validation: 0.003749616375053224]
	TIME [epoch: 33.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022482793407808486		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0022482793407808486 | validation: 0.0019950090693425216]
	TIME [epoch: 33.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016364857392030532		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.0016364857392030532 | validation: 0.0020486587683382646]
	TIME [epoch: 33.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002256531073747894		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.002256531073747894 | validation: 0.002618994327961339]
	TIME [epoch: 33.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001636571011521729		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.001636571011521729 | validation: 0.002356378111228075]
	TIME [epoch: 33.7 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016452460678145404		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0016452460678145404 | validation: 0.0023803285160144466]
	TIME [epoch: 33.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019125826027487822		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.0019125826027487822 | validation: 0.0018504482843694874]
	TIME [epoch: 33.7 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022717268400162614		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.0022717268400162614 | validation: 0.005504604503161604]
	TIME [epoch: 33.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034680243879984017		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0034680243879984017 | validation: 0.0024237366423725845]
	TIME [epoch: 33.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017998302969314877		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0017998302969314877 | validation: 0.0022803175779815285]
	TIME [epoch: 33.7 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018632571214975515		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0018632571214975515 | validation: 0.001575262607456576]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015643721137669783		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0015643721137669783 | validation: 0.0021251703987739835]
	TIME [epoch: 33.7 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014764533176456892		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.0014764533176456892 | validation: 0.002143515618721515]
	TIME [epoch: 33.7 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002129867945815511		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.002129867945815511 | validation: 0.0026415047736470508]
	TIME [epoch: 33.7 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018132535985668878		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0018132535985668878 | validation: 0.0021064209026888615]
	TIME [epoch: 33.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014939981834287833		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0014939981834287833 | validation: 0.0021854778718349665]
	TIME [epoch: 33.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017646018159825675		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.0017646018159825675 | validation: 0.0022311506983610056]
	TIME [epoch: 33.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018778091318528566		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0018778091318528566 | validation: 0.0022619051891643533]
	TIME [epoch: 33.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018956879244777217		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0018956879244777217 | validation: 0.0017328444569133482]
	TIME [epoch: 33.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018865696151536902		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0018865696151536902 | validation: 0.0033668138748304734]
	TIME [epoch: 33.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018569637632587146		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0018569637632587146 | validation: 0.0025300690291189367]
	TIME [epoch: 33.7 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015746843035829442		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0015746843035829442 | validation: 0.0019688941219871713]
	TIME [epoch: 33.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001987115599611009		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.001987115599611009 | validation: 0.0022990579535234782]
	TIME [epoch: 33.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002063874758415951		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.002063874758415951 | validation: 0.002160589411052573]
	TIME [epoch: 33.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001942673374690264		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.001942673374690264 | validation: 0.002215511636917275]
	TIME [epoch: 33.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015239064361287067		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0015239064361287067 | validation: 0.0013371251237981757]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002007897512159048		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.002007897512159048 | validation: 0.0013652671266972663]
	TIME [epoch: 33.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017355575165647786		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0017355575165647786 | validation: 0.0019218614800811063]
	TIME [epoch: 33.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001404908868671385		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.001404908868671385 | validation: 0.001979342888991658]
	TIME [epoch: 33.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013464161174857108		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0013464161174857108 | validation: 0.0021597810935460445]
	TIME [epoch: 33.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017121554740774736		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0017121554740774736 | validation: 0.0021079926665177914]
	TIME [epoch: 33.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015464111110583137		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.0015464111110583137 | validation: 0.0012693396409552574]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001306145134348923		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.001306145134348923 | validation: 0.0021861227713449063]
	TIME [epoch: 33.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001381773511747874		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.001381773511747874 | validation: 0.0015499091301553927]
	TIME [epoch: 33.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001613394998937214		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.001613394998937214 | validation: 0.002173460085771292]
	TIME [epoch: 33.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017274297673807318		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.0017274297673807318 | validation: 0.001741157964284686]
	TIME [epoch: 33.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001535169333404121		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.001535169333404121 | validation: 0.0021902857937845326]
	TIME [epoch: 33.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002527854066470731		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.002527854066470731 | validation: 0.0026003380052107596]
	TIME [epoch: 33.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018340612776814246		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0018340612776814246 | validation: 0.002064195898101412]
	TIME [epoch: 33.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017718747013165603		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0017718747013165603 | validation: 0.002298360385820218]
	TIME [epoch: 33.7 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001804275358476221		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.001804275358476221 | validation: 0.0017869698949280412]
	TIME [epoch: 33.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012889808207945746		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0012889808207945746 | validation: 0.0021726123626679542]
	TIME [epoch: 33.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002166149682131022		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.002166149682131022 | validation: 0.0017908842803780675]
	TIME [epoch: 33.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014538916789395462		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0014538916789395462 | validation: 0.0017029974786953853]
	TIME [epoch: 33.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016391570689386504		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.0016391570689386504 | validation: 0.0022945342989260566]
	TIME [epoch: 33.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013918596461398125		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0013918596461398125 | validation: 0.002192603377132026]
	TIME [epoch: 33.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016520952108537135		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0016520952108537135 | validation: 0.001355297770271486]
	TIME [epoch: 33.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001722347970519943		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.001722347970519943 | validation: 0.001692891550915168]
	TIME [epoch: 33.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001162658534736352		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.001162658534736352 | validation: 0.0013470676015454872]
	TIME [epoch: 33.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016245009890863895		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0016245009890863895 | validation: 0.0017013567412934565]
	TIME [epoch: 33.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013864341450202297		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.0013864341450202297 | validation: 0.0023887318938820884]
	TIME [epoch: 33.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016870251412015062		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0016870251412015062 | validation: 0.0024164427796173086]
	TIME [epoch: 33.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016331157763039988		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.0016331157763039988 | validation: 0.0022577066501324617]
	TIME [epoch: 33.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016085319229139417		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.0016085319229139417 | validation: 0.0014209049208835484]
	TIME [epoch: 33.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014564569474048786		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0014564569474048786 | validation: 0.0021816331831115755]
	TIME [epoch: 33.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017893298207303293		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0017893298207303293 | validation: 0.000971531450680348]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013414961807163392		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0013414961807163392 | validation: 0.0013123907167508736]
	TIME [epoch: 33.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000915332624941716		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.000915332624941716 | validation: 0.0019350374928175037]
	TIME [epoch: 33.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016471575739575948		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0016471575739575948 | validation: 0.001612051671255216]
	TIME [epoch: 33.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0019146459603971056		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0019146459603971056 | validation: 0.0016533045861579314]
	TIME [epoch: 33.7 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014287495253209864		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.0014287495253209864 | validation: 0.0014055454455414828]
	TIME [epoch: 33.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00162265291867845		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.00162265291867845 | validation: 0.0018789234095214617]
	TIME [epoch: 33.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011525388713700903		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0011525388713700903 | validation: 0.002146958646143436]
	TIME [epoch: 33.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011974272299959707		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.0011974272299959707 | validation: 0.0013683425455666837]
	TIME [epoch: 33.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013106984938175065		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.0013106984938175065 | validation: 0.002132672688422261]
	TIME [epoch: 33.7 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016014146230983947		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0016014146230983947 | validation: 0.0016568165419249708]
	TIME [epoch: 33.7 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011518473548463649		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0011518473548463649 | validation: 0.0019429832547330852]
	TIME [epoch: 33.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010588924514744308		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0010588924514744308 | validation: 0.0011342771599731406]
	TIME [epoch: 33.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013935194104592319		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0013935194104592319 | validation: 0.0011556220333594923]
	TIME [epoch: 33.7 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011850770041338988		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0011850770041338988 | validation: 0.0024176786114577823]
	TIME [epoch: 33.7 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016670609815660799		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0016670609815660799 | validation: 0.0011282310125306622]
	TIME [epoch: 33.7 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012300174818946628		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.0012300174818946628 | validation: 0.00199534110724123]
	TIME [epoch: 33.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001822745447652016		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.001822745447652016 | validation: 0.001471871626555588]
	TIME [epoch: 33.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017768529284889196		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.0017768529284889196 | validation: 0.0014351732522903805]
	TIME [epoch: 33.7 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011178782455855297		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0011178782455855297 | validation: 0.0013307021502259148]
	TIME [epoch: 33.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001393984175774631		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.001393984175774631 | validation: 0.0014837046526954812]
	TIME [epoch: 33.7 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001307753512498108		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.001307753512498108 | validation: 0.0009830589090703166]
	TIME [epoch: 33.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011072005706719303		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0011072005706719303 | validation: 0.00206402805863184]
	TIME [epoch: 33.7 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009905997874913623		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0009905997874913623 | validation: 0.001380893049838024]
	TIME [epoch: 33.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010724894527389325		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0010724894527389325 | validation: 0.0015754244418196856]
	TIME [epoch: 33.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012902657892252115		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.0012902657892252115 | validation: 0.0009247681091285153]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_917.pth
	Model improved!!!
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011370597417122274		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0011370597417122274 | validation: 0.0013914732712575484]
	TIME [epoch: 33.7 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011963055349712597		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0011963055349712597 | validation: 0.0015047855164009435]
	TIME [epoch: 33.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014824965707921085		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0014824965707921085 | validation: 0.0010709759599089948]
	TIME [epoch: 33.7 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013627312240477051		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0013627312240477051 | validation: 0.0011050724262900618]
	TIME [epoch: 33.7 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011722278288526903		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0011722278288526903 | validation: 0.0012361600486029457]
	TIME [epoch: 33.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008420498019166287		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.0008420498019166287 | validation: 0.000683075125105331]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011347653278550363		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0011347653278550363 | validation: 0.001199665003054501]
	TIME [epoch: 33.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013774300486653005		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0013774300486653005 | validation: 0.0010737604744132786]
	TIME [epoch: 33.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008269418961365474		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0008269418961365474 | validation: 0.0017412644067759304]
	TIME [epoch: 33.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001258078553752086		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.001258078553752086 | validation: 0.0013871516026259592]
	TIME [epoch: 33.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001587627513500836		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.001587627513500836 | validation: 0.001999327877605669]
	TIME [epoch: 33.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011311864941483964		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0011311864941483964 | validation: 0.001458464507254825]
	TIME [epoch: 33.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011911127870670147		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0011911127870670147 | validation: 0.0012774942617088591]
	TIME [epoch: 33.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010351850128482707		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.0010351850128482707 | validation: 0.000969520838400343]
	TIME [epoch: 33.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012349790541629354		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.0012349790541629354 | validation: 0.0017848191599142051]
	TIME [epoch: 33.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013819885141650255		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0013819885141650255 | validation: 0.001080677779741439]
	TIME [epoch: 33.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013019978732815895		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.0013019978732815895 | validation: 0.001490518677508919]
	TIME [epoch: 33.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011328019031472312		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0011328019031472312 | validation: 0.0014707340430506308]
	TIME [epoch: 33.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008596252632211196		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0008596252632211196 | validation: 0.001149060419741776]
	TIME [epoch: 33.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001080132479109647		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.001080132479109647 | validation: 0.0010283745757320012]
	TIME [epoch: 33.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012228071318747316		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0012228071318747316 | validation: 0.0018311468914870856]
	TIME [epoch: 33.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007254837688520617		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0007254837688520617 | validation: 0.001478819166861876]
	TIME [epoch: 33.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010364667701224296		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0010364667701224296 | validation: 0.0017146687415045365]
	TIME [epoch: 33.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014538341342040875		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0014538341342040875 | validation: 0.0019228374316154159]
	TIME [epoch: 33.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007975150129424142		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0007975150129424142 | validation: 0.0012169856912617014]
	TIME [epoch: 33.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008488638249296685		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0008488638249296685 | validation: 0.001634243028727573]
	TIME [epoch: 33.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000842847840644847		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.000842847840644847 | validation: 0.00202847878024501]
	TIME [epoch: 33.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00099673424736579		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.00099673424736579 | validation: 0.0020179708696379957]
	TIME [epoch: 33.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011326701355247298		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0011326701355247298 | validation: 0.0016157362913383163]
	TIME [epoch: 33.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016595954233748795		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.0016595954233748795 | validation: 0.0014474864495040533]
	TIME [epoch: 33.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011474802716511608		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.0011474802716511608 | validation: 0.000742000320433565]
	TIME [epoch: 33.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009584860203449944		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0009584860203449944 | validation: 0.0016807994881547542]
	TIME [epoch: 33.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001204399695357108		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.001204399695357108 | validation: 0.0008461768295985569]
	TIME [epoch: 33.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013357715026541188		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0013357715026541188 | validation: 0.0010198189030151184]
	TIME [epoch: 33.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001224346648000114		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.001224346648000114 | validation: 0.0013311209900483433]
	TIME [epoch: 33.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007626213882971053		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0007626213882971053 | validation: 0.000846679756035738]
	TIME [epoch: 33.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014159753900816377		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0014159753900816377 | validation: 0.0016339724817673665]
	TIME [epoch: 33.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001320621015233029		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.001320621015233029 | validation: 0.0011040598519502867]
	TIME [epoch: 33.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013222834012792824		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0013222834012792824 | validation: 0.0012408082969594353]
	TIME [epoch: 33.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007144472104510349		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.0007144472104510349 | validation: 0.0006645299448523056]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013131407900518954		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0013131407900518954 | validation: 0.001301461333468561]
	TIME [epoch: 33.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013197370978882568		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0013197370978882568 | validation: 0.0017341044362583543]
	TIME [epoch: 33.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009178368605698259		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.0009178368605698259 | validation: 0.0010248444602369898]
	TIME [epoch: 33.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000903927404504648		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.000903927404504648 | validation: 0.0013990239682537427]
	TIME [epoch: 33.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007559411791397185		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0007559411791397185 | validation: 0.0013178025201125623]
	TIME [epoch: 33.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010562500995865308		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.0010562500995865308 | validation: 0.0007028654308895953]
	TIME [epoch: 33.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010231658053555374		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0010231658053555374 | validation: 0.002703980622517689]
	TIME [epoch: 33.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010192562691275044		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0010192562691275044 | validation: 0.0014014053934876297]
	TIME [epoch: 33.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010613812145365438		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0010613812145365438 | validation: 0.001054237913549251]
	TIME [epoch: 33.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013183485841186376		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0013183485841186376 | validation: 0.001015898912351596]
	TIME [epoch: 33.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011278631378732658		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0011278631378732658 | validation: 0.002101415989412341]
	TIME [epoch: 33.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010155296278548077		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0010155296278548077 | validation: 0.001096973108356737]
	TIME [epoch: 33.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010310005618915843		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0010310005618915843 | validation: 0.0007659622693020074]
	TIME [epoch: 33.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010969090695296944		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0010969090695296944 | validation: 0.001333522595747417]
	TIME [epoch: 33.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010519118799515908		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0010519118799515908 | validation: 0.0009421668482171306]
	TIME [epoch: 33.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007506196321896905		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0007506196321896905 | validation: 0.0018662168129181343]
	TIME [epoch: 33.7 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013466699346270684		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0013466699346270684 | validation: 0.001497034179632295]
	TIME [epoch: 33.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008037980619162458		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0008037980619162458 | validation: 0.001120959189964246]
	TIME [epoch: 33.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010233273139672434		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0010233273139672434 | validation: 0.0007790053679784484]
	TIME [epoch: 33.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014549616763921275		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0014549616763921275 | validation: 0.001279259238884613]
	TIME [epoch: 33.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007891199725759062		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0007891199725759062 | validation: 0.0006559634766745051]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_978.pth
	Model improved!!!
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006105026634588782		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0006105026634588782 | validation: 0.0014672538685082223]
	TIME [epoch: 33.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008631622998294185		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.0008631622998294185 | validation: 0.0006368175319337874]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_980.pth
	Model improved!!!
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007139258244333218		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0007139258244333218 | validation: 0.0009657803926186616]
	TIME [epoch: 33.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008563014756953416		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0008563014756953416 | validation: 0.0018076064634022782]
	TIME [epoch: 33.7 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011780236259533018		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0011780236259533018 | validation: 0.0014149873617344123]
	TIME [epoch: 33.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011094326121880976		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0011094326121880976 | validation: 0.0008558981069587563]
	TIME [epoch: 33.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007357791554843976		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0007357791554843976 | validation: 0.0009050176326630366]
	TIME [epoch: 33.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008999268622901724		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.0008999268622901724 | validation: 0.0016623195779516618]
	TIME [epoch: 33.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000856025449577644		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.000856025449577644 | validation: 0.0014048888859581946]
	TIME [epoch: 33.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010717151022919629		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0010717151022919629 | validation: 0.0006123934953547056]
	TIME [epoch: 33.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006474406158898512		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0006474406158898512 | validation: 0.0007726527895017492]
	TIME [epoch: 33.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006178203407406948		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0006178203407406948 | validation: 0.0013062761163911319]
	TIME [epoch: 33.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008206285537333044		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0008206285537333044 | validation: 0.0012654546840850012]
	TIME [epoch: 33.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009208522375010703		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0009208522375010703 | validation: 0.0016544433132509137]
	TIME [epoch: 33.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009584493980223515		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0009584493980223515 | validation: 0.0010919021221939768]
	TIME [epoch: 33.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008196602310891124		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.0008196602310891124 | validation: 0.0009639908379619345]
	TIME [epoch: 33.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005786455699697784		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0005786455699697784 | validation: 0.0018592122637868407]
	TIME [epoch: 33.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008608795918611452		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0008608795918611452 | validation: 0.001049398469811317]
	TIME [epoch: 33.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010443544977913415		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0010443544977913415 | validation: 0.0009343755008770138]
	TIME [epoch: 33.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005794107565133667		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0005794107565133667 | validation: 0.001181211030187578]
	TIME [epoch: 33.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008369359582670515		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.0008369359582670515 | validation: 0.0005640116942721302]
	TIME [epoch: 33.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011474822904628049		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.0011474822904628049 | validation: 0.0010545522486806842]
	TIME [epoch: 33.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010746834491175243		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0010746834491175243 | validation: 0.0014917624299271405]
	TIME [epoch: 87.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006879914833374141		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0006879914833374141 | validation: 0.0002514827706522329]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007877050403099172		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0007877050403099172 | validation: 0.000918327525749846]
	TIME [epoch: 71.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005097555554265781		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0005097555554265781 | validation: 0.0012690542885527628]
	TIME [epoch: 71.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008166603944415593		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0008166603944415593 | validation: 0.0009094109932629096]
	TIME [epoch: 71.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007503344345463286		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0007503344345463286 | validation: 0.0005520582611298784]
	TIME [epoch: 71.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000570875149901757		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.000570875149901757 | validation: 0.001444846983389497]
	TIME [epoch: 71.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006927335365796694		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0006927335365796694 | validation: 0.0012864733868459437]
	TIME [epoch: 71.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006019968005454333		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.0006019968005454333 | validation: 0.0014407999388196977]
	TIME [epoch: 71.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009060470568449761		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0009060470568449761 | validation: 0.00045128048380101404]
	TIME [epoch: 71.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007023952086598564		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.0007023952086598564 | validation: 0.0012993515990412032]
	TIME [epoch: 71.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008912093282749973		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0008912093282749973 | validation: 0.0015993251409801685]
	TIME [epoch: 71.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000732794811755084		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.000732794811755084 | validation: 0.001308775098306856]
	TIME [epoch: 71.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006855619804857096		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0006855619804857096 | validation: 0.0011007280281255447]
	TIME [epoch: 71.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006013244892643062		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0006013244892643062 | validation: 0.0017081832520087455]
	TIME [epoch: 71.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006692175238201667		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0006692175238201667 | validation: 0.0010476210515588935]
	TIME [epoch: 71.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011828576791197967		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0011828576791197967 | validation: 0.0017167940622420696]
	TIME [epoch: 71.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008520256893452057		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0008520256893452057 | validation: 0.0007402574364563152]
	TIME [epoch: 71.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000400222666054288		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.000400222666054288 | validation: 0.0008287701595958117]
	TIME [epoch: 71.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007644886181097801		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.0007644886181097801 | validation: 0.0006859920419608159]
	TIME [epoch: 71.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000679582124682911		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.000679582124682911 | validation: 0.0010156091370722056]
	TIME [epoch: 71.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000682595935938394		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.000682595935938394 | validation: 0.0013523629187067784]
	TIME [epoch: 71.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005390615355110844		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0005390615355110844 | validation: 0.0017912768626273208]
	TIME [epoch: 71.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007358048996622864		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0007358048996622864 | validation: 0.0011477884652472243]
	TIME [epoch: 71.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005394214870696095		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0005394214870696095 | validation: 0.0008306332915382609]
	TIME [epoch: 71.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008928805261114045		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.0008928805261114045 | validation: 0.0009925857110505395]
	TIME [epoch: 71.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005509006301370327		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0005509006301370327 | validation: 0.0008028850760553796]
	TIME [epoch: 71.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005310597333641174		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0005310597333641174 | validation: 0.0004250339035736328]
	TIME [epoch: 71.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008308331622506455		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0008308331622506455 | validation: 0.0011815591504131008]
	TIME [epoch: 71.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006423134045612585		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0006423134045612585 | validation: 0.000859722724145084]
	TIME [epoch: 71.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000771022339461628		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.000771022339461628 | validation: 0.0007958976404620897]
	TIME [epoch: 71.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041058767529953896		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.00041058767529953896 | validation: 0.0009332882558917816]
	TIME [epoch: 71.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006920446135066285		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0006920446135066285 | validation: 0.00019526315818240824]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008104466733687508		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.0008104466733687508 | validation: 0.0010528078574266245]
	TIME [epoch: 71.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005357631724875138		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0005357631724875138 | validation: 0.0008477059836834223]
	TIME [epoch: 71.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007525300124574848		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0007525300124574848 | validation: 0.000514512116254962]
	TIME [epoch: 71.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007247531355400328		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0007247531355400328 | validation: 0.0019944956485548645]
	TIME [epoch: 71.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005044849012995527		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0005044849012995527 | validation: 0.000747588466506592]
	TIME [epoch: 71.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044418017942545075		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.00044418017942545075 | validation: 0.00082312878012878]
	TIME [epoch: 71.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010252180355663281		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0010252180355663281 | validation: 0.0009611944590058581]
	TIME [epoch: 71.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007565036530987917		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0007565036530987917 | validation: 0.0009303025195785128]
	TIME [epoch: 71.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009836771353452952		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0009836771353452952 | validation: 0.0007359620569059446]
	TIME [epoch: 71.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005383515689705523		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0005383515689705523 | validation: 0.0006626524321156734]
	TIME [epoch: 71.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006724049362572333		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0006724049362572333 | validation: 0.00028816458499715655]
	TIME [epoch: 71.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006957153250755677		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0006957153250755677 | validation: 0.0006828670642250456]
	TIME [epoch: 71.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005039991388897072		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0005039991388897072 | validation: 0.000538426834025688]
	TIME [epoch: 71.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00052517980664016		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.00052517980664016 | validation: 0.0004193938584803716]
	TIME [epoch: 71.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003393422200148131		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0003393422200148131 | validation: 0.0006883725594251341]
	TIME [epoch: 71.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006353132892019458		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.0006353132892019458 | validation: 0.0007887243472520051]
	TIME [epoch: 71.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006979175305255066		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.0006979175305255066 | validation: 0.0012642650034508573]
	TIME [epoch: 71.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014100317516648146		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0014100317516648146 | validation: 0.0013455119424465752]
	TIME [epoch: 71.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004157284715446241		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0004157284715446241 | validation: 0.0008893166241190507]
	TIME [epoch: 71.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036026142465432696		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.00036026142465432696 | validation: 0.0011375031320403385]
	TIME [epoch: 71.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006207831990785917		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0006207831990785917 | validation: 0.00036385093009110215]
	TIME [epoch: 71.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004408630287208226		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0004408630287208226 | validation: 0.0005972240389320352]
	TIME [epoch: 71.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008085711098571261		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0008085711098571261 | validation: 0.0008436152659432167]
	TIME [epoch: 71.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005660185796821525		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0005660185796821525 | validation: 0.002978307885985491]
	TIME [epoch: 71.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013984323517550513		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0013984323517550513 | validation: 0.00046254726120882866]
	TIME [epoch: 71.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004693040553814514		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0004693040553814514 | validation: 0.0012761435785796581]
	TIME [epoch: 71.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030326526638468286		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.00030326526638468286 | validation: 0.00035900913050678174]
	TIME [epoch: 71.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037724510433534625		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.00037724510433534625 | validation: 0.0004737456185388158]
	TIME [epoch: 71.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035396747682872576		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.00035396747682872576 | validation: 0.0008944423429393967]
	TIME [epoch: 71.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006008494102192951		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0006008494102192951 | validation: 0.001023541494565457]
	TIME [epoch: 71.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006492177677092248		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0006492177677092248 | validation: 0.000871924207229509]
	TIME [epoch: 71.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005982783871420921		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0005982783871420921 | validation: 0.0010033081195945446]
	TIME [epoch: 71.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005660305885229258		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0005660305885229258 | validation: 0.0011181867035839179]
	TIME [epoch: 71.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000475345669534887		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.000475345669534887 | validation: 0.00018736139590613376]
	TIME [epoch: 71.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004848164874928709		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0004848164874928709 | validation: 0.000964720669365505]
	TIME [epoch: 71.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005339423413335928		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.0005339423413335928 | validation: 0.0005119880426297776]
	TIME [epoch: 71.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035903630798136255		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.00035903630798136255 | validation: 0.0007981632517368321]
	TIME [epoch: 71.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032176539587084577		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.00032176539587084577 | validation: 0.0006603473090187633]
	TIME [epoch: 71.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006600573441888545		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0006600573441888545 | validation: 0.001820951704098924]
	TIME [epoch: 71.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037207782258017044		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.00037207782258017044 | validation: 0.00174491142187751]
	TIME [epoch: 71.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007062815288495781		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0007062815288495781 | validation: 0.0010837187227111952]
	TIME [epoch: 71.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006307914035398966		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0006307914035398966 | validation: 1.600735101817552e-05]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1075.pth
	Model improved!!!
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005232309665701192		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0005232309665701192 | validation: 0.00026380336782993205]
	TIME [epoch: 71.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004401392385434437		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0004401392385434437 | validation: 0.0006442176458924749]
	TIME [epoch: 71.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004175877118227971		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0004175877118227971 | validation: 0.0002537592179674624]
	TIME [epoch: 71.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005868612619973919		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0005868612619973919 | validation: 0.00013467225632818813]
	TIME [epoch: 71.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005014749291644176		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.0005014749291644176 | validation: 0.0006070001982288789]
	TIME [epoch: 71.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047984608466110855		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.00047984608466110855 | validation: 0.0013334426820250825]
	TIME [epoch: 71.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00052818295070063		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.00052818295070063 | validation: 0.0008782468401758612]
	TIME [epoch: 71.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000488995739276376		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.000488995739276376 | validation: 0.0008418150762098496]
	TIME [epoch: 71.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005376333960025695		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0005376333960025695 | validation: 0.0010121844861363796]
	TIME [epoch: 71.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039939490543418923		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.00039939490543418923 | validation: 0.00036756268595992036]
	TIME [epoch: 71.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003368058181941209		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0003368058181941209 | validation: 0.0005025065003686012]
	TIME [epoch: 71.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003940613001222539		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0003940613001222539 | validation: 0.00031529220198382556]
	TIME [epoch: 71.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007080082670769314		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0007080082670769314 | validation: 0.0003241100029993396]
	TIME [epoch: 71.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043659923648845834		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.00043659923648845834 | validation: 0.00015397425693071208]
	TIME [epoch: 71.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004203346079031778		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0004203346079031778 | validation: 0.001454003184001775]
	TIME [epoch: 71.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008312991426894443		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0008312991426894443 | validation: 0.0002889777795495361]
	TIME [epoch: 71.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001429379205502341		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.001429379205502341 | validation: 0.002231686754583218]
	TIME [epoch: 71.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013412631480410244		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0013412631480410244 | validation: 0.00040109071312309475]
	TIME [epoch: 71.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007181146405561302		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0007181146405561302 | validation: 0.0008118515036599492]
	TIME [epoch: 71.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043211784234455		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.00043211784234455 | validation: 0.001271226539970562]
	TIME [epoch: 71.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006581388396376081		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.0006581388396376081 | validation: 0.0010818647371928173]
	TIME [epoch: 71.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002928740617162593		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0002928740617162593 | validation: 0.00031219612028528763]
	TIME [epoch: 71.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043615957945283905		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.00043615957945283905 | validation: 0.0004423513218958828]
	TIME [epoch: 71.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005914349235379074		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0005914349235379074 | validation: 0.0009111613094385174]
	TIME [epoch: 71.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006983860212017453		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0006983860212017453 | validation: 0.0002344476037851004]
	TIME [epoch: 71.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004973774281252925		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0004973774281252925 | validation: 0.0005556824939462119]
	TIME [epoch: 71.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005312357243811223		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.0005312357243811223 | validation: 0.0006861833205797554]
	TIME [epoch: 71.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031392957530249776		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.00031392957530249776 | validation: 0.00036292552022703983]
	TIME [epoch: 71.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003143386021605814		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0003143386021605814 | validation: 0.0002539218517007713]
	TIME [epoch: 71.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011322762602779958		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.00011322762602779958 | validation: 0.0009307192660026563]
	TIME [epoch: 71.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019194076367526924		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.00019194076367526924 | validation: -8.887862421432935e-05]
	TIME [epoch: 71.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1106.pth
	Model improved!!!
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003916002017743512		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0003916002017743512 | validation: 0.000720611947517857]
	TIME [epoch: 71.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006758338344426949		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0006758338344426949 | validation: 0.0005222097356591435]
	TIME [epoch: 71.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009216300122529813		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0009216300122529813 | validation: 0.00036553620400212505]
	TIME [epoch: 71.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007844946408539009		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0007844946408539009 | validation: 0.0009261309381410304]
	TIME [epoch: 71.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005713549843925659		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.0005713549843925659 | validation: 0.0008281547739382234]
	TIME [epoch: 71.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005004491912150742		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0005004491912150742 | validation: 0.00044176917514080134]
	TIME [epoch: 71.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031232859022460493		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.00031232859022460493 | validation: 0.0009488985871464753]
	TIME [epoch: 71.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005098986709634998		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.0005098986709634998 | validation: 0.0005208493830188497]
	TIME [epoch: 71.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001816512422983716		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0001816512422983716 | validation: 0.0005948165242349149]
	TIME [epoch: 71.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005867837920648018		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0005867837920648018 | validation: 0.0011590698649337225]
	TIME [epoch: 71.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027084128621812154		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.00027084128621812154 | validation: 0.0013788488454762353]
	TIME [epoch: 71.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005856092426485891		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0005856092426485891 | validation: 0.000535731569922941]
	TIME [epoch: 71.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004202860449125727		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0004202860449125727 | validation: 0.0007610524842197881]
	TIME [epoch: 71.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044861455713780617		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.00044861455713780617 | validation: 0.0008877448446323699]
	TIME [epoch: 71.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005240197805859661		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0005240197805859661 | validation: 0.0002692035913647608]
	TIME [epoch: 71.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002086271108457891		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0002086271108457891 | validation: 0.0007844652574035767]
	TIME [epoch: 71.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000331755498478848		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.000331755498478848 | validation: 0.00013483023768638079]
	TIME [epoch: 71.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020939878185974873		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.00020939878185974873 | validation: 0.0003373611364942759]
	TIME [epoch: 71.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037871798096199803		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.00037871798096199803 | validation: 0.0012278053285260616]
	TIME [epoch: 71.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002946665043050527		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.0002946665043050527 | validation: 0.0008208576174023694]
	TIME [epoch: 71.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005484141928096016		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0005484141928096016 | validation: 0.0001515273375114159]
	TIME [epoch: 71.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002164427727307621		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0002164427727307621 | validation: 0.0006616001120468473]
	TIME [epoch: 71.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002367741853366876		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0002367741853366876 | validation: 0.0006496836864898095]
	TIME [epoch: 71.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006433421878449606		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.0006433421878449606 | validation: 0.0005459962608935563]
	TIME [epoch: 71.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004237753958357955		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0004237753958357955 | validation: 0.0007023729544527307]
	TIME [epoch: 71.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003368639152917914		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0003368639152917914 | validation: 0.0016714957587770584]
	TIME [epoch: 71.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006896543464354061		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0006896543464354061 | validation: 0.0004954811912430959]
	TIME [epoch: 71.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000275740524594547		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.000275740524594547 | validation: 0.0003087830993331062]
	TIME [epoch: 71.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010280088592357097		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.00010280088592357097 | validation: 0.001194583172907219]
	TIME [epoch: 71.2 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.500422157376074e-05		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 6.500422157376074e-05 | validation: 0.00045241780305882353]
	TIME [epoch: 71.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00035559373203522825		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.00035559373203522825 | validation: 0.001116660609145728]
	TIME [epoch: 71.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039119668678627885		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.00039119668678627885 | validation: 0.00023720223134158402]
	TIME [epoch: 71.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019428412049353374		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.00019428412049353374 | validation: 0.00037776935694455466]
	TIME [epoch: 71.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021501614469737328		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.00021501614469737328 | validation: 0.0005283293454764068]
	TIME [epoch: 71.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000273762108762909		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.000273762108762909 | validation: 0.0004896929529087002]
	TIME [epoch: 71.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037819070445570664		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.00037819070445570664 | validation: 0.00024801827448494597]
	TIME [epoch: 71.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041432533169821826		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.00041432533169821826 | validation: 0.0009038334786425875]
	TIME [epoch: 71.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006506218953968055		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0006506218953968055 | validation: 0.0005647565123703311]
	TIME [epoch: 71.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002465065233230377		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.0002465065233230377 | validation: 0.0006135247651998768]
	TIME [epoch: 71.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002888705057759504		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0002888705057759504 | validation: 0.0012331892892111557]
	TIME [epoch: 71.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023227597740742723		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.00023227597740742723 | validation: 0.0013123331455607214]
	TIME [epoch: 71.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003225985509984417		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0003225985509984417 | validation: 0.0005370113019580458]
	TIME [epoch: 71.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003547913697465279		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0003547913697465279 | validation: 0.0008673482717224692]
	TIME [epoch: 71.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026356505989892144		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.00026356505989892144 | validation: 0.0006468199076619365]
	TIME [epoch: 71.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037650490364842047		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.00037650490364842047 | validation: 0.0004205307953879673]
	TIME [epoch: 71.2 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002897606984131731		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.0002897606984131731 | validation: 0.0009441281502708954]
	TIME [epoch: 71.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005091510737592602		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0005091510737592602 | validation: 0.00024395795257142972]
	TIME [epoch: 71.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002250115752703399		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0002250115752703399 | validation: 0.0003234221648045619]
	TIME [epoch: 71.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013847925456084533		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.00013847925456084533 | validation: 0.0003691279682887156]
	TIME [epoch: 71.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003370503901061359		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.0003370503901061359 | validation: 7.335815928104328e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023393971053381767		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.00023393971053381767 | validation: 0.0012458934428833484]
	TIME [epoch: 71.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004787326561720496		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0004787326561720496 | validation: 0.0004517964339713396]
	TIME [epoch: 71.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005987667306987055		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0005987667306987055 | validation: 0.0007770350370872032]
	TIME [epoch: 71.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014535134698623797		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.00014535134698623797 | validation: 0.0003553599447011671]
	TIME [epoch: 71.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004147186204726057		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0004147186204726057 | validation: 0.000935642396847598]
	TIME [epoch: 71.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023985500813741625		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.00023985500813741625 | validation: 0.0005358750858348165]
	TIME [epoch: 71.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022641798011849312		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.00022641798011849312 | validation: 0.0006446670870394531]
	TIME [epoch: 71.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005085803610212773		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0005085803610212773 | validation: 0.0004806471977863724]
	TIME [epoch: 71.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002803258218011999		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0002803258218011999 | validation: 0.0006357624527945057]
	TIME [epoch: 71.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003774375470324047		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0003774375470324047 | validation: 0.00026270764751429266]
	TIME [epoch: 71.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001855272048028498		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0001855272048028498 | validation: 0.00040670840834511953]
	TIME [epoch: 71.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007768454961376243		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0007768454961376243 | validation: 0.00032310142644546055]
	TIME [epoch: 71.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00026407825927059523		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.00026407825927059523 | validation: 7.043704722817346e-06]
	TIME [epoch: 71.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003061217954700788		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0003061217954700788 | validation: 0.0006975523556842092]
	TIME [epoch: 71.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011783237052016249		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.00011783237052016249 | validation: 0.0003347590811238561]
	TIME [epoch: 71.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003907534377814859		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.0003907534377814859 | validation: -0.00010550399908255235]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1172.pth
	Model improved!!!
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003001217256995447		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0003001217256995447 | validation: 0.00033247858306357256]
	TIME [epoch: 71.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002973246782965493		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0002973246782965493 | validation: 0.0002814917334702107]
	TIME [epoch: 71.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004670989123496638		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0004670989123496638 | validation: 0.0003948041218554232]
	TIME [epoch: 71.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031395742998155597		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.00031395742998155597 | validation: 0.0001473043646659118]
	TIME [epoch: 71.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032739863236624304		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.00032739863236624304 | validation: 0.0010043886588740679]
	TIME [epoch: 71.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002606134862011642		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0002606134862011642 | validation: 0.0002872523649380634]
	TIME [epoch: 71.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6915189539404876e-05		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 4.6915189539404876e-05 | validation: 0.0013271399503580713]
	TIME [epoch: 71.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015590643717359434		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.00015590643717359434 | validation: 0.0002584809972643196]
	TIME [epoch: 71.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028179479281869994		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.00028179479281869994 | validation: 0.0006081864057286728]
	TIME [epoch: 71.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.299165311382838e-05		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 8.299165311382838e-05 | validation: 0.00040823303010979566]
	TIME [epoch: 71.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002687179534345929		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.0002687179534345929 | validation: 0.0002807338633212435]
	TIME [epoch: 71.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027773339950184345		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.00027773339950184345 | validation: 0.0002960027659386961]
	TIME [epoch: 71.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037288562126777094		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.00037288562126777094 | validation: 0.00019789344995271475]
	TIME [epoch: 71.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000652562656039094		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.000652562656039094 | validation: 0.0008025984293249532]
	TIME [epoch: 71.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038171630772851903		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.00038171630772851903 | validation: 0.0013110513662579564]
	TIME [epoch: 71.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3480988554034346e-05		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 1.3480988554034346e-05 | validation: 0.0006008137000265421]
	TIME [epoch: 71.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041709635005152774		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.00041709635005152774 | validation: 0.0008238135270675802]
	TIME [epoch: 71.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021484682622182282		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.00021484682622182282 | validation: 0.00018972383531503658]
	TIME [epoch: 71.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022367318129812717		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.00022367318129812717 | validation: 0.0011633957742800095]
	TIME [epoch: 71.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027818063155986544		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.00027818063155986544 | validation: 0.000843986096193393]
	TIME [epoch: 71.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027283200773059257		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.00027283200773059257 | validation: 0.00021427101612408707]
	TIME [epoch: 71.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017252257033916661		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.00017252257033916661 | validation: 0.00032713072134659885]
	TIME [epoch: 71.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038482026387932705		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.00038482026387932705 | validation: 0.00021271197242146388]
	TIME [epoch: 71.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002506207629009771		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0002506207629009771 | validation: 0.000908076729558105]
	TIME [epoch: 71.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00034403049124118114		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.00034403049124118114 | validation: 0.000501178021255579]
	TIME [epoch: 71.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: -3.254231047396327e-05		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: -3.254231047396327e-05 | validation: 0.0008014180271057469]
	TIME [epoch: 71.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021302487012668105		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.00021302487012668105 | validation: -0.00012073377105136274]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038583790822904903		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.00038583790822904903 | validation: 0.000357160640963933]
	TIME [epoch: 71.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031758412885253654		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.00031758412885253654 | validation: 0.0006371493071977178]
	TIME [epoch: 71.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006292394570400583		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0006292394570400583 | validation: 0.0011042411082208227]
	TIME [epoch: 71.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001295908995642607		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.0001295908995642607 | validation: 0.00045906072328207867]
	TIME [epoch: 71.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005784097119175768		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0005784097119175768 | validation: 0.00019571305164485598]
	TIME [epoch: 71.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002417472641093481		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0002417472641093481 | validation: 2.345135669082056e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013093141904486094		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.00013093141904486094 | validation: 0.0003785279493739546]
	TIME [epoch: 71.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019549775306362216		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.00019549775306362216 | validation: 0.0006354366778163768]
	TIME [epoch: 71.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015739071507435765		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.00015739071507435765 | validation: -1.008920481770478e-06]
	TIME [epoch: 71.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00031375944795390546		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.00031375944795390546 | validation: 0.0005962887580071464]
	TIME [epoch: 71.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00011500654620262439		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.00011500654620262439 | validation: 8.011136485894799e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.527309314283224e-05		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 6.527309314283224e-05 | validation: -0.0003509577523162832]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1211.pth
	Model improved!!!
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030903888510369604		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.00030903888510369604 | validation: -1.0877226636955226e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: -2.520979856056038e-05		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: -2.520979856056038e-05 | validation: 0.0005151578920909818]
	TIME [epoch: 71.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.487012758958818e-05		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 4.487012758958818e-05 | validation: 0.0004163139827760105]
	TIME [epoch: 71.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6697031686024915e-05		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 4.6697031686024915e-05 | validation: -3.638933528559374e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002416233678789195		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.0002416233678789195 | validation: 0.00042267051509030384]
	TIME [epoch: 71.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00044679401393431364		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.00044679401393431364 | validation: 0.0010692567365547534]
	TIME [epoch: 71.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039744894107314455		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.00039744894107314455 | validation: 0.00013501170974125376]
	TIME [epoch: 71.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00033963846143962687		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.00033963846143962687 | validation: 0.0003967379230394901]
	TIME [epoch: 71.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.493621461257781e-05		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 6.493621461257781e-05 | validation: -0.0003709964942879829]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1220.pth
	Model improved!!!
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.748934030285071e-05		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 7.748934030285071e-05 | validation: 0.00038642925769178846]
	TIME [epoch: 71.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039922034672951884		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.00039922034672951884 | validation: 7.143673094440128e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00029901294553292163		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.00029901294553292163 | validation: -1.867406057469623e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003825235147942909		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.0003825235147942909 | validation: 0.0010511559007194578]
	TIME [epoch: 71.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002909625936958684		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.0002909625936958684 | validation: 0.00037227035183179423]
	TIME [epoch: 71.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002043630953370188		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0002043630953370188 | validation: -0.00019714020551139734]
	TIME [epoch: 71.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002848328779275609		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.0002848328779275609 | validation: 1.7799400026787818e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00032320960646040287		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.00032320960646040287 | validation: 0.0001616722319594759]
	TIME [epoch: 71.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005091770787136846		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.0005091770787136846 | validation: 0.00020273636985349965]
	TIME [epoch: 71.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00040118249981009504		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.00040118249981009504 | validation: 6.150389057563111e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00047303080107874787		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.00047303080107874787 | validation: 5.9043851439986926e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: -2.0083688547933626e-05		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: -2.0083688547933626e-05 | validation: 0.00012578806275305877]
	TIME [epoch: 71.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002593704918924693		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0002593704918924693 | validation: 0.0006583163568639314]
	TIME [epoch: 71.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: -3.572967042954822e-05		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: -3.572967042954822e-05 | validation: 0.0003790131548356728]
	TIME [epoch: 71.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003627553980358753		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.0003627553980358753 | validation: 0.0004790619583673533]
	TIME [epoch: 71.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: -3.474480440387607e-05		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: -3.474480440387607e-05 | validation: 5.899055106637805e-06]
	TIME [epoch: 71.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016254106172805384		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.00016254106172805384 | validation: 0.0001613013227250715]
	TIME [epoch: 71.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00015138082287356158		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.00015138082287356158 | validation: 0.0006457052943613491]
	TIME [epoch: 71.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003340422897845184		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.0003340422897845184 | validation: 5.172376018769676e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014098607017145847		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.00014098607017145847 | validation: -8.520692626229744e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010935563673170767		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.00010935563673170767 | validation: 0.00032447152123322277]
	TIME [epoch: 71.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00027799150609643044		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.00027799150609643044 | validation: 0.00039970985722874897]
	TIME [epoch: 71.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00010748242421246013		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.00010748242421246013 | validation: 0.0005154377424753518]
	TIME [epoch: 71.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006525758581068913		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.0006525758581068913 | validation: 0.0001958983160414718]
	TIME [epoch: 71.2 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002627584448876084		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.0002627584448876084 | validation: 0.00024255417608879168]
	TIME [epoch: 71.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003727248148240377		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0003727248148240377 | validation: 0.00027385051026318497]
	TIME [epoch: 71.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00025127364082560264		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.00025127364082560264 | validation: 1.288503956608356e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: -1.3181315593399973e-05		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: -1.3181315593399973e-05 | validation: 0.0004921531194183987]
	TIME [epoch: 71.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.602758626487651e-05		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 8.602758626487651e-05 | validation: 0.0005682990966143339]
	TIME [epoch: 71.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017205307691410177		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.00017205307691410177 | validation: 0.00022130154516499092]
	TIME [epoch: 71.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.851347861210535e-05		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 8.851347861210535e-05 | validation: -0.0001181809407043346]
	TIME [epoch: 71.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00030052385754876455		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.00030052385754876455 | validation: 0.000527070090719188]
	TIME [epoch: 71.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022823113996818024		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.00022823113996818024 | validation: 0.00037581511062564845]
	TIME [epoch: 71.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000386563772520778		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.000386563772520778 | validation: 0.0010724667577580505]
	TIME [epoch: 71.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004176860914218401		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.0004176860914218401 | validation: 0.0006803778915336327]
	TIME [epoch: 71.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013851668552490736		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.00013851668552490736 | validation: -0.00019413366304109393]
	TIME [epoch: 71.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4483414799474353e-05		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 2.4483414799474353e-05 | validation: 0.00011740544353924065]
	TIME [epoch: 71.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024378540102037083		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.00024378540102037083 | validation: 4.016278651582984e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003576352360464481		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.0003576352360464481 | validation: 0.0012139579754919519]
	TIME [epoch: 71.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00016350889126133696		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.00016350889126133696 | validation: 0.00020593483532895096]
	TIME [epoch: 71.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017043070669218397		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.00017043070669218397 | validation: 0.0007675384559150791]
	TIME [epoch: 71.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003246972784019828		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.0003246972784019828 | validation: 0.0005301800854848163]
	TIME [epoch: 71.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008037934166611935		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.0008037934166611935 | validation: 0.00012663241836584582]
	TIME [epoch: 71.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013343658744366073		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.00013343658744366073 | validation: 0.00022227472113538394]
	TIME [epoch: 71.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004103459818212243		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0004103459818212243 | validation: 0.0004430249184285939]
	TIME [epoch: 71.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018525022305485053		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.00018525022305485053 | validation: 0.0003567030389129888]
	TIME [epoch: 71.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00046576873417384415		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.00046576873417384415 | validation: 0.00010539976380426454]
	TIME [epoch: 71.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002803727109344398		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.0002803727109344398 | validation: 3.449510690689016e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.355138356166881e-05		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 6.355138356166881e-05 | validation: 2.4773056955532455e-05]
	TIME [epoch: 71.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00019947970669998184		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.00019947970669998184 | validation: -0.0004966675510129086]
	TIME [epoch: 71.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240704_134101/states/model_phi1_1a_v_mmd1_presample_1270.pth
	Model improved!!!
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00036889704424643234		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.00036889704424643234 | validation: 0.0009446397639178246]
	TIME [epoch: 71.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.69388073238981e-05		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 5.69388073238981e-05 | validation: 0.0012986505261758756]
	TIME [epoch: 71.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.122382390945026e-05		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 7.122382390945026e-05 | validation: 0.00030796117934261513]
	TIME [epoch: 71.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003052683846300377		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.0003052683846300377 | validation: 0.00034698497665155514]
	TIME [epoch: 71.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003103923328058076		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.0003103923328058076 | validation: 0.0003989551284330215]
	TIME [epoch: 71.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.822177345957451e-05		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 8.822177345957451e-05 | validation: 0.0007013445074540083]
	TIME [epoch: 71.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022364989271296708		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.00022364989271296708 | validation: 0.00034931951676257176]
	TIME [epoch: 71.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001593916447512558		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.0001593916447512558 | validation: 0.00015144215354739464]
	TIME [epoch: 71.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003489810924106496		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.0003489810924106496 | validation: 0.0003595073241112434]
	TIME [epoch: 71.3 sec]
EPOCH 1280/2000:
	Training over batches...
