Args:
Namespace(name='model_phi1_1a_v_mmd1_smallnet', outdir='out/model_training/model_phi1_1a_v_mmd1_smallnet', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[8, 16, 8], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4172506278

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.394118569509835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.394118569509835 | validation: 4.758042426342691]
	TIME [epoch: 95.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0461290898238005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0461290898238005 | validation: 4.531101488731166]
	TIME [epoch: 5.19 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.75709779084124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.75709779084124 | validation: 4.323551111076988]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5089427147858485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5089427147858485 | validation: 4.048577624250943]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.083517950155528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.083517950155528 | validation: 4.0292165294982265]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.727123616720111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727123616720111 | validation: 3.4387888316391306]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3017054766224567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3017054766224567 | validation: 3.403898319711745]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.098303575597796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.098303575597796 | validation: 3.375571887468845]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9110971710254563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9110971710254563 | validation: 3.130314813129789]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7690135483245246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7690135483245246 | validation: 2.97834176002514]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6924912520153823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6924912520153823 | validation: 2.9545282036066]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6213106269167255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6213106269167255 | validation: 2.8784001784369555]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.525836161689241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.525836161689241 | validation: 2.885613413296732]
	TIME [epoch: 5.11 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.489813616272998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.489813616272998 | validation: 2.7760735141507022]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4053217617355522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4053217617355522 | validation: 2.7292658627882203]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3108420069365785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3108420069365785 | validation: 2.7135418162760834]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.204009307557257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.204009307557257 | validation: 2.5951837443033448]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0818127829539614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0818127829539614 | validation: 2.557515280744841]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0505173054048687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0505173054048687 | validation: 2.53106950112017]
	TIME [epoch: 5.08 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.971070696335223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.971070696335223 | validation: 2.5082903255536837]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9820544708258165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9820544708258165 | validation: 2.4162767307492823]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9312890769849635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9312890769849635 | validation: 2.486913157931058]
	TIME [epoch: 5.1 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8943096829426946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8943096829426946 | validation: 2.5346643116573655]
	TIME [epoch: 5.1 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8841599436025696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8841599436025696 | validation: 2.4837829958257904]
	TIME [epoch: 5.1 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8827440632931811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8827440632931811 | validation: 2.606130096753164]
	TIME [epoch: 5.22 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527945074730647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8527945074730647 | validation: 2.4533308347513145]
	TIME [epoch: 5.19 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8480228839303674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8480228839303674 | validation: 2.485785568200818]
	TIME [epoch: 5.16 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8243401043114795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8243401043114795 | validation: 2.4862716393498165]
	TIME [epoch: 5.16 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8231462741508389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8231462741508389 | validation: 2.4746759142117094]
	TIME [epoch: 5.16 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7892917301450617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7892917301450617 | validation: 2.413113287766475]
	TIME [epoch: 5.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7493419532861867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7493419532861867 | validation: 2.4318372406612454]
	TIME [epoch: 5.15 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7943328935264609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7943328935264609 | validation: 2.399939498626001]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7548742270925426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7548742270925426 | validation: 2.4416450685023285]
	TIME [epoch: 5.19 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7590329095333945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7590329095333945 | validation: 2.479104871673903]
	TIME [epoch: 5.18 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7202849774411129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7202849774411129 | validation: 2.4395996268215487]
	TIME [epoch: 5.15 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7193316144646542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7193316144646542 | validation: 2.266253777148418]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7053265054347857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7053265054347857 | validation: 2.496554025130663]
	TIME [epoch: 5.16 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6880381743368478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6880381743368478 | validation: 2.2038487351301423]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6752724075379666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6752724075379666 | validation: 2.2493840435059487]
	TIME [epoch: 5.14 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6683913516346078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6683913516346078 | validation: 2.206657155965485]
	TIME [epoch: 5.13 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6588786803979598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6588786803979598 | validation: 2.126126064777016]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5575633290483368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5575633290483368 | validation: 2.3498391249368744]
	TIME [epoch: 5.15 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6335371481060998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6335371481060998 | validation: 2.34245879457915]
	TIME [epoch: 5.16 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.580570972989672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.580570972989672 | validation: 2.172423058998719]
	TIME [epoch: 5.14 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.60557158650436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.60557158650436 | validation: 2.155411634413671]
	TIME [epoch: 5.13 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5690608943286577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5690608943286577 | validation: 2.129618910141336]
	TIME [epoch: 5.13 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5625703209754473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5625703209754473 | validation: 2.0952815507602365]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5326205488099025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5326205488099025 | validation: 2.1067551588435274]
	TIME [epoch: 5.15 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5324544908048399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5324544908048399 | validation: 2.1632836661897272]
	TIME [epoch: 5.14 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5749009589433896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5749009589433896 | validation: 2.1077276423471427]
	TIME [epoch: 5.15 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5823860858122651		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.5823860858122651 | validation: 2.325369953447102]
	TIME [epoch: 5.17 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5312283115948586		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.5312283115948586 | validation: 2.2317790658000467]
	TIME [epoch: 5.17 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5486947556182438		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.5486947556182438 | validation: 2.1172969108377973]
	TIME [epoch: 5.14 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.562029919670144		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.562029919670144 | validation: 2.0495668921630923]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.559503905290148		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.559503905290148 | validation: 2.1801488541450813]
	TIME [epoch: 5.13 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4803320047829513		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.4803320047829513 | validation: 2.068223980625418]
	TIME [epoch: 5.14 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4719259680578942		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.4719259680578942 | validation: 2.0884166693439905]
	TIME [epoch: 5.13 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4938790109695759		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.4938790109695759 | validation: 2.0255675105107493]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.44170967392646		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.44170967392646 | validation: 2.053787358851758]
	TIME [epoch: 5.13 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.466700779552729		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.466700779552729 | validation: 2.036991075939804]
	TIME [epoch: 5.16 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4178775685342546		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.4178775685342546 | validation: 2.0282520783183458]
	TIME [epoch: 5.16 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4478439296843504		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.4478439296843504 | validation: 2.071983472794627]
	TIME [epoch: 5.14 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4413597140700425		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.4413597140700425 | validation: 2.236751729961608]
	TIME [epoch: 5.13 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4769758396174137		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.4769758396174137 | validation: 2.0368562400505055]
	TIME [epoch: 5.13 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3841141277534263		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.3841141277534263 | validation: 2.140709885021114]
	TIME [epoch: 5.13 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3944734886734873		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 1.3944734886734873 | validation: 2.005936049668724]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.398398476878118		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.398398476878118 | validation: 2.002132550378882]
	TIME [epoch: 5.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3551825757728444		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.3551825757728444 | validation: 2.042897319373798]
	TIME [epoch: 5.13 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3764398820049006		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.3764398820049006 | validation: 2.0527710093583353]
	TIME [epoch: 5.18 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3959747587114095		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 1.3959747587114095 | validation: 1.95127814450341]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.32203371353348		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 1.32203371353348 | validation: 1.8241608540908203]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.217977753388922		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 1.217977753388922 | validation: 1.3858905211751438]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0467757789886947		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.0467757789886947 | validation: 1.2110433031889445]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8544071853626867		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.8544071853626867 | validation: 0.9669909677723233]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8031611294678093		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.8031611294678093 | validation: 0.8881551915931827]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9791190297299108		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.9791190297299108 | validation: 0.8212044579978082]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6452533142805413		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.6452533142805413 | validation: 0.7856926721295387]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7300394558217178		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.7300394558217178 | validation: 0.7185196239176111]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7314116237103732		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.7314116237103732 | validation: 1.0483539722125028]
	TIME [epoch: 5.13 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8210725676633738		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.8210725676633738 | validation: 0.6550177991305717]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6258262528120826		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.6258262528120826 | validation: 0.7470781909648981]
	TIME [epoch: 5.13 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6847254775150198		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.6847254775150198 | validation: 0.6940465042112665]
	TIME [epoch: 5.12 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6287154216907167		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6287154216907167 | validation: 0.7236956530529944]
	TIME [epoch: 5.12 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6436959019871328		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.6436959019871328 | validation: 0.6226820064633956]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5803278423703022		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5803278423703022 | validation: 0.5287351943523166]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5929664612622518		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5929664612622518 | validation: 0.7284509386117665]
	TIME [epoch: 5.17 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.681004680490745		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.681004680490745 | validation: 0.4880710359636664]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5824877814652535		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5824877814652535 | validation: 1.4274029816735583]
	TIME [epoch: 5.13 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9948290446333781		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.9948290446333781 | validation: 0.7466620763479714]
	TIME [epoch: 5.13 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7248235339054435		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.7248235339054435 | validation: 0.4657606909749741]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6236033852435657		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.6236033852435657 | validation: 0.71071502373921]
	TIME [epoch: 5.13 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7471618736888976		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.7471618736888976 | validation: 0.5984156888995276]
	TIME [epoch: 5.12 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6701158369703275		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.6701158369703275 | validation: 0.5321045907073452]
	TIME [epoch: 5.13 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7206181139593542		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.7206181139593542 | validation: 0.5823335861679988]
	TIME [epoch: 5.15 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5696131524660746		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.5696131524660746 | validation: 0.6372789668835885]
	TIME [epoch: 5.15 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7888948578685272		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.7888948578685272 | validation: 0.5665317373854801]
	TIME [epoch: 5.13 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164303088415064		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.5164303088415064 | validation: 0.5401550175413161]
	TIME [epoch: 5.12 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6664305030619179		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.6664305030619179 | validation: 0.8978156791364257]
	TIME [epoch: 5.13 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8336253794456083		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.8336253794456083 | validation: 0.7324344584097731]
	TIME [epoch: 5.12 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9557616592141692		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.9557616592141692 | validation: 0.7498576486499964]
	TIME [epoch: 5.13 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6449902522607704		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.6449902522607704 | validation: 0.8237477646362683]
	TIME [epoch: 5.13 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8697188287390242		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.8697188287390242 | validation: 0.4831079484123007]
	TIME [epoch: 5.13 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5996731636760118		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.5996731636760118 | validation: 0.6624149981052319]
	TIME [epoch: 5.14 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.863659797485224		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.863659797485224 | validation: 0.5686248159303309]
	TIME [epoch: 5.15 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9649032659460962		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.9649032659460962 | validation: 0.6129432067061829]
	TIME [epoch: 5.13 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7449934832963585		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.7449934832963585 | validation: 0.788343792853565]
	TIME [epoch: 5.12 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8136069030159614		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.8136069030159614 | validation: 0.8308766406654502]
	TIME [epoch: 5.12 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1265679896206304		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 1.1265679896206304 | validation: 0.8488464281907311]
	TIME [epoch: 5.13 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8991568906442156		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.8991568906442156 | validation: 0.8392029277410882]
	TIME [epoch: 5.12 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7476740760528828		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.7476740760528828 | validation: 1.3589407630571002]
	TIME [epoch: 5.12 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4723981164363467		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 1.4723981164363467 | validation: 0.6575225081503551]
	TIME [epoch: 5.12 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9445715752248129		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.9445715752248129 | validation: 0.5909847022612629]
	TIME [epoch: 5.15 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8972582488764954		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.8972582488764954 | validation: 1.7937127794188994]
	TIME [epoch: 5.15 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.13326899003385		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 1.13326899003385 | validation: 0.8324421395982169]
	TIME [epoch: 5.13 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8365762098762752		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.8365762098762752 | validation: 0.9512317059962494]
	TIME [epoch: 5.12 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0344620771787472		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 1.0344620771787472 | validation: 1.8908435542480104]
	TIME [epoch: 5.13 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4633955829084893		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 1.4633955829084893 | validation: 1.3120013367914032]
	TIME [epoch: 5.13 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0190566349652708		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 1.0190566349652708 | validation: 0.749831008947923]
	TIME [epoch: 5.13 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7720453397041822		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.7720453397041822 | validation: 0.7546801608288747]
	TIME [epoch: 5.13 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7998210894444262		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.7998210894444262 | validation: 1.9076756023140318]
	TIME [epoch: 5.13 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1496143991860628		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 1.1496143991860628 | validation: 0.9685121067472989]
	TIME [epoch: 5.15 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9437716766582946		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.9437716766582946 | validation: 0.6265343979578902]
	TIME [epoch: 5.15 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.084505473584792		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 1.084505473584792 | validation: 0.5373778826804525]
	TIME [epoch: 5.13 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2375585559678857		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 1.2375585559678857 | validation: 1.9517978234453706]
	TIME [epoch: 5.13 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1636190578263772		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 1.1636190578263772 | validation: 0.8958227410435908]
	TIME [epoch: 5.13 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.859218512382135		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.859218512382135 | validation: 1.6430421473098764]
	TIME [epoch: 5.13 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1393316441210233		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 1.1393316441210233 | validation: 0.6673461159711274]
	TIME [epoch: 5.13 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1464241387523713		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 1.1464241387523713 | validation: 0.5721444079832894]
	TIME [epoch: 5.13 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0781539387720627		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 1.0781539387720627 | validation: 0.9732620408809345]
	TIME [epoch: 5.13 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0933095348024333		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 1.0933095348024333 | validation: 1.15800027210084]
	TIME [epoch: 5.15 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.088008649820166		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 1.088008649820166 | validation: 0.8189724074534595]
	TIME [epoch: 5.15 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9344849034837969		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.9344849034837969 | validation: 0.8315120665369959]
	TIME [epoch: 5.13 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8891511897519488		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.8891511897519488 | validation: 1.4044372879986715]
	TIME [epoch: 5.12 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2250237172700058		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 1.2250237172700058 | validation: 0.7046577340747147]
	TIME [epoch: 5.13 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8678714607113638		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.8678714607113638 | validation: 0.7816587655691329]
	TIME [epoch: 5.13 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9126928359620801		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.9126928359620801 | validation: 0.9758140929989294]
	TIME [epoch: 5.13 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8364587408631863		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.8364587408631863 | validation: 0.5998698604303925]
	TIME [epoch: 5.13 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6801451671636422		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.6801451671636422 | validation: 1.0309117455569423]
	TIME [epoch: 5.12 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9761127045884586		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.9761127045884586 | validation: 0.7322032639081995]
	TIME [epoch: 5.15 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8007221670656205		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.8007221670656205 | validation: 0.519736980290894]
	TIME [epoch: 5.15 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8355679804890462		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.8355679804890462 | validation: 0.45412315723451074]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4145689106752004		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 1.4145689106752004 | validation: 0.871676569978588]
	TIME [epoch: 5.13 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8659283843877861		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.8659283843877861 | validation: 0.6663642390229542]
	TIME [epoch: 5.12 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9048228520638486		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.9048228520638486 | validation: 0.6816591914632608]
	TIME [epoch: 5.12 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7207542606996276		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.7207542606996276 | validation: 0.5932534332090383]
	TIME [epoch: 5.13 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7710248820395568		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.7710248820395568 | validation: 0.5888760629504002]
	TIME [epoch: 5.13 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7352993237571711		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.7352993237571711 | validation: 0.5630644681760402]
	TIME [epoch: 5.13 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.93090514772739		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.93090514772739 | validation: 0.6425771747832294]
	TIME [epoch: 5.16 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8257877631049119		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.8257877631049119 | validation: 0.713064112937214]
	TIME [epoch: 5.16 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7236951973308996		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.7236951973308996 | validation: 0.631224183125116]
	TIME [epoch: 5.14 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5992508292128496		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.5992508292128496 | validation: 0.5668962235506487]
	TIME [epoch: 5.13 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7380532034159057		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.7380532034159057 | validation: 0.6368056685896137]
	TIME [epoch: 5.13 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6957756120750838		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.6957756120750838 | validation: 0.6452361361646901]
	TIME [epoch: 5.13 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6255198989446592		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.6255198989446592 | validation: 0.5640884040896006]
	TIME [epoch: 5.13 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7600061727507414		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.7600061727507414 | validation: 0.7193174024229827]
	TIME [epoch: 5.12 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040055321137914		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.6040055321137914 | validation: 0.5980721398510691]
	TIME [epoch: 5.13 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7640777284102968		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.7640777284102968 | validation: 0.4666953658589795]
	TIME [epoch: 5.15 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5297502386398633		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.5297502386398633 | validation: 0.711532017970699]
	TIME [epoch: 5.15 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561223634940087		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.5561223634940087 | validation: 0.5198544723206228]
	TIME [epoch: 5.13 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6075564348006124		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.6075564348006124 | validation: 0.4069197015709468]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4919589298302825		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.4919589298302825 | validation: 0.651151743988863]
	TIME [epoch: 5.13 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6946995270804055		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.6946995270804055 | validation: 0.46936776015501547]
	TIME [epoch: 5.13 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5445782939666217		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.5445782939666217 | validation: 0.4288017551249541]
	TIME [epoch: 5.12 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.529285797342642		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.529285797342642 | validation: 0.6562257586938745]
	TIME [epoch: 5.13 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6195165553236266		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.6195165553236266 | validation: 0.518336586086335]
	TIME [epoch: 5.13 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5759625083463406		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.5759625083463406 | validation: 0.3903479510949558]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4902867284442992		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.4902867284442992 | validation: 0.568449963592746]
	TIME [epoch: 5.17 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4558953843250347		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.4558953843250347 | validation: 0.42435053317494387]
	TIME [epoch: 5.14 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42969030738329306		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.42969030738329306 | validation: 0.6182088966224687]
	TIME [epoch: 5.14 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5632360693583205		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.5632360693583205 | validation: 0.4306875036193733]
	TIME [epoch: 5.14 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47404065856123784		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.47404065856123784 | validation: 0.41823975057474705]
	TIME [epoch: 5.14 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4532050031585336		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.4532050031585336 | validation: 1.091267269027166]
	TIME [epoch: 5.14 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6094151739417633		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.6094151739417633 | validation: 0.3971962141125168]
	TIME [epoch: 5.14 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41162734519190625		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.41162734519190625 | validation: 0.37113710744692024]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42238219309891445		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.42238219309891445 | validation: 0.4542214795882368]
	TIME [epoch: 5.17 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6557704229460002		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.6557704229460002 | validation: 0.466027797172997]
	TIME [epoch: 5.16 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4607128143957941		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.4607128143957941 | validation: 0.5579160729675218]
	TIME [epoch: 5.14 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49493133836972936		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.49493133836972936 | validation: 0.5149179295606774]
	TIME [epoch: 5.14 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48691649864095554		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.48691649864095554 | validation: 0.37083399542656714]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39243320028748563		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.39243320028748563 | validation: 0.3959561377690929]
	TIME [epoch: 5.1 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3760608218019569		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.3760608218019569 | validation: 0.3941923674546979]
	TIME [epoch: 5.14 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5144867826241534		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.5144867826241534 | validation: 0.529975222662703]
	TIME [epoch: 5.16 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4541872143052511		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.4541872143052511 | validation: 0.3765906381260461]
	TIME [epoch: 5.14 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3453566343773965		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.3453566343773965 | validation: 0.6709884716690266]
	TIME [epoch: 5.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5832669935315712		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.5832669935315712 | validation: 0.624532471288205]
	TIME [epoch: 5.15 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4861165769610276		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.4861165769610276 | validation: 0.35034449414091207]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36644814560182704		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.36644814560182704 | validation: 0.3835455522155594]
	TIME [epoch: 5.16 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42655134926420735		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.42655134926420735 | validation: 0.37526664211504507]
	TIME [epoch: 5.14 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34933375026651525		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.34933375026651525 | validation: 0.33814462607599577]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4700348605541552		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.4700348605541552 | validation: 0.3783467884451622]
	TIME [epoch: 5.13 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36000588425391983		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.36000588425391983 | validation: 0.38189859086427724]
	TIME [epoch: 5.13 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4010224058680222		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.4010224058680222 | validation: 0.40464312490895105]
	TIME [epoch: 5.13 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4041997126273362		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.4041997126273362 | validation: 0.47009726213198144]
	TIME [epoch: 5.17 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45313521735930096		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.45313521735930096 | validation: 0.4285413367218575]
	TIME [epoch: 5.14 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41967680854821643		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.41967680854821643 | validation: 0.3815228049953622]
	TIME [epoch: 5.14 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3707156112151435		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.3707156112151435 | validation: 0.4550400614204866]
	TIME [epoch: 5.14 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3771419854433706		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.3771419854433706 | validation: 0.4223431772270996]
	TIME [epoch: 5.13 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3458461736147913		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.3458461736147913 | validation: 0.3567928176924319]
	TIME [epoch: 5.13 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3228821368436617		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.3228821368436617 | validation: 0.6666410158287308]
	TIME [epoch: 5.13 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47109767589732643		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.47109767589732643 | validation: 0.33693258294685713]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3633274390273744		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.3633274390273744 | validation: 0.3970604439339199]
	TIME [epoch: 5.16 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.362044866119652		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.362044866119652 | validation: 0.3247098118055968]
	TIME [epoch: 5.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3994451836928332		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.3994451836928332 | validation: 0.42403656288689806]
	TIME [epoch: 5.14 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38625897795806974		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.38625897795806974 | validation: 0.32232948844070475]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3538317442561728		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.3538317442561728 | validation: 0.3036710484182419]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.357349667459683		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.357349667459683 | validation: 0.31406181330055033]
	TIME [epoch: 5.14 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30700148280804884		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.30700148280804884 | validation: 0.4174288757768686]
	TIME [epoch: 5.13 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30131274201991454		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.30131274201991454 | validation: 0.33535589749950256]
	TIME [epoch: 5.13 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29021992251682227		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.29021992251682227 | validation: 0.570992438631347]
	TIME [epoch: 5.14 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3665295025303844		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.3665295025303844 | validation: 0.2997233355785289]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3698649822337755		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.3698649822337755 | validation: 0.3978461011304447]
	TIME [epoch: 5.17 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.386844605114993		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.386844605114993 | validation: 0.33793393442774955]
	TIME [epoch: 5.13 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2825606693457261		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.2825606693457261 | validation: 0.25919739306806816]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3653040373627291		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.3653040373627291 | validation: 0.30458209738811937]
	TIME [epoch: 5.13 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39777729459700956		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.39777729459700956 | validation: 0.385311312813583]
	TIME [epoch: 5.14 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3474694465426801		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.3474694465426801 | validation: 0.41112581637521217]
	TIME [epoch: 5.13 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32594903966017663		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.32594903966017663 | validation: 0.28570700335289545]
	TIME [epoch: 5.13 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27207958587860986		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.27207958587860986 | validation: 0.3533835555541133]
	TIME [epoch: 5.14 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2933325381266714		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.2933325381266714 | validation: 0.30629056862633397]
	TIME [epoch: 5.16 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3396832875593412		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.3396832875593412 | validation: 0.3188834235616749]
	TIME [epoch: 5.15 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28273645554039584		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.28273645554039584 | validation: 0.3409039399061167]
	TIME [epoch: 5.13 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34674378930944955		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.34674378930944955 | validation: 0.3394259271517058]
	TIME [epoch: 5.13 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2828239662410377		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.2828239662410377 | validation: 0.32359859891103715]
	TIME [epoch: 5.13 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2535676174645892		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.2535676174645892 | validation: 0.43195164606925607]
	TIME [epoch: 5.13 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4035583880594248		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.4035583880594248 | validation: 0.2947836558056994]
	TIME [epoch: 5.14 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2728375183896527		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.2728375183896527 | validation: 0.2678898146929333]
	TIME [epoch: 5.13 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3131896193078184		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.3131896193078184 | validation: 0.2802950144803451]
	TIME [epoch: 5.13 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.296539957063417		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.296539957063417 | validation: 0.40225018386471245]
	TIME [epoch: 5.16 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2940376525409233		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.2940376525409233 | validation: 0.40515288115878734]
	TIME [epoch: 5.15 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37972425629418377		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.37972425629418377 | validation: 0.2996534894313404]
	TIME [epoch: 5.13 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577512102062611		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.2577512102062611 | validation: 0.2654350134131172]
	TIME [epoch: 5.13 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2635813326810982		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.2635813326810982 | validation: 0.30586284175713996]
	TIME [epoch: 5.13 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3420962256844135		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.3420962256844135 | validation: 0.2948370910554221]
	TIME [epoch: 5.13 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34055396354239315		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.34055396354239315 | validation: 0.3115960952351786]
	TIME [epoch: 5.13 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.283643386768574		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.283643386768574 | validation: 0.3433474354048794]
	TIME [epoch: 5.13 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2747707869306975		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.2747707869306975 | validation: 0.3250490663643218]
	TIME [epoch: 5.13 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39796361966574423		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.39796361966574423 | validation: 0.24028551268857753]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25104856465331615		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.25104856465331615 | validation: 0.22849480815719136]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2806515860780502		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.2806515860780502 | validation: 0.24761097863978296]
	TIME [epoch: 5.13 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22797346894625725		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.22797346894625725 | validation: 0.3175038193668763]
	TIME [epoch: 5.13 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27012102239136504		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.27012102239136504 | validation: 0.2948818161949629]
	TIME [epoch: 5.13 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676028496404523		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.2676028496404523 | validation: 0.3109995560461988]
	TIME [epoch: 5.13 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24435870952181643		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.24435870952181643 | validation: 0.24631002180886585]
	TIME [epoch: 5.13 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3183631189608357		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.3183631189608357 | validation: 0.23060736809843718]
	TIME [epoch: 5.13 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599420040497278		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.2599420040497278 | validation: 0.2840271780433649]
	TIME [epoch: 5.13 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22882347466621578		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.22882347466621578 | validation: 0.24876721704478477]
	TIME [epoch: 5.17 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576566137030154		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.2576566137030154 | validation: 0.3137101044831879]
	TIME [epoch: 5.14 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28071491393029485		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.28071491393029485 | validation: 0.21538758855603207]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26280384648200117		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.26280384648200117 | validation: 0.22588984388474534]
	TIME [epoch: 5.13 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26133727229327214		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.26133727229327214 | validation: 0.3775999490249289]
	TIME [epoch: 5.13 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27382584391797427		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.27382584391797427 | validation: 0.2253261535737557]
	TIME [epoch: 5.13 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22851316926060444		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.22851316926060444 | validation: 0.3796960471981779]
	TIME [epoch: 5.13 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631473482645183		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.2631473482645183 | validation: 0.34136327435931]
	TIME [epoch: 5.13 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151170495073831		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.3151170495073831 | validation: 0.31320408390546906]
	TIME [epoch: 5.13 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23241679522884467		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.23241679522884467 | validation: 0.22196246993722268]
	TIME [epoch: 5.17 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23450535135150297		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.23450535135150297 | validation: 0.33082934746911385]
	TIME [epoch: 5.14 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24375272653657812		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.24375272653657812 | validation: 0.23058312368363681]
	TIME [epoch: 5.14 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2353326038615181		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.2353326038615181 | validation: 0.2184904985910774]
	TIME [epoch: 5.14 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2227492509424821		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.2227492509424821 | validation: 0.27563395894566783]
	TIME [epoch: 5.13 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.242469901506041		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.242469901506041 | validation: 0.22240855183474]
	TIME [epoch: 5.13 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22366789765722553		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.22366789765722553 | validation: 0.24448253158711114]
	TIME [epoch: 5.13 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22745943878644925		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.22745943878644925 | validation: 0.29343884799845593]
	TIME [epoch: 5.13 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2612628568151434		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.2612628568151434 | validation: 0.23544182260866564]
	TIME [epoch: 5.13 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3227646648564064		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.3227646648564064 | validation: 0.27503354039424144]
	TIME [epoch: 5.17 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.308801512814466		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.308801512814466 | validation: 0.1743039774037578]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2475664649103767		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.2475664649103767 | validation: 0.25692011323123604]
	TIME [epoch: 5.12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2049291434638633		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.2049291434638633 | validation: 0.25160105478028716]
	TIME [epoch: 5.12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20329273026424935		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.20329273026424935 | validation: 0.21725496320009952]
	TIME [epoch: 5.13 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19550219043578856		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.19550219043578856 | validation: 0.23195896048923442]
	TIME [epoch: 5.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24444426566103133		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.24444426566103133 | validation: 0.23833942048479786]
	TIME [epoch: 5.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22683654994358787		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.22683654994358787 | validation: 0.20588159549675705]
	TIME [epoch: 5.12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20204537776478165		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.20204537776478165 | validation: 0.20856112807557975]
	TIME [epoch: 5.13 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19768851417551353		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.19768851417551353 | validation: 0.2546953621265728]
	TIME [epoch: 5.16 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28055582893622383		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.28055582893622383 | validation: 0.3559591361368167]
	TIME [epoch: 5.13 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24144382229476233		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.24144382229476233 | validation: 0.21132322537276627]
	TIME [epoch: 5.12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2898834209288352		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.2898834209288352 | validation: 0.19457282015168423]
	TIME [epoch: 5.13 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1877256090248331		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.1877256090248331 | validation: 0.21233717275945924]
	TIME [epoch: 5.13 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18114022477648928		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.18114022477648928 | validation: 0.19801587164832105]
	TIME [epoch: 5.12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21060949976535728		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.21060949976535728 | validation: 0.2886145767925617]
	TIME [epoch: 5.12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20350976180880717		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.20350976180880717 | validation: 0.17603597058575898]
	TIME [epoch: 5.12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20010528599039046		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.20010528599039046 | validation: 0.3037304155416303]
	TIME [epoch: 5.13 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2571508960568318		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.2571508960568318 | validation: 0.23968432627759892]
	TIME [epoch: 5.16 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21318611023018558		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.21318611023018558 | validation: 0.19873788043353435]
	TIME [epoch: 5.13 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18500519328229942		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.18500519328229942 | validation: 0.19233405048859648]
	TIME [epoch: 5.13 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18200740143924551		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.18200740143924551 | validation: 0.1814659145081156]
	TIME [epoch: 5.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17455002322628096		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.17455002322628096 | validation: 0.19114229622979906]
	TIME [epoch: 5.12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2450473052021281		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.2450473052021281 | validation: 0.20122748685752512]
	TIME [epoch: 5.12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1997917183312482		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.1997917183312482 | validation: 0.18479904075069029]
	TIME [epoch: 5.12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1911028181964884		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.1911028181964884 | validation: 0.46508762553194793]
	TIME [epoch: 5.13 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3032681140874435		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.3032681140874435 | validation: 0.19013526028014505]
	TIME [epoch: 5.13 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17061260661915092		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.17061260661915092 | validation: 0.19886918002829534]
	TIME [epoch: 5.16 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21649584204452332		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.21649584204452332 | validation: 0.20110803238965408]
	TIME [epoch: 5.14 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19314955293856845		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.19314955293856845 | validation: 0.24505763015236903]
	TIME [epoch: 5.13 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21283427807259236		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.21283427807259236 | validation: 0.20707365063782965]
	TIME [epoch: 5.13 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1914398477965371		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.1914398477965371 | validation: 0.16777875018805816]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19380934123095903		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.19380934123095903 | validation: 0.17316070745545656]
	TIME [epoch: 5.14 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21979402956783262		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.21979402956783262 | validation: 0.1964967999787136]
	TIME [epoch: 5.14 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16898346986932702		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.16898346986932702 | validation: 0.17178166153584468]
	TIME [epoch: 5.13 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22949978257809428		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.22949978257809428 | validation: 0.18389962917441977]
	TIME [epoch: 5.14 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20951371434721755		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.20951371434721755 | validation: 0.21020614090979756]
	TIME [epoch: 5.17 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17797044585171612		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.17797044585171612 | validation: 0.18911046271709914]
	TIME [epoch: 5.16 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19270738942398288		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.19270738942398288 | validation: 0.18575739346741188]
	TIME [epoch: 5.14 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17342332663214904		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.17342332663214904 | validation: 0.18512794936081953]
	TIME [epoch: 5.14 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23949428174778514		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.23949428174778514 | validation: 0.2648502291451333]
	TIME [epoch: 5.14 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18442589395372377		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.18442589395372377 | validation: 0.17176262695604738]
	TIME [epoch: 5.14 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16503528929885164		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.16503528929885164 | validation: 0.16525926713874117]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19277853529885008		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.19277853529885008 | validation: 0.17564025580049053]
	TIME [epoch: 5.13 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16441799208178373		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.16441799208178373 | validation: 0.1696103451952462]
	TIME [epoch: 5.14 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18290033374452885		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.18290033374452885 | validation: 0.16058221651741683]
	TIME [epoch: 5.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1914591980832738		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.1914591980832738 | validation: 0.16794224542442854]
	TIME [epoch: 5.13 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17969044304443652		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.17969044304443652 | validation: 0.24210849809849047]
	TIME [epoch: 5.12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17813738887639657		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.17813738887639657 | validation: 0.15930544117456713]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16316391383874326		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.16316391383874326 | validation: 0.22994883556026802]
	TIME [epoch: 5.12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20968075564532326		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.20968075564532326 | validation: 0.18624323284281974]
	TIME [epoch: 5.12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18115839549007834		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.18115839549007834 | validation: 0.29420499306466413]
	TIME [epoch: 5.12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1870611871016739		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.1870611871016739 | validation: 0.2335088465619206]
	TIME [epoch: 5.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1757831549051154		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.1757831549051154 | validation: 0.1897331252859793]
	TIME [epoch: 5.14 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1636481036519286		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.1636481036519286 | validation: 0.23013371482544648]
	TIME [epoch: 5.14 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1934553657405158		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.1934553657405158 | validation: 0.20317806470310684]
	TIME [epoch: 5.13 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16988735085546292		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.16988735085546292 | validation: 0.16243293051947894]
	TIME [epoch: 5.12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15843921668863797		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.15843921668863797 | validation: 0.1807332783955939]
	TIME [epoch: 5.13 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17188833209527168		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.17188833209527168 | validation: 0.182416612725839]
	TIME [epoch: 5.12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16377231824554342		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.16377231824554342 | validation: 0.18828798154716717]
	TIME [epoch: 5.12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21088135860423976		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.21088135860423976 | validation: 0.20614954261242407]
	TIME [epoch: 5.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16721828601345012		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.16721828601345012 | validation: 0.1537812614930429]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1635833017882203		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.1635833017882203 | validation: 0.17208926176118622]
	TIME [epoch: 5.16 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1836898696877292		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.1836898696877292 | validation: 0.1794745345029884]
	TIME [epoch: 5.15 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18984492293574762		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.18984492293574762 | validation: 0.17810700937049306]
	TIME [epoch: 5.13 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19007228667138257		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.19007228667138257 | validation: 0.1537663967934157]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16279992494692958		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.16279992494692958 | validation: 0.18771721457718302]
	TIME [epoch: 5.13 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15179625176813943		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.15179625176813943 | validation: 0.15623254032243092]
	TIME [epoch: 5.13 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15891271964130682		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.15891271964130682 | validation: 0.1655453054149552]
	TIME [epoch: 5.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17413810949761072		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.17413810949761072 | validation: 0.15790590729091414]
	TIME [epoch: 5.12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14477818002837084		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.14477818002837084 | validation: 0.18114146358053418]
	TIME [epoch: 5.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1741914130578222		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.1741914130578222 | validation: 0.14836501916190223]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18271494410726455		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.18271494410726455 | validation: 0.19061664621247368]
	TIME [epoch: 5.17 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15674050851715676		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.15674050851715676 | validation: 0.1633254484087071]
	TIME [epoch: 5.14 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1582160817747026		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.1582160817747026 | validation: 0.18407040140210534]
	TIME [epoch: 5.14 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15683119848422308		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.15683119848422308 | validation: 0.21096703482472395]
	TIME [epoch: 5.14 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20730212086874678		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.20730212086874678 | validation: 0.18461490100336314]
	TIME [epoch: 5.14 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15432904571398048		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.15432904571398048 | validation: 0.15477820000342318]
	TIME [epoch: 5.14 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14390522892517396		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.14390522892517396 | validation: 0.1444940204263398]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15085743927557277		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.15085743927557277 | validation: 0.170765595528284]
	TIME [epoch: 5.14 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20478800987375675		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.20478800987375675 | validation: 0.2163189046963383]
	TIME [epoch: 5.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16543711986234283		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.16543711986234283 | validation: 0.14981721923475416]
	TIME [epoch: 5.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13757052781988802		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.13757052781988802 | validation: 0.1631720968011906]
	TIME [epoch: 5.09 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16767355442486895		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.16767355442486895 | validation: 0.17354012516909828]
	TIME [epoch: 5.17 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14827691885680502		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.14827691885680502 | validation: 0.1755596362473702]
	TIME [epoch: 5.19 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1571098278620977		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.1571098278620977 | validation: 0.2992007706369654]
	TIME [epoch: 5.15 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18172178527807792		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.18172178527807792 | validation: 0.16063635489882766]
	TIME [epoch: 5.15 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16465252211785392		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.16465252211785392 | validation: 0.24642051090704947]
	TIME [epoch: 5.16 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17797283406662412		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.17797283406662412 | validation: 0.15463714905383458]
	TIME [epoch: 5.17 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13368436758593202		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.13368436758593202 | validation: 0.16243429507098636]
	TIME [epoch: 5.19 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13227071173900065		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.13227071173900065 | validation: 0.16816465728388663]
	TIME [epoch: 5.16 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14507023333330807		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.14507023333330807 | validation: 0.24280783179645887]
	TIME [epoch: 5.15 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19237399591179138		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.19237399591179138 | validation: 0.17402704381102466]
	TIME [epoch: 5.14 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16615852546644738		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.16615852546644738 | validation: 0.15582194980802377]
	TIME [epoch: 5.15 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16287610399771096		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.16287610399771096 | validation: 0.21109699384425856]
	TIME [epoch: 5.14 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13369924544155215		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.13369924544155215 | validation: 0.18800186498473537]
	TIME [epoch: 5.16 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14690223869844482		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.14690223869844482 | validation: 0.18997352968518805]
	TIME [epoch: 5.17 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16330171217554834		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.16330171217554834 | validation: 0.16531684133350602]
	TIME [epoch: 5.15 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14327635440668268		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.14327635440668268 | validation: 0.2067882080599051]
	TIME [epoch: 5.19 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15346565438518134		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.15346565438518134 | validation: 0.14279190070214895]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15201690286913117		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.15201690286913117 | validation: 0.18866846276545815]
	TIME [epoch: 5.14 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13709093554450813		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.13709093554450813 | validation: 0.1572443074719485]
	TIME [epoch: 5.14 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14403000650761486		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.14403000650761486 | validation: 0.1364543166493059]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1520822983157495		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.1520822983157495 | validation: 0.153325259195022]
	TIME [epoch: 5.13 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1354248728177665		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.1354248728177665 | validation: 0.160771029051123]
	TIME [epoch: 5.13 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14578153952009207		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.14578153952009207 | validation: 0.1418687106690791]
	TIME [epoch: 5.13 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13146862275300306		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.13146862275300306 | validation: 0.19853102616363116]
	TIME [epoch: 5.14 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24214860713131292		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.24214860713131292 | validation: 0.15244519949016977]
	TIME [epoch: 5.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12437026521012376		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.12437026521012376 | validation: 0.16376288225580074]
	TIME [epoch: 5.13 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12291724081805969		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.12291724081805969 | validation: 0.15204677338793404]
	TIME [epoch: 5.13 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17120780438024513		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.17120780438024513 | validation: 0.2067787502723887]
	TIME [epoch: 5.13 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18605355939663334		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.18605355939663334 | validation: 0.19745510359953733]
	TIME [epoch: 5.13 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14264946370527298		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.14264946370527298 | validation: 0.1738639338305053]
	TIME [epoch: 5.13 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14318487206726202		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.14318487206726202 | validation: 0.13850907985821115]
	TIME [epoch: 5.13 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15511672483455155		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.15511672483455155 | validation: 0.1377416771932323]
	TIME [epoch: 5.13 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14859388261183223		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.14859388261183223 | validation: 0.1502542451798009]
	TIME [epoch: 5.14 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1365393382682585		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.1365393382682585 | validation: 0.17037149734030363]
	TIME [epoch: 5.17 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1554615043956295		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.1554615043956295 | validation: 0.13729578941075415]
	TIME [epoch: 5.14 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13203542238304328		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.13203542238304328 | validation: 0.15890382225406632]
	TIME [epoch: 5.14 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13421882583291728		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.13421882583291728 | validation: 0.13529951680296848]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12153972870290583		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.12153972870290583 | validation: 0.14892958329174175]
	TIME [epoch: 5.13 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12881274346495908		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.12881274346495908 | validation: 0.18032201734883152]
	TIME [epoch: 5.13 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18594218470062443		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.18594218470062443 | validation: 0.1907355909033958]
	TIME [epoch: 5.13 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14327344676811976		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.14327344676811976 | validation: 0.1340810028846904]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11384731317562985		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.11384731317562985 | validation: 0.155890495174429]
	TIME [epoch: 5.15 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13390948018563137		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.13390948018563137 | validation: 0.1368025158787084]
	TIME [epoch: 5.16 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15961350524122392		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.15961350524122392 | validation: 0.18146460216748206]
	TIME [epoch: 5.14 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14599569364863782		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.14599569364863782 | validation: 0.14894778201460468]
	TIME [epoch: 5.12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12752672011610347		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.12752672011610347 | validation: 0.15960473687480162]
	TIME [epoch: 5.13 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13869071343421052		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.13869071343421052 | validation: 0.19041435972246157]
	TIME [epoch: 5.13 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17440881611606604		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.17440881611606604 | validation: 0.19717375075333893]
	TIME [epoch: 5.13 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13808640080319484		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.13808640080319484 | validation: 0.1266482994785116]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12006159721312842		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.12006159721312842 | validation: 0.12901647275572362]
	TIME [epoch: 5.13 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11551705066708445		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.11551705066708445 | validation: 0.17237809596669212]
	TIME [epoch: 5.14 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17301703488797748		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.17301703488797748 | validation: 0.12542852842055704]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11841792518280857		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.11841792518280857 | validation: 0.1814300945170521]
	TIME [epoch: 5.14 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15483136818176838		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.15483136818176838 | validation: 0.11935336006380676]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1307939080456126		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.1307939080456126 | validation: 0.14590196183737225]
	TIME [epoch: 5.14 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1226431081238232		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.1226431081238232 | validation: 0.13577919275361403]
	TIME [epoch: 5.13 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12438950810193367		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.12438950810193367 | validation: 0.12031224306644595]
	TIME [epoch: 5.13 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12065577166677205		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.12065577166677205 | validation: 0.1374061298341071]
	TIME [epoch: 5.13 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1467180485630324		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.1467180485630324 | validation: 0.17446178023306147]
	TIME [epoch: 5.13 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14847776261422155		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.14847776261422155 | validation: 0.14187430229431755]
	TIME [epoch: 5.15 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11240393282577578		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.11240393282577578 | validation: 0.15772448880935458]
	TIME [epoch: 5.15 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13469299356389497		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.13469299356389497 | validation: 0.1151872679638456]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12034076601469111		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.12034076601469111 | validation: 0.14933898598178486]
	TIME [epoch: 5.14 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13234434614832308		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.13234434614832308 | validation: 0.20957661043809067]
	TIME [epoch: 5.13 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15165032249153773		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.15165032249153773 | validation: 0.13075894469138727]
	TIME [epoch: 5.13 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13480932845120353		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.13480932845120353 | validation: 0.13884159752216543]
	TIME [epoch: 5.13 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11836519714824868		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.11836519714824868 | validation: 0.1211665518592754]
	TIME [epoch: 5.13 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1090964261569212		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.1090964261569212 | validation: 0.13477773342064378]
	TIME [epoch: 5.13 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13124599536786463		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.13124599536786463 | validation: 0.1255108338569149]
	TIME [epoch: 5.16 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1126411770536315		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.1126411770536315 | validation: 0.16282569972688454]
	TIME [epoch: 5.17 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.126244793900551		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.126244793900551 | validation: 0.14207408810660188]
	TIME [epoch: 5.14 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14527602628102318		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.14527602628102318 | validation: 0.11499657934873952]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11893814681867337		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.11893814681867337 | validation: 0.1605390561249509]
	TIME [epoch: 5.13 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13706172949688206		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.13706172949688206 | validation: 0.1114617758593803]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10590403794087365		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.10590403794087365 | validation: 0.10962313509284964]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11157481945252483		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.11157481945252483 | validation: 0.13258190544226672]
	TIME [epoch: 5.13 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11911547363834375		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.11911547363834375 | validation: 0.19123494713041134]
	TIME [epoch: 5.13 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14653174460610757		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.14653174460610757 | validation: 0.12010838516022301]
	TIME [epoch: 5.16 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11143681166101571		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.11143681166101571 | validation: 0.13058173771885248]
	TIME [epoch: 5.13 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11418735947084949		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.11418735947084949 | validation: 0.12660978832662326]
	TIME [epoch: 5.13 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11767920394218159		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.11767920394218159 | validation: 0.10446827572134038]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10590550869396842		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.10590550869396842 | validation: 0.11704918992258782]
	TIME [epoch: 5.13 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11989608163748222		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.11989608163748222 | validation: 0.13218867094625822]
	TIME [epoch: 5.12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11560818706988743		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.11560818706988743 | validation: 0.19295319633271865]
	TIME [epoch: 5.13 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1251703277860353		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.1251703277860353 | validation: 0.10700659033208473]
	TIME [epoch: 5.13 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0966511613010715		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.0966511613010715 | validation: 0.11946576411190749]
	TIME [epoch: 5.13 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1215543775511979		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.1215543775511979 | validation: 0.17764549472055474]
	TIME [epoch: 5.16 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11863928905342053		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.11863928905342053 | validation: 0.11170067896478339]
	TIME [epoch: 5.13 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13687021701589597		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.13687021701589597 | validation: 0.11724807507952612]
	TIME [epoch: 5.13 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1152025852090925		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.1152025852090925 | validation: 0.13210139133935722]
	TIME [epoch: 5.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13362589592146823		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.13362589592146823 | validation: 0.12783829698951915]
	TIME [epoch: 5.13 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1031482525848749		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.1031482525848749 | validation: 0.15290871026505667]
	TIME [epoch: 5.12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12612648652620695		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.12612648652620695 | validation: 0.11817617143064355]
	TIME [epoch: 5.12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.111565946523264		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.111565946523264 | validation: 0.7634277938757261]
	TIME [epoch: 5.12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43524319804199824		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.43524319804199824 | validation: 0.2099909811878839]
	TIME [epoch: 5.13 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17341763407663424		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.17341763407663424 | validation: 0.12324844764942944]
	TIME [epoch: 5.17 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10971709415591978		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.10971709415591978 | validation: 0.09845615200522438]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1021440698354385		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.1021440698354385 | validation: 0.10498534166572533]
	TIME [epoch: 5.14 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1051205012996721		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.1051205012996721 | validation: 0.10500164156999639]
	TIME [epoch: 5.14 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12225826017891786		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.12225826017891786 | validation: 0.10625312228514092]
	TIME [epoch: 5.13 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11547821896854712		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.11547821896854712 | validation: 0.11858418963376341]
	TIME [epoch: 5.13 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11942793517721052		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.11942793517721052 | validation: 0.10845290198946617]
	TIME [epoch: 5.13 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10280068563812228		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.10280068563812228 | validation: 0.09919175994524149]
	TIME [epoch: 5.13 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10236911046187452		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.10236911046187452 | validation: 0.10618231346562716]
	TIME [epoch: 5.15 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14043548590049387		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.14043548590049387 | validation: 0.12345985013519958]
	TIME [epoch: 5.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13422903879874437		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.13422903879874437 | validation: 0.10783395524367298]
	TIME [epoch: 5.14 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10379381607173437		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.10379381607173437 | validation: 0.10695929820524273]
	TIME [epoch: 5.13 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10280497764865		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.10280497764865 | validation: 0.12158314542177776]
	TIME [epoch: 5.14 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12619367553336713		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.12619367553336713 | validation: 0.1251846414296574]
	TIME [epoch: 5.14 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10655551081069586		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.10655551081069586 | validation: 0.10522533058561045]
	TIME [epoch: 5.13 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12916656826191458		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.12916656826191458 | validation: 0.10575797706552086]
	TIME [epoch: 5.22 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10453775372373568		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.10453775372373568 | validation: 0.0999671369649826]
	TIME [epoch: 5.14 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10742507057489162		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.10742507057489162 | validation: 0.098170124125713]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09265507222810702		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.09265507222810702 | validation: 0.13316161854050915]
	TIME [epoch: 5.18 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11292630246035631		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.11292630246035631 | validation: 0.1150281948596063]
	TIME [epoch: 5.15 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12998332455400974		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.12998332455400974 | validation: 0.09961221740130241]
	TIME [epoch: 5.13 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09480960255424792		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.09480960255424792 | validation: 0.09445069826115217]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09597643732319476		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.09597643732319476 | validation: 0.10798420280374713]
	TIME [epoch: 5.14 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14230432267599386		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.14230432267599386 | validation: 0.15040618083157575]
	TIME [epoch: 5.13 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1222489835630162		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.1222489835630162 | validation: 0.11041268379831941]
	TIME [epoch: 5.13 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09865767848166138		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.09865767848166138 | validation: 0.09304016780357313]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09585503379623386		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.09585503379623386 | validation: 0.0949006763489301]
	TIME [epoch: 5.15 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11015301156401561		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.11015301156401561 | validation: 0.17985711610073074]
	TIME [epoch: 5.16 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11620618718914998		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.11620618718914998 | validation: 0.09822284787088392]
	TIME [epoch: 5.13 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09608660448679883		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.09608660448679883 | validation: 0.09590553383584255]
	TIME [epoch: 5.13 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0993014115654959		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.0993014115654959 | validation: 0.11865489196981968]
	TIME [epoch: 5.13 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11662256178420272		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.11662256178420272 | validation: 0.1001523645620051]
	TIME [epoch: 5.13 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09922448001518186		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.09922448001518186 | validation: 0.10634881517274297]
	TIME [epoch: 5.13 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10031083634999464		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.10031083634999464 | validation: 0.09471133463593207]
	TIME [epoch: 5.13 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.094959503173179		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.094959503173179 | validation: 0.09811289580025793]
	TIME [epoch: 5.13 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08816918559877819		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.08816918559877819 | validation: 0.10099364374964745]
	TIME [epoch: 5.15 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1146115951345151		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.1146115951345151 | validation: 0.09304525065831165]
	TIME [epoch: 5.16 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11051863907117493		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.11051863907117493 | validation: 0.11273104655525665]
	TIME [epoch: 5.14 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09533265825885992		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.09533265825885992 | validation: 0.10303382139607961]
	TIME [epoch: 5.13 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09887374077848486		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.09887374077848486 | validation: 0.09164869303518744]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0981995951549855		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.0981995951549855 | validation: 0.08915018346469278]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09089922250782896		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.09089922250782896 | validation: 0.09226180652935417]
	TIME [epoch: 5.13 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10475850603167736		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.10475850603167736 | validation: 0.09273967251646022]
	TIME [epoch: 5.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08801309248275617		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.08801309248275617 | validation: 0.09380113527304818]
	TIME [epoch: 5.12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10511003390603768		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.10511003390603768 | validation: 0.10169335664826051]
	TIME [epoch: 5.14 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0925027375664225		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.0925027375664225 | validation: 0.11915970096410733]
	TIME [epoch: 5.15 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11278840577036583		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.11278840577036583 | validation: 0.09751625126606697]
	TIME [epoch: 5.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08918705149277248		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.08918705149277248 | validation: 0.09627743585090504]
	TIME [epoch: 5.13 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09902368255461576		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.09902368255461576 | validation: 0.1511864603303661]
	TIME [epoch: 5.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10232904662990304		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.10232904662990304 | validation: 0.08725545539724577]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08154760404958394		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.08154760404958394 | validation: 0.08431025563818137]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10306206779067048		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.10306206779067048 | validation: 0.08902857921466878]
	TIME [epoch: 5.13 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09418681186060654		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.09418681186060654 | validation: 0.09199120259926274]
	TIME [epoch: 5.13 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09328948899532427		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.09328948899532427 | validation: 0.09411653104734723]
	TIME [epoch: 5.17 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1902041912232849		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.1902041912232849 | validation: 0.16178869067267096]
	TIME [epoch: 5.14 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10130591547375886		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.10130591547375886 | validation: 0.09688209984335658]
	TIME [epoch: 5.13 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08771857389523222		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.08771857389523222 | validation: 0.10977727563387052]
	TIME [epoch: 5.13 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1086651903246985		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.1086651903246985 | validation: 0.11054852545762384]
	TIME [epoch: 5.13 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09848360915695673		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.09848360915695673 | validation: 0.08849063997348486]
	TIME [epoch: 5.12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08596690600622832		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.08596690600622832 | validation: 0.09144958870852163]
	TIME [epoch: 5.14 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08725596634092142		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.08725596634092142 | validation: 0.08747307202372637]
	TIME [epoch: 5.14 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09165948456989004		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.09165948456989004 | validation: 0.09003150939740165]
	TIME [epoch: 5.15 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08724543032685156		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.08724543032685156 | validation: 0.10629189265382517]
	TIME [epoch: 5.18 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12335834188818123		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.12335834188818123 | validation: 0.10182747925677035]
	TIME [epoch: 5.15 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09156843915729841		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.09156843915729841 | validation: 0.09757474373715436]
	TIME [epoch: 5.14 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10343727152237264		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.10343727152237264 | validation: 0.10417046552885134]
	TIME [epoch: 5.14 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08573675277299596		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.08573675277299596 | validation: 0.08620997291481007]
	TIME [epoch: 5.14 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0789731748850894		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.0789731748850894 | validation: 0.08513456079656824]
	TIME [epoch: 5.14 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09378937341745679		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.09378937341745679 | validation: 0.09369355795833845]
	TIME [epoch: 5.14 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08938908540348366		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.08938908540348366 | validation: 0.10574590562716321]
	TIME [epoch: 5.14 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09492753485195331		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.09492753485195331 | validation: 0.10529431968895697]
	TIME [epoch: 5.14 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09259078566925866		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.09259078566925866 | validation: 0.13299971990787113]
	TIME [epoch: 5.18 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11054468426235561		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.11054468426235561 | validation: 0.08468939653931125]
	TIME [epoch: 5.14 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08536095092826469		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.08536095092826469 | validation: 0.08428861248935299]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11881017769101629		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.11881017769101629 | validation: 0.11311473285788444]
	TIME [epoch: 5.14 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08555286505201548		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.08555286505201548 | validation: 0.09592305745961169]
	TIME [epoch: 5.14 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08699911365725564		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.08699911365725564 | validation: 0.086746236327669]
	TIME [epoch: 5.14 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08272446830457475		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.08272446830457475 | validation: 0.08053130772974926]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08483092828040521		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.08483092828040521 | validation: 0.0972964986899617]
	TIME [epoch: 5.14 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09033585788849487		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.09033585788849487 | validation: 0.0952039322541892]
	TIME [epoch: 5.14 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08554415735737517		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.08554415735737517 | validation: 0.08687441055424097]
	TIME [epoch: 5.17 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08374490528509047		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.08374490528509047 | validation: 0.12089065799485704]
	TIME [epoch: 5.14 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09761359116240023		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.09761359116240023 | validation: 0.08466869965975635]
	TIME [epoch: 5.13 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08183295558280372		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.08183295558280372 | validation: 0.09212474343351473]
	TIME [epoch: 5.13 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08306970292978365		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.08306970292978365 | validation: 0.08010449713119883]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07743075815570666		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.07743075815570666 | validation: 0.10010985749858817]
	TIME [epoch: 5.13 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08543934863783567		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.08543934863783567 | validation: 0.08526684436592269]
	TIME [epoch: 5.12 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09453889244353679		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.09453889244353679 | validation: 0.09048882077188222]
	TIME [epoch: 5.12 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08326329725352331		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.08326329725352331 | validation: 0.10044652179084654]
	TIME [epoch: 5.14 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10455270494374688		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.10455270494374688 | validation: 0.09405856604527377]
	TIME [epoch: 5.14 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08580432805523165		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.08580432805523165 | validation: 0.0861507220511783]
	TIME [epoch: 5.13 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07702425463096188		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.07702425463096188 | validation: 0.10367274312019639]
	TIME [epoch: 5.12 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10241011868960795		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.10241011868960795 | validation: 0.099820860684821]
	TIME [epoch: 5.13 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.131371573655891		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.131371573655891 | validation: 0.13742289040834688]
	TIME [epoch: 5.12 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08888328031466053		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.08888328031466053 | validation: 0.07162506257491133]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07501435985116392		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.07501435985116392 | validation: 0.08511713656960856]
	TIME [epoch: 5.12 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08237306595652791		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.08237306595652791 | validation: 0.07590263475050105]
	TIME [epoch: 5.13 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08707367220616721		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.08707367220616721 | validation: 0.08101947005962182]
	TIME [epoch: 5.15 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07888761509218094		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.07888761509218094 | validation: 0.08610430601243987]
	TIME [epoch: 5.15 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08044278343271999		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.08044278343271999 | validation: 0.07751901768925536]
	TIME [epoch: 5.13 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08139503513607202		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.08139503513607202 | validation: 0.08341163531301443]
	TIME [epoch: 5.12 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08460905471731645		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.08460905471731645 | validation: 0.09411027921664217]
	TIME [epoch: 5.12 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08575655388091813		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.08575655388091813 | validation: 0.09512836179154141]
	TIME [epoch: 5.12 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08120044753230476		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.08120044753230476 | validation: 0.07650551074978278]
	TIME [epoch: 5.12 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07268185030029005		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.07268185030029005 | validation: 0.11377558944959874]
	TIME [epoch: 5.12 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10625280322976906		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.10625280322976906 | validation: 0.07912045241311938]
	TIME [epoch: 5.13 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12089335837181546		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.12089335837181546 | validation: 0.09286247555370877]
	TIME [epoch: 5.15 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08045981814977007		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.08045981814977007 | validation: 0.08299757443257808]
	TIME [epoch: 5.15 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07637338475596787		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.07637338475596787 | validation: 0.08926014017573503]
	TIME [epoch: 5.13 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07419261386459977		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.07419261386459977 | validation: 0.07874418471792109]
	TIME [epoch: 5.13 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12579542606167177		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.12579542606167177 | validation: 0.07477182249144816]
	TIME [epoch: 5.13 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07696711137628436		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.07696711137628436 | validation: 0.0873698276833739]
	TIME [epoch: 5.13 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07694844528748997		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.07694844528748997 | validation: 0.07362243978894162]
	TIME [epoch: 5.12 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08160017578512524		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.08160017578512524 | validation: 0.07535757143890537]
	TIME [epoch: 5.13 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08266959306919881		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.08266959306919881 | validation: 0.10883940100161138]
	TIME [epoch: 5.13 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0851288481045879		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.0851288481045879 | validation: 0.0797604187055086]
	TIME [epoch: 5.15 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07623561621292282		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.07623561621292282 | validation: 0.07386434562605768]
	TIME [epoch: 5.15 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07353943615155294		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.07353943615155294 | validation: 0.093384820932338]
	TIME [epoch: 5.13 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07949992598040886		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.07949992598040886 | validation: 0.0720112789393329]
	TIME [epoch: 5.13 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08286840089600843		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.08286840089600843 | validation: 0.08100776879507059]
	TIME [epoch: 5.13 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0894644414421477		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0894644414421477 | validation: 0.08741764256974]
	TIME [epoch: 5.12 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07716745296122227		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.07716745296122227 | validation: 0.07823477498935116]
	TIME [epoch: 5.13 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.083405054401579		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.083405054401579 | validation: 0.08148098220102803]
	TIME [epoch: 5.13 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0762503715738803		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.0762503715738803 | validation: 0.08010195269740385]
	TIME [epoch: 5.13 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08248619630674107		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.08248619630674107 | validation: 0.07673465243815789]
	TIME [epoch: 5.15 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07520607361671		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.07520607361671 | validation: 0.13116464538933797]
	TIME [epoch: 5.15 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12005193534175003		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.12005193534175003 | validation: 0.07877111744287155]
	TIME [epoch: 5.14 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07261316029869602		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.07261316029869602 | validation: 0.07462835149934871]
	TIME [epoch: 5.13 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0743380440423712		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.0743380440423712 | validation: 0.08190443848544152]
	TIME [epoch: 5.12 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06987436202264179		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.06987436202264179 | validation: 0.06912855176996845]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0701488254911059		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.0701488254911059 | validation: 0.09684795137820879]
	TIME [epoch: 5.12 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08967092333616256		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.08967092333616256 | validation: 0.09520019232764589]
	TIME [epoch: 5.12 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07654662838391355		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.07654662838391355 | validation: 0.07185305255706181]
	TIME [epoch: 5.12 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07430056120797628		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.07430056120797628 | validation: 0.07666977659701332]
	TIME [epoch: 5.14 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07259394107279303		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.07259394107279303 | validation: 0.07043353823386338]
	TIME [epoch: 5.14 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06975214175399111		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.06975214175399111 | validation: 0.09456357699284168]
	TIME [epoch: 5.12 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08161781793052969		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.08161781793052969 | validation: 0.07547130689077389]
	TIME [epoch: 5.12 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08113586249369381		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.08113586249369381 | validation: 0.11481881374013046]
	TIME [epoch: 5.12 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08102281178409637		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.08102281178409637 | validation: 0.07197540823066227]
	TIME [epoch: 5.12 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07165932440435163		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.07165932440435163 | validation: 0.07276352188075888]
	TIME [epoch: 5.12 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07263922804932771		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.07263922804932771 | validation: 0.06572356085655481]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06950242391576456		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.06950242391576456 | validation: 0.07130856114408121]
	TIME [epoch: 5.14 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0700329676080661		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.0700329676080661 | validation: 0.08462403280089273]
	TIME [epoch: 5.16 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07461076970948793		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.07461076970948793 | validation: 0.06490935327511707]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07765199274634466		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.07765199274634466 | validation: 0.09268735705296449]
	TIME [epoch: 5.14 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07307754401739118		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.07307754401739118 | validation: 0.06723118503273041]
	TIME [epoch: 5.13 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06747269863785438		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.06747269863785438 | validation: 0.06996077525474423]
	TIME [epoch: 5.15 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07257609684120167		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.07257609684120167 | validation: 0.07736470800854747]
	TIME [epoch: 5.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07178257006996333		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.07178257006996333 | validation: 0.07094517259073585]
	TIME [epoch: 5.09 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0691920347044332		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.0691920347044332 | validation: 0.07148773949629894]
	TIME [epoch: 5.16 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07388231094689218		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.07388231094689218 | validation: 0.0712062995228555]
	TIME [epoch: 5.19 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07996519570717386		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.07996519570717386 | validation: 0.0664143498870839]
	TIME [epoch: 5.18 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06737621916543253		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.06737621916543253 | validation: 0.09220360088608634]
	TIME [epoch: 5.18 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07057550341941508		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.07057550341941508 | validation: 0.06579658474872827]
	TIME [epoch: 5.16 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07051040469768402		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.07051040469768402 | validation: 0.06897013159792689]
	TIME [epoch: 5.18 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07758545262209657		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.07758545262209657 | validation: 0.07498136698705765]
	TIME [epoch: 5.15 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06682012448414151		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.06682012448414151 | validation: 0.06459710087434414]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06829198785677895		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.06829198785677895 | validation: 0.07064071696554612]
	TIME [epoch: 5.14 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06572205834591074		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.06572205834591074 | validation: 0.07233732245404279]
	TIME [epoch: 5.14 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09213757527456184		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.09213757527456184 | validation: 0.07941846094024356]
	TIME [epoch: 5.14 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06826780314081105		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.06826780314081105 | validation: 0.07862437188721089]
	TIME [epoch: 5.18 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06618787301546061		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.06618787301546061 | validation: 0.06469952806432136]
	TIME [epoch: 5.16 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06871294982121816		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.06871294982121816 | validation: 0.07696501686195582]
	TIME [epoch: 5.16 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06759185054190947		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.06759185054190947 | validation: 0.12272382878911653]
	TIME [epoch: 5.14 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08938250075815096		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.08938250075815096 | validation: 0.060420311953512766]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06245982611580922		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.06245982611580922 | validation: 0.06146186656065621]
	TIME [epoch: 5.13 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06192480551819364		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.06192480551819364 | validation: 0.08676128344764682]
	TIME [epoch: 5.13 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08855866756187472		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.08855866756187472 | validation: 0.06473707595258307]
	TIME [epoch: 5.13 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07122059420813177		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.07122059420813177 | validation: 0.059148749270315076]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06236623190979958		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.06236623190979958 | validation: 0.06365667177799259]
	TIME [epoch: 5.17 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0656162087044001		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.0656162087044001 | validation: 0.06726156489425784]
	TIME [epoch: 5.13 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06785081497989588		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.06785081497989588 | validation: 0.07110494560515]
	TIME [epoch: 5.12 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06694670025580911		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.06694670025580911 | validation: 0.06045736142312962]
	TIME [epoch: 5.13 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06347262352221222		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.06347262352221222 | validation: 0.07966260769233935]
	TIME [epoch: 5.13 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06908179381186272		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.06908179381186272 | validation: 0.06339664255676489]
	TIME [epoch: 5.13 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05918103349568585		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.05918103349568585 | validation: 0.0633138088754875]
	TIME [epoch: 5.12 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0624184075606084		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.0624184075606084 | validation: 0.0642429080211758]
	TIME [epoch: 5.13 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06606772442367212		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.06606772442367212 | validation: 0.06566610332026171]
	TIME [epoch: 5.14 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07902012622982195		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.07902012622982195 | validation: 0.0786370054776061]
	TIME [epoch: 5.16 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07245126587161506		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.07245126587161506 | validation: 0.089434458108963]
	TIME [epoch: 5.13 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08020922513423742		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.08020922513423742 | validation: 0.06290259515612157]
	TIME [epoch: 5.13 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06318644838525571		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.06318644838525571 | validation: 0.06277320029343769]
	TIME [epoch: 5.13 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06563957882347668		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.06563957882347668 | validation: 0.06763286021552725]
	TIME [epoch: 5.13 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06452945268171516		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.06452945268171516 | validation: 0.057219075281548]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06048338764071691		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.06048338764071691 | validation: 0.05692869453612167]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06830670359506605		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.06830670359506605 | validation: 0.07185851965329097]
	TIME [epoch: 5.13 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06455283740646595		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.06455283740646595 | validation: 0.06369381926419006]
	TIME [epoch: 5.15 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08280523481900882		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.08280523481900882 | validation: 0.07194878934545884]
	TIME [epoch: 5.16 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059868358678563646		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.059868358678563646 | validation: 0.0623664009870155]
	TIME [epoch: 5.14 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06248406644133053		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.06248406644133053 | validation: 0.06313200999930067]
	TIME [epoch: 5.13 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06422914054921448		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.06422914054921448 | validation: 0.05781793883004145]
	TIME [epoch: 5.18 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058465320386563054		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.058465320386563054 | validation: 0.07457287169099455]
	TIME [epoch: 5.13 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06617226294401463		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.06617226294401463 | validation: 0.0608296065417018]
	TIME [epoch: 5.13 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06243262260218732		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.06243262260218732 | validation: 0.06370289388617012]
	TIME [epoch: 5.13 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0657645486434826		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.0657645486434826 | validation: 0.05610898282987803]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07122291640916457		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.07122291640916457 | validation: 0.07026945484060429]
	TIME [epoch: 5.16 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06484525121245831		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.06484525121245831 | validation: 0.06259291572063387]
	TIME [epoch: 5.16 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640619854640106		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.06640619854640106 | validation: 0.06492510357223799]
	TIME [epoch: 5.14 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06053003813598543		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.06053003813598543 | validation: 0.05970944578045799]
	TIME [epoch: 5.13 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06100121347167231		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.06100121347167231 | validation: 0.06171306263679146]
	TIME [epoch: 5.13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06031865740711319		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.06031865740711319 | validation: 0.05768434775830816]
	TIME [epoch: 5.13 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06821478103958883		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.06821478103958883 | validation: 0.05710422525921488]
	TIME [epoch: 5.13 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05796017925762362		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.05796017925762362 | validation: 0.05629090384290711]
	TIME [epoch: 5.13 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06314562186813123		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.06314562186813123 | validation: 0.061118016050816494]
	TIME [epoch: 5.14 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05968865828304874		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.05968865828304874 | validation: 0.07013468866158414]
	TIME [epoch: 5.15 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059363492796450874		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.059363492796450874 | validation: 0.05660551272944933]
	TIME [epoch: 5.16 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06258935605519589		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.06258935605519589 | validation: 0.06637182543075537]
	TIME [epoch: 5.14 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0587966545877802		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.0587966545877802 | validation: 0.07496886019291829]
	TIME [epoch: 5.14 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15792889141437122		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.15792889141437122 | validation: 0.10944909756973017]
	TIME [epoch: 5.14 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09581114230418736		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.09581114230418736 | validation: 0.06174978770295259]
	TIME [epoch: 5.14 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060265299705577		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.06060265299705577 | validation: 0.056529681404186176]
	TIME [epoch: 5.14 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061633299200019684		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.061633299200019684 | validation: 0.059264285812794325]
	TIME [epoch: 5.14 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05738357332056258		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.05738357332056258 | validation: 0.05523886316562843]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06259610276538698		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.06259610276538698 | validation: 0.060016915059106966]
	TIME [epoch: 5.15 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05662311184100834		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.05662311184100834 | validation: 0.05643285124932312]
	TIME [epoch: 5.16 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0643012802694562		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.0643012802694562 | validation: 0.05338525703428226]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05716441578888125		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.05716441578888125 | validation: 0.05478790568847708]
	TIME [epoch: 5.12 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054863368041504136		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.054863368041504136 | validation: 0.06833252563711983]
	TIME [epoch: 5.12 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05704758881134186		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.05704758881134186 | validation: 0.06290430153415003]
	TIME [epoch: 5.12 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06109528388569817		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.06109528388569817 | validation: 0.058543526257327755]
	TIME [epoch: 5.12 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06064770126334092		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.06064770126334092 | validation: 0.0531748057402318]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057432185300401194		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.057432185300401194 | validation: 0.05423236688716395]
	TIME [epoch: 5.12 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058879420144945974		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.058879420144945974 | validation: 0.06032615187390461]
	TIME [epoch: 5.16 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06753150391875495		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.06753150391875495 | validation: 0.05236771746110248]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05341206304121694		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.05341206304121694 | validation: 0.04914731214951587]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05391933567134924		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.05391933567134924 | validation: 0.0588670214263758]
	TIME [epoch: 5.12 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06583352720778643		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.06583352720778643 | validation: 0.05533728052656553]
	TIME [epoch: 5.12 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05303897148057889		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.05303897148057889 | validation: 0.05699212793301241]
	TIME [epoch: 5.12 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055681282793138206		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.055681282793138206 | validation: 0.06002144096338322]
	TIME [epoch: 5.12 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05632129011044762		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.05632129011044762 | validation: 0.055573382523585166]
	TIME [epoch: 5.12 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05423484985427793		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.05423484985427793 | validation: 0.06522806827069672]
	TIME [epoch: 5.12 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06072967663638168		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.06072967663638168 | validation: 0.05523727794550153]
	TIME [epoch: 5.16 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05599737501788882		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.05599737501788882 | validation: 0.06263556342526277]
	TIME [epoch: 5.12 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05810784389071358		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.05810784389071358 | validation: 0.061842482686976716]
	TIME [epoch: 5.12 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09160980835015908		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.09160980835015908 | validation: 0.16274907645833359]
	TIME [epoch: 5.12 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11965764926946681		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.11965764926946681 | validation: 0.06617809142979274]
	TIME [epoch: 5.12 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056622211309820654		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.056622211309820654 | validation: 0.04911192426673887]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05329602007218451		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.05329602007218451 | validation: 0.05326542250962641]
	TIME [epoch: 5.13 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052242958386296216		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.052242958386296216 | validation: 0.048423762671147355]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05047672139105755		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.05047672139105755 | validation: 0.05284384920191406]
	TIME [epoch: 5.14 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05460338149988201		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.05460338149988201 | validation: 0.05150139477078269]
	TIME [epoch: 5.17 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051753157137242004		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.051753157137242004 | validation: 0.056625017122521104]
	TIME [epoch: 5.14 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0541033407211413		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0541033407211413 | validation: 0.08051879668100109]
	TIME [epoch: 5.13 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0680430425143426		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.0680430425143426 | validation: 0.052517102553296086]
	TIME [epoch: 5.13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051312551850949996		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.051312551850949996 | validation: 0.05252490620657209]
	TIME [epoch: 5.13 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07271362039489908		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.07271362039489908 | validation: 0.0800715561103945]
	TIME [epoch: 5.14 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05980259076263786		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.05980259076263786 | validation: 0.05140587361492412]
	TIME [epoch: 5.13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05051953491539459		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.05051953491539459 | validation: 0.050539303656827135]
	TIME [epoch: 5.13 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05004486178359517		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.05004486178359517 | validation: 0.051459219077202925]
	TIME [epoch: 5.15 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050702397251501996		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.050702397251501996 | validation: 0.0482920914961542]
	TIME [epoch: 5.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05721258833580973		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.05721258833580973 | validation: 0.06684404410226753]
	TIME [epoch: 5.14 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062256033222933396		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.062256033222933396 | validation: 0.05547060184762803]
	TIME [epoch: 5.13 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05249619543077021		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.05249619543077021 | validation: 0.046933343911016924]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05337453261087076		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.05337453261087076 | validation: 0.04702739918911356]
	TIME [epoch: 5.13 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0513366874549938		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.0513366874549938 | validation: 0.051752386734788096]
	TIME [epoch: 5.13 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0556704710938719		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.0556704710938719 | validation: 0.05180488240223212]
	TIME [epoch: 5.13 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05255302420985422		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.05255302420985422 | validation: 0.05354414901299566]
	TIME [epoch: 5.13 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0595094989068779		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.0595094989068779 | validation: 0.05280185664571234]
	TIME [epoch: 5.15 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05124827269255339		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.05124827269255339 | validation: 0.04845786883733291]
	TIME [epoch: 5.16 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05062575817158353		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.05062575817158353 | validation: 0.04771027102987588]
	TIME [epoch: 5.13 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050319115865503763		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.050319115865503763 | validation: 0.06807823336206062]
	TIME [epoch: 5.12 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05790077689481707		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.05790077689481707 | validation: 0.048158690217844566]
	TIME [epoch: 5.12 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05409799690250622		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.05409799690250622 | validation: 0.05234070721632199]
	TIME [epoch: 5.12 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05066741485575763		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.05066741485575763 | validation: 0.053086323613026495]
	TIME [epoch: 5.12 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05558251992332651		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.05558251992332651 | validation: 0.05187530510511737]
	TIME [epoch: 5.12 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050763182156173495		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.050763182156173495 | validation: 0.05067710302597916]
	TIME [epoch: 5.12 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052311966576852796		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.052311966576852796 | validation: 0.0460766916908463]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05199336459169848		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.05199336459169848 | validation: 0.06396289935061691]
	TIME [epoch: 5.14 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05115348184387081		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.05115348184387081 | validation: 0.04749931490135764]
	TIME [epoch: 5.12 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05467689674016523		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.05467689674016523 | validation: 0.048850439360509826]
	TIME [epoch: 5.12 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0739437785783459		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0739437785783459 | validation: 0.0609219590672849]
	TIME [epoch: 5.12 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06464980960468625		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.06464980960468625 | validation: 0.04670099559302949]
	TIME [epoch: 5.12 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0507328410742257		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0507328410742257 | validation: 0.04755075266504949]
	TIME [epoch: 5.12 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04640019080397216		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.04640019080397216 | validation: 0.0497326646704156]
	TIME [epoch: 5.12 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04668147502506274		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.04668147502506274 | validation: 0.05329798534589665]
	TIME [epoch: 5.12 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049553124860909646		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.049553124860909646 | validation: 0.04780576178262072]
	TIME [epoch: 5.14 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04907498667049384		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.04907498667049384 | validation: 0.0501694803299776]
	TIME [epoch: 5.15 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05056730366811918		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.05056730366811918 | validation: 0.08039544549853556]
	TIME [epoch: 5.13 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05728476363718067		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.05728476363718067 | validation: 0.05157172240993077]
	TIME [epoch: 5.12 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04830513085968261		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.04830513085968261 | validation: 0.04828499167870516]
	TIME [epoch: 5.12 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06597221301006592		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.06597221301006592 | validation: 0.05778263188621566]
	TIME [epoch: 5.12 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05307323711594371		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.05307323711594371 | validation: 0.04960607293118563]
	TIME [epoch: 5.12 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05164159996016629		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.05164159996016629 | validation: 0.0466612455362321]
	TIME [epoch: 5.12 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04704077555031285		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.04704077555031285 | validation: 0.0468285974172027]
	TIME [epoch: 5.12 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04705266046708268		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.04705266046708268 | validation: 0.048013802452170545]
	TIME [epoch: 5.14 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05246047732318406		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.05246047732318406 | validation: 0.0475268976686278]
	TIME [epoch: 5.15 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05555517310755301		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.05555517310755301 | validation: 0.04830646271052837]
	TIME [epoch: 5.12 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04882377610999287		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.04882377610999287 | validation: 0.050410530322187896]
	TIME [epoch: 5.12 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04967232701619292		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.04967232701619292 | validation: 0.047965760396427276]
	TIME [epoch: 5.12 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04779508431765829		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.04779508431765829 | validation: 0.04872829121669792]
	TIME [epoch: 5.14 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051157807577234767		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.051157807577234767 | validation: 0.047178683758836285]
	TIME [epoch: 5.09 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046219198644207836		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.046219198644207836 | validation: 0.05958072026364197]
	TIME [epoch: 5.13 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048607911917733344		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.048607911917733344 | validation: 0.06258348756316799]
	TIME [epoch: 5.14 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052599918902367884		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.052599918902367884 | validation: 0.054414529194865335]
	TIME [epoch: 5.16 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04696800503037837		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.04696800503037837 | validation: 0.04450074864565799]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04799262089615522		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.04799262089615522 | validation: 0.05654127463387986]
	TIME [epoch: 5.14 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05020632561828876		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.05020632561828876 | validation: 0.05709245974431644]
	TIME [epoch: 5.13 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047660572572414676		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.047660572572414676 | validation: 0.04806709423911671]
	TIME [epoch: 5.12 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04661917976382604		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.04661917976382604 | validation: 0.04735773851153251]
	TIME [epoch: 5.12 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0474372580244835		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0474372580244835 | validation: 0.044523251590930846]
	TIME [epoch: 5.12 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044603985944150194		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.044603985944150194 | validation: 0.048638177671403784]
	TIME [epoch: 5.12 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05311720848161175		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.05311720848161175 | validation: 0.04593232430437913]
	TIME [epoch: 5.12 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050718729110401733		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.050718729110401733 | validation: 0.04665217830723528]
	TIME [epoch: 5.15 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050328254260110145		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.050328254260110145 | validation: 0.044323965960599676]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04958457694053209		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.04958457694053209 | validation: 0.04560996916700748]
	TIME [epoch: 5.16 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044674843932280876		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.044674843932280876 | validation: 0.04376892056526466]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04633589160889739		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.04633589160889739 | validation: 0.04024816994477552]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046776475417152796		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.046776475417152796 | validation: 0.04379650130736037]
	TIME [epoch: 5.14 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04329254039895332		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.04329254039895332 | validation: 0.04036897252033206]
	TIME [epoch: 5.13 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04915771698432737		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.04915771698432737 | validation: 0.04826480342167932]
	TIME [epoch: 5.14 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048093734567675836		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.048093734567675836 | validation: 0.043994265127809296]
	TIME [epoch: 5.15 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046792689182092215		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.046792689182092215 | validation: 0.042072838964271324]
	TIME [epoch: 5.17 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07132222695589555		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.07132222695589555 | validation: 0.06556875763173614]
	TIME [epoch: 5.14 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050233729124616835		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.050233729124616835 | validation: 0.0404834569273512]
	TIME [epoch: 5.13 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04664233400383083		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.04664233400383083 | validation: 0.04250560214522689]
	TIME [epoch: 5.14 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043299228223311656		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.043299228223311656 | validation: 0.04673472754741931]
	TIME [epoch: 5.13 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042978749033820124		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.042978749033820124 | validation: 0.04226837419197295]
	TIME [epoch: 5.13 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04324190010694083		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.04324190010694083 | validation: 0.04453804278338702]
	TIME [epoch: 5.13 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044974305178014054		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.044974305178014054 | validation: 0.045570106960875864]
	TIME [epoch: 5.13 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044268239622039		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.044268239622039 | validation: 0.04318988688480916]
	TIME [epoch: 5.14 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04540084531331578		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.04540084531331578 | validation: 0.055836764196220426]
	TIME [epoch: 5.17 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04828746254317875		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.04828746254317875 | validation: 0.055082381944947176]
	TIME [epoch: 5.14 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0487467560834064		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0487467560834064 | validation: 0.06633956075554426]
	TIME [epoch: 5.13 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04891094946190337		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.04891094946190337 | validation: 0.04201767564427597]
	TIME [epoch: 5.13 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042303916028667045		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.042303916028667045 | validation: 0.0464685915505985]
	TIME [epoch: 5.13 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04307069954561721		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.04307069954561721 | validation: 0.05167300240811201]
	TIME [epoch: 5.13 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04347865260532103		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.04347865260532103 | validation: 0.03969395034991639]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04370035530483761		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.04370035530483761 | validation: 0.042558504384594326]
	TIME [epoch: 5.13 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04116229601995705		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.04116229601995705 | validation: 0.03838300505669484]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046132249039124794		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.046132249039124794 | validation: 0.05957337038702813]
	TIME [epoch: 5.16 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054389742283078656		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.054389742283078656 | validation: 0.04146916928725513]
	TIME [epoch: 5.12 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043504601492755396		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.043504601492755396 | validation: 0.047398354938966256]
	TIME [epoch: 5.12 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04364573613362279		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.04364573613362279 | validation: 0.04163768198153986]
	TIME [epoch: 5.12 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04872611943428533		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.04872611943428533 | validation: 0.04199100715559121]
	TIME [epoch: 5.12 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04479076262293892		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.04479076262293892 | validation: 0.04036618097921587]
	TIME [epoch: 5.12 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042637176829013586		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.042637176829013586 | validation: 0.04145108242532685]
	TIME [epoch: 5.12 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0415107417245396		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0415107417245396 | validation: 0.03915526695385876]
	TIME [epoch: 5.12 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04216231047692728		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.04216231047692728 | validation: 0.04054515086960837]
	TIME [epoch: 5.15 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04247534084530907		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.04247534084530907 | validation: 0.04149821071093508]
	TIME [epoch: 5.15 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04443337602351776		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.04443337602351776 | validation: 0.039264727460845424]
	TIME [epoch: 5.13 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04126801481460512		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.04126801481460512 | validation: 0.04072279229095142]
	TIME [epoch: 5.12 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07266968165452914		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.07266968165452914 | validation: 0.14021281081698184]
	TIME [epoch: 5.12 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09757340784878671		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.09757340784878671 | validation: 0.05449771764623979]
	TIME [epoch: 5.12 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045242245958864016		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.045242245958864016 | validation: 0.03681082418290425]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040468025899124606		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.040468025899124606 | validation: 0.03768786996301861]
	TIME [epoch: 5.12 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04039299215716265		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.04039299215716265 | validation: 0.03988812426446428]
	TIME [epoch: 5.12 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0405134477592354		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.0405134477592354 | validation: 0.041268872044580227]
	TIME [epoch: 5.14 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04099601110056142		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.04099601110056142 | validation: 0.036689682125842366]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041616709370377966		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.041616709370377966 | validation: 0.04340183041176935]
	TIME [epoch: 5.12 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04199821105220769		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.04199821105220769 | validation: 0.03889937124680716]
	TIME [epoch: 5.11 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04125961647285306		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.04125961647285306 | validation: 0.03794442791599649]
	TIME [epoch: 5.11 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04078069528825774		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.04078069528825774 | validation: 0.03951220525782688]
	TIME [epoch: 5.12 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03988873640266556		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.03988873640266556 | validation: 0.0374248959705637]
	TIME [epoch: 5.11 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06432226099132476		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.06432226099132476 | validation: 0.04703845049031706]
	TIME [epoch: 5.12 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041164356462501744		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.041164356462501744 | validation: 0.038106145004130315]
	TIME [epoch: 5.11 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04161378960149764		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.04161378960149764 | validation: 0.03674408780983775]
	TIME [epoch: 5.14 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040249237078274366		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.040249237078274366 | validation: 0.03674658115601721]
	TIME [epoch: 5.14 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040637655087533184		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.040637655087533184 | validation: 0.03764405112439691]
	TIME [epoch: 5.12 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041023998873002974		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.041023998873002974 | validation: 0.05045628805665385]
	TIME [epoch: 5.11 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04744971365196596		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.04744971365196596 | validation: 0.04593209494111264]
	TIME [epoch: 5.14 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04129415373808578		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.04129415373808578 | validation: 0.038532822231569744]
	TIME [epoch: 5.14 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042438163730748324		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.042438163730748324 | validation: 0.04966205410548709]
	TIME [epoch: 5.13 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042613799390529936		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.042613799390529936 | validation: 0.05093199531712578]
	TIME [epoch: 5.14 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04025492800676452		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.04025492800676452 | validation: 0.03930352749389213]
	TIME [epoch: 5.13 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03924665912869467		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.03924665912869467 | validation: 0.03857391204447243]
	TIME [epoch: 5.16 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03982077017175338		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.03982077017175338 | validation: 0.038458141179336894]
	TIME [epoch: 5.16 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04105507558124081		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.04105507558124081 | validation: 0.03835228025551703]
	TIME [epoch: 5.14 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03819635015244507		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.03819635015244507 | validation: 0.03834949846126163]
	TIME [epoch: 5.13 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0399631830754966		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0399631830754966 | validation: 0.04306902368567275]
	TIME [epoch: 5.13 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04066458577754971		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.04066458577754971 | validation: 0.0401461907866226]
	TIME [epoch: 5.13 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03935196299163262		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.03935196299163262 | validation: 0.03446914679953719]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0423470934356549		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0423470934356549 | validation: 0.03673956504289769]
	TIME [epoch: 5.14 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037521721354647215		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.037521721354647215 | validation: 0.037208995154697555]
	TIME [epoch: 5.13 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041025095582878986		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.041025095582878986 | validation: 0.04026270012078801]
	TIME [epoch: 5.18 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039811755599982415		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.039811755599982415 | validation: 0.03976242994895579]
	TIME [epoch: 5.15 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03855940527370759		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.03855940527370759 | validation: 0.03746219059423968]
	TIME [epoch: 5.14 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038528946163709096		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.038528946163709096 | validation: 0.040151872076066714]
	TIME [epoch: 5.14 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04249253845792524		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.04249253845792524 | validation: 0.03630473402858623]
	TIME [epoch: 5.14 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04185149918486146		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.04185149918486146 | validation: 0.04195911975337496]
	TIME [epoch: 5.14 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03840740676832543		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.03840740676832543 | validation: 0.03808559556405004]
	TIME [epoch: 5.14 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047868304279930204		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.047868304279930204 | validation: 0.059517676595972846]
	TIME [epoch: 5.14 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05464344995955244		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.05464344995955244 | validation: 0.0338817208695866]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03821599048956359		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.03821599048956359 | validation: 0.03635910099435613]
	TIME [epoch: 5.17 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03903995237551772		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.03903995237551772 | validation: 0.03979492166730336]
	TIME [epoch: 5.14 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03967561500104219		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.03967561500104219 | validation: 0.03955794147895512]
	TIME [epoch: 5.13 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038017141362865554		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.038017141362865554 | validation: 0.036918749133377636]
	TIME [epoch: 5.13 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041522991558542485		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.041522991558542485 | validation: 0.03474452094267007]
	TIME [epoch: 5.13 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03675629856343169		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.03675629856343169 | validation: 0.03483609995880477]
	TIME [epoch: 5.13 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03696866528614778		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.03696866528614778 | validation: 0.03730416285862023]
	TIME [epoch: 5.13 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04230621324686037		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.04230621324686037 | validation: 0.035853662187855684]
	TIME [epoch: 5.13 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037508689304751736		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.037508689304751736 | validation: 0.0389895928371521]
	TIME [epoch: 5.13 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03921028393650396		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.03921028393650396 | validation: 0.03470927694259924]
	TIME [epoch: 5.17 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037464729708866495		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.037464729708866495 | validation: 0.03370705284201725]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03928713095046528		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.03928713095046528 | validation: 0.03617518671974317]
	TIME [epoch: 5.13 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0377733512995185		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0377733512995185 | validation: 0.03522975743504869]
	TIME [epoch: 5.12 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03622019158448699		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.03622019158448699 | validation: 0.03438657300519709]
	TIME [epoch: 5.13 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03656619045503011		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.03656619045503011 | validation: 0.03482795723079504]
	TIME [epoch: 5.13 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040330301444983375		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.040330301444983375 | validation: 0.036337748427485314]
	TIME [epoch: 5.13 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039807998697743326		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.039807998697743326 | validation: 0.03897227219695249]
	TIME [epoch: 5.13 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03715925061312898		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.03715925061312898 | validation: 0.034330007582812294]
	TIME [epoch: 5.13 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03714206223332955		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.03714206223332955 | validation: 0.03795373632213142]
	TIME [epoch: 5.17 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03707374811010617		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.03707374811010617 | validation: 0.033747196369418614]
	TIME [epoch: 5.13 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036318050183418084		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.036318050183418084 | validation: 0.04440016775597037]
	TIME [epoch: 5.13 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04109791901937758		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.04109791901937758 | validation: 0.04014645436822556]
	TIME [epoch: 5.13 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03872682562884645		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.03872682562884645 | validation: 0.03754620047493528]
	TIME [epoch: 5.13 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03677184888645596		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.03677184888645596 | validation: 0.035117240605808946]
	TIME [epoch: 5.13 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03594905183651322		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.03594905183651322 | validation: 0.03293168640237542]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03435933192288928		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.03435933192288928 | validation: 0.03285078152546313]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037861807754749795		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.037861807754749795 | validation: 0.03628693930608499]
	TIME [epoch: 5.13 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0359488357027217		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0359488357027217 | validation: 0.036552966387330604]
	TIME [epoch: 5.14 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03940306778123135		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.03940306778123135 | validation: 0.04002361353468214]
	TIME [epoch: 5.19 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038028794794747724		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.038028794794747724 | validation: 0.03654400995316995]
	TIME [epoch: 5.11 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03566796601150736		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.03566796601150736 | validation: 0.03395499000078295]
	TIME [epoch: 5.12 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035481753189745883		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.035481753189745883 | validation: 0.03436427241538853]
	TIME [epoch: 5.11 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03655911024755026		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.03655911024755026 | validation: 0.033417254576494596]
	TIME [epoch: 5.12 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03448012116392556		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.03448012116392556 | validation: 0.03410286330562885]
	TIME [epoch: 5.11 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03408493861760959		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.03408493861760959 | validation: 0.03384260203386429]
	TIME [epoch: 5.12 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042178250443932724		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.042178250443932724 | validation: 0.040384956173362935]
	TIME [epoch: 5.12 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039319650923195636		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.039319650923195636 | validation: 0.033920760275959444]
	TIME [epoch: 5.15 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03355193145193218		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.03355193145193218 | validation: 0.03332493089426543]
	TIME [epoch: 5.12 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0344052239605792		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0344052239605792 | validation: 0.03435063989134489]
	TIME [epoch: 5.11 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03589898270588303		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.03589898270588303 | validation: 0.03291746466270077]
	TIME [epoch: 5.11 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0363620608481459		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0363620608481459 | validation: 0.03580806647253443]
	TIME [epoch: 5.11 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037230428226038184		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.037230428226038184 | validation: 0.03379411624753682]
	TIME [epoch: 5.11 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03477141385951103		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.03477141385951103 | validation: 0.035275190722033514]
	TIME [epoch: 5.11 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03517598156565841		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.03517598156565841 | validation: 0.03500948943823504]
	TIME [epoch: 5.11 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03429366067631902		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.03429366067631902 | validation: 0.036442945512970656]
	TIME [epoch: 5.12 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038814122499738674		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.038814122499738674 | validation: 0.03252927026019874]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03461047152372071		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.03461047152372071 | validation: 0.034388811253777796]
	TIME [epoch: 5.12 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034196191553376235		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.034196191553376235 | validation: 0.0332615215949317]
	TIME [epoch: 5.11 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034641572954915247		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.034641572954915247 | validation: 0.03768369568194693]
	TIME [epoch: 5.12 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036107978734359635		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.036107978734359635 | validation: 0.03983238695599063]
	TIME [epoch: 5.11 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035608661812926906		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.035608661812926906 | validation: 0.03261836482457002]
	TIME [epoch: 5.12 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03491628627964843		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.03491628627964843 | validation: 0.03647787491008693]
	TIME [epoch: 5.11 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03472406621056787		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.03472406621056787 | validation: 0.03377496537233389]
	TIME [epoch: 5.11 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035340422578740024		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.035340422578740024 | validation: 0.03032313354795886]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033982177649518136		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.033982177649518136 | validation: 0.034509537544922586]
	TIME [epoch: 5.18 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03514457628380757		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.03514457628380757 | validation: 0.033325776736089656]
	TIME [epoch: 5.14 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0332534107262701		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0332534107262701 | validation: 0.03079834475139081]
	TIME [epoch: 5.13 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034016963268392444		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.034016963268392444 | validation: 0.03304394799674579]
	TIME [epoch: 5.13 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03469334652457413		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.03469334652457413 | validation: 0.031723078112705916]
	TIME [epoch: 5.13 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03266757323467412		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.03266757323467412 | validation: 0.03157902855959989]
	TIME [epoch: 5.13 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033431379359738014		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.033431379359738014 | validation: 0.032979825913317384]
	TIME [epoch: 5.13 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03604214677427539		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.03604214677427539 | validation: 0.03715859060143738]
	TIME [epoch: 5.13 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03445953576219087		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.03445953576219087 | validation: 0.032551387711273784]
	TIME [epoch: 5.16 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03563464083641548		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.03563464083641548 | validation: 0.03356302849269761]
	TIME [epoch: 5.16 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03282781231706518		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.03282781231706518 | validation: 0.02920641463853659]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_888.pth
	Model improved!!!
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033660558892295536		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.033660558892295536 | validation: 0.03619100485090274]
	TIME [epoch: 5.14 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03472674703774023		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.03472674703774023 | validation: 0.02892770148150327]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033258418196379376		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.033258418196379376 | validation: 0.030019387007344068]
	TIME [epoch: 5.13 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03347034943469933		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.03347034943469933 | validation: 0.03170048799467094]
	TIME [epoch: 5.12 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03836781184126688		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.03836781184126688 | validation: 0.03950943730612086]
	TIME [epoch: 5.13 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033178265519887735		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.033178265519887735 | validation: 0.03424617986364192]
	TIME [epoch: 5.13 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032585245412152526		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.032585245412152526 | validation: 0.030633839530763436]
	TIME [epoch: 5.15 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03279890803656947		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.03279890803656947 | validation: 0.03141121091235874]
	TIME [epoch: 5.15 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032669773692272655		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.032669773692272655 | validation: 0.03268290958233794]
	TIME [epoch: 5.13 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03288384635973199		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.03288384635973199 | validation: 0.03440888119293276]
	TIME [epoch: 5.13 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03364086911777995		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.03364086911777995 | validation: 0.04922910696366081]
	TIME [epoch: 5.13 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037245024056030494		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.037245024056030494 | validation: 0.030406068666579397]
	TIME [epoch: 5.13 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03308651916839242		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.03308651916839242 | validation: 0.028868928360511886]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03185647309645296		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.03185647309645296 | validation: 0.030886971019505053]
	TIME [epoch: 5.13 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03230850658083544		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.03230850658083544 | validation: 0.03604656817236533]
	TIME [epoch: 5.12 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03327253027072175		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.03327253027072175 | validation: 0.03265143821065147]
	TIME [epoch: 5.16 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0317940480552001		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0317940480552001 | validation: 0.030223133709303823]
	TIME [epoch: 5.13 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03158158721322976		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.03158158721322976 | validation: 0.02872986336159373]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031506732710681155		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.031506732710681155 | validation: 0.02986961304193695]
	TIME [epoch: 5.12 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033601145215663104		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.033601145215663104 | validation: 0.031430888923766866]
	TIME [epoch: 5.12 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03308271536827911		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.03308271536827911 | validation: 0.030421292215694708]
	TIME [epoch: 5.12 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03172616912164021		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.03172616912164021 | validation: 0.032757542490297026]
	TIME [epoch: 5.11 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0313866927276815		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0313866927276815 | validation: 0.03034737064430729]
	TIME [epoch: 5.11 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03225229921236403		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.03225229921236403 | validation: 0.0308157780478564]
	TIME [epoch: 5.11 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03215152199539939		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.03215152199539939 | validation: 0.03329853066945663]
	TIME [epoch: 5.15 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03292630988902154		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.03292630988902154 | validation: 0.03234133120266331]
	TIME [epoch: 5.12 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03282452959765527		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.03282452959765527 | validation: 0.029225496231715227]
	TIME [epoch: 5.11 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03195211820781532		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.03195211820781532 | validation: 0.032370198140337064]
	TIME [epoch: 5.11 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032849048402650696		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.032849048402650696 | validation: 0.029542650457429095]
	TIME [epoch: 5.11 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03218401265132621		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.03218401265132621 | validation: 0.029460335884512683]
	TIME [epoch: 5.11 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030168013361225203		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.030168013361225203 | validation: 0.02978051851639081]
	TIME [epoch: 5.11 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030836666771004936		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.030836666771004936 | validation: 0.03690117825179997]
	TIME [epoch: 5.11 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03288843873011946		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.03288843873011946 | validation: 0.03295740349567227]
	TIME [epoch: 5.11 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033137206984305		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.033137206984305 | validation: 0.029833498398510017]
	TIME [epoch: 5.15 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030603731569472787		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.030603731569472787 | validation: 0.03455521285220786]
	TIME [epoch: 5.12 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031347219669301034		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.031347219669301034 | validation: 0.02766702645036582]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03185942484539561		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.03185942484539561 | validation: 0.027500376944462234]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_925.pth
	Model improved!!!
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045782961155376264		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.045782961155376264 | validation: 0.05483992359935144]
	TIME [epoch: 5.12 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042213922430810305		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.042213922430810305 | validation: 0.035085718000932985]
	TIME [epoch: 5.12 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03178089124773573		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.03178089124773573 | validation: 0.02897437207562209]
	TIME [epoch: 5.11 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030428045619907036		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.030428045619907036 | validation: 0.028997122154001097]
	TIME [epoch: 5.11 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029947188805120352		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.029947188805120352 | validation: 0.028795697843773574]
	TIME [epoch: 5.12 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030228637960202098		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.030228637960202098 | validation: 0.03291096586627821]
	TIME [epoch: 5.14 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030272665064273348		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.030272665064273348 | validation: 0.028501699469606567]
	TIME [epoch: 5.12 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029350911298818012		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.029350911298818012 | validation: 0.02844342088352214]
	TIME [epoch: 5.11 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035660199847251516		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.035660199847251516 | validation: 0.028499946040147054]
	TIME [epoch: 5.12 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030875565102988157		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.030875565102988157 | validation: 0.029416920765250286]
	TIME [epoch: 5.11 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02991951058036575		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.02991951058036575 | validation: 0.031571863556891996]
	TIME [epoch: 5.11 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03185742639114833		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.03185742639114833 | validation: 0.028561824168996216]
	TIME [epoch: 5.11 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030636263560686113		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.030636263560686113 | validation: 0.029233233752886882]
	TIME [epoch: 5.11 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03190783063468019		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.03190783063468019 | validation: 0.028312538344087847]
	TIME [epoch: 5.12 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028972167447492442		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.028972167447492442 | validation: 0.02808916843813602]
	TIME [epoch: 5.15 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030096404567972183		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.030096404567972183 | validation: 0.028551854468403963]
	TIME [epoch: 5.12 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029546565097044737		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.029546565097044737 | validation: 0.02764540336184238]
	TIME [epoch: 5.11 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029906076779738673		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.029906076779738673 | validation: 0.02686573343173974]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02942166875400327		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.02942166875400327 | validation: 0.03203899984584837]
	TIME [epoch: 5.13 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04258537827192942		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.04258537827192942 | validation: 0.030188005070417766]
	TIME [epoch: 5.14 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030534857037143426		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.030534857037143426 | validation: 0.030253001629830426]
	TIME [epoch: 5.13 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029610025747557653		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.029610025747557653 | validation: 0.028025261443200347]
	TIME [epoch: 5.14 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030694417281351236		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.030694417281351236 | validation: 0.02944256443356231]
	TIME [epoch: 5.14 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031496043396222265		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.031496043396222265 | validation: 0.03280871717569601]
	TIME [epoch: 5.17 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030435266600879383		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.030435266600879383 | validation: 0.028672231900139483]
	TIME [epoch: 5.14 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029279759972489404		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.029279759972489404 | validation: 0.02740263965181195]
	TIME [epoch: 5.13 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028528413858739898		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.028528413858739898 | validation: 0.029162041481893712]
	TIME [epoch: 5.13 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030359112749765847		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.030359112749765847 | validation: 0.02996314464032234]
	TIME [epoch: 5.13 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029919577835887265		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.029919577835887265 | validation: 0.026533122535350645]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02970455301673687		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.02970455301673687 | validation: 0.028738257158135355]
	TIME [epoch: 5.13 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02932106791921908		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.02932106791921908 | validation: 0.026411055406859868]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_956.pth
	Model improved!!!
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028963790451606762		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.028963790451606762 | validation: 0.028324723244123474]
	TIME [epoch: 5.15 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02895636322924071		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.02895636322924071 | validation: 0.03073215005583929]
	TIME [epoch: 5.15 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0325871559774591		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0325871559774591 | validation: 0.04070200757662371]
	TIME [epoch: 5.13 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036308764220117286		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.036308764220117286 | validation: 0.02730754107497073]
	TIME [epoch: 5.13 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027896241979684735		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.027896241979684735 | validation: 0.028541536443854522]
	TIME [epoch: 5.13 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028609413491837092		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.028609413491837092 | validation: 0.026376143238212347]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02809909222274133		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.02809909222274133 | validation: 0.027052353537959217]
	TIME [epoch: 5.12 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028655851659057093		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.028655851659057093 | validation: 0.025318163818184733]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029112266699177623		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.029112266699177623 | validation: 0.027786158661321714]
	TIME [epoch: 5.12 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02924740437993547		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.02924740437993547 | validation: 0.026598083092258525]
	TIME [epoch: 5.14 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029861981224383105		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.029861981224383105 | validation: 0.02772472978898988]
	TIME [epoch: 5.14 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02879565016695321		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.02879565016695321 | validation: 0.02971030923073452]
	TIME [epoch: 5.12 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029071826762299233		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.029071826762299233 | validation: 0.026844815229006187]
	TIME [epoch: 5.12 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02824078560032791		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.02824078560032791 | validation: 0.034811729382517705]
	TIME [epoch: 5.12 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028997812248605908		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.028997812248605908 | validation: 0.02663623274516774]
	TIME [epoch: 5.12 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030650423009146957		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.030650423009146957 | validation: 0.02842247808057506]
	TIME [epoch: 5.12 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028231733057455966		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.028231733057455966 | validation: 0.028436951666376643]
	TIME [epoch: 5.12 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028064756397386717		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.028064756397386717 | validation: 0.02833629824572826]
	TIME [epoch: 5.12 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027531934371962073		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.027531934371962073 | validation: 0.026248338413385704]
	TIME [epoch: 5.14 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028810260825174606		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.028810260825174606 | validation: 0.025749252092527174]
	TIME [epoch: 5.15 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02738508314660977		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.02738508314660977 | validation: 0.027138413182458493]
	TIME [epoch: 5.13 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03022373395678465		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.03022373395678465 | validation: 0.027722337855056553]
	TIME [epoch: 5.12 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02931408662516978		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.02931408662516978 | validation: 0.027520734479332343]
	TIME [epoch: 5.12 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030168113538527615		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.030168113538527615 | validation: 0.02764591922062331]
	TIME [epoch: 5.13 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027857597864442547		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.027857597864442547 | validation: 0.025141988283274887]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_981.pth
	Model improved!!!
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027628572079859554		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.027628572079859554 | validation: 0.026321879918313664]
	TIME [epoch: 5.12 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028580346823066895		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.028580346823066895 | validation: 0.027299480494495678]
	TIME [epoch: 5.12 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028733673976874645		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.028733673976874645 | validation: 0.025782365905455264]
	TIME [epoch: 5.14 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028372579015391933		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.028372579015391933 | validation: 0.02648277572041125]
	TIME [epoch: 5.14 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029509153733653967		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.029509153733653967 | validation: 0.03429340813028643]
	TIME [epoch: 5.12 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02997767338963787		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.02997767338963787 | validation: 0.029195904890685728]
	TIME [epoch: 5.12 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029157205257689868		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.029157205257689868 | validation: 0.025975402431554797]
	TIME [epoch: 5.12 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02703902250626622		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.02703902250626622 | validation: 0.026027689282392893]
	TIME [epoch: 5.12 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02716026896289558		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.02716026896289558 | validation: 0.026043068191296145]
	TIME [epoch: 5.12 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027517930627215567		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.027517930627215567 | validation: 0.027919749148588573]
	TIME [epoch: 5.12 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028875941896254975		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.028875941896254975 | validation: 0.02835667886946306]
	TIME [epoch: 5.12 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029821488744661832		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.029821488744661832 | validation: 0.03097668226348471]
	TIME [epoch: 5.15 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029549850169595535		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.029549850169595535 | validation: 0.025861213697090983]
	TIME [epoch: 5.14 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02772629351387131		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.02772629351387131 | validation: 0.027948956748948393]
	TIME [epoch: 5.12 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027301643117829476		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.027301643117829476 | validation: 0.025308204641685095]
	TIME [epoch: 5.12 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026886804365476986		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.026886804365476986 | validation: 0.025956270241824748]
	TIME [epoch: 5.12 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027852676854901705		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.027852676854901705 | validation: 0.026943060847155843]
	TIME [epoch: 5.12 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0266090366794565		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.0266090366794565 | validation: 0.026403208052150223]
	TIME [epoch: 5.12 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028281985714722763		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.028281985714722763 | validation: 0.025787457418280736]
	TIME [epoch: 5.11 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027537952875371592		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.027537952875371592 | validation: 0.025033033526151195]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027081883402272078		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.027081883402272078 | validation: 0.024472590192371274]
	TIME [epoch: 5.18 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026707465503970497		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.026707465503970497 | validation: 0.02529267192646617]
	TIME [epoch: 5.14 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026924880010191783		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.026924880010191783 | validation: 0.024957252215080706]
	TIME [epoch: 5.13 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02767745614242216		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.02767745614242216 | validation: 0.024966639163267572]
	TIME [epoch: 5.13 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027298706435071367		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.027298706435071367 | validation: 0.02603763324612788]
	TIME [epoch: 5.13 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027466542236627096		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.027466542236627096 | validation: 0.0256601059185059]
	TIME [epoch: 5.13 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026919258828394015		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.026919258828394015 | validation: 0.027046944813655723]
	TIME [epoch: 5.12 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029398383094101834		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.029398383094101834 | validation: 0.02624903534682188]
	TIME [epoch: 5.12 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028281591583719047		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.028281591583719047 | validation: 0.025338633938234045]
	TIME [epoch: 5.14 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026805082961615698		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.026805082961615698 | validation: 0.025282456892505016]
	TIME [epoch: 5.16 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028559538638594385		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.028559538638594385 | validation: 0.026536061457420424]
	TIME [epoch: 5.14 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026294087281872495		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.026294087281872495 | validation: 0.024837494087836465]
	TIME [epoch: 5.12 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02595909693480935		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.02595909693480935 | validation: 0.024935939693755883]
	TIME [epoch: 5.12 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02659417183529921		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.02659417183529921 | validation: 0.025372861741881837]
	TIME [epoch: 5.13 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026321681969840263		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.026321681969840263 | validation: 0.02597383761010293]
	TIME [epoch: 5.13 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026763603841747953		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.026763603841747953 | validation: 0.024109875358841848]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1017.pth
	Model improved!!!
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02713300188756774		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.02713300188756774 | validation: 0.023542208869879407]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027686496148225614		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.027686496148225614 | validation: 0.02564321539459328]
	TIME [epoch: 5.13 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025993562195910796		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.025993562195910796 | validation: 0.025266165439222645]
	TIME [epoch: 5.13 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026464777464785455		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.026464777464785455 | validation: 0.0269231204326053]
	TIME [epoch: 5.12 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02588136447236239		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.02588136447236239 | validation: 0.02437712800007516]
	TIME [epoch: 5.11 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026375111100530757		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.026375111100530757 | validation: 0.024289121455482243]
	TIME [epoch: 5.11 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02565851433992519		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.02565851433992519 | validation: 0.035179105546628724]
	TIME [epoch: 5.11 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029445985524262697		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.029445985524262697 | validation: 0.025988521018101597]
	TIME [epoch: 5.11 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02643013858772131		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.02643013858772131 | validation: 0.026435239080186874]
	TIME [epoch: 5.11 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02599631826088508		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.02599631826088508 | validation: 0.023522233972159037]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1027.pth
	Model improved!!!
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025080327436308378		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.025080327436308378 | validation: 0.025416930728095095]
	TIME [epoch: 5.13 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02551640550002534		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.02551640550002534 | validation: 0.024631683310737416]
	TIME [epoch: 5.14 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025106026203117596		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.025106026203117596 | validation: 0.024807974815457268]
	TIME [epoch: 5.13 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026198214806193113		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.026198214806193113 | validation: 0.026225380009962326]
	TIME [epoch: 5.11 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026307733597425928		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.026307733597425928 | validation: 0.026609396779053055]
	TIME [epoch: 5.11 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02606416201699054		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.02606416201699054 | validation: 0.02512185054000768]
	TIME [epoch: 5.11 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025423661558225848		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.025423661558225848 | validation: 0.024683261462804015]
	TIME [epoch: 5.11 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025704866287825923		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.025704866287825923 | validation: 0.02396555482088987]
	TIME [epoch: 5.11 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02602037974120989		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.02602037974120989 | validation: 0.024219572745352455]
	TIME [epoch: 8.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027445732658507145		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.027445732658507145 | validation: 0.02428424361834293]
	TIME [epoch: 5.13 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025070476468820158		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.025070476468820158 | validation: 0.023209025882707984]
	TIME [epoch: 5.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1038.pth
	Model improved!!!
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026073457049902805		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.026073457049902805 | validation: 0.02697376189859553]
	TIME [epoch: 5.12 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02695430692176029		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.02695430692176029 | validation: 0.023862767933219113]
	TIME [epoch: 5.11 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0261657281848079		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0261657281848079 | validation: 0.023358968897944983]
	TIME [epoch: 5.11 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02539373819468388		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.02539373819468388 | validation: 0.023454064190454406]
	TIME [epoch: 5.11 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02468118700349089		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.02468118700349089 | validation: 0.025163072445407234]
	TIME [epoch: 5.11 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02857837914307224		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.02857837914307224 | validation: 0.02357408438713919]
	TIME [epoch: 5.12 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02553771866800049		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.02553771866800049 | validation: 0.024222262389180897]
	TIME [epoch: 5.11 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02706801447492121		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.02706801447492121 | validation: 0.02408011067561687]
	TIME [epoch: 5.14 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026176368749454157		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.026176368749454157 | validation: 0.025267577311441816]
	TIME [epoch: 5.13 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025548034477398777		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.025548034477398777 | validation: 0.023703615998175397]
	TIME [epoch: 5.11 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025289643025190225		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.025289643025190225 | validation: 0.025448687647471303]
	TIME [epoch: 5.11 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026007770402416037		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.026007770402416037 | validation: 0.024074192918486052]
	TIME [epoch: 5.11 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03222123202922204		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.03222123202922204 | validation: 0.023523653015898757]
	TIME [epoch: 5.11 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02560447512503413		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.02560447512503413 | validation: 0.024260212321044947]
	TIME [epoch: 5.11 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02432383121333304		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.02432383121333304 | validation: 0.023094734524364266]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1053.pth
	Model improved!!!
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024525850058023026		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.024525850058023026 | validation: 0.023715124755319297]
	TIME [epoch: 5.11 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024566199940457847		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.024566199940457847 | validation: 0.024922070298695434]
	TIME [epoch: 5.13 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024837824190390493		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.024837824190390493 | validation: 0.023399154529517373]
	TIME [epoch: 5.13 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02495128238824077		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.02495128238824077 | validation: 0.02264963742063306]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026483013002426788		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.026483013002426788 | validation: 0.02337430914877575]
	TIME [epoch: 5.13 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024693268953595818		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.024693268953595818 | validation: 0.02417955347062413]
	TIME [epoch: 5.12 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024496133690771214		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.024496133690771214 | validation: 0.02429962368217022]
	TIME [epoch: 5.12 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025137379980453477		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.025137379980453477 | validation: 0.023599730464277067]
	TIME [epoch: 5.13 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025960465230713832		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.025960465230713832 | validation: 0.02593355658645532]
	TIME [epoch: 5.13 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025348530591003518		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.025348530591003518 | validation: 0.02466836406730131]
	TIME [epoch: 5.13 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024643711178790852		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.024643711178790852 | validation: 0.02241760025233459]
	TIME [epoch: 5.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025927340103553048		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.025927340103553048 | validation: 0.02245833117328338]
	TIME [epoch: 5.15 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024949082402231197		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.024949082402231197 | validation: 0.023175873312627177]
	TIME [epoch: 5.13 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026938078074505774		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.026938078074505774 | validation: 0.024511980627092558]
	TIME [epoch: 5.13 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02494321579675353		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.02494321579675353 | validation: 0.024699205409362456]
	TIME [epoch: 5.13 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03869805603248998		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.03869805603248998 | validation: 0.03588988738183642]
	TIME [epoch: 5.12 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03107591276271257		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.03107591276271257 | validation: 0.029711901423186672]
	TIME [epoch: 5.13 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02659767099026661		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.02659767099026661 | validation: 0.025575308052509148]
	TIME [epoch: 5.13 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02510056408710318		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.02510056408710318 | validation: 0.02395133091777185]
	TIME [epoch: 5.12 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024360124834518222		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.024360124834518222 | validation: 0.022729677851168792]
	TIME [epoch: 5.16 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023980896767700727		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.023980896767700727 | validation: 0.022338524775889435]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1074.pth
	Model improved!!!
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024086931217317864		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.024086931217317864 | validation: 0.02524179437161438]
	TIME [epoch: 5.13 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024606591296253952		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.024606591296253952 | validation: 0.022192806772865177]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1076.pth
	Model improved!!!
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023968409381216402		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.023968409381216402 | validation: 0.022920954481358682]
	TIME [epoch: 5.11 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024875148784742056		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.024875148784742056 | validation: 0.024132323295339854]
	TIME [epoch: 5.11 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024599404885578303		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.024599404885578303 | validation: 0.02359590805109677]
	TIME [epoch: 5.12 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024264808191659368		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.024264808191659368 | validation: 0.02274325590160073]
	TIME [epoch: 5.12 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024381152899699318		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.024381152899699318 | validation: 0.02337439692323602]
	TIME [epoch: 5.13 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024115061739578232		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.024115061739578232 | validation: 0.023518136672561057]
	TIME [epoch: 5.16 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024119750822057598		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.024119750822057598 | validation: 0.023070507644692293]
	TIME [epoch: 5.13 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023965297706260623		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.023965297706260623 | validation: 0.023786890866371643]
	TIME [epoch: 5.12 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02481337433166903		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.02481337433166903 | validation: 0.024537111203943467]
	TIME [epoch: 5.13 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025230223997161367		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.025230223997161367 | validation: 0.02350931486189487]
	TIME [epoch: 5.12 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024163583926013168		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.024163583926013168 | validation: 0.021894715852471255]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1087.pth
	Model improved!!!
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02434260198469979		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.02434260198469979 | validation: 0.02298586355857556]
	TIME [epoch: 5.12 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02469402718841522		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.02469402718841522 | validation: 0.023860233406670484]
	TIME [epoch: 5.12 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023724511139771597		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.023724511139771597 | validation: 0.02252534007450805]
	TIME [epoch: 5.13 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02431310388913148		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.02431310388913148 | validation: 0.025271409704783684]
	TIME [epoch: 5.15 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023813927079740314		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.023813927079740314 | validation: 0.021993507794416285]
	TIME [epoch: 5.13 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024991926120612684		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.024991926120612684 | validation: 0.02372180621834747]
	TIME [epoch: 5.12 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02430899710480907		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.02430899710480907 | validation: 0.022374671362188354]
	TIME [epoch: 5.12 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024411978077472904		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.024411978077472904 | validation: 0.02821647973359765]
	TIME [epoch: 5.12 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028286288658175167		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.028286288658175167 | validation: 0.023931180581135338]
	TIME [epoch: 5.12 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025740660309999656		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.025740660309999656 | validation: 0.025334964259324383]
	TIME [epoch: 5.12 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02377268211981928		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.02377268211981928 | validation: 0.02273622130292568]
	TIME [epoch: 5.12 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02402947003943365		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.02402947003943365 | validation: 0.023150822458992377]
	TIME [epoch: 5.13 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02346597607910044		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.02346597607910044 | validation: 0.024250450531765465]
	TIME [epoch: 5.15 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023270567394602676		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.023270567394602676 | validation: 0.023830739684108232]
	TIME [epoch: 5.13 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024395269487740262		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.024395269487740262 | validation: 0.022457632964419723]
	TIME [epoch: 5.12 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024923891623848067		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.024923891623848067 | validation: 0.022505069859967572]
	TIME [epoch: 5.12 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02299207453414979		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.02299207453414979 | validation: 0.022160521007989445]
	TIME [epoch: 5.12 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023053157353887042		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.023053157353887042 | validation: 0.02163236990210056]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1105.pth
	Model improved!!!
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02374848469029127		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.02374848469029127 | validation: 0.023194576798362627]
	TIME [epoch: 5.12 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023773730861520362		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.023773730861520362 | validation: 0.022891603646454376]
	TIME [epoch: 5.12 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024489699083640928		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.024489699083640928 | validation: 0.024025190956160708]
	TIME [epoch: 5.14 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02293706818784532		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.02293706818784532 | validation: 0.022992206141357517]
	TIME [epoch: 5.14 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02470725865925109		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.02470725865925109 | validation: 0.02378788141773257]
	TIME [epoch: 5.12 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02368126386002171		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.02368126386002171 | validation: 0.023685924773255443]
	TIME [epoch: 5.11 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023758909891302117		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.023758909891302117 | validation: 0.023618056320063793]
	TIME [epoch: 5.11 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025850954884267034		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.025850954884267034 | validation: 0.02235208219768207]
	TIME [epoch: 5.11 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023543447955774503		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.023543447955774503 | validation: 0.02153822109493877]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023287049742328193		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.023287049742328193 | validation: 0.021584528020002804]
	TIME [epoch: 5.12 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023063800569089774		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.023063800569089774 | validation: 0.022210542090165836]
	TIME [epoch: 5.13 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02341189580838076		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.02341189580838076 | validation: 0.023891285871436567]
	TIME [epoch: 5.15 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02329213864347026		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.02329213864347026 | validation: 0.022665395204774064]
	TIME [epoch: 5.15 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02409196322213843		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.02409196322213843 | validation: 0.025834017020910947]
	TIME [epoch: 5.13 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024986501448636483		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.024986501448636483 | validation: 0.022916768278213928]
	TIME [epoch: 5.13 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023280203731722418		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.023280203731722418 | validation: 0.021667270605550903]
	TIME [epoch: 5.12 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02358112969424319		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.02358112969424319 | validation: 0.025122565845848925]
	TIME [epoch: 5.13 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024310230905134247		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.024310230905134247 | validation: 0.02153667627612963]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1123.pth
	Model improved!!!
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023145732438112437		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.023145732438112437 | validation: 0.021675471723456706]
	TIME [epoch: 5.13 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022528369103183964		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.022528369103183964 | validation: 0.022133780186114834]
	TIME [epoch: 5.13 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023396200509321965		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.023396200509321965 | validation: 0.0210023472000472]
	TIME [epoch: 5.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1126.pth
	Model improved!!!
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02323216741503257		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.02323216741503257 | validation: 0.02237111643501373]
	TIME [epoch: 5.15 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02386875520195279		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.02386875520195279 | validation: 0.021848451326992004]
	TIME [epoch: 5.37 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022522102561547762		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.022522102561547762 | validation: 0.02274416385697839]
	TIME [epoch: 5.13 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02289341867746621		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.02289341867746621 | validation: 0.022170138211517556]
	TIME [epoch: 5.13 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023356976844144763		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.023356976844144763 | validation: 0.023153289125082563]
	TIME [epoch: 5.13 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0239192452883252		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0239192452883252 | validation: 0.021375766036251714]
	TIME [epoch: 5.13 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024010948325237102		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.024010948325237102 | validation: 0.022841197945023612]
	TIME [epoch: 5.13 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023462199687222372		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.023462199687222372 | validation: 0.020919609393997323]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1134.pth
	Model improved!!!
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022434640334607285		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.022434640334607285 | validation: 0.02246582028652653]
	TIME [epoch: 5.16 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022296086334112845		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.022296086334112845 | validation: 0.021313733416329562]
	TIME [epoch: 5.13 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022827624206568226		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.022827624206568226 | validation: 0.020887552165487364]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1137.pth
	Model improved!!!
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02222484573664603		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.02222484573664603 | validation: 0.021986774611953773]
	TIME [epoch: 5.12 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03445137797950319		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.03445137797950319 | validation: 0.026061957892048564]
	TIME [epoch: 5.14 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025669713989290226		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.025669713989290226 | validation: 0.023438033382927666]
	TIME [epoch: 5.13 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023114631274662237		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.023114631274662237 | validation: 0.02137115669242539]
	TIME [epoch: 5.13 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022342230351708278		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.022342230351708278 | validation: 0.019445220274833662]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1142.pth
	Model improved!!!
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022013563861069008		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.022013563861069008 | validation: 0.020636451924419902]
	TIME [epoch: 5.14 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0223239101837998		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0223239101837998 | validation: 0.019945602979513186]
	TIME [epoch: 5.16 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022024343915927103		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.022024343915927103 | validation: 0.020712387357814326]
	TIME [epoch: 5.13 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022683336333114756		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.022683336333114756 | validation: 0.020372872708218533]
	TIME [epoch: 5.12 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022423766188829197		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.022423766188829197 | validation: 0.02109087222060964]
	TIME [epoch: 5.12 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022881306896452132		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.022881306896452132 | validation: 0.023246921419879503]
	TIME [epoch: 5.12 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023944790878747957		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.023944790878747957 | validation: 0.024226901722137005]
	TIME [epoch: 5.12 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024299750204624902		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.024299750204624902 | validation: 0.02186925654181579]
	TIME [epoch: 5.12 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022804834407845295		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.022804834407845295 | validation: 0.02071346467579118]
	TIME [epoch: 5.13 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02283035895362009		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.02283035895362009 | validation: 0.021962868437792583]
	TIME [epoch: 5.13 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022542569466980016		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.022542569466980016 | validation: 0.020510497304541176]
	TIME [epoch: 5.16 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022800331187734744		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.022800331187734744 | validation: 0.02050110689217465]
	TIME [epoch: 5.13 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022967058561679778		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.022967058561679778 | validation: 0.02106591654341259]
	TIME [epoch: 5.12 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023089762176659714		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.023089762176659714 | validation: 0.022890540748152728]
	TIME [epoch: 5.12 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02194855406098416		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.02194855406098416 | validation: 0.020996932233776487]
	TIME [epoch: 5.13 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022014312952650772		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.022014312952650772 | validation: 0.02335035184985461]
	TIME [epoch: 5.13 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023022431809070674		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.023022431809070674 | validation: 0.022166929788934964]
	TIME [epoch: 5.12 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02244390155579284		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.02244390155579284 | validation: 0.022859554963807482]
	TIME [epoch: 5.12 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0219971739521249		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0219971739521249 | validation: 0.022192878485688808]
	TIME [epoch: 5.12 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022312314309183453		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.022312314309183453 | validation: 0.020355649269740896]
	TIME [epoch: 5.16 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022182184148917174		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.022182184148917174 | validation: 0.020599071389986946]
	TIME [epoch: 5.12 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021919652209410206		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.021919652209410206 | validation: 0.021744236433284426]
	TIME [epoch: 5.12 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022476815998154		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.022476815998154 | validation: 0.020982413405050776]
	TIME [epoch: 5.12 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022398272172099377		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.022398272172099377 | validation: 0.02146852662501908]
	TIME [epoch: 5.12 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02206447110295226		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.02206447110295226 | validation: 0.021867069785476458]
	TIME [epoch: 5.12 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022790399147394717		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.022790399147394717 | validation: 0.021760706115221552]
	TIME [epoch: 5.12 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021524660313789835		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.021524660313789835 | validation: 0.021210015679817444]
	TIME [epoch: 5.12 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02229756367534412		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.02229756367534412 | validation: 0.023223206027075723]
	TIME [epoch: 5.13 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024477589479625333		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.024477589479625333 | validation: 0.022757184005840488]
	TIME [epoch: 5.16 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0233560690351734		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.0233560690351734 | validation: 0.022625676776579423]
	TIME [epoch: 5.12 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022120883966540425		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.022120883966540425 | validation: 0.019430147393918663]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1173.pth
	Model improved!!!
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02253745970256917		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.02253745970256917 | validation: 0.019701796698984135]
	TIME [epoch: 5.13 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02193179123180745		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.02193179123180745 | validation: 0.020770703147059692]
	TIME [epoch: 5.12 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023031143171840726		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.023031143171840726 | validation: 0.02085112572957744]
	TIME [epoch: 5.12 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021716687006152632		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.021716687006152632 | validation: 0.02169896193380915]
	TIME [epoch: 5.12 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02260426080963669		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.02260426080963669 | validation: 0.020489556448437436]
	TIME [epoch: 5.12 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021612285107963328		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.021612285107963328 | validation: 0.021504819070085276]
	TIME [epoch: 5.13 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022122780252538013		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.022122780252538013 | validation: 0.021233587176977792]
	TIME [epoch: 5.15 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021866345060683638		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.021866345060683638 | validation: 0.019742303126745533]
	TIME [epoch: 5.13 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0214192822542743		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0214192822542743 | validation: 0.02038321163237103]
	TIME [epoch: 5.12 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022977982933523364		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.022977982933523364 | validation: 0.01949925246494627]
	TIME [epoch: 5.12 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0219677956837595		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.0219677956837595 | validation: 0.019216030935254653]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1184.pth
	Model improved!!!
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022261561253710864		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.022261561253710864 | validation: 0.023579747010887674]
	TIME [epoch: 5.13 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023124645410850322		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.023124645410850322 | validation: 0.021364440220767042]
	TIME [epoch: 5.12 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02164755822425197		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.02164755822425197 | validation: 0.01914355704265392]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1187.pth
	Model improved!!!
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02209223042399944		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.02209223042399944 | validation: 0.01989474439992496]
	TIME [epoch: 5.15 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022316818263233125		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.022316818263233125 | validation: 0.019857910189136466]
	TIME [epoch: 5.15 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021325858135231433		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.021325858135231433 | validation: 0.02058323715256425]
	TIME [epoch: 5.13 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021358772114555093		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.021358772114555093 | validation: 0.020906290942462907]
	TIME [epoch: 5.13 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021670934226266574		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.021670934226266574 | validation: 0.020420634393613173]
	TIME [epoch: 5.13 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021651869348560834		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.021651869348560834 | validation: 0.02073315130687974]
	TIME [epoch: 5.13 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02173040549749133		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.02173040549749133 | validation: 0.019993605339494423]
	TIME [epoch: 5.13 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021589075316903284		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.021589075316903284 | validation: 0.020374856818766683]
	TIME [epoch: 5.13 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0213930769147841		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0213930769147841 | validation: 0.021693358014400643]
	TIME [epoch: 5.13 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022889731204436086		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.022889731204436086 | validation: 0.021135584575651588]
	TIME [epoch: 5.15 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020957052289802332		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.020957052289802332 | validation: 0.02015346290652019]
	TIME [epoch: 5.15 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02119968044473644		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.02119968044473644 | validation: 0.0219246784090147]
	TIME [epoch: 5.13 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021476308815106496		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.021476308815106496 | validation: 0.019896470421345958]
	TIME [epoch: 5.13 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02073292754442068		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.02073292754442068 | validation: 0.01949708241007867]
	TIME [epoch: 5.12 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021480419587483226		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.021480419587483226 | validation: 0.020542868141940875]
	TIME [epoch: 5.12 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02146001948366297		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.02146001948366297 | validation: 0.02292743012119164]
	TIME [epoch: 5.12 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02257144201862911		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.02257144201862911 | validation: 0.023123943505128995]
	TIME [epoch: 5.12 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022408372571866675		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.022408372571866675 | validation: 0.020907143020840187]
	TIME [epoch: 5.13 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02183678473369355		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.02183678473369355 | validation: 0.02083427524767297]
	TIME [epoch: 5.15 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020724990133932364		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.020724990133932364 | validation: 0.02015198386847535]
	TIME [epoch: 5.15 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02064187921032101		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.02064187921032101 | validation: 0.019200981218483272]
	TIME [epoch: 5.13 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022050535963789475		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.022050535963789475 | validation: 0.02137889373632799]
	TIME [epoch: 5.13 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028239065225816436		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.028239065225816436 | validation: 0.029790141800258324]
	TIME [epoch: 5.12 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026690748543514338		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.026690748543514338 | validation: 0.022524971516995694]
	TIME [epoch: 5.12 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021195372371969976		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.021195372371969976 | validation: 0.019273857483706226]
	TIME [epoch: 5.12 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020916181124414426		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.020916181124414426 | validation: 0.0216636658867201]
	TIME [epoch: 5.12 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02049327242384199		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.02049327242384199 | validation: 0.019694909194184422]
	TIME [epoch: 5.13 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021367469874464935		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.021367469874464935 | validation: 0.019172580545312335]
	TIME [epoch: 5.15 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02046629140977022		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.02046629140977022 | validation: 0.020552133584451102]
	TIME [epoch: 5.15 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021294166998743334		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.021294166998743334 | validation: 0.021131773062642533]
	TIME [epoch: 5.12 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021450708776842766		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.021450708776842766 | validation: 0.02061918805748834]
	TIME [epoch: 5.12 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02096668016431178		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.02096668016431178 | validation: 0.021466509424723897]
	TIME [epoch: 5.12 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02059456319926193		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.02059456319926193 | validation: 0.020471870829770533]
	TIME [epoch: 5.13 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020136362509283235		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.020136362509283235 | validation: 0.018789776811893436]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1221.pth
	Model improved!!!
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02125753992235721		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.02125753992235721 | validation: 0.019850058704805017]
	TIME [epoch: 5.12 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021264475754247503		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.021264475754247503 | validation: 0.020286844405552125]
	TIME [epoch: 5.11 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020650393631442443		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.020650393631442443 | validation: 0.021020493858644133]
	TIME [epoch: 5.14 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020886435812611442		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.020886435812611442 | validation: 0.02096904982153966]
	TIME [epoch: 5.14 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020857865449700058		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.020857865449700058 | validation: 0.019912795534030428]
	TIME [epoch: 5.12 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020361202542002306		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.020361202542002306 | validation: 0.01963706150645058]
	TIME [epoch: 5.12 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020977208717185135		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.020977208717185135 | validation: 0.02037498814891344]
	TIME [epoch: 5.12 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021241951228694222		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.021241951228694222 | validation: 0.01951680286719152]
	TIME [epoch: 5.12 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021353633465285522		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.021353633465285522 | validation: 0.02132489682555355]
	TIME [epoch: 5.12 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020880225864760973		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.020880225864760973 | validation: 0.023037647715900233]
	TIME [epoch: 5.12 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021133367655321478		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.021133367655321478 | validation: 0.020264100115806094]
	TIME [epoch: 5.12 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020173121570864615		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.020173121570864615 | validation: 0.019467148379574785]
	TIME [epoch: 5.14 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02042519496123752		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.02042519496123752 | validation: 0.020108413974522735]
	TIME [epoch: 5.14 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020480293322949743		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.020480293322949743 | validation: 0.01970163846405363]
	TIME [epoch: 5.12 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020964381082954665		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.020964381082954665 | validation: 0.01988821695752789]
	TIME [epoch: 5.12 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01994664479373858		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.01994664479373858 | validation: 0.018854697543860724]
	TIME [epoch: 5.12 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02109146107013864		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.02109146107013864 | validation: 0.020675135503671095]
	TIME [epoch: 5.11 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021000241801320773		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.021000241801320773 | validation: 0.01999453384334587]
	TIME [epoch: 5.12 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023091416174721012		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.023091416174721012 | validation: 0.022311285399671706]
	TIME [epoch: 5.12 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021456479721804637		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.021456479721804637 | validation: 0.01945953568746838]
	TIME [epoch: 5.12 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021120541945778		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.02021120541945778 | validation: 0.02006672566548904]
	TIME [epoch: 5.14 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020173582561269617		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.020173582561269617 | validation: 0.018818998449748214]
	TIME [epoch: 5.14 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020056340150351668		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.020056340150351668 | validation: 0.01986003233703232]
	TIME [epoch: 5.11 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02129087600977559		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.02129087600977559 | validation: 0.02042753698673341]
	TIME [epoch: 5.11 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019837134728188682		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.019837134728188682 | validation: 0.018527821693975786]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1246.pth
	Model improved!!!
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02037432834518137		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.02037432834518137 | validation: 0.020997785156127907]
	TIME [epoch: 5.11 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021323537856629392		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.021323537856629392 | validation: 0.020228592318005542]
	TIME [epoch: 5.11 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020197550051433453		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.020197550051433453 | validation: 0.02036528654422059]
	TIME [epoch: 5.11 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020288217213983106		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.020288217213983106 | validation: 0.018749996995842955]
	TIME [epoch: 5.11 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020704301858982158		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.020704301858982158 | validation: 0.01934127165385021]
	TIME [epoch: 5.14 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019701511782333186		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.019701511782333186 | validation: 0.01903926398568629]
	TIME [epoch: 5.13 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02092616679695981		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.02092616679695981 | validation: 0.018504870628881384]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1253.pth
	Model improved!!!
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020542727562515267		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.020542727562515267 | validation: 0.019129036993452905]
	TIME [epoch: 5.13 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020611942145145064		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.020611942145145064 | validation: 0.020775242164057572]
	TIME [epoch: 5.12 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02067193288297378		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.02067193288297378 | validation: 0.019019324479806622]
	TIME [epoch: 5.12 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020474643846041765		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.020474643846041765 | validation: 0.01838612121426776]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1257.pth
	Model improved!!!
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02027653102002		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.02027653102002 | validation: 0.018632534341403932]
	TIME [epoch: 5.12 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01975317550685277		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.01975317550685277 | validation: 0.01852817530128175]
	TIME [epoch: 5.12 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0201206366188705		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0201206366188705 | validation: 0.019989169298384186]
	TIME [epoch: 5.15 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02024530889780979		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.02024530889780979 | validation: 0.01937157730084321]
	TIME [epoch: 5.13 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020265804418068523		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.020265804418068523 | validation: 0.019408332868988548]
	TIME [epoch: 5.12 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01960270663824321		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.01960270663824321 | validation: 0.02034690766013801]
	TIME [epoch: 5.12 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02001018978354607		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.02001018978354607 | validation: 0.020017024445006255]
	TIME [epoch: 5.13 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021027598346111977		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.021027598346111977 | validation: 0.019398791725702635]
	TIME [epoch: 5.12 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019952671769518066		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.019952671769518066 | validation: 0.019473271353864202]
	TIME [epoch: 5.12 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02057881398194643		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.02057881398194643 | validation: 0.02080903516106674]
	TIME [epoch: 5.12 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020800904781849733		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.020800904781849733 | validation: 0.019392707321775598]
	TIME [epoch: 5.12 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01978100024419328		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.01978100024419328 | validation: 0.01972467655079619]
	TIME [epoch: 5.16 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019175908550863718		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.019175908550863718 | validation: 0.018821061634766264]
	TIME [epoch: 5.13 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01980922328201737		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.01980922328201737 | validation: 0.018895850028779467]
	TIME [epoch: 5.12 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019754934299606074		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.019754934299606074 | validation: 0.01911176744112194]
	TIME [epoch: 5.12 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01953641637841816		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.01953641637841816 | validation: 0.019820537285878473]
	TIME [epoch: 5.12 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020238086058091968		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.020238086058091968 | validation: 0.019388337453944963]
	TIME [epoch: 5.12 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019997328381441918		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.019997328381441918 | validation: 0.019430018255597724]
	TIME [epoch: 5.12 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020036891108949233		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.020036891108949233 | validation: 0.02148229935609757]
	TIME [epoch: 5.12 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019591018366209134		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.019591018366209134 | validation: 0.01907756785389447]
	TIME [epoch: 5.13 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019913899877953403		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.019913899877953403 | validation: 0.01840250574016308]
	TIME [epoch: 5.16 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019774463257551906		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.019774463257551906 | validation: 0.019223899120065578]
	TIME [epoch: 5.13 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02050966444250498		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.02050966444250498 | validation: 0.01773214384907249]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1280.pth
	Model improved!!!
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021663523597692		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.02021663523597692 | validation: 0.020154412308510818]
	TIME [epoch: 5.12 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019545360232726		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.019545360232726 | validation: 0.01887429048423268]
	TIME [epoch: 5.12 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019504122797509293		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.019504122797509293 | validation: 0.018334373351067017]
	TIME [epoch: 5.12 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019484940000750444		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.019484940000750444 | validation: 0.020586788212342586]
	TIME [epoch: 5.12 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019239433980713413		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.019239433980713413 | validation: 0.019546925025062306]
	TIME [epoch: 5.12 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019824101095677282		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.019824101095677282 | validation: 0.01880614837890686]
	TIME [epoch: 5.13 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019279576414299678		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.019279576414299678 | validation: 0.01962285905994715]
	TIME [epoch: 5.15 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01906210756457504		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.01906210756457504 | validation: 0.0185917399815236]
	TIME [epoch: 5.13 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020371634123310084		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.020371634123310084 | validation: 0.019246141476974052]
	TIME [epoch: 5.11 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01956445472723662		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.01956445472723662 | validation: 0.01996638899850861]
	TIME [epoch: 5.11 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019624389808309967		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.019624389808309967 | validation: 0.01916295492011705]
	TIME [epoch: 5.12 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01965621942037282		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.01965621942037282 | validation: 0.01918359584334902]
	TIME [epoch: 5.12 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01943382992860219		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.01943382992860219 | validation: 0.017601808982398957]
	TIME [epoch: 5.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1293.pth
	Model improved!!!
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01902886478919303		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.01902886478919303 | validation: 0.020079180434258787]
	TIME [epoch: 5.12 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019669239861501677		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.019669239861501677 | validation: 0.019723075519164454]
	TIME [epoch: 5.13 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018916256283865788		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.018916256283865788 | validation: 0.019435973146665078]
	TIME [epoch: 5.16 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019142970899084243		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.019142970899084243 | validation: 0.018434832110239954]
	TIME [epoch: 5.13 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01928200610543534		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.01928200610543534 | validation: 0.019780506913020184]
	TIME [epoch: 5.13 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01894051590724238		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.01894051590724238 | validation: 0.0193460701443572]
	TIME [epoch: 5.13 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019273182024318533		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.019273182024318533 | validation: 0.020627121564383046]
	TIME [epoch: 5.13 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020142811995703502		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.020142811995703502 | validation: 0.01893755309054524]
	TIME [epoch: 5.12 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019199670097172276		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.019199670097172276 | validation: 0.019794715395835473]
	TIME [epoch: 5.12 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01952995628724592		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.01952995628724592 | validation: 0.019117522486662625]
	TIME [epoch: 5.12 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019347114421158065		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.019347114421158065 | validation: 0.019742172650792828]
	TIME [epoch: 5.14 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01876423577738852		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.01876423577738852 | validation: 0.018837388755924302]
	TIME [epoch: 5.14 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018840863769677366		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.018840863769677366 | validation: 0.017163657891058885]
	TIME [epoch: 5.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1306.pth
	Model improved!!!
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01920015275818165		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.01920015275818165 | validation: 0.018712365805624697]
	TIME [epoch: 5.11 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01872851460142413		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.01872851460142413 | validation: 0.018591010987525983]
	TIME [epoch: 5.12 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018684140368130053		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.018684140368130053 | validation: 0.018259177160551913]
	TIME [epoch: 5.12 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019222999077070712		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.019222999077070712 | validation: 0.019081718833460593]
	TIME [epoch: 5.12 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019563046552489034		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.019563046552489034 | validation: 0.017581524279009657]
	TIME [epoch: 5.11 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018957406823934198		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.018957406823934198 | validation: 0.018391990970090838]
	TIME [epoch: 5.11 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018379277437564522		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.018379277437564522 | validation: 0.017932287124060184]
	TIME [epoch: 5.14 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019295682099054988		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.019295682099054988 | validation: 0.018509345378385444]
	TIME [epoch: 5.14 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018275965996215778		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.018275965996215778 | validation: 0.01882460319209717]
	TIME [epoch: 5.12 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018487598221095854		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.018487598221095854 | validation: 0.016664538348643385]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1316.pth
	Model improved!!!
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018978772444558485		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.018978772444558485 | validation: 0.018879349772919315]
	TIME [epoch: 5.11 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0190197440028141		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.0190197440028141 | validation: 0.01948792276770537]
	TIME [epoch: 5.11 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019223300819573762		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.019223300819573762 | validation: 0.021321202566458353]
	TIME [epoch: 5.11 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019212080491386424		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.019212080491386424 | validation: 0.018769753721674127]
	TIME [epoch: 5.11 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01870322507037717		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.01870322507037717 | validation: 0.019480451322409602]
	TIME [epoch: 5.11 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019129179236988904		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.019129179236988904 | validation: 0.018855611496646964]
	TIME [epoch: 5.13 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018599635636454462		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.018599635636454462 | validation: 0.018781506823889606]
	TIME [epoch: 5.14 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019193186657544062		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.019193186657544062 | validation: 0.01873209577833968]
	TIME [epoch: 5.11 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018507332756092964		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.018507332756092964 | validation: 0.019246269445448147]
	TIME [epoch: 5.11 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01870324597202723		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.01870324597202723 | validation: 0.018394882601959296]
	TIME [epoch: 5.11 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01854819976601728		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.01854819976601728 | validation: 0.017885638789665658]
	TIME [epoch: 5.11 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01810545778313936		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.01810545778313936 | validation: 0.01777513577106441]
	TIME [epoch: 5.11 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01838948723749085		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.01838948723749085 | validation: 0.019188514104003213]
	TIME [epoch: 5.11 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019631349632122895		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.019631349632122895 | validation: 0.019588145699243747]
	TIME [epoch: 5.11 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01831309254310596		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.01831309254310596 | validation: 0.018691014750871912]
	TIME [epoch: 5.13 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0184871160949993		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.0184871160949993 | validation: 0.017992220838953223]
	TIME [epoch: 5.13 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017997927915449536		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.017997927915449536 | validation: 0.01776412775155751]
	TIME [epoch: 5.11 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01818817485431471		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.01818817485431471 | validation: 0.01858172498471159]
	TIME [epoch: 5.11 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01865056020321983		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.01865056020321983 | validation: 0.017506654653479836]
	TIME [epoch: 5.11 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01857331960273384		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.01857331960273384 | validation: 0.01824979067063469]
	TIME [epoch: 5.1 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018273024659197803		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.018273024659197803 | validation: 0.017127960926249457]
	TIME [epoch: 5.11 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018094861997622624		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.018094861997622624 | validation: 0.01817267323767894]
	TIME [epoch: 5.11 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018817760831179427		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.018817760831179427 | validation: 0.018514819403390254]
	TIME [epoch: 5.11 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01824319919313985		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.01824319919313985 | validation: 0.01889432729545209]
	TIME [epoch: 5.13 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018673510506251816		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.018673510506251816 | validation: 0.018789418141498877]
	TIME [epoch: 5.14 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018468184969202647		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.018468184969202647 | validation: 0.017908634124852225]
	TIME [epoch: 5.11 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018321615812822795		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.018321615812822795 | validation: 0.0180004044206876]
	TIME [epoch: 5.11 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018273224673788353		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.018273224673788353 | validation: 0.018303050545650082]
	TIME [epoch: 5.11 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018193397282985448		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.018193397282985448 | validation: 0.019245758719568816]
	TIME [epoch: 5.11 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01802189119185292		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.01802189119185292 | validation: 0.018208719393632833]
	TIME [epoch: 5.11 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018550608099479227		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.018550608099479227 | validation: 0.01829554852216739]
	TIME [epoch: 5.11 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01795446820330736		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.01795446820330736 | validation: 0.017075361510721202]
	TIME [epoch: 5.11 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018011931608961976		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.018011931608961976 | validation: 0.017840729385242983]
	TIME [epoch: 5.13 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018679628064637802		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.018679628064637802 | validation: 0.016874670320202283]
	TIME [epoch: 5.14 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0182394694104773		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.0182394694104773 | validation: 0.01750321692281957]
	TIME [epoch: 5.12 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018064534409585095		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.018064534409585095 | validation: 0.01899680250586506]
	TIME [epoch: 5.11 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01797450909520217		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.01797450909520217 | validation: 0.018022182625542407]
	TIME [epoch: 5.11 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018640590506352958		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.018640590506352958 | validation: 0.018528985713543975]
	TIME [epoch: 5.11 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017889548393354843		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.017889548393354843 | validation: 0.018275273027652306]
	TIME [epoch: 5.11 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017889781736225144		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.017889781736225144 | validation: 0.017946280405130383]
	TIME [epoch: 5.11 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018102418321520684		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.018102418321520684 | validation: 0.01883236773900169]
	TIME [epoch: 5.12 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018295761033465464		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.018295761033465464 | validation: 0.018712184915818334]
	TIME [epoch: 5.18 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017711998679636413		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.017711998679636413 | validation: 0.017814395891626776]
	TIME [epoch: 5.14 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017924586131615076		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.017924586131615076 | validation: 0.01932520455857257]
	TIME [epoch: 5.12 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0181149773670482		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.0181149773670482 | validation: 0.01718744088028958]
	TIME [epoch: 5.11 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01795509558383344		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.01795509558383344 | validation: 0.017090043300319567]
	TIME [epoch: 5.11 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017902612744641777		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.017902612744641777 | validation: 0.01813894878037407]
	TIME [epoch: 5.11 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017821586102438787		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.017821586102438787 | validation: 0.018755917385147904]
	TIME [epoch: 5.11 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01813576923743071		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.01813576923743071 | validation: 0.017357826420556357]
	TIME [epoch: 5.11 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018137197972769952		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.018137197972769952 | validation: 0.01778667749605923]
	TIME [epoch: 5.11 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017863440797070557		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.017863440797070557 | validation: 0.018093336538396027]
	TIME [epoch: 5.13 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017376778914971955		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.017376778914971955 | validation: 0.018279229833535183]
	TIME [epoch: 5.13 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017916943294480226		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.017916943294480226 | validation: 0.018126419088452125]
	TIME [epoch: 5.12 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018140719577328383		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.018140719577328383 | validation: 0.0185369635760403]
	TIME [epoch: 5.11 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016986465372431238		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.016986465372431238 | validation: 0.01867994973716985]
	TIME [epoch: 5.11 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017629559109308933		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.017629559109308933 | validation: 0.01956356280041788]
	TIME [epoch: 5.11 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017520732049590867		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.017520732049590867 | validation: 0.019121949697663128]
	TIME [epoch: 5.12 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017534533564047983		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.017534533564047983 | validation: 0.01801365145977814]
	TIME [epoch: 5.11 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017402494842533714		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.017402494842533714 | validation: 0.020336192921898977]
	TIME [epoch: 5.11 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018268542402748396		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.018268542402748396 | validation: 0.01803494970639509]
	TIME [epoch: 5.13 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017261825466281324		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.017261825466281324 | validation: 0.01793815693252675]
	TIME [epoch: 5.13 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01773013443257506		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.01773013443257506 | validation: 0.017977914560863888]
	TIME [epoch: 5.1 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01757324935526282		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.01757324935526282 | validation: 0.018113135027474338]
	TIME [epoch: 5.07 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017486338901637682		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.017486338901637682 | validation: 0.018665145157497606]
	TIME [epoch: 5.07 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017095643724975174		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.017095643724975174 | validation: 0.01780550216461283]
	TIME [epoch: 5.08 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017751973953313103		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.017751973953313103 | validation: 0.017973773826100287]
	TIME [epoch: 5.08 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017250310382194566		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.017250310382194566 | validation: 0.017568298624961797]
	TIME [epoch: 5.09 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174428957776547		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.0174428957776547 | validation: 0.017443183297562163]
	TIME [epoch: 5.09 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017554563344947868		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.017554563344947868 | validation: 0.018392835508248544]
	TIME [epoch: 5.1 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017414417561438637		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.017414417561438637 | validation: 0.017086897249449347]
	TIME [epoch: 5.1 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018069922339315793		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.018069922339315793 | validation: 0.01773599769872783]
	TIME [epoch: 5.08 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017115862711362798		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.017115862711362798 | validation: 0.017643351220019768]
	TIME [epoch: 5.07 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017083880135757654		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.017083880135757654 | validation: 0.01784142145013405]
	TIME [epoch: 5.07 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017308584329542895		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.017308584329542895 | validation: 0.018332973533937674]
	TIME [epoch: 5.07 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018126626183872608		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.018126626183872608 | validation: 0.018188432649476508]
	TIME [epoch: 5.07 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01889815835673948		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.01889815835673948 | validation: 0.017753326399545143]
	TIME [epoch: 5.07 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017476847931556492		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.017476847931556492 | validation: 0.017008680417882886]
	TIME [epoch: 5.07 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01728929675745094		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.01728929675745094 | validation: 0.016991016233395082]
	TIME [epoch: 5.09 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017172457432065084		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.017172457432065084 | validation: 0.0188228777828109]
	TIME [epoch: 5.1 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017475991970280606		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.017475991970280606 | validation: 0.018682193105272586]
	TIME [epoch: 5.08 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017140063705492006		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.017140063705492006 | validation: 0.017854865612764174]
	TIME [epoch: 5.07 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017096974206601855		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.017096974206601855 | validation: 0.017584896901928646]
	TIME [epoch: 5.07 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017577018596773505		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.017577018596773505 | validation: 0.018577624855695453]
	TIME [epoch: 5.07 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01816476090353273		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.01816476090353273 | validation: 0.016815892330959946]
	TIME [epoch: 5.08 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017246017336986944		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.017246017336986944 | validation: 0.018503509085749187]
	TIME [epoch: 5.07 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016615931461719496		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.016615931461719496 | validation: 0.017485015833281572]
	TIME [epoch: 5.07 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016962582599629794		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.016962582599629794 | validation: 0.018139590879717653]
	TIME [epoch: 5.08 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016905246616657377		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.016905246616657377 | validation: 0.017917983114771212]
	TIME [epoch: 5.1 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017330008078978137		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.017330008078978137 | validation: 0.016464394094493767]
	TIME [epoch: 5.08 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1405.pth
	Model improved!!!
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01695580511014709		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.01695580511014709 | validation: 0.018964249374325733]
	TIME [epoch: 5.08 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017224498375358322		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.017224498375358322 | validation: 0.017434785202430605]
	TIME [epoch: 5.09 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016919859522095446		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.016919859522095446 | validation: 0.01733317793396722]
	TIME [epoch: 5.08 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01699147678030298		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.01699147678030298 | validation: 0.018578809548655442]
	TIME [epoch: 5.08 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016200207395213388		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.016200207395213388 | validation: 0.018408614583118654]
	TIME [epoch: 5.07 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01735221401310851		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.01735221401310851 | validation: 0.018264048367653822]
	TIME [epoch: 5.07 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016445931439500537		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.016445931439500537 | validation: 0.018400439511489453]
	TIME [epoch: 5.07 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016796594903102192		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.016796594903102192 | validation: 0.01802816139119389]
	TIME [epoch: 5.1 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016963490782525584		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.016963490782525584 | validation: 0.01704419544180542]
	TIME [epoch: 5.07 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016413011806254256		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.016413011806254256 | validation: 0.017968037420797563]
	TIME [epoch: 5.06 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01753652045515115		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.01753652045515115 | validation: 0.01687462116811681]
	TIME [epoch: 5.07 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016937465372795755		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.016937465372795755 | validation: 0.017291482934213113]
	TIME [epoch: 5.06 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01657437026591256		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.01657437026591256 | validation: 0.01783850505124542]
	TIME [epoch: 5.07 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0169335458743693		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.0169335458743693 | validation: 0.01641746914919991]
	TIME [epoch: 5.07 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1419.pth
	Model improved!!!
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016147923812469635		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.016147923812469635 | validation: 0.01647871641702245]
	TIME [epoch: 5.09 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01636056668979747		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.01636056668979747 | validation: 0.018254084665487273]
	TIME [epoch: 5.1 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016670238434432376		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.016670238434432376 | validation: 0.018762738938072496]
	TIME [epoch: 5.11 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01645934034529042		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.01645934034529042 | validation: 0.018277434283585307]
	TIME [epoch: 5.09 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01642495023501733		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.01642495023501733 | validation: 0.01727382083462555]
	TIME [epoch: 5.08 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01699924272695086		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.01699924272695086 | validation: 0.017264150620074394]
	TIME [epoch: 5.08 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01680443142884289		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.01680443142884289 | validation: 0.018083872002262386]
	TIME [epoch: 5.09 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016334964261424462		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.016334964261424462 | validation: 0.017167356591839857]
	TIME [epoch: 5.09 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016485796619737855		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.016485796619737855 | validation: 0.017879523488357855]
	TIME [epoch: 5.09 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017290534705886644		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.017290534705886644 | validation: 0.017563077850316258]
	TIME [epoch: 5.1 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016340687930776435		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.016340687930776435 | validation: 0.018982031538114293]
	TIME [epoch: 5.1 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016492937092338643		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.016492937092338643 | validation: 0.017628641252711016]
	TIME [epoch: 5.13 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017325058023823864		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.017325058023823864 | validation: 0.017603453746257443]
	TIME [epoch: 5.1 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016178296067613642		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.016178296067613642 | validation: 0.017879338902117056]
	TIME [epoch: 5.09 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0161544202855232		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.0161544202855232 | validation: 0.01718158128277274]
	TIME [epoch: 5.09 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016215874841028086		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.016215874841028086 | validation: 0.018118008443827025]
	TIME [epoch: 5.09 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016670331498055982		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.016670331498055982 | validation: 0.01753652237799902]
	TIME [epoch: 5.1 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01615881376257616		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.01615881376257616 | validation: 0.017653736252921413]
	TIME [epoch: 5.09 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016560981617674848		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.016560981617674848 | validation: 0.01761434910755552]
	TIME [epoch: 5.09 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01645985185732256		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.01645985185732256 | validation: 0.01845585124160329]
	TIME [epoch: 5.11 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01700685235228328		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.01700685235228328 | validation: 0.018380227858506126]
	TIME [epoch: 5.13 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016679606094772002		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.016679606094772002 | validation: 0.019027642676710352]
	TIME [epoch: 5.11 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01680955545419534		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.01680955545419534 | validation: 0.018335602617651475]
	TIME [epoch: 5.09 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015895210937255495		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.015895210937255495 | validation: 0.01780613990792996]
	TIME [epoch: 5.09 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017209251415289308		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.017209251415289308 | validation: 0.01704975787307428]
	TIME [epoch: 5.09 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016527245086646866		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.016527245086646866 | validation: 0.016410248593230932]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1445.pth
	Model improved!!!
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01668942142474588		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.01668942142474588 | validation: 0.017925253497062487]
	TIME [epoch: 5.1 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016731054873793127		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.016731054873793127 | validation: 0.018266548686237033]
	TIME [epoch: 5.09 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017036711767590966		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.017036711767590966 | validation: 0.018391740445796508]
	TIME [epoch: 5.1 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016086311237015826		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.016086311237015826 | validation: 0.017540301885459977]
	TIME [epoch: 5.12 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016245068699301396		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.016245068699301396 | validation: 0.018885346216590847]
	TIME [epoch: 5.1 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01611875261522777		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.01611875261522777 | validation: 0.017591060257022475]
	TIME [epoch: 5.09 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016422821213667182		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.016422821213667182 | validation: 0.018024120011741383]
	TIME [epoch: 5.09 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016710399803853963		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.016710399803853963 | validation: 0.016825786410583167]
	TIME [epoch: 5.09 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01602103895374754		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.01602103895374754 | validation: 0.017415514799200095]
	TIME [epoch: 5.09 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016562335549346142		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.016562335549346142 | validation: 0.017071102831717453]
	TIME [epoch: 5.09 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015960887406971488		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.015960887406971488 | validation: 0.018869795259069507]
	TIME [epoch: 5.09 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015950382719382802		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.015950382719382802 | validation: 0.017257326281788974]
	TIME [epoch: 5.1 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01577742391457043		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.01577742391457043 | validation: 0.01744633338192926]
	TIME [epoch: 5.12 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01565065508535001		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.01565065508535001 | validation: 0.01670003720245842]
	TIME [epoch: 5.1 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016160588551920846		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.016160588551920846 | validation: 0.01744576720127041]
	TIME [epoch: 5.09 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015842908949211157		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.015842908949211157 | validation: 0.016819872927636665]
	TIME [epoch: 5.09 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016180729655076526		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.016180729655076526 | validation: 0.017844471006818623]
	TIME [epoch: 5.09 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016057195704731707		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.016057195704731707 | validation: 0.017919808216324884]
	TIME [epoch: 5.09 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016179125158203796		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.016179125158203796 | validation: 0.017788530142398246]
	TIME [epoch: 5.09 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01675750211158393		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.01675750211158393 | validation: 0.01756125447368624]
	TIME [epoch: 5.09 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016086914671593754		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.016086914671593754 | validation: 0.016906089268916943]
	TIME [epoch: 5.1 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015974233114668453		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.015974233114668453 | validation: 0.01689048111460009]
	TIME [epoch: 5.12 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01592103080971231		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.01592103080971231 | validation: 0.0167098578232326]
	TIME [epoch: 5.1 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016297040115035247		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.016297040115035247 | validation: 0.01671944549469476]
	TIME [epoch: 5.09 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016159318079547104		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.016159318079547104 | validation: 0.018264589209549514]
	TIME [epoch: 5.09 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015911145104005632		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.015911145104005632 | validation: 0.017360324057642477]
	TIME [epoch: 5.09 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0159194063063461		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.0159194063063461 | validation: 0.017706667510856186]
	TIME [epoch: 5.09 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015897882815794864		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.015897882815794864 | validation: 0.017784369635590425]
	TIME [epoch: 5.09 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01578450600631258		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.01578450600631258 | validation: 0.01739232103922828]
	TIME [epoch: 5.09 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01618314284265408		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.01618314284265408 | validation: 0.01581380375805705]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1475.pth
	Model improved!!!
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016355127253841813		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.016355127253841813 | validation: 0.016524125069640787]
	TIME [epoch: 5.12 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016721994965202394		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.016721994965202394 | validation: 0.01806208032112172]
	TIME [epoch: 5.09 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01583081735670065		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.01583081735670065 | validation: 0.017769809168227006]
	TIME [epoch: 5.08 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015996127314572055		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.015996127314572055 | validation: 0.016969852684785673]
	TIME [epoch: 5.08 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015986601029964152		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.015986601029964152 | validation: 0.017712042495357353]
	TIME [epoch: 5.08 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016030036915151306		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.016030036915151306 | validation: 0.017773800314184234]
	TIME [epoch: 5.08 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016424187510124057		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.016424187510124057 | validation: 0.017232141712217517]
	TIME [epoch: 5.08 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01625042741652491		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.01625042741652491 | validation: 0.01704397161092112]
	TIME [epoch: 5.08 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01622027552084296		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.01622027552084296 | validation: 0.017057381960856096]
	TIME [epoch: 5.09 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015737113928825066		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.015737113928825066 | validation: 0.017235272250511788]
	TIME [epoch: 5.11 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015838078867713403		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.015838078867713403 | validation: 0.016343625365378527]
	TIME [epoch: 5.09 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01641142586682632		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.01641142586682632 | validation: 0.016623479021673153]
	TIME [epoch: 5.08 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01601718285268002		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.01601718285268002 | validation: 0.016907521362386255]
	TIME [epoch: 5.08 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01602349963891661		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.01602349963891661 | validation: 0.01611111900830821]
	TIME [epoch: 5.08 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016031136503378323		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.016031136503378323 | validation: 0.016089104756418223]
	TIME [epoch: 5.08 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01565775219890745		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.01565775219890745 | validation: 0.017164269271112906]
	TIME [epoch: 5.08 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016686610819346064		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.016686610819346064 | validation: 0.018686949559868517]
	TIME [epoch: 5.08 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015940927506989493		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.015940927506989493 | validation: 0.01753668631634161]
	TIME [epoch: 5.08 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015474522598155953		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.015474522598155953 | validation: 0.01686287508211037]
	TIME [epoch: 5.11 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016176037778383865		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.016176037778383865 | validation: 0.017485980799741715]
	TIME [epoch: 5.08 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015347000333405384		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.015347000333405384 | validation: 0.018260061899317215]
	TIME [epoch: 5.08 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01551491333984514		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.01551491333984514 | validation: 0.017877585080921108]
	TIME [epoch: 5.08 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015521098387564844		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.015521098387564844 | validation: 0.016813934762585497]
	TIME [epoch: 5.08 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015645425387256287		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.015645425387256287 | validation: 0.017272798304344816]
	TIME [epoch: 5.08 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01576402961971702		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.01576402961971702 | validation: 0.018151572227294504]
	TIME [epoch: 5.08 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01592194383261041		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.01592194383261041 | validation: 0.01770910948560092]
	TIME [epoch: 5.07 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015551690132604698		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.015551690132604698 | validation: 0.018155743182607993]
	TIME [epoch: 5.08 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01578830912610798		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.01578830912610798 | validation: 0.016776540430981257]
	TIME [epoch: 5.11 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01537069151741634		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.01537069151741634 | validation: 0.017200068562996204]
	TIME [epoch: 5.08 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016249720892582885		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.016249720892582885 | validation: 0.016075059035260263]
	TIME [epoch: 5.07 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015378873695771952		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.015378873695771952 | validation: 0.01812730092575475]
	TIME [epoch: 5.08 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015571346922986003		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.015571346922986003 | validation: 0.016953471293051844]
	TIME [epoch: 5.07 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015494906593752168		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.015494906593752168 | validation: 0.01775003945157021]
	TIME [epoch: 5.07 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015651369500461376		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.015651369500461376 | validation: 0.017425394617299812]
	TIME [epoch: 5.07 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015271056925379386		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.015271056925379386 | validation: 0.016931294113403637]
	TIME [epoch: 5.07 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015597742368001866		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.015597742368001866 | validation: 0.018019135662016135]
	TIME [epoch: 5.07 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01564884711482352		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.01564884711482352 | validation: 0.018258619974770823]
	TIME [epoch: 5.11 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015327887623930383		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.015327887623930383 | validation: 0.018359571114338842]
	TIME [epoch: 5.08 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015624701565075047		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.015624701565075047 | validation: 0.01696401453154802]
	TIME [epoch: 5.07 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01613807350361338		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.01613807350361338 | validation: 0.01804210600390163]
	TIME [epoch: 5.07 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01575668187313635		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.01575668187313635 | validation: 0.017643132026540987]
	TIME [epoch: 5.07 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01580522362394772		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.01580522362394772 | validation: 0.01678982972191652]
	TIME [epoch: 5.08 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015500903738012672		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.015500903738012672 | validation: 0.016750328377258403]
	TIME [epoch: 5.08 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014825231463781703		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.014825231463781703 | validation: 0.016440041654757646]
	TIME [epoch: 5.07 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015237219328607925		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.015237219328607925 | validation: 0.017472979966437728]
	TIME [epoch: 5.07 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015237781023492013		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.015237781023492013 | validation: 0.016940282543699378]
	TIME [epoch: 5.11 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015625161843872172		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.015625161843872172 | validation: 0.016919399267993913]
	TIME [epoch: 5.08 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015754693601502594		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.015754693601502594 | validation: 0.01794132036623801]
	TIME [epoch: 5.07 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015449855503477494		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.015449855503477494 | validation: 0.016646339018904978]
	TIME [epoch: 5.07 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014990331450872127		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.014990331450872127 | validation: 0.01678513314561839]
	TIME [epoch: 5.07 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015599157435413563		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.015599157435413563 | validation: 0.016704026726285508]
	TIME [epoch: 5.07 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015450599669953588		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.015450599669953588 | validation: 0.01691382667829474]
	TIME [epoch: 5.07 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015106325258123412		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.015106325258123412 | validation: 0.017061629553032868]
	TIME [epoch: 5.07 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016022369579068916		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.016022369579068916 | validation: 0.016969388014481696]
	TIME [epoch: 5.08 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015188799515178125		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.015188799515178125 | validation: 0.017121335726828474]
	TIME [epoch: 5.11 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01563997675227488		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.01563997675227488 | validation: 0.018493144374371377]
	TIME [epoch: 5.07 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015857560791410114		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.015857560791410114 | validation: 0.01592082451699185]
	TIME [epoch: 5.07 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015871361739365562		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.015871361739365562 | validation: 0.018845872315583424]
	TIME [epoch: 5.07 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015459901443096738		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.015459901443096738 | validation: 0.01672482754763392]
	TIME [epoch: 5.07 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015387780862031446		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.015387780862031446 | validation: 0.016797089617772282]
	TIME [epoch: 5.07 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014996785529242489		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.014996785529242489 | validation: 0.01686643260703107]
	TIME [epoch: 5.07 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015239891148942126		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.015239891148942126 | validation: 0.01779018455878774]
	TIME [epoch: 5.07 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015224050075975661		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.015224050075975661 | validation: 0.016171683073541453]
	TIME [epoch: 5.07 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015778850395168582		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.015778850395168582 | validation: 0.017418529018836743]
	TIME [epoch: 5.1 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015188166346936396		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.015188166346936396 | validation: 0.017763657890959756]
	TIME [epoch: 5.08 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015270959017043305		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.015270959017043305 | validation: 0.016632265608550592]
	TIME [epoch: 5.07 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016160527149594282		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.016160527149594282 | validation: 0.017719186021344167]
	TIME [epoch: 5.07 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015559951399972648		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.015559951399972648 | validation: 0.016206762654769384]
	TIME [epoch: 5.07 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015098628466360297		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.015098628466360297 | validation: 0.016786428130985936]
	TIME [epoch: 5.07 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01567929672077502		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.01567929672077502 | validation: 0.016876860151487722]
	TIME [epoch: 5.08 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015279258865310546		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.015279258865310546 | validation: 0.017647044340125928]
	TIME [epoch: 5.07 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015672059677728833		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.015672059677728833 | validation: 0.01753025903196634]
	TIME [epoch: 5.07 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01504855574845922		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.01504855574845922 | validation: 0.01713214461217048]
	TIME [epoch: 5.09 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014969995180697644		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.014969995180697644 | validation: 0.017120207410513663]
	TIME [epoch: 5.1 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015197774382462486		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.015197774382462486 | validation: 0.015450744952741312]
	TIME [epoch: 5.08 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1550.pth
	Model improved!!!
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014987716565416798		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.014987716565416798 | validation: 0.017155101367236757]
	TIME [epoch: 5.07 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014874617926212176		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.014874617926212176 | validation: 0.01599175086758654]
	TIME [epoch: 5.07 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015163259705925296		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.015163259705925296 | validation: 0.016915638771418336]
	TIME [epoch: 5.07 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01486527687236652		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.01486527687236652 | validation: 0.016258305883200118]
	TIME [epoch: 5.07 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015118490492881498		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.015118490492881498 | validation: 0.017739321989078082]
	TIME [epoch: 5.07 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01511816710614089		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.01511816710614089 | validation: 0.01742494985625301]
	TIME [epoch: 5.07 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014748657807279759		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.014748657807279759 | validation: 0.017043107281104516]
	TIME [epoch: 5.09 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01515099521732459		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.01515099521732459 | validation: 0.015719208179505534]
	TIME [epoch: 5.09 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014241327818617191		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.014241327818617191 | validation: 0.018325809219412945]
	TIME [epoch: 5.07 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015284980357077467		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.015284980357077467 | validation: 0.016401067600476053]
	TIME [epoch: 5.07 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015080743702982114		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.015080743702982114 | validation: 0.0167660948111491]
	TIME [epoch: 5.07 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015006327056914756		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.015006327056914756 | validation: 0.016027592734786014]
	TIME [epoch: 5.07 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014930425118094701		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.014930425118094701 | validation: 0.0177338846687356]
	TIME [epoch: 5.07 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015545721931524582		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.015545721931524582 | validation: 0.017683147767735986]
	TIME [epoch: 5.07 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015202924069201132		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.015202924069201132 | validation: 0.016437117898561896]
	TIME [epoch: 5.07 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014795204087643626		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.014795204087643626 | validation: 0.01580031410082247]
	TIME [epoch: 5.09 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014909269684883908		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.014909269684883908 | validation: 0.017543778356224032]
	TIME [epoch: 5.09 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015736197653644268		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.015736197653644268 | validation: 0.017738540913111174]
	TIME [epoch: 5.07 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015663765208417328		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.015663765208417328 | validation: 0.01778416210327273]
	TIME [epoch: 5.07 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014568246365648706		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.014568246365648706 | validation: 0.0159563921735877]
	TIME [epoch: 5.07 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014886855460975111		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.014886855460975111 | validation: 0.01615035552809485]
	TIME [epoch: 5.07 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0150024457840197		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 0.0150024457840197 | validation: 0.018122233360994937]
	TIME [epoch: 5.08 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014939482116435109		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 0.014939482116435109 | validation: 0.016667919668389087]
	TIME [epoch: 5.07 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014765371613545172		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 0.014765371613545172 | validation: 0.01598612850868093]
	TIME [epoch: 5.09 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014723773019561463		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 0.014723773019561463 | validation: 0.017722938069451813]
	TIME [epoch: 5.09 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01515489640809993		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 0.01515489640809993 | validation: 0.015187904921224694]
	TIME [epoch: 5.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1576.pth
	Model improved!!!
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014810966888956119		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 0.014810966888956119 | validation: 0.01614418182306861]
	TIME [epoch: 5.09 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014990034425472214		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 0.014990034425472214 | validation: 0.018648516194453953]
	TIME [epoch: 5.08 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015262068417436447		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 0.015262068417436447 | validation: 0.017293312639286948]
	TIME [epoch: 5.09 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014887131203451535		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 0.014887131203451535 | validation: 0.01657725793031876]
	TIME [epoch: 5.08 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01421544325470266		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.01421544325470266 | validation: 0.017112840270844556]
	TIME [epoch: 5.09 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014621731374129178		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 0.014621731374129178 | validation: 0.017192159918409552]
	TIME [epoch: 5.08 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014899227320724052		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 0.014899227320724052 | validation: 0.0175322625716272]
	TIME [epoch: 5.08 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01491997947211444		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 0.01491997947211444 | validation: 0.016121745009555818]
	TIME [epoch: 5.1 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014339202945158688		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 0.014339202945158688 | validation: 0.015163200137211848]
	TIME [epoch: 5.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1585.pth
	Model improved!!!
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01517463029190532		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 0.01517463029190532 | validation: 0.01751131502065811]
	TIME [epoch: 5.09 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014553500637762649		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 0.014553500637762649 | validation: 0.018001540556714386]
	TIME [epoch: 5.08 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014834745203952024		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 0.014834745203952024 | validation: 0.016489955467048255]
	TIME [epoch: 5.08 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014963923265530693		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 0.014963923265530693 | validation: 0.016201378414618803]
	TIME [epoch: 5.08 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014761364153124037		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 0.014761364153124037 | validation: 0.016417468243896116]
	TIME [epoch: 5.08 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015599278084365014		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 0.015599278084365014 | validation: 0.017395989279028076]
	TIME [epoch: 5.08 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015511377468563573		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 0.015511377468563573 | validation: 0.01660627091332574]
	TIME [epoch: 5.08 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014819883614028414		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 0.014819883614028414 | validation: 0.0168855913803131]
	TIME [epoch: 5.1 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014715760372790335		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 0.014715760372790335 | validation: 0.015305940569784644]
	TIME [epoch: 5.11 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014358324925684127		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 0.014358324925684127 | validation: 0.017586063129073137]
	TIME [epoch: 5.08 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014530780654343106		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 0.014530780654343106 | validation: 0.016435161042504816]
	TIME [epoch: 5.08 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01444351971045095		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 0.01444351971045095 | validation: 0.0156355421402612]
	TIME [epoch: 5.08 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014669292570246004		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 0.014669292570246004 | validation: 0.016168437661144775]
	TIME [epoch: 5.09 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014201679768055607		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 0.014201679768055607 | validation: 0.018566230647420447]
	TIME [epoch: 5.08 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014453158672675534		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 0.014453158672675534 | validation: 0.01684037624213759]
	TIME [epoch: 5.08 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015154865876824217		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 0.015154865876824217 | validation: 0.016903561902594327]
	TIME [epoch: 5.1 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015083307647956623		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 0.015083307647956623 | validation: 0.017498104541687123]
	TIME [epoch: 5.11 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014349092655136817		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 0.014349092655136817 | validation: 0.015902593676349795]
	TIME [epoch: 5.12 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015071085363394824		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 0.015071085363394824 | validation: 0.017074367108435934]
	TIME [epoch: 5.1 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014568287807480883		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 0.014568287807480883 | validation: 0.01631744257485929]
	TIME [epoch: 5.1 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01486017431686851		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 0.01486017431686851 | validation: 0.016249279967921332]
	TIME [epoch: 5.09 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0148386132125046		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 0.0148386132125046 | validation: 0.01702207149276386]
	TIME [epoch: 5.1 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014891815484588603		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 0.014891815484588603 | validation: 0.01694629576892976]
	TIME [epoch: 5.09 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014429614611477177		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 0.014429614611477177 | validation: 0.01650416236784526]
	TIME [epoch: 5.1 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014974629641267806		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 0.014974629641267806 | validation: 0.016633292948496886]
	TIME [epoch: 5.1 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014898083451111571		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 0.014898083451111571 | validation: 0.01564296104638332]
	TIME [epoch: 5.12 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015071245134703152		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 0.015071245134703152 | validation: 0.017268648691588815]
	TIME [epoch: 5.12 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014726094516835183		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 0.014726094516835183 | validation: 0.016363468205263913]
	TIME [epoch: 5.1 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01462003733615094		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 0.01462003733615094 | validation: 0.01623379610898268]
	TIME [epoch: 5.1 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014757416157442686		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 0.014757416157442686 | validation: 0.016281826177597304]
	TIME [epoch: 5.1 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014763515473103636		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 0.014763515473103636 | validation: 0.015987327065817042]
	TIME [epoch: 5.1 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014950413059907786		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 0.014950413059907786 | validation: 0.016619381795206507]
	TIME [epoch: 5.1 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014619791288692308		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 0.014619791288692308 | validation: 0.01665028342829304]
	TIME [epoch: 5.1 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015013285200046675		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 0.015013285200046675 | validation: 0.01674120355052177]
	TIME [epoch: 5.1 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014720321195632586		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.014720321195632586 | validation: 0.016204668365972626]
	TIME [epoch: 5.12 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01410770520593099		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 0.01410770520593099 | validation: 0.017106839976275736]
	TIME [epoch: 5.12 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014610180859358285		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 0.014610180859358285 | validation: 0.015368548265948958]
	TIME [epoch: 5.1 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01467763672925061		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 0.01467763672925061 | validation: 0.016232249277028113]
	TIME [epoch: 5.1 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014823021084586465		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 0.014823021084586465 | validation: 0.01652532106052586]
	TIME [epoch: 5.1 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01480043073521771		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 0.01480043073521771 | validation: 0.017223933465454998]
	TIME [epoch: 5.1 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01459272172634945		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 0.01459272172634945 | validation: 0.017631821729632506]
	TIME [epoch: 5.1 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01432907192827932		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 0.01432907192827932 | validation: 0.017183942754551495]
	TIME [epoch: 5.1 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014550733357562441		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 0.014550733357562441 | validation: 0.015907805750473458]
	TIME [epoch: 5.1 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014996282666376178		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 0.014996282666376178 | validation: 0.017126007514397114]
	TIME [epoch: 5.12 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014978606653975053		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 0.014978606653975053 | validation: 0.016381406783875493]
	TIME [epoch: 5.12 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014669822594446578		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 0.014669822594446578 | validation: 0.0173659967548027]
	TIME [epoch: 5.1 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014580321813119107		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 0.014580321813119107 | validation: 0.016207912568975705]
	TIME [epoch: 5.1 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014007527182271398		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 0.014007527182271398 | validation: 0.01561477069655816]
	TIME [epoch: 5.1 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014731778649247351		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 0.014731778649247351 | validation: 0.017149134461767218]
	TIME [epoch: 5.1 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014249735063525015		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 0.014249735063525015 | validation: 0.016294098325531216]
	TIME [epoch: 5.1 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01435121326633011		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 0.01435121326633011 | validation: 0.017472549644051587]
	TIME [epoch: 5.09 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014629716993842867		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 0.014629716993842867 | validation: 0.015605724008278893]
	TIME [epoch: 5.1 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014034964748936398		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 0.014034964748936398 | validation: 0.01795756105110073]
	TIME [epoch: 5.12 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014674462531662276		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 0.014674462531662276 | validation: 0.016481105743146155]
	TIME [epoch: 5.12 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014737993690745549		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 0.014737993690745549 | validation: 0.01567754344381691]
	TIME [epoch: 5.1 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014742657213752237		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 0.014742657213752237 | validation: 0.01631901597642912]
	TIME [epoch: 5.1 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014197635552215773		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 0.014197635552215773 | validation: 0.016563597628812154]
	TIME [epoch: 5.1 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014462751357352742		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 0.014462751357352742 | validation: 0.0160427916559873]
	TIME [epoch: 5.1 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014383608178653364		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 0.014383608178653364 | validation: 0.017568841692334424]
	TIME [epoch: 5.1 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014270406486298882		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 0.014270406486298882 | validation: 0.0176205332271089]
	TIME [epoch: 5.1 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014641624314444688		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 0.014641624314444688 | validation: 0.016905694088782543]
	TIME [epoch: 5.1 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014676344565963426		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 0.014676344565963426 | validation: 0.016159969789256347]
	TIME [epoch: 5.12 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014713919028418933		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 0.014713919028418933 | validation: 0.016797106827924936]
	TIME [epoch: 5.12 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014483266188603487		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 0.014483266188603487 | validation: 0.01703075083533185]
	TIME [epoch: 5.1 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01406509631228409		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 0.01406509631228409 | validation: 0.017095066029010297]
	TIME [epoch: 5.1 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014382572259534099		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 0.014382572259534099 | validation: 0.01699692690917116]
	TIME [epoch: 5.09 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014435548285948374		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 0.014435548285948374 | validation: 0.01620910784911325]
	TIME [epoch: 5.1 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014607828873773118		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 0.014607828873773118 | validation: 0.016947187515766786]
	TIME [epoch: 5.1 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014340927100757048		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 0.014340927100757048 | validation: 0.017318965082496893]
	TIME [epoch: 5.1 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014758635548967745		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 0.014758635548967745 | validation: 0.01574713515950423]
	TIME [epoch: 5.1 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014161279992524027		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 0.014161279992524027 | validation: 0.01616699338894378]
	TIME [epoch: 5.12 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014420783597722295		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 0.014420783597722295 | validation: 0.016850731961939112]
	TIME [epoch: 5.13 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014899982215638511		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 0.014899982215638511 | validation: 0.01575824691499967]
	TIME [epoch: 5.1 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014087413454106586		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.014087413454106586 | validation: 0.015733576864454674]
	TIME [epoch: 5.1 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014622728151890137		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 0.014622728151890137 | validation: 0.017372378230617547]
	TIME [epoch: 5.1 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013869717199317072		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 0.013869717199317072 | validation: 0.01779680802629631]
	TIME [epoch: 5.1 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014018770417141372		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 0.014018770417141372 | validation: 0.01724604946622553]
	TIME [epoch: 5.1 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014847029267437539		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 0.014847029267437539 | validation: 0.017229622403728367]
	TIME [epoch: 5.1 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014348590822750897		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 0.014348590822750897 | validation: 0.017068193206471127]
	TIME [epoch: 5.1 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013878331006110536		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 0.013878331006110536 | validation: 0.017605644587780363]
	TIME [epoch: 5.12 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014661817980429322		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 0.014661817980429322 | validation: 0.01686117877618359]
	TIME [epoch: 5.12 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014803742706765351		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 0.014803742706765351 | validation: 0.016036919108463533]
	TIME [epoch: 5.1 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014274233514917047		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 0.014274233514917047 | validation: 0.015465924698624825]
	TIME [epoch: 5.09 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013857672735260288		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 0.013857672735260288 | validation: 0.01664961135689519]
	TIME [epoch: 5.09 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01400295008601761		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 0.01400295008601761 | validation: 0.016231148915973964]
	TIME [epoch: 5.1 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014214118536671766		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 0.014214118536671766 | validation: 0.016978432332180292]
	TIME [epoch: 5.1 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014323031946870766		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 0.014323031946870766 | validation: 0.016942660535464133]
	TIME [epoch: 5.1 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013703926798595056		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 0.013703926798595056 | validation: 0.0163191917210043]
	TIME [epoch: 5.1 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014464496377714499		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 0.014464496377714499 | validation: 0.016460792895163622]
	TIME [epoch: 5.12 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014150041874295919		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 0.014150041874295919 | validation: 0.016952962612851973]
	TIME [epoch: 5.11 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01422187125778385		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 0.01422187125778385 | validation: 0.01684313200156883]
	TIME [epoch: 5.1 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014172753905209447		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 0.014172753905209447 | validation: 0.016726923052422103]
	TIME [epoch: 5.1 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014336321640753246		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 0.014336321640753246 | validation: 0.016023511932958628]
	TIME [epoch: 5.1 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014087165486347052		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 0.014087165486347052 | validation: 0.018078796150338618]
	TIME [epoch: 5.1 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014036000887408974		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 0.014036000887408974 | validation: 0.016225441832173892]
	TIME [epoch: 5.1 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014010720641389801		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 0.014010720641389801 | validation: 0.015818316609035096]
	TIME [epoch: 5.09 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01405207687480909		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 0.01405207687480909 | validation: 0.016441232608297232]
	TIME [epoch: 5.1 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01402475447826354		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 0.01402475447826354 | validation: 0.01615147456806129]
	TIME [epoch: 5.12 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01392213692421746		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 0.01392213692421746 | validation: 0.016264203182415803]
	TIME [epoch: 5.12 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013908070233349784		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 0.013908070233349784 | validation: 0.01612894601575788]
	TIME [epoch: 5.1 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01397496662544365		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 0.01397496662544365 | validation: 0.016018510968040117]
	TIME [epoch: 5.09 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240703_142931/states/model_phi1_1a_v_mmd1_smallnet_1686.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8917.032 seconds.
