Args:
Namespace(name='model_phi1_1a_v_mmd2', outdir='out/model_training/model_phi1_1a_v_mmd2', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=10.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2920968357

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.05922937393323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.05922937393323 | validation: 5.704549069810595]
	TIME [epoch: 113 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.802702235780588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.802702235780588 | validation: 5.551100836289046]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.610787573613604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.610787573613604 | validation: 5.354963771478781]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.374902890516952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.374902890516952 | validation: 5.236751269286218]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1509963091999875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1509963091999875 | validation: 4.917657615580537]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.044393868459728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.044393868459728 | validation: 4.9409600561148785]
	TIME [epoch: 7.85 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822032099543838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.822032099543838 | validation: 4.719265040984322]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.640685866260654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.640685866260654 | validation: 4.585607481432771]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5442397218682276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5442397218682276 | validation: 4.5341003081895055]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.530358815079076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.530358815079076 | validation: 4.496998272777024]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.440136663816421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.440136663816421 | validation: 4.388904818506045]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.359032556245761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.359032556245761 | validation: 4.338151678240671]
	TIME [epoch: 7.91 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.309779264240275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.309779264240275 | validation: 4.28285235163758]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249180584068352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.249180584068352 | validation: 4.226154704281389]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187629825560637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.187629825560637 | validation: 4.196112174610716]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.185726008664483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.185726008664483 | validation: 4.109230172623317]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.07799644555568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.07799644555568 | validation: 4.05043606716636]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0111677359327205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0111677359327205 | validation: 4.01901848295197]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0718561201391275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0718561201391275 | validation: 4.009495718387095]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.962141757775794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.962141757775794 | validation: 3.9388811560821133]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9023087508889214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9023087508889214 | validation: 3.8682133094968982]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.86165494394354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.86165494394354 | validation: 3.9374759616183526]
	TIME [epoch: 7.86 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.869540830273703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.869540830273703 | validation: 3.8263564652712594]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7976458511122395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7976458511122395 | validation: 3.7705732835505708]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.764457345305073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.764457345305073 | validation: 3.906014150068653]
	TIME [epoch: 7.89 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8323441693462463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8323441693462463 | validation: 3.768368649603403]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7313859584983264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7313859584983264 | validation: 3.7094686899386007]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6974853902605958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6974853902605958 | validation: 3.6962938355858608]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.692463332463799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.692463332463799 | validation: 3.7442988945943667]
	TIME [epoch: 7.87 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67815171850265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.67815171850265 | validation: 3.679681259989148]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6373834069834703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6373834069834703 | validation: 3.6534128938623365]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6210650907895863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6210650907895863 | validation: 3.6404835089334977]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.657721265186402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.657721265186402 | validation: 3.719284681565826]
	TIME [epoch: 7.85 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6106938743794452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6106938743794452 | validation: 3.6686170145630497]
	TIME [epoch: 7.89 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5749486911252393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5749486911252393 | validation: 3.6103760354599914]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.548258097341441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.548258097341441 | validation: 3.6291314563231962]
	TIME [epoch: 7.87 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5274836143267096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5274836143267096 | validation: 3.549472113262256]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.50940816119067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.50940816119067 | validation: 3.5960927867484394]
	TIME [epoch: 7.88 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5434930660875783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5434930660875783 | validation: 3.523568324338501]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4911678916718536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4911678916718536 | validation: 3.4920495783336847]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4844552477025132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4844552477025132 | validation: 3.6584453304508147]
	TIME [epoch: 7.86 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5483879697821594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5483879697821594 | validation: 3.6497305268414073]
	TIME [epoch: 7.86 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.564718981062251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.564718981062251 | validation: 3.5270100339867216]
	TIME [epoch: 7.9 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4617666357724106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4617666357724106 | validation: 3.461053527128416]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427782991432527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.427782991432527 | validation: 3.540381482057828]
	TIME [epoch: 7.86 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5308605937175837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5308605937175837 | validation: 3.568985415218854]
	TIME [epoch: 7.85 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.474420153631995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.474420153631995 | validation: 3.446653310141163]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4098686184739413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4098686184739413 | validation: 3.4157056334249845]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4196332673940084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4196332673940084 | validation: 3.549850504674951]
	TIME [epoch: 7.86 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4820706701317947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4820706701317947 | validation: 3.4630844658879174]
	TIME [epoch: 7.84 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4126390783226968		[learning rate: 0.0099823]
	Learning Rate: 0.0099823
	LOSS [training: 3.4126390783226968 | validation: 3.4179112734641164]
	TIME [epoch: 7.85 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.390641573774704		[learning rate: 0.0099588]
	Learning Rate: 0.00995876
	LOSS [training: 3.390641573774704 | validation: 3.4189483468186976]
	TIME [epoch: 7.86 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.447099657113611		[learning rate: 0.0099353]
	Learning Rate: 0.00993527
	LOSS [training: 3.447099657113611 | validation: 3.411239574507083]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.385231530562248		[learning rate: 0.0099118]
	Learning Rate: 0.00991183
	LOSS [training: 3.385231530562248 | validation: 3.3871157442838538]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394718844028512		[learning rate: 0.0098884]
	Learning Rate: 0.00988845
	LOSS [training: 3.394718844028512 | validation: 3.464977728560285]
	TIME [epoch: 7.86 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4139040331084116		[learning rate: 0.0098651]
	Learning Rate: 0.00986512
	LOSS [training: 3.4139040331084116 | validation: 3.3877072814453517]
	TIME [epoch: 7.85 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3830396335327233		[learning rate: 0.0098419]
	Learning Rate: 0.00984185
	LOSS [training: 3.3830396335327233 | validation: 3.4002671497818446]
	TIME [epoch: 7.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3618532915264554		[learning rate: 0.0098186]
	Learning Rate: 0.00981864
	LOSS [training: 3.3618532915264554 | validation: 3.365837809920831]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3902344135621063		[learning rate: 0.0097955]
	Learning Rate: 0.00979548
	LOSS [training: 3.3902344135621063 | validation: 3.3700090311799524]
	TIME [epoch: 7.87 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3723228045120255		[learning rate: 0.0097724]
	Learning Rate: 0.00977237
	LOSS [training: 3.3723228045120255 | validation: 3.368970287518633]
	TIME [epoch: 7.85 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3466622391889964		[learning rate: 0.0097493]
	Learning Rate: 0.00974932
	LOSS [training: 3.3466622391889964 | validation: 3.41954518769489]
	TIME [epoch: 7.86 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3748011571812695		[learning rate: 0.0097263]
	Learning Rate: 0.00972632
	LOSS [training: 3.3748011571812695 | validation: 3.3609271125572606]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3432079388155405		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 3.3432079388155405 | validation: 3.3658392096593968]
	TIME [epoch: 7.86 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3490458526887457		[learning rate: 0.0096805]
	Learning Rate: 0.00968049
	LOSS [training: 3.3490458526887457 | validation: 3.3515500910866503]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328366647977338		[learning rate: 0.0096577]
	Learning Rate: 0.00965766
	LOSS [training: 3.328366647977338 | validation: 3.3456449499070535]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3589704015767077		[learning rate: 0.0096349]
	Learning Rate: 0.00963488
	LOSS [training: 3.3589704015767077 | validation: 3.340812412569362]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327428851434643		[learning rate: 0.0096121]
	Learning Rate: 0.00961215
	LOSS [training: 3.327428851434643 | validation: 3.3669950744598265]
	TIME [epoch: 7.86 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.39757116114407		[learning rate: 0.0095895]
	Learning Rate: 0.00958948
	LOSS [training: 3.39757116114407 | validation: 3.3687572056795823]
	TIME [epoch: 7.84 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3828547675291407		[learning rate: 0.0095669]
	Learning Rate: 0.00956686
	LOSS [training: 3.3828547675291407 | validation: 3.3657810570430327]
	TIME [epoch: 7.84 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333052646125763		[learning rate: 0.0095443]
	Learning Rate: 0.00954429
	LOSS [training: 3.333052646125763 | validation: 3.340355270971137]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3400715239718806		[learning rate: 0.0095218]
	Learning Rate: 0.00952177
	LOSS [training: 3.3400715239718806 | validation: 3.346593885090524]
	TIME [epoch: 7.9 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32855453423091		[learning rate: 0.0094993]
	Learning Rate: 0.00949931
	LOSS [training: 3.32855453423091 | validation: 3.3553466646831063]
	TIME [epoch: 7.86 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3331047327434815		[learning rate: 0.0094769]
	Learning Rate: 0.00947691
	LOSS [training: 3.3331047327434815 | validation: 3.3602711208615035]
	TIME [epoch: 7.83 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3557754607254404		[learning rate: 0.0094546]
	Learning Rate: 0.00945455
	LOSS [training: 3.3557754607254404 | validation: 3.3493680596451276]
	TIME [epoch: 7.85 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332559428309038		[learning rate: 0.0094323]
	Learning Rate: 0.00943225
	LOSS [training: 3.332559428309038 | validation: 3.3701662557063474]
	TIME [epoch: 7.85 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3409878222477216		[learning rate: 0.00941]
	Learning Rate: 0.00941
	LOSS [training: 3.3409878222477216 | validation: 3.330515142041807]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3336665103741576		[learning rate: 0.0093878]
	Learning Rate: 0.00938781
	LOSS [training: 3.3336665103741576 | validation: 3.3564381577756275]
	TIME [epoch: 7.87 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3416883687221715		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 3.3416883687221715 | validation: 3.342586324460561]
	TIME [epoch: 7.85 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.651412602636198		[learning rate: 0.0093436]
	Learning Rate: 0.00934357
	LOSS [training: 3.651412602636198 | validation: 3.636165277909357]
	TIME [epoch: 7.85 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6115438015817		[learning rate: 0.0093215]
	Learning Rate: 0.00932153
	LOSS [training: 3.6115438015817 | validation: 3.494073486548978]
	TIME [epoch: 7.89 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.558060255175078		[learning rate: 0.0092995]
	Learning Rate: 0.00929954
	LOSS [training: 3.558060255175078 | validation: 3.460997238391111]
	TIME [epoch: 7.86 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5374000694125303		[learning rate: 0.0092776]
	Learning Rate: 0.00927761
	LOSS [training: 3.5374000694125303 | validation: 3.4504360891331602]
	TIME [epoch: 7.85 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4497028595653334		[learning rate: 0.0092557]
	Learning Rate: 0.00925572
	LOSS [training: 3.4497028595653334 | validation: 3.3964613960386494]
	TIME [epoch: 7.86 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3854473454441463		[learning rate: 0.0092339]
	Learning Rate: 0.00923389
	LOSS [training: 3.3854473454441463 | validation: 3.3765250195589207]
	TIME [epoch: 7.85 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354262667179925		[learning rate: 0.0092121]
	Learning Rate: 0.00921211
	LOSS [training: 3.354262667179925 | validation: 3.361081954253073]
	TIME [epoch: 7.9 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3519117175212996		[learning rate: 0.0091904]
	Learning Rate: 0.00919038
	LOSS [training: 3.3519117175212996 | validation: 3.359002629072596]
	TIME [epoch: 7.86 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336623870248789		[learning rate: 0.0091687]
	Learning Rate: 0.0091687
	LOSS [training: 3.336623870248789 | validation: 3.391532918693046]
	TIME [epoch: 7.85 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.35227348706089		[learning rate: 0.0091471]
	Learning Rate: 0.00914707
	LOSS [training: 3.35227348706089 | validation: 3.3760141224185096]
	TIME [epoch: 7.85 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379019838291962		[learning rate: 0.0091255]
	Learning Rate: 0.00912549
	LOSS [training: 3.379019838291962 | validation: 3.405719835336879]
	TIME [epoch: 7.86 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3634996405660917		[learning rate: 0.009104]
	Learning Rate: 0.00910397
	LOSS [training: 3.3634996405660917 | validation: 3.3429884534982333]
	TIME [epoch: 7.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3258756824584745		[learning rate: 0.0090825]
	Learning Rate: 0.00908249
	LOSS [training: 3.3258756824584745 | validation: 3.3312262473678578]
	TIME [epoch: 7.85 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329515714891792		[learning rate: 0.0090611]
	Learning Rate: 0.00906107
	LOSS [training: 3.329515714891792 | validation: 3.3364252000621706]
	TIME [epoch: 7.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.357079439869847		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 3.357079439869847 | validation: 3.345366921525955]
	TIME [epoch: 7.85 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3302558455412217		[learning rate: 0.0090184]
	Learning Rate: 0.00901837
	LOSS [training: 3.3302558455412217 | validation: 3.3397368048921847]
	TIME [epoch: 7.86 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.319342975686168		[learning rate: 0.0089971]
	Learning Rate: 0.0089971
	LOSS [training: 3.319342975686168 | validation: 3.336464022079494]
	TIME [epoch: 7.89 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337054188245009		[learning rate: 0.0089759]
	Learning Rate: 0.00897588
	LOSS [training: 3.337054188245009 | validation: 3.3648130410968387]
	TIME [epoch: 7.85 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3675379091660242		[learning rate: 0.0089547]
	Learning Rate: 0.0089547
	LOSS [training: 3.3675379091660242 | validation: 3.3893961478539008]
	TIME [epoch: 7.86 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4211972836698994		[learning rate: 0.0089336]
	Learning Rate: 0.00893358
	LOSS [training: 3.4211972836698994 | validation: 3.3996103314641495]
	TIME [epoch: 7.86 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3654000545256366		[learning rate: 0.0089125]
	Learning Rate: 0.00891251
	LOSS [training: 3.3654000545256366 | validation: 3.357165045258121]
	TIME [epoch: 7.89 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3442013611478805		[learning rate: 0.0088915]
	Learning Rate: 0.00889149
	LOSS [training: 3.3442013611478805 | validation: 3.3339492861699087]
	TIME [epoch: 7.87 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3184981455739795		[learning rate: 0.0088705]
	Learning Rate: 0.00887051
	LOSS [training: 3.3184981455739795 | validation: 3.3326938437191354]
	TIME [epoch: 7.85 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373708399475908		[learning rate: 0.0088496]
	Learning Rate: 0.00884959
	LOSS [training: 3.373708399475908 | validation: 3.3766106350140896]
	TIME [epoch: 7.85 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4890122027603807		[learning rate: 0.0088287]
	Learning Rate: 0.00882871
	LOSS [training: 3.4890122027603807 | validation: 4.270299299046166]
	TIME [epoch: 7.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.023222471986158		[learning rate: 0.0088079]
	Learning Rate: 0.00880789
	LOSS [training: 4.023222471986158 | validation: 3.9157562279827767]
	TIME [epoch: 7.9 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.860758366960426		[learning rate: 0.0087871]
	Learning Rate: 0.00878711
	LOSS [training: 3.860758366960426 | validation: 3.7996404326147637]
	TIME [epoch: 7.86 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7709683100780085		[learning rate: 0.0087664]
	Learning Rate: 0.00876638
	LOSS [training: 3.7709683100780085 | validation: 3.722117382805397]
	TIME [epoch: 7.85 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.676316701951077		[learning rate: 0.0087457]
	Learning Rate: 0.00874571
	LOSS [training: 3.676316701951077 | validation: 3.636801529101678]
	TIME [epoch: 7.86 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6028066261889133		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 3.6028066261889133 | validation: 3.5733018053077803]
	TIME [epoch: 7.85 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.525204672391478		[learning rate: 0.0087045]
	Learning Rate: 0.00870449
	LOSS [training: 3.525204672391478 | validation: 3.4707919924225465]
	TIME [epoch: 7.9 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4428547100056033		[learning rate: 0.008684]
	Learning Rate: 0.00868396
	LOSS [training: 3.4428547100056033 | validation: 3.41229702520993]
	TIME [epoch: 7.85 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.397275075558369		[learning rate: 0.0086635]
	Learning Rate: 0.00866348
	LOSS [training: 3.397275075558369 | validation: 3.3938823138936907]
	TIME [epoch: 7.85 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379763919758222		[learning rate: 0.008643]
	Learning Rate: 0.00864304
	LOSS [training: 3.379763919758222 | validation: 3.394185136034314]
	TIME [epoch: 7.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3832716759229884		[learning rate: 0.0086227]
	Learning Rate: 0.00862265
	LOSS [training: 3.3832716759229884 | validation: 3.412924453083333]
	TIME [epoch: 7.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3875502198349254		[learning rate: 0.0086023]
	Learning Rate: 0.00860232
	LOSS [training: 3.3875502198349254 | validation: 3.376053276880293]
	TIME [epoch: 7.89 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3638472653556155		[learning rate: 0.008582]
	Learning Rate: 0.00858202
	LOSS [training: 3.3638472653556155 | validation: 3.393042872244089]
	TIME [epoch: 7.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3607571238861196		[learning rate: 0.0085618]
	Learning Rate: 0.00856178
	LOSS [training: 3.3607571238861196 | validation: 3.3601272763408487]
	TIME [epoch: 7.85 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3444538141482227		[learning rate: 0.0085416]
	Learning Rate: 0.00854158
	LOSS [training: 3.3444538141482227 | validation: 3.3615890607990204]
	TIME [epoch: 7.85 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3799866128916096		[learning rate: 0.0085214]
	Learning Rate: 0.00852144
	LOSS [training: 3.3799866128916096 | validation: 3.3720964184163407]
	TIME [epoch: 7.88 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353606724446272		[learning rate: 0.0085013]
	Learning Rate: 0.00850134
	LOSS [training: 3.353606724446272 | validation: 3.352075594058535]
	TIME [epoch: 7.87 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.369987178755842		[learning rate: 0.0084813]
	Learning Rate: 0.00848128
	LOSS [training: 3.369987178755842 | validation: 3.344069337041648]
	TIME [epoch: 7.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3398557014831654		[learning rate: 0.0084613]
	Learning Rate: 0.00846128
	LOSS [training: 3.3398557014831654 | validation: 3.3454350128407073]
	TIME [epoch: 7.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3260356798293826		[learning rate: 0.0084413]
	Learning Rate: 0.00844132
	LOSS [training: 3.3260356798293826 | validation: 3.3375320215411497]
	TIME [epoch: 7.85 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343685861530123		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 3.343685861530123 | validation: 3.3866534374772446]
	TIME [epoch: 7.89 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3717668318147647		[learning rate: 0.0084015]
	Learning Rate: 0.00840154
	LOSS [training: 3.3717668318147647 | validation: 3.5008701950109273]
	TIME [epoch: 7.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3683023309860927		[learning rate: 0.0083817]
	Learning Rate: 0.00838172
	LOSS [training: 3.3683023309860927 | validation: 3.4251970639977953]
	TIME [epoch: 7.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.539689599788682		[learning rate: 0.008362]
	Learning Rate: 0.00836195
	LOSS [training: 3.539689599788682 | validation: 3.4102308064036397]
	TIME [epoch: 7.85 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3843123533983777		[learning rate: 0.0083422]
	Learning Rate: 0.00834223
	LOSS [training: 3.3843123533983777 | validation: 3.368795978400997]
	TIME [epoch: 7.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332095008673181		[learning rate: 0.0083225]
	Learning Rate: 0.00832255
	LOSS [training: 3.332095008673181 | validation: 3.3288237227833077]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3109284266368926		[learning rate: 0.0083029]
	Learning Rate: 0.00830292
	LOSS [training: 3.3109284266368926 | validation: 3.308710989182753]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331266624164344		[learning rate: 0.0082833]
	Learning Rate: 0.00828333
	LOSS [training: 3.331266624164344 | validation: 3.350253745835454]
	TIME [epoch: 7.86 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3405391362667776		[learning rate: 0.0082638]
	Learning Rate: 0.00826379
	LOSS [training: 3.3405391362667776 | validation: 3.362229714654399]
	TIME [epoch: 7.86 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318379567035059		[learning rate: 0.0082443]
	Learning Rate: 0.0082443
	LOSS [training: 3.318379567035059 | validation: 3.303715183357818]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3214003101175673		[learning rate: 0.0082249]
	Learning Rate: 0.00822485
	LOSS [training: 3.3214003101175673 | validation: 3.326149273604807]
	TIME [epoch: 7.88 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3257393664149157		[learning rate: 0.0082055]
	Learning Rate: 0.00820545
	LOSS [training: 3.3257393664149157 | validation: 3.493424778367036]
	TIME [epoch: 7.85 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.380895479282865		[learning rate: 0.0081861]
	Learning Rate: 0.0081861
	LOSS [training: 3.380895479282865 | validation: 3.3659791833676227]
	TIME [epoch: 7.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3520467964088887		[learning rate: 0.0081668]
	Learning Rate: 0.00816679
	LOSS [training: 3.3520467964088887 | validation: 3.374371290706448]
	TIME [epoch: 7.85 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299787610679043		[learning rate: 0.0081475]
	Learning Rate: 0.00814752
	LOSS [training: 3.299787610679043 | validation: 3.306814739982137]
	TIME [epoch: 7.9 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3208488499222995		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 3.3208488499222995 | validation: 3.302252310107238]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.312122179301734		[learning rate: 0.0081091]
	Learning Rate: 0.00810913
	LOSS [training: 3.312122179301734 | validation: 3.7855678229772978]
	TIME [epoch: 7.86 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7790593568930584		[learning rate: 0.00809]
	Learning Rate: 0.00809
	LOSS [training: 3.7790593568930584 | validation: 3.937450211955274]
	TIME [epoch: 7.86 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.629560468220207		[learning rate: 0.0080709]
	Learning Rate: 0.00807092
	LOSS [training: 3.629560468220207 | validation: 3.739330795135359]
	TIME [epoch: 7.86 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.553647245724182		[learning rate: 0.0080519]
	Learning Rate: 0.00805188
	LOSS [training: 3.553647245724182 | validation: 3.721546899829147]
	TIME [epoch: 7.9 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.530226297338739		[learning rate: 0.0080329]
	Learning Rate: 0.00803289
	LOSS [training: 3.530226297338739 | validation: 3.8019099878638105]
	TIME [epoch: 7.87 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5650682859968845		[learning rate: 0.0080139]
	Learning Rate: 0.00801394
	LOSS [training: 3.5650682859968845 | validation: 3.698413258004658]
	TIME [epoch: 7.86 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5148546425862457		[learning rate: 0.007995]
	Learning Rate: 0.00799504
	LOSS [training: 3.5148546425862457 | validation: 3.757064903149878]
	TIME [epoch: 7.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5569718937264874		[learning rate: 0.0079762]
	Learning Rate: 0.00797618
	LOSS [training: 3.5569718937264874 | validation: 3.6619398589792036]
	TIME [epoch: 7.87 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.493621593176868		[learning rate: 0.0079574]
	Learning Rate: 0.00795736
	LOSS [training: 3.493621593176868 | validation: 3.608093067513366]
	TIME [epoch: 7.9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4768503592501148		[learning rate: 0.0079386]
	Learning Rate: 0.00793859
	LOSS [training: 3.4768503592501148 | validation: 3.7318725175515275]
	TIME [epoch: 7.86 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.47148454376837		[learning rate: 0.0079199]
	Learning Rate: 0.00791987
	LOSS [training: 3.47148454376837 | validation: 3.32214293824038]
	TIME [epoch: 7.85 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315253140949585		[learning rate: 0.0079012]
	Learning Rate: 0.00790119
	LOSS [training: 3.315253140949585 | validation: 3.2966242835062367]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3008140991933144		[learning rate: 0.0078825]
	Learning Rate: 0.00788255
	LOSS [training: 3.3008140991933144 | validation: 3.364518047526172]
	TIME [epoch: 7.91 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4108325609311096		[learning rate: 0.007864]
	Learning Rate: 0.00786396
	LOSS [training: 3.4108325609311096 | validation: 3.304171664944956]
	TIME [epoch: 7.86 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285151258138229		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 3.285151258138229 | validation: 3.2787952372863485]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd2_20240518_164235/states/model_phi1_1a_v_mmd2_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.272131177447962		[learning rate: 0.0078269]
	Learning Rate: 0.0078269
	LOSS [training: 3.272131177447962 | validation: 3.28391183589887]
	TIME [epoch: 7.86 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283642088634254		[learning rate: 0.0078084]
	Learning Rate: 0.00780844
	LOSS [training: 3.283642088634254 | validation: 3.330622833694817]
	TIME [epoch: 7.86 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7172839351081124		[learning rate: 0.00779]
	Learning Rate: 0.00779002
	LOSS [training: 3.7172839351081124 | validation: 3.867732814436958]
	TIME [epoch: 7.91 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.108847472618019		[learning rate: 0.0077716]
	Learning Rate: 0.00777164
	LOSS [training: 4.108847472618019 | validation: 4.21027570686802]
	TIME [epoch: 7.86 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.106898028228009		[learning rate: 0.0077533]
	Learning Rate: 0.00775331
	LOSS [training: 4.106898028228009 | validation: 3.9600389084097083]
	TIME [epoch: 7.84 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8567384523173076		[learning rate: 0.007735]
	Learning Rate: 0.00773502
	LOSS [training: 3.8567384523173076 | validation: 3.7666109879373333]
	TIME [epoch: 7.85 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.738607448083739		[learning rate: 0.0077168]
	Learning Rate: 0.00771678
	LOSS [training: 3.738607448083739 | validation: 3.7001051230518325]
	TIME [epoch: 7.87 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.632183034226327		[learning rate: 0.0076986]
	Learning Rate: 0.00769857
	LOSS [training: 3.632183034226327 | validation: 3.636569484097233]
	TIME [epoch: 7.9 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5814018046027067		[learning rate: 0.0076804]
	Learning Rate: 0.00768041
	LOSS [training: 3.5814018046027067 | validation: 3.6019206010538642]
	TIME [epoch: 7.86 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5398175037817836		[learning rate: 0.0076623]
	Learning Rate: 0.0076623
	LOSS [training: 3.5398175037817836 | validation: 3.566629521252569]
	TIME [epoch: 7.85 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5107753471355627		[learning rate: 0.0076442]
	Learning Rate: 0.00764422
	LOSS [training: 3.5107753471355627 | validation: 3.6606430551260463]
	TIME [epoch: 7.85 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6284173883292246		[learning rate: 0.0076262]
	Learning Rate: 0.00762619
	LOSS [training: 3.6284173883292246 | validation: 3.7471796924698832]
	TIME [epoch: 7.88 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5686922754449633		[learning rate: 0.0076082]
	Learning Rate: 0.0076082
	LOSS [training: 3.5686922754449633 | validation: 3.5954386577767963]
	TIME [epoch: 7.88 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.542578838395415		[learning rate: 0.0075903]
	Learning Rate: 0.00759025
	LOSS [training: 3.542578838395415 | validation: 3.654660134341135]
	TIME [epoch: 7.86 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.563915733106249		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 3.563915733106249 | validation: 3.669398959411096]
	TIME [epoch: 7.86 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5564313789547213		[learning rate: 0.0075545]
	Learning Rate: 0.00755449
	LOSS [training: 3.5564313789547213 | validation: 3.6314756495136384]
	TIME [epoch: 7.86 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5424672651351288		[learning rate: 0.0075367]
	Learning Rate: 0.00753667
	LOSS [training: 3.5424672651351288 | validation: 3.605067054168334]
	TIME [epoch: 7.91 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4898980451746278		[learning rate: 0.0075189]
	Learning Rate: 0.00751889
	LOSS [training: 3.4898980451746278 | validation: 3.5130550701608887]
	TIME [epoch: 7.87 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4720204989496715		[learning rate: 0.0075012]
	Learning Rate: 0.00750116
	LOSS [training: 3.4720204989496715 | validation: 3.544969020135224]
	TIME [epoch: 7.85 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.53095888129267		[learning rate: 0.0074835]
	Learning Rate: 0.00748346
	LOSS [training: 3.53095888129267 | validation: 3.486306551225523]
	TIME [epoch: 7.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5023749374142428		[learning rate: 0.0074658]
	Learning Rate: 0.00746581
	LOSS [training: 3.5023749374142428 | validation: 3.4776370358377604]
	TIME [epoch: 7.85 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4830340240169138		[learning rate: 0.0074482]
	Learning Rate: 0.0074482
	LOSS [training: 3.4830340240169138 | validation: 3.479231785356893]
	TIME [epoch: 7.9 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4697245883129084		[learning rate: 0.0074306]
	Learning Rate: 0.00743063
	LOSS [training: 3.4697245883129084 | validation: 3.430381821468833]
	TIME [epoch: 7.86 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4263499371221897		[learning rate: 0.0074131]
	Learning Rate: 0.0074131
	LOSS [training: 3.4263499371221897 | validation: 3.4549301762322644]
	TIME [epoch: 7.85 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4222949796179347		[learning rate: 0.0073956]
	Learning Rate: 0.00739562
	LOSS [training: 3.4222949796179347 | validation: 3.382725337439147]
	TIME [epoch: 7.85 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3750191357056334		[learning rate: 0.0073782]
	Learning Rate: 0.00737817
	LOSS [training: 3.3750191357056334 | validation: 3.3758301987024675]
	TIME [epoch: 7.86 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.374062955248884		[learning rate: 0.0073608]
	Learning Rate: 0.00736077
	LOSS [training: 3.374062955248884 | validation: 3.4119558270106616]
	TIME [epoch: 7.9 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.384691920112786		[learning rate: 0.0073434]
	Learning Rate: 0.0073434
	LOSS [training: 3.384691920112786 | validation: 3.399092871548924]
	TIME [epoch: 7.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.420346696550968		[learning rate: 0.0073261]
	Learning Rate: 0.00732608
	LOSS [training: 3.420346696550968 | validation: 3.4243412204726473]
	TIME [epoch: 7.85 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.391888543782541		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 3.391888543782541 | validation: 3.3799458574788384]
	TIME [epoch: 7.85 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3668499278330417		[learning rate: 0.0072916]
	Learning Rate: 0.00729156
	LOSS [training: 3.3668499278330417 | validation: 3.3742004199694593]
	TIME [epoch: 7.88 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367155335216515		[learning rate: 0.0072744]
	Learning Rate: 0.00727436
	LOSS [training: 3.367155335216515 | validation: 3.3709388068423207]
	TIME [epoch: 7.88 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4042510571268685		[learning rate: 0.0072572]
	Learning Rate: 0.0072572
	LOSS [training: 3.4042510571268685 | validation: 3.4300697461049303]
	TIME [epoch: 7.86 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4253757675108383		[learning rate: 0.0072401]
	Learning Rate: 0.00724008
	LOSS [training: 3.4253757675108383 | validation: 3.40556549747875]
	TIME [epoch: 7.86 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3724097784119165		[learning rate: 0.007223]
	Learning Rate: 0.007223
	LOSS [training: 3.3724097784119165 | validation: 3.38935855514832]
	TIME [epoch: 7.85 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3971823685508244		[learning rate: 0.007206]
	Learning Rate: 0.00720597
	LOSS [training: 3.3971823685508244 | validation: 3.433540739969012]
	TIME [epoch: 7.9 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.435837138419166		[learning rate: 0.007189]
	Learning Rate: 0.00718897
	LOSS [training: 3.435837138419166 | validation: 3.405635898200179]
	TIME [epoch: 7.85 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410808965981274		[learning rate: 0.007172]
	Learning Rate: 0.00717201
	LOSS [training: 3.410808965981274 | validation: 3.402673871731526]
	TIME [epoch: 7.85 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3962180734416108		[learning rate: 0.0071551]
	Learning Rate: 0.00715509
	LOSS [training: 3.3962180734416108 | validation: 3.453321802173642]
	TIME [epoch: 7.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.450237004010649		[learning rate: 0.0071382]
	Learning Rate: 0.00713822
	LOSS [training: 3.450237004010649 | validation: 3.4477997427738094]
	TIME [epoch: 7.86 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3998035597263723		[learning rate: 0.0071214]
	Learning Rate: 0.00712138
	LOSS [training: 3.3998035597263723 | validation: 3.3873971726172307]
	TIME [epoch: 7.91 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3636368904772564		[learning rate: 0.0071046]
	Learning Rate: 0.00710458
	LOSS [training: 3.3636368904772564 | validation: 3.365515098521495]
	TIME [epoch: 7.85 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3736929996289815		[learning rate: 0.0070878]
	Learning Rate: 0.00708782
	LOSS [training: 3.3736929996289815 | validation: 3.4781595102939247]
	TIME [epoch: 7.86 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4283760698798673		[learning rate: 0.0070711]
	Learning Rate: 0.0070711
	LOSS [training: 3.4283760698798673 | validation: 3.3721915004754037]
	TIME [epoch: 7.86 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3989788651833948		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 3.3989788651833948 | validation: 3.393229099919539]
	TIME [epoch: 7.86 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3662000077236645		[learning rate: 0.0070378]
	Learning Rate: 0.00703778
	LOSS [training: 3.3662000077236645 | validation: 3.3560128548494896]
	TIME [epoch: 7.9 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354287185952491		[learning rate: 0.0070212]
	Learning Rate: 0.00702118
	LOSS [training: 3.354287185952491 | validation: 3.377176356085425]
	TIME [epoch: 7.86 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3672327884535336		[learning rate: 0.0070046]
	Learning Rate: 0.00700462
	LOSS [training: 3.3672327884535336 | validation: 3.35932327296364]
	TIME [epoch: 7.85 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4644902552116505		[learning rate: 0.0069881]
	Learning Rate: 0.0069881
	LOSS [training: 3.4644902552116505 | validation: 3.486416699023941]
	TIME [epoch: 7.85 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4380550193504122		[learning rate: 0.0069716]
	Learning Rate: 0.00697161
	LOSS [training: 3.4380550193504122 | validation: 3.3731885631567358]
	TIME [epoch: 7.88 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5007071576764703		[learning rate: 0.0069552]
	Learning Rate: 0.00695517
	LOSS [training: 3.5007071576764703 | validation: 3.610838308589896]
	TIME [epoch: 7.89 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6500256788594827		[learning rate: 0.0069388]
	Learning Rate: 0.00693876
	LOSS [training: 3.6500256788594827 | validation: 3.6188567536207614]
	TIME [epoch: 7.85 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.649428945088103		[learning rate: 0.0069224]
	Learning Rate: 0.00692239
	LOSS [training: 3.649428945088103 | validation: 3.595286250482733]
	TIME [epoch: 7.86 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6363827248904306		[learning rate: 0.0069061]
	Learning Rate: 0.00690607
	LOSS [training: 3.6363827248904306 | validation: 3.5854812751025307]
	TIME [epoch: 7.86 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6248246879033		[learning rate: 0.0068898]
	Learning Rate: 0.00688978
	LOSS [training: 3.6248246879033 | validation: 3.5606937040039828]
	TIME [epoch: 7.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6097445286921928		[learning rate: 0.0068735]
	Learning Rate: 0.00687352
	LOSS [training: 3.6097445286921928 | validation: 3.5439229070432514]
	TIME [epoch: 7.87 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6005814007105656		[learning rate: 0.0068573]
	Learning Rate: 0.00685731
	LOSS [training: 3.6005814007105656 | validation: 3.52502430227275]
	TIME [epoch: 7.85 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5951361398881425		[learning rate: 0.0068411]
	Learning Rate: 0.00684114
	LOSS [training: 3.5951361398881425 | validation: 3.5119350977784305]
	TIME [epoch: 7.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.586544235127909		[learning rate: 0.006825]
	Learning Rate: 0.006825
	LOSS [training: 3.586544235127909 | validation: 3.509473869296034]
	TIME [epoch: 7.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5813838611035655		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 3.5813838611035655 | validation: 3.507168271529605]
	TIME [epoch: 7.9 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5766172762088506		[learning rate: 0.0067928]
	Learning Rate: 0.00679284
	LOSS [training: 3.5766172762088506 | validation: 3.4972301242991954]
	TIME [epoch: 7.86 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5701834118418394		[learning rate: 0.0067768]
	Learning Rate: 0.00677681
	LOSS [training: 3.5701834118418394 | validation: 3.5013675759041254]
	TIME [epoch: 7.86 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5582610516567024		[learning rate: 0.0067608]
	Learning Rate: 0.00676083
	LOSS [training: 3.5582610516567024 | validation: 3.476844537383289]
	TIME [epoch: 7.86 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5365579019482984		[learning rate: 0.0067449]
	Learning Rate: 0.00674488
	LOSS [training: 3.5365579019482984 | validation: 3.46773621022362]
	TIME [epoch: 7.85 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.512444315160339		[learning rate: 0.006729]
	Learning Rate: 0.00672897
	LOSS [training: 3.512444315160339 | validation: 3.459224100725797]
	TIME [epoch: 7.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4915586795233295		[learning rate: 0.0067131]
	Learning Rate: 0.0067131
	LOSS [training: 3.4915586795233295 | validation: 3.5246989641257036]
	TIME [epoch: 7.85 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4922086482529466		[learning rate: 0.0066973]
	Learning Rate: 0.00669726
	LOSS [training: 3.4922086482529466 | validation: 3.4990242538515703]
	TIME [epoch: 7.86 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4498822412632557		[learning rate: 0.0066815]
	Learning Rate: 0.00668147
	LOSS [training: 3.4498822412632557 | validation: 3.4277985209045183]
	TIME [epoch: 7.85 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3978675236884897		[learning rate: 0.0066657]
	Learning Rate: 0.00666571
	LOSS [training: 3.3978675236884897 | validation: 3.3968251635794156]
	TIME [epoch: 7.87 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375838628253073		[learning rate: 0.00665]
	Learning Rate: 0.00664998
	LOSS [training: 3.375838628253073 | validation: 3.3807832250827694]
	TIME [epoch: 7.89 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363006403898391		[learning rate: 0.0066343]
	Learning Rate: 0.0066343
	LOSS [training: 3.363006403898391 | validation: 3.3808925108314734]
	TIME [epoch: 7.86 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.364108979446604		[learning rate: 0.0066186]
	Learning Rate: 0.00661865
	LOSS [training: 3.364108979446604 | validation: 3.4056858537476162]
	TIME [epoch: 7.85 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.408830520207424		[learning rate: 0.006603]
	Learning Rate: 0.00660304
	LOSS [training: 3.408830520207424 | validation: 3.4206620770668876]
	TIME [epoch: 7.85 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3805210904645184		[learning rate: 0.0065875]
	Learning Rate: 0.00658746
	LOSS [training: 3.3805210904645184 | validation: 3.387172652427125]
	TIME [epoch: 7.9 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.358690457609292		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 3.358690457609292 | validation: 3.3717829640746415]
	TIME [epoch: 7.87 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353501937405723		[learning rate: 0.0065564]
	Learning Rate: 0.00655642
	LOSS [training: 3.353501937405723 | validation: 3.367004072604881]
	TIME [epoch: 7.86 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3509502136949787		[learning rate: 0.006541]
	Learning Rate: 0.00654095
	LOSS [training: 3.3509502136949787 | validation: 3.358471570758859]
	TIME [epoch: 7.85 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3481630809209806		[learning rate: 0.0065255]
	Learning Rate: 0.00652552
	LOSS [training: 3.3481630809209806 | validation: 3.365981143968878]
	TIME [epoch: 7.86 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3449460000884317		[learning rate: 0.0065101]
	Learning Rate: 0.00651013
	LOSS [training: 3.3449460000884317 | validation: 3.3584856371514142]
	TIME [epoch: 7.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3609702634073035		[learning rate: 0.0064948]
	Learning Rate: 0.00649477
	LOSS [training: 3.3609702634073035 | validation: 3.3670779799592525]
	TIME [epoch: 7.86 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3489468377898715		[learning rate: 0.0064795]
	Learning Rate: 0.00647945
	LOSS [training: 3.3489468377898715 | validation: 3.350361851745701]
	TIME [epoch: 7.86 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34619212620392		[learning rate: 0.0064642]
	Learning Rate: 0.00646417
	LOSS [training: 3.34619212620392 | validation: 3.3611480895908064]
	TIME [epoch: 7.87 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3593204435021757		[learning rate: 0.0064489]
	Learning Rate: 0.00644892
	LOSS [training: 3.3593204435021757 | validation: 3.3820767565640297]
	TIME [epoch: 7.86 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4128766610745354		[learning rate: 0.0064337]
	Learning Rate: 0.00643371
	LOSS [training: 3.4128766610745354 | validation: 3.4275388418662884]
	TIME [epoch: 7.9 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4074229704958356		[learning rate: 0.0064185]
	Learning Rate: 0.00641853
	LOSS [training: 3.4074229704958356 | validation: 3.368938957740699]
	TIME [epoch: 7.86 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3665272752947235		[learning rate: 0.0064034]
	Learning Rate: 0.00640339
	LOSS [training: 3.3665272752947235 | validation: 3.40152993888887]
	TIME [epoch: 7.86 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3894177893949493		[learning rate: 0.0063883]
	Learning Rate: 0.00638829
	LOSS [training: 3.3894177893949493 | validation: 3.3702788611537184]
	TIME [epoch: 7.85 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3495221460101128		[learning rate: 0.0063732]
	Learning Rate: 0.00637322
	LOSS [training: 3.3495221460101128 | validation: 3.3601506426159045]
	TIME [epoch: 7.87 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3463339129971317		[learning rate: 0.0063582]
	Learning Rate: 0.00635819
	LOSS [training: 3.3463339129971317 | validation: 3.3744302728740925]
	TIME [epoch: 7.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.357548161821988		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 3.357548161821988 | validation: 3.3675499398164446]
	TIME [epoch: 7.85 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3498051431014115		[learning rate: 0.0063282]
	Learning Rate: 0.00632823
	LOSS [training: 3.3498051431014115 | validation: 3.344646378803515]
	TIME [epoch: 7.86 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3326399896692265		[learning rate: 0.0063133]
	Learning Rate: 0.0063133
	LOSS [training: 3.3326399896692265 | validation: 3.3533284043550706]
	TIME [epoch: 7.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.372076577983097		[learning rate: 0.0062984]
	Learning Rate: 0.00629841
	LOSS [training: 3.372076577983097 | validation: 3.3756447919484662]
	TIME [epoch: 7.9 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.349277420140744		[learning rate: 0.0062836]
	Learning Rate: 0.00628355
	LOSS [training: 3.349277420140744 | validation: 3.3426771296319844]
	TIME [epoch: 7.87 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34070724755899		[learning rate: 0.0062687]
	Learning Rate: 0.00626873
	LOSS [training: 3.34070724755899 | validation: 3.3599877373657723]
	TIME [epoch: 7.86 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3560966400147603		[learning rate: 0.0062539]
	Learning Rate: 0.00625394
	LOSS [training: 3.3560966400147603 | validation: 3.367348210342912]
	TIME [epoch: 7.86 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353923475944382		[learning rate: 0.0062392]
	Learning Rate: 0.00623919
	LOSS [training: 3.353923475944382 | validation: 3.3404444017098194]
	TIME [epoch: 7.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3307112979275706		[learning rate: 0.0062245]
	Learning Rate: 0.00622447
	LOSS [training: 3.3307112979275706 | validation: 3.3344329607135776]
	TIME [epoch: 7.91 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.368572120457185		[learning rate: 0.0062098]
	Learning Rate: 0.00620979
	LOSS [training: 3.368572120457185 | validation: 3.401500010561131]
	TIME [epoch: 7.87 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.412358247241333		[learning rate: 0.0061951]
	Learning Rate: 0.00619514
	LOSS [training: 3.412358247241333 | validation: 3.359977916294719]
	TIME [epoch: 7.86 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3448360407821847		[learning rate: 0.0061805]
	Learning Rate: 0.00618053
	LOSS [training: 3.3448360407821847 | validation: 3.3981521805940056]
	TIME [epoch: 7.86 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3656310041092867		[learning rate: 0.0061659]
	Learning Rate: 0.00616595
	LOSS [training: 3.3656310041092867 | validation: 3.396108613747849]
	TIME [epoch: 7.86 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3677136878793474		[learning rate: 0.0061514]
	Learning Rate: 0.00615141
	LOSS [training: 3.3677136878793474 | validation: 3.367833585922959]
	TIME [epoch: 7.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3418480393606633		[learning rate: 0.0061369]
	Learning Rate: 0.0061369
	LOSS [training: 3.3418480393606633 | validation: 3.3538305839613205]
	TIME [epoch: 7.86 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33371630309728		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 3.33371630309728 | validation: 3.360717567309885]
	TIME [epoch: 7.86 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3305839143227782		[learning rate: 0.006108]
	Learning Rate: 0.00610798
	LOSS [training: 3.3305839143227782 | validation: 3.3357846556452637]
	TIME [epoch: 7.85 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3226485869749762		[learning rate: 0.0060936]
	Learning Rate: 0.00609357
	LOSS [training: 3.3226485869749762 | validation: 3.333598105547029]
	TIME [epoch: 7.86 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3350321086717765		[learning rate: 0.0060792]
	Learning Rate: 0.0060792
	LOSS [training: 3.3350321086717765 | validation: 3.336290603845832]
	TIME [epoch: 7.9 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321917050095552		[learning rate: 0.0060649]
	Learning Rate: 0.00606486
	LOSS [training: 3.321917050095552 | validation: 3.339813026447259]
	TIME [epoch: 7.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3283773440753226		[learning rate: 0.0060505]
	Learning Rate: 0.00605055
	LOSS [training: 3.3283773440753226 | validation: 3.347747451935568]
	TIME [epoch: 7.85 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3350655379059857		[learning rate: 0.0060363]
	Learning Rate: 0.00603628
	LOSS [training: 3.3350655379059857 | validation: 3.3367705124248412]
	TIME [epoch: 7.85 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324942959843849		[learning rate: 0.006022]
	Learning Rate: 0.00602204
	LOSS [training: 3.324942959843849 | validation: 3.331853481772039]
	TIME [epoch: 7.91 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3209696666334905		[learning rate: 0.0060078]
	Learning Rate: 0.00600783
	LOSS [training: 3.3209696666334905 | validation: 3.357123314158514]
	TIME [epoch: 7.86 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336580721677289		[learning rate: 0.0059937]
	Learning Rate: 0.00599366
	LOSS [training: 3.336580721677289 | validation: 3.3683918533500616]
	TIME [epoch: 7.85 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340975223428145		[learning rate: 0.0059795]
	Learning Rate: 0.00597952
	LOSS [training: 3.340975223428145 | validation: 3.376079414617492]
	TIME [epoch: 7.86 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.362580645773456		[learning rate: 0.0059654]
	Learning Rate: 0.00596542
	LOSS [training: 3.362580645773456 | validation: 3.358639110919551]
	TIME [epoch: 7.85 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.356428929874366		[learning rate: 0.0059513]
	Learning Rate: 0.00595135
	LOSS [training: 3.356428929874366 | validation: 3.3472027929303136]
	TIME [epoch: 7.9 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3392951796803567		[learning rate: 0.0059373]
	Learning Rate: 0.00593731
	LOSS [training: 3.3392951796803567 | validation: 3.3731793334301683]
	TIME [epoch: 7.87 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3435758108351483		[learning rate: 0.0059233]
	Learning Rate: 0.00592331
	LOSS [training: 3.3435758108351483 | validation: 3.3754630827309917]
	TIME [epoch: 7.85 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3468134163763623		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 3.3468134163763623 | validation: 3.3682516595545593]
	TIME [epoch: 7.86 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3341817609022857		[learning rate: 0.0058954]
	Learning Rate: 0.00589539
	LOSS [training: 3.3341817609022857 | validation: 3.350345259635331]
	TIME [epoch: 7.86 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342603521864224		[learning rate: 0.0058815]
	Learning Rate: 0.00588149
	LOSS [training: 3.342603521864224 | validation: 3.3753239946098423]
	TIME [epoch: 7.89 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3414448633283764		[learning rate: 0.0058676]
	Learning Rate: 0.00586761
	LOSS [training: 3.3414448633283764 | validation: 3.366803590852668]
	TIME [epoch: 7.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.492578091915349		[learning rate: 0.0058538]
	Learning Rate: 0.00585377
	LOSS [training: 3.492578091915349 | validation: 3.673913810154777]
	TIME [epoch: 7.85 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8462620087688726		[learning rate: 0.00584]
	Learning Rate: 0.00583996
	LOSS [training: 3.8462620087688726 | validation: 3.8941846226029186]
	TIME [epoch: 7.85 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780760357698772		[learning rate: 0.0058262]
	Learning Rate: 0.00582619
	LOSS [training: 3.780760357698772 | validation: 3.6133387303894704]
	TIME [epoch: 7.86 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5970079144284277		[learning rate: 0.0058124]
	Learning Rate: 0.00581245
	LOSS [training: 3.5970079144284277 | validation: 3.6515585488927558]
	TIME [epoch: 7.89 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852322078569506		[learning rate: 0.0057987]
	Learning Rate: 0.00579874
	LOSS [training: 3.852322078569506 | validation: 3.908265516243277]
	TIME [epoch: 7.85 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8321443939168436		[learning rate: 0.0057851]
	Learning Rate: 0.00578506
	LOSS [training: 3.8321443939168436 | validation: 3.982806330184207]
	TIME [epoch: 7.85 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.262189943312907		[learning rate: 0.0057714]
	Learning Rate: 0.00577141
	LOSS [training: 4.262189943312907 | validation: 4.550952562198536]
	TIME [epoch: 7.86 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.68261398536638		[learning rate: 0.0057578]
	Learning Rate: 0.0057578
	LOSS [training: 4.68261398536638 | validation: 4.576967793541834]
	TIME [epoch: 7.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.686063250987613		[learning rate: 0.0057442]
	Learning Rate: 0.00574422
	LOSS [training: 4.686063250987613 | validation: 4.664655305730166]
	TIME [epoch: 7.87 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.768476168197923		[learning rate: 0.0057307]
	Learning Rate: 0.00573067
	LOSS [training: 4.768476168197923 | validation: 4.852218023345683]
	TIME [epoch: 7.85 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.905409325421236		[learning rate: 0.0057171]
	Learning Rate: 0.00571715
	LOSS [training: 4.905409325421236 | validation: 4.816729176417542]
	TIME [epoch: 7.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.987553754823225		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 4.987553754823225 | validation: 5.304685869385965]
	TIME [epoch: 7.86 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.703098245493345		[learning rate: 0.0056902]
	Learning Rate: 0.00569021
	LOSS [training: 5.703098245493345 | validation: 5.9294186198512975]
	TIME [epoch: 7.9 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.017202592519088		[learning rate: 0.0056768]
	Learning Rate: 0.00567679
	LOSS [training: 6.017202592519088 | validation: 6.045689141750263]
	TIME [epoch: 7.87 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.041467973619934		[learning rate: 0.0056634]
	Learning Rate: 0.0056634
	LOSS [training: 6.041467973619934 | validation: 6.011859911596494]
	TIME [epoch: 7.85 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.001249639932008		[learning rate: 0.00565]
	Learning Rate: 0.00565004
	LOSS [training: 6.001249639932008 | validation: 5.9677257531717025]
	TIME [epoch: 7.86 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.883340487473177		[learning rate: 0.0056367]
	Learning Rate: 0.00563671
	LOSS [training: 5.883340487473177 | validation: 5.82674919721553]
	TIME [epoch: 7.86 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.7919023892626615		[learning rate: 0.0056234]
	Learning Rate: 0.00562341
	LOSS [training: 5.7919023892626615 | validation: 5.691670980980144]
	TIME [epoch: 7.9 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.655603065034886		[learning rate: 0.0056101]
	Learning Rate: 0.00561015
	LOSS [training: 5.655603065034886 | validation: 5.557395439392407]
	TIME [epoch: 7.86 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.539754952039953		[learning rate: 0.0055969]
	Learning Rate: 0.00559691
	LOSS [training: 5.539754952039953 | validation: 5.465661227226744]
	TIME [epoch: 7.85 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.43663944486584		[learning rate: 0.0055837]
	Learning Rate: 0.00558371
	LOSS [training: 5.43663944486584 | validation: 5.338059731990914]
	TIME [epoch: 7.86 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.284925072449992		[learning rate: 0.0055705]
	Learning Rate: 0.00557054
	LOSS [training: 5.284925072449992 | validation: 5.149490221011637]
	TIME [epoch: 7.87 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.068315063867667		[learning rate: 0.0055574]
	Learning Rate: 0.0055574
	LOSS [training: 5.068315063867667 | validation: 4.723453666632329]
	TIME [epoch: 7.9 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.477675912746246		[learning rate: 0.0055443]
	Learning Rate: 0.00554429
	LOSS [training: 4.477675912746246 | validation: 4.269303547081211]
	TIME [epoch: 7.86 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.207951056533784		[learning rate: 0.0055312]
	Learning Rate: 0.00553121
	LOSS [training: 4.207951056533784 | validation: 4.14876718546583]
	TIME [epoch: 7.85 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9886208914394095		[learning rate: 0.0055182]
	Learning Rate: 0.00551817
	LOSS [training: 3.9886208914394095 | validation: 3.905882296530679]
	TIME [epoch: 7.86 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.828384447729388		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 3.828384447729388 | validation: 3.8098772757601087]
	TIME [epoch: 7.88 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8037408143508205		[learning rate: 0.0054922]
	Learning Rate: 0.00549216
	LOSS [training: 3.8037408143508205 | validation: 3.7113250523880343]
	TIME [epoch: 7.88 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.708937541424726		[learning rate: 0.0054792]
	Learning Rate: 0.00547921
	LOSS [training: 3.708937541424726 | validation: 3.6440566684809133]
	TIME [epoch: 7.86 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6365543323566305		[learning rate: 0.0054663]
	Learning Rate: 0.00546629
	LOSS [training: 3.6365543323566305 | validation: 3.6008306303783675]
	TIME [epoch: 7.85 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5790866697631447		[learning rate: 0.0054534]
	Learning Rate: 0.00545339
	LOSS [training: 3.5790866697631447 | validation: 3.5581256937479413]
	TIME [epoch: 7.85 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5424021983651786		[learning rate: 0.0054405]
	Learning Rate: 0.00544053
	LOSS [training: 3.5424021983651786 | validation: 3.529654159968464]
	TIME [epoch: 7.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5176753157461946		[learning rate: 0.0054277]
	Learning Rate: 0.00542769
	LOSS [training: 3.5176753157461946 | validation: 3.5135616253027857]
	TIME [epoch: 7.86 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.500649945501199		[learning rate: 0.0054149]
	Learning Rate: 0.00541489
	LOSS [training: 3.500649945501199 | validation: 3.501111857111111]
	TIME [epoch: 7.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4885459594094876		[learning rate: 0.0054021]
	Learning Rate: 0.00540212
	LOSS [training: 3.4885459594094876 | validation: 3.493244933744684]
	TIME [epoch: 7.86 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4769810665430883		[learning rate: 0.0053894]
	Learning Rate: 0.00538938
	LOSS [training: 3.4769810665430883 | validation: 3.484820225511425]
	TIME [epoch: 7.86 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4686701322436977		[learning rate: 0.0053767]
	Learning Rate: 0.00537666
	LOSS [training: 3.4686701322436977 | validation: 3.477956612193289]
	TIME [epoch: 7.91 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4659882679808205		[learning rate: 0.005364]
	Learning Rate: 0.00536398
	LOSS [training: 3.4659882679808205 | validation: 3.4754824907299797]
	TIME [epoch: 7.85 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5965562188236495		[learning rate: 0.0053513]
	Learning Rate: 0.00535133
	LOSS [training: 3.5965562188236495 | validation: 3.7890964122267947]
	TIME [epoch: 7.86 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6852723113504626		[learning rate: 0.0053387]
	Learning Rate: 0.00533871
	LOSS [training: 3.6852723113504626 | validation: 3.5246157776162685]
	TIME [epoch: 7.86 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4922202481547595		[learning rate: 0.0053261]
	Learning Rate: 0.00532611
	LOSS [training: 3.4922202481547595 | validation: 3.4827364750868153]
	TIME [epoch: 7.87 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4713119435480397		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 3.4713119435480397 | validation: 3.4751846762423506]
	TIME [epoch: 8.15 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.460646790701697		[learning rate: 0.005301]
	Learning Rate: 0.00530101
	LOSS [training: 3.460646790701697 | validation: 3.468409136367807]
	TIME [epoch: 7.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.455073971563289		[learning rate: 0.0052885]
	Learning Rate: 0.00528851
	LOSS [training: 3.455073971563289 | validation: 3.462753129871647]
	TIME [epoch: 7.86 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4516668197547817		[learning rate: 0.005276]
	Learning Rate: 0.00527604
	LOSS [training: 3.4516668197547817 | validation: 3.4579189019341063]
	TIME [epoch: 7.86 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4447528597620796		[learning rate: 0.0052636]
	Learning Rate: 0.00526359
	LOSS [training: 3.4447528597620796 | validation: 3.450954856044112]
	TIME [epoch: 7.88 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.436950985569293		[learning rate: 0.0052512]
	Learning Rate: 0.00525117
	LOSS [training: 3.436950985569293 | validation: 3.4438760144887253]
	TIME [epoch: 7.88 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4311754112882324		[learning rate: 0.0052388]
	Learning Rate: 0.00523879
	LOSS [training: 3.4311754112882324 | validation: 3.4383535408746173]
	TIME [epoch: 7.87 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426982758863655		[learning rate: 0.0052264]
	Learning Rate: 0.00522643
	LOSS [training: 3.426982758863655 | validation: 3.4304627623773083]
	TIME [epoch: 7.86 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.417124619709832		[learning rate: 0.0052141]
	Learning Rate: 0.0052141
	LOSS [training: 3.417124619709832 | validation: 3.432723575234684]
	TIME [epoch: 7.86 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.416836191120919		[learning rate: 0.0052018]
	Learning Rate: 0.0052018
	LOSS [training: 3.416836191120919 | validation: 3.4210109548052143]
	TIME [epoch: 7.9 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.404749170201787		[learning rate: 0.0051895]
	Learning Rate: 0.00518953
	LOSS [training: 3.404749170201787 | validation: 3.4146654954185305]
	TIME [epoch: 7.86 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402987452220278		[learning rate: 0.0051773]
	Learning Rate: 0.00517729
	LOSS [training: 3.402987452220278 | validation: 3.4111023508972793]
	TIME [epoch: 7.86 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.395959873139634		[learning rate: 0.0051651]
	Learning Rate: 0.00516508
	LOSS [training: 3.395959873139634 | validation: 3.404966923151816]
	TIME [epoch: 7.86 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3959970545421623		[learning rate: 0.0051529]
	Learning Rate: 0.00515289
	LOSS [training: 3.3959970545421623 | validation: 3.402542029372107]
	TIME [epoch: 7.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3974953108690733		[learning rate: 0.0051407]
	Learning Rate: 0.00514074
	LOSS [training: 3.3974953108690733 | validation: 3.4367289424909777]
	TIME [epoch: 7.9 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3982413036259724		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 3.3982413036259724 | validation: 3.39968938659615]
	TIME [epoch: 7.86 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.380199365008192		[learning rate: 0.0051165]
	Learning Rate: 0.00511652
	LOSS [training: 3.380199365008192 | validation: 3.3911317369994824]
	TIME [epoch: 7.85 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.374381524480313		[learning rate: 0.0051044]
	Learning Rate: 0.00510445
	LOSS [training: 3.374381524480313 | validation: 3.390566869296597]
	TIME [epoch: 7.85 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.370056957499921		[learning rate: 0.0050924]
	Learning Rate: 0.00509241
	LOSS [training: 3.370056957499921 | validation: 3.4130528563766336]
	TIME [epoch: 7.86 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3887167619451226		[learning rate: 0.0050804]
	Learning Rate: 0.00508039
	LOSS [training: 3.3887167619451226 | validation: 3.382167998353172]
	TIME [epoch: 7.89 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.364938634592159		[learning rate: 0.0050684]
	Learning Rate: 0.00506841
	LOSS [training: 3.364938634592159 | validation: 3.391517892204159]
	TIME [epoch: 7.85 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3716571497082466		[learning rate: 0.0050565]
	Learning Rate: 0.00505645
	LOSS [training: 3.3716571497082466 | validation: 3.376928906562917]
	TIME [epoch: 7.86 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.365768745535966		[learning rate: 0.0050445]
	Learning Rate: 0.00504453
	LOSS [training: 3.365768745535966 | validation: 3.3751575517440964]
	TIME [epoch: 7.86 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3661215340251625		[learning rate: 0.0050326]
	Learning Rate: 0.00503263
	LOSS [training: 3.3661215340251625 | validation: 3.3714375191249184]
	TIME [epoch: 7.88 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3490526559272253		[learning rate: 0.0050208]
	Learning Rate: 0.00502076
	LOSS [training: 3.3490526559272253 | validation: 3.3691714989204353]
	TIME [epoch: 7.87 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351813425395349		[learning rate: 0.0050089]
	Learning Rate: 0.00500891
	LOSS [training: 3.351813425395349 | validation: 3.3807235243027227]
	TIME [epoch: 7.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3445862389906824		[learning rate: 0.0049971]
	Learning Rate: 0.0049971
	LOSS [training: 3.3445862389906824 | validation: 3.345510613035117]
	TIME [epoch: 7.85 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3673561912477874		[learning rate: 0.0049853]
	Learning Rate: 0.00498531
	LOSS [training: 3.3673561912477874 | validation: 3.3406279950094455]
	TIME [epoch: 7.85 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3239815198564733		[learning rate: 0.0049736]
	Learning Rate: 0.00497355
	LOSS [training: 3.3239815198564733 | validation: 3.3226478637857593]
	TIME [epoch: 7.89 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.307857342132414		[learning rate: 0.0049618]
	Learning Rate: 0.00496182
	LOSS [training: 3.307857342132414 | validation: 3.389007551576432]
	TIME [epoch: 7.86 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340713886935033		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 3.340713886935033 | validation: 3.3320974347500565]
	TIME [epoch: 7.85 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.349225403496991		[learning rate: 0.0049384]
	Learning Rate: 0.00493844
	LOSS [training: 3.349225403496991 | validation: 3.3369833056139773]
	TIME [epoch: 7.85 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.300576221697987		[learning rate: 0.0049268]
	Learning Rate: 0.00492679
	LOSS [training: 3.300576221697987 | validation: 3.3066220328721103]
	TIME [epoch: 7.85 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2866533480445614		[learning rate: 0.0049152]
	Learning Rate: 0.00491517
	LOSS [training: 3.2866533480445614 | validation: 3.317540053982393]
	TIME [epoch: 7.89 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327895659425145		[learning rate: 0.0049036]
	Learning Rate: 0.00490357
	LOSS [training: 3.327895659425145 | validation: 3.313054527904474]
	TIME [epoch: 7.85 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2848370039113624		[learning rate: 0.004892]
	Learning Rate: 0.00489201
	LOSS [training: 3.2848370039113624 | validation: 3.3039626154729405]
	TIME [epoch: 7.85 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325075500870002		[learning rate: 0.0048805]
	Learning Rate: 0.00488047
	LOSS [training: 3.325075500870002 | validation: 3.3029759457956542]
	TIME [epoch: 7.85 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2791041857716734		[learning rate: 0.004869]
	Learning Rate: 0.00486896
	LOSS [training: 3.2791041857716734 | validation: 3.2935884934595716]
	TIME [epoch: 7.87 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.277645304024953		[learning rate: 0.0048575]
	Learning Rate: 0.00485747
	LOSS [training: 3.277645304024953 | validation: 3.3026386839244113]
	TIME [epoch: 7.89 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3037657134683758		[learning rate: 0.004846]
	Learning Rate: 0.00484601
	LOSS [training: 3.3037657134683758 | validation: 3.365592529133667]
	TIME [epoch: 7.84 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7130530029865447		[learning rate: 0.0048346]
	Learning Rate: 0.00483458
	LOSS [training: 3.7130530029865447 | validation: 4.818767987403801]
	TIME [epoch: 7.85 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0581654182463485		[learning rate: 0.0048232]
	Learning Rate: 0.00482318
	LOSS [training: 5.0581654182463485 | validation: 5.043848339855041]
	TIME [epoch: 7.86 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.429966472699584		[learning rate: 0.0048118]
	Learning Rate: 0.0048118
	LOSS [training: 4.429966472699584 | validation: 3.9988729029639596]
	TIME [epoch: 7.88 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.287668365988976		[learning rate: 0.0048005]
	Learning Rate: 0.00480045
	LOSS [training: 4.287668365988976 | validation: 4.997256888870593]
	TIME [epoch: 7.87 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0985831842274445		[learning rate: 0.0047891]
	Learning Rate: 0.00478913
	LOSS [training: 5.0985831842274445 | validation: 5.217041027301773]
	TIME [epoch: 7.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.190839210302324		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 5.190839210302324 | validation: 5.202565803826519]
	TIME [epoch: 7.85 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.300075107901361		[learning rate: 0.0047666]
	Learning Rate: 0.00476656
	LOSS [training: 5.300075107901361 | validation: 5.360558349046858]
	TIME [epoch: 7.85 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.495093637837984		[learning rate: 0.0047553]
	Learning Rate: 0.00475532
	LOSS [training: 5.495093637837984 | validation: 5.4369638580212225]
	TIME [epoch: 7.88 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.586973769820136		[learning rate: 0.0047441]
	Learning Rate: 0.0047441
	LOSS [training: 5.586973769820136 | validation: 5.625967729047839]
	TIME [epoch: 7.85 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.59748186157241		[learning rate: 0.0047329]
	Learning Rate: 0.00473291
	LOSS [training: 5.59748186157241 | validation: 5.3884431907587125]
	TIME [epoch: 7.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.586429461411414		[learning rate: 0.0047217]
	Learning Rate: 0.00472175
	LOSS [training: 5.586429461411414 | validation: 5.634019191655796]
	TIME [epoch: 7.85 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.74407788652683		[learning rate: 0.0047106]
	Learning Rate: 0.00471061
	LOSS [training: 5.74407788652683 | validation: 5.697575141483943]
	TIME [epoch: 7.84 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.669330055876558		[learning rate: 0.0046995]
	Learning Rate: 0.0046995
	LOSS [training: 5.669330055876558 | validation: 5.519685805625123]
	TIME [epoch: 7.89 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.32290842193709		[learning rate: 0.0046884]
	Learning Rate: 0.00468841
	LOSS [training: 5.32290842193709 | validation: 5.099914341479778]
	TIME [epoch: 7.86 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.18944234408163		[learning rate: 0.0046774]
	Learning Rate: 0.00467735
	LOSS [training: 5.18944234408163 | validation: 5.153783130477581]
	TIME [epoch: 7.85 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.010111399177529		[learning rate: 0.0046663]
	Learning Rate: 0.00466632
	LOSS [training: 5.010111399177529 | validation: 4.505605924965623]
	TIME [epoch: 7.85 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.579479331259179		[learning rate: 0.0046553]
	Learning Rate: 0.00465531
	LOSS [training: 4.579479331259179 | validation: 4.717453676952767]
	TIME [epoch: 7.87 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5423677592713325		[learning rate: 0.0046443]
	Learning Rate: 0.00464433
	LOSS [training: 4.5423677592713325 | validation: 5.040878858925666]
	TIME [epoch: 7.9 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.830229306607031		[learning rate: 0.0046334]
	Learning Rate: 0.00463338
	LOSS [training: 4.830229306607031 | validation: 5.129645585384027]
	TIME [epoch: 7.86 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.854064936148219		[learning rate: 0.0046224]
	Learning Rate: 0.00462245
	LOSS [training: 4.854064936148219 | validation: 5.28840689037918]
	TIME [epoch: 7.86 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.386848732645953		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 5.386848732645953 | validation: 5.5949040430715815]
	TIME [epoch: 7.85 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.560697074998211		[learning rate: 0.0046007]
	Learning Rate: 0.00460066
	LOSS [training: 5.560697074998211 | validation: 5.528723642601653]
	TIME [epoch: 7.88 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.043304840367346		[learning rate: 0.0045898]
	Learning Rate: 0.00458981
	LOSS [training: 5.043304840367346 | validation: 4.7905215092297855]
	TIME [epoch: 7.88 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.520067343061244		[learning rate: 0.004579]
	Learning Rate: 0.00457899
	LOSS [training: 4.520067343061244 | validation: 4.436634649577639]
	TIME [epoch: 7.85 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123330157774778		[learning rate: 0.0045682]
	Learning Rate: 0.00456818
	LOSS [training: 4.123330157774778 | validation: 4.599832781504761]
	TIME [epoch: 7.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.639599566330168		[learning rate: 0.0045574]
	Learning Rate: 0.00455741
	LOSS [training: 4.639599566330168 | validation: 4.737121600816826]
	TIME [epoch: 7.86 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.621150579123546		[learning rate: 0.0045467]
	Learning Rate: 0.00454666
	LOSS [training: 4.621150579123546 | validation: 5.1715369293478854]
	TIME [epoch: 7.9 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2608814565368		[learning rate: 0.0045359]
	Learning Rate: 0.00453593
	LOSS [training: 5.2608814565368 | validation: 5.215848671389995]
	TIME [epoch: 7.86 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.404780426711753		[learning rate: 0.0045252]
	Learning Rate: 0.00452523
	LOSS [training: 5.404780426711753 | validation: 5.42827948644468]
	TIME [epoch: 7.85 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.511218598905903		[learning rate: 0.0045146]
	Learning Rate: 0.00451456
	LOSS [training: 5.511218598905903 | validation: 5.381827815653923]
	TIME [epoch: 7.85 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.487765354010814		[learning rate: 0.0045039]
	Learning Rate: 0.00450391
	LOSS [training: 5.487765354010814 | validation: 5.170542702781544]
	TIME [epoch: 7.86 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.931087584503503		[learning rate: 0.0044933]
	Learning Rate: 0.00449329
	LOSS [training: 4.931087584503503 | validation: 4.442990004170994]
	TIME [epoch: 7.92 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.504690647501095		[learning rate: 0.0044827]
	Learning Rate: 0.00448269
	LOSS [training: 4.504690647501095 | validation: 4.747874208852586]
	TIME [epoch: 7.85 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.522924084338736		[learning rate: 0.0044721]
	Learning Rate: 0.00447211
	LOSS [training: 4.522924084338736 | validation: 4.621528264944965]
	TIME [epoch: 7.86 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.353105832508478		[learning rate: 0.0044616]
	Learning Rate: 0.00446156
	LOSS [training: 4.353105832508478 | validation: 4.575354533539924]
	TIME [epoch: 7.86 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.333331804253624		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 4.333331804253624 | validation: 4.435033065557712]
	TIME [epoch: 7.86 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.373725959761809		[learning rate: 0.0044405]
	Learning Rate: 0.00444054
	LOSS [training: 4.373725959761809 | validation: 4.432205607560425]
	TIME [epoch: 7.91 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.557527502296861		[learning rate: 0.0044301]
	Learning Rate: 0.00443007
	LOSS [training: 4.557527502296861 | validation: 4.817156364463683]
	TIME [epoch: 7.84 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.543308329942462		[learning rate: 0.0044196]
	Learning Rate: 0.00441962
	LOSS [training: 4.543308329942462 | validation: 4.498716619620936]
	TIME [epoch: 7.85 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.452740481177853		[learning rate: 0.0044092]
	Learning Rate: 0.00440919
	LOSS [training: 4.452740481177853 | validation: 4.445377477103233]
	TIME [epoch: 7.85 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.366604853416334		[learning rate: 0.0043988]
	Learning Rate: 0.00439879
	LOSS [training: 4.366604853416334 | validation: 4.356876711105496]
	TIME [epoch: 7.87 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.37016455231314		[learning rate: 0.0043884]
	Learning Rate: 0.00438841
	LOSS [training: 4.37016455231314 | validation: 4.302121486684141]
	TIME [epoch: 7.88 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.385564347026448		[learning rate: 0.0043781]
	Learning Rate: 0.00437806
	LOSS [training: 4.385564347026448 | validation: 4.4825531751888]
	TIME [epoch: 7.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.812438016671424		[learning rate: 0.0043677]
	Learning Rate: 0.00436774
	LOSS [training: 4.812438016671424 | validation: 4.889755019149421]
	TIME [epoch: 7.86 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.197881309136941		[learning rate: 0.0043574]
	Learning Rate: 0.00435743
	LOSS [training: 5.197881309136941 | validation: 5.156585598714871]
	TIME [epoch: 7.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.128165521263713		[learning rate: 0.0043472]
	Learning Rate: 0.00434715
	LOSS [training: 5.128165521263713 | validation: 4.727830426681983]
	TIME [epoch: 7.89 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837483492146095		[learning rate: 0.0043369]
	Learning Rate: 0.0043369
	LOSS [training: 4.837483492146095 | validation: 4.693288896377324]
	TIME [epoch: 7.85 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.780540183045076		[learning rate: 0.0043267]
	Learning Rate: 0.00432667
	LOSS [training: 4.780540183045076 | validation: 4.480500750503345]
	TIME [epoch: 7.85 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8075261299889736		[learning rate: 0.0043165]
	Learning Rate: 0.00431646
	LOSS [training: 4.8075261299889736 | validation: 5.1589165580767045]
	TIME [epoch: 7.86 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.711860914702208		[learning rate: 0.0043063]
	Learning Rate: 0.00430628
	LOSS [training: 5.711860914702208 | validation: 5.849420779735906]
	TIME [epoch: 7.85 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.794730678153882		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 5.794730678153882 | validation: 5.280537372676884]
	TIME [epoch: 7.9 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.461458883072704		[learning rate: 0.004286]
	Learning Rate: 0.00428599
	LOSS [training: 5.461458883072704 | validation: 5.1377201814854825]
	TIME [epoch: 7.86 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.411391601005214		[learning rate: 0.0042759]
	Learning Rate: 0.00427588
	LOSS [training: 5.411391601005214 | validation: 5.1589339201221165]
	TIME [epoch: 7.85 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.26712053043481		[learning rate: 0.0042658]
	Learning Rate: 0.0042658
	LOSS [training: 5.26712053043481 | validation: 5.099390457037911]
	TIME [epoch: 7.84 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.872248199984825		[learning rate: 0.0042557]
	Learning Rate: 0.00425573
	LOSS [training: 4.872248199984825 | validation: 4.156734757422836]
	TIME [epoch: 7.85 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.043359797958632		[learning rate: 0.0042457]
	Learning Rate: 0.00424569
	LOSS [training: 4.043359797958632 | validation: 3.989304910826604]
	TIME [epoch: 7.89 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9613119368204384		[learning rate: 0.0042357]
	Learning Rate: 0.00423568
	LOSS [training: 3.9613119368204384 | validation: 3.867976568668487]
	TIME [epoch: 7.85 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.851780509256296		[learning rate: 0.0042257]
	Learning Rate: 0.00422569
	LOSS [training: 3.851780509256296 | validation: 3.8544405945860323]
	TIME [epoch: 7.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8532796971151697		[learning rate: 0.0042157]
	Learning Rate: 0.00421572
	LOSS [training: 3.8532796971151697 | validation: 3.970689796632025]
	TIME [epoch: 7.85 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.904822418588381		[learning rate: 0.0042058]
	Learning Rate: 0.00420578
	LOSS [training: 3.904822418588381 | validation: 3.9556135733755786]
	TIME [epoch: 7.86 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8999733603989757		[learning rate: 0.0041959]
	Learning Rate: 0.00419585
	LOSS [training: 3.8999733603989757 | validation: 3.946499941928365]
	TIME [epoch: 7.88 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.950528839102712		[learning rate: 0.004186]
	Learning Rate: 0.00418596
	LOSS [training: 3.950528839102712 | validation: 4.018269106660998]
	TIME [epoch: 7.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9506152849608624		[learning rate: 0.0041761]
	Learning Rate: 0.00417608
	LOSS [training: 3.9506152849608624 | validation: 3.963468680674052]
	TIME [epoch: 7.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9346630457621736		[learning rate: 0.0041662]
	Learning Rate: 0.00416623
	LOSS [training: 3.9346630457621736 | validation: 4.031591328002705]
	TIME [epoch: 7.84 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9663127102456066		[learning rate: 0.0041564]
	Learning Rate: 0.00415641
	LOSS [training: 3.9663127102456066 | validation: 3.9681353651297506]
	TIME [epoch: 7.89 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9063195092204754		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 3.9063195092204754 | validation: 3.955784352904753]
	TIME [epoch: 7.86 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.908706299574439		[learning rate: 0.0041368]
	Learning Rate: 0.00413682
	LOSS [training: 3.908706299574439 | validation: 3.9276605090590517]
	TIME [epoch: 7.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879978861897145		[learning rate: 0.0041271]
	Learning Rate: 0.00412706
	LOSS [training: 3.879978861897145 | validation: 3.880720413039496]
	TIME [epoch: 7.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.79776790455863		[learning rate: 0.0041173]
	Learning Rate: 0.00411733
	LOSS [training: 3.79776790455863 | validation: 3.8197893139237933]
	TIME [epoch: 7.85 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8122242390358463		[learning rate: 0.0041076]
	Learning Rate: 0.00410762
	LOSS [training: 3.8122242390358463 | validation: 3.884139752349875]
	TIME [epoch: 7.9 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9255627760265366		[learning rate: 0.0040979]
	Learning Rate: 0.00409793
	LOSS [training: 3.9255627760265366 | validation: 3.897204469125616]
	TIME [epoch: 7.85 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8248996456948943		[learning rate: 0.0040883]
	Learning Rate: 0.00408826
	LOSS [training: 3.8248996456948943 | validation: 3.830401964982302]
	TIME [epoch: 7.85 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879753765520367		[learning rate: 0.0040786]
	Learning Rate: 0.00407862
	LOSS [training: 3.879753765520367 | validation: 3.848999182319536]
	TIME [epoch: 7.84 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870558783948164		[learning rate: 0.004069]
	Learning Rate: 0.004069
	LOSS [training: 3.870558783948164 | validation: 3.8735819529870557]
	TIME [epoch: 7.85 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9367513705581527		[learning rate: 0.0040594]
	Learning Rate: 0.0040594
	LOSS [training: 3.9367513705581527 | validation: 4.1234177017208165]
	TIME [epoch: 7.89 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2179760309379155		[learning rate: 0.0040498]
	Learning Rate: 0.00404982
	LOSS [training: 4.2179760309379155 | validation: 4.3583593836448475]
	TIME [epoch: 7.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.824172767332911		[learning rate: 0.0040403]
	Learning Rate: 0.00404027
	LOSS [training: 4.824172767332911 | validation: 5.162607227165984]
	TIME [epoch: 7.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007373113753782		[learning rate: 0.0040307]
	Learning Rate: 0.00403074
	LOSS [training: 5.007373113753782 | validation: 4.118951267041872]
	TIME [epoch: 7.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.966670996325469		[learning rate: 0.0040212]
	Learning Rate: 0.00402123
	LOSS [training: 3.966670996325469 | validation: 3.91088339344021]
	TIME [epoch: 7.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.95490674403173		[learning rate: 0.0040117]
	Learning Rate: 0.00401175
	LOSS [training: 3.95490674403173 | validation: 4.220701660190189]
	TIME [epoch: 7.88 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.429081675670554		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 4.429081675670554 | validation: 4.359670442374073]
	TIME [epoch: 7.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.512754390629808		[learning rate: 0.0039928]
	Learning Rate: 0.00399284
	LOSS [training: 4.512754390629808 | validation: 4.542691575366636]
	TIME [epoch: 7.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.819434940227342		[learning rate: 0.0039834]
	Learning Rate: 0.00398342
	LOSS [training: 4.819434940227342 | validation: 4.722723882581382]
	TIME [epoch: 7.83 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.913665501210474		[learning rate: 0.003974]
	Learning Rate: 0.00397403
	LOSS [training: 4.913665501210474 | validation: 4.772865829760315]
	TIME [epoch: 7.88 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.942948916836749		[learning rate: 0.0039647]
	Learning Rate: 0.00396465
	LOSS [training: 4.942948916836749 | validation: 4.731087154243001]
	TIME [epoch: 7.85 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.914388325807561		[learning rate: 0.0039553]
	Learning Rate: 0.0039553
	LOSS [training: 4.914388325807561 | validation: 4.6613224653318985]
	TIME [epoch: 7.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.879805069624871		[learning rate: 0.003946]
	Learning Rate: 0.00394597
	LOSS [training: 4.879805069624871 | validation: 4.6625887525684595]
	TIME [epoch: 7.84 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.812992296602855		[learning rate: 0.0039367]
	Learning Rate: 0.00393666
	LOSS [training: 4.812992296602855 | validation: 4.558334064020104]
	TIME [epoch: 7.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5281012484106515		[learning rate: 0.0039274]
	Learning Rate: 0.00392738
	LOSS [training: 4.5281012484106515 | validation: 4.161286115213929]
	TIME [epoch: 7.89 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.093889422687861		[learning rate: 0.0039181]
	Learning Rate: 0.00391811
	LOSS [training: 4.093889422687861 | validation: 3.8654021518466077]
	TIME [epoch: 7.85 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7917580076160013		[learning rate: 0.0039089]
	Learning Rate: 0.00390887
	LOSS [training: 3.7917580076160013 | validation: 3.70690375066316]
	TIME [epoch: 7.84 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.726559091905891		[learning rate: 0.0038996]
	Learning Rate: 0.00389965
	LOSS [training: 3.726559091905891 | validation: 3.9105394841627215]
	TIME [epoch: 7.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8696814098803824		[learning rate: 0.0038905]
	Learning Rate: 0.00389045
	LOSS [training: 3.8696814098803824 | validation: 3.833374985447524]
	TIME [epoch: 7.83 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.755885623541779		[learning rate: 0.0038813]
	Learning Rate: 0.00388127
	LOSS [training: 3.755885623541779 | validation: 3.7337322620512423]
	TIME [epoch: 7.88 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8071817570400595		[learning rate: 0.0038721]
	Learning Rate: 0.00387212
	LOSS [training: 3.8071817570400595 | validation: 3.9862055430348073]
	TIME [epoch: 7.84 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9812696952296287		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 3.9812696952296287 | validation: 4.091123645037127]
	TIME [epoch: 7.83 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.222084789200036		[learning rate: 0.0038539]
	Learning Rate: 0.00385387
	LOSS [training: 4.222084789200036 | validation: 4.335386723801147]
	TIME [epoch: 7.84 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.55818974427107		[learning rate: 0.0038448]
	Learning Rate: 0.00384478
	LOSS [training: 4.55818974427107 | validation: 4.356453787319639]
	TIME [epoch: 7.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5696318449130064		[learning rate: 0.0038357]
	Learning Rate: 0.00383571
	LOSS [training: 4.5696318449130064 | validation: 4.254457732606732]
	TIME [epoch: 7.88 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.453925005440252		[learning rate: 0.0038267]
	Learning Rate: 0.00382667
	LOSS [training: 4.453925005440252 | validation: 4.25483173810548]
	TIME [epoch: 7.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.27830515707365		[learning rate: 0.0038176]
	Learning Rate: 0.00381764
	LOSS [training: 4.27830515707365 | validation: 3.9439084423143402]
	TIME [epoch: 7.85 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8589742000111884		[learning rate: 0.0038086]
	Learning Rate: 0.00380863
	LOSS [training: 3.8589742000111884 | validation: 3.6962376813524744]
	TIME [epoch: 7.83 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6704454252038117		[learning rate: 0.0037996]
	Learning Rate: 0.00379965
	LOSS [training: 3.6704454252038117 | validation: 3.834560794240266]
	TIME [epoch: 7.87 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8430988694704573		[learning rate: 0.0037907]
	Learning Rate: 0.00379069
	LOSS [training: 3.8430988694704573 | validation: 3.885716858531584]
	TIME [epoch: 7.86 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8755256090834242		[learning rate: 0.0037817]
	Learning Rate: 0.00378175
	LOSS [training: 3.8755256090834242 | validation: 3.8662589107600907]
	TIME [epoch: 7.84 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8508953775919275		[learning rate: 0.0037728]
	Learning Rate: 0.00377283
	LOSS [training: 3.8508953775919275 | validation: 3.843121479700458]
	TIME [epoch: 7.84 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8121349616943725		[learning rate: 0.0037639]
	Learning Rate: 0.00376393
	LOSS [training: 3.8121349616943725 | validation: 3.817611770016125]
	TIME [epoch: 7.84 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7761595801568664		[learning rate: 0.003755]
	Learning Rate: 0.00375505
	LOSS [training: 3.7761595801568664 | validation: 3.8400795034826185]
	TIME [epoch: 7.87 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7489769262326145		[learning rate: 0.0037462]
	Learning Rate: 0.00374619
	LOSS [training: 3.7489769262326145 | validation: 3.8397664819306856]
	TIME [epoch: 7.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7340582361430767		[learning rate: 0.0037374]
	Learning Rate: 0.00373735
	LOSS [training: 3.7340582361430767 | validation: 3.827216844858172]
	TIME [epoch: 7.84 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.701661201385209		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 3.701661201385209 | validation: 3.808369325827276]
	TIME [epoch: 7.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683693420378719		[learning rate: 0.0037197]
	Learning Rate: 0.00371974
	LOSS [training: 3.683693420378719 | validation: 3.7881512104028268]
	TIME [epoch: 7.84 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.664438572666713		[learning rate: 0.003711]
	Learning Rate: 0.00371097
	LOSS [training: 3.664438572666713 | validation: 3.7717500428151185]
	TIME [epoch: 7.89 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6379842092066768		[learning rate: 0.0037022]
	Learning Rate: 0.00370221
	LOSS [training: 3.6379842092066768 | validation: 3.7407505790837416]
	TIME [epoch: 7.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.603571044575225		[learning rate: 0.0036935]
	Learning Rate: 0.00369348
	LOSS [training: 3.603571044575225 | validation: 3.717137815559857]
	TIME [epoch: 7.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5773604249325075		[learning rate: 0.0036848]
	Learning Rate: 0.00368477
	LOSS [training: 3.5773604249325075 | validation: 3.7020048426547767]
	TIME [epoch: 7.84 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5570647927149235		[learning rate: 0.0036761]
	Learning Rate: 0.00367608
	LOSS [training: 3.5570647927149235 | validation: 3.668369775424831]
	TIME [epoch: 7.85 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5426873078682943		[learning rate: 0.0036674]
	Learning Rate: 0.00366741
	LOSS [training: 3.5426873078682943 | validation: 3.631308097509227]
	TIME [epoch: 7.88 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5301173916770745		[learning rate: 0.0036588]
	Learning Rate: 0.00365875
	LOSS [training: 3.5301173916770745 | validation: 3.5961751535652344]
	TIME [epoch: 7.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.499706112816777		[learning rate: 0.0036501]
	Learning Rate: 0.00365012
	LOSS [training: 3.499706112816777 | validation: 3.537243833687195]
	TIME [epoch: 7.83 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4695425051175413		[learning rate: 0.0036415]
	Learning Rate: 0.00364151
	LOSS [training: 3.4695425051175413 | validation: 3.468511485745263]
	TIME [epoch: 7.83 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.421610785552694		[learning rate: 0.0036329]
	Learning Rate: 0.00363292
	LOSS [training: 3.421610785552694 | validation: 3.4253594557332607]
	TIME [epoch: 7.87 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.41990736168683		[learning rate: 0.0036244]
	Learning Rate: 0.00362436
	LOSS [training: 3.41990736168683 | validation: 3.4209986977201456]
	TIME [epoch: 7.87 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.388290968646899		[learning rate: 0.0036158]
	Learning Rate: 0.00361581
	LOSS [training: 3.388290968646899 | validation: 3.4031772864893934]
	TIME [epoch: 7.85 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.438079596384522		[learning rate: 0.0036073]
	Learning Rate: 0.00360728
	LOSS [training: 3.438079596384522 | validation: 3.4937921179075326]
	TIME [epoch: 7.84 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426113436875105		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 3.426113436875105 | validation: 3.409157799901641]
	TIME [epoch: 7.84 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3852484824541405		[learning rate: 0.0035903]
	Learning Rate: 0.00359028
	LOSS [training: 3.3852484824541405 | validation: 3.4173792421274447]
	TIME [epoch: 7.88 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4147302435990396		[learning rate: 0.0035818]
	Learning Rate: 0.00358181
	LOSS [training: 3.4147302435990396 | validation: 3.4602131326503676]
	TIME [epoch: 7.85 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3889280998105176		[learning rate: 0.0035734]
	Learning Rate: 0.00357336
	LOSS [training: 3.3889280998105176 | validation: 3.4027442945346404]
	TIME [epoch: 7.85 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379159893303182		[learning rate: 0.0035649]
	Learning Rate: 0.00356493
	LOSS [training: 3.379159893303182 | validation: 3.413874732304588]
	TIME [epoch: 7.85 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.383143377175247		[learning rate: 0.0035565]
	Learning Rate: 0.00355652
	LOSS [training: 3.383143377175247 | validation: 3.4132994533633396]
	TIME [epoch: 7.85 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3780445934135237		[learning rate: 0.0035481]
	Learning Rate: 0.00354813
	LOSS [training: 3.3780445934135237 | validation: 3.3874563019393342]
	TIME [epoch: 7.88 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3754438561117532		[learning rate: 0.0035398]
	Learning Rate: 0.00353976
	LOSS [training: 3.3754438561117532 | validation: 3.41540831317341]
	TIME [epoch: 7.85 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3529203948653987		[learning rate: 0.0035314]
	Learning Rate: 0.00353141
	LOSS [training: 3.3529203948653987 | validation: 3.3793891412614703]
	TIME [epoch: 7.84 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344774588152449		[learning rate: 0.0035231]
	Learning Rate: 0.00352308
	LOSS [training: 3.344774588152449 | validation: 3.433629230651699]
	TIME [epoch: 7.83 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.411024767234492		[learning rate: 0.0035148]
	Learning Rate: 0.00351477
	LOSS [training: 3.411024767234492 | validation: 3.48731618717866]
	TIME [epoch: 7.84 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4424613091226544		[learning rate: 0.0035065]
	Learning Rate: 0.00350648
	LOSS [training: 3.4424613091226544 | validation: 3.4577019840179455]
	TIME [epoch: 7.88 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.453245886727466		[learning rate: 0.0034982]
	Learning Rate: 0.00349821
	LOSS [training: 3.453245886727466 | validation: 3.6187846585969847]
	TIME [epoch: 7.84 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6087424992123003		[learning rate: 0.00349]
	Learning Rate: 0.00348996
	LOSS [training: 3.6087424992123003 | validation: 3.5820655488360305]
	TIME [epoch: 7.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6626574246631143		[learning rate: 0.0034817]
	Learning Rate: 0.00348173
	LOSS [training: 3.6626574246631143 | validation: 3.8171355905667905]
	TIME [epoch: 7.84 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.73673376544982		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 3.73673376544982 | validation: 3.71490301337184]
	TIME [epoch: 7.84 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6915018271616558		[learning rate: 0.0034653]
	Learning Rate: 0.00346532
	LOSS [training: 3.6915018271616558 | validation: 3.66125923321245]
	TIME [epoch: 7.88 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.586117354600535		[learning rate: 0.0034571]
	Learning Rate: 0.00345715
	LOSS [training: 3.586117354600535 | validation: 3.6009937475905414]
	TIME [epoch: 7.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.564340830217799		[learning rate: 0.003449]
	Learning Rate: 0.00344899
	LOSS [training: 3.564340830217799 | validation: 3.590396673649236]
	TIME [epoch: 7.85 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5253873120332653		[learning rate: 0.0034409]
	Learning Rate: 0.00344086
	LOSS [training: 3.5253873120332653 | validation: 3.549640109403186]
	TIME [epoch: 7.84 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5090416218390534		[learning rate: 0.0034327]
	Learning Rate: 0.00343274
	LOSS [training: 3.5090416218390534 | validation: 3.512136878700007]
	TIME [epoch: 7.89 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4599763042809726		[learning rate: 0.0034246]
	Learning Rate: 0.00342464
	LOSS [training: 3.4599763042809726 | validation: 3.4931840646676457]
	TIME [epoch: 7.85 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4373662917767533		[learning rate: 0.0034166]
	Learning Rate: 0.00341657
	LOSS [training: 3.4373662917767533 | validation: 3.425929530280739]
	TIME [epoch: 7.85 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.383059467185245		[learning rate: 0.0034085]
	Learning Rate: 0.00340851
	LOSS [training: 3.383059467185245 | validation: 3.409356825733699]
	TIME [epoch: 7.84 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3853962090201457		[learning rate: 0.0034005]
	Learning Rate: 0.00340047
	LOSS [training: 3.3853962090201457 | validation: 3.4123070418978068]
	TIME [epoch: 7.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.406421461207789		[learning rate: 0.0033924]
	Learning Rate: 0.00339244
	LOSS [training: 3.406421461207789 | validation: 3.3905695585509603]
	TIME [epoch: 7.89 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3951842929585965		[learning rate: 0.0033844]
	Learning Rate: 0.00338444
	LOSS [training: 3.3951842929585965 | validation: 3.521294106711461]
	TIME [epoch: 7.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.514878117663152		[learning rate: 0.0033765]
	Learning Rate: 0.00337646
	LOSS [training: 3.514878117663152 | validation: 3.600936358344054]
	TIME [epoch: 7.85 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.513367895448331		[learning rate: 0.0033685]
	Learning Rate: 0.0033685
	LOSS [training: 3.513367895448331 | validation: 3.5445077288935884]
	TIME [epoch: 7.84 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4964077156427127		[learning rate: 0.0033605]
	Learning Rate: 0.00336055
	LOSS [training: 3.4964077156427127 | validation: 3.6277793651747734]
	TIME [epoch: 7.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5481731367651594		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 3.5481731367651594 | validation: 3.6504179680675266]
	TIME [epoch: 7.89 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5617625540698956		[learning rate: 0.0033447]
	Learning Rate: 0.00334471
	LOSS [training: 3.5617625540698956 | validation: 3.645190690736599]
	TIME [epoch: 7.85 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.542799953098393		[learning rate: 0.0033368]
	Learning Rate: 0.00333682
	LOSS [training: 3.542799953098393 | validation: 3.645114958134558]
	TIME [epoch: 7.85 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6265987541490707		[learning rate: 0.003329]
	Learning Rate: 0.00332895
	LOSS [training: 3.6265987541490707 | validation: 3.759908615191695]
	TIME [epoch: 7.85 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7035189800257498		[learning rate: 0.0033211]
	Learning Rate: 0.0033211
	LOSS [training: 3.7035189800257498 | validation: 3.9585799359637255]
	TIME [epoch: 7.85 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.915179487764592		[learning rate: 0.0033133]
	Learning Rate: 0.00331327
	LOSS [training: 3.915179487764592 | validation: 3.866761684008833]
	TIME [epoch: 7.88 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.970623947934559		[learning rate: 0.0033055]
	Learning Rate: 0.00330545
	LOSS [training: 3.970623947934559 | validation: 3.958346570048258]
	TIME [epoch: 7.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127627006681303		[learning rate: 0.0032977]
	Learning Rate: 0.00329765
	LOSS [training: 4.127627006681303 | validation: 3.976921371568289]
	TIME [epoch: 7.85 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.099140095944118		[learning rate: 0.0032899]
	Learning Rate: 0.00328988
	LOSS [training: 4.099140095944118 | validation: 3.8799046818309844]
	TIME [epoch: 7.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.033223633795669		[learning rate: 0.0032821]
	Learning Rate: 0.00328212
	LOSS [training: 4.033223633795669 | validation: 3.901820980246752]
	TIME [epoch: 7.89 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.050532494108317		[learning rate: 0.0032744]
	Learning Rate: 0.00327437
	LOSS [training: 4.050532494108317 | validation: 3.8502052809077494]
	TIME [epoch: 7.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9806698534510723		[learning rate: 0.0032666]
	Learning Rate: 0.00326665
	LOSS [training: 3.9806698534510723 | validation: 3.821565632656739]
	TIME [epoch: 7.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9538395883946227		[learning rate: 0.0032589]
	Learning Rate: 0.00325894
	LOSS [training: 3.9538395883946227 | validation: 3.809404360319865]
	TIME [epoch: 7.84 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9384130430234996		[learning rate: 0.0032513]
	Learning Rate: 0.00325126
	LOSS [training: 3.9384130430234996 | validation: 3.777876445140313]
	TIME [epoch: 7.84 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016515726903076		[learning rate: 0.0032436]
	Learning Rate: 0.00324359
	LOSS [training: 4.016515726903076 | validation: 4.105736279757139]
	TIME [epoch: 7.89 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.418060540287277		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 4.418060540287277 | validation: 4.243956761459023]
	TIME [epoch: 7.85 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.466124264830748		[learning rate: 0.0032283]
	Learning Rate: 0.0032283
	LOSS [training: 4.466124264830748 | validation: 4.237209106047458]
	TIME [epoch: 7.84 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.460288669322506		[learning rate: 0.0032207]
	Learning Rate: 0.00322069
	LOSS [training: 4.460288669322506 | validation: 4.1102538923758996]
	TIME [epoch: 7.85 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.245141083304272		[learning rate: 0.0032131]
	Learning Rate: 0.00321309
	LOSS [training: 4.245141083304272 | validation: 4.031414687257601]
	TIME [epoch: 7.85 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.226080225285285		[learning rate: 0.0032055]
	Learning Rate: 0.00320551
	LOSS [training: 4.226080225285285 | validation: 4.002746650210245]
	TIME [epoch: 7.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164159076830627		[learning rate: 0.003198]
	Learning Rate: 0.00319795
	LOSS [training: 4.164159076830627 | validation: 3.951975034464876]
	TIME [epoch: 7.83 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.086459452461413		[learning rate: 0.0031904]
	Learning Rate: 0.00319041
	LOSS [training: 4.086459452461413 | validation: 3.9398178984241703]
	TIME [epoch: 7.85 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.967017005745801		[learning rate: 0.0031829]
	Learning Rate: 0.00318288
	LOSS [training: 3.967017005745801 | validation: 3.6891237252376428]
	TIME [epoch: 7.85 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5538289421930265		[learning rate: 0.0031754]
	Learning Rate: 0.00317537
	LOSS [training: 3.5538289421930265 | validation: 3.5457616379913626]
	TIME [epoch: 7.86 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.512359961830123		[learning rate: 0.0031679]
	Learning Rate: 0.00316788
	LOSS [training: 3.512359961830123 | validation: 3.5494975993821605]
	TIME [epoch: 7.89 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.527454468334535		[learning rate: 0.0031604]
	Learning Rate: 0.00316041
	LOSS [training: 3.527454468334535 | validation: 3.5947893904882564]
	TIME [epoch: 7.85 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.534512957933808		[learning rate: 0.003153]
	Learning Rate: 0.00315296
	LOSS [training: 3.534512957933808 | validation: 3.579846000161262]
	TIME [epoch: 7.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5476766022962725		[learning rate: 0.0031455]
	Learning Rate: 0.00314552
	LOSS [training: 3.5476766022962725 | validation: 3.663468916737307]
	TIME [epoch: 7.84 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6818592880452217		[learning rate: 0.0031381]
	Learning Rate: 0.0031381
	LOSS [training: 3.6818592880452217 | validation: 3.7239256469380404]
	TIME [epoch: 7.89 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8068466957830736		[learning rate: 0.0031307]
	Learning Rate: 0.0031307
	LOSS [training: 3.8068466957830736 | validation: 3.9245843210106193]
	TIME [epoch: 7.86 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7805134856394407		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 3.7805134856394407 | validation: 3.7324889615362915]
	TIME [epoch: 7.84 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6221470367751625		[learning rate: 0.0031159]
	Learning Rate: 0.00311594
	LOSS [training: 3.6221470367751625 | validation: 3.6594185340237724]
	TIME [epoch: 7.84 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.596823617371647		[learning rate: 0.0031086]
	Learning Rate: 0.0031086
	LOSS [training: 3.596823617371647 | validation: 3.66357837830762]
	TIME [epoch: 7.86 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5875460262812204		[learning rate: 0.0031013]
	Learning Rate: 0.00310126
	LOSS [training: 3.5875460262812204 | validation: 3.632965852470437]
	TIME [epoch: 7.88 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5689617443791235		[learning rate: 0.0030939]
	Learning Rate: 0.00309395
	LOSS [training: 3.5689617443791235 | validation: 3.632697314423142]
	TIME [epoch: 7.85 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5692675649025833		[learning rate: 0.0030866]
	Learning Rate: 0.00308665
	LOSS [training: 3.5692675649025833 | validation: 3.6357388471243635]
	TIME [epoch: 7.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.561555060159554		[learning rate: 0.0030794]
	Learning Rate: 0.00307937
	LOSS [training: 3.561555060159554 | validation: 3.6565771505380233]
	TIME [epoch: 7.85 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5532384705459794		[learning rate: 0.0030721]
	Learning Rate: 0.0030721
	LOSS [training: 3.5532384705459794 | validation: 3.6132516017291967]
	TIME [epoch: 7.85 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5402661535181794		[learning rate: 0.0030649]
	Learning Rate: 0.00306486
	LOSS [training: 3.5402661535181794 | validation: 3.62300942386996]
	TIME [epoch: 7.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5528506335793124		[learning rate: 0.0030576]
	Learning Rate: 0.00305763
	LOSS [training: 3.5528506335793124 | validation: 3.6162623138223946]
	TIME [epoch: 7.85 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.568902792829962		[learning rate: 0.0030504]
	Learning Rate: 0.00305042
	LOSS [training: 3.568902792829962 | validation: 3.652875497538928]
	TIME [epoch: 7.85 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5965301097604323		[learning rate: 0.0030432]
	Learning Rate: 0.00304322
	LOSS [training: 3.5965301097604323 | validation: 3.659549983612428]
	TIME [epoch: 7.84 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6326727943031933		[learning rate: 0.003036]
	Learning Rate: 0.00303604
	LOSS [training: 3.6326727943031933 | validation: 3.704797655835428]
	TIME [epoch: 7.86 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.647034907626397		[learning rate: 0.0030289]
	Learning Rate: 0.00302888
	LOSS [training: 3.647034907626397 | validation: 3.7007037079252894]
	TIME [epoch: 7.88 sec]
EPOCH 557/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 3.725033223642873		[learning rate: 0.0030217]
	Learning Rate: 0.00302174
	LOSS [training: 3.725033223642873 | validation: 3.8553963411070358]
	TIME [epoch: 115 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9269190744086013		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 3.9269190744086013 | validation: 3.9080420579699178]
	TIME [epoch: 7.86 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9561701281051693		[learning rate: 0.0030075]
	Learning Rate: 0.0030075
	LOSS [training: 3.9561701281051693 | validation: 3.8533219568250012]
	TIME [epoch: 7.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.060283423014225		[learning rate: 0.0030004]
	Learning Rate: 0.0030004
	LOSS [training: 4.060283423014225 | validation: 4.1441300837680854]
	TIME [epoch: 7.86 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.334814120280771		[learning rate: 0.0029933]
	Learning Rate: 0.00299332
	LOSS [training: 4.334814120280771 | validation: 4.381277902964211]
	TIME [epoch: 7.87 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.412012532177675		[learning rate: 0.0029863]
	Learning Rate: 0.00298626
	LOSS [training: 4.412012532177675 | validation: 4.263311394002969]
	TIME [epoch: 7.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3808970990881155		[learning rate: 0.0029792]
	Learning Rate: 0.00297922
	LOSS [training: 4.3808970990881155 | validation: 4.354454223554152]
	TIME [epoch: 7.85 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.428381301584888		[learning rate: 0.0029722]
	Learning Rate: 0.00297219
	LOSS [training: 4.428381301584888 | validation: 4.273099823484188]
	TIME [epoch: 7.84 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.295941788499978		[learning rate: 0.0029652]
	Learning Rate: 0.00296518
	LOSS [training: 4.295941788499978 | validation: 4.171855918311354]
	TIME [epoch: 7.85 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.204534296072602		[learning rate: 0.0029582]
	Learning Rate: 0.00295819
	LOSS [training: 4.204534296072602 | validation: 4.068253277095103]
	TIME [epoch: 7.89 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.081048630193562		[learning rate: 0.0029512]
	Learning Rate: 0.00295121
	LOSS [training: 4.081048630193562 | validation: 4.023994258511335]
	TIME [epoch: 7.84 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.06637667186367		[learning rate: 0.0029442]
	Learning Rate: 0.00294425
	LOSS [training: 4.06637667186367 | validation: 4.129331812207239]
	TIME [epoch: 7.84 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.119624117887526		[learning rate: 0.0029373]
	Learning Rate: 0.0029373
	LOSS [training: 4.119624117887526 | validation: 4.080539093635341]
	TIME [epoch: 7.86 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1397886289645305		[learning rate: 0.0029304]
	Learning Rate: 0.00293037
	LOSS [training: 4.1397886289645305 | validation: 4.200049174075376]
	TIME [epoch: 7.85 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.323240235083612		[learning rate: 0.0029235]
	Learning Rate: 0.00292346
	LOSS [training: 4.323240235083612 | validation: 4.351458523430472]
	TIME [epoch: 7.89 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.40012016013571		[learning rate: 0.0029166]
	Learning Rate: 0.00291657
	LOSS [training: 4.40012016013571 | validation: 4.416087330975341]
	TIME [epoch: 7.84 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.473050224735454		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 4.473050224735454 | validation: 4.461404246941175]
	TIME [epoch: 7.84 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.527378167001373		[learning rate: 0.0029028]
	Learning Rate: 0.00290282
	LOSS [training: 4.527378167001373 | validation: 4.4727327677926665]
	TIME [epoch: 7.82 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.493332915946141		[learning rate: 0.002896]
	Learning Rate: 0.00289598
	LOSS [training: 4.493332915946141 | validation: 4.452667517720161]
	TIME [epoch: 7.83 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.484779183429968		[learning rate: 0.0028891]
	Learning Rate: 0.00288914
	LOSS [training: 4.484779183429968 | validation: 4.439769173487145]
	TIME [epoch: 7.88 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.460073893157604		[learning rate: 0.0028823]
	Learning Rate: 0.00288233
	LOSS [training: 4.460073893157604 | validation: 4.3729161925912505]
	TIME [epoch: 7.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.32865780312739		[learning rate: 0.0028755]
	Learning Rate: 0.00287553
	LOSS [training: 4.32865780312739 | validation: 4.352988846683511]
	TIME [epoch: 7.84 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.376560401331273		[learning rate: 0.0028687]
	Learning Rate: 0.00286875
	LOSS [training: 4.376560401331273 | validation: 4.47991637430491]
	TIME [epoch: 7.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.471917360129707		[learning rate: 0.002862]
	Learning Rate: 0.00286198
	LOSS [training: 4.471917360129707 | validation: 4.521372262816783]
	TIME [epoch: 7.84 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.460708373147557		[learning rate: 0.0028552]
	Learning Rate: 0.00285523
	LOSS [training: 4.460708373147557 | validation: 4.434298997359817]
	TIME [epoch: 7.89 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.411549299383403		[learning rate: 0.0028485]
	Learning Rate: 0.00284849
	LOSS [training: 4.411549299383403 | validation: 4.45246820549395]
	TIME [epoch: 7.88 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.404912802876217		[learning rate: 0.0028418]
	Learning Rate: 0.00284178
	LOSS [training: 4.404912802876217 | validation: 4.37003761191975]
	TIME [epoch: 7.86 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.34279917277263		[learning rate: 0.0028351]
	Learning Rate: 0.00283507
	LOSS [training: 4.34279917277263 | validation: 4.312470986861947]
	TIME [epoch: 7.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.231298148845794		[learning rate: 0.0028284]
	Learning Rate: 0.00282838
	LOSS [training: 4.231298148845794 | validation: 4.063202961401796]
	TIME [epoch: 7.85 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9493548614825715		[learning rate: 0.0028217]
	Learning Rate: 0.00282171
	LOSS [training: 3.9493548614825715 | validation: 3.8380737816015804]
	TIME [epoch: 7.86 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7900397282278857		[learning rate: 0.0028151]
	Learning Rate: 0.00281506
	LOSS [training: 3.7900397282278857 | validation: 3.780048795613354]
	TIME [epoch: 7.89 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7792777530654518		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 3.7792777530654518 | validation: 3.868061158406579]
	TIME [epoch: 7.85 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.883666262082289		[learning rate: 0.0028018]
	Learning Rate: 0.00280179
	LOSS [training: 3.883666262082289 | validation: 3.850511552771926]
	TIME [epoch: 7.84 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.853387680113494		[learning rate: 0.0027952]
	Learning Rate: 0.00279518
	LOSS [training: 3.853387680113494 | validation: 3.7802382284766463]
	TIME [epoch: 7.83 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7718638050330493		[learning rate: 0.0027886]
	Learning Rate: 0.00278859
	LOSS [training: 3.7718638050330493 | validation: 3.701490424616833]
	TIME [epoch: 7.84 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7559924725929994		[learning rate: 0.002782]
	Learning Rate: 0.00278201
	LOSS [training: 3.7559924725929994 | validation: 3.747494448374479]
	TIME [epoch: 7.89 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777148231520137		[learning rate: 0.0027754]
	Learning Rate: 0.00277545
	LOSS [training: 3.777148231520137 | validation: 3.7472076905196277]
	TIME [epoch: 7.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7650961064781256		[learning rate: 0.0027689]
	Learning Rate: 0.0027689
	LOSS [training: 3.7650961064781256 | validation: 3.7291969983137063]
	TIME [epoch: 7.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7720993114032173		[learning rate: 0.0027624]
	Learning Rate: 0.00276237
	LOSS [training: 3.7720993114032173 | validation: 3.8172271223341205]
	TIME [epoch: 7.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.817402965239732		[learning rate: 0.0027559]
	Learning Rate: 0.00275586
	LOSS [training: 3.817402965239732 | validation: 3.927780428953803]
	TIME [epoch: 7.84 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8911552297927807		[learning rate: 0.0027494]
	Learning Rate: 0.00274936
	LOSS [training: 3.8911552297927807 | validation: 3.878817127591782]
	TIME [epoch: 7.89 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850462408517365		[learning rate: 0.0027429]
	Learning Rate: 0.00274287
	LOSS [training: 3.850462408517365 | validation: 3.942911462134994]
	TIME [epoch: 7.86 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9660700253231385		[learning rate: 0.0027364]
	Learning Rate: 0.0027364
	LOSS [training: 3.9660700253231385 | validation: 4.0810222943120715]
	TIME [epoch: 7.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9604888001043785		[learning rate: 0.0027299]
	Learning Rate: 0.00272994
	LOSS [training: 3.9604888001043785 | validation: 3.8912276628737414]
	TIME [epoch: 7.85 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814985204944046		[learning rate: 0.0027235]
	Learning Rate: 0.00272351
	LOSS [training: 3.814985204944046 | validation: 3.7698233697800796]
	TIME [epoch: 7.85 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7773744575371047		[learning rate: 0.0027171]
	Learning Rate: 0.00271708
	LOSS [training: 3.7773744575371047 | validation: 3.8868356406893914]
	TIME [epoch: 7.88 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8290999915104784		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 3.8290999915104784 | validation: 3.8574142910628124]
	TIME [epoch: 7.88 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.812673105303224		[learning rate: 0.0027043]
	Learning Rate: 0.00270428
	LOSS [training: 3.812673105303224 | validation: 3.8632271636048996]
	TIME [epoch: 7.85 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7917390890407887		[learning rate: 0.0026979]
	Learning Rate: 0.0026979
	LOSS [training: 3.7917390890407887 | validation: 3.7685232355818767]
	TIME [epoch: 7.85 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7490204199931774		[learning rate: 0.0026915]
	Learning Rate: 0.00269153
	LOSS [training: 3.7490204199931774 | validation: 3.7420303335362877]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7391960982420365		[learning rate: 0.0026852]
	Learning Rate: 0.00268519
	LOSS [training: 3.7391960982420365 | validation: 3.714011918208034]
	TIME [epoch: 7.86 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.734625189506275		[learning rate: 0.0026789]
	Learning Rate: 0.00267885
	LOSS [training: 3.734625189506275 | validation: 3.7069759240640536]
	TIME [epoch: 7.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7416591701885533		[learning rate: 0.0026725]
	Learning Rate: 0.00267253
	LOSS [training: 3.7416591701885533 | validation: 3.7714725679379075]
	TIME [epoch: 7.85 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.769824123992326		[learning rate: 0.0026662]
	Learning Rate: 0.00266623
	LOSS [training: 3.769824123992326 | validation: 3.747512043134093]
	TIME [epoch: 7.85 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7740238618734088		[learning rate: 0.0026599]
	Learning Rate: 0.00265994
	LOSS [training: 3.7740238618734088 | validation: 3.7586525593022815]
	TIME [epoch: 7.85 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779898583835433		[learning rate: 0.0026537]
	Learning Rate: 0.00265367
	LOSS [training: 3.779898583835433 | validation: 3.8339636934423598]
	TIME [epoch: 7.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8155539515827703		[learning rate: 0.0026474]
	Learning Rate: 0.00264741
	LOSS [training: 3.8155539515827703 | validation: 3.828919411874966]
	TIME [epoch: 7.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8125363639151435		[learning rate: 0.0026412]
	Learning Rate: 0.00264116
	LOSS [training: 3.8125363639151435 | validation: 3.8405563249732513]
	TIME [epoch: 7.85 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8187586624350343		[learning rate: 0.0026349]
	Learning Rate: 0.00263493
	LOSS [training: 3.8187586624350343 | validation: 3.865426405257111]
	TIME [epoch: 7.84 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838416047575825		[learning rate: 0.0026287]
	Learning Rate: 0.00262872
	LOSS [training: 3.838416047575825 | validation: 3.8888933024606023]
	TIME [epoch: 7.86 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8410032522982647		[learning rate: 0.0026225]
	Learning Rate: 0.00262251
	LOSS [training: 3.8410032522982647 | validation: 3.8721785652323533]
	TIME [epoch: 7.85 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826212777875382		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 3.826212777875382 | validation: 3.856398293481644]
	TIME [epoch: 7.88 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8167407088751792		[learning rate: 0.0026102]
	Learning Rate: 0.00261016
	LOSS [training: 3.8167407088751792 | validation: 3.849183164455967]
	TIME [epoch: 7.87 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.828467157866603		[learning rate: 0.002604]
	Learning Rate: 0.002604
	LOSS [training: 3.828467157866603 | validation: 3.8766117433612903]
	TIME [epoch: 7.85 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852910324108312		[learning rate: 0.0025979]
	Learning Rate: 0.00259786
	LOSS [training: 3.852910324108312 | validation: 3.9022294406864515]
	TIME [epoch: 7.85 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8702202314449057		[learning rate: 0.0025917]
	Learning Rate: 0.00259173
	LOSS [training: 3.8702202314449057 | validation: 3.917998215736326]
	TIME [epoch: 7.85 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.879251355805235		[learning rate: 0.0025856]
	Learning Rate: 0.00258562
	LOSS [training: 3.879251355805235 | validation: 3.9258594673535336]
	TIME [epoch: 7.89 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870792108317472		[learning rate: 0.0025795]
	Learning Rate: 0.00257952
	LOSS [training: 3.870792108317472 | validation: 3.9054426328666247]
	TIME [epoch: 7.87 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8502479911880307		[learning rate: 0.0025734]
	Learning Rate: 0.00257343
	LOSS [training: 3.8502479911880307 | validation: 3.8971119293943812]
	TIME [epoch: 7.86 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8505665853340854		[learning rate: 0.0025674]
	Learning Rate: 0.00256736
	LOSS [training: 3.8505665853340854 | validation: 3.899227941285589]
	TIME [epoch: 7.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8514919323938055		[learning rate: 0.0025613]
	Learning Rate: 0.00256131
	LOSS [training: 3.8514919323938055 | validation: 3.8968097929365566]
	TIME [epoch: 7.85 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8463903320985375		[learning rate: 0.0025553]
	Learning Rate: 0.00255526
	LOSS [training: 3.8463903320985375 | validation: 3.8906353591122755]
	TIME [epoch: 7.88 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8578001060131046		[learning rate: 0.0025492]
	Learning Rate: 0.00254924
	LOSS [training: 3.8578001060131046 | validation: 3.895095953625815]
	TIME [epoch: 7.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.851858640544153		[learning rate: 0.0025432]
	Learning Rate: 0.00254322
	LOSS [training: 3.851858640544153 | validation: 3.87714759906065]
	TIME [epoch: 7.86 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8388264154933434		[learning rate: 0.0025372]
	Learning Rate: 0.00253722
	LOSS [training: 3.8388264154933434 | validation: 3.89282631498926]
	TIME [epoch: 7.85 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8415650465254987		[learning rate: 0.0025312]
	Learning Rate: 0.00253124
	LOSS [training: 3.8415650465254987 | validation: 3.892060259903266]
	TIME [epoch: 7.85 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.844390415254791		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 3.844390415254791 | validation: 3.8841461771364636]
	TIME [epoch: 7.86 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849614762194917		[learning rate: 0.0025193]
	Learning Rate: 0.00251931
	LOSS [training: 3.849614762194917 | validation: 3.8818037041364457]
	TIME [epoch: 7.9 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.83008076680531		[learning rate: 0.0025134]
	Learning Rate: 0.00251337
	LOSS [training: 3.83008076680531 | validation: 3.863985855407646]
	TIME [epoch: 7.86 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811057496848311		[learning rate: 0.0025074]
	Learning Rate: 0.00250744
	LOSS [training: 3.811057496848311 | validation: 3.8498412009008147]
	TIME [epoch: 7.85 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8041180597705857		[learning rate: 0.0025015]
	Learning Rate: 0.00250153
	LOSS [training: 3.8041180597705857 | validation: 3.8557758678663365]
	TIME [epoch: 7.85 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.801881195544303		[learning rate: 0.0024956]
	Learning Rate: 0.00249563
	LOSS [training: 3.801881195544303 | validation: 3.8372334316781784]
	TIME [epoch: 7.85 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8091924206943415		[learning rate: 0.0024897]
	Learning Rate: 0.00248974
	LOSS [training: 3.8091924206943415 | validation: 3.853012700556829]
	TIME [epoch: 7.89 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8179797338445476		[learning rate: 0.0024839]
	Learning Rate: 0.00248387
	LOSS [training: 3.8179797338445476 | validation: 3.875527327846176]
	TIME [epoch: 7.85 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8409869565787034		[learning rate: 0.002478]
	Learning Rate: 0.00247801
	LOSS [training: 3.8409869565787034 | validation: 3.877380856011329]
	TIME [epoch: 7.85 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847654200668754		[learning rate: 0.0024722]
	Learning Rate: 0.00247216
	LOSS [training: 3.847654200668754 | validation: 3.8726193588205025]
	TIME [epoch: 7.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8300005855799477		[learning rate: 0.0024663]
	Learning Rate: 0.00246633
	LOSS [training: 3.8300005855799477 | validation: 3.8158030428724476]
	TIME [epoch: 7.83 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8049190137169226		[learning rate: 0.0024605]
	Learning Rate: 0.00246051
	LOSS [training: 3.8049190137169226 | validation: 3.798799259718785]
	TIME [epoch: 7.87 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795070611295828		[learning rate: 0.0024547]
	Learning Rate: 0.00245471
	LOSS [training: 3.795070611295828 | validation: 3.789388241090467]
	TIME [epoch: 7.87 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.797848308555813		[learning rate: 0.0024489]
	Learning Rate: 0.00244892
	LOSS [training: 3.797848308555813 | validation: 3.8107723197952845]
	TIME [epoch: 7.85 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824170966613494		[learning rate: 0.0024431]
	Learning Rate: 0.00244314
	LOSS [training: 3.824170966613494 | validation: 3.8481306443713112]
	TIME [epoch: 7.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8159811674988475		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 3.8159811674988475 | validation: 3.8044111519661845]
	TIME [epoch: 7.85 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.803524405364828		[learning rate: 0.0024316]
	Learning Rate: 0.00243163
	LOSS [training: 3.803524405364828 | validation: 3.832706115031749]
	TIME [epoch: 7.86 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81680635730342		[learning rate: 0.0024259]
	Learning Rate: 0.00242589
	LOSS [training: 3.81680635730342 | validation: 3.85877996670317]
	TIME [epoch: 7.89 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.830051172408229		[learning rate: 0.0024202]
	Learning Rate: 0.00242017
	LOSS [training: 3.830051172408229 | validation: 3.844854190410019]
	TIME [epoch: 7.85 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8052983046361186		[learning rate: 0.0024145]
	Learning Rate: 0.00241446
	LOSS [training: 3.8052983046361186 | validation: 3.8220915480660342]
	TIME [epoch: 7.85 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8028470522598314		[learning rate: 0.0024088]
	Learning Rate: 0.00240877
	LOSS [training: 3.8028470522598314 | validation: 3.8489120356714226]
	TIME [epoch: 7.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824527852512594		[learning rate: 0.0024031]
	Learning Rate: 0.00240309
	LOSS [training: 3.824527852512594 | validation: 3.85993260746144]
	TIME [epoch: 7.85 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8215718183639544		[learning rate: 0.0023974]
	Learning Rate: 0.00239742
	LOSS [training: 3.8215718183639544 | validation: 3.8322058194632893]
	TIME [epoch: 7.89 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8062028112426436		[learning rate: 0.0023918]
	Learning Rate: 0.00239176
	LOSS [training: 3.8062028112426436 | validation: 3.800464057280575]
	TIME [epoch: 7.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7774260912175452		[learning rate: 0.0023861]
	Learning Rate: 0.00238612
	LOSS [training: 3.7774260912175452 | validation: 3.7695982400139982]
	TIME [epoch: 7.85 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7493749937615486		[learning rate: 0.0023805]
	Learning Rate: 0.00238049
	LOSS [training: 3.7493749937615486 | validation: 3.7162122176531316]
	TIME [epoch: 7.84 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.737250375678329		[learning rate: 0.0023749]
	Learning Rate: 0.00237488
	LOSS [training: 3.737250375678329 | validation: 3.6925692613627756]
	TIME [epoch: 7.86 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715383671948887		[learning rate: 0.0023693]
	Learning Rate: 0.00236927
	LOSS [training: 3.715383671948887 | validation: 3.683619315900774]
	TIME [epoch: 7.89 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7164380121357663		[learning rate: 0.0023637]
	Learning Rate: 0.00236369
	LOSS [training: 3.7164380121357663 | validation: 3.6802353570933706]
	TIME [epoch: 7.86 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719006472528596		[learning rate: 0.0023581]
	Learning Rate: 0.00235811
	LOSS [training: 3.719006472528596 | validation: 3.704231326747893]
	TIME [epoch: 7.86 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715453228981728		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 3.715453228981728 | validation: 3.7039079291032677]
	TIME [epoch: 7.86 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7134358989492817		[learning rate: 0.002347]
	Learning Rate: 0.002347
	LOSS [training: 3.7134358989492817 | validation: 3.6750171788167547]
	TIME [epoch: 7.85 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.705889854183302		[learning rate: 0.0023415]
	Learning Rate: 0.00234146
	LOSS [training: 3.705889854183302 | validation: 3.7357308111481498]
	TIME [epoch: 7.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.770688692473381		[learning rate: 0.0023359]
	Learning Rate: 0.00233594
	LOSS [training: 3.770688692473381 | validation: 3.8669067390761755]
	TIME [epoch: 7.87 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7952960203292534		[learning rate: 0.0023304]
	Learning Rate: 0.00233043
	LOSS [training: 3.7952960203292534 | validation: 3.8055934692295597]
	TIME [epoch: 7.85 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7606393067931356		[learning rate: 0.0023249]
	Learning Rate: 0.00232493
	LOSS [training: 3.7606393067931356 | validation: 3.726619801071179]
	TIME [epoch: 7.86 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7383275567011354		[learning rate: 0.0023194]
	Learning Rate: 0.00231945
	LOSS [training: 3.7383275567011354 | validation: 3.6932392567682895]
	TIME [epoch: 7.85 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7176309769845193		[learning rate: 0.002314]
	Learning Rate: 0.00231398
	LOSS [training: 3.7176309769845193 | validation: 3.683249419166117]
	TIME [epoch: 7.85 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7173445991170357		[learning rate: 0.0023085]
	Learning Rate: 0.00230852
	LOSS [training: 3.7173445991170357 | validation: 3.668470074141057]
	TIME [epoch: 7.87 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7108539120651405		[learning rate: 0.0023031]
	Learning Rate: 0.00230307
	LOSS [training: 3.7108539120651405 | validation: 3.6991395818775414]
	TIME [epoch: 7.85 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7632818475259757		[learning rate: 0.0022976]
	Learning Rate: 0.00229764
	LOSS [training: 3.7632818475259757 | validation: 3.7315687767870953]
	TIME [epoch: 7.85 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7664798507978006		[learning rate: 0.0022922]
	Learning Rate: 0.00229222
	LOSS [training: 3.7664798507978006 | validation: 3.759666588130697]
	TIME [epoch: 7.84 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7903123694047944		[learning rate: 0.0022868]
	Learning Rate: 0.00228681
	LOSS [training: 3.7903123694047944 | validation: 3.8103797029457187]
	TIME [epoch: 7.86 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810999457437468		[learning rate: 0.0022814]
	Learning Rate: 0.00228142
	LOSS [training: 3.810999457437468 | validation: 3.8221142948772426]
	TIME [epoch: 7.89 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.822438418949196		[learning rate: 0.002276]
	Learning Rate: 0.00227604
	LOSS [training: 3.822438418949196 | validation: 3.8438576287923034]
	TIME [epoch: 7.85 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8344805792031353		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 3.8344805792031353 | validation: 3.8517302597670176]
	TIME [epoch: 7.85 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.829688448602804		[learning rate: 0.0022653]
	Learning Rate: 0.00226531
	LOSS [training: 3.829688448602804 | validation: 3.866177641845827]
	TIME [epoch: 7.85 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842603912745603		[learning rate: 0.00226]
	Learning Rate: 0.00225997
	LOSS [training: 3.842603912745603 | validation: 3.8647800165072743]
	TIME [epoch: 7.87 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8554393560968094		[learning rate: 0.0022546]
	Learning Rate: 0.00225464
	LOSS [training: 3.8554393560968094 | validation: 3.888158922077808]
	TIME [epoch: 7.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8661568107957147		[learning rate: 0.0022493]
	Learning Rate: 0.00224932
	LOSS [training: 3.8661568107957147 | validation: 3.8972148053573616]
	TIME [epoch: 7.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8696076729697255		[learning rate: 0.002244]
	Learning Rate: 0.00224401
	LOSS [training: 3.8696076729697255 | validation: 3.907768580408951]
	TIME [epoch: 7.85 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872765895521938		[learning rate: 0.0022387]
	Learning Rate: 0.00223872
	LOSS [training: 3.872765895521938 | validation: 3.902072161482465]
	TIME [epoch: 7.85 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8729276236951344		[learning rate: 0.0022334]
	Learning Rate: 0.00223344
	LOSS [training: 3.8729276236951344 | validation: 3.9035032671742296]
	TIME [epoch: 7.83 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.874935250322992		[learning rate: 0.0022282]
	Learning Rate: 0.00222817
	LOSS [training: 3.874935250322992 | validation: 3.904803312250903]
	TIME [epoch: 7.89 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8763264772550547		[learning rate: 0.0022229]
	Learning Rate: 0.00222292
	LOSS [training: 3.8763264772550547 | validation: 3.9013888273625996]
	TIME [epoch: 7.86 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8709610326387116		[learning rate: 0.0022177]
	Learning Rate: 0.00221767
	LOSS [training: 3.8709610326387116 | validation: 3.895828237846102]
	TIME [epoch: 7.85 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.865042805704344		[learning rate: 0.0022124]
	Learning Rate: 0.00221244
	LOSS [training: 3.865042805704344 | validation: 3.8847782107468003]
	TIME [epoch: 7.86 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8704375949534593		[learning rate: 0.0022072]
	Learning Rate: 0.00220722
	LOSS [training: 3.8704375949534593 | validation: 3.905099256733487]
	TIME [epoch: 7.86 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9781711588058055		[learning rate: 0.002202]
	Learning Rate: 0.00220202
	LOSS [training: 3.9781711588058055 | validation: 4.2383724626194645]
	TIME [epoch: 7.86 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.340347219552053		[learning rate: 0.0021968]
	Learning Rate: 0.00219682
	LOSS [training: 4.340347219552053 | validation: 4.305129572255485]
	TIME [epoch: 7.89 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.430448670116247		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 4.430448670116247 | validation: 4.417299578486685]
	TIME [epoch: 7.85 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.477896416581167		[learning rate: 0.0021865]
	Learning Rate: 0.00218647
	LOSS [training: 4.477896416581167 | validation: 4.323218934115117]
	TIME [epoch: 7.86 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.396391539743682		[learning rate: 0.0021813]
	Learning Rate: 0.00218131
	LOSS [training: 4.396391539743682 | validation: 4.338613553889294]
	TIME [epoch: 7.86 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.411753862395843		[learning rate: 0.0021762]
	Learning Rate: 0.00217617
	LOSS [training: 4.411753862395843 | validation: 4.397748281096241]
	TIME [epoch: 7.85 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.458483119759876		[learning rate: 0.002171]
	Learning Rate: 0.00217103
	LOSS [training: 4.458483119759876 | validation: 4.468663610650671]
	TIME [epoch: 7.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.581658945866289		[learning rate: 0.0021659]
	Learning Rate: 0.00216591
	LOSS [training: 4.581658945866289 | validation: 4.549969051646427]
	TIME [epoch: 7.85 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.57662248507173		[learning rate: 0.0021608]
	Learning Rate: 0.0021608
	LOSS [training: 4.57662248507173 | validation: 4.482900600384282]
	TIME [epoch: 7.85 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.510657081401192		[learning rate: 0.0021557]
	Learning Rate: 0.00215571
	LOSS [training: 4.510657081401192 | validation: 4.407551064800927]
	TIME [epoch: 7.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.412689933069403		[learning rate: 0.0021506]
	Learning Rate: 0.00215062
	LOSS [training: 4.412689933069403 | validation: 4.212337774027415]
	TIME [epoch: 7.85 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.290528084102224		[learning rate: 0.0021455]
	Learning Rate: 0.00214555
	LOSS [training: 4.290528084102224 | validation: 4.300312432873964]
	TIME [epoch: 7.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.438602387579925		[learning rate: 0.0021405]
	Learning Rate: 0.00214049
	LOSS [training: 4.438602387579925 | validation: 4.318496654838234]
	TIME [epoch: 7.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.394487111172018		[learning rate: 0.0021354]
	Learning Rate: 0.00213544
	LOSS [training: 4.394487111172018 | validation: 4.27906526772561]
	TIME [epoch: 7.85 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.382711701090672		[learning rate: 0.0021304]
	Learning Rate: 0.0021304
	LOSS [training: 4.382711701090672 | validation: 4.258296630374511]
	TIME [epoch: 7.85 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.293604592225519		[learning rate: 0.0021254]
	Learning Rate: 0.00212538
	LOSS [training: 4.293604592225519 | validation: 4.1793428026548884]
	TIME [epoch: 7.85 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.162284758397605		[learning rate: 0.0021204]
	Learning Rate: 0.00212036
	LOSS [training: 4.162284758397605 | validation: 4.074096025383976]
	TIME [epoch: 7.89 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134050356197613		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 4.134050356197613 | validation: 4.049222127249324]
	TIME [epoch: 7.87 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.086116543710717		[learning rate: 0.0021104]
	Learning Rate: 0.00211037
	LOSS [training: 4.086116543710717 | validation: 3.9435760865447254]
	TIME [epoch: 7.85 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9203541609648256		[learning rate: 0.0021054]
	Learning Rate: 0.00210539
	LOSS [training: 3.9203541609648256 | validation: 3.88928154253698]
	TIME [epoch: 7.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8731939277780674		[learning rate: 0.0021004]
	Learning Rate: 0.00210043
	LOSS [training: 3.8731939277780674 | validation: 3.908368261248884]
	TIME [epoch: 7.85 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.877275032256472		[learning rate: 0.0020955]
	Learning Rate: 0.00209547
	LOSS [training: 3.877275032256472 | validation: 3.9032948853267877]
	TIME [epoch: 7.89 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8837371374448586		[learning rate: 0.0020905]
	Learning Rate: 0.00209053
	LOSS [training: 3.8837371374448586 | validation: 3.9178683066141993]
	TIME [epoch: 7.87 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8837412587687283		[learning rate: 0.0020856]
	Learning Rate: 0.0020856
	LOSS [training: 3.8837412587687283 | validation: 3.899162411936209]
	TIME [epoch: 7.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.869051646704831		[learning rate: 0.0020807]
	Learning Rate: 0.00208068
	LOSS [training: 3.869051646704831 | validation: 3.893943687528166]
	TIME [epoch: 7.86 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8686623481520446		[learning rate: 0.0020758]
	Learning Rate: 0.00207577
	LOSS [training: 3.8686623481520446 | validation: 3.9024746289659937]
	TIME [epoch: 7.85 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8752256513328436		[learning rate: 0.0020709]
	Learning Rate: 0.00207087
	LOSS [training: 3.8752256513328436 | validation: 3.9125332021697243]
	TIME [epoch: 7.87 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8785724680101454		[learning rate: 0.002066]
	Learning Rate: 0.00206599
	LOSS [training: 3.8785724680101454 | validation: 3.9130189694265107]
	TIME [epoch: 7.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8881533860735624		[learning rate: 0.0020611]
	Learning Rate: 0.00206112
	LOSS [training: 3.8881533860735624 | validation: 3.931696455126693]
	TIME [epoch: 7.86 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8992276763825595		[learning rate: 0.0020563]
	Learning Rate: 0.00205626
	LOSS [training: 3.8992276763825595 | validation: 3.925148013265953]
	TIME [epoch: 7.87 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.88455425977055		[learning rate: 0.0020514]
	Learning Rate: 0.0020514
	LOSS [training: 3.88455425977055 | validation: 3.9283021877982915]
	TIME [epoch: 7.84 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8866337273687974		[learning rate: 0.0020466]
	Learning Rate: 0.00204657
	LOSS [training: 3.8866337273687974 | validation: 3.910720069898968]
	TIME [epoch: 7.86 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8659870227808906		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 3.8659870227808906 | validation: 3.874664067324131]
	TIME [epoch: 7.89 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.856189213665252		[learning rate: 0.0020369]
	Learning Rate: 0.00203692
	LOSS [training: 3.856189213665252 | validation: 3.8546944324040764]
	TIME [epoch: 7.85 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.84004916553945		[learning rate: 0.0020321]
	Learning Rate: 0.00203212
	LOSS [training: 3.84004916553945 | validation: 3.838365977025286]
	TIME [epoch: 7.85 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8190492746724454		[learning rate: 0.0020273]
	Learning Rate: 0.00202732
	LOSS [training: 3.8190492746724454 | validation: 3.7862505297682185]
	TIME [epoch: 7.86 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8119417306093344		[learning rate: 0.0020225]
	Learning Rate: 0.00202254
	LOSS [training: 3.8119417306093344 | validation: 3.781967545818414]
	TIME [epoch: 7.86 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810087360507422		[learning rate: 0.0020178]
	Learning Rate: 0.00201777
	LOSS [training: 3.810087360507422 | validation: 3.7901729629021057]
	TIME [epoch: 7.89 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800101308156889		[learning rate: 0.002013]
	Learning Rate: 0.00201301
	LOSS [training: 3.800101308156889 | validation: 3.7684938991277397]
	TIME [epoch: 7.87 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782869895885405		[learning rate: 0.0020083]
	Learning Rate: 0.00200826
	LOSS [training: 3.782869895885405 | validation: 3.7424411997455875]
	TIME [epoch: 7.86 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777349241291234		[learning rate: 0.0020035]
	Learning Rate: 0.00200353
	LOSS [training: 3.777349241291234 | validation: 3.765107857097588]
	TIME [epoch: 7.85 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7959516312343022		[learning rate: 0.0019988]
	Learning Rate: 0.0019988
	LOSS [training: 3.7959516312343022 | validation: 3.8056904145742525]
	TIME [epoch: 7.85 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.808403365883327		[learning rate: 0.0019941]
	Learning Rate: 0.00199408
	LOSS [training: 3.808403365883327 | validation: 3.8220861849798986]
	TIME [epoch: 7.89 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819430354984293		[learning rate: 0.0019894]
	Learning Rate: 0.00198938
	LOSS [training: 3.819430354984293 | validation: 3.827133117338411]
	TIME [epoch: 7.87 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8137238632694697		[learning rate: 0.0019847]
	Learning Rate: 0.00198469
	LOSS [training: 3.8137238632694697 | validation: 3.810803335728772]
	TIME [epoch: 7.85 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.805941095849988		[learning rate: 0.00198]
	Learning Rate: 0.00198001
	LOSS [training: 3.805941095849988 | validation: 3.7793659331514107]
	TIME [epoch: 7.86 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777333949062079		[learning rate: 0.0019753]
	Learning Rate: 0.00197534
	LOSS [training: 3.777333949062079 | validation: 3.7267692079509054]
	TIME [epoch: 7.85 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.743308571850548		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 3.743308571850548 | validation: 3.707621777511358]
	TIME [epoch: 7.86 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.739179347880703		[learning rate: 0.001966]
	Learning Rate: 0.00196603
	LOSS [training: 3.739179347880703 | validation: 3.729897847636103]
	TIME [epoch: 7.88 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7607418003103255		[learning rate: 0.0019614]
	Learning Rate: 0.00196139
	LOSS [training: 3.7607418003103255 | validation: 3.7781685400523886]
	TIME [epoch: 7.85 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8015391558998974		[learning rate: 0.0019568]
	Learning Rate: 0.00195676
	LOSS [training: 3.8015391558998974 | validation: 3.818709683729778]
	TIME [epoch: 7.85 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820031343491009		[learning rate: 0.0019521]
	Learning Rate: 0.00195215
	LOSS [training: 3.820031343491009 | validation: 3.8357742609399934]
	TIME [epoch: 7.86 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8375157296637554		[learning rate: 0.0019475]
	Learning Rate: 0.00194754
	LOSS [training: 3.8375157296637554 | validation: 3.8662583043011156]
	TIME [epoch: 7.86 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858771371761369		[learning rate: 0.0019429]
	Learning Rate: 0.00194295
	LOSS [training: 3.858771371761369 | validation: 3.874045604332782]
	TIME [epoch: 7.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854201811251792		[learning rate: 0.0019384]
	Learning Rate: 0.00193837
	LOSS [training: 3.854201811251792 | validation: 3.8626718129240842]
	TIME [epoch: 7.86 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850072702880412		[learning rate: 0.0019338]
	Learning Rate: 0.00193379
	LOSS [training: 3.850072702880412 | validation: 3.83429067782259]
	TIME [epoch: 7.84 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8246557375397554		[learning rate: 0.0019292]
	Learning Rate: 0.00192923
	LOSS [training: 3.8246557375397554 | validation: 3.7913276576777633]
	TIME [epoch: 7.86 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.791887665425172		[learning rate: 0.0019247]
	Learning Rate: 0.00192468
	LOSS [training: 3.791887665425172 | validation: 3.7493912204721713]
	TIME [epoch: 7.86 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768502817539327		[learning rate: 0.0019201]
	Learning Rate: 0.00192014
	LOSS [training: 3.768502817539327 | validation: 3.7266216789815187]
	TIME [epoch: 7.85 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763634010262012		[learning rate: 0.0019156]
	Learning Rate: 0.00191561
	LOSS [training: 3.763634010262012 | validation: 3.7313699536503866]
	TIME [epoch: 7.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7729197257869087		[learning rate: 0.0019111]
	Learning Rate: 0.00191109
	LOSS [training: 3.7729197257869087 | validation: 3.733349633957549]
	TIME [epoch: 7.86 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7784960655202307		[learning rate: 0.0019066]
	Learning Rate: 0.00190659
	LOSS [training: 3.7784960655202307 | validation: 3.7365914550197648]
	TIME [epoch: 7.84 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783956397569198		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 3.783956397569198 | validation: 3.763572438158158]
	TIME [epoch: 7.86 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8066824594781923		[learning rate: 0.0018976]
	Learning Rate: 0.0018976
	LOSS [training: 3.8066824594781923 | validation: 3.7956289115671487]
	TIME [epoch: 7.89 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820541798773606		[learning rate: 0.0018931]
	Learning Rate: 0.00189313
	LOSS [training: 3.820541798773606 | validation: 3.8103017150038756]
	TIME [epoch: 7.87 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8317302161986446		[learning rate: 0.0018887]
	Learning Rate: 0.00188866
	LOSS [training: 3.8317302161986446 | validation: 3.819702478279221]
	TIME [epoch: 7.86 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826738103414181		[learning rate: 0.0018842]
	Learning Rate: 0.00188421
	LOSS [training: 3.826738103414181 | validation: 3.7796559792861535]
	TIME [epoch: 7.86 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.80204146023091		[learning rate: 0.0018798]
	Learning Rate: 0.00187976
	LOSS [training: 3.80204146023091 | validation: 3.777650946948188]
	TIME [epoch: 7.86 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8066161986007705		[learning rate: 0.0018753]
	Learning Rate: 0.00187533
	LOSS [training: 3.8066161986007705 | validation: 3.8073045888606307]
	TIME [epoch: 7.88 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8327313282194932		[learning rate: 0.0018709]
	Learning Rate: 0.0018709
	LOSS [training: 3.8327313282194932 | validation: 3.8434924139235855]
	TIME [epoch: 7.88 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8476002862733156		[learning rate: 0.0018665]
	Learning Rate: 0.00186649
	LOSS [training: 3.8476002862733156 | validation: 3.842632463507224]
	TIME [epoch: 7.86 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8408576683761506		[learning rate: 0.0018621]
	Learning Rate: 0.00186209
	LOSS [training: 3.8408576683761506 | validation: 3.832525770895418]
	TIME [epoch: 7.86 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8482551329374473		[learning rate: 0.0018577]
	Learning Rate: 0.00185769
	LOSS [training: 3.8482551329374473 | validation: 3.8638156917975057]
	TIME [epoch: 7.85 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8552521262080983		[learning rate: 0.0018533]
	Learning Rate: 0.00185331
	LOSS [training: 3.8552521262080983 | validation: 3.8403444408049077]
	TIME [epoch: 7.86 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8325558691493247		[learning rate: 0.0018489]
	Learning Rate: 0.00184894
	LOSS [training: 3.8325558691493247 | validation: 3.808666280211905]
	TIME [epoch: 7.91 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8203498330534313		[learning rate: 0.0018446]
	Learning Rate: 0.00184458
	LOSS [training: 3.8203498330534313 | validation: 3.814506360752489]
	TIME [epoch: 7.86 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.837021487941352		[learning rate: 0.0018402]
	Learning Rate: 0.00184023
	LOSS [training: 3.837021487941352 | validation: 3.830997087026753]
	TIME [epoch: 7.84 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8486152456019105		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 3.8486152456019105 | validation: 3.855924689586218]
	TIME [epoch: 7.85 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850248588737131		[learning rate: 0.0018316]
	Learning Rate: 0.00183156
	LOSS [training: 3.850248588737131 | validation: 3.831975087145085]
	TIME [epoch: 7.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.833470081075621		[learning rate: 0.0018272]
	Learning Rate: 0.00182724
	LOSS [training: 3.833470081075621 | validation: 3.8237431600033864]
	TIME [epoch: 7.88 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.829713630763687		[learning rate: 0.0018229]
	Learning Rate: 0.00182293
	LOSS [training: 3.829713630763687 | validation: 3.8304775494061243]
	TIME [epoch: 7.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.839381473940869		[learning rate: 0.0018186]
	Learning Rate: 0.00181863
	LOSS [training: 3.839381473940869 | validation: 3.8579191726506123]
	TIME [epoch: 7.83 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.849520529450058		[learning rate: 0.0018143]
	Learning Rate: 0.00181434
	LOSS [training: 3.849520529450058 | validation: 3.823741352164798]
	TIME [epoch: 7.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821173434323134		[learning rate: 0.0018101]
	Learning Rate: 0.00181006
	LOSS [training: 3.821173434323134 | validation: 3.807791479730649]
	TIME [epoch: 7.86 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8160969397563846		[learning rate: 0.0018058]
	Learning Rate: 0.00180579
	LOSS [training: 3.8160969397563846 | validation: 3.7712055358001857]
	TIME [epoch: 7.88 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7892675172685397		[learning rate: 0.0018015]
	Learning Rate: 0.00180153
	LOSS [training: 3.7892675172685397 | validation: 3.769621499739853]
	TIME [epoch: 7.86 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7984655306191053		[learning rate: 0.0017973]
	Learning Rate: 0.00179728
	LOSS [training: 3.7984655306191053 | validation: 3.7813711502634675]
	TIME [epoch: 7.86 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.796630030459965		[learning rate: 0.001793]
	Learning Rate: 0.00179304
	LOSS [training: 3.796630030459965 | validation: 3.7501287994145747]
	TIME [epoch: 7.85 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7689029370178058		[learning rate: 0.0017888]
	Learning Rate: 0.00178881
	LOSS [training: 3.7689029370178058 | validation: 3.745621718807965]
	TIME [epoch: 7.85 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7797423930362624		[learning rate: 0.0017846]
	Learning Rate: 0.00178459
	LOSS [training: 3.7797423930362624 | validation: 3.770779856658026]
	TIME [epoch: 7.86 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8011966579849306		[learning rate: 0.0017804]
	Learning Rate: 0.00178038
	LOSS [training: 3.8011966579849306 | validation: 3.791861553867248]
	TIME [epoch: 7.87 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8323340456085178		[learning rate: 0.0017762]
	Learning Rate: 0.00177618
	LOSS [training: 3.8323340456085178 | validation: 3.8339568126736747]
	TIME [epoch: 7.85 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.884464437084789		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 3.884464437084789 | validation: 3.9005801109766187]
	TIME [epoch: 7.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.969775661763561		[learning rate: 0.0017678]
	Learning Rate: 0.00176781
	LOSS [training: 3.969775661763561 | validation: 4.037204257687407]
	TIME [epoch: 7.85 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.369998879375664		[learning rate: 0.0017636]
	Learning Rate: 0.00176364
	LOSS [training: 4.369998879375664 | validation: 4.503541409668879]
	TIME [epoch: 7.86 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6340516214594825		[learning rate: 0.0017595]
	Learning Rate: 0.00175948
	LOSS [training: 4.6340516214594825 | validation: 4.490948918874926]
	TIME [epoch: 7.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.628790463091637		[learning rate: 0.0017553]
	Learning Rate: 0.00175533
	LOSS [training: 4.628790463091637 | validation: 4.4875128750915545]
	TIME [epoch: 7.85 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.619581431695009		[learning rate: 0.0017512]
	Learning Rate: 0.00175119
	LOSS [training: 4.619581431695009 | validation: 4.4275767463916695]
	TIME [epoch: 7.85 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.653295977039091		[learning rate: 0.0017471]
	Learning Rate: 0.00174706
	LOSS [training: 4.653295977039091 | validation: 4.543072396783055]
	TIME [epoch: 7.84 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.621087278185767		[learning rate: 0.0017429]
	Learning Rate: 0.00174294
	LOSS [training: 4.621087278185767 | validation: 4.406449277818373]
	TIME [epoch: 7.85 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.600826440465714		[learning rate: 0.0017388]
	Learning Rate: 0.00173883
	LOSS [training: 4.600826440465714 | validation: 4.470662318878807]
	TIME [epoch: 7.89 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.695887773217011		[learning rate: 0.0017347]
	Learning Rate: 0.00173473
	LOSS [training: 4.695887773217011 | validation: 4.654106787838679]
	TIME [epoch: 7.85 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.840414342702792		[learning rate: 0.0017306]
	Learning Rate: 0.00173063
	LOSS [training: 4.840414342702792 | validation: 4.730020852484685]
	TIME [epoch: 7.86 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826707541735298		[learning rate: 0.0017266]
	Learning Rate: 0.00172655
	LOSS [training: 4.826707541735298 | validation: 4.698467358197059]
	TIME [epoch: 7.85 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878904752684597		[learning rate: 0.0017225]
	Learning Rate: 0.00172248
	LOSS [training: 4.878904752684597 | validation: 4.7795872831474435]
	TIME [epoch: 7.85 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9094925714052895		[learning rate: 0.0017184]
	Learning Rate: 0.00171842
	LOSS [training: 4.9094925714052895 | validation: 4.811214468675652]
	TIME [epoch: 7.88 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.950494817633579		[learning rate: 0.0017144]
	Learning Rate: 0.00171436
	LOSS [training: 4.950494817633579 | validation: 4.878483799257852]
	TIME [epoch: 7.85 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024346924807052		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 5.024346924807052 | validation: 4.959965133633842]
	TIME [epoch: 7.84 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063147510353174		[learning rate: 0.0017063]
	Learning Rate: 0.00170628
	LOSS [training: 5.063147510353174 | validation: 5.010538410561708]
	TIME [epoch: 7.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.062151518650461		[learning rate: 0.0017023]
	Learning Rate: 0.00170226
	LOSS [training: 5.062151518650461 | validation: 4.773637610784471]
	TIME [epoch: 7.86 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.932190473968116		[learning rate: 0.0016982]
	Learning Rate: 0.00169824
	LOSS [training: 4.932190473968116 | validation: 4.848210344417306]
	TIME [epoch: 7.83 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.942554203791634		[learning rate: 0.0016942]
	Learning Rate: 0.00169424
	LOSS [training: 4.942554203791634 | validation: 4.801227938013571]
	TIME [epoch: 7.85 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.931457361028274		[learning rate: 0.0016902]
	Learning Rate: 0.00169024
	LOSS [training: 4.931457361028274 | validation: 4.800685430384981]
	TIME [epoch: 7.84 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9212194848743565		[learning rate: 0.0016863]
	Learning Rate: 0.00168625
	LOSS [training: 4.9212194848743565 | validation: 4.76928279701722]
	TIME [epoch: 7.8 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873674626471465		[learning rate: 0.0016823]
	Learning Rate: 0.00168228
	LOSS [training: 4.873674626471465 | validation: 4.7661328171024975]
	TIME [epoch: 7.82 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.876003462608915		[learning rate: 0.0016783]
	Learning Rate: 0.00167831
	LOSS [training: 4.876003462608915 | validation: 4.8034497046071305]
	TIME [epoch: 7.85 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.921113622358062		[learning rate: 0.0016743]
	Learning Rate: 0.00167435
	LOSS [training: 4.921113622358062 | validation: 4.825875539242325]
	TIME [epoch: 7.86 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8645798626649555		[learning rate: 0.0016704]
	Learning Rate: 0.0016704
	LOSS [training: 4.8645798626649555 | validation: 4.783020011337454]
	TIME [epoch: 7.83 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889180425887647		[learning rate: 0.0016665]
	Learning Rate: 0.00166646
	LOSS [training: 4.889180425887647 | validation: 4.952414019171474]
	TIME [epoch: 7.83 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.973453848001065		[learning rate: 0.0016625]
	Learning Rate: 0.00166253
	LOSS [training: 4.973453848001065 | validation: 4.977514139413325]
	TIME [epoch: 7.81 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.988370720083266		[learning rate: 0.0016586]
	Learning Rate: 0.00165861
	LOSS [training: 4.988370720083266 | validation: 4.981489226243063]
	TIME [epoch: 7.82 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063459615967146		[learning rate: 0.0016547]
	Learning Rate: 0.0016547
	LOSS [training: 5.063459615967146 | validation: 5.173217597893579]
	TIME [epoch: 7.88 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.177135462752798		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 5.177135462752798 | validation: 5.2159729829132315]
	TIME [epoch: 7.81 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.213197707793718		[learning rate: 0.0016469]
	Learning Rate: 0.0016469
	LOSS [training: 5.213197707793718 | validation: 5.252026267370572]
	TIME [epoch: 7.82 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.229220889216766		[learning rate: 0.001643]
	Learning Rate: 0.00164301
	LOSS [training: 5.229220889216766 | validation: 5.231183956593171]
	TIME [epoch: 7.82 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.222298155857877		[learning rate: 0.0016391]
	Learning Rate: 0.00163914
	LOSS [training: 5.222298155857877 | validation: 5.255405249591147]
	TIME [epoch: 7.82 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.258147069337114		[learning rate: 0.0016353]
	Learning Rate: 0.00163527
	LOSS [training: 5.258147069337114 | validation: 5.252342805379541]
	TIME [epoch: 7.86 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.179077706706012		[learning rate: 0.0016314]
	Learning Rate: 0.00163141
	LOSS [training: 5.179077706706012 | validation: 5.070049651483016]
	TIME [epoch: 7.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.085206590643523		[learning rate: 0.0016276]
	Learning Rate: 0.00162757
	LOSS [training: 5.085206590643523 | validation: 5.089265693525949]
	TIME [epoch: 7.82 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.110423454085927		[learning rate: 0.0016237]
	Learning Rate: 0.00162373
	LOSS [training: 5.110423454085927 | validation: 5.1697839695577965]
	TIME [epoch: 7.84 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.219718413999681		[learning rate: 0.0016199]
	Learning Rate: 0.0016199
	LOSS [training: 5.219718413999681 | validation: 5.2873002061909045]
	TIME [epoch: 7.81 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.327869991081664		[learning rate: 0.0016161]
	Learning Rate: 0.00161608
	LOSS [training: 5.327869991081664 | validation: 5.349182188884228]
	TIME [epoch: 7.84 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3269143034651085		[learning rate: 0.0016123]
	Learning Rate: 0.00161226
	LOSS [training: 5.3269143034651085 | validation: 5.2970117562655465]
	TIME [epoch: 7.88 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2114765929556635		[learning rate: 0.0016085]
	Learning Rate: 0.00160846
	LOSS [training: 5.2114765929556635 | validation: 5.122376991923197]
	TIME [epoch: 7.81 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.208247338965141		[learning rate: 0.0016047]
	Learning Rate: 0.00160467
	LOSS [training: 5.208247338965141 | validation: 5.2757672477987985]
	TIME [epoch: 7.85 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.289988621574366		[learning rate: 0.0016009]
	Learning Rate: 0.00160088
	LOSS [training: 5.289988621574366 | validation: 5.347291226293711]
	TIME [epoch: 7.84 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.323964664553622		[learning rate: 0.0015971]
	Learning Rate: 0.0015971
	LOSS [training: 5.323964664553622 | validation: 5.367438830548078]
	TIME [epoch: 7.83 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.357811302998038		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 5.357811302998038 | validation: 5.416930092658854]
	TIME [epoch: 7.86 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.361384375503959		[learning rate: 0.0015896]
	Learning Rate: 0.00158958
	LOSS [training: 5.361384375503959 | validation: 5.3527928809971534]
	TIME [epoch: 7.82 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.307610594234278		[learning rate: 0.0015858]
	Learning Rate: 0.00158583
	LOSS [training: 5.307610594234278 | validation: 5.310770380412406]
	TIME [epoch: 7.82 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.270627571378212		[learning rate: 0.0015821]
	Learning Rate: 0.00158209
	LOSS [training: 5.270627571378212 | validation: 5.270831602900195]
	TIME [epoch: 7.82 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.25159486786057		[learning rate: 0.0015784]
	Learning Rate: 0.00157836
	LOSS [training: 5.25159486786057 | validation: 5.233714591466338]
	TIME [epoch: 7.86 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.197532084757674		[learning rate: 0.0015746]
	Learning Rate: 0.00157463
	LOSS [training: 5.197532084757674 | validation: 5.206474321931237]
	TIME [epoch: 7.86 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.178305087738109		[learning rate: 0.0015709]
	Learning Rate: 0.00157092
	LOSS [training: 5.178305087738109 | validation: 5.208886364065922]
	TIME [epoch: 7.83 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.176819894382207		[learning rate: 0.0015672]
	Learning Rate: 0.00156721
	LOSS [training: 5.176819894382207 | validation: 5.163910350820256]
	TIME [epoch: 7.82 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.134604409818952		[learning rate: 0.0015635]
	Learning Rate: 0.00156352
	LOSS [training: 5.134604409818952 | validation: 5.174551397979824]
	TIME [epoch: 7.81 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.193869353599514		[learning rate: 0.0015598]
	Learning Rate: 0.00155983
	LOSS [training: 5.193869353599514 | validation: 5.196630593129441]
	TIME [epoch: 7.82 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.152128066691146		[learning rate: 0.0015561]
	Learning Rate: 0.00155615
	LOSS [training: 5.152128066691146 | validation: 5.03761512962299]
	TIME [epoch: 7.87 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028072144119498		[learning rate: 0.0015525]
	Learning Rate: 0.00155248
	LOSS [training: 5.028072144119498 | validation: 4.918492434356052]
	TIME [epoch: 7.84 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906298443653365		[learning rate: 0.0015488]
	Learning Rate: 0.00154882
	LOSS [training: 4.906298443653365 | validation: 4.752682110628017]
	TIME [epoch: 7.85 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7890413912672845		[learning rate: 0.0015452]
	Learning Rate: 0.00154516
	LOSS [training: 4.7890413912672845 | validation: 4.750379289540529]
	TIME [epoch: 7.82 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794390435360642		[learning rate: 0.0015415]
	Learning Rate: 0.00154152
	LOSS [training: 4.794390435360642 | validation: 4.707191259056846]
	TIME [epoch: 7.82 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7084896470408815		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 4.7084896470408815 | validation: 4.604884702206206]
	TIME [epoch: 7.83 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.617930905086071		[learning rate: 0.0015343]
	Learning Rate: 0.00153425
	LOSS [training: 4.617930905086071 | validation: 4.529373823803594]
	TIME [epoch: 7.89 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.597916339762708		[learning rate: 0.0015306]
	Learning Rate: 0.00153064
	LOSS [training: 4.597916339762708 | validation: 4.548203905124531]
	TIME [epoch: 7.81 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.596418359671009		[learning rate: 0.001527]
	Learning Rate: 0.00152703
	LOSS [training: 4.596418359671009 | validation: 4.52026622351995]
	TIME [epoch: 7.82 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.653443379121464		[learning rate: 0.0015234]
	Learning Rate: 0.00152342
	LOSS [training: 4.653443379121464 | validation: 4.570601823259592]
	TIME [epoch: 7.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.715229113413663		[learning rate: 0.0015198]
	Learning Rate: 0.00151983
	LOSS [training: 4.715229113413663 | validation: 4.640333209786477]
	TIME [epoch: 7.82 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.734018580641182		[learning rate: 0.0015162]
	Learning Rate: 0.00151624
	LOSS [training: 4.734018580641182 | validation: 4.615425406276953]
	TIME [epoch: 7.86 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.748072096444351		[learning rate: 0.0015127]
	Learning Rate: 0.00151267
	LOSS [training: 4.748072096444351 | validation: 4.675111260450022]
	TIME [epoch: 7.81 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8224562102029775		[learning rate: 0.0015091]
	Learning Rate: 0.0015091
	LOSS [training: 4.8224562102029775 | validation: 4.7500861755552775]
	TIME [epoch: 7.81 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8229029181697305		[learning rate: 0.0015055]
	Learning Rate: 0.00150554
	LOSS [training: 4.8229029181697305 | validation: 4.736821181666247]
	TIME [epoch: 7.85 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.846136258723244		[learning rate: 0.001502]
	Learning Rate: 0.00150199
	LOSS [training: 4.846136258723244 | validation: 4.777689056010633]
	TIME [epoch: 7.82 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.893511377908757		[learning rate: 0.0014984]
	Learning Rate: 0.00149845
	LOSS [training: 4.893511377908757 | validation: 4.784426838079806]
	TIME [epoch: 7.89 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.848254032585505		[learning rate: 0.0014949]
	Learning Rate: 0.00149491
	LOSS [training: 4.848254032585505 | validation: 4.798900922312987]
	TIME [epoch: 7.87 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.857572647263252		[learning rate: 0.0014914]
	Learning Rate: 0.00149139
	LOSS [training: 4.857572647263252 | validation: 4.719010803520664]
	TIME [epoch: 7.82 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.790824118638827		[learning rate: 0.0014879]
	Learning Rate: 0.00148787
	LOSS [training: 4.790824118638827 | validation: 4.739447313269893]
	TIME [epoch: 7.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.844788975796744		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 4.844788975796744 | validation: 4.730618903067087]
	TIME [epoch: 7.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873575422515792		[learning rate: 0.0014809]
	Learning Rate: 0.00148086
	LOSS [training: 4.873575422515792 | validation: 4.857616432313143]
	TIME [epoch: 7.88 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.92426339729549		[learning rate: 0.0014774]
	Learning Rate: 0.00147736
	LOSS [training: 4.92426339729549 | validation: 4.802130233524963]
	TIME [epoch: 7.83 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901227288228019		[learning rate: 0.0014739]
	Learning Rate: 0.00147388
	LOSS [training: 4.901227288228019 | validation: 4.8092425423389304]
	TIME [epoch: 7.81 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.940417242789351		[learning rate: 0.0014704]
	Learning Rate: 0.0014704
	LOSS [training: 4.940417242789351 | validation: 4.870608532338457]
	TIME [epoch: 7.81 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011195499092561		[learning rate: 0.0014669]
	Learning Rate: 0.00146693
	LOSS [training: 5.011195499092561 | validation: 4.865210943325179]
	TIME [epoch: 7.8 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.017300442866136		[learning rate: 0.0014635]
	Learning Rate: 0.00146347
	LOSS [training: 5.017300442866136 | validation: 4.876885119571664]
	TIME [epoch: 7.84 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.981960608677081		[learning rate: 0.00146]
	Learning Rate: 0.00146002
	LOSS [training: 4.981960608677081 | validation: 4.764418930224652]
	TIME [epoch: 7.86 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.929216437744708		[learning rate: 0.0014566]
	Learning Rate: 0.00145658
	LOSS [training: 4.929216437744708 | validation: 4.756464095464586]
	TIME [epoch: 7.83 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.88180541538901		[learning rate: 0.0014531]
	Learning Rate: 0.00145314
	LOSS [training: 4.88180541538901 | validation: 4.641274315890972]
	TIME [epoch: 7.8 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8113406067256905		[learning rate: 0.0014497]
	Learning Rate: 0.00144971
	LOSS [training: 4.8113406067256905 | validation: 4.622498690565224]
	TIME [epoch: 7.81 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7841275829980905		[learning rate: 0.0014463]
	Learning Rate: 0.00144629
	LOSS [training: 4.7841275829980905 | validation: 4.5524197835331]
	TIME [epoch: 7.83 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7167974137131115		[learning rate: 0.0014429]
	Learning Rate: 0.00144288
	LOSS [training: 4.7167974137131115 | validation: 4.47810628469706]
	TIME [epoch: 7.89 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6751086513398805		[learning rate: 0.0014395]
	Learning Rate: 0.00143948
	LOSS [training: 4.6751086513398805 | validation: 4.50503410306748]
	TIME [epoch: 7.81 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6965597444575575		[learning rate: 0.0014361]
	Learning Rate: 0.00143608
	LOSS [training: 4.6965597444575575 | validation: 4.527940791917484]
	TIME [epoch: 7.86 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7650307149003925		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 4.7650307149003925 | validation: 4.6722765796413155]
	TIME [epoch: 7.85 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.872870545618078		[learning rate: 0.0014293]
	Learning Rate: 0.00142932
	LOSS [training: 4.872870545618078 | validation: 4.72480996957065]
	TIME [epoch: 7.81 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.915760121070647		[learning rate: 0.0014259]
	Learning Rate: 0.00142594
	LOSS [training: 4.915760121070647 | validation: 4.744172852548399]
	TIME [epoch: 7.83 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9329677128727685		[learning rate: 0.0014226]
	Learning Rate: 0.00142258
	LOSS [training: 4.9329677128727685 | validation: 4.823393553066872]
	TIME [epoch: 7.87 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030817569038303		[learning rate: 0.0014192]
	Learning Rate: 0.00141923
	LOSS [training: 5.030817569038303 | validation: 4.954560260862964]
	TIME [epoch: 7.8 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1215059687959235		[learning rate: 0.0014159]
	Learning Rate: 0.00141588
	LOSS [training: 5.1215059687959235 | validation: 5.071909795334015]
	TIME [epoch: 7.81 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.234225847471924		[learning rate: 0.0014125]
	Learning Rate: 0.00141254
	LOSS [training: 5.234225847471924 | validation: 5.242176809056373]
	TIME [epoch: 7.79 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.316320209625866		[learning rate: 0.0014092]
	Learning Rate: 0.00140921
	LOSS [training: 5.316320209625866 | validation: 5.220186124622915]
	TIME [epoch: 7.84 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.277603839961561		[learning rate: 0.0014059]
	Learning Rate: 0.00140588
	LOSS [training: 5.277603839961561 | validation: 5.207349995463692]
	TIME [epoch: 7.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2916508621573195		[learning rate: 0.0014026]
	Learning Rate: 0.00140257
	LOSS [training: 5.2916508621573195 | validation: 5.221403978274159]
	TIME [epoch: 7.82 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.30997799134523		[learning rate: 0.0013993]
	Learning Rate: 0.00139926
	LOSS [training: 5.30997799134523 | validation: 5.202298069844536]
	TIME [epoch: 7.8 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2710451345889595		[learning rate: 0.001396]
	Learning Rate: 0.00139596
	LOSS [training: 5.2710451345889595 | validation: 5.13383275812377]
	TIME [epoch: 7.82 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.218488321044248		[learning rate: 0.0013927]
	Learning Rate: 0.00139266
	LOSS [training: 5.218488321044248 | validation: 5.153210703714121]
	TIME [epoch: 7.82 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.294723571891329		[learning rate: 0.0013894]
	Learning Rate: 0.00138938
	LOSS [training: 5.294723571891329 | validation: 5.2646950489119515]
	TIME [epoch: 7.88 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.32623857422531		[learning rate: 0.0013861]
	Learning Rate: 0.0013861
	LOSS [training: 5.32623857422531 | validation: 5.1721389935529105]
	TIME [epoch: 7.83 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.286343058646844		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 5.286343058646844 | validation: 5.226377511271406]
	TIME [epoch: 7.79 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.311410729544263		[learning rate: 0.0013796]
	Learning Rate: 0.00137957
	LOSS [training: 5.311410729544263 | validation: 5.193983479988374]
	TIME [epoch: 7.84 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.238659934350421		[learning rate: 0.0013763]
	Learning Rate: 0.00137632
	LOSS [training: 5.238659934350421 | validation: 5.081710048636161]
	TIME [epoch: 7.86 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.213169775695734		[learning rate: 0.0013731]
	Learning Rate: 0.00137307
	LOSS [training: 5.213169775695734 | validation: 5.0817399595658275]
	TIME [epoch: 7.89 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.221198669887636		[learning rate: 0.0013698]
	Learning Rate: 0.00136983
	LOSS [training: 5.221198669887636 | validation: 5.121380062037533]
	TIME [epoch: 7.8 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.279815892106686		[learning rate: 0.0013666]
	Learning Rate: 0.0013666
	LOSS [training: 5.279815892106686 | validation: 5.258198579125818]
	TIME [epoch: 7.85 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.382439512166598		[learning rate: 0.0013634]
	Learning Rate: 0.00136338
	LOSS [training: 5.382439512166598 | validation: 5.351024559824479]
	TIME [epoch: 7.81 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.420434452793984		[learning rate: 0.0013602]
	Learning Rate: 0.00136016
	LOSS [training: 5.420434452793984 | validation: 5.376351919275933]
	TIME [epoch: 7.8 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.423254529311256		[learning rate: 0.001357]
	Learning Rate: 0.00135695
	LOSS [training: 5.423254529311256 | validation: 5.334819148120864]
	TIME [epoch: 7.88 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.371288251252318		[learning rate: 0.0013538]
	Learning Rate: 0.00135375
	LOSS [training: 5.371288251252318 | validation: 5.35008272845831]
	TIME [epoch: 7.87 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.455109766825982		[learning rate: 0.0013506]
	Learning Rate: 0.00135056
	LOSS [training: 5.455109766825982 | validation: 5.484049741313242]
	TIME [epoch: 7.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5366931300266415		[learning rate: 0.0013474]
	Learning Rate: 0.00134737
	LOSS [training: 5.5366931300266415 | validation: 5.5456980319844735]
	TIME [epoch: 7.8 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.57664786318988		[learning rate: 0.0013442]
	Learning Rate: 0.00134419
	LOSS [training: 5.57664786318988 | validation: 5.560005936785458]
	TIME [epoch: 7.82 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.578907424213785		[learning rate: 0.001341]
	Learning Rate: 0.00134102
	LOSS [training: 5.578907424213785 | validation: 5.566960557005718]
	TIME [epoch: 7.85 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.6142855986924785		[learning rate: 0.0013379]
	Learning Rate: 0.00133786
	LOSS [training: 5.6142855986924785 | validation: 5.664718574912615]
	TIME [epoch: 7.83 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.730154611308009		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 5.730154611308009 | validation: 5.8080340337209595]
	TIME [epoch: 7.81 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.83014873923851		[learning rate: 0.0013316]
	Learning Rate: 0.00133155
	LOSS [training: 5.83014873923851 | validation: 5.8423688371069815]
	TIME [epoch: 7.81 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.854563785530284		[learning rate: 0.0013284]
	Learning Rate: 0.00132841
	LOSS [training: 5.854563785530284 | validation: 5.905390863891595]
	TIME [epoch: 7.85 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.940441869351616		[learning rate: 0.0013253]
	Learning Rate: 0.00132528
	LOSS [training: 5.940441869351616 | validation: 5.95415796198259]
	TIME [epoch: 7.85 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.953635175504063		[learning rate: 0.0013222]
	Learning Rate: 0.00132215
	LOSS [training: 5.953635175504063 | validation: 5.979023317196223]
	TIME [epoch: 7.84 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.974804130051412		[learning rate: 0.001319]
	Learning Rate: 0.00131904
	LOSS [training: 5.974804130051412 | validation: 5.97688186049438]
	TIME [epoch: 7.85 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.966634074133822		[learning rate: 0.0013159]
	Learning Rate: 0.00131592
	LOSS [training: 5.966634074133822 | validation: 5.971281281063048]
	TIME [epoch: 7.81 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.958954175784168		[learning rate: 0.0013128]
	Learning Rate: 0.00131282
	LOSS [training: 5.958954175784168 | validation: 5.961154989172778]
	TIME [epoch: 7.82 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.94294683146556		[learning rate: 0.0013097]
	Learning Rate: 0.00130972
	LOSS [training: 5.94294683146556 | validation: 5.941625755827269]
	TIME [epoch: 7.85 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.917038166861261		[learning rate: 0.0013066]
	Learning Rate: 0.00130663
	LOSS [training: 5.917038166861261 | validation: 5.904709481137555]
	TIME [epoch: 7.84 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.906949717211237		[learning rate: 0.0013036]
	Learning Rate: 0.00130355
	LOSS [training: 5.906949717211237 | validation: 5.922559299708903]
	TIME [epoch: 7.85 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.9077825302559015		[learning rate: 0.0013005]
	Learning Rate: 0.00130048
	LOSS [training: 5.9077825302559015 | validation: 5.903043824563424]
	TIME [epoch: 7.81 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.883658643268941		[learning rate: 0.0012974]
	Learning Rate: 0.00129741
	LOSS [training: 5.883658643268941 | validation: 5.871536322714174]
	TIME [epoch: 7.82 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.8760358986078565		[learning rate: 0.0012943]
	Learning Rate: 0.00129435
	LOSS [training: 5.8760358986078565 | validation: 5.815061837131111]
	TIME [epoch: 7.89 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.8365315895467385		[learning rate: 0.0012913]
	Learning Rate: 0.0012913
	LOSS [training: 5.8365315895467385 | validation: 5.83678353973616]
	TIME [epoch: 7.84 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.837055011295524		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 5.837055011295524 | validation: 5.871387986879682]
	TIME [epoch: 7.84 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.886888418507352		[learning rate: 0.0012852]
	Learning Rate: 0.00128521
	LOSS [training: 5.886888418507352 | validation: 5.884257707021289]
	TIME [epoch: 7.84 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.884927097941288		[learning rate: 0.0012822]
	Learning Rate: 0.00128218
	LOSS [training: 5.884927097941288 | validation: 5.898106781377493]
	TIME [epoch: 7.82 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.903066026897363		[learning rate: 0.0012792]
	Learning Rate: 0.00127915
	LOSS [training: 5.903066026897363 | validation: 6.005579618347095]
	TIME [epoch: 7.84 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.005213249782177		[learning rate: 0.0012761]
	Learning Rate: 0.00127614
	LOSS [training: 6.005213249782177 | validation: 6.019949194177864]
	TIME [epoch: 7.85 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.004964244040128		[learning rate: 0.0012731]
	Learning Rate: 0.00127313
	LOSS [training: 6.004964244040128 | validation: 6.009927222119404]
	TIME [epoch: 7.85 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.994804736003706		[learning rate: 0.0012701]
	Learning Rate: 0.00127012
	LOSS [training: 5.994804736003706 | validation: 5.995656206388321]
	TIME [epoch: 7.81 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.980827712070189		[learning rate: 0.0012671]
	Learning Rate: 0.00126713
	LOSS [training: 5.980827712070189 | validation: 5.972117174847364]
	TIME [epoch: 7.85 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.963847870049885		[learning rate: 0.0012641]
	Learning Rate: 0.00126414
	LOSS [training: 5.963847870049885 | validation: 5.95897780983034]
	TIME [epoch: 7.85 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.944885422912606		[learning rate: 0.0012612]
	Learning Rate: 0.00126116
	LOSS [training: 5.944885422912606 | validation: 5.94288360265972]
	TIME [epoch: 7.85 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.9174323383353995		[learning rate: 0.0012582]
	Learning Rate: 0.00125818
	LOSS [training: 5.9174323383353995 | validation: 5.903248326152799]
	TIME [epoch: 7.85 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.893924308064831		[learning rate: 0.0012552]
	Learning Rate: 0.00125521
	LOSS [training: 5.893924308064831 | validation: 5.891147002761809]
	TIME [epoch: 7.81 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.888086389997383		[learning rate: 0.0012523]
	Learning Rate: 0.00125225
	LOSS [training: 5.888086389997383 | validation: 5.8751847066291125]
	TIME [epoch: 7.82 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.855947554295108		[learning rate: 0.0012493]
	Learning Rate: 0.0012493
	LOSS [training: 5.855947554295108 | validation: 5.835979350631849]
	TIME [epoch: 7.86 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.821054713284985		[learning rate: 0.0012464]
	Learning Rate: 0.00124635
	LOSS [training: 5.821054713284985 | validation: 5.794427773025829]
	TIME [epoch: 7.84 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.7817477299758435		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 5.7817477299758435 | validation: 5.751116135128398]
	TIME [epoch: 7.82 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.737672651558814		[learning rate: 0.0012405]
	Learning Rate: 0.00124048
	LOSS [training: 5.737672651558814 | validation: 5.698210834269349]
	TIME [epoch: 7.86 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.689809092753912		[learning rate: 0.0012376]
	Learning Rate: 0.00123755
	LOSS [training: 5.689809092753912 | validation: 5.621125653163898]
	TIME [epoch: 7.82 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.622014784599186		[learning rate: 0.0012346]
	Learning Rate: 0.00123463
	LOSS [training: 5.622014784599186 | validation: 5.549779130553448]
	TIME [epoch: 7.89 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.569286674231513		[learning rate: 0.0012317]
	Learning Rate: 0.00123172
	LOSS [training: 5.569286674231513 | validation: 5.528251412761735]
	TIME [epoch: 7.87 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.594572174567892		[learning rate: 0.0012288]
	Learning Rate: 0.00122882
	LOSS [training: 5.594572174567892 | validation: 5.537177180673305]
	TIME [epoch: 7.83 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5855085605491235		[learning rate: 0.0012259]
	Learning Rate: 0.00122592
	LOSS [training: 5.5855085605491235 | validation: 5.510791151442777]
	TIME [epoch: 7.83 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.553719414517097		[learning rate: 0.001223]
	Learning Rate: 0.00122303
	LOSS [training: 5.553719414517097 | validation: 5.392793291777167]
	TIME [epoch: 7.84 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.471335228021919		[learning rate: 0.0012201]
	Learning Rate: 0.00122014
	LOSS [training: 5.471335228021919 | validation: 5.359192944784395]
	TIME [epoch: 7.85 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.45114154358931		[learning rate: 0.0012173]
	Learning Rate: 0.00121726
	LOSS [training: 5.45114154358931 | validation: 5.352195919047393]
	TIME [epoch: 7.86 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.461482886432567		[learning rate: 0.0012144]
	Learning Rate: 0.00121439
	LOSS [training: 5.461482886432567 | validation: 5.380628856396614]
	TIME [epoch: 7.83 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.511934691800572		[learning rate: 0.0012115]
	Learning Rate: 0.00121153
	LOSS [training: 5.511934691800572 | validation: 5.488481557608004]
	TIME [epoch: 7.83 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.541909983822257		[learning rate: 0.0012087]
	Learning Rate: 0.00120867
	LOSS [training: 5.541909983822257 | validation: 5.478201477234579]
	TIME [epoch: 7.84 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.52717891600441		[learning rate: 0.0012058]
	Learning Rate: 0.00120582
	LOSS [training: 5.52717891600441 | validation: 5.449026461467839]
	TIME [epoch: 7.86 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5074588975459156		[learning rate: 0.001203]
	Learning Rate: 0.00120297
	LOSS [training: 5.5074588975459156 | validation: 5.418761111310635]
	TIME [epoch: 7.86 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.485927740755571		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 5.485927740755571 | validation: 5.38343688359301]
	TIME [epoch: 7.84 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.466780184223657		[learning rate: 0.0011973]
	Learning Rate: 0.00119731
	LOSS [training: 5.466780184223657 | validation: 5.4136065067125845]
	TIME [epoch: 7.83 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.476702805250175		[learning rate: 0.0011945]
	Learning Rate: 0.00119448
	LOSS [training: 5.476702805250175 | validation: 5.3990397416219915]
	TIME [epoch: 7.83 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.492865039479248		[learning rate: 0.0011917]
	Learning Rate: 0.00119166
	LOSS [training: 5.492865039479248 | validation: 5.427460607239222]
	TIME [epoch: 7.87 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.4516162291742125		[learning rate: 0.0011889]
	Learning Rate: 0.00118885
	LOSS [training: 5.4516162291742125 | validation: 5.320846373874907]
	TIME [epoch: 7.87 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.41294547951608		[learning rate: 0.001186]
	Learning Rate: 0.00118605
	LOSS [training: 5.41294547951608 | validation: 5.310761645227543]
	TIME [epoch: 7.84 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.39206708465586		[learning rate: 0.0011833]
	Learning Rate: 0.00118325
	LOSS [training: 5.39206708465586 | validation: 5.321217315841343]
	TIME [epoch: 7.86 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.423178440231503		[learning rate: 0.0011805]
	Learning Rate: 0.00118046
	LOSS [training: 5.423178440231503 | validation: 5.34305238889448]
	TIME [epoch: 7.86 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.396153292939956		[learning rate: 0.0011777]
	Learning Rate: 0.00117768
	LOSS [training: 5.396153292939956 | validation: 5.269818938474304]
	TIME [epoch: 7.87 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.413150478007804		[learning rate: 0.0011749]
	Learning Rate: 0.0011749
	LOSS [training: 5.413150478007804 | validation: 5.40127009364292]
	TIME [epoch: 7.89 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.494501417737666		[learning rate: 0.0011721]
	Learning Rate: 0.00117213
	LOSS [training: 5.494501417737666 | validation: 5.478779986081135]
	TIME [epoch: 7.85 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5384094657808784		[learning rate: 0.0011694]
	Learning Rate: 0.00116936
	LOSS [training: 5.5384094657808784 | validation: 5.488006634660328]
	TIME [epoch: 7.85 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.537531868263381		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 5.537531868263381 | validation: 5.471381295180952]
	TIME [epoch: 7.85 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.526297071798524		[learning rate: 0.0011639]
	Learning Rate: 0.00116385
	LOSS [training: 5.526297071798524 | validation: 5.458981311685816]
	TIME [epoch: 7.88 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.511588763425682		[learning rate: 0.0011611]
	Learning Rate: 0.00116111
	LOSS [training: 5.511588763425682 | validation: 5.445241666325025]
	TIME [epoch: 7.89 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.510126102998953		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 5.510126102998953 | validation: 5.449479999574477]
	TIME [epoch: 7.86 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.501780231504297		[learning rate: 0.0011556]
	Learning Rate: 0.00115563
	LOSS [training: 5.501780231504297 | validation: 5.422581878208719]
	TIME [epoch: 7.85 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.481030079940174		[learning rate: 0.0011529]
	Learning Rate: 0.00115291
	LOSS [training: 5.481030079940174 | validation: 5.390608178354272]
	TIME [epoch: 7.84 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.451204012110161		[learning rate: 0.0011502]
	Learning Rate: 0.00115019
	LOSS [training: 5.451204012110161 | validation: 5.379198494935869]
	TIME [epoch: 7.87 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.468929244560043		[learning rate: 0.0011475]
	Learning Rate: 0.00114748
	LOSS [training: 5.468929244560043 | validation: 5.375225391363186]
	TIME [epoch: 7.88 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.463939043635422		[learning rate: 0.0011448]
	Learning Rate: 0.00114477
	LOSS [training: 5.463939043635422 | validation: 5.3592541332694745]
	TIME [epoch: 7.85 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.440353072637172		[learning rate: 0.0011421]
	Learning Rate: 0.00114207
	LOSS [training: 5.440353072637172 | validation: 5.311038031481939]
	TIME [epoch: 7.86 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.415643342491898		[learning rate: 0.0011394]
	Learning Rate: 0.00113937
	LOSS [training: 5.415643342491898 | validation: 5.285805144943923]
	TIME [epoch: 7.85 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.399261991487343		[learning rate: 0.0011367]
	Learning Rate: 0.00113669
	LOSS [training: 5.399261991487343 | validation: 5.267264151437894]
	TIME [epoch: 7.88 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.39755671958788		[learning rate: 0.001134]
	Learning Rate: 0.00113401
	LOSS [training: 5.39755671958788 | validation: 5.258404370246234]
	TIME [epoch: 7.89 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3834548357332315		[learning rate: 0.0011313]
	Learning Rate: 0.00113133
	LOSS [training: 5.3834548357332315 | validation: 5.253578467176714]
	TIME [epoch: 7.85 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.325269872615655		[learning rate: 0.0011287]
	Learning Rate: 0.00112866
	LOSS [training: 5.325269872615655 | validation: 5.176946861170709]
	TIME [epoch: 7.84 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.322842788642616		[learning rate: 0.001126]
	Learning Rate: 0.001126
	LOSS [training: 5.322842788642616 | validation: 5.246905813230589]
	TIME [epoch: 7.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.374203038701733		[learning rate: 0.0011233]
	Learning Rate: 0.00112334
	LOSS [training: 5.374203038701733 | validation: 5.260256976904838]
	TIME [epoch: 7.88 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.377409456493856		[learning rate: 0.0011207]
	Learning Rate: 0.00112069
	LOSS [training: 5.377409456493856 | validation: 5.263296302591584]
	TIME [epoch: 7.88 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.405178067792912		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 5.405178067792912 | validation: 5.359607329440943]
	TIME [epoch: 7.85 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.432971289966942		[learning rate: 0.0011154]
	Learning Rate: 0.00111541
	LOSS [training: 5.432971289966942 | validation: 5.361217529482754]
	TIME [epoch: 7.85 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.393913910261587		[learning rate: 0.0011128]
	Learning Rate: 0.00111278
	LOSS [training: 5.393913910261587 | validation: 5.215686423651476]
	TIME [epoch: 7.86 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.307363017241647		[learning rate: 0.0011102]
	Learning Rate: 0.00111016
	LOSS [training: 5.307363017241647 | validation: 5.194055585357985]
	TIME [epoch: 7.88 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.314888406282823		[learning rate: 0.0011075]
	Learning Rate: 0.00110754
	LOSS [training: 5.314888406282823 | validation: 5.219976809121084]
	TIME [epoch: 7.89 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.32562603518425		[learning rate: 0.0011049]
	Learning Rate: 0.00110493
	LOSS [training: 5.32562603518425 | validation: 5.2128610924657135]
	TIME [epoch: 7.86 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.324099079019932		[learning rate: 0.0011023]
	Learning Rate: 0.00110232
	LOSS [training: 5.324099079019932 | validation: 5.207713708073635]
	TIME [epoch: 7.85 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.33130800183388		[learning rate: 0.0010997]
	Learning Rate: 0.00109972
	LOSS [training: 5.33130800183388 | validation: 5.2058351209712335]
	TIME [epoch: 7.85 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.329722587680127		[learning rate: 0.0010971]
	Learning Rate: 0.00109713
	LOSS [training: 5.329722587680127 | validation: 5.207527837231572]
	TIME [epoch: 7.88 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.32185527475471		[learning rate: 0.0010945]
	Learning Rate: 0.00109454
	LOSS [training: 5.32185527475471 | validation: 5.197328697892884]
	TIME [epoch: 7.89 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.288527598339982		[learning rate: 0.001092]
	Learning Rate: 0.00109196
	LOSS [training: 5.288527598339982 | validation: 5.190443088588035]
	TIME [epoch: 7.84 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.297543794477686		[learning rate: 0.0010894]
	Learning Rate: 0.00108938
	LOSS [training: 5.297543794477686 | validation: 5.172397372680951]
	TIME [epoch: 7.85 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.221055091042387		[learning rate: 0.0010868]
	Learning Rate: 0.00108681
	LOSS [training: 5.221055091042387 | validation: 4.896563021126866]
	TIME [epoch: 7.85 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.985959031061709		[learning rate: 0.0010842]
	Learning Rate: 0.00108425
	LOSS [training: 4.985959031061709 | validation: 4.637699236391798]
	TIME [epoch: 7.88 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.789697379228106		[learning rate: 0.0010817]
	Learning Rate: 0.00108169
	LOSS [training: 4.789697379228106 | validation: 4.9134766780055354]
	TIME [epoch: 7.89 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.134199175041222		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 5.134199175041222 | validation: 5.075506481655919]
	TIME [epoch: 7.85 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.191836762848523		[learning rate: 0.0010766]
	Learning Rate: 0.00107659
	LOSS [training: 5.191836762848523 | validation: 5.082928894610772]
	TIME [epoch: 7.85 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.188052350630565		[learning rate: 0.0010741]
	Learning Rate: 0.00107405
	LOSS [training: 5.188052350630565 | validation: 5.07099559372061]
	TIME [epoch: 7.85 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.178589448103432		[learning rate: 0.0010715]
	Learning Rate: 0.00107152
	LOSS [training: 5.178589448103432 | validation: 5.0679512107449405]
	TIME [epoch: 7.86 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.146768856045121		[learning rate: 0.001069]
	Learning Rate: 0.00106899
	LOSS [training: 5.146768856045121 | validation: 5.077774180829063]
	TIME [epoch: 7.89 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.186267683098887		[learning rate: 0.0010665]
	Learning Rate: 0.00106647
	LOSS [training: 5.186267683098887 | validation: 5.104479187559021]
	TIME [epoch: 7.86 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.222230776385767		[learning rate: 0.001064]
	Learning Rate: 0.00106395
	LOSS [training: 5.222230776385767 | validation: 5.1546246146705]
	TIME [epoch: 7.85 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.251051256126624		[learning rate: 0.0010614]
	Learning Rate: 0.00106144
	LOSS [training: 5.251051256126624 | validation: 5.1581565524513895]
	TIME [epoch: 7.86 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.267403435525911		[learning rate: 0.0010589]
	Learning Rate: 0.00105894
	LOSS [training: 5.267403435525911 | validation: 5.186208780092262]
	TIME [epoch: 7.87 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.293446107281573		[learning rate: 0.0010564]
	Learning Rate: 0.00105644
	LOSS [training: 5.293446107281573 | validation: 5.2124039610723525]
	TIME [epoch: 7.89 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3013201240739845		[learning rate: 0.001054]
	Learning Rate: 0.00105395
	LOSS [training: 5.3013201240739845 | validation: 5.2020007362741385]
	TIME [epoch: 7.86 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.294247090279518		[learning rate: 0.0010515]
	Learning Rate: 0.00105147
	LOSS [training: 5.294247090279518 | validation: 5.183749167738471]
	TIME [epoch: 7.85 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.252084574239769		[learning rate: 0.001049]
	Learning Rate: 0.00104898
	LOSS [training: 5.252084574239769 | validation: 5.148890572755694]
	TIME [epoch: 7.85 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.272756331847988		[learning rate: 0.0010465]
	Learning Rate: 0.00104651
	LOSS [training: 5.272756331847988 | validation: 5.199442594051076]
	TIME [epoch: 7.86 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.294924489631737		[learning rate: 0.001044]
	Learning Rate: 0.00104404
	LOSS [training: 5.294924489631737 | validation: 5.191338557517867]
	TIME [epoch: 7.88 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.293348274661676		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 5.293348274661676 | validation: 5.178214134346557]
	TIME [epoch: 7.86 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.285672176144967		[learning rate: 0.0010391]
	Learning Rate: 0.00103912
	LOSS [training: 5.285672176144967 | validation: 5.189670981884628]
	TIME [epoch: 7.84 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.289745108193705		[learning rate: 0.0010367]
	Learning Rate: 0.00103667
	LOSS [training: 5.289745108193705 | validation: 5.211926844990927]
	TIME [epoch: 7.84 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3186070304339435		[learning rate: 0.0010342]
	Learning Rate: 0.00103423
	LOSS [training: 5.3186070304339435 | validation: 5.221624464109603]
	TIME [epoch: 7.86 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.317611623947402		[learning rate: 0.0010318]
	Learning Rate: 0.00103179
	LOSS [training: 5.317611623947402 | validation: 5.210621183621495]
	TIME [epoch: 7.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.311633530995634		[learning rate: 0.0010294]
	Learning Rate: 0.00102935
	LOSS [training: 5.311633530995634 | validation: 5.1934051811631505]
	TIME [epoch: 7.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.297714249939332		[learning rate: 0.0010269]
	Learning Rate: 0.00102692
	LOSS [training: 5.297714249939332 | validation: 5.175465830262253]
	TIME [epoch: 7.86 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.28434410980927		[learning rate: 0.0010245]
	Learning Rate: 0.0010245
	LOSS [training: 5.28434410980927 | validation: 5.164397201087237]
	TIME [epoch: 7.85 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.273386486182103		[learning rate: 0.0010221]
	Learning Rate: 0.00102209
	LOSS [training: 5.273386486182103 | validation: 5.1392173386191535]
	TIME [epoch: 7.86 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2578276593458675		[learning rate: 0.0010197]
	Learning Rate: 0.00101967
	LOSS [training: 5.2578276593458675 | validation: 5.105896169080673]
	TIME [epoch: 7.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2167733126429185		[learning rate: 0.0010173]
	Learning Rate: 0.00101727
	LOSS [training: 5.2167733126429185 | validation: 5.082787000756529]
	TIME [epoch: 7.86 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.189496841459749		[learning rate: 0.0010149]
	Learning Rate: 0.00101487
	LOSS [training: 5.189496841459749 | validation: 5.084208700496369]
	TIME [epoch: 7.85 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.171371995124872		[learning rate: 0.0010125]
	Learning Rate: 0.00101248
	LOSS [training: 5.171371995124872 | validation: 5.132733051896152]
	TIME [epoch: 7.85 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.219028191052773		[learning rate: 0.0010101]
	Learning Rate: 0.00101009
	LOSS [training: 5.219028191052773 | validation: 5.209409448520644]
	TIME [epoch: 7.88 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.226939456352268		[learning rate: 0.0010077]
	Learning Rate: 0.0010077
	LOSS [training: 5.226939456352268 | validation: 5.171764535152586]
	TIME [epoch: 7.89 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.223682909416333		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 5.223682909416333 | validation: 5.207492395493562]
	TIME [epoch: 7.84 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.210471003192185		[learning rate: 0.001003]
	Learning Rate: 0.00100296
	LOSS [training: 5.210471003192185 | validation: 5.129323336685962]
	TIME [epoch: 7.86 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.169117683237548		[learning rate: 0.0010006]
	Learning Rate: 0.00100059
	LOSS [training: 5.169117683237548 | validation: 5.038672312209112]
	TIME [epoch: 7.84 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.136211681871449		[learning rate: 0.00099823]
	Learning Rate: 0.000998231
	LOSS [training: 5.136211681871449 | validation: 5.075046549967551]
	TIME [epoch: 7.86 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.149038496056522		[learning rate: 0.00099588]
	Learning Rate: 0.000995876
	LOSS [training: 5.149038496056522 | validation: 5.13036156641369]
	TIME [epoch: 7.89 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.149179977648393		[learning rate: 0.00099353]
	Learning Rate: 0.000993527
	LOSS [training: 5.149179977648393 | validation: 5.125232715362991]
	TIME [epoch: 7.85 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.145601207671305		[learning rate: 0.00099118]
	Learning Rate: 0.000991183
	LOSS [training: 5.145601207671305 | validation: 5.1046107563269825]
	TIME [epoch: 7.85 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1291429471140075		[learning rate: 0.00098885]
	Learning Rate: 0.000988845
	LOSS [training: 5.1291429471140075 | validation: 5.081187594245954]
	TIME [epoch: 7.85 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.137992711428313		[learning rate: 0.00098651]
	Learning Rate: 0.000986513
	LOSS [training: 5.137992711428313 | validation: 5.052045608039925]
	TIME [epoch: 7.86 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.123571037777168		[learning rate: 0.00098419]
	Learning Rate: 0.000984185
	LOSS [training: 5.123571037777168 | validation: 5.021854745024381]
	TIME [epoch: 7.89 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0934110182258205		[learning rate: 0.00098186]
	Learning Rate: 0.000981864
	LOSS [training: 5.0934110182258205 | validation: 5.033512720753609]
	TIME [epoch: 7.84 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1074165806638865		[learning rate: 0.00097955]
	Learning Rate: 0.000979548
	LOSS [training: 5.1074165806638865 | validation: 5.0365803686879875]
	TIME [epoch: 7.85 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.113002014830375		[learning rate: 0.00097724]
	Learning Rate: 0.000977237
	LOSS [training: 5.113002014830375 | validation: 5.080039901725015]
	TIME [epoch: 7.85 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.14655065598456		[learning rate: 0.00097493]
	Learning Rate: 0.000974932
	LOSS [training: 5.14655065598456 | validation: 5.132711082842183]
	TIME [epoch: 7.86 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.194857673118271		[learning rate: 0.00097263]
	Learning Rate: 0.000972632
	LOSS [training: 5.194857673118271 | validation: 5.185725192946184]
	TIME [epoch: 7.89 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.205922481246786		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 5.205922481246786 | validation: 5.187680303660508]
	TIME [epoch: 7.84 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.213954463832645		[learning rate: 0.00096805]
	Learning Rate: 0.000968049
	LOSS [training: 5.213954463832645 | validation: 5.221852205346158]
	TIME [epoch: 7.85 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.198264406436316		[learning rate: 0.00096577]
	Learning Rate: 0.000965766
	LOSS [training: 5.198264406436316 | validation: 5.209160800362207]
	TIME [epoch: 7.85 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2144688084092135		[learning rate: 0.00096349]
	Learning Rate: 0.000963488
	LOSS [training: 5.2144688084092135 | validation: 5.2168851165662335]
	TIME [epoch: 7.87 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.214865784489614		[learning rate: 0.00096121]
	Learning Rate: 0.000961215
	LOSS [training: 5.214865784489614 | validation: 5.185105861068518]
	TIME [epoch: 7.88 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.202992596713813		[learning rate: 0.00095895]
	Learning Rate: 0.000958948
	LOSS [training: 5.202992596713813 | validation: 5.184280123914778]
	TIME [epoch: 7.85 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2067599540131635		[learning rate: 0.00095669]
	Learning Rate: 0.000956686
	LOSS [training: 5.2067599540131635 | validation: 5.189627812876042]
	TIME [epoch: 7.86 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.216154496846849		[learning rate: 0.00095443]
	Learning Rate: 0.000954429
	LOSS [training: 5.216154496846849 | validation: 5.216066062617099]
	TIME [epoch: 7.85 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.189296769264513		[learning rate: 0.00095218]
	Learning Rate: 0.000952178
	LOSS [training: 5.189296769264513 | validation: 5.217717160096923]
	TIME [epoch: 7.87 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.225784356096582		[learning rate: 0.00094993]
	Learning Rate: 0.000949932
	LOSS [training: 5.225784356096582 | validation: 5.260621787098359]
	TIME [epoch: 7.89 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.26756515537059		[learning rate: 0.00094769]
	Learning Rate: 0.000947691
	LOSS [training: 5.26756515537059 | validation: 5.273232053588501]
	TIME [epoch: 7.85 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2664935217016975		[learning rate: 0.00094546]
	Learning Rate: 0.000945455
	LOSS [training: 5.2664935217016975 | validation: 5.259201912481904]
	TIME [epoch: 7.85 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.238631801568445		[learning rate: 0.00094323]
	Learning Rate: 0.000943225
	LOSS [training: 5.238631801568445 | validation: 5.258274009554901]
	TIME [epoch: 7.86 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.267468869017104		[learning rate: 0.000941]
	Learning Rate: 0.000941
	LOSS [training: 5.267468869017104 | validation: 5.275144881646812]
	TIME [epoch: 7.87 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.279199289945964		[learning rate: 0.00093878]
	Learning Rate: 0.000938781
	LOSS [training: 5.279199289945964 | validation: 5.3019691268424864]
	TIME [epoch: 7.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2942220158476365		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 5.2942220158476365 | validation: 5.320509460168902]
	TIME [epoch: 7.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.316504580646887		[learning rate: 0.00093436]
	Learning Rate: 0.000934357
	LOSS [training: 5.316504580646887 | validation: 5.295803354519954]
	TIME [epoch: 7.85 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.293891554582672		[learning rate: 0.00093215]
	Learning Rate: 0.000932153
	LOSS [training: 5.293891554582672 | validation: 5.295530136725468]
	TIME [epoch: 7.85 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.303783738577246		[learning rate: 0.00092995]
	Learning Rate: 0.000929954
	LOSS [training: 5.303783738577246 | validation: 5.325916978027738]
	TIME [epoch: 7.87 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.325244611977814		[learning rate: 0.00092776]
	Learning Rate: 0.000927761
	LOSS [training: 5.325244611977814 | validation: 5.346751782174742]
	TIME [epoch: 7.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.341237579937076		[learning rate: 0.00092557]
	Learning Rate: 0.000925572
	LOSS [training: 5.341237579937076 | validation: 5.3501350899275355]
	TIME [epoch: 7.85 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.333532040196587		[learning rate: 0.00092339]
	Learning Rate: 0.000923389
	LOSS [training: 5.333532040196587 | validation: 5.350079695176808]
	TIME [epoch: 7.85 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3243943413962995		[learning rate: 0.00092121]
	Learning Rate: 0.000921211
	LOSS [training: 5.3243943413962995 | validation: 5.3325047586019]
	TIME [epoch: 7.86 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3163042069992335		[learning rate: 0.00091904]
	Learning Rate: 0.000919038
	LOSS [training: 5.3163042069992335 | validation: 5.329480460980725]
	TIME [epoch: 7.87 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3059939763750466		[learning rate: 0.00091687]
	Learning Rate: 0.00091687
	LOSS [training: 5.3059939763750466 | validation: 5.335889557648946]
	TIME [epoch: 7.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.339734649859762		[learning rate: 0.00091471]
	Learning Rate: 0.000914707
	LOSS [training: 5.339734649859762 | validation: 5.3846656186399615]
	TIME [epoch: 7.85 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.363045924661285		[learning rate: 0.00091255]
	Learning Rate: 0.00091255
	LOSS [training: 5.363045924661285 | validation: 5.386242073508141]
	TIME [epoch: 7.85 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.358226378925989		[learning rate: 0.0009104]
	Learning Rate: 0.000910397
	LOSS [training: 5.358226378925989 | validation: 5.354790793263263]
	TIME [epoch: 7.86 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.341410005048335		[learning rate: 0.00090825]
	Learning Rate: 0.00090825
	LOSS [training: 5.341410005048335 | validation: 5.354619392053481]
	TIME [epoch: 7.87 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.346314279708095		[learning rate: 0.00090611]
	Learning Rate: 0.000906107
	LOSS [training: 5.346314279708095 | validation: 5.37780194454027]
	TIME [epoch: 7.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.356409885435369		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 5.356409885435369 | validation: 5.37641611521992]
	TIME [epoch: 7.85 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.356451982807891		[learning rate: 0.00090184]
	Learning Rate: 0.000901837
	LOSS [training: 5.356451982807891 | validation: 5.3601884865748435]
	TIME [epoch: 7.86 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3483930493735565		[learning rate: 0.00089971]
	Learning Rate: 0.00089971
	LOSS [training: 5.3483930493735565 | validation: 5.359473084089242]
	TIME [epoch: 7.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.347200943796633		[learning rate: 0.00089759]
	Learning Rate: 0.000897588
	LOSS [training: 5.347200943796633 | validation: 5.352915387609761]
	TIME [epoch: 7.87 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.334756552179201		[learning rate: 0.00089547]
	Learning Rate: 0.00089547
	LOSS [training: 5.334756552179201 | validation: 5.344702122249149]
	TIME [epoch: 7.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.310277295842027		[learning rate: 0.00089336]
	Learning Rate: 0.000893358
	LOSS [training: 5.310277295842027 | validation: 5.231339043505432]
	TIME [epoch: 7.86 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.275982819723598		[learning rate: 0.00089125]
	Learning Rate: 0.000891251
	LOSS [training: 5.275982819723598 | validation: 5.167423248000043]
	TIME [epoch: 7.85 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.186967651343951		[learning rate: 0.00088915]
	Learning Rate: 0.000889149
	LOSS [training: 5.186967651343951 | validation: 5.0224409929910525]
	TIME [epoch: 7.86 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.122358536217367		[learning rate: 0.00088705]
	Learning Rate: 0.000887051
	LOSS [training: 5.122358536217367 | validation: 5.059587360113527]
	TIME [epoch: 7.88 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.16834645641563		[learning rate: 0.00088496]
	Learning Rate: 0.000884959
	LOSS [training: 5.16834645641563 | validation: 5.066650429474136]
	TIME [epoch: 7.89 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.168688397610931		[learning rate: 0.00088287]
	Learning Rate: 0.000882871
	LOSS [training: 5.168688397610931 | validation: 5.178004176125217]
	TIME [epoch: 7.86 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2462061912644256		[learning rate: 0.00088079]
	Learning Rate: 0.000880789
	LOSS [training: 5.2462061912644256 | validation: 5.196735029302136]
	TIME [epoch: 7.85 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2269590095671505		[learning rate: 0.00087871]
	Learning Rate: 0.000878711
	LOSS [training: 5.2269590095671505 | validation: 5.097414967307978]
	TIME [epoch: 7.86 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2061600622949005		[learning rate: 0.00087664]
	Learning Rate: 0.000876639
	LOSS [training: 5.2061600622949005 | validation: 5.152125314812772]
	TIME [epoch: 7.88 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.237990562809532		[learning rate: 0.00087457]
	Learning Rate: 0.000874571
	LOSS [training: 5.237990562809532 | validation: 5.138119705566671]
	TIME [epoch: 7.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.200569551423543		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 5.200569551423543 | validation: 5.07250733005311]
	TIME [epoch: 7.86 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.150688417878116		[learning rate: 0.00087045]
	Learning Rate: 0.00087045
	LOSS [training: 5.150688417878116 | validation: 4.997462865394249]
	TIME [epoch: 7.85 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.148265754767271		[learning rate: 0.0008684]
	Learning Rate: 0.000868396
	LOSS [training: 5.148265754767271 | validation: 5.131912689997404]
	TIME [epoch: 7.86 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.18626055130911		[learning rate: 0.00086635]
	Learning Rate: 0.000866348
	LOSS [training: 5.18626055130911 | validation: 5.101308101565214]
	TIME [epoch: 7.87 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.168373822761514		[learning rate: 0.0008643]
	Learning Rate: 0.000864304
	LOSS [training: 5.168373822761514 | validation: 5.040290283705138]
	TIME [epoch: 7.89 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.127500711253239		[learning rate: 0.00086227]
	Learning Rate: 0.000862266
	LOSS [training: 5.127500711253239 | validation: 4.977687410151692]
	TIME [epoch: 7.85 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.100636793488629		[learning rate: 0.00086023]
	Learning Rate: 0.000860232
	LOSS [training: 5.100636793488629 | validation: 5.025882192452921]
	TIME [epoch: 7.85 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.190226311777652		[learning rate: 0.0008582]
	Learning Rate: 0.000858202
	LOSS [training: 5.190226311777652 | validation: 5.211403365619965]
	TIME [epoch: 7.86 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.230558037953263		[learning rate: 0.00085618]
	Learning Rate: 0.000856178
	LOSS [training: 5.230558037953263 | validation: 5.123925848733645]
	TIME [epoch: 7.87 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.168380883021049		[learning rate: 0.00085416]
	Learning Rate: 0.000854159
	LOSS [training: 5.168380883021049 | validation: 5.114792077358109]
	TIME [epoch: 7.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.209893599056247		[learning rate: 0.00085214]
	Learning Rate: 0.000852144
	LOSS [training: 5.209893599056247 | validation: 5.270147119917365]
	TIME [epoch: 7.85 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.294207612443269		[learning rate: 0.00085013]
	Learning Rate: 0.000850134
	LOSS [training: 5.294207612443269 | validation: 5.313094355821135]
	TIME [epoch: 7.85 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.306205528887661		[learning rate: 0.00084813]
	Learning Rate: 0.000848128
	LOSS [training: 5.306205528887661 | validation: 5.315038245918597]
	TIME [epoch: 7.84 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.28890080106116		[learning rate: 0.00084613]
	Learning Rate: 0.000846128
	LOSS [training: 5.28890080106116 | validation: 5.283120158210634]
	TIME [epoch: 7.86 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.279715784693972		[learning rate: 0.00084413]
	Learning Rate: 0.000844132
	LOSS [training: 5.279715784693972 | validation: 5.286124816787192]
	TIME [epoch: 7.89 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.303115308222275		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 5.303115308222275 | validation: 5.309543332259478]
	TIME [epoch: 7.85 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2947950021965		[learning rate: 0.00084015]
	Learning Rate: 0.000840154
	LOSS [training: 5.2947950021965 | validation: 5.288234580033308]
	TIME [epoch: 7.86 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.269002031395985		[learning rate: 0.00083817]
	Learning Rate: 0.000838173
	LOSS [training: 5.269002031395985 | validation: 5.29192112065237]
	TIME [epoch: 7.85 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.279245425909357		[learning rate: 0.0008362]
	Learning Rate: 0.000836195
	LOSS [training: 5.279245425909357 | validation: 5.303719379404341]
	TIME [epoch: 7.88 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.298520195514217		[learning rate: 0.00083422]
	Learning Rate: 0.000834223
	LOSS [training: 5.298520195514217 | validation: 5.262468134373346]
	TIME [epoch: 7.89 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.260443955039712		[learning rate: 0.00083226]
	Learning Rate: 0.000832255
	LOSS [training: 5.260443955039712 | validation: 5.2068971418637915]
	TIME [epoch: 7.85 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.193789784966441		[learning rate: 0.00083029]
	Learning Rate: 0.000830292
	LOSS [training: 5.193789784966441 | validation: 5.08584520305412]
	TIME [epoch: 7.85 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.15057280587353		[learning rate: 0.00082833]
	Learning Rate: 0.000828333
	LOSS [training: 5.15057280587353 | validation: 5.070658121389826]
	TIME [epoch: 7.85 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1532470303025555		[learning rate: 0.00082638]
	Learning Rate: 0.00082638
	LOSS [training: 5.1532470303025555 | validation: 5.126734322322131]
	TIME [epoch: 7.87 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.143586813744399		[learning rate: 0.00082443]
	Learning Rate: 0.00082443
	LOSS [training: 5.143586813744399 | validation: 5.101164912786897]
	TIME [epoch: 7.89 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.128614878995266		[learning rate: 0.00082249]
	Learning Rate: 0.000822485
	LOSS [training: 5.128614878995266 | validation: 5.063325849604315]
	TIME [epoch: 7.86 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.060941669160814		[learning rate: 0.00082055]
	Learning Rate: 0.000820545
	LOSS [training: 5.060941669160814 | validation: 4.883250100003098]
	TIME [epoch: 7.86 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.946833232471502		[learning rate: 0.00081861]
	Learning Rate: 0.00081861
	LOSS [training: 4.946833232471502 | validation: 4.787553755620797]
	TIME [epoch: 7.86 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.910993238220211		[learning rate: 0.00081668]
	Learning Rate: 0.000816679
	LOSS [training: 4.910993238220211 | validation: 4.8099178109789404]
	TIME [epoch: 7.88 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922912089336018		[learning rate: 0.00081475]
	Learning Rate: 0.000814752
	LOSS [training: 4.922912089336018 | validation: 4.819723846158574]
	TIME [epoch: 7.9 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.933694032989914		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 4.933694032989914 | validation: 4.79741253446166]
	TIME [epoch: 7.85 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9034486671571464		[learning rate: 0.00081091]
	Learning Rate: 0.000810913
	LOSS [training: 4.9034486671571464 | validation: 4.784029172136714]
	TIME [epoch: 7.85 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.904513519288499		[learning rate: 0.000809]
	Learning Rate: 0.000809
	LOSS [training: 4.904513519288499 | validation: 4.814694267796099]
	TIME [epoch: 7.86 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922967735001049		[learning rate: 0.00080709]
	Learning Rate: 0.000807092
	LOSS [training: 4.922967735001049 | validation: 4.833872226493078]
	TIME [epoch: 7.87 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907325620989966		[learning rate: 0.00080519]
	Learning Rate: 0.000805188
	LOSS [training: 4.907325620989966 | validation: 4.761539560901426]
	TIME [epoch: 7.9 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8656943927830625		[learning rate: 0.00080329]
	Learning Rate: 0.000803289
	LOSS [training: 4.8656943927830625 | validation: 4.740983764633077]
	TIME [epoch: 7.86 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.846961976596507		[learning rate: 0.00080139]
	Learning Rate: 0.000801394
	LOSS [training: 4.846961976596507 | validation: 4.750468164067474]
	TIME [epoch: 7.86 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.853496071229708		[learning rate: 0.0007995]
	Learning Rate: 0.000799504
	LOSS [training: 4.853496071229708 | validation: 4.7607649535101455]
	TIME [epoch: 7.86 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.872513499714509		[learning rate: 0.00079762]
	Learning Rate: 0.000797618
	LOSS [training: 4.872513499714509 | validation: 4.78632297197909]
	TIME [epoch: 7.88 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.915670471475003		[learning rate: 0.00079574]
	Learning Rate: 0.000795736
	LOSS [training: 4.915670471475003 | validation: 4.815522227065147]
	TIME [epoch: 7.9 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9332256170068		[learning rate: 0.00079386]
	Learning Rate: 0.000793859
	LOSS [training: 4.9332256170068 | validation: 4.813972903716767]
	TIME [epoch: 7.86 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.96216094560712		[learning rate: 0.00079199]
	Learning Rate: 0.000791987
	LOSS [training: 4.96216094560712 | validation: 4.874197493701615]
	TIME [epoch: 7.85 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.995196920301547		[learning rate: 0.00079012]
	Learning Rate: 0.000790119
	LOSS [training: 4.995196920301547 | validation: 4.8979345780104335]
	TIME [epoch: 7.86 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9797311572692635		[learning rate: 0.00078826]
	Learning Rate: 0.000788255
	LOSS [training: 4.9797311572692635 | validation: 4.83595920033236]
	TIME [epoch: 7.87 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.962839606001516		[learning rate: 0.0007864]
	Learning Rate: 0.000786396
	LOSS [training: 4.962839606001516 | validation: 4.872130353641754]
	TIME [epoch: 7.9 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0054880981124		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 5.0054880981124 | validation: 4.925363328511514]
	TIME [epoch: 7.86 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028255077619744		[learning rate: 0.00078269]
	Learning Rate: 0.00078269
	LOSS [training: 5.028255077619744 | validation: 4.89280248007533]
	TIME [epoch: 7.86 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.973872740493191		[learning rate: 0.00078084]
	Learning Rate: 0.000780844
	LOSS [training: 4.973872740493191 | validation: 4.889756421848984]
	TIME [epoch: 7.87 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.998216958803379		[learning rate: 0.000779]
	Learning Rate: 0.000779002
	LOSS [training: 4.998216958803379 | validation: 4.902360464484687]
	TIME [epoch: 7.86 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.974293809649495		[learning rate: 0.00077716]
	Learning Rate: 0.000777164
	LOSS [training: 4.974293809649495 | validation: 4.867532475050967]
	TIME [epoch: 7.89 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9536553703708535		[learning rate: 0.00077533]
	Learning Rate: 0.000775331
	LOSS [training: 4.9536553703708535 | validation: 4.8566940345258915]
	TIME [epoch: 7.86 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9897290069926346		[learning rate: 0.0007735]
	Learning Rate: 0.000773502
	LOSS [training: 4.9897290069926346 | validation: 4.952135376659504]
	TIME [epoch: 7.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.043746191565096		[learning rate: 0.00077168]
	Learning Rate: 0.000771678
	LOSS [training: 5.043746191565096 | validation: 5.060787059293924]
	TIME [epoch: 7.85 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.086118699778405		[learning rate: 0.00076986]
	Learning Rate: 0.000769857
	LOSS [training: 5.086118699778405 | validation: 5.022799954607846]
	TIME [epoch: 7.86 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0354010112335255		[learning rate: 0.00076804]
	Learning Rate: 0.000768041
	LOSS [training: 5.0354010112335255 | validation: 4.913919775369194]
	TIME [epoch: 7.89 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9609514822206595		[learning rate: 0.00076623]
	Learning Rate: 0.00076623
	LOSS [training: 4.9609514822206595 | validation: 4.817412806242215]
	TIME [epoch: 7.85 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922905210077587		[learning rate: 0.00076442]
	Learning Rate: 0.000764422
	LOSS [training: 4.922905210077587 | validation: 4.813482791772246]
	TIME [epoch: 7.88 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.916260123748781		[learning rate: 0.00076262]
	Learning Rate: 0.000762619
	LOSS [training: 4.916260123748781 | validation: 4.816175303629529]
	TIME [epoch: 7.85 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9130680336843975		[learning rate: 0.00076082]
	Learning Rate: 0.00076082
	LOSS [training: 4.9130680336843975 | validation: 4.822974612450888]
	TIME [epoch: 7.87 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.923255078118469		[learning rate: 0.00075903]
	Learning Rate: 0.000759026
	LOSS [training: 4.923255078118469 | validation: 4.824326910467828]
	TIME [epoch: 7.89 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.933336485983507		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 4.933336485983507 | validation: 4.876183226641407]
	TIME [epoch: 7.86 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.966722666859863		[learning rate: 0.00075545]
	Learning Rate: 0.000755449
	LOSS [training: 4.966722666859863 | validation: 4.914605757638543]
	TIME [epoch: 7.86 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.972110046775731		[learning rate: 0.00075367]
	Learning Rate: 0.000753667
	LOSS [training: 4.972110046775731 | validation: 4.8921542176161665]
	TIME [epoch: 7.86 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9550236477718785		[learning rate: 0.00075189]
	Learning Rate: 0.000751889
	LOSS [training: 4.9550236477718785 | validation: 4.90995736793834]
	TIME [epoch: 7.87 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.964149114237942		[learning rate: 0.00075012]
	Learning Rate: 0.000750116
	LOSS [training: 4.964149114237942 | validation: 4.857426560799281]
	TIME [epoch: 7.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.916342486834729		[learning rate: 0.00074835]
	Learning Rate: 0.000748346
	LOSS [training: 4.916342486834729 | validation: 4.792104319686268]
	TIME [epoch: 7.85 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.896927927970128		[learning rate: 0.00074658]
	Learning Rate: 0.000746581
	LOSS [training: 4.896927927970128 | validation: 4.779188013917778]
	TIME [epoch: 7.86 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883946276968655		[learning rate: 0.00074482]
	Learning Rate: 0.00074482
	LOSS [training: 4.883946276968655 | validation: 4.764780687090824]
	TIME [epoch: 7.86 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.877136028087973		[learning rate: 0.00074306]
	Learning Rate: 0.000743063
	LOSS [training: 4.877136028087973 | validation: 4.772109354181763]
	TIME [epoch: 7.87 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882824580321714		[learning rate: 0.00074131]
	Learning Rate: 0.00074131
	LOSS [training: 4.882824580321714 | validation: 4.805915285813266]
	TIME [epoch: 7.9 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.888688793838961		[learning rate: 0.00073956]
	Learning Rate: 0.000739562
	LOSS [training: 4.888688793838961 | validation: 4.779979008139772]
	TIME [epoch: 7.87 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861930037580319		[learning rate: 0.00073782]
	Learning Rate: 0.000737817
	LOSS [training: 4.861930037580319 | validation: 4.74701179859094]
	TIME [epoch: 7.86 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.83240622287259		[learning rate: 0.00073608]
	Learning Rate: 0.000736077
	LOSS [training: 4.83240622287259 | validation: 4.73450581647796]
	TIME [epoch: 7.87 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837986123361855		[learning rate: 0.00073434]
	Learning Rate: 0.000734341
	LOSS [training: 4.837986123361855 | validation: 4.743670057502069]
	TIME [epoch: 7.87 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.852964340798703		[learning rate: 0.00073261]
	Learning Rate: 0.000732608
	LOSS [training: 4.852964340798703 | validation: 4.753150405513541]
	TIME [epoch: 7.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861300695846101		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 4.861300695846101 | validation: 4.762317860526785]
	TIME [epoch: 7.85 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.860932083566588		[learning rate: 0.00072916]
	Learning Rate: 0.000729156
	LOSS [training: 4.860932083566588 | validation: 4.788667191711484]
	TIME [epoch: 7.85 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882486717312681		[learning rate: 0.00072744]
	Learning Rate: 0.000727436
	LOSS [training: 4.882486717312681 | validation: 4.774216681972762]
	TIME [epoch: 7.86 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.849227967172416		[learning rate: 0.00072572]
	Learning Rate: 0.00072572
	LOSS [training: 4.849227967172416 | validation: 4.74605735244867]
	TIME [epoch: 7.88 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.836259174343699		[learning rate: 0.00072401]
	Learning Rate: 0.000724008
	LOSS [training: 4.836259174343699 | validation: 4.761005239490068]
	TIME [epoch: 7.89 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.83007161830681		[learning rate: 0.0007223]
	Learning Rate: 0.000722301
	LOSS [training: 4.83007161830681 | validation: 4.737326268939283]
	TIME [epoch: 7.86 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.829022600922228		[learning rate: 0.0007206]
	Learning Rate: 0.000720597
	LOSS [training: 4.829022600922228 | validation: 4.742875581519145]
	TIME [epoch: 7.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.829935847092699		[learning rate: 0.0007189]
	Learning Rate: 0.000718897
	LOSS [training: 4.829935847092699 | validation: 4.757559167563392]
	TIME [epoch: 7.86 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8686671939902375		[learning rate: 0.0007172]
	Learning Rate: 0.000717201
	LOSS [training: 4.8686671939902375 | validation: 4.780015312124275]
	TIME [epoch: 7.87 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.894683918197675		[learning rate: 0.00071551]
	Learning Rate: 0.00071551
	LOSS [training: 4.894683918197675 | validation: 4.793149115912184]
	TIME [epoch: 7.9 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8992876073878975		[learning rate: 0.00071382]
	Learning Rate: 0.000713822
	LOSS [training: 4.8992876073878975 | validation: 4.791927358470165]
	TIME [epoch: 7.86 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.89742594966539		[learning rate: 0.00071214]
	Learning Rate: 0.000712138
	LOSS [training: 4.89742594966539 | validation: 4.7923563846088575]
	TIME [epoch: 7.86 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907847590643036		[learning rate: 0.00071046]
	Learning Rate: 0.000710458
	LOSS [training: 4.907847590643036 | validation: 4.834736154891054]
	TIME [epoch: 7.86 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.924130192327179		[learning rate: 0.00070878]
	Learning Rate: 0.000708782
	LOSS [training: 4.924130192327179 | validation: 4.842893653322793]
	TIME [epoch: 7.87 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919942897707443		[learning rate: 0.00070711]
	Learning Rate: 0.00070711
	LOSS [training: 4.919942897707443 | validation: 4.862155671993333]
	TIME [epoch: 7.89 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.912780395902434		[learning rate: 0.00070544]
	Learning Rate: 0.000705443
	LOSS [training: 4.912780395902434 | validation: 4.807397215435824]
	TIME [epoch: 7.86 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.849459035880167		[learning rate: 0.00070378]
	Learning Rate: 0.000703778
	LOSS [training: 4.849459035880167 | validation: 4.746222604509556]
	TIME [epoch: 7.86 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.828404484874703		[learning rate: 0.00070212]
	Learning Rate: 0.000702118
	LOSS [training: 4.828404484874703 | validation: 4.750558574035553]
	TIME [epoch: 7.85 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.844750365383559		[learning rate: 0.00070046]
	Learning Rate: 0.000700462
	LOSS [training: 4.844750365383559 | validation: 4.7913186850642004]
	TIME [epoch: 7.87 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.863934956927659		[learning rate: 0.00069881]
	Learning Rate: 0.00069881
	LOSS [training: 4.863934956927659 | validation: 4.776048850126825]
	TIME [epoch: 7.9 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.850147129463801		[learning rate: 0.00069716]
	Learning Rate: 0.000697161
	LOSS [training: 4.850147129463801 | validation: 4.747611136339026]
	TIME [epoch: 7.87 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837482802362729		[learning rate: 0.00069552]
	Learning Rate: 0.000695517
	LOSS [training: 4.837482802362729 | validation: 4.757139561230315]
	TIME [epoch: 7.86 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.854444056861606		[learning rate: 0.00069388]
	Learning Rate: 0.000693876
	LOSS [training: 4.854444056861606 | validation: 4.754703091784414]
	TIME [epoch: 7.86 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878574887867759		[learning rate: 0.00069224]
	Learning Rate: 0.00069224
	LOSS [training: 4.878574887867759 | validation: 4.759193562280661]
	TIME [epoch: 7.88 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.870559744641478		[learning rate: 0.00069061]
	Learning Rate: 0.000690607
	LOSS [training: 4.870559744641478 | validation: 4.755424980267348]
	TIME [epoch: 7.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.865720368298593		[learning rate: 0.00068898]
	Learning Rate: 0.000688978
	LOSS [training: 4.865720368298593 | validation: 4.7712502806291965]
	TIME [epoch: 7.86 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873360518987844		[learning rate: 0.00068735]
	Learning Rate: 0.000687352
	LOSS [training: 4.873360518987844 | validation: 4.759030299675087]
	TIME [epoch: 7.86 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8692290677584085		[learning rate: 0.00068573]
	Learning Rate: 0.000685731
	LOSS [training: 4.8692290677584085 | validation: 4.781339252822164]
	TIME [epoch: 7.86 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861215702346089		[learning rate: 0.00068411]
	Learning Rate: 0.000684114
	LOSS [training: 4.861215702346089 | validation: 4.790187793109466]
	TIME [epoch: 7.88 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882922459022708		[learning rate: 0.0006825]
	Learning Rate: 0.0006825
	LOSS [training: 4.882922459022708 | validation: 4.8555144227136875]
	TIME [epoch: 7.9 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.934274352040415		[learning rate: 0.00068089]
	Learning Rate: 0.00068089
	LOSS [training: 4.934274352040415 | validation: 4.955444153274286]
	TIME [epoch: 7.86 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.953969088928838		[learning rate: 0.00067928]
	Learning Rate: 0.000679284
	LOSS [training: 4.953969088928838 | validation: 4.918852356631625]
	TIME [epoch: 7.85 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939993760905399		[learning rate: 0.00067768]
	Learning Rate: 0.000677682
	LOSS [training: 4.939993760905399 | validation: 4.899528150050185]
	TIME [epoch: 7.85 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919640108560471		[learning rate: 0.00067608]
	Learning Rate: 0.000676083
	LOSS [training: 4.919640108560471 | validation: 4.873497205796257]
	TIME [epoch: 7.87 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901235856612201		[learning rate: 0.00067449]
	Learning Rate: 0.000674488
	LOSS [training: 4.901235856612201 | validation: 4.845707269861209]
	TIME [epoch: 7.9 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.855897927033208		[learning rate: 0.0006729]
	Learning Rate: 0.000672897
	LOSS [training: 4.855897927033208 | validation: 4.779206705478439]
	TIME [epoch: 7.86 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.81960217018475		[learning rate: 0.00067131]
	Learning Rate: 0.00067131
	LOSS [training: 4.81960217018475 | validation: 4.78529434001493]
	TIME [epoch: 7.86 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822035284396694		[learning rate: 0.00066973]
	Learning Rate: 0.000669726
	LOSS [training: 4.822035284396694 | validation: 4.755547767876367]
	TIME [epoch: 7.86 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.781185115562784		[learning rate: 0.00066815]
	Learning Rate: 0.000668147
	LOSS [training: 4.781185115562784 | validation: 4.684907908874765]
	TIME [epoch: 7.87 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.747347523629691		[learning rate: 0.00066657]
	Learning Rate: 0.000666571
	LOSS [training: 4.747347523629691 | validation: 4.661635903981925]
	TIME [epoch: 7.91 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.755169666410967		[learning rate: 0.000665]
	Learning Rate: 0.000664998
	LOSS [training: 4.755169666410967 | validation: 4.6789339251175175]
	TIME [epoch: 7.86 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.757356903679526		[learning rate: 0.00066343]
	Learning Rate: 0.00066343
	LOSS [training: 4.757356903679526 | validation: 4.719189340919947]
	TIME [epoch: 7.87 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.796111487819022		[learning rate: 0.00066186]
	Learning Rate: 0.000661865
	LOSS [training: 4.796111487819022 | validation: 4.726972920440313]
	TIME [epoch: 7.86 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.79531973992044		[learning rate: 0.0006603]
	Learning Rate: 0.000660304
	LOSS [training: 4.79531973992044 | validation: 4.707838179078733]
	TIME [epoch: 7.88 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794571191051108		[learning rate: 0.00065875]
	Learning Rate: 0.000658746
	LOSS [training: 4.794571191051108 | validation: 4.713032999073333]
	TIME [epoch: 7.91 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.807074742061778		[learning rate: 0.00065719]
	Learning Rate: 0.000657192
	LOSS [training: 4.807074742061778 | validation: 4.706788264055686]
	TIME [epoch: 7.87 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.777451265774941		[learning rate: 0.00065564]
	Learning Rate: 0.000655642
	LOSS [training: 4.777451265774941 | validation: 4.660229768580328]
	TIME [epoch: 7.86 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.729965840633615		[learning rate: 0.0006541]
	Learning Rate: 0.000654095
	LOSS [training: 4.729965840633615 | validation: 4.643712026984544]
	TIME [epoch: 7.86 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.716314348754737		[learning rate: 0.00065255]
	Learning Rate: 0.000652552
	LOSS [training: 4.716314348754737 | validation: 4.637042900370669]
	TIME [epoch: 7.88 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.730015347416309		[learning rate: 0.00065101]
	Learning Rate: 0.000651013
	LOSS [training: 4.730015347416309 | validation: 4.694360074978471]
	TIME [epoch: 7.91 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7762486781953735		[learning rate: 0.00064948]
	Learning Rate: 0.000649478
	LOSS [training: 4.7762486781953735 | validation: 4.676028461756993]
	TIME [epoch: 7.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7301002474609		[learning rate: 0.00064795]
	Learning Rate: 0.000647945
	LOSS [training: 4.7301002474609 | validation: 4.652197481470017]
	TIME [epoch: 7.87 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.740245304631321		[learning rate: 0.00064642]
	Learning Rate: 0.000646417
	LOSS [training: 4.740245304631321 | validation: 4.732122441179804]
	TIME [epoch: 7.85 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7865624667887845		[learning rate: 0.00064489]
	Learning Rate: 0.000644892
	LOSS [training: 4.7865624667887845 | validation: 4.723098042600456]
	TIME [epoch: 7.88 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.793420020459568		[learning rate: 0.00064337]
	Learning Rate: 0.000643371
	LOSS [training: 4.793420020459568 | validation: 4.733606951003597]
	TIME [epoch: 7.91 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.80754298957922		[learning rate: 0.00064185]
	Learning Rate: 0.000641854
	LOSS [training: 4.80754298957922 | validation: 4.710245969269733]
	TIME [epoch: 7.86 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803499850974012		[learning rate: 0.00064034]
	Learning Rate: 0.00064034
	LOSS [training: 4.803499850974012 | validation: 4.710021246591517]
	TIME [epoch: 7.86 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803294689609972		[learning rate: 0.00063883]
	Learning Rate: 0.000638829
	LOSS [training: 4.803294689609972 | validation: 4.7051382552494525]
	TIME [epoch: 7.86 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.799912766870886		[learning rate: 0.00063732]
	Learning Rate: 0.000637322
	LOSS [training: 4.799912766870886 | validation: 4.705614483064665]
	TIME [epoch: 7.87 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.801032949668502		[learning rate: 0.00063582]
	Learning Rate: 0.000635819
	LOSS [training: 4.801032949668502 | validation: 4.715694697432065]
	TIME [epoch: 7.9 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.811185668346859		[learning rate: 0.00063432]
	Learning Rate: 0.000634319
	LOSS [training: 4.811185668346859 | validation: 4.751402045207929]
	TIME [epoch: 7.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837384345237458		[learning rate: 0.00063282]
	Learning Rate: 0.000632823
	LOSS [training: 4.837384345237458 | validation: 4.745394587136742]
	TIME [epoch: 7.86 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816246133424482		[learning rate: 0.00063133]
	Learning Rate: 0.00063133
	LOSS [training: 4.816246133424482 | validation: 4.71818156820895]
	TIME [epoch: 7.85 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.800478748235648		[learning rate: 0.00062984]
	Learning Rate: 0.000629841
	LOSS [training: 4.800478748235648 | validation: 4.710923861656801]
	TIME [epoch: 7.88 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.813343012692298		[learning rate: 0.00062836]
	Learning Rate: 0.000628355
	LOSS [training: 4.813343012692298 | validation: 4.728160345434716]
	TIME [epoch: 7.9 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.836560818098976		[learning rate: 0.00062687]
	Learning Rate: 0.000626873
	LOSS [training: 4.836560818098976 | validation: 4.734670827193793]
	TIME [epoch: 7.86 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.835901444421394		[learning rate: 0.00062539]
	Learning Rate: 0.000625394
	LOSS [training: 4.835901444421394 | validation: 4.742211362793738]
	TIME [epoch: 7.85 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861963349693136		[learning rate: 0.00062392]
	Learning Rate: 0.000623919
	LOSS [training: 4.861963349693136 | validation: 4.7733010578623585]
	TIME [epoch: 7.86 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878032041875002		[learning rate: 0.00062245]
	Learning Rate: 0.000622447
	LOSS [training: 4.878032041875002 | validation: 4.785904483099722]
	TIME [epoch: 7.88 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890407483777101		[learning rate: 0.00062098]
	Learning Rate: 0.000620979
	LOSS [training: 4.890407483777101 | validation: 4.776258181122503]
	TIME [epoch: 7.9 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.88381096527253		[learning rate: 0.00061951]
	Learning Rate: 0.000619514
	LOSS [training: 4.88381096527253 | validation: 4.764414198032504]
	TIME [epoch: 7.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.867600253339372		[learning rate: 0.00061805]
	Learning Rate: 0.000618053
	LOSS [training: 4.867600253339372 | validation: 4.7760705352953385]
	TIME [epoch: 7.85 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889753678270395		[learning rate: 0.0006166]
	Learning Rate: 0.000616595
	LOSS [training: 4.889753678270395 | validation: 4.795393706693291]
	TIME [epoch: 7.85 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9119142257692525		[learning rate: 0.00061514]
	Learning Rate: 0.000615141
	LOSS [training: 4.9119142257692525 | validation: 4.809880524302835]
	TIME [epoch: 7.87 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93473201858625		[learning rate: 0.00061369]
	Learning Rate: 0.00061369
	LOSS [training: 4.93473201858625 | validation: 4.843228023985462]
	TIME [epoch: 7.9 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939318873359225		[learning rate: 0.00061224]
	Learning Rate: 0.000612242
	LOSS [training: 4.939318873359225 | validation: 4.835361276372599]
	TIME [epoch: 7.86 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.934953333974661		[learning rate: 0.0006108]
	Learning Rate: 0.000610798
	LOSS [training: 4.934953333974661 | validation: 4.833300798369997]
	TIME [epoch: 7.86 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.940294155680983		[learning rate: 0.00060936]
	Learning Rate: 0.000609357
	LOSS [training: 4.940294155680983 | validation: 4.837653755224986]
	TIME [epoch: 7.86 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93344033999187		[learning rate: 0.00060792]
	Learning Rate: 0.00060792
	LOSS [training: 4.93344033999187 | validation: 4.8435807446440275]
	TIME [epoch: 7.88 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.954229133374401		[learning rate: 0.00060649]
	Learning Rate: 0.000606486
	LOSS [training: 4.954229133374401 | validation: 4.885811542901412]
	TIME [epoch: 7.9 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.010209750693135		[learning rate: 0.00060506]
	Learning Rate: 0.000605055
	LOSS [training: 5.010209750693135 | validation: 5.001089927106025]
	TIME [epoch: 7.85 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.074699076661842		[learning rate: 0.00060363]
	Learning Rate: 0.000603628
	LOSS [training: 5.074699076661842 | validation: 5.031880036861615]
	TIME [epoch: 7.86 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0917385748080966		[learning rate: 0.0006022]
	Learning Rate: 0.000602204
	LOSS [training: 5.0917385748080966 | validation: 5.0185926868459685]
	TIME [epoch: 7.85 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.082211472770657		[learning rate: 0.00060078]
	Learning Rate: 0.000600784
	LOSS [training: 5.082211472770657 | validation: 5.004104538953918]
	TIME [epoch: 7.87 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.035549832140014		[learning rate: 0.00059937]
	Learning Rate: 0.000599366
	LOSS [training: 5.035549832140014 | validation: 4.945200641593264]
	TIME [epoch: 7.89 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.033103850323554		[learning rate: 0.00059795]
	Learning Rate: 0.000597953
	LOSS [training: 5.033103850323554 | validation: 4.9668024938971]
	TIME [epoch: 7.86 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.058637624567642		[learning rate: 0.00059654]
	Learning Rate: 0.000596542
	LOSS [training: 5.058637624567642 | validation: 4.972957486911255]
	TIME [epoch: 7.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.032026522633829		[learning rate: 0.00059513]
	Learning Rate: 0.000595135
	LOSS [training: 5.032026522633829 | validation: 4.962251264628142]
	TIME [epoch: 7.85 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.042331206074481		[learning rate: 0.00059373]
	Learning Rate: 0.000593731
	LOSS [training: 5.042331206074481 | validation: 5.010993009011187]
	TIME [epoch: 7.87 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.06599646060646		[learning rate: 0.00059233]
	Learning Rate: 0.000592331
	LOSS [training: 5.06599646060646 | validation: 4.981704678811441]
	TIME [epoch: 7.9 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.036962015068424		[learning rate: 0.00059093]
	Learning Rate: 0.000590933
	LOSS [training: 5.036962015068424 | validation: 4.958143726648388]
	TIME [epoch: 7.85 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.019358863407717		[learning rate: 0.00058954]
	Learning Rate: 0.000589539
	LOSS [training: 5.019358863407717 | validation: 4.943272340998139]
	TIME [epoch: 7.86 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.016903366607048		[learning rate: 0.00058815]
	Learning Rate: 0.000588149
	LOSS [training: 5.016903366607048 | validation: 4.957197542492787]
	TIME [epoch: 7.85 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030692144185927		[learning rate: 0.00058676]
	Learning Rate: 0.000586761
	LOSS [training: 5.030692144185927 | validation: 4.957056966704659]
	TIME [epoch: 7.87 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025168153182888		[learning rate: 0.00058538]
	Learning Rate: 0.000585377
	LOSS [training: 5.025168153182888 | validation: 4.939356050674746]
	TIME [epoch: 7.89 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.008966667622854		[learning rate: 0.000584]
	Learning Rate: 0.000583997
	LOSS [training: 5.008966667622854 | validation: 4.934831908713807]
	TIME [epoch: 7.85 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.991976374700752		[learning rate: 0.00058262]
	Learning Rate: 0.000582619
	LOSS [training: 4.991976374700752 | validation: 4.858323324301448]
	TIME [epoch: 7.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.938776331745954		[learning rate: 0.00058124]
	Learning Rate: 0.000581245
	LOSS [training: 4.938776331745954 | validation: 4.865438957654035]
	TIME [epoch: 7.85 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9286350606260925		[learning rate: 0.00057987]
	Learning Rate: 0.000579874
	LOSS [training: 4.9286350606260925 | validation: 4.843634135004873]
	TIME [epoch: 7.86 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93456598304273		[learning rate: 0.00057851]
	Learning Rate: 0.000578506
	LOSS [training: 4.93456598304273 | validation: 4.831582131198708]
	TIME [epoch: 7.89 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.911117931558996		[learning rate: 0.00057714]
	Learning Rate: 0.000577141
	LOSS [training: 4.911117931558996 | validation: 4.79903671588214]
	TIME [epoch: 7.86 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8844155045425675		[learning rate: 0.00057578]
	Learning Rate: 0.00057578
	LOSS [training: 4.8844155045425675 | validation: 4.787757811559126]
	TIME [epoch: 7.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.874196863123013		[learning rate: 0.00057442]
	Learning Rate: 0.000574422
	LOSS [training: 4.874196863123013 | validation: 4.786809364179598]
	TIME [epoch: 7.85 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873144379243798		[learning rate: 0.00057307]
	Learning Rate: 0.000573067
	LOSS [training: 4.873144379243798 | validation: 4.801080674643464]
	TIME [epoch: 7.86 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.894351209978576		[learning rate: 0.00057171]
	Learning Rate: 0.000571715
	LOSS [training: 4.894351209978576 | validation: 4.811111726338484]
	TIME [epoch: 7.91 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.887492129497933		[learning rate: 0.00057037]
	Learning Rate: 0.000570366
	LOSS [training: 4.887492129497933 | validation: 4.806767403480734]
	TIME [epoch: 7.86 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.869438769709083		[learning rate: 0.00056902]
	Learning Rate: 0.000569021
	LOSS [training: 4.869438769709083 | validation: 4.7657592957287385]
	TIME [epoch: 7.85 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.85884319812042		[learning rate: 0.00056768]
	Learning Rate: 0.000567679
	LOSS [training: 4.85884319812042 | validation: 4.770759262073971]
	TIME [epoch: 7.85 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.866147751635028		[learning rate: 0.00056634]
	Learning Rate: 0.00056634
	LOSS [training: 4.866147751635028 | validation: 4.788237527893107]
	TIME [epoch: 7.87 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.870974610714257		[learning rate: 0.000565]
	Learning Rate: 0.000565004
	LOSS [training: 4.870974610714257 | validation: 4.7862011183610385]
	TIME [epoch: 7.9 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871069282186655		[learning rate: 0.00056367]
	Learning Rate: 0.000563671
	LOSS [training: 4.871069282186655 | validation: 4.772433815240438]
	TIME [epoch: 7.86 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86142419192339		[learning rate: 0.00056234]
	Learning Rate: 0.000562341
	LOSS [training: 4.86142419192339 | validation: 4.783486456584032]
	TIME [epoch: 7.86 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86505717106198		[learning rate: 0.00056101]
	Learning Rate: 0.000561015
	LOSS [training: 4.86505717106198 | validation: 4.757700161159301]
	TIME [epoch: 7.85 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.83740950314556		[learning rate: 0.00055969]
	Learning Rate: 0.000559692
	LOSS [training: 4.83740950314556 | validation: 4.740132962819915]
	TIME [epoch: 7.88 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.829584991292865		[learning rate: 0.00055837]
	Learning Rate: 0.000558371
	LOSS [training: 4.829584991292865 | validation: 4.75274851089476]
	TIME [epoch: 7.9 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.845491352241233		[learning rate: 0.00055705]
	Learning Rate: 0.000557054
	LOSS [training: 4.845491352241233 | validation: 4.777842489595944]
	TIME [epoch: 7.86 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8666602345331285		[learning rate: 0.00055574]
	Learning Rate: 0.00055574
	LOSS [training: 4.8666602345331285 | validation: 4.7720248397154315]
	TIME [epoch: 7.85 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.85135897397532		[learning rate: 0.00055443]
	Learning Rate: 0.000554429
	LOSS [training: 4.85135897397532 | validation: 4.732311458634369]
	TIME [epoch: 7.86 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.789043356374988		[learning rate: 0.00055312]
	Learning Rate: 0.000553121
	LOSS [training: 4.789043356374988 | validation: 4.701277701581035]
	TIME [epoch: 7.87 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.758029526433468		[learning rate: 0.00055182]
	Learning Rate: 0.000551817
	LOSS [training: 4.758029526433468 | validation: 4.682145859721036]
	TIME [epoch: 7.9 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.768846932922127		[learning rate: 0.00055052]
	Learning Rate: 0.000550515
	LOSS [training: 4.768846932922127 | validation: 4.717310263106303]
	TIME [epoch: 7.85 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.785433873511692		[learning rate: 0.00054922]
	Learning Rate: 0.000549216
	LOSS [training: 4.785433873511692 | validation: 4.728620212342895]
	TIME [epoch: 7.85 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8122414105697935		[learning rate: 0.00054792]
	Learning Rate: 0.000547921
	LOSS [training: 4.8122414105697935 | validation: 4.7566340143630175]
	TIME [epoch: 7.85 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.838057301140534		[learning rate: 0.00054663]
	Learning Rate: 0.000546629
	LOSS [training: 4.838057301140534 | validation: 4.770218017332769]
	TIME [epoch: 7.88 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.869611479891232		[learning rate: 0.00054534]
	Learning Rate: 0.000545339
	LOSS [training: 4.869611479891232 | validation: 4.784404220115475]
	TIME [epoch: 7.9 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890982319239754		[learning rate: 0.00054405]
	Learning Rate: 0.000544053
	LOSS [training: 4.890982319239754 | validation: 4.810188046180619]
	TIME [epoch: 7.86 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907468387892322		[learning rate: 0.00054277]
	Learning Rate: 0.000542769
	LOSS [training: 4.907468387892322 | validation: 4.819192092927057]
	TIME [epoch: 7.86 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.925762159687566		[learning rate: 0.00054149]
	Learning Rate: 0.000541489
	LOSS [training: 4.925762159687566 | validation: 4.833730669846156]
	TIME [epoch: 7.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9216461176976045		[learning rate: 0.00054021]
	Learning Rate: 0.000540212
	LOSS [training: 4.9216461176976045 | validation: 4.815069629112834]
	TIME [epoch: 7.86 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90528564244311		[learning rate: 0.00053894]
	Learning Rate: 0.000538938
	LOSS [training: 4.90528564244311 | validation: 4.806235203256145]
	TIME [epoch: 7.89 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.902288635049407		[learning rate: 0.00053767]
	Learning Rate: 0.000537666
	LOSS [training: 4.902288635049407 | validation: 4.8131325229775594]
	TIME [epoch: 7.86 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.914222604262756		[learning rate: 0.0005364]
	Learning Rate: 0.000536398
	LOSS [training: 4.914222604262756 | validation: 4.807440195905571]
	TIME [epoch: 7.85 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9064256512659705		[learning rate: 0.00053513]
	Learning Rate: 0.000535133
	LOSS [training: 4.9064256512659705 | validation: 4.80563853635837]
	TIME [epoch: 7.86 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90787351634164		[learning rate: 0.00053387]
	Learning Rate: 0.00053387
	LOSS [training: 4.90787351634164 | validation: 4.828713813566608]
	TIME [epoch: 7.87 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.910875474533029		[learning rate: 0.00053261]
	Learning Rate: 0.000532611
	LOSS [training: 4.910875474533029 | validation: 4.814438004598998]
	TIME [epoch: 7.88 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.905287128725785		[learning rate: 0.00053135]
	Learning Rate: 0.000531355
	LOSS [training: 4.905287128725785 | validation: 4.82066812353941]
	TIME [epoch: 7.85 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908436910073406		[learning rate: 0.0005301]
	Learning Rate: 0.000530101
	LOSS [training: 4.908436910073406 | validation: 4.809520519953806]
	TIME [epoch: 7.84 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.911091084311945		[learning rate: 0.00052885]
	Learning Rate: 0.000528851
	LOSS [training: 4.911091084311945 | validation: 4.830859738586953]
	TIME [epoch: 7.85 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919577472028412		[learning rate: 0.0005276]
	Learning Rate: 0.000527603
	LOSS [training: 4.919577472028412 | validation: 4.85695130955345]
	TIME [epoch: 7.87 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.940645258806438		[learning rate: 0.00052636]
	Learning Rate: 0.000526359
	LOSS [training: 4.940645258806438 | validation: 4.887286726154727]
	TIME [epoch: 7.89 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.938730359211759		[learning rate: 0.00052512]
	Learning Rate: 0.000525117
	LOSS [training: 4.938730359211759 | validation: 4.877660790269006]
	TIME [epoch: 7.86 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.928247243037267		[learning rate: 0.00052388]
	Learning Rate: 0.000523879
	LOSS [training: 4.928247243037267 | validation: 4.855995553863224]
	TIME [epoch: 7.85 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909579729518251		[learning rate: 0.00052264]
	Learning Rate: 0.000522643
	LOSS [training: 4.909579729518251 | validation: 4.821883350713712]
	TIME [epoch: 7.86 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901983920979337		[learning rate: 0.00052141]
	Learning Rate: 0.00052141
	LOSS [training: 4.901983920979337 | validation: 4.906137863638337]
	TIME [epoch: 7.88 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.962076619836936		[learning rate: 0.00052018]
	Learning Rate: 0.00052018
	LOSS [training: 4.962076619836936 | validation: 4.948669406551398]
	TIME [epoch: 7.88 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.986780670417557		[learning rate: 0.00051895]
	Learning Rate: 0.000518953
	LOSS [training: 4.986780670417557 | validation: 4.969239483940958]
	TIME [epoch: 7.86 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0102560390910575		[learning rate: 0.00051773]
	Learning Rate: 0.000517729
	LOSS [training: 5.0102560390910575 | validation: 5.019055786708015]
	TIME [epoch: 7.85 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.038516275956545		[learning rate: 0.00051651]
	Learning Rate: 0.000516508
	LOSS [training: 5.038516275956545 | validation: 4.9926952166777845]
	TIME [epoch: 7.84 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030471198647252		[learning rate: 0.00051529]
	Learning Rate: 0.000515289
	LOSS [training: 5.030471198647252 | validation: 4.99445338922092]
	TIME [epoch: 7.86 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0248095526408525		[learning rate: 0.00051407]
	Learning Rate: 0.000514074
	LOSS [training: 5.0248095526408525 | validation: 4.988063804984359]
	TIME [epoch: 7.89 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025906132163974		[learning rate: 0.00051286]
	Learning Rate: 0.000512861
	LOSS [training: 5.025906132163974 | validation: 4.963948000200861]
	TIME [epoch: 7.86 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.019508299475805		[learning rate: 0.00051165]
	Learning Rate: 0.000511652
	LOSS [training: 5.019508299475805 | validation: 4.969421851890352]
	TIME [epoch: 7.85 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.016248030632348		[learning rate: 0.00051044]
	Learning Rate: 0.000510445
	LOSS [training: 5.016248030632348 | validation: 4.936221050898892]
	TIME [epoch: 7.85 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.953950249013875		[learning rate: 0.00050924]
	Learning Rate: 0.000509241
	LOSS [training: 4.953950249013875 | validation: 4.881268410243848]
	TIME [epoch: 7.86 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922291828517178		[learning rate: 0.00050804]
	Learning Rate: 0.000508039
	LOSS [training: 4.922291828517178 | validation: 4.854310367586077]
	TIME [epoch: 7.89 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919621467066606		[learning rate: 0.00050684]
	Learning Rate: 0.000506841
	LOSS [training: 4.919621467066606 | validation: 4.838302074531667]
	TIME [epoch: 7.86 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895345152030763		[learning rate: 0.00050565]
	Learning Rate: 0.000505646
	LOSS [training: 4.895345152030763 | validation: 4.829757789465602]
	TIME [epoch: 7.85 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917877519791094		[learning rate: 0.00050445]
	Learning Rate: 0.000504453
	LOSS [training: 4.917877519791094 | validation: 4.872958351354755]
	TIME [epoch: 7.85 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917267878643511		[learning rate: 0.00050326]
	Learning Rate: 0.000503263
	LOSS [training: 4.917267878643511 | validation: 4.861665833683144]
	TIME [epoch: 7.86 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.89218675080954		[learning rate: 0.00050208]
	Learning Rate: 0.000502076
	LOSS [training: 4.89218675080954 | validation: 4.835278343910236]
	TIME [epoch: 7.9 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883602834010516		[learning rate: 0.00050089]
	Learning Rate: 0.000500891
	LOSS [training: 4.883602834010516 | validation: 4.866380164458166]
	TIME [epoch: 7.86 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.899293293321457		[learning rate: 0.00049971]
	Learning Rate: 0.00049971
	LOSS [training: 4.899293293321457 | validation: 4.880471459036758]
	TIME [epoch: 7.85 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890926054725512		[learning rate: 0.00049853]
	Learning Rate: 0.000498531
	LOSS [training: 4.890926054725512 | validation: 4.890154647667987]
	TIME [epoch: 7.85 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8934896662949745		[learning rate: 0.00049736]
	Learning Rate: 0.000497355
	LOSS [training: 4.8934896662949745 | validation: 4.863138757528451]
	TIME [epoch: 7.87 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871235097021083		[learning rate: 0.00049618]
	Learning Rate: 0.000496182
	LOSS [training: 4.871235097021083 | validation: 4.834253320716454]
	TIME [epoch: 7.9 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.888312631967887		[learning rate: 0.00049501]
	Learning Rate: 0.000495012
	LOSS [training: 4.888312631967887 | validation: 4.9027160339728395]
	TIME [epoch: 7.85 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909168091746081		[learning rate: 0.00049384]
	Learning Rate: 0.000493844
	LOSS [training: 4.909168091746081 | validation: 4.826117012519553]
	TIME [epoch: 7.85 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.828251004992036		[learning rate: 0.00049268]
	Learning Rate: 0.000492679
	LOSS [training: 4.828251004992036 | validation: 4.773721256630623]
	TIME [epoch: 7.85 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816557215383074		[learning rate: 0.00049152]
	Learning Rate: 0.000491517
	LOSS [training: 4.816557215383074 | validation: 4.759537585213657]
	TIME [epoch: 7.86 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.80075272364006		[learning rate: 0.00049036]
	Learning Rate: 0.000490358
	LOSS [training: 4.80075272364006 | validation: 4.748977614920573]
	TIME [epoch: 7.9 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8140948918677795		[learning rate: 0.0004892]
	Learning Rate: 0.000489201
	LOSS [training: 4.8140948918677795 | validation: 4.760451440457283]
	TIME [epoch: 7.85 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8216472939378106		[learning rate: 0.00048805]
	Learning Rate: 0.000488047
	LOSS [training: 4.8216472939378106 | validation: 4.727759173295107]
	TIME [epoch: 7.85 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.801496005422938		[learning rate: 0.0004869]
	Learning Rate: 0.000486896
	LOSS [training: 4.801496005422938 | validation: 4.778028098957193]
	TIME [epoch: 7.85 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814561750727767		[learning rate: 0.00048575]
	Learning Rate: 0.000485747
	LOSS [training: 4.814561750727767 | validation: 4.724993109057964]
	TIME [epoch: 7.86 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.764517525949438		[learning rate: 0.0004846]
	Learning Rate: 0.000484601
	LOSS [training: 4.764517525949438 | validation: 4.694077818369799]
	TIME [epoch: 7.89 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.769229942309752		[learning rate: 0.00048346]
	Learning Rate: 0.000483458
	LOSS [training: 4.769229942309752 | validation: 4.719221450119295]
	TIME [epoch: 7.86 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816211486798404		[learning rate: 0.00048232]
	Learning Rate: 0.000482318
	LOSS [training: 4.816211486798404 | validation: 4.762807000111387]
	TIME [epoch: 7.85 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8354419462941856		[learning rate: 0.00048118]
	Learning Rate: 0.00048118
	LOSS [training: 4.8354419462941856 | validation: 4.719228930608583]
	TIME [epoch: 7.86 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.786544073147656		[learning rate: 0.00048005]
	Learning Rate: 0.000480045
	LOSS [training: 4.786544073147656 | validation: 4.734456650694417]
	TIME [epoch: 7.86 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.813814229454609		[learning rate: 0.00047891]
	Learning Rate: 0.000478913
	LOSS [training: 4.813814229454609 | validation: 4.764438845365815]
	TIME [epoch: 7.89 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.832436915003842		[learning rate: 0.00047778]
	Learning Rate: 0.000477783
	LOSS [training: 4.832436915003842 | validation: 4.765836419255048]
	TIME [epoch: 7.86 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.84114748079023		[learning rate: 0.00047666]
	Learning Rate: 0.000476656
	LOSS [training: 4.84114748079023 | validation: 4.8006100893295205]
	TIME [epoch: 7.86 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.879615016135289		[learning rate: 0.00047553]
	Learning Rate: 0.000475532
	LOSS [training: 4.879615016135289 | validation: 4.8265443845463665]
	TIME [epoch: 7.86 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878568044723989		[learning rate: 0.00047441]
	Learning Rate: 0.00047441
	LOSS [training: 4.878568044723989 | validation: 4.783139484666858]
	TIME [epoch: 7.87 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.841871164821961		[learning rate: 0.00047329]
	Learning Rate: 0.000473291
	LOSS [training: 4.841871164821961 | validation: 4.771309812969282]
	TIME [epoch: 7.89 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.821248352654069		[learning rate: 0.00047217]
	Learning Rate: 0.000472175
	LOSS [training: 4.821248352654069 | validation: 4.743020623709404]
	TIME [epoch: 7.85 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803803950411899		[learning rate: 0.00047106]
	Learning Rate: 0.000471061
	LOSS [training: 4.803803950411899 | validation: 4.745503783227733]
	TIME [epoch: 7.84 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.819045687798058		[learning rate: 0.00046995]
	Learning Rate: 0.00046995
	LOSS [training: 4.819045687798058 | validation: 4.754050546460048]
	TIME [epoch: 7.85 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.833903880411735		[learning rate: 0.00046884]
	Learning Rate: 0.000468841
	LOSS [training: 4.833903880411735 | validation: 4.76143419539483]
	TIME [epoch: 7.86 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.834451742914801		[learning rate: 0.00046774]
	Learning Rate: 0.000467735
	LOSS [training: 4.834451742914801 | validation: 4.762289860737748]
	TIME [epoch: 7.89 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847497804001214		[learning rate: 0.00046663]
	Learning Rate: 0.000466632
	LOSS [training: 4.847497804001214 | validation: 4.799112609470992]
	TIME [epoch: 7.85 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.879160056773669		[learning rate: 0.00046553]
	Learning Rate: 0.000465531
	LOSS [training: 4.879160056773669 | validation: 4.812309347512706]
	TIME [epoch: 7.85 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889667098447902		[learning rate: 0.00046443]
	Learning Rate: 0.000464433
	LOSS [training: 4.889667098447902 | validation: 4.820372014186926]
	TIME [epoch: 7.85 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890699824431457		[learning rate: 0.00046334]
	Learning Rate: 0.000463338
	LOSS [training: 4.890699824431457 | validation: 4.821666347279687]
	TIME [epoch: 7.87 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.903411876622777		[learning rate: 0.00046224]
	Learning Rate: 0.000462245
	LOSS [training: 4.903411876622777 | validation: 4.809806019828397]
	TIME [epoch: 7.9 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891525039813254		[learning rate: 0.00046115]
	Learning Rate: 0.000461154
	LOSS [training: 4.891525039813254 | validation: 4.797824364134224]
	TIME [epoch: 7.85 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883660823264384		[learning rate: 0.00046007]
	Learning Rate: 0.000460066
	LOSS [training: 4.883660823264384 | validation: 4.80775226099292]
	TIME [epoch: 7.85 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.885120413794129		[learning rate: 0.00045898]
	Learning Rate: 0.000458981
	LOSS [training: 4.885120413794129 | validation: 4.814158322754471]
	TIME [epoch: 7.85 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891461995297813		[learning rate: 0.0004579]
	Learning Rate: 0.000457898
	LOSS [training: 4.891461995297813 | validation: 4.800127629343135]
	TIME [epoch: 7.87 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.864897767156111		[learning rate: 0.00045682]
	Learning Rate: 0.000456818
	LOSS [training: 4.864897767156111 | validation: 4.799175760924388]
	TIME [epoch: 7.89 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.88460407717019		[learning rate: 0.00045574]
	Learning Rate: 0.000455741
	LOSS [training: 4.88460407717019 | validation: 4.81720956769175]
	TIME [epoch: 7.84 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.89946172738774		[learning rate: 0.00045467]
	Learning Rate: 0.000454666
	LOSS [training: 4.89946172738774 | validation: 4.821774142050182]
	TIME [epoch: 7.85 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906699080905462		[learning rate: 0.00045359]
	Learning Rate: 0.000453593
	LOSS [training: 4.906699080905462 | validation: 4.827710947137195]
	TIME [epoch: 7.86 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.902352068410495		[learning rate: 0.00045252]
	Learning Rate: 0.000452523
	LOSS [training: 4.902352068410495 | validation: 4.802893895790549]
	TIME [epoch: 7.86 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873416702613819		[learning rate: 0.00045146]
	Learning Rate: 0.000451456
	LOSS [training: 4.873416702613819 | validation: 4.77542505041915]
	TIME [epoch: 7.9 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847259159925041		[learning rate: 0.00045039]
	Learning Rate: 0.000450391
	LOSS [training: 4.847259159925041 | validation: 4.770555534672765]
	TIME [epoch: 7.84 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847412432173629		[learning rate: 0.00044933]
	Learning Rate: 0.000449329
	LOSS [training: 4.847412432173629 | validation: 4.780144265690969]
	TIME [epoch: 7.85 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847163528832243		[learning rate: 0.00044827]
	Learning Rate: 0.000448269
	LOSS [training: 4.847163528832243 | validation: 4.762706274431622]
	TIME [epoch: 7.86 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.848182293153857		[learning rate: 0.00044721]
	Learning Rate: 0.000447211
	LOSS [training: 4.848182293153857 | validation: 4.798337927372222]
	TIME [epoch: 7.86 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.887831971710058		[learning rate: 0.00044616]
	Learning Rate: 0.000446157
	LOSS [training: 4.887831971710058 | validation: 4.811333143552773]
	TIME [epoch: 7.93 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8813072520191		[learning rate: 0.0004451]
	Learning Rate: 0.000445104
	LOSS [training: 4.8813072520191 | validation: 4.80461602512682]
	TIME [epoch: 7.86 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.887184965745769		[learning rate: 0.00044405]
	Learning Rate: 0.000444054
	LOSS [training: 4.887184965745769 | validation: 4.81123859537699]
	TIME [epoch: 7.85 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889088767493571		[learning rate: 0.00044301]
	Learning Rate: 0.000443007
	LOSS [training: 4.889088767493571 | validation: 4.834578096538483]
	TIME [epoch: 7.85 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901825740193316		[learning rate: 0.00044196]
	Learning Rate: 0.000441962
	LOSS [training: 4.901825740193316 | validation: 4.852064585232648]
	TIME [epoch: 7.88 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.89560217667778		[learning rate: 0.00044092]
	Learning Rate: 0.000440919
	LOSS [training: 4.89560217667778 | validation: 4.799075046523033]
	TIME [epoch: 7.89 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.868326530429611		[learning rate: 0.00043988]
	Learning Rate: 0.000439879
	LOSS [training: 4.868326530429611 | validation: 4.8072258243572135]
	TIME [epoch: 7.85 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.862799462231726		[learning rate: 0.00043884]
	Learning Rate: 0.000438842
	LOSS [training: 4.862799462231726 | validation: 4.802931278124852]
	TIME [epoch: 7.85 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.876175894792636		[learning rate: 0.00043781]
	Learning Rate: 0.000437806
	LOSS [training: 4.876175894792636 | validation: 4.816431259033042]
	TIME [epoch: 7.85 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8784963569543125		[learning rate: 0.00043677]
	Learning Rate: 0.000436774
	LOSS [training: 4.8784963569543125 | validation: 4.803291416870371]
	TIME [epoch: 7.88 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882769622658762		[learning rate: 0.00043574]
	Learning Rate: 0.000435743
	LOSS [training: 4.882769622658762 | validation: 4.818061708398805]
	TIME [epoch: 7.89 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.874449690124051		[learning rate: 0.00043472]
	Learning Rate: 0.000434715
	LOSS [training: 4.874449690124051 | validation: 4.8015712993366915]
	TIME [epoch: 7.86 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878985192620188		[learning rate: 0.00043369]
	Learning Rate: 0.00043369
	LOSS [training: 4.878985192620188 | validation: 4.8013365280418565]
	TIME [epoch: 7.85 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.870768654858838		[learning rate: 0.00043267]
	Learning Rate: 0.000432667
	LOSS [training: 4.870768654858838 | validation: 4.795698313409047]
	TIME [epoch: 7.85 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883471622943169		[learning rate: 0.00043165]
	Learning Rate: 0.000431647
	LOSS [training: 4.883471622943169 | validation: 4.829977335697566]
	TIME [epoch: 7.88 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909003644439646		[learning rate: 0.00043063]
	Learning Rate: 0.000430628
	LOSS [training: 4.909003644439646 | validation: 4.837266143947448]
	TIME [epoch: 7.88 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.911583255931426		[learning rate: 0.00042961]
	Learning Rate: 0.000429613
	LOSS [training: 4.911583255931426 | validation: 4.8278080557572585]
	TIME [epoch: 7.86 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.900891294174729		[learning rate: 0.0004286]
	Learning Rate: 0.000428599
	LOSS [training: 4.900891294174729 | validation: 4.818643066408221]
	TIME [epoch: 7.86 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.902495039980771		[learning rate: 0.00042759]
	Learning Rate: 0.000427588
	LOSS [training: 4.902495039980771 | validation: 4.8280334830881255]
	TIME [epoch: 7.85 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.900989379109331		[learning rate: 0.00042658]
	Learning Rate: 0.000426579
	LOSS [training: 4.900989379109331 | validation: 4.8152952847092845]
	TIME [epoch: 7.88 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889569690153875		[learning rate: 0.00042557]
	Learning Rate: 0.000425573
	LOSS [training: 4.889569690153875 | validation: 4.816427540402794]
	TIME [epoch: 7.89 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.896566667209582		[learning rate: 0.00042457]
	Learning Rate: 0.000424569
	LOSS [training: 4.896566667209582 | validation: 4.812157020576964]
	TIME [epoch: 7.85 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895429317309807		[learning rate: 0.00042357]
	Learning Rate: 0.000423568
	LOSS [training: 4.895429317309807 | validation: 4.8207712197937225]
	TIME [epoch: 7.86 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.920107711383816		[learning rate: 0.00042257]
	Learning Rate: 0.000422569
	LOSS [training: 4.920107711383816 | validation: 4.849262221593297]
	TIME [epoch: 7.85 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.947275812510841		[learning rate: 0.00042157]
	Learning Rate: 0.000421572
	LOSS [training: 4.947275812510841 | validation: 4.861509516178595]
	TIME [epoch: 7.88 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.94904872299192		[learning rate: 0.00042058]
	Learning Rate: 0.000420578
	LOSS [training: 4.94904872299192 | validation: 4.872647342960025]
	TIME [epoch: 7.88 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.956773345947499		[learning rate: 0.00041959]
	Learning Rate: 0.000419586
	LOSS [training: 4.956773345947499 | validation: 4.873288229609757]
	TIME [epoch: 7.85 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.969787357857246		[learning rate: 0.0004186]
	Learning Rate: 0.000418596
	LOSS [training: 4.969787357857246 | validation: 4.885704944072056]
	TIME [epoch: 7.86 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9691800247169935		[learning rate: 0.00041761]
	Learning Rate: 0.000417608
	LOSS [training: 4.9691800247169935 | validation: 4.885746237791006]
	TIME [epoch: 7.85 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9804958502455765		[learning rate: 0.00041662]
	Learning Rate: 0.000416623
	LOSS [training: 4.9804958502455765 | validation: 4.90272536096398]
	TIME [epoch: 7.88 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.995334069556469		[learning rate: 0.00041564]
	Learning Rate: 0.000415641
	LOSS [training: 4.995334069556469 | validation: 4.904972482583095]
	TIME [epoch: 7.89 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.990671204161961		[learning rate: 0.00041466]
	Learning Rate: 0.00041466
	LOSS [training: 4.990671204161961 | validation: 4.888327767588883]
	TIME [epoch: 7.86 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.970734788710423		[learning rate: 0.00041368]
	Learning Rate: 0.000413682
	LOSS [training: 4.970734788710423 | validation: 4.864554387974266]
	TIME [epoch: 7.86 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.944733852883921		[learning rate: 0.00041271]
	Learning Rate: 0.000412706
	LOSS [training: 4.944733852883921 | validation: 4.854573156272604]
	TIME [epoch: 7.86 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.948489476611698		[learning rate: 0.00041173]
	Learning Rate: 0.000411733
	LOSS [training: 4.948489476611698 | validation: 4.86763042163936]
	TIME [epoch: 7.88 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.947132105368763		[learning rate: 0.00041076]
	Learning Rate: 0.000410761
	LOSS [training: 4.947132105368763 | validation: 4.857442967902772]
	TIME [epoch: 7.89 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.940777185446631		[learning rate: 0.00040979]
	Learning Rate: 0.000409793
	LOSS [training: 4.940777185446631 | validation: 4.856433401147211]
	TIME [epoch: 7.85 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.933914594595584		[learning rate: 0.00040883]
	Learning Rate: 0.000408826
	LOSS [training: 4.933914594595584 | validation: 4.838994756024979]
	TIME [epoch: 7.86 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.927780686087498		[learning rate: 0.00040786]
	Learning Rate: 0.000407862
	LOSS [training: 4.927780686087498 | validation: 4.851903726252759]
	TIME [epoch: 7.86 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.941265489231022		[learning rate: 0.0004069]
	Learning Rate: 0.0004069
	LOSS [training: 4.941265489231022 | validation: 4.861402662759074]
	TIME [epoch: 7.88 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.961747682750316		[learning rate: 0.00040594]
	Learning Rate: 0.00040594
	LOSS [training: 4.961747682750316 | validation: 4.868318160296637]
	TIME [epoch: 7.89 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.964491914689857		[learning rate: 0.00040498]
	Learning Rate: 0.000404982
	LOSS [training: 4.964491914689857 | validation: 4.871808872885238]
	TIME [epoch: 7.86 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.963567896039658		[learning rate: 0.00040403]
	Learning Rate: 0.000404027
	LOSS [training: 4.963567896039658 | validation: 4.85466498833072]
	TIME [epoch: 7.86 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.955814848600646		[learning rate: 0.00040307]
	Learning Rate: 0.000403074
	LOSS [training: 4.955814848600646 | validation: 4.880897918195694]
	TIME [epoch: 7.86 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.963427652731864		[learning rate: 0.00040212]
	Learning Rate: 0.000402123
	LOSS [training: 4.963427652731864 | validation: 4.8671722020654435]
	TIME [epoch: 7.89 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.950152385139351		[learning rate: 0.00040117]
	Learning Rate: 0.000401175
	LOSS [training: 4.950152385139351 | validation: 4.866161584781897]
	TIME [epoch: 7.89 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.95482224609705		[learning rate: 0.00040023]
	Learning Rate: 0.000400228
	LOSS [training: 4.95482224609705 | validation: 4.8502714732179175]
	TIME [epoch: 7.86 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.946293584534536		[learning rate: 0.00039928]
	Learning Rate: 0.000399284
	LOSS [training: 4.946293584534536 | validation: 4.850858884604966]
	TIME [epoch: 7.85 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.94412596692569		[learning rate: 0.00039834]
	Learning Rate: 0.000398342
	LOSS [training: 4.94412596692569 | validation: 4.842116494976725]
	TIME [epoch: 7.85 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.928471014892089		[learning rate: 0.0003974]
	Learning Rate: 0.000397403
	LOSS [training: 4.928471014892089 | validation: 4.835888467915667]
	TIME [epoch: 7.88 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.913612026332432		[learning rate: 0.00039647]
	Learning Rate: 0.000396465
	LOSS [training: 4.913612026332432 | validation: 4.811953725286904]
	TIME [epoch: 7.89 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9028171688652264		[learning rate: 0.00039553]
	Learning Rate: 0.00039553
	LOSS [training: 4.9028171688652264 | validation: 4.825474952945132]
	TIME [epoch: 7.85 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.912413781539658		[learning rate: 0.0003946]
	Learning Rate: 0.000394597
	LOSS [training: 4.912413781539658 | validation: 4.83747722027403]
	TIME [epoch: 7.85 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9151785458758575		[learning rate: 0.00039367]
	Learning Rate: 0.000393666
	LOSS [training: 4.9151785458758575 | validation: 4.826485780505916]
	TIME [epoch: 7.85 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.920666800939384		[learning rate: 0.00039274]
	Learning Rate: 0.000392738
	LOSS [training: 4.920666800939384 | validation: 4.84603308769655]
	TIME [epoch: 7.88 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.932998172778567		[learning rate: 0.00039181]
	Learning Rate: 0.000391811
	LOSS [training: 4.932998172778567 | validation: 4.846442276866883]
	TIME [epoch: 7.89 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.925903558581935		[learning rate: 0.00039089]
	Learning Rate: 0.000390887
	LOSS [training: 4.925903558581935 | validation: 4.827065485893614]
	TIME [epoch: 7.86 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909595411794349		[learning rate: 0.00038997]
	Learning Rate: 0.000389965
	LOSS [training: 4.909595411794349 | validation: 4.8235467043577]
	TIME [epoch: 7.86 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.910011839085452		[learning rate: 0.00038905]
	Learning Rate: 0.000389045
	LOSS [training: 4.910011839085452 | validation: 4.813830322820506]
	TIME [epoch: 7.86 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901117155351948		[learning rate: 0.00038813]
	Learning Rate: 0.000388127
	LOSS [training: 4.901117155351948 | validation: 4.81678823125393]
	TIME [epoch: 7.89 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.896818989664599		[learning rate: 0.00038721]
	Learning Rate: 0.000387212
	LOSS [training: 4.896818989664599 | validation: 4.821286473532005]
	TIME [epoch: 7.89 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.903696361368365		[learning rate: 0.0003863]
	Learning Rate: 0.000386299
	LOSS [training: 4.903696361368365 | validation: 4.825065222739777]
	TIME [epoch: 7.86 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.903619357212246		[learning rate: 0.00038539]
	Learning Rate: 0.000385387
	LOSS [training: 4.903619357212246 | validation: 4.815203007470544]
	TIME [epoch: 7.86 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.899250552160639		[learning rate: 0.00038448]
	Learning Rate: 0.000384478
	LOSS [training: 4.899250552160639 | validation: 4.811685415967599]
	TIME [epoch: 7.85 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.899147370555086		[learning rate: 0.00038357]
	Learning Rate: 0.000383571
	LOSS [training: 4.899147370555086 | validation: 4.8191894947386125]
	TIME [epoch: 7.88 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891744595863889		[learning rate: 0.00038267]
	Learning Rate: 0.000382667
	LOSS [training: 4.891744595863889 | validation: 4.803064539280313]
	TIME [epoch: 7.89 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891009787785656		[learning rate: 0.00038176]
	Learning Rate: 0.000381764
	LOSS [training: 4.891009787785656 | validation: 4.821693729892333]
	TIME [epoch: 7.86 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.905914811707984		[learning rate: 0.00038086]
	Learning Rate: 0.000380863
	LOSS [training: 4.905914811707984 | validation: 4.834226684640126]
	TIME [epoch: 7.85 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.918606669366753		[learning rate: 0.00037997]
	Learning Rate: 0.000379965
	LOSS [training: 4.918606669366753 | validation: 4.849233542976153]
	TIME [epoch: 7.85 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.928510984598553		[learning rate: 0.00037907]
	Learning Rate: 0.000379069
	LOSS [training: 4.928510984598553 | validation: 4.868792161684715]
	TIME [epoch: 7.88 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.935358378996431		[learning rate: 0.00037817]
	Learning Rate: 0.000378175
	LOSS [training: 4.935358378996431 | validation: 4.8766208462918055]
	TIME [epoch: 7.87 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939077926076097		[learning rate: 0.00037728]
	Learning Rate: 0.000377283
	LOSS [training: 4.939077926076097 | validation: 4.862236582038015]
	TIME [epoch: 7.85 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.929230365419272		[learning rate: 0.00037639]
	Learning Rate: 0.000376393
	LOSS [training: 4.929230365419272 | validation: 4.854749410778032]
	TIME [epoch: 7.84 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917001533470018		[learning rate: 0.0003755]
	Learning Rate: 0.000375505
	LOSS [training: 4.917001533470018 | validation: 4.835846903465188]
	TIME [epoch: 7.86 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.896189046769617		[learning rate: 0.00037462]
	Learning Rate: 0.000374619
	LOSS [training: 4.896189046769617 | validation: 4.831134134918013]
	TIME [epoch: 7.88 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.888094046474433		[learning rate: 0.00037374]
	Learning Rate: 0.000373735
	LOSS [training: 4.888094046474433 | validation: 4.809463716245871]
	TIME [epoch: 7.88 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8690104614323495		[learning rate: 0.00037285]
	Learning Rate: 0.000372854
	LOSS [training: 4.8690104614323495 | validation: 4.815446309571417]
	TIME [epoch: 7.85 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891453677676347		[learning rate: 0.00037197]
	Learning Rate: 0.000371974
	LOSS [training: 4.891453677676347 | validation: 4.831716447608356]
	TIME [epoch: 7.86 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.900270393010608		[learning rate: 0.0003711]
	Learning Rate: 0.000371097
	LOSS [training: 4.900270393010608 | validation: 4.839765549441521]
	TIME [epoch: 7.85 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908184724971187		[learning rate: 0.00037022]
	Learning Rate: 0.000370221
	LOSS [training: 4.908184724971187 | validation: 4.846719087347742]
	TIME [epoch: 7.88 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.918288676408003		[learning rate: 0.00036935]
	Learning Rate: 0.000369348
	LOSS [training: 4.918288676408003 | validation: 4.8589268237902825]
	TIME [epoch: 7.89 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908029769678675		[learning rate: 0.00036848]
	Learning Rate: 0.000368477
	LOSS [training: 4.908029769678675 | validation: 4.836258644613425]
	TIME [epoch: 7.86 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.900939240101199		[learning rate: 0.00036761]
	Learning Rate: 0.000367608
	LOSS [training: 4.900939240101199 | validation: 4.832485760453906]
	TIME [epoch: 7.86 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.898850199791839		[learning rate: 0.00036674]
	Learning Rate: 0.000366741
	LOSS [training: 4.898850199791839 | validation: 4.8500930722136175]
	TIME [epoch: 7.87 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891595575589019		[learning rate: 0.00036588]
	Learning Rate: 0.000365876
	LOSS [training: 4.891595575589019 | validation: 4.851549505141572]
	TIME [epoch: 7.89 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9068504713632715		[learning rate: 0.00036501]
	Learning Rate: 0.000365012
	LOSS [training: 4.9068504713632715 | validation: 4.8606031191820005]
	TIME [epoch: 7.9 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9041982143165		[learning rate: 0.00036415]
	Learning Rate: 0.000364152
	LOSS [training: 4.9041982143165 | validation: 4.843820435984897]
	TIME [epoch: 7.87 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890269537253637		[learning rate: 0.00036329]
	Learning Rate: 0.000363293
	LOSS [training: 4.890269537253637 | validation: 4.827309621496399]
	TIME [epoch: 7.87 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.887350615386229		[learning rate: 0.00036244]
	Learning Rate: 0.000362436
	LOSS [training: 4.887350615386229 | validation: 4.848958088155465]
	TIME [epoch: 7.87 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.902007108317471		[learning rate: 0.00036158]
	Learning Rate: 0.000361581
	LOSS [training: 4.902007108317471 | validation: 4.871135689544011]
	TIME [epoch: 7.9 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.897968667891959		[learning rate: 0.00036073]
	Learning Rate: 0.000360728
	LOSS [training: 4.897968667891959 | validation: 4.829897265294987]
	TIME [epoch: 7.9 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.870934948353533		[learning rate: 0.00035988]
	Learning Rate: 0.000359877
	LOSS [training: 4.870934948353533 | validation: 4.822790812425936]
	TIME [epoch: 7.87 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.875540747521953		[learning rate: 0.00035903]
	Learning Rate: 0.000359028
	LOSS [training: 4.875540747521953 | validation: 4.855714020323099]
	TIME [epoch: 7.87 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882162699789044		[learning rate: 0.00035818]
	Learning Rate: 0.000358181
	LOSS [training: 4.882162699789044 | validation: 4.853732607829711]
	TIME [epoch: 7.87 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90230923372525		[learning rate: 0.00035734]
	Learning Rate: 0.000357336
	LOSS [training: 4.90230923372525 | validation: 4.857265654690879]
	TIME [epoch: 7.91 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907158547252132		[learning rate: 0.00035649]
	Learning Rate: 0.000356493
	LOSS [training: 4.907158547252132 | validation: 4.851128135502708]
	TIME [epoch: 7.89 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909814234514613		[learning rate: 0.00035565]
	Learning Rate: 0.000355652
	LOSS [training: 4.909814234514613 | validation: 4.841546050887116]
	TIME [epoch: 7.86 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9143565569787135		[learning rate: 0.00035481]
	Learning Rate: 0.000354813
	LOSS [training: 4.9143565569787135 | validation: 4.85446571240575]
	TIME [epoch: 7.87 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.926385103054837		[learning rate: 0.00035398]
	Learning Rate: 0.000353976
	LOSS [training: 4.926385103054837 | validation: 4.84385553633172]
	TIME [epoch: 7.87 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919787340586212		[learning rate: 0.00035314]
	Learning Rate: 0.000353141
	LOSS [training: 4.919787340586212 | validation: 4.873689793716948]
	TIME [epoch: 7.9 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.928116561116646		[learning rate: 0.00035231]
	Learning Rate: 0.000352309
	LOSS [training: 4.928116561116646 | validation: 4.859271506591039]
	TIME [epoch: 7.89 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90956090079473		[learning rate: 0.00035148]
	Learning Rate: 0.000351477
	LOSS [training: 4.90956090079473 | validation: 4.849926041088779]
	TIME [epoch: 7.87 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895246277150335		[learning rate: 0.00035065]
	Learning Rate: 0.000350648
	LOSS [training: 4.895246277150335 | validation: 4.846354823841368]
	TIME [epoch: 7.87 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.894086531254743		[learning rate: 0.00034982]
	Learning Rate: 0.000349821
	LOSS [training: 4.894086531254743 | validation: 4.830755299967217]
	TIME [epoch: 7.87 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.872010285630547		[learning rate: 0.000349]
	Learning Rate: 0.000348996
	LOSS [training: 4.872010285630547 | validation: 4.801309098312668]
	TIME [epoch: 7.9 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.851458797194246		[learning rate: 0.00034817]
	Learning Rate: 0.000348173
	LOSS [training: 4.851458797194246 | validation: 4.788174572867733]
	TIME [epoch: 7.89 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.827046200849015		[learning rate: 0.00034735]
	Learning Rate: 0.000347352
	LOSS [training: 4.827046200849015 | validation: 4.743013033006955]
	TIME [epoch: 7.87 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8092878819289995		[learning rate: 0.00034653]
	Learning Rate: 0.000346532
	LOSS [training: 4.8092878819289995 | validation: 4.73495079656403]
	TIME [epoch: 7.87 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8089660321240055		[learning rate: 0.00034571]
	Learning Rate: 0.000345715
	LOSS [training: 4.8089660321240055 | validation: 4.738228950889462]
	TIME [epoch: 7.87 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.808867967705236		[learning rate: 0.0003449]
	Learning Rate: 0.000344899
	LOSS [training: 4.808867967705236 | validation: 4.752067126355291]
	TIME [epoch: 7.89 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816795834898764		[learning rate: 0.00034409]
	Learning Rate: 0.000344086
	LOSS [training: 4.816795834898764 | validation: 4.747036978015613]
	TIME [epoch: 7.91 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.819655376489454		[learning rate: 0.00034327]
	Learning Rate: 0.000343274
	LOSS [training: 4.819655376489454 | validation: 4.747365859505699]
	TIME [epoch: 7.86 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826299507594385		[learning rate: 0.00034246]
	Learning Rate: 0.000342464
	LOSS [training: 4.826299507594385 | validation: 4.74407684679197]
	TIME [epoch: 7.87 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.811508347110253		[learning rate: 0.00034166]
	Learning Rate: 0.000341657
	LOSS [training: 4.811508347110253 | validation: 4.735853055063059]
	TIME [epoch: 7.87 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.817625443197871		[learning rate: 0.00034085]
	Learning Rate: 0.000340851
	LOSS [training: 4.817625443197871 | validation: 4.754334432604276]
	TIME [epoch: 7.9 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.819168671250898		[learning rate: 0.00034005]
	Learning Rate: 0.000340047
	LOSS [training: 4.819168671250898 | validation: 4.732456869635467]
	TIME [epoch: 7.89 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.806415671637121		[learning rate: 0.00033924]
	Learning Rate: 0.000339245
	LOSS [training: 4.806415671637121 | validation: 4.742554970284186]
	TIME [epoch: 7.86 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815750039004795		[learning rate: 0.00033844]
	Learning Rate: 0.000338444
	LOSS [training: 4.815750039004795 | validation: 4.745139279155463]
	TIME [epoch: 7.87 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8276398482483085		[learning rate: 0.00033765]
	Learning Rate: 0.000337646
	LOSS [training: 4.8276398482483085 | validation: 4.763766687416731]
	TIME [epoch: 7.86 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.850480310863812		[learning rate: 0.00033685]
	Learning Rate: 0.00033685
	LOSS [training: 4.850480310863812 | validation: 4.78499129687323]
	TIME [epoch: 7.9 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.866048145710681		[learning rate: 0.00033605]
	Learning Rate: 0.000336055
	LOSS [training: 4.866048145710681 | validation: 4.787432671575454]
	TIME [epoch: 7.89 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.870261966521059		[learning rate: 0.00033526]
	Learning Rate: 0.000335262
	LOSS [training: 4.870261966521059 | validation: 4.788747047954853]
	TIME [epoch: 7.86 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.862518967914782		[learning rate: 0.00033447]
	Learning Rate: 0.000334471
	LOSS [training: 4.862518967914782 | validation: 4.784911603653021]
	TIME [epoch: 7.86 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861112405682218		[learning rate: 0.00033368]
	Learning Rate: 0.000333682
	LOSS [training: 4.861112405682218 | validation: 4.78901648250154]
	TIME [epoch: 7.87 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.867709056419346		[learning rate: 0.0003329]
	Learning Rate: 0.000332895
	LOSS [training: 4.867709056419346 | validation: 4.802135103384069]
	TIME [epoch: 7.91 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.872266527212601		[learning rate: 0.00033211]
	Learning Rate: 0.00033211
	LOSS [training: 4.872266527212601 | validation: 4.813344236062738]
	TIME [epoch: 7.89 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.886362625253007		[learning rate: 0.00033133]
	Learning Rate: 0.000331327
	LOSS [training: 4.886362625253007 | validation: 4.829342860808671]
	TIME [epoch: 7.87 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889494720174367		[learning rate: 0.00033055]
	Learning Rate: 0.000330545
	LOSS [training: 4.889494720174367 | validation: 4.8192573748685135]
	TIME [epoch: 7.87 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883326452378651		[learning rate: 0.00032977]
	Learning Rate: 0.000329765
	LOSS [training: 4.883326452378651 | validation: 4.802265342218318]
	TIME [epoch: 7.87 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.876902680378947		[learning rate: 0.00032899]
	Learning Rate: 0.000328988
	LOSS [training: 4.876902680378947 | validation: 4.810285758862967]
	TIME [epoch: 7.91 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.859217547130125		[learning rate: 0.00032821]
	Learning Rate: 0.000328212
	LOSS [training: 4.859217547130125 | validation: 4.785854613447066]
	TIME [epoch: 7.88 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.828968208026979		[learning rate: 0.00032744]
	Learning Rate: 0.000327437
	LOSS [training: 4.828968208026979 | validation: 4.750107262008345]
	TIME [epoch: 7.85 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.821724268681633		[learning rate: 0.00032666]
	Learning Rate: 0.000326665
	LOSS [training: 4.821724268681633 | validation: 4.768192938239263]
	TIME [epoch: 7.86 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826643193423895		[learning rate: 0.00032589]
	Learning Rate: 0.000325894
	LOSS [training: 4.826643193423895 | validation: 4.749985980034561]
	TIME [epoch: 7.86 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.823508675870764		[learning rate: 0.00032513]
	Learning Rate: 0.000325126
	LOSS [training: 4.823508675870764 | validation: 4.756950850927151]
	TIME [epoch: 7.9 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.833950149453724		[learning rate: 0.00032436]
	Learning Rate: 0.000324359
	LOSS [training: 4.833950149453724 | validation: 4.74857331783952]
	TIME [epoch: 7.89 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815973398004691		[learning rate: 0.00032359]
	Learning Rate: 0.000323594
	LOSS [training: 4.815973398004691 | validation: 4.737897919273115]
	TIME [epoch: 7.87 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.810274904701665		[learning rate: 0.00032283]
	Learning Rate: 0.00032283
	LOSS [training: 4.810274904701665 | validation: 4.726759969622897]
	TIME [epoch: 7.87 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.804202989057949		[learning rate: 0.00032207]
	Learning Rate: 0.000322069
	LOSS [training: 4.804202989057949 | validation: 4.728515854305975]
	TIME [epoch: 7.86 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8022458792744676		[learning rate: 0.00032131]
	Learning Rate: 0.000321309
	LOSS [training: 4.8022458792744676 | validation: 4.7164913251819005]
	TIME [epoch: 7.9 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.785541004634551		[learning rate: 0.00032055]
	Learning Rate: 0.000320551
	LOSS [training: 4.785541004634551 | validation: 4.690438433783261]
	TIME [epoch: 7.88 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.770509454702854		[learning rate: 0.0003198]
	Learning Rate: 0.000319795
	LOSS [training: 4.770509454702854 | validation: 4.695486884381987]
	TIME [epoch: 7.87 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.77658497281356		[learning rate: 0.00031904]
	Learning Rate: 0.000319041
	LOSS [training: 4.77658497281356 | validation: 4.721208913127004]
	TIME [epoch: 7.87 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.784803520181717		[learning rate: 0.00031829]
	Learning Rate: 0.000318288
	LOSS [training: 4.784803520181717 | validation: 4.701579870612143]
	TIME [epoch: 7.86 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.781472447731827		[learning rate: 0.00031754]
	Learning Rate: 0.000317537
	LOSS [training: 4.781472447731827 | validation: 4.708208393524918]
	TIME [epoch: 7.91 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.78720841847349		[learning rate: 0.00031679]
	Learning Rate: 0.000316788
	LOSS [training: 4.78720841847349 | validation: 4.723908199477667]
	TIME [epoch: 7.89 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.813806654634142		[learning rate: 0.00031604]
	Learning Rate: 0.000316041
	LOSS [training: 4.813806654634142 | validation: 4.743203453872422]
	TIME [epoch: 7.86 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814124674913079		[learning rate: 0.0003153]
	Learning Rate: 0.000315296
	LOSS [training: 4.814124674913079 | validation: 4.738476745600348]
	TIME [epoch: 7.86 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.820706000291825		[learning rate: 0.00031455]
	Learning Rate: 0.000314552
	LOSS [training: 4.820706000291825 | validation: 4.731484326315352]
	TIME [epoch: 7.86 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.813619440906988		[learning rate: 0.00031381]
	Learning Rate: 0.00031381
	LOSS [training: 4.813619440906988 | validation: 4.730368526094773]
	TIME [epoch: 7.91 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.802887546941413		[learning rate: 0.00031307]
	Learning Rate: 0.00031307
	LOSS [training: 4.802887546941413 | validation: 4.719246508529527]
	TIME [epoch: 7.89 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.801389506170498		[learning rate: 0.00031233]
	Learning Rate: 0.000312331
	LOSS [training: 4.801389506170498 | validation: 4.742330529452895]
	TIME [epoch: 7.86 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.828550127895387		[learning rate: 0.00031159]
	Learning Rate: 0.000311594
	LOSS [training: 4.828550127895387 | validation: 4.749506182205879]
	TIME [epoch: 7.86 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.836655358932033		[learning rate: 0.00031086]
	Learning Rate: 0.00031086
	LOSS [training: 4.836655358932033 | validation: 4.755791563291658]
	TIME [epoch: 7.86 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.833459496981992		[learning rate: 0.00031013]
	Learning Rate: 0.000310126
	LOSS [training: 4.833459496981992 | validation: 4.743071454701825]
	TIME [epoch: 7.9 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.824044837606194		[learning rate: 0.00030939]
	Learning Rate: 0.000309395
	LOSS [training: 4.824044837606194 | validation: 4.746070600686252]
	TIME [epoch: 7.88 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.831731419159676		[learning rate: 0.00030866]
	Learning Rate: 0.000308665
	LOSS [training: 4.831731419159676 | validation: 4.7676435273614635]
	TIME [epoch: 7.86 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.84723359951599		[learning rate: 0.00030794]
	Learning Rate: 0.000307937
	LOSS [training: 4.84723359951599 | validation: 4.762152548997914]
	TIME [epoch: 7.86 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.852264819298758		[learning rate: 0.00030721]
	Learning Rate: 0.00030721
	LOSS [training: 4.852264819298758 | validation: 4.760002800754648]
	TIME [epoch: 7.87 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.832320383711179		[learning rate: 0.00030649]
	Learning Rate: 0.000306486
	LOSS [training: 4.832320383711179 | validation: 4.754195528924344]
	TIME [epoch: 7.89 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.838005735047317		[learning rate: 0.00030576]
	Learning Rate: 0.000305763
	LOSS [training: 4.838005735047317 | validation: 4.756543287262069]
	TIME [epoch: 7.88 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.838153623647942		[learning rate: 0.00030504]
	Learning Rate: 0.000305042
	LOSS [training: 4.838153623647942 | validation: 4.762214820858453]
	TIME [epoch: 7.86 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8387564821824975		[learning rate: 0.00030432]
	Learning Rate: 0.000304322
	LOSS [training: 4.8387564821824975 | validation: 4.768069596471927]
	TIME [epoch: 7.86 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.856105279374362		[learning rate: 0.0003036]
	Learning Rate: 0.000303604
	LOSS [training: 4.856105279374362 | validation: 4.798830570047388]
	TIME [epoch: 7.86 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883280780250329		[learning rate: 0.00030289]
	Learning Rate: 0.000302888
	LOSS [training: 4.883280780250329 | validation: 4.807215237396255]
	TIME [epoch: 7.9 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.89326167601967		[learning rate: 0.00030217]
	Learning Rate: 0.000302174
	LOSS [training: 4.89326167601967 | validation: 4.814299655843019]
	TIME [epoch: 7.88 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890235183500966		[learning rate: 0.00030146]
	Learning Rate: 0.000301461
	LOSS [training: 4.890235183500966 | validation: 4.803870563156014]
	TIME [epoch: 7.86 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.889446909969806		[learning rate: 0.00030075]
	Learning Rate: 0.00030075
	LOSS [training: 4.889446909969806 | validation: 4.804959907278458]
	TIME [epoch: 7.85 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.887843203001417		[learning rate: 0.00030004]
	Learning Rate: 0.00030004
	LOSS [training: 4.887843203001417 | validation: 4.810620455109181]
	TIME [epoch: 7.86 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.881231211210591		[learning rate: 0.00029933]
	Learning Rate: 0.000299332
	LOSS [training: 4.881231211210591 | validation: 4.80290363404882]
	TIME [epoch: 7.9 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8734870895892834		[learning rate: 0.00029863]
	Learning Rate: 0.000298626
	LOSS [training: 4.8734870895892834 | validation: 4.790112527339751]
	TIME [epoch: 7.88 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.874875588567054		[learning rate: 0.00029792]
	Learning Rate: 0.000297922
	LOSS [training: 4.874875588567054 | validation: 4.807208447849158]
	TIME [epoch: 7.86 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882600772104068		[learning rate: 0.00029722]
	Learning Rate: 0.000297219
	LOSS [training: 4.882600772104068 | validation: 4.800169959190955]
	TIME [epoch: 7.86 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873089422674816		[learning rate: 0.00029652]
	Learning Rate: 0.000296518
	LOSS [training: 4.873089422674816 | validation: 4.78647959206427]
	TIME [epoch: 7.85 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.858433237982799		[learning rate: 0.00029582]
	Learning Rate: 0.000295819
	LOSS [training: 4.858433237982799 | validation: 4.780623985563082]
	TIME [epoch: 7.9 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8574502269740005		[learning rate: 0.00029512]
	Learning Rate: 0.000295121
	LOSS [training: 4.8574502269740005 | validation: 4.778712728701281]
	TIME [epoch: 7.88 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8614693329193885		[learning rate: 0.00029442]
	Learning Rate: 0.000294425
	LOSS [training: 4.8614693329193885 | validation: 4.782703944273104]
	TIME [epoch: 7.85 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.855861120714762		[learning rate: 0.00029373]
	Learning Rate: 0.00029373
	LOSS [training: 4.855861120714762 | validation: 4.77911459348187]
	TIME [epoch: 7.86 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.866253731096811		[learning rate: 0.00029304]
	Learning Rate: 0.000293037
	LOSS [training: 4.866253731096811 | validation: 4.786755872808152]
	TIME [epoch: 7.85 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871053177896794		[learning rate: 0.00029235]
	Learning Rate: 0.000292346
	LOSS [training: 4.871053177896794 | validation: 4.7961977541401035]
	TIME [epoch: 7.89 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873613480609182		[learning rate: 0.00029166]
	Learning Rate: 0.000291657
	LOSS [training: 4.873613480609182 | validation: 4.792874168005243]
	TIME [epoch: 7.87 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.87454982552322		[learning rate: 0.00029097]
	Learning Rate: 0.000290969
	LOSS [training: 4.87454982552322 | validation: 4.81094294069351]
	TIME [epoch: 7.86 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.888213722613193		[learning rate: 0.00029028]
	Learning Rate: 0.000290282
	LOSS [training: 4.888213722613193 | validation: 4.832295153092185]
	TIME [epoch: 7.86 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.90588197072371		[learning rate: 0.0002896]
	Learning Rate: 0.000289598
	LOSS [training: 4.90588197072371 | validation: 4.831533701552557]
	TIME [epoch: 7.86 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895665957607799		[learning rate: 0.00028891]
	Learning Rate: 0.000288914
	LOSS [training: 4.895665957607799 | validation: 4.811299920240132]
	TIME [epoch: 7.89 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878232139008757		[learning rate: 0.00028823]
	Learning Rate: 0.000288233
	LOSS [training: 4.878232139008757 | validation: 4.801015257062703]
	TIME [epoch: 7.88 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.883257993037996		[learning rate: 0.00028755]
	Learning Rate: 0.000287553
	LOSS [training: 4.883257993037996 | validation: 4.801977597183658]
	TIME [epoch: 7.86 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8885185042217385		[learning rate: 0.00028687]
	Learning Rate: 0.000286875
	LOSS [training: 4.8885185042217385 | validation: 4.815201059660535]
	TIME [epoch: 7.85 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8954026443196		[learning rate: 0.0002862]
	Learning Rate: 0.000286198
	LOSS [training: 4.8954026443196 | validation: 4.816451562603293]
	TIME [epoch: 7.85 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895151580398175		[learning rate: 0.00028552]
	Learning Rate: 0.000285523
	LOSS [training: 4.895151580398175 | validation: 4.817792592081064]
	TIME [epoch: 7.89 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908442920714412		[learning rate: 0.00028485]
	Learning Rate: 0.000284849
	LOSS [training: 4.908442920714412 | validation: 4.825636612197112]
	TIME [epoch: 7.88 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.898204642582125		[learning rate: 0.00028418]
	Learning Rate: 0.000284178
	LOSS [training: 4.898204642582125 | validation: 4.816194388456404]
	TIME [epoch: 7.86 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.897504113252158		[learning rate: 0.00028351]
	Learning Rate: 0.000283507
	LOSS [training: 4.897504113252158 | validation: 4.815533332302161]
	TIME [epoch: 7.86 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.900290696846216		[learning rate: 0.00028284]
	Learning Rate: 0.000282839
	LOSS [training: 4.900290696846216 | validation: 4.821971949272541]
	TIME [epoch: 7.85 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906652900717612		[learning rate: 0.00028217]
	Learning Rate: 0.000282171
	LOSS [training: 4.906652900717612 | validation: 4.8330667258717845]
	TIME [epoch: 7.89 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.916193017122277		[learning rate: 0.00028151]
	Learning Rate: 0.000281506
	LOSS [training: 4.916193017122277 | validation: 4.82954676557655]
	TIME [epoch: 7.88 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907068997007274		[learning rate: 0.00028084]
	Learning Rate: 0.000280842
	LOSS [training: 4.907068997007274 | validation: 4.806954414283673]
	TIME [epoch: 7.85 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.886026476060568		[learning rate: 0.00028018]
	Learning Rate: 0.000280179
	LOSS [training: 4.886026476060568 | validation: 4.80551843475752]
	TIME [epoch: 7.85 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.886738145948279		[learning rate: 0.00027952]
	Learning Rate: 0.000279518
	LOSS [training: 4.886738145948279 | validation: 4.788843992535266]
	TIME [epoch: 7.86 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86649453977593		[learning rate: 0.00027886]
	Learning Rate: 0.000278859
	LOSS [training: 4.86649453977593 | validation: 4.787544970075001]
	TIME [epoch: 7.9 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.859754172514045		[learning rate: 0.0002782]
	Learning Rate: 0.000278201
	LOSS [training: 4.859754172514045 | validation: 4.774629700805905]
	TIME [epoch: 7.89 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.852173524620687		[learning rate: 0.00027755]
	Learning Rate: 0.000277545
	LOSS [training: 4.852173524620687 | validation: 4.769406608050625]
	TIME [epoch: 7.86 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847857047655113		[learning rate: 0.00027689]
	Learning Rate: 0.00027689
	LOSS [training: 4.847857047655113 | validation: 4.774089770999375]
	TIME [epoch: 7.85 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.845767860546987		[learning rate: 0.00027624]
	Learning Rate: 0.000276237
	LOSS [training: 4.845767860546987 | validation: 4.763105601802351]
	TIME [epoch: 7.85 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.830557890988291		[learning rate: 0.00027559]
	Learning Rate: 0.000275586
	LOSS [training: 4.830557890988291 | validation: 4.746557310897561]
	TIME [epoch: 7.9 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816134032601128		[learning rate: 0.00027494]
	Learning Rate: 0.000274935
	LOSS [training: 4.816134032601128 | validation: 4.727699776386499]
	TIME [epoch: 7.87 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.79837807855609		[learning rate: 0.00027429]
	Learning Rate: 0.000274287
	LOSS [training: 4.79837807855609 | validation: 4.719446673834623]
	TIME [epoch: 7.86 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794178417547316		[learning rate: 0.00027364]
	Learning Rate: 0.00027364
	LOSS [training: 4.794178417547316 | validation: 4.713139110650928]
	TIME [epoch: 7.85 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.792112795637337		[learning rate: 0.00027299]
	Learning Rate: 0.000272994
	LOSS [training: 4.792112795637337 | validation: 4.717686425797555]
	TIME [epoch: 7.86 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.788033229557606		[learning rate: 0.00027235]
	Learning Rate: 0.000272351
	LOSS [training: 4.788033229557606 | validation: 4.710025419672834]
	TIME [epoch: 7.9 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.779230522183287		[learning rate: 0.00027171]
	Learning Rate: 0.000271708
	LOSS [training: 4.779230522183287 | validation: 4.715956093290351]
	TIME [epoch: 7.87 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794717948897793		[learning rate: 0.00027107]
	Learning Rate: 0.000271067
	LOSS [training: 4.794717948897793 | validation: 4.738299160929335]
	TIME [epoch: 7.86 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8133591859914775		[learning rate: 0.00027043]
	Learning Rate: 0.000270428
	LOSS [training: 4.8133591859914775 | validation: 4.751817713427648]
	TIME [epoch: 7.86 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.82966017686299		[learning rate: 0.00026979]
	Learning Rate: 0.00026979
	LOSS [training: 4.82966017686299 | validation: 4.75887041420954]
	TIME [epoch: 7.86 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.837139059896382		[learning rate: 0.00026915]
	Learning Rate: 0.000269154
	LOSS [training: 4.837139059896382 | validation: 4.773616937077721]
	TIME [epoch: 7.9 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8420640347016395		[learning rate: 0.00026852]
	Learning Rate: 0.000268519
	LOSS [training: 4.8420640347016395 | validation: 4.765297958534427]
	TIME [epoch: 7.87 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.84014939823145		[learning rate: 0.00026789]
	Learning Rate: 0.000267885
	LOSS [training: 4.84014939823145 | validation: 4.7670116349887355]
	TIME [epoch: 7.86 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8282574188661584		[learning rate: 0.00026725]
	Learning Rate: 0.000267253
	LOSS [training: 4.8282574188661584 | validation: 4.756924021040966]
	TIME [epoch: 7.85 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822750610384939		[learning rate: 0.00026662]
	Learning Rate: 0.000266623
	LOSS [training: 4.822750610384939 | validation: 4.756928974045772]
	TIME [epoch: 7.86 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.829361182357729		[learning rate: 0.00026599]
	Learning Rate: 0.000265994
	LOSS [training: 4.829361182357729 | validation: 4.745296823753333]
	TIME [epoch: 7.9 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826070729191775		[learning rate: 0.00026537]
	Learning Rate: 0.000265367
	LOSS [training: 4.826070729191775 | validation: 4.737770293887072]
	TIME [epoch: 7.88 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8085296513527025		[learning rate: 0.00026474]
	Learning Rate: 0.000264741
	LOSS [training: 4.8085296513527025 | validation: 4.743914270250123]
	TIME [epoch: 7.86 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822790143520634		[learning rate: 0.00026412]
	Learning Rate: 0.000264116
	LOSS [training: 4.822790143520634 | validation: 4.76041378719224]
	TIME [epoch: 7.86 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.830497015238999		[learning rate: 0.00026349]
	Learning Rate: 0.000263493
	LOSS [training: 4.830497015238999 | validation: 4.754779644788872]
	TIME [epoch: 7.86 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.81840281976975		[learning rate: 0.00026287]
	Learning Rate: 0.000262872
	LOSS [training: 4.81840281976975 | validation: 4.748147069459192]
	TIME [epoch: 7.91 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814631314713814		[learning rate: 0.00026225]
	Learning Rate: 0.000262252
	LOSS [training: 4.814631314713814 | validation: 4.7341045801875]
	TIME [epoch: 7.87 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7953246338643165		[learning rate: 0.00026163]
	Learning Rate: 0.000261633
	LOSS [training: 4.7953246338643165 | validation: 4.729825891417729]
	TIME [epoch: 7.86 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.787627000202148		[learning rate: 0.00026102]
	Learning Rate: 0.000261016
	LOSS [training: 4.787627000202148 | validation: 4.710020163513204]
	TIME [epoch: 7.86 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.772085575455137		[learning rate: 0.0002604]
	Learning Rate: 0.0002604
	LOSS [training: 4.772085575455137 | validation: 4.6889429537035205]
	TIME [epoch: 7.86 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.754429660635014		[learning rate: 0.00025979]
	Learning Rate: 0.000259786
	LOSS [training: 4.754429660635014 | validation: 4.706554494622492]
	TIME [epoch: 7.9 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.771942300768461		[learning rate: 0.00025917]
	Learning Rate: 0.000259173
	LOSS [training: 4.771942300768461 | validation: 4.688686598290007]
	TIME [epoch: 7.87 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.754073956054574		[learning rate: 0.00025856]
	Learning Rate: 0.000258562
	LOSS [training: 4.754073956054574 | validation: 4.680905496218376]
	TIME [epoch: 7.86 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.743782649968788		[learning rate: 0.00025795]
	Learning Rate: 0.000257952
	LOSS [training: 4.743782649968788 | validation: 4.689870003110972]
	TIME [epoch: 7.86 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7453086020633926		[learning rate: 0.00025734]
	Learning Rate: 0.000257343
	LOSS [training: 4.7453086020633926 | validation: 4.688091676104694]
	TIME [epoch: 7.86 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.753189747872105		[learning rate: 0.00025674]
	Learning Rate: 0.000256736
	LOSS [training: 4.753189747872105 | validation: 4.7103980321323355]
	TIME [epoch: 7.9 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.769699448745436		[learning rate: 0.00025613]
	Learning Rate: 0.000256131
	LOSS [training: 4.769699448745436 | validation: 4.7135825880764415]
	TIME [epoch: 7.87 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7641405920218265		[learning rate: 0.00025553]
	Learning Rate: 0.000255527
	LOSS [training: 4.7641405920218265 | validation: 4.699700551845542]
	TIME [epoch: 7.87 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.750060828787278		[learning rate: 0.00025492]
	Learning Rate: 0.000254924
	LOSS [training: 4.750060828787278 | validation: 4.708491900770064]
	TIME [epoch: 7.86 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.746158767484257		[learning rate: 0.00025432]
	Learning Rate: 0.000254322
	LOSS [training: 4.746158767484257 | validation: 4.686158196057098]
	TIME [epoch: 7.87 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.743977344741488		[learning rate: 0.00025372]
	Learning Rate: 0.000253723
	LOSS [training: 4.743977344741488 | validation: 4.7184966014280825]
	TIME [epoch: 7.91 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.761375972311097		[learning rate: 0.00025312]
	Learning Rate: 0.000253124
	LOSS [training: 4.761375972311097 | validation: 4.7298793267564285]
	TIME [epoch: 7.88 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.780539078412265		[learning rate: 0.00025253]
	Learning Rate: 0.000252527
	LOSS [training: 4.780539078412265 | validation: 4.744549490795483]
	TIME [epoch: 7.87 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.789053229605234		[learning rate: 0.00025193]
	Learning Rate: 0.000251931
	LOSS [training: 4.789053229605234 | validation: 4.749291698664563]
	TIME [epoch: 7.86 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803691192668553		[learning rate: 0.00025134]
	Learning Rate: 0.000251337
	LOSS [training: 4.803691192668553 | validation: 4.792512218766134]
	TIME [epoch: 7.86 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.825500184013968		[learning rate: 0.00025074]
	Learning Rate: 0.000250744
	LOSS [training: 4.825500184013968 | validation: 4.785278663394313]
	TIME [epoch: 7.91 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.817470347536238		[learning rate: 0.00025015]
	Learning Rate: 0.000250153
	LOSS [training: 4.817470347536238 | validation: 4.792881287023618]
	TIME [epoch: 7.87 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.81738241796045		[learning rate: 0.00024956]
	Learning Rate: 0.000249563
	LOSS [training: 4.81738241796045 | validation: 4.780343463083861]
	TIME [epoch: 7.86 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.802280614718347		[learning rate: 0.00024897]
	Learning Rate: 0.000248974
	LOSS [training: 4.802280614718347 | validation: 4.7574158998086755]
	TIME [epoch: 7.86 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.795368541694751		[learning rate: 0.00024839]
	Learning Rate: 0.000248387
	LOSS [training: 4.795368541694751 | validation: 4.776446599805232]
	TIME [epoch: 7.87 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803951111485108		[learning rate: 0.0002478]
	Learning Rate: 0.000247801
	LOSS [training: 4.803951111485108 | validation: 4.774799135337835]
	TIME [epoch: 7.91 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.79134207821736		[learning rate: 0.00024722]
	Learning Rate: 0.000247216
	LOSS [training: 4.79134207821736 | validation: 4.759226153776296]
	TIME [epoch: 7.87 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.785458250152063		[learning rate: 0.00024663]
	Learning Rate: 0.000246633
	LOSS [training: 4.785458250152063 | validation: 4.750803989218446]
	TIME [epoch: 7.87 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.789567748412545		[learning rate: 0.00024605]
	Learning Rate: 0.000246051
	LOSS [training: 4.789567748412545 | validation: 4.766715738420228]
	TIME [epoch: 7.87 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.784610648053135		[learning rate: 0.00024547]
	Learning Rate: 0.000245471
	LOSS [training: 4.784610648053135 | validation: 4.741477232093571]
	TIME [epoch: 7.86 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7712927777015		[learning rate: 0.00024489]
	Learning Rate: 0.000244892
	LOSS [training: 4.7712927777015 | validation: 4.74204823695379]
	TIME [epoch: 7.91 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.774994171693415		[learning rate: 0.00024431]
	Learning Rate: 0.000244314
	LOSS [training: 4.774994171693415 | validation: 4.767201851096951]
	TIME [epoch: 7.86 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794640676760231		[learning rate: 0.00024374]
	Learning Rate: 0.000243738
	LOSS [training: 4.794640676760231 | validation: 4.757162348328995]
	TIME [epoch: 7.86 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.794039263363826		[learning rate: 0.00024316]
	Learning Rate: 0.000243163
	LOSS [training: 4.794039263363826 | validation: 4.78574752872481]
	TIME [epoch: 7.86 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.800851668848318		[learning rate: 0.00024259]
	Learning Rate: 0.000242589
	LOSS [training: 4.800851668848318 | validation: 4.779743590469366]
	TIME [epoch: 7.87 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.797039146314626		[learning rate: 0.00024202]
	Learning Rate: 0.000242017
	LOSS [training: 4.797039146314626 | validation: 4.776343867757321]
	TIME [epoch: 7.92 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803700070042486		[learning rate: 0.00024145]
	Learning Rate: 0.000241446
	LOSS [training: 4.803700070042486 | validation: 4.760739840424657]
	TIME [epoch: 7.87 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.808012967282114		[learning rate: 0.00024088]
	Learning Rate: 0.000240877
	LOSS [training: 4.808012967282114 | validation: 4.790267825225001]
	TIME [epoch: 7.86 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.813066837932177		[learning rate: 0.00024031]
	Learning Rate: 0.000240309
	LOSS [training: 4.813066837932177 | validation: 4.782386678187429]
	TIME [epoch: 7.87 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.827604087320671		[learning rate: 0.00023974]
	Learning Rate: 0.000239742
	LOSS [training: 4.827604087320671 | validation: 4.784501886183932]
	TIME [epoch: 7.87 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.819136147777618		[learning rate: 0.00023918]
	Learning Rate: 0.000239176
	LOSS [training: 4.819136147777618 | validation: 4.777362130829027]
	TIME [epoch: 7.92 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.816321818614817		[learning rate: 0.00023861]
	Learning Rate: 0.000238612
	LOSS [training: 4.816321818614817 | validation: 4.771793273764096]
	TIME [epoch: 7.87 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.802945964669078		[learning rate: 0.00023805]
	Learning Rate: 0.000238049
	LOSS [training: 4.802945964669078 | validation: 4.7751826000242446]
	TIME [epoch: 7.86 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814967240739102		[learning rate: 0.00023749]
	Learning Rate: 0.000237488
	LOSS [training: 4.814967240739102 | validation: 4.795895148121868]
	TIME [epoch: 7.86 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822036735537591		[learning rate: 0.00023693]
	Learning Rate: 0.000236927
	LOSS [training: 4.822036735537591 | validation: 4.782042164944533]
	TIME [epoch: 7.87 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8041410614192195		[learning rate: 0.00023637]
	Learning Rate: 0.000236369
	LOSS [training: 4.8041410614192195 | validation: 4.756592210570693]
	TIME [epoch: 7.91 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.788531774082779		[learning rate: 0.00023581]
	Learning Rate: 0.000235811
	LOSS [training: 4.788531774082779 | validation: 4.7502063808393356]
	TIME [epoch: 7.87 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.791382176509369		[learning rate: 0.00023525]
	Learning Rate: 0.000235255
	LOSS [training: 4.791382176509369 | validation: 4.765161154929183]
	TIME [epoch: 7.86 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.799551622057217		[learning rate: 0.0002347]
	Learning Rate: 0.0002347
	LOSS [training: 4.799551622057217 | validation: 4.760229173601829]
	TIME [epoch: 7.86 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.803845958239295		[learning rate: 0.00023415]
	Learning Rate: 0.000234146
	LOSS [training: 4.803845958239295 | validation: 4.7880187288576534]
	TIME [epoch: 7.86 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.814140070153627		[learning rate: 0.00023359]
	Learning Rate: 0.000233594
	LOSS [training: 4.814140070153627 | validation: 4.793064782758664]
	TIME [epoch: 7.92 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815133615451326		[learning rate: 0.00023304]
	Learning Rate: 0.000233043
	LOSS [training: 4.815133615451326 | validation: 4.805542596142844]
	TIME [epoch: 7.87 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8106217031129965		[learning rate: 0.00023249]
	Learning Rate: 0.000232493
	LOSS [training: 4.8106217031129965 | validation: 4.773828659305037]
	TIME [epoch: 7.86 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.79398090703816		[learning rate: 0.00023194]
	Learning Rate: 0.000231945
	LOSS [training: 4.79398090703816 | validation: 4.768523131172101]
	TIME [epoch: 7.87 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.798371789632205		[learning rate: 0.0002314]
	Learning Rate: 0.000231398
	LOSS [training: 4.798371789632205 | validation: 4.761465288935774]
	TIME [epoch: 7.87 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.798217996239523		[learning rate: 0.00023085]
	Learning Rate: 0.000230852
	LOSS [training: 4.798217996239523 | validation: 4.772915831803207]
	TIME [epoch: 7.92 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.808491609866811		[learning rate: 0.00023031]
	Learning Rate: 0.000230307
	LOSS [training: 4.808491609866811 | validation: 4.78916461123842]
	TIME [epoch: 7.87 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.832603289528192		[learning rate: 0.00022976]
	Learning Rate: 0.000229764
	LOSS [training: 4.832603289528192 | validation: 4.827248707187511]
	TIME [epoch: 7.87 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.857056355428761		[learning rate: 0.00022922]
	Learning Rate: 0.000229222
	LOSS [training: 4.857056355428761 | validation: 4.8261896542542715]
	TIME [epoch: 7.87 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.863800119752471		[learning rate: 0.00022868]
	Learning Rate: 0.000228681
	LOSS [training: 4.863800119752471 | validation: 4.832502134086357]
	TIME [epoch: 7.88 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.851170468169508		[learning rate: 0.00022814]
	Learning Rate: 0.000228142
	LOSS [training: 4.851170468169508 | validation: 4.8279273949864425]
	TIME [epoch: 7.92 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.859078686192852		[learning rate: 0.0002276]
	Learning Rate: 0.000227604
	LOSS [training: 4.859078686192852 | validation: 4.842098338403398]
	TIME [epoch: 7.87 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871595473497437		[learning rate: 0.00022707]
	Learning Rate: 0.000227067
	LOSS [training: 4.871595473497437 | validation: 4.840514021394872]
	TIME [epoch: 7.87 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.861677716769061		[learning rate: 0.00022653]
	Learning Rate: 0.000226531
	LOSS [training: 4.861677716769061 | validation: 4.832844187573855]
	TIME [epoch: 7.86 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.858756430919673		[learning rate: 0.000226]
	Learning Rate: 0.000225997
	LOSS [training: 4.858756430919673 | validation: 4.845520136293709]
	TIME [epoch: 7.87 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.877388920709701		[learning rate: 0.00022546]
	Learning Rate: 0.000225464
	LOSS [training: 4.877388920709701 | validation: 4.8652806193953575]
	TIME [epoch: 7.92 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.868414343289283		[learning rate: 0.00022493]
	Learning Rate: 0.000224932
	LOSS [training: 4.868414343289283 | validation: 4.849342698383612]
	TIME [epoch: 7.87 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.858343406334059		[learning rate: 0.0002244]
	Learning Rate: 0.000224401
	LOSS [training: 4.858343406334059 | validation: 4.844667055634677]
	TIME [epoch: 7.86 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86080434440122		[learning rate: 0.00022387]
	Learning Rate: 0.000223872
	LOSS [training: 4.86080434440122 | validation: 4.86489627222573]
	TIME [epoch: 7.85 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871875533321369		[learning rate: 0.00022334]
	Learning Rate: 0.000223344
	LOSS [training: 4.871875533321369 | validation: 4.8377795519312485]
	TIME [epoch: 7.87 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.864258391843789		[learning rate: 0.00022282]
	Learning Rate: 0.000222817
	LOSS [training: 4.864258391843789 | validation: 4.846687738581769]
	TIME [epoch: 7.91 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.87777136584856		[learning rate: 0.00022229]
	Learning Rate: 0.000222292
	LOSS [training: 4.87777136584856 | validation: 4.872558340263486]
	TIME [epoch: 7.86 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.897006092579891		[learning rate: 0.00022177]
	Learning Rate: 0.000221767
	LOSS [training: 4.897006092579891 | validation: 4.873855045121286]
	TIME [epoch: 7.86 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8947453441398086		[learning rate: 0.00022124]
	Learning Rate: 0.000221244
	LOSS [training: 4.8947453441398086 | validation: 4.879005113124874]
	TIME [epoch: 7.86 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.891390749812239		[learning rate: 0.00022072]
	Learning Rate: 0.000220722
	LOSS [training: 4.891390749812239 | validation: 4.863370783022297]
	TIME [epoch: 7.86 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.886612650763891		[learning rate: 0.0002202]
	Learning Rate: 0.000220202
	LOSS [training: 4.886612650763891 | validation: 4.872795085450002]
	TIME [epoch: 7.92 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.885562058092592		[learning rate: 0.00021968]
	Learning Rate: 0.000219682
	LOSS [training: 4.885562058092592 | validation: 4.859207570912867]
	TIME [epoch: 7.86 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882436237457458		[learning rate: 0.00021916]
	Learning Rate: 0.000219164
	LOSS [training: 4.882436237457458 | validation: 4.839389214078047]
	TIME [epoch: 7.86 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.879377236425886		[learning rate: 0.00021865]
	Learning Rate: 0.000218647
	LOSS [training: 4.879377236425886 | validation: 4.85853164390845]
	TIME [epoch: 7.87 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8834988035667175		[learning rate: 0.00021813]
	Learning Rate: 0.000218131
	LOSS [training: 4.8834988035667175 | validation: 4.875550698400449]
	TIME [epoch: 7.86 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9057265863157085		[learning rate: 0.00021762]
	Learning Rate: 0.000217617
	LOSS [training: 4.9057265863157085 | validation: 4.896744210864659]
	TIME [epoch: 7.9 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.926063846598287		[learning rate: 0.0002171]
	Learning Rate: 0.000217103
	LOSS [training: 4.926063846598287 | validation: 4.902684200112244]
	TIME [epoch: 7.86 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.921163550131173		[learning rate: 0.00021659]
	Learning Rate: 0.000216591
	LOSS [training: 4.921163550131173 | validation: 4.895102980026598]
	TIME [epoch: 7.87 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922120733359076		[learning rate: 0.00021608]
	Learning Rate: 0.00021608
	LOSS [training: 4.922120733359076 | validation: 4.891068057876124]
	TIME [epoch: 7.86 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.915642320922233		[learning rate: 0.00021557]
	Learning Rate: 0.000215571
	LOSS [training: 4.915642320922233 | validation: 4.876994970421821]
	TIME [epoch: 7.87 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9204602757265645		[learning rate: 0.00021506]
	Learning Rate: 0.000215062
	LOSS [training: 4.9204602757265645 | validation: 4.888759870771745]
	TIME [epoch: 7.91 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.929083325484094		[learning rate: 0.00021455]
	Learning Rate: 0.000214555
	LOSS [training: 4.929083325484094 | validation: 4.906476150816873]
	TIME [epoch: 7.87 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.926983113714681		[learning rate: 0.00021405]
	Learning Rate: 0.000214049
	LOSS [training: 4.926983113714681 | validation: 4.910253698595579]
	TIME [epoch: 7.87 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.932043080924518		[learning rate: 0.00021354]
	Learning Rate: 0.000213544
	LOSS [training: 4.932043080924518 | validation: 4.929547388024058]
	TIME [epoch: 7.87 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.934013906055622		[learning rate: 0.00021304]
	Learning Rate: 0.00021304
	LOSS [training: 4.934013906055622 | validation: 4.926531931485916]
	TIME [epoch: 7.88 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.942621184701579		[learning rate: 0.00021254]
	Learning Rate: 0.000212538
	LOSS [training: 4.942621184701579 | validation: 4.925380620833991]
	TIME [epoch: 7.92 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.954892622237923		[learning rate: 0.00021204]
	Learning Rate: 0.000212036
	LOSS [training: 4.954892622237923 | validation: 4.9375497082905815]
	TIME [epoch: 7.87 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9510540272730985		[learning rate: 0.00021154]
	Learning Rate: 0.000211536
	LOSS [training: 4.9510540272730985 | validation: 4.924612499390487]
	TIME [epoch: 7.87 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.945752464648646		[learning rate: 0.00021104]
	Learning Rate: 0.000211037
	LOSS [training: 4.945752464648646 | validation: 4.935595538247281]
	TIME [epoch: 7.86 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.953440847996283		[learning rate: 0.00021054]
	Learning Rate: 0.000210539
	LOSS [training: 4.953440847996283 | validation: 4.937623608226607]
	TIME [epoch: 7.87 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9564295978672295		[learning rate: 0.00021004]
	Learning Rate: 0.000210043
	LOSS [training: 4.9564295978672295 | validation: 4.942576780282935]
	TIME [epoch: 7.92 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.959759042852891		[learning rate: 0.00020955]
	Learning Rate: 0.000209547
	LOSS [training: 4.959759042852891 | validation: 4.956030984510886]
	TIME [epoch: 7.87 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.967584206255801		[learning rate: 0.00020905]
	Learning Rate: 0.000209053
	LOSS [training: 4.967584206255801 | validation: 4.94531307095699]
	TIME [epoch: 7.87 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.951417433528813		[learning rate: 0.00020856]
	Learning Rate: 0.00020856
	LOSS [training: 4.951417433528813 | validation: 4.930492852883477]
	TIME [epoch: 7.86 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9476968739849525		[learning rate: 0.00020807]
	Learning Rate: 0.000208068
	LOSS [training: 4.9476968739849525 | validation: 4.936895220277316]
	TIME [epoch: 7.86 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.955810761403581		[learning rate: 0.00020758]
	Learning Rate: 0.000207577
	LOSS [training: 4.955810761403581 | validation: 4.943641808731631]
	TIME [epoch: 7.91 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.955588651739141		[learning rate: 0.00020709]
	Learning Rate: 0.000207087
	LOSS [training: 4.955588651739141 | validation: 4.919276893400504]
	TIME [epoch: 7.87 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939469700383645		[learning rate: 0.0002066]
	Learning Rate: 0.000206599
	LOSS [training: 4.939469700383645 | validation: 4.917246134432874]
	TIME [epoch: 7.86 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.936206748514146		[learning rate: 0.00020611]
	Learning Rate: 0.000206112
	LOSS [training: 4.936206748514146 | validation: 4.904039925644685]
	TIME [epoch: 7.87 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939929800198438		[learning rate: 0.00020563]
	Learning Rate: 0.000205625
	LOSS [training: 4.939929800198438 | validation: 4.92017144081723]
	TIME [epoch: 7.88 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.936545416077852		[learning rate: 0.00020514]
	Learning Rate: 0.00020514
	LOSS [training: 4.936545416077852 | validation: 4.900252079559238]
	TIME [epoch: 7.91 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.930991201275984		[learning rate: 0.00020466]
	Learning Rate: 0.000204657
	LOSS [training: 4.930991201275984 | validation: 4.887883562779945]
	TIME [epoch: 7.86 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939468516238639		[learning rate: 0.00020417]
	Learning Rate: 0.000204174
	LOSS [training: 4.939468516238639 | validation: 4.903876509989891]
	TIME [epoch: 7.87 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.948558717776823		[learning rate: 0.00020369]
	Learning Rate: 0.000203692
	LOSS [training: 4.948558717776823 | validation: 4.926965329826839]
	TIME [epoch: 7.87 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.961001683008982		[learning rate: 0.00020321]
	Learning Rate: 0.000203212
	LOSS [training: 4.961001683008982 | validation: 4.943211359546254]
	TIME [epoch: 7.87 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.967840025753282		[learning rate: 0.00020273]
	Learning Rate: 0.000202732
	LOSS [training: 4.967840025753282 | validation: 4.940735143527778]
	TIME [epoch: 7.92 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.969195525604603		[learning rate: 0.00020225]
	Learning Rate: 0.000202254
	LOSS [training: 4.969195525604603 | validation: 4.943128590977798]
	TIME [epoch: 7.86 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.980682057259071		[learning rate: 0.00020178]
	Learning Rate: 0.000201777
	LOSS [training: 4.980682057259071 | validation: 4.957118113697321]
	TIME [epoch: 7.86 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.98212026210142		[learning rate: 0.0002013]
	Learning Rate: 0.000201301
	LOSS [training: 4.98212026210142 | validation: 4.97121079119893]
	TIME [epoch: 7.87 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.991401669016135		[learning rate: 0.00020083]
	Learning Rate: 0.000200826
	LOSS [training: 4.991401669016135 | validation: 4.977916654143131]
	TIME [epoch: 7.88 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9943962823982755		[learning rate: 0.00020035]
	Learning Rate: 0.000200353
	LOSS [training: 4.9943962823982755 | validation: 4.985053710943562]
	TIME [epoch: 7.91 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.998230660718505		[learning rate: 0.00019988]
	Learning Rate: 0.00019988
	LOSS [training: 4.998230660718505 | validation: 4.988715872580217]
	TIME [epoch: 7.87 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.012554825843213		[learning rate: 0.00019941]
	Learning Rate: 0.000199408
	LOSS [training: 5.012554825843213 | validation: 5.009356910723819]
	TIME [epoch: 7.87 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.022708216874654		[learning rate: 0.00019894]
	Learning Rate: 0.000198938
	LOSS [training: 5.022708216874654 | validation: 5.008165002903192]
	TIME [epoch: 7.86 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028195611915491		[learning rate: 0.00019847]
	Learning Rate: 0.000198469
	LOSS [training: 5.028195611915491 | validation: 5.001546434163311]
	TIME [epoch: 7.88 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0259019432203225		[learning rate: 0.000198]
	Learning Rate: 0.000198001
	LOSS [training: 5.0259019432203225 | validation: 5.002046224690846]
	TIME [epoch: 7.91 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.021192036734161		[learning rate: 0.00019753]
	Learning Rate: 0.000197534
	LOSS [training: 5.021192036734161 | validation: 4.990194816442505]
	TIME [epoch: 7.86 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.015489811040519		[learning rate: 0.00019707]
	Learning Rate: 0.000197068
	LOSS [training: 5.015489811040519 | validation: 4.959094673905283]
	TIME [epoch: 7.86 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.006533240116041		[learning rate: 0.0001966]
	Learning Rate: 0.000196603
	LOSS [training: 5.006533240116041 | validation: 4.96178096950165]
	TIME [epoch: 7.87 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007820980564241		[learning rate: 0.00019614]
	Learning Rate: 0.000196139
	LOSS [training: 5.007820980564241 | validation: 4.969276079451541]
	TIME [epoch: 7.87 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.017085688369321		[learning rate: 0.00019568]
	Learning Rate: 0.000195676
	LOSS [training: 5.017085688369321 | validation: 4.971083720017525]
	TIME [epoch: 7.91 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011210500312776		[learning rate: 0.00019521]
	Learning Rate: 0.000195215
	LOSS [training: 5.011210500312776 | validation: 4.945283158360493]
	TIME [epoch: 7.87 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.008471062987059		[learning rate: 0.00019475]
	Learning Rate: 0.000194754
	LOSS [training: 5.008471062987059 | validation: 4.954237990690519]
	TIME [epoch: 7.86 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01526053418155		[learning rate: 0.00019429]
	Learning Rate: 0.000194295
	LOSS [training: 5.01526053418155 | validation: 4.958347738643873]
	TIME [epoch: 7.86 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.00992623788824		[learning rate: 0.00019384]
	Learning Rate: 0.000193837
	LOSS [training: 5.00992623788824 | validation: 4.946733167311966]
	TIME [epoch: 7.88 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.017017461695767		[learning rate: 0.00019338]
	Learning Rate: 0.000193379
	LOSS [training: 5.017017461695767 | validation: 4.943989065260627]
	TIME [epoch: 7.91 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.015900032568078		[learning rate: 0.00019292]
	Learning Rate: 0.000192923
	LOSS [training: 5.015900032568078 | validation: 4.93440362939689]
	TIME [epoch: 7.86 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011562082201434		[learning rate: 0.00019247]
	Learning Rate: 0.000192468
	LOSS [training: 5.011562082201434 | validation: 4.943291752798497]
	TIME [epoch: 7.87 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0107464423887045		[learning rate: 0.00019201]
	Learning Rate: 0.000192014
	LOSS [training: 5.0107464423887045 | validation: 4.92751810978681]
	TIME [epoch: 7.86 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0032570405033505		[learning rate: 0.00019156]
	Learning Rate: 0.000191561
	LOSS [training: 5.0032570405033505 | validation: 4.919150547597258]
	TIME [epoch: 7.87 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0066208356741635		[learning rate: 0.00019111]
	Learning Rate: 0.000191109
	LOSS [training: 5.0066208356741635 | validation: 4.937308794129022]
	TIME [epoch: 7.91 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01049397664539		[learning rate: 0.00019066]
	Learning Rate: 0.000190659
	LOSS [training: 5.01049397664539 | validation: 4.944232359529173]
	TIME [epoch: 7.87 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007068156549612		[learning rate: 0.00019021]
	Learning Rate: 0.000190209
	LOSS [training: 5.007068156549612 | validation: 4.921341214711603]
	TIME [epoch: 7.87 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004401711950362		[learning rate: 0.00018976]
	Learning Rate: 0.00018976
	LOSS [training: 5.004401711950362 | validation: 4.923421019248286]
	TIME [epoch: 7.87 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.999493293650318		[learning rate: 0.00018931]
	Learning Rate: 0.000189313
	LOSS [training: 4.999493293650318 | validation: 4.918713829256277]
	TIME [epoch: 7.89 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.997311640816241		[learning rate: 0.00018887]
	Learning Rate: 0.000188866
	LOSS [training: 4.997311640816241 | validation: 4.906808135587108]
	TIME [epoch: 7.9 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.993022102043415		[learning rate: 0.00018842]
	Learning Rate: 0.00018842
	LOSS [training: 4.993022102043415 | validation: 4.915942953169086]
	TIME [epoch: 7.86 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.99403958465316		[learning rate: 0.00018798]
	Learning Rate: 0.000187976
	LOSS [training: 4.99403958465316 | validation: 4.906618901462515]
	TIME [epoch: 7.87 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.996462580672931		[learning rate: 0.00018753]
	Learning Rate: 0.000187533
	LOSS [training: 4.996462580672931 | validation: 4.916918939523267]
	TIME [epoch: 7.85 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005402575155768		[learning rate: 0.00018709]
	Learning Rate: 0.00018709
	LOSS [training: 5.005402575155768 | validation: 4.940906861667105]
	TIME [epoch: 7.88 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.018465671351306		[learning rate: 0.00018665]
	Learning Rate: 0.000186649
	LOSS [training: 5.018465671351306 | validation: 4.970219019090376]
	TIME [epoch: 7.91 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.035471887149092		[learning rate: 0.00018621]
	Learning Rate: 0.000186209
	LOSS [training: 5.035471887149092 | validation: 4.988955179798843]
	TIME [epoch: 7.87 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.043029138424897		[learning rate: 0.00018577]
	Learning Rate: 0.000185769
	LOSS [training: 5.043029138424897 | validation: 4.976664779221785]
	TIME [epoch: 7.86 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.039352545208637		[learning rate: 0.00018533]
	Learning Rate: 0.000185331
	LOSS [training: 5.039352545208637 | validation: 4.958545038533037]
	TIME [epoch: 7.87 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.03546929711962		[learning rate: 0.00018489]
	Learning Rate: 0.000184894
	LOSS [training: 5.03546929711962 | validation: 4.957888245112029]
	TIME [epoch: 7.88 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.035523827944791		[learning rate: 0.00018446]
	Learning Rate: 0.000184458
	LOSS [training: 5.035523827944791 | validation: 4.950810745001541]
	TIME [epoch: 7.91 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.03027323452712		[learning rate: 0.00018402]
	Learning Rate: 0.000184023
	LOSS [training: 5.03027323452712 | validation: 4.954008892330686]
	TIME [epoch: 7.86 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.033453285156092		[learning rate: 0.00018359]
	Learning Rate: 0.000183589
	LOSS [training: 5.033453285156092 | validation: 4.96431697965918]
	TIME [epoch: 7.86 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.033472450761757		[learning rate: 0.00018316]
	Learning Rate: 0.000183156
	LOSS [training: 5.033472450761757 | validation: 4.945875429619015]
	TIME [epoch: 7.86 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.032157548414136		[learning rate: 0.00018272]
	Learning Rate: 0.000182724
	LOSS [training: 5.032157548414136 | validation: 4.9413767840796545]
	TIME [epoch: 7.87 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0275185459652665		[learning rate: 0.00018229]
	Learning Rate: 0.000182293
	LOSS [training: 5.0275185459652665 | validation: 4.948565307155619]
	TIME [epoch: 7.92 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0369397392965825		[learning rate: 0.00018186]
	Learning Rate: 0.000181863
	LOSS [training: 5.0369397392965825 | validation: 4.962131835759129]
	TIME [epoch: 7.86 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.049592886737037		[learning rate: 0.00018143]
	Learning Rate: 0.000181434
	LOSS [training: 5.049592886737037 | validation: 4.992660626586878]
	TIME [epoch: 7.86 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063393254001042		[learning rate: 0.00018101]
	Learning Rate: 0.000181006
	LOSS [training: 5.063393254001042 | validation: 4.996705508382803]
	TIME [epoch: 7.86 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.062058749552347		[learning rate: 0.00018058]
	Learning Rate: 0.000180579
	LOSS [training: 5.062058749552347 | validation: 4.992883732516573]
	TIME [epoch: 7.88 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.058573157983527		[learning rate: 0.00018015]
	Learning Rate: 0.000180153
	LOSS [training: 5.058573157983527 | validation: 4.988992148833506]
	TIME [epoch: 7.91 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.062458189085517		[learning rate: 0.00017973]
	Learning Rate: 0.000179728
	LOSS [training: 5.062458189085517 | validation: 4.9863485995040335]
	TIME [epoch: 7.86 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.049798367784994		[learning rate: 0.0001793]
	Learning Rate: 0.000179304
	LOSS [training: 5.049798367784994 | validation: 4.973157383718125]
	TIME [epoch: 7.86 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.043375788632579		[learning rate: 0.00017888]
	Learning Rate: 0.000178881
	LOSS [training: 5.043375788632579 | validation: 4.991449286041828]
	TIME [epoch: 7.86 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.048449012529039		[learning rate: 0.00017846]
	Learning Rate: 0.000178459
	LOSS [training: 5.048449012529039 | validation: 4.985067971994504]
	TIME [epoch: 7.87 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.05280142259749		[learning rate: 0.00017804]
	Learning Rate: 0.000178038
	LOSS [training: 5.05280142259749 | validation: 4.989950798624324]
	TIME [epoch: 7.91 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.047178345290687		[learning rate: 0.00017762]
	Learning Rate: 0.000177618
	LOSS [training: 5.047178345290687 | validation: 4.988376003651393]
	TIME [epoch: 7.86 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.05262330779201		[learning rate: 0.0001772]
	Learning Rate: 0.000177199
	LOSS [training: 5.05262330779201 | validation: 4.998635672209277]
	TIME [epoch: 7.86 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.051387829909092		[learning rate: 0.00017678]
	Learning Rate: 0.000176781
	LOSS [training: 5.051387829909092 | validation: 4.9997278027101295]
	TIME [epoch: 7.86 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0532194747472525		[learning rate: 0.00017636]
	Learning Rate: 0.000176364
	LOSS [training: 5.0532194747472525 | validation: 5.014135667002492]
	TIME [epoch: 7.88 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.056322154359306		[learning rate: 0.00017595]
	Learning Rate: 0.000175948
	LOSS [training: 5.056322154359306 | validation: 5.023257691244513]
	TIME [epoch: 7.91 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063497642830428		[learning rate: 0.00017553]
	Learning Rate: 0.000175533
	LOSS [training: 5.063497642830428 | validation: 5.019521753496601]
	TIME [epoch: 7.87 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.06487600183008		[learning rate: 0.00017512]
	Learning Rate: 0.000175119
	LOSS [training: 5.06487600183008 | validation: 5.025675609296716]
	TIME [epoch: 7.87 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.065326681999523		[learning rate: 0.00017471]
	Learning Rate: 0.000174706
	LOSS [training: 5.065326681999523 | validation: 5.032031024390516]
	TIME [epoch: 7.87 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.066638360722059		[learning rate: 0.00017429]
	Learning Rate: 0.000174294
	LOSS [training: 5.066638360722059 | validation: 5.01797720365267]
	TIME [epoch: 7.88 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.057894286602236		[learning rate: 0.00017388]
	Learning Rate: 0.000173883
	LOSS [training: 5.057894286602236 | validation: 5.003691709920572]
	TIME [epoch: 7.91 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.051119599232223		[learning rate: 0.00017347]
	Learning Rate: 0.000173473
	LOSS [training: 5.051119599232223 | validation: 4.990398515367817]
	TIME [epoch: 7.87 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.04370195309555		[learning rate: 0.00017306]
	Learning Rate: 0.000173063
	LOSS [training: 5.04370195309555 | validation: 4.987425119675164]
	TIME [epoch: 7.86 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0334971613742		[learning rate: 0.00017266]
	Learning Rate: 0.000172655
	LOSS [training: 5.0334971613742 | validation: 4.969279908094371]
	TIME [epoch: 7.86 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.031897237604817		[learning rate: 0.00017225]
	Learning Rate: 0.000172248
	LOSS [training: 5.031897237604817 | validation: 4.965896536209486]
	TIME [epoch: 7.88 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.036753411154665		[learning rate: 0.00017184]
	Learning Rate: 0.000171842
	LOSS [training: 5.036753411154665 | validation: 4.981172548329621]
	TIME [epoch: 7.91 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.034649850600509		[learning rate: 0.00017144]
	Learning Rate: 0.000171436
	LOSS [training: 5.034649850600509 | validation: 4.960102469959354]
	TIME [epoch: 7.86 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025322881725695		[learning rate: 0.00017103]
	Learning Rate: 0.000171032
	LOSS [training: 5.025322881725695 | validation: 4.961820147699379]
	TIME [epoch: 7.86 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.033457187456898		[learning rate: 0.00017063]
	Learning Rate: 0.000170628
	LOSS [training: 5.033457187456898 | validation: 4.972023061366482]
	TIME [epoch: 7.86 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.034098405272498		[learning rate: 0.00017023]
	Learning Rate: 0.000170226
	LOSS [training: 5.034098405272498 | validation: 4.960633143456693]
	TIME [epoch: 7.87 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.036305092185874		[learning rate: 0.00016982]
	Learning Rate: 0.000169824
	LOSS [training: 5.036305092185874 | validation: 4.966968671348288]
	TIME [epoch: 7.91 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.03092780777669		[learning rate: 0.00016942]
	Learning Rate: 0.000169424
	LOSS [training: 5.03092780777669 | validation: 4.953580622039445]
	TIME [epoch: 7.86 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.026546562070704		[learning rate: 0.00016902]
	Learning Rate: 0.000169024
	LOSS [training: 5.026546562070704 | validation: 4.96985938128521]
	TIME [epoch: 7.85 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030397586745904		[learning rate: 0.00016863]
	Learning Rate: 0.000168625
	LOSS [training: 5.030397586745904 | validation: 4.965165346796876]
	TIME [epoch: 7.86 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.026767391946638		[learning rate: 0.00016823]
	Learning Rate: 0.000168228
	LOSS [training: 5.026767391946638 | validation: 4.963734801107979]
	TIME [epoch: 7.87 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025913965482991		[learning rate: 0.00016783]
	Learning Rate: 0.000167831
	LOSS [training: 5.025913965482991 | validation: 4.969933731297379]
	TIME [epoch: 7.9 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02664469827211		[learning rate: 0.00016743]
	Learning Rate: 0.000167435
	LOSS [training: 5.02664469827211 | validation: 4.962095150277156]
	TIME [epoch: 7.85 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.020483857448718		[learning rate: 0.00016704]
	Learning Rate: 0.00016704
	LOSS [training: 5.020483857448718 | validation: 4.952148488311815]
	TIME [epoch: 7.86 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.018252882388675		[learning rate: 0.00016665]
	Learning Rate: 0.000166646
	LOSS [training: 5.018252882388675 | validation: 4.946422611265797]
	TIME [epoch: 7.86 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007556777095331		[learning rate: 0.00016625]
	Learning Rate: 0.000166253
	LOSS [training: 5.007556777095331 | validation: 4.9421622073098295]
	TIME [epoch: 7.88 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.006897569950936		[learning rate: 0.00016586]
	Learning Rate: 0.000165861
	LOSS [training: 5.006897569950936 | validation: 4.9418932670163045]
	TIME [epoch: 7.91 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005536247954943		[learning rate: 0.00016547]
	Learning Rate: 0.00016547
	LOSS [training: 5.005536247954943 | validation: 4.9280783815025195]
	TIME [epoch: 7.86 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0005777311290505		[learning rate: 0.00016508]
	Learning Rate: 0.000165079
	LOSS [training: 5.0005777311290505 | validation: 4.941851215356047]
	TIME [epoch: 7.86 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.001476897296455		[learning rate: 0.00016469]
	Learning Rate: 0.00016469
	LOSS [training: 5.001476897296455 | validation: 4.932718475208671]
	TIME [epoch: 7.86 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.995817677445352		[learning rate: 0.0001643]
	Learning Rate: 0.000164301
	LOSS [training: 4.995817677445352 | validation: 4.915796954091877]
	TIME [epoch: 7.87 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.990720061347521		[learning rate: 0.00016391]
	Learning Rate: 0.000163914
	LOSS [training: 4.990720061347521 | validation: 4.919794464555125]
	TIME [epoch: 7.9 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.990988644171654		[learning rate: 0.00016353]
	Learning Rate: 0.000163527
	LOSS [training: 4.990988644171654 | validation: 4.9165083603225455]
	TIME [epoch: 7.87 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.992861078360643		[learning rate: 0.00016314]
	Learning Rate: 0.000163141
	LOSS [training: 4.992861078360643 | validation: 4.9277103023871724]
	TIME [epoch: 7.86 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.99733583815355		[learning rate: 0.00016276]
	Learning Rate: 0.000162757
	LOSS [training: 4.99733583815355 | validation: 4.939442003255629]
	TIME [epoch: 7.86 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.993910826574656		[learning rate: 0.00016237]
	Learning Rate: 0.000162373
	LOSS [training: 4.993910826574656 | validation: 4.9307468171517375]
	TIME [epoch: 7.88 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.986339000815388		[learning rate: 0.00016199]
	Learning Rate: 0.00016199
	LOSS [training: 4.986339000815388 | validation: 4.9390023956844225]
	TIME [epoch: 7.89 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.979201615077154		[learning rate: 0.00016161]
	Learning Rate: 0.000161608
	LOSS [training: 4.979201615077154 | validation: 4.93051306103647]
	TIME [epoch: 7.86 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.978736627725808		[learning rate: 0.00016123]
	Learning Rate: 0.000161226
	LOSS [training: 4.978736627725808 | validation: 4.938588583578884]
	TIME [epoch: 7.85 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.987638396552793		[learning rate: 0.00016085]
	Learning Rate: 0.000160846
	LOSS [training: 4.987638396552793 | validation: 4.94559032495602]
	TIME [epoch: 7.86 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.991185664585907		[learning rate: 0.00016047]
	Learning Rate: 0.000160467
	LOSS [training: 4.991185664585907 | validation: 4.938770717317398]
	TIME [epoch: 7.89 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.990492217580301		[learning rate: 0.00016009]
	Learning Rate: 0.000160088
	LOSS [training: 4.990492217580301 | validation: 4.9464340792372825]
	TIME [epoch: 7.89 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9915104175481995		[learning rate: 0.00015971]
	Learning Rate: 0.00015971
	LOSS [training: 4.9915104175481995 | validation: 4.95657626432533]
	TIME [epoch: 7.86 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.995933915549216		[learning rate: 0.00015933]
	Learning Rate: 0.000159334
	LOSS [training: 4.995933915549216 | validation: 4.976863972153625]
	TIME [epoch: 7.86 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005415615534754		[learning rate: 0.00015896]
	Learning Rate: 0.000158958
	LOSS [training: 5.005415615534754 | validation: 4.995400003275687]
	TIME [epoch: 7.86 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009073321503361		[learning rate: 0.00015858]
	Learning Rate: 0.000158583
	LOSS [training: 5.009073321503361 | validation: 5.001634210785896]
	TIME [epoch: 7.88 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.017857908958892		[learning rate: 0.00015821]
	Learning Rate: 0.000158209
	LOSS [training: 5.017857908958892 | validation: 4.991564069594857]
	TIME [epoch: 7.89 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0104981501412995		[learning rate: 0.00015784]
	Learning Rate: 0.000157836
	LOSS [training: 5.0104981501412995 | validation: 4.980674043487184]
	TIME [epoch: 7.86 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.006540131320204		[learning rate: 0.00015746]
	Learning Rate: 0.000157463
	LOSS [training: 5.006540131320204 | validation: 4.987992114036201]
	TIME [epoch: 7.85 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004580753499024		[learning rate: 0.00015709]
	Learning Rate: 0.000157092
	LOSS [training: 5.004580753499024 | validation: 4.99312175788385]
	TIME [epoch: 7.85 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.003356434555432		[learning rate: 0.00015672]
	Learning Rate: 0.000156721
	LOSS [training: 5.003356434555432 | validation: 4.98692380534894]
	TIME [epoch: 7.88 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004062074141931		[learning rate: 0.00015635]
	Learning Rate: 0.000156352
	LOSS [training: 5.004062074141931 | validation: 4.9852111749173025]
	TIME [epoch: 7.89 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004216619570425		[learning rate: 0.00015598]
	Learning Rate: 0.000155983
	LOSS [training: 5.004216619570425 | validation: 4.99454970775297]
	TIME [epoch: 7.86 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0090041876299285		[learning rate: 0.00015561]
	Learning Rate: 0.000155615
	LOSS [training: 5.0090041876299285 | validation: 5.011837239803253]
	TIME [epoch: 7.85 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.016382570994721		[learning rate: 0.00015525]
	Learning Rate: 0.000155248
	LOSS [training: 5.016382570994721 | validation: 5.006541831197058]
	TIME [epoch: 7.85 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0193133290451355		[learning rate: 0.00015488]
	Learning Rate: 0.000154882
	LOSS [training: 5.0193133290451355 | validation: 5.011772965359039]
	TIME [epoch: 7.89 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.018208086222302		[learning rate: 0.00015452]
	Learning Rate: 0.000154516
	LOSS [training: 5.018208086222302 | validation: 5.011370044155149]
	TIME [epoch: 7.89 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025490330899992		[learning rate: 0.00015415]
	Learning Rate: 0.000154152
	LOSS [training: 5.025490330899992 | validation: 5.0099335814494985]
	TIME [epoch: 7.86 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.033373268471792		[learning rate: 0.00015379]
	Learning Rate: 0.000153788
	LOSS [training: 5.033373268471792 | validation: 5.0159212070688675]
	TIME [epoch: 7.86 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.027908111197668		[learning rate: 0.00015343]
	Learning Rate: 0.000153425
	LOSS [training: 5.027908111197668 | validation: 5.0160117654992975]
	TIME [epoch: 7.85 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.039559957517163		[learning rate: 0.00015306]
	Learning Rate: 0.000153064
	LOSS [training: 5.039559957517163 | validation: 5.038583742383339]
	TIME [epoch: 7.89 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.046921159037858		[learning rate: 0.0001527]
	Learning Rate: 0.000152703
	LOSS [training: 5.046921159037858 | validation: 5.045505913307135]
	TIME [epoch: 7.89 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.053224967415746		[learning rate: 0.00015234]
	Learning Rate: 0.000152342
	LOSS [training: 5.053224967415746 | validation: 5.05023364791661]
	TIME [epoch: 7.86 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0498880903867835		[learning rate: 0.00015198]
	Learning Rate: 0.000151983
	LOSS [training: 5.0498880903867835 | validation: 5.059428300865958]
	TIME [epoch: 7.85 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.058732246310452		[learning rate: 0.00015162]
	Learning Rate: 0.000151624
	LOSS [training: 5.058732246310452 | validation: 5.07393019630905]
	TIME [epoch: 7.86 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.059425212585102		[learning rate: 0.00015127]
	Learning Rate: 0.000151267
	LOSS [training: 5.059425212585102 | validation: 5.055148949655483]
	TIME [epoch: 7.89 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.049880516207659		[learning rate: 0.00015091]
	Learning Rate: 0.00015091
	LOSS [training: 5.049880516207659 | validation: 5.0453363757110115]
	TIME [epoch: 7.89 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0462437712665915		[learning rate: 0.00015055]
	Learning Rate: 0.000150554
	LOSS [training: 5.0462437712665915 | validation: 5.046190846927381]
	TIME [epoch: 7.85 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.040143305452026		[learning rate: 0.0001502]
	Learning Rate: 0.000150199
	LOSS [training: 5.040143305452026 | validation: 5.048698702650589]
	TIME [epoch: 7.86 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.05051416624302		[learning rate: 0.00014984]
	Learning Rate: 0.000149845
	LOSS [training: 5.05051416624302 | validation: 5.056505438648925]
	TIME [epoch: 7.85 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.049986669952085		[learning rate: 0.00014949]
	Learning Rate: 0.000149491
	LOSS [training: 5.049986669952085 | validation: 5.056242096762412]
	TIME [epoch: 7.9 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.045192278296874		[learning rate: 0.00014914]
	Learning Rate: 0.000149139
	LOSS [training: 5.045192278296874 | validation: 5.053805147747987]
	TIME [epoch: 7.88 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.047234222163402		[learning rate: 0.00014879]
	Learning Rate: 0.000148787
	LOSS [training: 5.047234222163402 | validation: 5.051484289998198]
	TIME [epoch: 7.86 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.040977132071915		[learning rate: 0.00014844]
	Learning Rate: 0.000148436
	LOSS [training: 5.040977132071915 | validation: 5.035355987222095]
	TIME [epoch: 7.86 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028968319245628		[learning rate: 0.00014809]
	Learning Rate: 0.000148086
	LOSS [training: 5.028968319245628 | validation: 5.0446678385922485]
	TIME [epoch: 7.86 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0328834826535696		[learning rate: 0.00014774]
	Learning Rate: 0.000147736
	LOSS [training: 5.0328834826535696 | validation: 5.041635022184899]
	TIME [epoch: 7.9 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.028659596572533		[learning rate: 0.00014739]
	Learning Rate: 0.000147388
	LOSS [training: 5.028659596572533 | validation: 5.035727398334746]
	TIME [epoch: 7.88 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0294291658746575		[learning rate: 0.00014704]
	Learning Rate: 0.00014704
	LOSS [training: 5.0294291658746575 | validation: 5.045989992274848]
	TIME [epoch: 7.86 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024347287988612		[learning rate: 0.00014669]
	Learning Rate: 0.000146693
	LOSS [training: 5.024347287988612 | validation: 5.026930820597585]
	TIME [epoch: 7.85 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.022545613180299		[learning rate: 0.00014635]
	Learning Rate: 0.000146347
	LOSS [training: 5.022545613180299 | validation: 5.04382616890106]
	TIME [epoch: 7.86 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024664693412614		[learning rate: 0.000146]
	Learning Rate: 0.000146002
	LOSS [training: 5.024664693412614 | validation: 5.041268791778885]
	TIME [epoch: 7.9 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.026603571644435		[learning rate: 0.00014566]
	Learning Rate: 0.000145658
	LOSS [training: 5.026603571644435 | validation: 5.038550733140783]
	TIME [epoch: 7.88 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.023712624409555		[learning rate: 0.00014531]
	Learning Rate: 0.000145314
	LOSS [training: 5.023712624409555 | validation: 5.040602630070334]
	TIME [epoch: 7.85 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.027048443966644		[learning rate: 0.00014497]
	Learning Rate: 0.000144971
	LOSS [training: 5.027048443966644 | validation: 5.040886387869758]
	TIME [epoch: 7.85 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.032193799661764		[learning rate: 0.00014463]
	Learning Rate: 0.000144629
	LOSS [training: 5.032193799661764 | validation: 5.053462868077444]
	TIME [epoch: 7.86 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.037342514352796		[learning rate: 0.00014429]
	Learning Rate: 0.000144288
	LOSS [training: 5.037342514352796 | validation: 5.050051607592033]
	TIME [epoch: 7.9 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030089587789844		[learning rate: 0.00014395]
	Learning Rate: 0.000143948
	LOSS [training: 5.030089587789844 | validation: 5.028228568752698]
	TIME [epoch: 7.88 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025125872411745		[learning rate: 0.00014361]
	Learning Rate: 0.000143608
	LOSS [training: 5.025125872411745 | validation: 5.040463528888454]
	TIME [epoch: 7.86 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.020621302582643		[learning rate: 0.00014327]
	Learning Rate: 0.00014327
	LOSS [training: 5.020621302582643 | validation: 5.033928650190948]
	TIME [epoch: 7.86 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024448504841114		[learning rate: 0.00014293]
	Learning Rate: 0.000142932
	LOSS [training: 5.024448504841114 | validation: 5.03805468917983]
	TIME [epoch: 7.86 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.020198354833538		[learning rate: 0.00014259]
	Learning Rate: 0.000142594
	LOSS [training: 5.020198354833538 | validation: 5.038488982793595]
	TIME [epoch: 7.89 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0186650020836225		[learning rate: 0.00014226]
	Learning Rate: 0.000142258
	LOSS [training: 5.0186650020836225 | validation: 5.033605941433584]
	TIME [epoch: 7.88 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.022611745460219		[learning rate: 0.00014192]
	Learning Rate: 0.000141923
	LOSS [training: 5.022611745460219 | validation: 5.043065281684511]
	TIME [epoch: 7.86 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.021870817737787		[learning rate: 0.00014159]
	Learning Rate: 0.000141588
	LOSS [training: 5.021870817737787 | validation: 5.0354474707499985]
	TIME [epoch: 7.87 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.018764945934293		[learning rate: 0.00014125]
	Learning Rate: 0.000141254
	LOSS [training: 5.018764945934293 | validation: 5.0339236024487874]
	TIME [epoch: 7.86 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01673953037082		[learning rate: 0.00014092]
	Learning Rate: 0.000140921
	LOSS [training: 5.01673953037082 | validation: 5.0328098095672935]
	TIME [epoch: 7.9 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013890258489841		[learning rate: 0.00014059]
	Learning Rate: 0.000140588
	LOSS [training: 5.013890258489841 | validation: 5.026679195841684]
	TIME [epoch: 7.89 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013398910953354		[learning rate: 0.00014026]
	Learning Rate: 0.000140257
	LOSS [training: 5.013398910953354 | validation: 5.028989538602604]
	TIME [epoch: 7.86 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01564623446544		[learning rate: 0.00013993]
	Learning Rate: 0.000139926
	LOSS [training: 5.01564623446544 | validation: 5.030725778328953]
	TIME [epoch: 7.85 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013501630541059		[learning rate: 0.0001396]
	Learning Rate: 0.000139596
	LOSS [training: 5.013501630541059 | validation: 5.022165698431873]
	TIME [epoch: 7.86 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0063432569214354		[learning rate: 0.00013927]
	Learning Rate: 0.000139266
	LOSS [training: 5.0063432569214354 | validation: 5.027779907993117]
	TIME [epoch: 7.88 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0130984605848266		[learning rate: 0.00013894]
	Learning Rate: 0.000138938
	LOSS [training: 5.0130984605848266 | validation: 5.0343225829195655]
	TIME [epoch: 7.88 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02831102731216		[learning rate: 0.00013861]
	Learning Rate: 0.00013861
	LOSS [training: 5.02831102731216 | validation: 5.042349896868457]
	TIME [epoch: 7.85 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.029667232858745		[learning rate: 0.00013828]
	Learning Rate: 0.000138283
	LOSS [training: 5.029667232858745 | validation: 5.044138850654297]
	TIME [epoch: 7.86 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.025245130078617		[learning rate: 0.00013796]
	Learning Rate: 0.000137957
	LOSS [training: 5.025245130078617 | validation: 5.036860989268346]
	TIME [epoch: 7.86 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0219034628308705		[learning rate: 0.00013763]
	Learning Rate: 0.000137632
	LOSS [training: 5.0219034628308705 | validation: 5.045355752839226]
	TIME [epoch: 7.89 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02905737858252		[learning rate: 0.00013731]
	Learning Rate: 0.000137307
	LOSS [training: 5.02905737858252 | validation: 5.0508337375841466]
	TIME [epoch: 7.88 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.03190035942998		[learning rate: 0.00013698]
	Learning Rate: 0.000136983
	LOSS [training: 5.03190035942998 | validation: 5.039343651250846]
	TIME [epoch: 7.86 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02664440387338		[learning rate: 0.00013666]
	Learning Rate: 0.00013666
	LOSS [training: 5.02664440387338 | validation: 5.035357934080782]
	TIME [epoch: 7.86 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0124747904699225		[learning rate: 0.00013634]
	Learning Rate: 0.000136338
	LOSS [training: 5.0124747904699225 | validation: 5.026222450002014]
	TIME [epoch: 7.85 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007231078858453		[learning rate: 0.00013602]
	Learning Rate: 0.000136016
	LOSS [training: 5.007231078858453 | validation: 5.017833239387036]
	TIME [epoch: 7.9 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007437707303114		[learning rate: 0.0001357]
	Learning Rate: 0.000135695
	LOSS [training: 5.007437707303114 | validation: 5.033965452286331]
	TIME [epoch: 7.88 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.019362622238754		[learning rate: 0.00013538]
	Learning Rate: 0.000135375
	LOSS [training: 5.019362622238754 | validation: 5.031577906282846]
	TIME [epoch: 7.85 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024950676779424		[learning rate: 0.00013506]
	Learning Rate: 0.000135056
	LOSS [training: 5.024950676779424 | validation: 5.0410227320419505]
	TIME [epoch: 7.86 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.02880601925818		[learning rate: 0.00013474]
	Learning Rate: 0.000134737
	LOSS [training: 5.02880601925818 | validation: 5.047587585087216]
	TIME [epoch: 7.86 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.035975900843142		[learning rate: 0.00013442]
	Learning Rate: 0.000134419
	LOSS [training: 5.035975900843142 | validation: 5.035516232819523]
	TIME [epoch: 7.9 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0321158520627325		[learning rate: 0.0001341]
	Learning Rate: 0.000134102
	LOSS [training: 5.0321158520627325 | validation: 5.036565874108141]
	TIME [epoch: 7.88 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0234096564754545		[learning rate: 0.00013379]
	Learning Rate: 0.000133786
	LOSS [training: 5.0234096564754545 | validation: 5.036105506674085]
	TIME [epoch: 7.86 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.015538323462252		[learning rate: 0.00013347]
	Learning Rate: 0.00013347
	LOSS [training: 5.015538323462252 | validation: 5.021138537228492]
	TIME [epoch: 7.85 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007013559686113		[learning rate: 0.00013316]
	Learning Rate: 0.000133155
	LOSS [training: 5.007013559686113 | validation: 5.023699942993011]
	TIME [epoch: 7.86 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.008537902861556		[learning rate: 0.00013284]
	Learning Rate: 0.000132841
	LOSS [training: 5.008537902861556 | validation: 5.026549392331285]
	TIME [epoch: 7.89 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.015606103162007		[learning rate: 0.00013253]
	Learning Rate: 0.000132528
	LOSS [training: 5.015606103162007 | validation: 5.0260790694261175]
	TIME [epoch: 7.88 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.016135329800509		[learning rate: 0.00013222]
	Learning Rate: 0.000132215
	LOSS [training: 5.016135329800509 | validation: 5.027009976765276]
	TIME [epoch: 7.85 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.010615806246666		[learning rate: 0.0001319]
	Learning Rate: 0.000131904
	LOSS [training: 5.010615806246666 | validation: 5.025950907060983]
	TIME [epoch: 7.85 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.018341138711615		[learning rate: 0.00013159]
	Learning Rate: 0.000131592
	LOSS [training: 5.018341138711615 | validation: 5.030753297405166]
	TIME [epoch: 7.85 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.016594326917732		[learning rate: 0.00013128]
	Learning Rate: 0.000131282
	LOSS [training: 5.016594326917732 | validation: 5.018963765198118]
	TIME [epoch: 7.89 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009807480191336		[learning rate: 0.00013097]
	Learning Rate: 0.000130972
	LOSS [training: 5.009807480191336 | validation: 5.004473263045156]
	TIME [epoch: 7.88 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.006873391748082		[learning rate: 0.00013066]
	Learning Rate: 0.000130663
	LOSS [training: 5.006873391748082 | validation: 5.023165025990874]
	TIME [epoch: 7.85 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.012135561587118		[learning rate: 0.00013036]
	Learning Rate: 0.000130355
	LOSS [training: 5.012135561587118 | validation: 5.025323273767102]
	TIME [epoch: 7.85 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01764422476053		[learning rate: 0.00013005]
	Learning Rate: 0.000130048
	LOSS [training: 5.01764422476053 | validation: 5.027004036697916]
	TIME [epoch: 7.85 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.012188840180817		[learning rate: 0.00012974]
	Learning Rate: 0.000129741
	LOSS [training: 5.012188840180817 | validation: 5.031725217933283]
	TIME [epoch: 7.89 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01027024284558		[learning rate: 0.00012943]
	Learning Rate: 0.000129435
	LOSS [training: 5.01027024284558 | validation: 5.01397798542407]
	TIME [epoch: 7.86 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.000513030385923		[learning rate: 0.00012913]
	Learning Rate: 0.00012913
	LOSS [training: 5.000513030385923 | validation: 5.01148387067131]
	TIME [epoch: 7.86 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.999130146421542		[learning rate: 0.00012882]
	Learning Rate: 0.000128825
	LOSS [training: 4.999130146421542 | validation: 5.0146703641403025]
	TIME [epoch: 7.85 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.000832315919432		[learning rate: 0.00012852]
	Learning Rate: 0.000128521
	LOSS [training: 5.000832315919432 | validation: 5.009799737815577]
	TIME [epoch: 7.86 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.00683563942919		[learning rate: 0.00012822]
	Learning Rate: 0.000128218
	LOSS [training: 5.00683563942919 | validation: 5.020259230867046]
	TIME [epoch: 7.88 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009849499801312		[learning rate: 0.00012792]
	Learning Rate: 0.000127915
	LOSS [training: 5.009849499801312 | validation: 5.023591362588326]
	TIME [epoch: 7.88 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009502994921871		[learning rate: 0.00012761]
	Learning Rate: 0.000127614
	LOSS [training: 5.009502994921871 | validation: 5.017594243163646]
	TIME [epoch: 7.86 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.010465092077136		[learning rate: 0.00012731]
	Learning Rate: 0.000127313
	LOSS [training: 5.010465092077136 | validation: 5.0176861653414235]
	TIME [epoch: 7.84 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.006409207764056		[learning rate: 0.00012701]
	Learning Rate: 0.000127012
	LOSS [training: 5.006409207764056 | validation: 5.017877118556031]
	TIME [epoch: 7.85 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0014254628954244		[learning rate: 0.00012671]
	Learning Rate: 0.000126713
	LOSS [training: 5.0014254628954244 | validation: 5.0151494550473]
	TIME [epoch: 7.9 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005553745655567		[learning rate: 0.00012641]
	Learning Rate: 0.000126414
	LOSS [training: 5.005553745655567 | validation: 5.012171801305343]
	TIME [epoch: 7.87 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009209507084974		[learning rate: 0.00012612]
	Learning Rate: 0.000126116
	LOSS [training: 5.009209507084974 | validation: 5.016293581953041]
	TIME [epoch: 7.86 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.007267674601526		[learning rate: 0.00012582]
	Learning Rate: 0.000125818
	LOSS [training: 5.007267674601526 | validation: 5.005681007288067]
	TIME [epoch: 7.86 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.999525459004904		[learning rate: 0.00012552]
	Learning Rate: 0.000125521
	LOSS [training: 4.999525459004904 | validation: 5.007092944962121]
	TIME [epoch: 7.85 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0028542348869465		[learning rate: 0.00012523]
	Learning Rate: 0.000125225
	LOSS [training: 5.0028542348869465 | validation: 5.008127715248051]
	TIME [epoch: 7.89 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004426470994281		[learning rate: 0.00012493]
	Learning Rate: 0.00012493
	LOSS [training: 5.004426470994281 | validation: 5.010253158992213]
	TIME [epoch: 7.88 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0001083785469636		[learning rate: 0.00012464]
	Learning Rate: 0.000124635
	LOSS [training: 5.0001083785469636 | validation: 5.006373110174428]
	TIME [epoch: 7.86 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004465501872267		[learning rate: 0.00012434]
	Learning Rate: 0.000124341
	LOSS [training: 5.004465501872267 | validation: 5.0116663087113835]
	TIME [epoch: 7.86 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005502685770445		[learning rate: 0.00012405]
	Learning Rate: 0.000124048
	LOSS [training: 5.005502685770445 | validation: 5.01507502878014]
	TIME [epoch: 7.86 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.00290424671881		[learning rate: 0.00012376]
	Learning Rate: 0.000123755
	LOSS [training: 5.00290424671881 | validation: 5.002043216274783]
	TIME [epoch: 7.9 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.997112958037688		[learning rate: 0.00012346]
	Learning Rate: 0.000123463
	LOSS [training: 4.997112958037688 | validation: 5.0041042304069006]
	TIME [epoch: 7.88 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.996176713310851		[learning rate: 0.00012317]
	Learning Rate: 0.000123172
	LOSS [training: 4.996176713310851 | validation: 5.0072186096158475]
	TIME [epoch: 7.86 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.999159610996804		[learning rate: 0.00012288]
	Learning Rate: 0.000122882
	LOSS [training: 4.999159610996804 | validation: 5.010407564970333]
	TIME [epoch: 7.85 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9975152334278325		[learning rate: 0.00012259]
	Learning Rate: 0.000122592
	LOSS [training: 4.9975152334278325 | validation: 5.006494717576074]
	TIME [epoch: 7.86 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.996401599566059		[learning rate: 0.0001223]
	Learning Rate: 0.000122303
	LOSS [training: 4.996401599566059 | validation: 5.008324956224474]
	TIME [epoch: 7.9 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.00243602872425		[learning rate: 0.00012201]
	Learning Rate: 0.000122014
	LOSS [training: 5.00243602872425 | validation: 5.016751598441934]
	TIME [epoch: 7.87 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.012373434967445		[learning rate: 0.00012173]
	Learning Rate: 0.000121726
	LOSS [training: 5.012373434967445 | validation: 5.02397810858227]
	TIME [epoch: 7.86 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013916840106036		[learning rate: 0.00012144]
	Learning Rate: 0.000121439
	LOSS [training: 5.013916840106036 | validation: 5.030312770941654]
	TIME [epoch: 7.85 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.024935010453612		[learning rate: 0.00012115]
	Learning Rate: 0.000121153
	LOSS [training: 5.024935010453612 | validation: 5.023624771117072]
	TIME [epoch: 7.86 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.017712313868044		[learning rate: 0.00012087]
	Learning Rate: 0.000120867
	LOSS [training: 5.017712313868044 | validation: 5.0189516703483745]
	TIME [epoch: 7.9 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011352088964794		[learning rate: 0.00012058]
	Learning Rate: 0.000120582
	LOSS [training: 5.011352088964794 | validation: 5.024019955508223]
	TIME [epoch: 7.87 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.009762061548942		[learning rate: 0.0001203]
	Learning Rate: 0.000120297
	LOSS [training: 5.009762061548942 | validation: 5.0169526912511495]
	TIME [epoch: 7.86 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.008797512456291		[learning rate: 0.00012001]
	Learning Rate: 0.000120014
	LOSS [training: 5.008797512456291 | validation: 5.022383450256606]
	TIME [epoch: 7.86 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.011305191796444		[learning rate: 0.00011973]
	Learning Rate: 0.000119731
	LOSS [training: 5.011305191796444 | validation: 5.015178102318856]
	TIME [epoch: 7.86 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.001397275334313		[learning rate: 0.00011945]
	Learning Rate: 0.000119448
	LOSS [training: 5.001397275334313 | validation: 5.015886258370392]
	TIME [epoch: 7.91 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005772773193725		[learning rate: 0.00011917]
	Learning Rate: 0.000119166
	LOSS [training: 5.005772773193725 | validation: 5.01215696618727]
	TIME [epoch: 7.87 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9936940944484025		[learning rate: 0.00011889]
	Learning Rate: 0.000118885
	LOSS [training: 4.9936940944484025 | validation: 4.9956224485610665]
	TIME [epoch: 7.86 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984997175821015		[learning rate: 0.0001186]
	Learning Rate: 0.000118605
	LOSS [training: 4.984997175821015 | validation: 4.990057236292152]
	TIME [epoch: 7.86 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984713380827869		[learning rate: 0.00011833]
	Learning Rate: 0.000118325
	LOSS [training: 4.984713380827869 | validation: 4.996245753776925]
	TIME [epoch: 7.86 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.994384464952837		[learning rate: 0.00011805]
	Learning Rate: 0.000118046
	LOSS [training: 4.994384464952837 | validation: 5.010837726107763]
	TIME [epoch: 7.91 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.001879702503959		[learning rate: 0.00011777]
	Learning Rate: 0.000117768
	LOSS [training: 5.001879702503959 | validation: 5.006920967297404]
	TIME [epoch: 7.87 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.001638362650709		[learning rate: 0.00011749]
	Learning Rate: 0.00011749
	LOSS [training: 5.001638362650709 | validation: 5.011308427752777]
	TIME [epoch: 7.86 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.996846080652011		[learning rate: 0.00011721]
	Learning Rate: 0.000117213
	LOSS [training: 4.996846080652011 | validation: 5.005454843831781]
	TIME [epoch: 7.86 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.992366565586012		[learning rate: 0.00011694]
	Learning Rate: 0.000116936
	LOSS [training: 4.992366565586012 | validation: 4.9971913307033144]
	TIME [epoch: 7.87 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.979824257143765		[learning rate: 0.00011666]
	Learning Rate: 0.00011666
	LOSS [training: 4.979824257143765 | validation: 4.9878854772912025]
	TIME [epoch: 7.91 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.977410873657245		[learning rate: 0.00011639]
	Learning Rate: 0.000116385
	LOSS [training: 4.977410873657245 | validation: 4.987675889979522]
	TIME [epoch: 7.87 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9830425574087185		[learning rate: 0.00011611]
	Learning Rate: 0.000116111
	LOSS [training: 4.9830425574087185 | validation: 4.990050292705066]
	TIME [epoch: 7.86 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.976563752065326		[learning rate: 0.00011584]
	Learning Rate: 0.000115837
	LOSS [training: 4.976563752065326 | validation: 4.981690650148241]
	TIME [epoch: 7.86 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9688264765555985		[learning rate: 0.00011556]
	Learning Rate: 0.000115563
	LOSS [training: 4.9688264765555985 | validation: 4.980508660840432]
	TIME [epoch: 7.86 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.969728289879043		[learning rate: 0.00011529]
	Learning Rate: 0.000115291
	LOSS [training: 4.969728289879043 | validation: 4.98440638115755]
	TIME [epoch: 7.9 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.966215229161903		[learning rate: 0.00011502]
	Learning Rate: 0.000115019
	LOSS [training: 4.966215229161903 | validation: 4.986947095501118]
	TIME [epoch: 7.86 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.96361530762602		[learning rate: 0.00011475]
	Learning Rate: 0.000114748
	LOSS [training: 4.96361530762602 | validation: 4.972650686571594]
	TIME [epoch: 7.85 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.959001592631799		[learning rate: 0.00011448]
	Learning Rate: 0.000114477
	LOSS [training: 4.959001592631799 | validation: 4.9684324203692904]
	TIME [epoch: 7.86 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.960563342889536		[learning rate: 0.00011421]
	Learning Rate: 0.000114207
	LOSS [training: 4.960563342889536 | validation: 4.985186021749108]
	TIME [epoch: 7.87 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.965751155734902		[learning rate: 0.00011394]
	Learning Rate: 0.000113938
	LOSS [training: 4.965751155734902 | validation: 4.971335216274193]
	TIME [epoch: 7.91 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9586348937171		[learning rate: 0.00011367]
	Learning Rate: 0.000113669
	LOSS [training: 4.9586348937171 | validation: 4.973791843006742]
	TIME [epoch: 7.87 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.961768986450571		[learning rate: 0.0001134]
	Learning Rate: 0.000113401
	LOSS [training: 4.961768986450571 | validation: 4.974388138346224]
	TIME [epoch: 7.86 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.962052292334106		[learning rate: 0.00011313]
	Learning Rate: 0.000113133
	LOSS [training: 4.962052292334106 | validation: 4.9808518897912855]
	TIME [epoch: 7.86 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.964435233549436		[learning rate: 0.00011287]
	Learning Rate: 0.000112866
	LOSS [training: 4.964435233549436 | validation: 4.984781302069902]
	TIME [epoch: 7.86 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.967961257535556		[learning rate: 0.0001126]
	Learning Rate: 0.0001126
	LOSS [training: 4.967961257535556 | validation: 4.977769826661341]
	TIME [epoch: 7.91 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.963696734862779		[learning rate: 0.00011233]
	Learning Rate: 0.000112334
	LOSS [training: 4.963696734862779 | validation: 4.964471764144042]
	TIME [epoch: 7.86 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.961753682230798		[learning rate: 0.00011207]
	Learning Rate: 0.000112069
	LOSS [training: 4.961753682230798 | validation: 4.963225156008543]
	TIME [epoch: 7.86 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.954444246720997		[learning rate: 0.00011181]
	Learning Rate: 0.000111805
	LOSS [training: 4.954444246720997 | validation: 4.958932112705055]
	TIME [epoch: 7.85 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9490600671964255		[learning rate: 0.00011154]
	Learning Rate: 0.000111541
	LOSS [training: 4.9490600671964255 | validation: 4.954255600248429]
	TIME [epoch: 7.85 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.949083232977559		[learning rate: 0.00011128]
	Learning Rate: 0.000111278
	LOSS [training: 4.949083232977559 | validation: 4.961100555478102]
	TIME [epoch: 7.9 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.950850205583418		[learning rate: 0.00011102]
	Learning Rate: 0.000111016
	LOSS [training: 4.950850205583418 | validation: 4.962977326470272]
	TIME [epoch: 7.86 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.941864989653766		[learning rate: 0.00011075]
	Learning Rate: 0.000110754
	LOSS [training: 4.941864989653766 | validation: 4.959549338257849]
	TIME [epoch: 7.85 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.949647114775152		[learning rate: 0.00011049]
	Learning Rate: 0.000110493
	LOSS [training: 4.949647114775152 | validation: 4.962506032289765]
	TIME [epoch: 7.87 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.945145739683535		[learning rate: 0.00011023]
	Learning Rate: 0.000110232
	LOSS [training: 4.945145739683535 | validation: 4.952338765864836]
	TIME [epoch: 7.86 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.945497289347189		[learning rate: 0.00010997]
	Learning Rate: 0.000109972
	LOSS [training: 4.945497289347189 | validation: 4.957806599406638]
	TIME [epoch: 7.9 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.944750693903583		[learning rate: 0.00010971]
	Learning Rate: 0.000109713
	LOSS [training: 4.944750693903583 | validation: 4.956757123121918]
	TIME [epoch: 7.85 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.944351721712369		[learning rate: 0.00010945]
	Learning Rate: 0.000109454
	LOSS [training: 4.944351721712369 | validation: 4.964950344201913]
	TIME [epoch: 7.86 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.956779680841517		[learning rate: 0.0001092]
	Learning Rate: 0.000109196
	LOSS [training: 4.956779680841517 | validation: 4.9693098495959465]
	TIME [epoch: 7.86 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.959718534389207		[learning rate: 0.00010894]
	Learning Rate: 0.000108938
	LOSS [training: 4.959718534389207 | validation: 4.9685519233946485]
	TIME [epoch: 7.87 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.96181046133635		[learning rate: 0.00010868]
	Learning Rate: 0.000108681
	LOSS [training: 4.96181046133635 | validation: 4.975865583648746]
	TIME [epoch: 7.91 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.967001947154566		[learning rate: 0.00010842]
	Learning Rate: 0.000108425
	LOSS [training: 4.967001947154566 | validation: 4.976615209352301]
	TIME [epoch: 7.87 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.968517977639969		[learning rate: 0.00010817]
	Learning Rate: 0.000108169
	LOSS [training: 4.968517977639969 | validation: 4.983821838293543]
	TIME [epoch: 7.86 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.972640921497132		[learning rate: 0.00010791]
	Learning Rate: 0.000107914
	LOSS [training: 4.972640921497132 | validation: 4.979059352933327]
	TIME [epoch: 7.85 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9701832893660285		[learning rate: 0.00010766]
	Learning Rate: 0.000107659
	LOSS [training: 4.9701832893660285 | validation: 4.984011348680797]
	TIME [epoch: 7.86 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.968596098401207		[learning rate: 0.00010741]
	Learning Rate: 0.000107405
	LOSS [training: 4.968596098401207 | validation: 4.9835665850580995]
	TIME [epoch: 7.91 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.97589220544055		[learning rate: 0.00010715]
	Learning Rate: 0.000107152
	LOSS [training: 4.97589220544055 | validation: 5.005010393327122]
	TIME [epoch: 7.86 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9815418243812575		[learning rate: 0.0001069]
	Learning Rate: 0.000106899
	LOSS [training: 4.9815418243812575 | validation: 5.00066298869457]
	TIME [epoch: 7.86 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984558871267299		[learning rate: 0.00010665]
	Learning Rate: 0.000106647
	LOSS [training: 4.984558871267299 | validation: 4.99553629696904]
	TIME [epoch: 7.86 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9800973304300005		[learning rate: 0.0001064]
	Learning Rate: 0.000106395
	LOSS [training: 4.9800973304300005 | validation: 4.993529871868752]
	TIME [epoch: 7.86 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.977052551208638		[learning rate: 0.00010614]
	Learning Rate: 0.000106145
	LOSS [training: 4.977052551208638 | validation: 4.990148055308877]
	TIME [epoch: 7.91 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.979321412611792		[learning rate: 0.00010589]
	Learning Rate: 0.000105894
	LOSS [training: 4.979321412611792 | validation: 4.996994838974305]
	TIME [epoch: 7.87 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.983590719675263		[learning rate: 0.00010564]
	Learning Rate: 0.000105644
	LOSS [training: 4.983590719675263 | validation: 5.000367732605951]
	TIME [epoch: 7.86 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984508515290263		[learning rate: 0.0001054]
	Learning Rate: 0.000105395
	LOSS [training: 4.984508515290263 | validation: 4.990907222123903]
	TIME [epoch: 7.87 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.980847141276342		[learning rate: 0.00010515]
	Learning Rate: 0.000105147
	LOSS [training: 4.980847141276342 | validation: 4.993660257686743]
	TIME [epoch: 7.87 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.97357892961224		[learning rate: 0.0001049]
	Learning Rate: 0.000104899
	LOSS [training: 4.97357892961224 | validation: 4.9819515149941225]
	TIME [epoch: 7.92 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.969862571825185		[learning rate: 0.00010465]
	Learning Rate: 0.000104651
	LOSS [training: 4.969862571825185 | validation: 4.988193075611095]
	TIME [epoch: 7.87 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.963602416196411		[learning rate: 0.0001044]
	Learning Rate: 0.000104404
	LOSS [training: 4.963602416196411 | validation: 4.979840156935303]
	TIME [epoch: 7.87 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.957764741899042		[learning rate: 0.00010416]
	Learning Rate: 0.000104158
	LOSS [training: 4.957764741899042 | validation: 4.965783962184651]
	TIME [epoch: 7.86 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.954027749347976		[learning rate: 0.00010391]
	Learning Rate: 0.000103912
	LOSS [training: 4.954027749347976 | validation: 4.9618757470778885]
	TIME [epoch: 7.86 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.952159270470471		[learning rate: 0.00010367]
	Learning Rate: 0.000103667
	LOSS [training: 4.952159270470471 | validation: 4.954575129594906]
	TIME [epoch: 7.91 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.936173108066514		[learning rate: 0.00010342]
	Learning Rate: 0.000103423
	LOSS [training: 4.936173108066514 | validation: 4.944863671870866]
	TIME [epoch: 7.87 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.941163474907422		[learning rate: 0.00010318]
	Learning Rate: 0.000103179
	LOSS [training: 4.941163474907422 | validation: 4.946735468199706]
	TIME [epoch: 7.87 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.936399637099243		[learning rate: 0.00010294]
	Learning Rate: 0.000102935
	LOSS [training: 4.936399637099243 | validation: 4.944321224181305]
	TIME [epoch: 7.86 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93294206557423		[learning rate: 0.00010269]
	Learning Rate: 0.000102692
	LOSS [training: 4.93294206557423 | validation: 4.954542096231476]
	TIME [epoch: 7.87 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.940870730156129		[learning rate: 0.00010245]
	Learning Rate: 0.00010245
	LOSS [training: 4.940870730156129 | validation: 4.956217213428558]
	TIME [epoch: 7.92 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9429735541157855		[learning rate: 0.00010221]
	Learning Rate: 0.000102209
	LOSS [training: 4.9429735541157855 | validation: 4.950238423368659]
	TIME [epoch: 7.86 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93822698640013		[learning rate: 0.00010197]
	Learning Rate: 0.000101967
	LOSS [training: 4.93822698640013 | validation: 4.95936677932048]
	TIME [epoch: 7.87 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.938264041946491		[learning rate: 0.00010173]
	Learning Rate: 0.000101727
	LOSS [training: 4.938264041946491 | validation: 4.949469114417038]
	TIME [epoch: 7.86 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.934283918451873		[learning rate: 0.00010149]
	Learning Rate: 0.000101487
	LOSS [training: 4.934283918451873 | validation: 4.959660659955498]
	TIME [epoch: 7.86 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9466400407924445		[learning rate: 0.00010125]
	Learning Rate: 0.000101248
	LOSS [training: 4.9466400407924445 | validation: 4.9558051234406]
	TIME [epoch: 7.92 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.942850711049017		[learning rate: 0.00010101]
	Learning Rate: 0.000101009
	LOSS [training: 4.942850711049017 | validation: 4.957964920308788]
	TIME [epoch: 7.87 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.938310319605858		[learning rate: 0.00010077]
	Learning Rate: 0.000100771
	LOSS [training: 4.938310319605858 | validation: 4.941631562392063]
	TIME [epoch: 7.87 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93364232976635		[learning rate: 0.00010053]
	Learning Rate: 0.000100533
	LOSS [training: 4.93364232976635 | validation: 4.941025490171992]
	TIME [epoch: 7.86 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.927998424662842		[learning rate: 0.0001003]
	Learning Rate: 0.000100296
	LOSS [training: 4.927998424662842 | validation: 4.9450259784593245]
	TIME [epoch: 7.87 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.932143578926271		[learning rate: 0.00010006]
	Learning Rate: 0.000100059
	LOSS [training: 4.932143578926271 | validation: 4.9439251967345985]
	TIME [epoch: 7.91 sec]
Finished training in 16670.195 seconds.
