Args:
Namespace(name='model_phi1_1a_v1', outdir='out/model_training/model_phi1_1a_v1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 421480593

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.936716831239318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.936716831239318 | validation: 9.441295120550398]
	TIME [epoch: 87.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.053655131835956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.053655131835956 | validation: 8.836445121032916]
	TIME [epoch: 7.18 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.789590541853629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.789590541853629 | validation: 8.58938547158754]
	TIME [epoch: 7.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.268489415768283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.268489415768283 | validation: 8.410410880084624]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.082144593126719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.082144593126719 | validation: 8.228286963160956]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.723755563291359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.723755563291359 | validation: 8.493475260669001]
	TIME [epoch: 7.13 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.51367341059757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.51367341059757 | validation: 8.600961685152985]
	TIME [epoch: 7.13 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.045559330085343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.045559330085343 | validation: 7.704120019406718]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.719027478305016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.719027478305016 | validation: 8.074436703490184]
	TIME [epoch: 7.13 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.7896569479813405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.7896569479813405 | validation: 7.558482196779124]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.298979363627367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.298979363627367 | validation: 7.230080913790867]
	TIME [epoch: 7.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.027576003913287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.027576003913287 | validation: 7.289581781622141]
	TIME [epoch: 7.14 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.0384584699109665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0384584699109665 | validation: 7.322562439243656]
	TIME [epoch: 7.13 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.0497783920622865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0497783920622865 | validation: 7.6742381323127375]
	TIME [epoch: 7.13 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.726091669333556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.726091669333556 | validation: 6.462278274050021]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.45485859973721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.45485859973721 | validation: 6.190598482324598]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.089922029376325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.089922029376325 | validation: 5.944003210538051]
	TIME [epoch: 7.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.998077023336263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.998077023336263 | validation: 5.847894664929896]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.588355179985461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.588355179985461 | validation: 5.816902207576749]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7858157092954405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7858157092954405 | validation: 5.498683231315576]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.549963058668661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.549963058668661 | validation: 5.279628105562178]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.436672578379448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.436672578379448 | validation: 5.558689508925592]
	TIME [epoch: 7.14 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.60101125259642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.60101125259642 | validation: 5.378722931344655]
	TIME [epoch: 7.16 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189432293939257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.189432293939257 | validation: 5.103797598038176]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.935538114137236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.935538114137236 | validation: 5.204638454474043]
	TIME [epoch: 7.13 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.858174817780777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.858174817780777 | validation: 5.009208652366705]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.933711722817415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.933711722817415 | validation: 4.901137142838268]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93179943636154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.93179943636154 | validation: 5.090588282425118]
	TIME [epoch: 7.16 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.686716643415392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686716643415392 | validation: 4.702560405162139]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.462052666965162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.462052666965162 | validation: 4.472718164279879]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.38428852575421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.38428852575421 | validation: 4.860412085687729]
	TIME [epoch: 7.13 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3720986364910575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3720986364910575 | validation: 4.069049493026341]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.101289187067048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.101289187067048 | validation: 4.035697645682841]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8379888304963288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8379888304963288 | validation: 4.439822075658786]
	TIME [epoch: 7.16 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8108974071020008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8108974071020008 | validation: 3.4699447549240414]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.425156522426086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.425156522426086 | validation: 3.2746480466379344]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3994365474589543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3994365474589543 | validation: 3.598676045354161]
	TIME [epoch: 7.13 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2962657007165412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2962657007165412 | validation: 3.212932505916075]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1610590215298755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1610590215298755 | validation: 2.828221787062403]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.796579955074632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.796579955074632 | validation: 3.118684973455204]
	TIME [epoch: 7.18 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9944145529445856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9944145529445856 | validation: 3.034913724943587]
	TIME [epoch: 7.13 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0099609416971473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0099609416971473 | validation: 2.5693799145629015]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.764028522415102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.764028522415102 | validation: 2.4487405482893987]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958100939008739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.958100939008739 | validation: 2.7408427493684093]
	TIME [epoch: 7.13 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8230262361425424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8230262361425424 | validation: 2.5039136791557306]
	TIME [epoch: 7.12 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5217540898138617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5217540898138617 | validation: 2.354881012169369]
	TIME [epoch: 7.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.563182859047797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.563182859047797 | validation: 2.311959383260449]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.475587854514458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.475587854514458 | validation: 2.3252095596408915]
	TIME [epoch: 7.13 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.414783540607483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.414783540607483 | validation: 2.312303388703638]
	TIME [epoch: 7.12 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.541085156686379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.541085156686379 | validation: 2.3278845525390617]
	TIME [epoch: 7.13 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4884711406692928		[learning rate: 0.0099811]
	Learning Rate: 0.00998112
	LOSS [training: 2.4884711406692928 | validation: 2.146657085895902]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.490686170775145		[learning rate: 0.0099576]
	Learning Rate: 0.00995758
	LOSS [training: 2.490686170775145 | validation: 2.4427447456285236]
	TIME [epoch: 7.15 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2082054180946513		[learning rate: 0.0099341]
	Learning Rate: 0.00993409
	LOSS [training: 2.2082054180946513 | validation: 2.4744919582039735]
	TIME [epoch: 7.13 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.408217800365804		[learning rate: 0.0099107]
	Learning Rate: 0.00991066
	LOSS [training: 2.408217800365804 | validation: 2.6901371508804806]
	TIME [epoch: 7.12 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3579818602294482		[learning rate: 0.0098873]
	Learning Rate: 0.00988728
	LOSS [training: 2.3579818602294482 | validation: 2.1033598076243485]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0128104472642385		[learning rate: 0.009864]
	Learning Rate: 0.00986396
	LOSS [training: 2.0128104472642385 | validation: 2.0389651187674644]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3148078048975984		[learning rate: 0.0098407]
	Learning Rate: 0.00984069
	LOSS [training: 2.3148078048975984 | validation: 2.1096630112529864]
	TIME [epoch: 7.15 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5999604155566614		[learning rate: 0.0098175]
	Learning Rate: 0.00981748
	LOSS [training: 2.5999604155566614 | validation: 2.2492906851315295]
	TIME [epoch: 7.14 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.04497084091332		[learning rate: 0.0097943]
	Learning Rate: 0.00979432
	LOSS [training: 2.04497084091332 | validation: 1.7880229124724676]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9959675748345092		[learning rate: 0.0097712]
	Learning Rate: 0.00977122
	LOSS [training: 1.9959675748345092 | validation: 2.471549361286929]
	TIME [epoch: 7.13 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0182472050205438		[learning rate: 0.0097482]
	Learning Rate: 0.00974817
	LOSS [training: 2.0182472050205438 | validation: 3.097066416652066]
	TIME [epoch: 7.14 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1399178870779947		[learning rate: 0.0097252]
	Learning Rate: 0.00972517
	LOSS [training: 2.1399178870779947 | validation: 1.9498381435375767]
	TIME [epoch: 7.13 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.060032577913219		[learning rate: 0.0097022]
	Learning Rate: 0.00970223
	LOSS [training: 2.060032577913219 | validation: 2.003266216215981]
	TIME [epoch: 7.16 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.913263613196356		[learning rate: 0.0096793]
	Learning Rate: 0.00967935
	LOSS [training: 1.913263613196356 | validation: 1.7038977393023302]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.053207102354247		[learning rate: 0.0096565]
	Learning Rate: 0.00965652
	LOSS [training: 2.053207102354247 | validation: 2.819927814735798]
	TIME [epoch: 7.13 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9656702028755813		[learning rate: 0.0096337]
	Learning Rate: 0.00963374
	LOSS [training: 1.9656702028755813 | validation: 1.7747315693384103]
	TIME [epoch: 7.12 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0800798572188306		[learning rate: 0.009611]
	Learning Rate: 0.00961101
	LOSS [training: 2.0800798572188306 | validation: 2.054118827117117]
	TIME [epoch: 7.12 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.978080360796671		[learning rate: 0.0095883]
	Learning Rate: 0.00958834
	LOSS [training: 1.978080360796671 | validation: 2.5401577537253113]
	TIME [epoch: 7.12 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.049836770111077		[learning rate: 0.0095657]
	Learning Rate: 0.00956573
	LOSS [training: 2.049836770111077 | validation: 1.9139255129692443]
	TIME [epoch: 7.17 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8075812359319623		[learning rate: 0.0095432]
	Learning Rate: 0.00954316
	LOSS [training: 1.8075812359319623 | validation: 1.961088902982318]
	TIME [epoch: 7.14 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8749035996005379		[learning rate: 0.0095207]
	Learning Rate: 0.00952065
	LOSS [training: 1.8749035996005379 | validation: 1.6557189446924698]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9299508049911303		[learning rate: 0.0094982]
	Learning Rate: 0.00949819
	LOSS [training: 1.9299508049911303 | validation: 1.9221391703198685]
	TIME [epoch: 7.14 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5895830248800147		[learning rate: 0.0094758]
	Learning Rate: 0.00947579
	LOSS [training: 2.5895830248800147 | validation: 1.9646221655039455]
	TIME [epoch: 7.13 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8655596595932054		[learning rate: 0.0094534]
	Learning Rate: 0.00945344
	LOSS [training: 1.8655596595932054 | validation: 1.789393616557461]
	TIME [epoch: 7.12 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6109960360357536		[learning rate: 0.0094311]
	Learning Rate: 0.00943114
	LOSS [training: 1.6109960360357536 | validation: 2.1244377265779515]
	TIME [epoch: 7.16 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3769691634166277		[learning rate: 0.0094089]
	Learning Rate: 0.00940889
	LOSS [training: 2.3769691634166277 | validation: 1.9792187661589828]
	TIME [epoch: 7.13 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6886381401158068		[learning rate: 0.0093867]
	Learning Rate: 0.0093867
	LOSS [training: 1.6886381401158068 | validation: 2.7595063146844896]
	TIME [epoch: 7.12 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8608510548417425		[learning rate: 0.0093646]
	Learning Rate: 0.00936456
	LOSS [training: 1.8608510548417425 | validation: 1.8755853949164516]
	TIME [epoch: 7.12 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6746833809072723		[learning rate: 0.0093425]
	Learning Rate: 0.00934247
	LOSS [training: 1.6746833809072723 | validation: 1.409963558223625]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5697162897453967		[learning rate: 0.0093204]
	Learning Rate: 0.00932043
	LOSS [training: 1.5697162897453967 | validation: 2.054866764858227]
	TIME [epoch: 7.12 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7961084438267672		[learning rate: 0.0092984]
	Learning Rate: 0.00929844
	LOSS [training: 1.7961084438267672 | validation: 1.5970780979402366]
	TIME [epoch: 7.16 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5359942433180098		[learning rate: 0.0092765]
	Learning Rate: 0.00927651
	LOSS [training: 1.5359942433180098 | validation: 1.5247421692077359]
	TIME [epoch: 7.13 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4613781437059827		[learning rate: 0.0092546]
	Learning Rate: 0.00925463
	LOSS [training: 1.4613781437059827 | validation: 2.16706949112422]
	TIME [epoch: 7.12 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9153724659049485		[learning rate: 0.0092328]
	Learning Rate: 0.0092328
	LOSS [training: 1.9153724659049485 | validation: 1.5600888366211798]
	TIME [epoch: 7.12 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.379690695135799		[learning rate: 0.009211]
	Learning Rate: 0.00921102
	LOSS [training: 2.379690695135799 | validation: 3.886685445892855]
	TIME [epoch: 7.12 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0971646987627817		[learning rate: 0.0091893]
	Learning Rate: 0.00918929
	LOSS [training: 2.0971646987627817 | validation: 1.4384944207835235]
	TIME [epoch: 7.13 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4205315488278294		[learning rate: 0.0091676]
	Learning Rate: 0.00916762
	LOSS [training: 1.4205315488278294 | validation: 1.5848807414624904]
	TIME [epoch: 7.15 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1670902537492642		[learning rate: 0.009146]
	Learning Rate: 0.00914599
	LOSS [training: 2.1670902537492642 | validation: 1.59787951674668]
	TIME [epoch: 7.13 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.675049764212406		[learning rate: 0.0091244]
	Learning Rate: 0.00912442
	LOSS [training: 1.675049764212406 | validation: 2.41332935566657]
	TIME [epoch: 7.12 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.505217479660261		[learning rate: 0.0091029]
	Learning Rate: 0.00910289
	LOSS [training: 2.505217479660261 | validation: 1.9438655817635744]
	TIME [epoch: 7.13 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6874713996313893		[learning rate: 0.0090814]
	Learning Rate: 0.00908142
	LOSS [training: 1.6874713996313893 | validation: 1.6762131499533783]
	TIME [epoch: 7.12 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8635455085813608		[learning rate: 0.00906]
	Learning Rate: 0.00906
	LOSS [training: 1.8635455085813608 | validation: 1.9730325022862552]
	TIME [epoch: 7.12 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.644860298917159		[learning rate: 0.0090386]
	Learning Rate: 0.00903863
	LOSS [training: 1.644860298917159 | validation: 1.6040471650767758]
	TIME [epoch: 7.15 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8596999280275484		[learning rate: 0.0090173]
	Learning Rate: 0.00901731
	LOSS [training: 1.8596999280275484 | validation: 1.7250388131171748]
	TIME [epoch: 7.13 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.555568016415195		[learning rate: 0.008996]
	Learning Rate: 0.00899604
	LOSS [training: 1.555568016415195 | validation: 2.888491859382208]
	TIME [epoch: 7.12 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7770264954042645		[learning rate: 0.0089748]
	Learning Rate: 0.00897482
	LOSS [training: 1.7770264954042645 | validation: 1.4425161981370391]
	TIME [epoch: 7.13 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4695274126265458		[learning rate: 0.0089536]
	Learning Rate: 0.00895365
	LOSS [training: 1.4695274126265458 | validation: 2.285624444942348]
	TIME [epoch: 7.12 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7931309473492434		[learning rate: 0.0089325]
	Learning Rate: 0.00893253
	LOSS [training: 1.7931309473492434 | validation: 1.7327467597456412]
	TIME [epoch: 7.13 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.726497253550474		[learning rate: 0.0089115]
	Learning Rate: 0.00891146
	LOSS [training: 1.726497253550474 | validation: 1.2932663558495947]
	TIME [epoch: 7.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.326110603399918		[learning rate: 0.0088904]
	Learning Rate: 0.00889044
	LOSS [training: 1.326110603399918 | validation: 1.8974692690211368]
	TIME [epoch: 7.13 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5889705061357888		[learning rate: 0.0088695]
	Learning Rate: 0.00886946
	LOSS [training: 1.5889705061357888 | validation: 1.606656082599414]
	TIME [epoch: 7.12 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3601210187275257		[learning rate: 0.0088485]
	Learning Rate: 0.00884854
	LOSS [training: 1.3601210187275257 | validation: 1.3019757114474828]
	TIME [epoch: 7.12 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0010661165477077		[learning rate: 0.0088277]
	Learning Rate: 0.00882767
	LOSS [training: 2.0010661165477077 | validation: 1.8523280107302607]
	TIME [epoch: 7.12 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5289448419251361		[learning rate: 0.0088068]
	Learning Rate: 0.00880685
	LOSS [training: 1.5289448419251361 | validation: 2.1428331363466473]
	TIME [epoch: 7.12 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6818939841035259		[learning rate: 0.0087861]
	Learning Rate: 0.00878607
	LOSS [training: 1.6818939841035259 | validation: 1.6837156116978864]
	TIME [epoch: 7.17 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.390244909913869		[learning rate: 0.0087653]
	Learning Rate: 0.00876535
	LOSS [training: 1.390244909913869 | validation: 1.6978191236953095]
	TIME [epoch: 7.13 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6126629964324		[learning rate: 0.0087447]
	Learning Rate: 0.00874467
	LOSS [training: 1.6126629964324 | validation: 1.2892807780348292]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7631935878867586		[learning rate: 0.008724]
	Learning Rate: 0.00872405
	LOSS [training: 1.7631935878867586 | validation: 1.4407583693551247]
	TIME [epoch: 7.13 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4324982460957958		[learning rate: 0.0087035]
	Learning Rate: 0.00870347
	LOSS [training: 1.4324982460957958 | validation: 1.472572843476609]
	TIME [epoch: 7.13 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3853585600564355		[learning rate: 0.0086829]
	Learning Rate: 0.00868294
	LOSS [training: 1.3853585600564355 | validation: 1.1488859303268422]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.087431861713777		[learning rate: 0.0086625]
	Learning Rate: 0.00866246
	LOSS [training: 2.087431861713777 | validation: 2.7917270921598556]
	TIME [epoch: 7.17 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7595588490778244		[learning rate: 0.008642]
	Learning Rate: 0.00864202
	LOSS [training: 1.7595588490778244 | validation: 1.210061622024723]
	TIME [epoch: 7.12 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5316959447384098		[learning rate: 0.0086216]
	Learning Rate: 0.00862164
	LOSS [training: 1.5316959447384098 | validation: 1.4014669697012354]
	TIME [epoch: 7.13 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5228730748716528		[learning rate: 0.0086013]
	Learning Rate: 0.0086013
	LOSS [training: 1.5228730748716528 | validation: 1.331933481162637]
	TIME [epoch: 7.12 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8123591806881119		[learning rate: 0.008581]
	Learning Rate: 0.00858101
	LOSS [training: 1.8123591806881119 | validation: 1.8041488785322444]
	TIME [epoch: 7.13 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7304037127480192		[learning rate: 0.0085608]
	Learning Rate: 0.00856077
	LOSS [training: 1.7304037127480192 | validation: 1.7603949653717965]
	TIME [epoch: 7.12 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.491706507183601		[learning rate: 0.0085406]
	Learning Rate: 0.00854058
	LOSS [training: 1.491706507183601 | validation: 1.4449801811985181]
	TIME [epoch: 7.16 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252328872754119		[learning rate: 0.0085204]
	Learning Rate: 0.00852043
	LOSS [training: 2.252328872754119 | validation: 1.4002655899801044]
	TIME [epoch: 7.12 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3849194167930903		[learning rate: 0.0085003]
	Learning Rate: 0.00850033
	LOSS [training: 1.3849194167930903 | validation: 1.185608354010924]
	TIME [epoch: 7.12 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3424009065264104		[learning rate: 0.0084803]
	Learning Rate: 0.00848028
	LOSS [training: 1.3424009065264104 | validation: 2.3925144815740707]
	TIME [epoch: 7.12 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8196248456509962		[learning rate: 0.0084603]
	Learning Rate: 0.00846028
	LOSS [training: 1.8196248456509962 | validation: 1.6052313184622689]
	TIME [epoch: 7.12 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4759863445143486		[learning rate: 0.0084403]
	Learning Rate: 0.00844032
	LOSS [training: 1.4759863445143486 | validation: 2.108553600895979]
	TIME [epoch: 7.13 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7159514629094832		[learning rate: 0.0084204]
	Learning Rate: 0.00842041
	LOSS [training: 1.7159514629094832 | validation: 1.3415067925882365]
	TIME [epoch: 7.17 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9240055042440143		[learning rate: 0.0084005]
	Learning Rate: 0.00840055
	LOSS [training: 1.9240055042440143 | validation: 1.5228139014872568]
	TIME [epoch: 7.13 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6026904682799255		[learning rate: 0.0083807]
	Learning Rate: 0.00838073
	LOSS [training: 1.6026904682799255 | validation: 1.448222050529326]
	TIME [epoch: 7.13 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6497518791683419		[learning rate: 0.008361]
	Learning Rate: 0.00836096
	LOSS [training: 1.6497518791683419 | validation: 2.079831229164962]
	TIME [epoch: 7.13 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4850193168777301		[learning rate: 0.0083412]
	Learning Rate: 0.00834124
	LOSS [training: 1.4850193168777301 | validation: 1.4016356996783301]
	TIME [epoch: 7.13 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4711932239922298		[learning rate: 0.0083216]
	Learning Rate: 0.00832157
	LOSS [training: 1.4711932239922298 | validation: 1.7830927214211008]
	TIME [epoch: 7.12 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.314061765639724		[learning rate: 0.0083019]
	Learning Rate: 0.00830194
	LOSS [training: 1.314061765639724 | validation: 1.2776002213475257]
	TIME [epoch: 7.15 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4216900005735194		[learning rate: 0.0082824]
	Learning Rate: 0.00828236
	LOSS [training: 1.4216900005735194 | validation: 1.6879513741462326]
	TIME [epoch: 7.13 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7499988898465013		[learning rate: 0.0082628]
	Learning Rate: 0.00826282
	LOSS [training: 1.7499988898465013 | validation: 1.511629379223114]
	TIME [epoch: 7.12 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4992060231427158		[learning rate: 0.0082433]
	Learning Rate: 0.00824333
	LOSS [training: 1.4992060231427158 | validation: 2.1978053786895595]
	TIME [epoch: 7.12 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6992598440734414		[learning rate: 0.0082239]
	Learning Rate: 0.00822388
	LOSS [training: 1.6992598440734414 | validation: 3.036863478028751]
	TIME [epoch: 7.11 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1284311625988375		[learning rate: 0.0082045]
	Learning Rate: 0.00820448
	LOSS [training: 2.1284311625988375 | validation: 3.527093182524146]
	TIME [epoch: 7.13 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8137546222668892		[learning rate: 0.0081851]
	Learning Rate: 0.00818513
	LOSS [training: 1.8137546222668892 | validation: 1.2557300900633397]
	TIME [epoch: 7.15 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1486387608479567		[learning rate: 0.0081658]
	Learning Rate: 0.00816582
	LOSS [training: 1.1486387608479567 | validation: 1.4124825831593004]
	TIME [epoch: 7.12 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5950848323734335		[learning rate: 0.0081466]
	Learning Rate: 0.00814656
	LOSS [training: 1.5950848323734335 | validation: 1.6338066724880584]
	TIME [epoch: 7.11 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.37451330803245		[learning rate: 0.0081273]
	Learning Rate: 0.00812735
	LOSS [training: 1.37451330803245 | validation: 1.2276088726528083]
	TIME [epoch: 7.13 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.174956032141955		[learning rate: 0.0081082]
	Learning Rate: 0.00810817
	LOSS [training: 1.174956032141955 | validation: 1.6998915514396002]
	TIME [epoch: 7.12 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.50572985126596		[learning rate: 0.008089]
	Learning Rate: 0.00808905
	LOSS [training: 1.50572985126596 | validation: 1.4617374382443757]
	TIME [epoch: 7.13 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6504356856014515		[learning rate: 0.00807]
	Learning Rate: 0.00806997
	LOSS [training: 1.6504356856014515 | validation: 1.8762195628976752]
	TIME [epoch: 7.16 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.249344670090167		[learning rate: 0.0080509]
	Learning Rate: 0.00805093
	LOSS [training: 1.249344670090167 | validation: 1.3394676167623851]
	TIME [epoch: 7.12 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.364624370896128		[learning rate: 0.0080319]
	Learning Rate: 0.00803194
	LOSS [training: 1.364624370896128 | validation: 1.1759878514696034]
	TIME [epoch: 7.12 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.434503153594023		[learning rate: 0.008013]
	Learning Rate: 0.00801299
	LOSS [training: 1.434503153594023 | validation: 1.5520037966018814]
	TIME [epoch: 7.12 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7201156738259455		[learning rate: 0.0079941]
	Learning Rate: 0.00799409
	LOSS [training: 1.7201156738259455 | validation: 1.7657234384901552]
	TIME [epoch: 7.12 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5401948695961056		[learning rate: 0.0079752]
	Learning Rate: 0.00797524
	LOSS [training: 1.5401948695961056 | validation: 1.994671542768308]
	TIME [epoch: 7.12 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5404385027052672		[learning rate: 0.0079564]
	Learning Rate: 0.00795642
	LOSS [training: 1.5404385027052672 | validation: 1.747463835900374]
	TIME [epoch: 7.16 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2221546263448029		[learning rate: 0.0079377]
	Learning Rate: 0.00793766
	LOSS [training: 1.2221546263448029 | validation: 1.7453899922350053]
	TIME [epoch: 7.12 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.278135747775083		[learning rate: 0.0079189]
	Learning Rate: 0.00791893
	LOSS [training: 1.278135747775083 | validation: 1.5324699171082536]
	TIME [epoch: 7.13 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4912632437241031		[learning rate: 0.0079003]
	Learning Rate: 0.00790025
	LOSS [training: 1.4912632437241031 | validation: 1.0375761264202286]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5153707266149368		[learning rate: 0.0078816]
	Learning Rate: 0.00788162
	LOSS [training: 1.5153707266149368 | validation: 1.6096699660183702]
	TIME [epoch: 7.13 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.665767768757495		[learning rate: 0.007863]
	Learning Rate: 0.00786303
	LOSS [training: 1.665767768757495 | validation: 2.692040267619901]
	TIME [epoch: 7.13 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6795405884573242		[learning rate: 0.0078445]
	Learning Rate: 0.00784448
	LOSS [training: 1.6795405884573242 | validation: 2.1159668006871497]
	TIME [epoch: 7.17 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.318853733152917		[learning rate: 0.007826]
	Learning Rate: 0.00782597
	LOSS [training: 1.318853733152917 | validation: 1.1514145907158886]
	TIME [epoch: 7.12 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5094422595412835		[learning rate: 0.0078075]
	Learning Rate: 0.00780751
	LOSS [training: 1.5094422595412835 | validation: 1.398230793185577]
	TIME [epoch: 7.12 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4009654464715064		[learning rate: 0.0077891]
	Learning Rate: 0.0077891
	LOSS [training: 1.4009654464715064 | validation: 1.9732698688704662]
	TIME [epoch: 7.12 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1567432656283563		[learning rate: 0.0077707]
	Learning Rate: 0.00777072
	LOSS [training: 2.1567432656283563 | validation: 2.2795969086859658]
	TIME [epoch: 7.13 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6576346996652163		[learning rate: 0.0077524]
	Learning Rate: 0.00775239
	LOSS [training: 1.6576346996652163 | validation: 1.3386571359926256]
	TIME [epoch: 7.12 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5741897305179053		[learning rate: 0.0077341]
	Learning Rate: 0.00773411
	LOSS [training: 1.5741897305179053 | validation: 1.1659757556647072]
	TIME [epoch: 7.17 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4898065767154407		[learning rate: 0.0077159]
	Learning Rate: 0.00771586
	LOSS [training: 1.4898065767154407 | validation: 1.522567315147041]
	TIME [epoch: 7.12 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.312976144701819		[learning rate: 0.0076977]
	Learning Rate: 0.00769766
	LOSS [training: 1.312976144701819 | validation: 1.3824081610914671]
	TIME [epoch: 7.13 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2125495981438354		[learning rate: 0.0076795]
	Learning Rate: 0.00767951
	LOSS [training: 1.2125495981438354 | validation: 3.900006803884514]
	TIME [epoch: 7.12 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9409079055892853		[learning rate: 0.0076614]
	Learning Rate: 0.00766139
	LOSS [training: 1.9409079055892853 | validation: 1.0361633193058353]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2458114766611135		[learning rate: 0.0076433]
	Learning Rate: 0.00764332
	LOSS [training: 1.2458114766611135 | validation: 2.524633923010365]
	TIME [epoch: 7.14 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8290364264621668		[learning rate: 0.0076253]
	Learning Rate: 0.00762529
	LOSS [training: 1.8290364264621668 | validation: 1.6170386540206318]
	TIME [epoch: 7.15 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5314286654056999		[learning rate: 0.0076073]
	Learning Rate: 0.0076073
	LOSS [training: 1.5314286654056999 | validation: 2.0161868894734507]
	TIME [epoch: 7.12 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4754233214400863		[learning rate: 0.0075894]
	Learning Rate: 0.00758936
	LOSS [training: 1.4754233214400863 | validation: 1.8546518547027944]
	TIME [epoch: 7.12 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7917588159095121		[learning rate: 0.0075715]
	Learning Rate: 0.00757146
	LOSS [training: 1.7917588159095121 | validation: 2.2521328297153786]
	TIME [epoch: 7.12 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.973465032082138		[learning rate: 0.0075536]
	Learning Rate: 0.0075536
	LOSS [training: 1.973465032082138 | validation: 2.445443104595584]
	TIME [epoch: 7.12 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8152255127624564		[learning rate: 0.0075358]
	Learning Rate: 0.00753578
	LOSS [training: 1.8152255127624564 | validation: 1.6584253947505143]
	TIME [epoch: 7.14 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6080263912621284		[learning rate: 0.007518]
	Learning Rate: 0.007518
	LOSS [training: 1.6080263912621284 | validation: 2.415653544823642]
	TIME [epoch: 7.15 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6243953752284752		[learning rate: 0.0075003]
	Learning Rate: 0.00750027
	LOSS [training: 1.6243953752284752 | validation: 1.176185924777318]
	TIME [epoch: 7.13 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2100807815704437		[learning rate: 0.0074826]
	Learning Rate: 0.00748258
	LOSS [training: 1.2100807815704437 | validation: 1.1734058311561744]
	TIME [epoch: 7.12 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.141969159247057		[learning rate: 0.0074649]
	Learning Rate: 0.00746493
	LOSS [training: 1.141969159247057 | validation: 3.0738866528122273]
	TIME [epoch: 7.13 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.959084672534179		[learning rate: 0.0074473]
	Learning Rate: 0.00744732
	LOSS [training: 1.959084672534179 | validation: 1.417607428265946]
	TIME [epoch: 7.12 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3280113877365127		[learning rate: 0.0074298]
	Learning Rate: 0.00742975
	LOSS [training: 1.3280113877365127 | validation: 1.478597979920976]
	TIME [epoch: 7.14 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4684227912347765		[learning rate: 0.0074122]
	Learning Rate: 0.00741223
	LOSS [training: 1.4684227912347765 | validation: 1.7319664654150386]
	TIME [epoch: 7.14 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3475992686575626		[learning rate: 0.0073947]
	Learning Rate: 0.00739474
	LOSS [training: 1.3475992686575626 | validation: 1.4250755151840062]
	TIME [epoch: 7.13 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1175990536892104		[learning rate: 0.0073773]
	Learning Rate: 0.0073773
	LOSS [training: 1.1175990536892104 | validation: 2.221712923552503]
	TIME [epoch: 7.12 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6228292228602244		[learning rate: 0.0073599]
	Learning Rate: 0.0073599
	LOSS [training: 1.6228292228602244 | validation: 1.1879393805973248]
	TIME [epoch: 7.13 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2422484420542381		[learning rate: 0.0073425]
	Learning Rate: 0.00734254
	LOSS [training: 1.2422484420542381 | validation: 1.0995919313948823]
	TIME [epoch: 7.13 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.952230308413776		[learning rate: 0.0073252]
	Learning Rate: 0.00732522
	LOSS [training: 1.952230308413776 | validation: 2.342504492208189]
	TIME [epoch: 7.14 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6057429841777477		[learning rate: 0.0073079]
	Learning Rate: 0.00730794
	LOSS [training: 1.6057429841777477 | validation: 1.060376104379451]
	TIME [epoch: 7.15 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3047313463767982		[learning rate: 0.0072907]
	Learning Rate: 0.0072907
	LOSS [training: 1.3047313463767982 | validation: 1.2088686804138113]
	TIME [epoch: 7.13 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.253260881623729		[learning rate: 0.0072735]
	Learning Rate: 0.0072735
	LOSS [training: 1.253260881623729 | validation: 0.9739795880156265]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0603446437774253		[learning rate: 0.0072563]
	Learning Rate: 0.00725635
	LOSS [training: 1.0603446437774253 | validation: 1.0269377590809108]
	TIME [epoch: 7.14 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.245746902039718		[learning rate: 0.0072392]
	Learning Rate: 0.00723923
	LOSS [training: 1.245746902039718 | validation: 1.0733453903013166]
	TIME [epoch: 7.13 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2040586582163093		[learning rate: 0.0072222]
	Learning Rate: 0.00722215
	LOSS [training: 1.2040586582163093 | validation: 1.4365658960239962]
	TIME [epoch: 7.14 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2298246999746194		[learning rate: 0.0072051]
	Learning Rate: 0.00720512
	LOSS [training: 1.2298246999746194 | validation: 2.776282274789187]
	TIME [epoch: 7.15 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5529010864095465		[learning rate: 0.0071881]
	Learning Rate: 0.00718812
	LOSS [training: 1.5529010864095465 | validation: 3.56360252788763]
	TIME [epoch: 7.14 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.08784398435064		[learning rate: 0.0071712]
	Learning Rate: 0.00717117
	LOSS [training: 2.08784398435064 | validation: 1.5942669500141062]
	TIME [epoch: 7.13 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8748172983914073		[learning rate: 0.0071542]
	Learning Rate: 0.00715425
	LOSS [training: 1.8748172983914073 | validation: 1.912139458579452]
	TIME [epoch: 7.13 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5115371596751543		[learning rate: 0.0071374]
	Learning Rate: 0.00713737
	LOSS [training: 1.5115371596751543 | validation: 1.3478977035290711]
	TIME [epoch: 7.13 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2333098519834553		[learning rate: 0.0071205]
	Learning Rate: 0.00712054
	LOSS [training: 1.2333098519834553 | validation: 1.441974386752285]
	TIME [epoch: 7.14 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5879667924045626		[learning rate: 0.0071037]
	Learning Rate: 0.00710374
	LOSS [training: 1.5879667924045626 | validation: 1.5635513121251052]
	TIME [epoch: 7.15 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5924800452643906		[learning rate: 0.007087]
	Learning Rate: 0.00708698
	LOSS [training: 1.5924800452643906 | validation: 1.2457567934513243]
	TIME [epoch: 7.12 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.349949578589218		[learning rate: 0.0070703]
	Learning Rate: 0.00707027
	LOSS [training: 1.349949578589218 | validation: 3.0156985230634223]
	TIME [epoch: 7.12 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.87777443885363		[learning rate: 0.0070536]
	Learning Rate: 0.00705359
	LOSS [training: 3.87777443885363 | validation: 2.436645795791139]
	TIME [epoch: 7.12 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757613411290966		[learning rate: 0.007037]
	Learning Rate: 0.00703695
	LOSS [training: 1.757613411290966 | validation: 1.806583934038002]
	TIME [epoch: 7.13 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.585834604063335		[learning rate: 0.0070204]
	Learning Rate: 0.00702035
	LOSS [training: 1.585834604063335 | validation: 1.9318403034700158]
	TIME [epoch: 7.13 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5054684753980943		[learning rate: 0.0070038]
	Learning Rate: 0.00700379
	LOSS [training: 1.5054684753980943 | validation: 1.66379766220949]
	TIME [epoch: 7.17 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6782666212692892		[learning rate: 0.0069873]
	Learning Rate: 0.00698727
	LOSS [training: 1.6782666212692892 | validation: 1.9264857540397944]
	TIME [epoch: 7.15 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.451516723178291		[learning rate: 0.0069708]
	Learning Rate: 0.00697079
	LOSS [training: 1.451516723178291 | validation: 1.8401587451187524]
	TIME [epoch: 7.15 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2570024949377931		[learning rate: 0.0069543]
	Learning Rate: 0.00695435
	LOSS [training: 1.2570024949377931 | validation: 1.2278748463728522]
	TIME [epoch: 7.13 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3636303998076247		[learning rate: 0.0069379]
	Learning Rate: 0.00693794
	LOSS [training: 1.3636303998076247 | validation: 1.2705479177204193]
	TIME [epoch: 7.12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.157725931061559		[learning rate: 0.0069216]
	Learning Rate: 0.00692158
	LOSS [training: 1.157725931061559 | validation: 2.021146460385315]
	TIME [epoch: 7.14 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4215622763694693		[learning rate: 0.0069053]
	Learning Rate: 0.00690525
	LOSS [training: 1.4215622763694693 | validation: 1.3142824006034042]
	TIME [epoch: 7.17 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2345427959180246		[learning rate: 0.006889]
	Learning Rate: 0.00688896
	LOSS [training: 1.2345427959180246 | validation: 1.5136121913793223]
	TIME [epoch: 7.13 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4496788792262714		[learning rate: 0.0068727]
	Learning Rate: 0.00687271
	LOSS [training: 1.4496788792262714 | validation: 1.6357669876136067]
	TIME [epoch: 7.13 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4274893206279677		[learning rate: 0.0068565]
	Learning Rate: 0.0068565
	LOSS [training: 1.4274893206279677 | validation: 1.8850591356135282]
	TIME [epoch: 7.13 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0296644812605473		[learning rate: 0.0068403]
	Learning Rate: 0.00684033
	LOSS [training: 2.0296644812605473 | validation: 2.3188154591887016]
	TIME [epoch: 7.13 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4698369384203176		[learning rate: 0.0068242]
	Learning Rate: 0.00682419
	LOSS [training: 1.4698369384203176 | validation: 1.373733632287069]
	TIME [epoch: 7.14 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1161995590873897		[learning rate: 0.0068081]
	Learning Rate: 0.0068081
	LOSS [training: 1.1161995590873897 | validation: 1.3564861232936307]
	TIME [epoch: 7.16 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2203217189848972		[learning rate: 0.006792]
	Learning Rate: 0.00679204
	LOSS [training: 1.2203217189848972 | validation: 1.1457531534532057]
	TIME [epoch: 7.12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1497246709243094		[learning rate: 0.006776]
	Learning Rate: 0.00677601
	LOSS [training: 1.1497246709243094 | validation: 1.581811327282033]
	TIME [epoch: 7.13 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8082040449698884		[learning rate: 0.00676]
	Learning Rate: 0.00676003
	LOSS [training: 1.8082040449698884 | validation: 2.715751143139949]
	TIME [epoch: 7.13 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6890239605787976		[learning rate: 0.0067441]
	Learning Rate: 0.00674409
	LOSS [training: 1.6890239605787976 | validation: 1.6009086637017176]
	TIME [epoch: 7.12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3549259811020948		[learning rate: 0.0067282]
	Learning Rate: 0.00672818
	LOSS [training: 1.3549259811020948 | validation: 2.675886528416995]
	TIME [epoch: 7.14 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.075614819458058		[learning rate: 0.0067123]
	Learning Rate: 0.00671231
	LOSS [training: 2.075614819458058 | validation: 1.5832677044152819]
	TIME [epoch: 7.16 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2429267637778973		[learning rate: 0.0066965]
	Learning Rate: 0.00669647
	LOSS [training: 1.2429267637778973 | validation: 1.1212095818878145]
	TIME [epoch: 7.13 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.858043365603919		[learning rate: 0.0066807]
	Learning Rate: 0.00668068
	LOSS [training: 1.858043365603919 | validation: 1.8115789757752907]
	TIME [epoch: 7.12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.374937090078782		[learning rate: 0.0066649]
	Learning Rate: 0.00666492
	LOSS [training: 2.374937090078782 | validation: 1.508254223910134]
	TIME [epoch: 7.13 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.587579435385802		[learning rate: 0.0066492]
	Learning Rate: 0.0066492
	LOSS [training: 1.587579435385802 | validation: 2.2453361649638732]
	TIME [epoch: 7.15 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.735369384769739		[learning rate: 0.0066335]
	Learning Rate: 0.00663351
	LOSS [training: 1.735369384769739 | validation: 1.4970621674548454]
	TIME [epoch: 7.15 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.46055200666152		[learning rate: 0.0066179]
	Learning Rate: 0.00661787
	LOSS [training: 1.46055200666152 | validation: 1.5541678957170881]
	TIME [epoch: 7.15 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2859050685707616		[learning rate: 0.0066023]
	Learning Rate: 0.00660226
	LOSS [training: 1.2859050685707616 | validation: 1.447898460870273]
	TIME [epoch: 7.13 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8021457049184004		[learning rate: 0.0065867]
	Learning Rate: 0.00658668
	LOSS [training: 1.8021457049184004 | validation: 1.2726099705644451]
	TIME [epoch: 7.13 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4911518001160884		[learning rate: 0.0065711]
	Learning Rate: 0.00657114
	LOSS [training: 2.4911518001160884 | validation: 2.8631272785831334]
	TIME [epoch: 7.13 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8532628784758856		[learning rate: 0.0065556]
	Learning Rate: 0.00655564
	LOSS [training: 1.8532628784758856 | validation: 1.3125924402023226]
	TIME [epoch: 7.13 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1484700450971699		[learning rate: 0.0065402]
	Learning Rate: 0.00654018
	LOSS [training: 1.1484700450971699 | validation: 1.0012216717935936]
	TIME [epoch: 7.14 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0895892213599059		[learning rate: 0.0065248]
	Learning Rate: 0.00652475
	LOSS [training: 1.0895892213599059 | validation: 1.1599459816061222]
	TIME [epoch: 7.15 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1552902809600418		[learning rate: 0.0065094]
	Learning Rate: 0.00650936
	LOSS [training: 1.1552902809600418 | validation: 1.2574956958168486]
	TIME [epoch: 7.13 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2500333437494104		[learning rate: 0.006494]
	Learning Rate: 0.00649401
	LOSS [training: 1.2500333437494104 | validation: 1.9784949031306844]
	TIME [epoch: 7.13 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3146744457161517		[learning rate: 0.0064787]
	Learning Rate: 0.00647869
	LOSS [training: 1.3146744457161517 | validation: 1.1031497569197821]
	TIME [epoch: 7.13 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3592362312784652		[learning rate: 0.0064634]
	Learning Rate: 0.00646341
	LOSS [training: 1.3592362312784652 | validation: 1.7041660408379729]
	TIME [epoch: 7.13 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9967559670307726		[learning rate: 0.0064482]
	Learning Rate: 0.00644816
	LOSS [training: 1.9967559670307726 | validation: 1.14665477717384]
	TIME [epoch: 7.13 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3227905820120622		[learning rate: 0.006433]
	Learning Rate: 0.00643295
	LOSS [training: 1.3227905820120622 | validation: 1.8102304995964933]
	TIME [epoch: 7.17 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3704654508646728		[learning rate: 0.0064178]
	Learning Rate: 0.00641778
	LOSS [training: 1.3704654508646728 | validation: 1.3793697652628631]
	TIME [epoch: 7.12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2024486389785387		[learning rate: 0.0064026]
	Learning Rate: 0.00640264
	LOSS [training: 1.2024486389785387 | validation: 1.612066630395205]
	TIME [epoch: 7.14 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.394304457537204		[learning rate: 0.0063875]
	Learning Rate: 0.00638754
	LOSS [training: 1.394304457537204 | validation: 1.0924817416861075]
	TIME [epoch: 7.13 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9624471523450087		[learning rate: 0.0063725]
	Learning Rate: 0.00637247
	LOSS [training: 0.9624471523450087 | validation: 1.3944481899247205]
	TIME [epoch: 7.14 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9924081891847086		[learning rate: 0.0063574]
	Learning Rate: 0.00635744
	LOSS [training: 0.9924081891847086 | validation: 2.1988705885386315]
	TIME [epoch: 7.15 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5220303695144561		[learning rate: 0.0063424]
	Learning Rate: 0.00634244
	LOSS [training: 1.5220303695144561 | validation: 1.0977277679325201]
	TIME [epoch: 7.16 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3497225278197773		[learning rate: 0.0063275]
	Learning Rate: 0.00632748
	LOSS [training: 1.3497225278197773 | validation: 1.19237535192787]
	TIME [epoch: 7.12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2405068664216097		[learning rate: 0.0063126]
	Learning Rate: 0.00631255
	LOSS [training: 1.2405068664216097 | validation: 1.165839308711225]
	TIME [epoch: 7.13 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0346954333514473		[learning rate: 0.0062977]
	Learning Rate: 0.00629766
	LOSS [training: 1.0346954333514473 | validation: 0.9585154294361015]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1081443331383212		[learning rate: 0.0062828]
	Learning Rate: 0.00628281
	LOSS [training: 1.1081443331383212 | validation: 1.269631262326528]
	TIME [epoch: 7.13 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9943026072445026		[learning rate: 0.006268]
	Learning Rate: 0.00626799
	LOSS [training: 0.9943026072445026 | validation: 0.9092077281190677]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9132555440598935		[learning rate: 0.0062532]
	Learning Rate: 0.0062532
	LOSS [training: 0.9132555440598935 | validation: 1.9650616317072607]
	TIME [epoch: 7.15 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1957829814144134		[learning rate: 0.0062385]
	Learning Rate: 0.00623845
	LOSS [training: 1.1957829814144134 | validation: 1.0761120878287387]
	TIME [epoch: 7.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0723933740528668		[learning rate: 0.0062237]
	Learning Rate: 0.00622374
	LOSS [training: 1.0723933740528668 | validation: 0.9253985914743998]
	TIME [epoch: 7.13 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.088338581064208		[learning rate: 0.0062091]
	Learning Rate: 0.00620906
	LOSS [training: 1.088338581064208 | validation: 1.5808265706515185]
	TIME [epoch: 7.12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3131325078050844		[learning rate: 0.0061944]
	Learning Rate: 0.00619441
	LOSS [training: 1.3131325078050844 | validation: 1.7707051182399383]
	TIME [epoch: 7.13 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2919907391883416		[learning rate: 0.0061798]
	Learning Rate: 0.0061798
	LOSS [training: 1.2919907391883416 | validation: 1.8821636836909363]
	TIME [epoch: 7.15 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3471686922697192		[learning rate: 0.0061652]
	Learning Rate: 0.00616522
	LOSS [training: 1.3471686922697192 | validation: 0.9805135877344595]
	TIME [epoch: 7.14 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0154853522814087		[learning rate: 0.0061507]
	Learning Rate: 0.00615068
	LOSS [training: 1.0154853522814087 | validation: 1.5262687130210937]
	TIME [epoch: 7.12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1285179104545076		[learning rate: 0.0061362]
	Learning Rate: 0.00613617
	LOSS [training: 1.1285179104545076 | validation: 1.4595967625367252]
	TIME [epoch: 7.13 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0728983336628442		[learning rate: 0.0061217]
	Learning Rate: 0.0061217
	LOSS [training: 1.0728983336628442 | validation: 1.5896353164114487]
	TIME [epoch: 7.12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0941720480499222		[learning rate: 0.0061073]
	Learning Rate: 0.00610726
	LOSS [training: 1.0941720480499222 | validation: 1.8297724766933814]
	TIME [epoch: 7.12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2600622859088055		[learning rate: 0.0060929]
	Learning Rate: 0.00609285
	LOSS [training: 1.2600622859088055 | validation: 1.0245298387411483]
	TIME [epoch: 7.16 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2841522111029788		[learning rate: 0.0060785]
	Learning Rate: 0.00607848
	LOSS [training: 1.2841522111029788 | validation: 1.2812787121236284]
	TIME [epoch: 7.14 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1744389429556517		[learning rate: 0.0060641]
	Learning Rate: 0.00606414
	LOSS [training: 1.1744389429556517 | validation: 2.38654900957485]
	TIME [epoch: 7.12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5655326969867729		[learning rate: 0.0060498]
	Learning Rate: 0.00604984
	LOSS [training: 1.5655326969867729 | validation: 1.1760519795983397]
	TIME [epoch: 7.12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0044914329116026		[learning rate: 0.0060356]
	Learning Rate: 0.00603556
	LOSS [training: 1.0044914329116026 | validation: 1.243265396347352]
	TIME [epoch: 7.13 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0528214128332984		[learning rate: 0.0060213]
	Learning Rate: 0.00602133
	LOSS [training: 2.0528214128332984 | validation: 2.754551728505461]
	TIME [epoch: 7.12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5921947459821024		[learning rate: 0.0060071]
	Learning Rate: 0.00600712
	LOSS [training: 1.5921947459821024 | validation: 1.5277514058624073]
	TIME [epoch: 7.16 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1202913860743147		[learning rate: 0.005993]
	Learning Rate: 0.00599296
	LOSS [training: 1.1202913860743147 | validation: 1.0381625420191252]
	TIME [epoch: 7.13 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0251628951289617		[learning rate: 0.0059788]
	Learning Rate: 0.00597882
	LOSS [training: 1.0251628951289617 | validation: 1.1305638331429595]
	TIME [epoch: 7.12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.070372716503605		[learning rate: 0.0059647]
	Learning Rate: 0.00596472
	LOSS [training: 1.070372716503605 | validation: 1.216040895835513]
	TIME [epoch: 7.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4781705871925566		[learning rate: 0.0059506]
	Learning Rate: 0.00595065
	LOSS [training: 1.4781705871925566 | validation: 1.2863086577779508]
	TIME [epoch: 7.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3770652155589522		[learning rate: 0.0059366]
	Learning Rate: 0.00593661
	LOSS [training: 1.3770652155589522 | validation: 1.4887943533459165]
	TIME [epoch: 7.12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.336398429130178		[learning rate: 0.0059226]
	Learning Rate: 0.00592261
	LOSS [training: 1.336398429130178 | validation: 1.2759664631368297]
	TIME [epoch: 7.16 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0704532120396972		[learning rate: 0.0059086]
	Learning Rate: 0.00590863
	LOSS [training: 1.0704532120396972 | validation: 1.0530183552532844]
	TIME [epoch: 7.14 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.115192428434929		[learning rate: 0.0058947]
	Learning Rate: 0.0058947
	LOSS [training: 1.115192428434929 | validation: 1.0930506412670347]
	TIME [epoch: 7.13 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0318433299892824		[learning rate: 0.0058808]
	Learning Rate: 0.00588079
	LOSS [training: 1.0318433299892824 | validation: 0.9104462102722951]
	TIME [epoch: 7.13 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9523677080536753		[learning rate: 0.0058669]
	Learning Rate: 0.00586692
	LOSS [training: 0.9523677080536753 | validation: 0.9283990520204841]
	TIME [epoch: 7.13 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8807266411328418		[learning rate: 0.0058531]
	Learning Rate: 0.00585308
	LOSS [training: 0.8807266411328418 | validation: 0.8833108003658566]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.405780232635861		[learning rate: 0.0058393]
	Learning Rate: 0.00583928
	LOSS [training: 1.405780232635861 | validation: 1.2053219704669438]
	TIME [epoch: 7.16 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.267215196600867		[learning rate: 0.0058255]
	Learning Rate: 0.0058255
	LOSS [training: 1.267215196600867 | validation: 1.4945865024602019]
	TIME [epoch: 7.13 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1856534634070073		[learning rate: 0.0058118]
	Learning Rate: 0.00581176
	LOSS [training: 1.1856534634070073 | validation: 0.9327716713458509]
	TIME [epoch: 7.12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.044697384695976		[learning rate: 0.0057981]
	Learning Rate: 0.00579805
	LOSS [training: 1.044697384695976 | validation: 1.322278173477346]
	TIME [epoch: 7.13 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2654260665208747		[learning rate: 0.0057844]
	Learning Rate: 0.00578438
	LOSS [training: 1.2654260665208747 | validation: 1.1358822886519826]
	TIME [epoch: 7.12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.056215892508828		[learning rate: 0.0057707]
	Learning Rate: 0.00577073
	LOSS [training: 1.056215892508828 | validation: 1.7103179908628805]
	TIME [epoch: 7.12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1799457347173052		[learning rate: 0.0057571]
	Learning Rate: 0.00575712
	LOSS [training: 1.1799457347173052 | validation: 0.9849465567937199]
	TIME [epoch: 7.15 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0182810044990827		[learning rate: 0.0057435]
	Learning Rate: 0.00574354
	LOSS [training: 1.0182810044990827 | validation: 1.3949272887858317]
	TIME [epoch: 7.13 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.18455256862296		[learning rate: 0.00573]
	Learning Rate: 0.00572999
	LOSS [training: 1.18455256862296 | validation: 0.8745986077325728]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0345813635254733		[learning rate: 0.0057165]
	Learning Rate: 0.00571647
	LOSS [training: 1.0345813635254733 | validation: 1.023621132886471]
	TIME [epoch: 7.14 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2711914891157823		[learning rate: 0.005703]
	Learning Rate: 0.00570299
	LOSS [training: 1.2711914891157823 | validation: 1.1933382121391163]
	TIME [epoch: 7.13 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.013681475138192		[learning rate: 0.0056895]
	Learning Rate: 0.00568954
	LOSS [training: 1.013681475138192 | validation: 0.8510602655468731]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2593578082688883		[learning rate: 0.0056761]
	Learning Rate: 0.00567612
	LOSS [training: 1.2593578082688883 | validation: 0.9515729747520707]
	TIME [epoch: 7.17 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0233046368468195		[learning rate: 0.0056627]
	Learning Rate: 0.00566273
	LOSS [training: 1.0233046368468195 | validation: 0.9997115357073041]
	TIME [epoch: 7.14 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.201583095087151		[learning rate: 0.0056494]
	Learning Rate: 0.00564937
	LOSS [training: 1.201583095087151 | validation: 0.8703058438942775]
	TIME [epoch: 7.13 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.089076714566709		[learning rate: 0.005636]
	Learning Rate: 0.00563604
	LOSS [training: 1.089076714566709 | validation: 1.5424326898677247]
	TIME [epoch: 7.13 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.106951822726409		[learning rate: 0.0056227]
	Learning Rate: 0.00562275
	LOSS [training: 1.106951822726409 | validation: 1.1118460762179652]
	TIME [epoch: 7.13 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2541332405063097		[learning rate: 0.0056095]
	Learning Rate: 0.00560949
	LOSS [training: 1.2541332405063097 | validation: 1.112917676803523]
	TIME [epoch: 7.13 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0988550307924687		[learning rate: 0.0055963]
	Learning Rate: 0.00559625
	LOSS [training: 1.0988550307924687 | validation: 1.077258590643997]
	TIME [epoch: 7.16 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9095142451406598		[learning rate: 0.0055831]
	Learning Rate: 0.00558305
	LOSS [training: 0.9095142451406598 | validation: 1.432150648386063]
	TIME [epoch: 7.14 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0634658411840623		[learning rate: 0.0055699]
	Learning Rate: 0.00556988
	LOSS [training: 1.0634658411840623 | validation: 1.1217966192372888]
	TIME [epoch: 7.13 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3008904612346963		[learning rate: 0.0055567]
	Learning Rate: 0.00555674
	LOSS [training: 1.3008904612346963 | validation: 2.6842611416977364]
	TIME [epoch: 7.13 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.582364947836386		[learning rate: 0.0055436]
	Learning Rate: 0.00554364
	LOSS [training: 1.582364947836386 | validation: 2.1219109885566967]
	TIME [epoch: 7.13 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1969981908727045		[learning rate: 0.0055306]
	Learning Rate: 0.00553056
	LOSS [training: 1.1969981908727045 | validation: 1.006936326142902]
	TIME [epoch: 7.13 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1309409623056472		[learning rate: 0.0055175]
	Learning Rate: 0.00551752
	LOSS [training: 1.1309409623056472 | validation: 0.9459252851849502]
	TIME [epoch: 7.16 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1013642461925996		[learning rate: 0.0055045]
	Learning Rate: 0.0055045
	LOSS [training: 1.1013642461925996 | validation: 1.3039728986185999]
	TIME [epoch: 7.13 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2646271823107427		[learning rate: 0.0054915]
	Learning Rate: 0.00549152
	LOSS [training: 1.2646271823107427 | validation: 0.8887082165969333]
	TIME [epoch: 7.12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8060879880955053		[learning rate: 0.0054786]
	Learning Rate: 0.00547856
	LOSS [training: 0.8060879880955053 | validation: 0.883641915804573]
	TIME [epoch: 7.13 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7955584695097573		[learning rate: 0.0054656]
	Learning Rate: 0.00546564
	LOSS [training: 0.7955584695097573 | validation: 0.754027689257432]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9967202931014958		[learning rate: 0.0054527]
	Learning Rate: 0.00545275
	LOSS [training: 0.9967202931014958 | validation: 1.3953194860228693]
	TIME [epoch: 7.15 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9529869364141998		[learning rate: 0.0054399]
	Learning Rate: 0.00543988
	LOSS [training: 0.9529869364141998 | validation: 2.109163844089934]
	TIME [epoch: 7.16 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1989016667978207		[learning rate: 0.0054271]
	Learning Rate: 0.00542705
	LOSS [training: 1.1989016667978207 | validation: 1.040970623348322]
	TIME [epoch: 7.14 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.932486623905738		[learning rate: 0.0054143]
	Learning Rate: 0.00541425
	LOSS [training: 0.932486623905738 | validation: 1.0008573320179892]
	TIME [epoch: 7.13 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3567911751590571		[learning rate: 0.0054015]
	Learning Rate: 0.00540148
	LOSS [training: 1.3567911751590571 | validation: 1.1359579029722209]
	TIME [epoch: 7.13 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0643533347624277		[learning rate: 0.0053887]
	Learning Rate: 0.00538874
	LOSS [training: 1.0643533347624277 | validation: 1.0410155698651327]
	TIME [epoch: 7.13 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0884511717646528		[learning rate: 0.005376]
	Learning Rate: 0.00537603
	LOSS [training: 1.0884511717646528 | validation: 1.2575527688063401]
	TIME [epoch: 7.13 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.187079306264393		[learning rate: 0.0053633]
	Learning Rate: 0.00536335
	LOSS [training: 1.187079306264393 | validation: 1.5483845102916782]
	TIME [epoch: 7.17 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2592139058477596		[learning rate: 0.0053507]
	Learning Rate: 0.00535069
	LOSS [training: 1.2592139058477596 | validation: 1.2801422805712641]
	TIME [epoch: 7.14 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1360096809955		[learning rate: 0.0053381]
	Learning Rate: 0.00533807
	LOSS [training: 1.1360096809955 | validation: 1.2261999935973127]
	TIME [epoch: 7.14 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8990236758799621		[learning rate: 0.0053255]
	Learning Rate: 0.00532548
	LOSS [training: 0.8990236758799621 | validation: 0.7555663951663092]
	TIME [epoch: 7.13 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8841524311957508		[learning rate: 0.0053129]
	Learning Rate: 0.00531292
	LOSS [training: 0.8841524311957508 | validation: 0.8881609783491873]
	TIME [epoch: 7.13 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9450714030273268		[learning rate: 0.0053004]
	Learning Rate: 0.00530039
	LOSS [training: 0.9450714030273268 | validation: 1.3674949332082516]
	TIME [epoch: 7.14 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0459929651841502		[learning rate: 0.0052879]
	Learning Rate: 0.00528789
	LOSS [training: 1.0459929651841502 | validation: 1.0536944926072143]
	TIME [epoch: 7.17 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9786984933933336		[learning rate: 0.0052754]
	Learning Rate: 0.00527541
	LOSS [training: 0.9786984933933336 | validation: 3.971105513250553]
	TIME [epoch: 7.14 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2216574346551043		[learning rate: 0.005263]
	Learning Rate: 0.00526297
	LOSS [training: 2.2216574346551043 | validation: 0.9103530886162895]
	TIME [epoch: 7.14 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8297587255160177		[learning rate: 0.0052506]
	Learning Rate: 0.00525055
	LOSS [training: 0.8297587255160177 | validation: 0.8396350508395413]
	TIME [epoch: 7.14 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4419017137535879		[learning rate: 0.0052382]
	Learning Rate: 0.00523817
	LOSS [training: 1.4419017137535879 | validation: 2.4492717437793248]
	TIME [epoch: 7.14 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3520427985116925		[learning rate: 0.0052258]
	Learning Rate: 0.00522581
	LOSS [training: 1.3520427985116925 | validation: 0.8014879763312919]
	TIME [epoch: 7.14 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.062076254414587		[learning rate: 0.0052135]
	Learning Rate: 0.00521349
	LOSS [training: 1.062076254414587 | validation: 1.1982241284872306]
	TIME [epoch: 7.17 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8878311486995513		[learning rate: 0.0052012]
	Learning Rate: 0.00520119
	LOSS [training: 0.8878311486995513 | validation: 1.4916937478415315]
	TIME [epoch: 7.14 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.234050507273109		[learning rate: 0.0051889]
	Learning Rate: 0.00518892
	LOSS [training: 1.234050507273109 | validation: 0.916657509935104]
	TIME [epoch: 7.14 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8394598818080485		[learning rate: 0.0051767]
	Learning Rate: 0.00517668
	LOSS [training: 0.8394598818080485 | validation: 0.7763238987548255]
	TIME [epoch: 7.15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0928406960753192		[learning rate: 0.0051645]
	Learning Rate: 0.00516447
	LOSS [training: 1.0928406960753192 | validation: 0.9203448913365542]
	TIME [epoch: 7.13 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1522952053921935		[learning rate: 0.0051523]
	Learning Rate: 0.00515229
	LOSS [training: 2.1522952053921935 | validation: 1.8911721964998467]
	TIME [epoch: 7.14 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.544184655639061		[learning rate: 0.0051401]
	Learning Rate: 0.00514013
	LOSS [training: 1.544184655639061 | validation: 0.9433723051729219]
	TIME [epoch: 7.17 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9043344028023655		[learning rate: 0.005128]
	Learning Rate: 0.00512801
	LOSS [training: 0.9043344028023655 | validation: 1.2152827283296]
	TIME [epoch: 7.15 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9718787665139921		[learning rate: 0.0051159]
	Learning Rate: 0.00511591
	LOSS [training: 0.9718787665139921 | validation: 0.8877605749401978]
	TIME [epoch: 7.13 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1115390129428162		[learning rate: 0.0051038]
	Learning Rate: 0.00510384
	LOSS [training: 1.1115390129428162 | validation: 1.1744965513817807]
	TIME [epoch: 7.14 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3755286671813478		[learning rate: 0.0050918]
	Learning Rate: 0.00509181
	LOSS [training: 1.3755286671813478 | validation: 1.3211246985065146]
	TIME [epoch: 7.13 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9741518366829898		[learning rate: 0.0050798]
	Learning Rate: 0.00507979
	LOSS [training: 0.9741518366829898 | validation: 0.9599907927230762]
	TIME [epoch: 7.15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8104793278484882		[learning rate: 0.0050678]
	Learning Rate: 0.00506781
	LOSS [training: 0.8104793278484882 | validation: 1.2671480764860208]
	TIME [epoch: 7.17 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.148320493132402		[learning rate: 0.0050559]
	Learning Rate: 0.00505586
	LOSS [training: 1.148320493132402 | validation: 0.8050434732209892]
	TIME [epoch: 7.14 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8517071287259135		[learning rate: 0.0050439]
	Learning Rate: 0.00504393
	LOSS [training: 0.8517071287259135 | validation: 4.1462178754802785]
	TIME [epoch: 7.13 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.30887333982474		[learning rate: 0.005032]
	Learning Rate: 0.00503203
	LOSS [training: 2.30887333982474 | validation: 0.9696107163833672]
	TIME [epoch: 7.15 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2050586133892904		[learning rate: 0.0050202]
	Learning Rate: 0.00502016
	LOSS [training: 1.2050586133892904 | validation: 1.595476442691011]
	TIME [epoch: 7.13 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0874335870299394		[learning rate: 0.0050083]
	Learning Rate: 0.00500832
	LOSS [training: 1.0874335870299394 | validation: 1.9191308880594224]
	TIME [epoch: 7.14 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1415010967645522		[learning rate: 0.0049965]
	Learning Rate: 0.00499651
	LOSS [training: 1.1415010967645522 | validation: 0.9563700204612671]
	TIME [epoch: 7.17 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3796059156999505		[learning rate: 0.0049847]
	Learning Rate: 0.00498472
	LOSS [training: 1.3796059156999505 | validation: 0.9091583755801131]
	TIME [epoch: 7.14 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9484049764766935		[learning rate: 0.004973]
	Learning Rate: 0.00497296
	LOSS [training: 0.9484049764766935 | validation: 1.9757878960373139]
	TIME [epoch: 7.14 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5200001573696755		[learning rate: 0.0049612]
	Learning Rate: 0.00496123
	LOSS [training: 1.5200001573696755 | validation: 0.821015815664947]
	TIME [epoch: 7.14 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0472196717453706		[learning rate: 0.0049495]
	Learning Rate: 0.00494953
	LOSS [training: 1.0472196717453706 | validation: 1.0578826877056244]
	TIME [epoch: 7.13 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8372110940763638		[learning rate: 0.0049379]
	Learning Rate: 0.00493786
	LOSS [training: 0.8372110940763638 | validation: 0.9088281013145536]
	TIME [epoch: 7.15 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8391642857440587		[learning rate: 0.0049262]
	Learning Rate: 0.00492621
	LOSS [training: 0.8391642857440587 | validation: 1.815159668674465]
	TIME [epoch: 7.16 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1393959048773927		[learning rate: 0.0049146]
	Learning Rate: 0.00491459
	LOSS [training: 1.1393959048773927 | validation: 1.090618467327749]
	TIME [epoch: 7.14 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5309498645833326		[learning rate: 0.004903]
	Learning Rate: 0.004903
	LOSS [training: 1.5309498645833326 | validation: 1.6257736066106236]
	TIME [epoch: 7.13 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6926203849289543		[learning rate: 0.0048914]
	Learning Rate: 0.00489143
	LOSS [training: 1.6926203849289543 | validation: 1.5082692681382073]
	TIME [epoch: 7.14 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9695420580077249		[learning rate: 0.0048799]
	Learning Rate: 0.00487989
	LOSS [training: 0.9695420580077249 | validation: 0.8413669209046046]
	TIME [epoch: 7.13 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8222496802637626		[learning rate: 0.0048684]
	Learning Rate: 0.00486838
	LOSS [training: 0.8222496802637626 | validation: 2.092804329101512]
	TIME [epoch: 7.14 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4239522229006996		[learning rate: 0.0048569]
	Learning Rate: 0.0048569
	LOSS [training: 1.4239522229006996 | validation: 1.2502724420204543]
	TIME [epoch: 7.17 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.005108798931266		[learning rate: 0.0048454]
	Learning Rate: 0.00484544
	LOSS [training: 1.005108798931266 | validation: 0.91860337403571]
	TIME [epoch: 7.14 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.085174970900289		[learning rate: 0.004834]
	Learning Rate: 0.00483401
	LOSS [training: 1.085174970900289 | validation: 1.1035359033551435]
	TIME [epoch: 7.14 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9355753028010613		[learning rate: 0.0048226]
	Learning Rate: 0.00482261
	LOSS [training: 0.9355753028010613 | validation: 1.4239484022737283]
	TIME [epoch: 7.14 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1217027842050382		[learning rate: 0.0048112]
	Learning Rate: 0.00481123
	LOSS [training: 1.1217027842050382 | validation: 1.1207996163505047]
	TIME [epoch: 7.14 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0457418664349833		[learning rate: 0.0047999]
	Learning Rate: 0.00479988
	LOSS [training: 1.0457418664349833 | validation: 1.0751299683647435]
	TIME [epoch: 7.15 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0102134712566717		[learning rate: 0.0047886]
	Learning Rate: 0.00478856
	LOSS [training: 1.0102134712566717 | validation: 0.6929937728717603]
	TIME [epoch: 7.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9013595768398505		[learning rate: 0.0047773]
	Learning Rate: 0.00477727
	LOSS [training: 0.9013595768398505 | validation: 1.1716910398658422]
	TIME [epoch: 7.14 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.036266582360729		[learning rate: 0.004766]
	Learning Rate: 0.004766
	LOSS [training: 1.036266582360729 | validation: 1.3789342315826314]
	TIME [epoch: 7.13 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0675739808608342		[learning rate: 0.0047548]
	Learning Rate: 0.00475476
	LOSS [training: 1.0675739808608342 | validation: 1.3332658393286336]
	TIME [epoch: 7.13 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0917302320399678		[learning rate: 0.0047435]
	Learning Rate: 0.00474354
	LOSS [training: 1.0917302320399678 | validation: 0.8830481641395185]
	TIME [epoch: 7.13 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7751244043784609		[learning rate: 0.0047324]
	Learning Rate: 0.00473235
	LOSS [training: 0.7751244043784609 | validation: 0.7109824791571677]
	TIME [epoch: 7.14 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708411526409398		[learning rate: 0.0047212]
	Learning Rate: 0.00472119
	LOSS [training: 0.708411526409398 | validation: 1.1501192772255724]
	TIME [epoch: 7.16 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9233753568183236		[learning rate: 0.0047101]
	Learning Rate: 0.00471005
	LOSS [training: 0.9233753568183236 | validation: 0.7199137011295761]
	TIME [epoch: 7.13 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8253848911030746		[learning rate: 0.0046989]
	Learning Rate: 0.00469894
	LOSS [training: 0.8253848911030746 | validation: 1.247804443462846]
	TIME [epoch: 7.14 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0013513082294678		[learning rate: 0.0046879]
	Learning Rate: 0.00468786
	LOSS [training: 1.0013513082294678 | validation: 0.6631623387418315]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9739230812555928		[learning rate: 0.0046768]
	Learning Rate: 0.0046768
	LOSS [training: 0.9739230812555928 | validation: 0.8295126863152379]
	TIME [epoch: 7.14 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0386713934163994		[learning rate: 0.0046658]
	Learning Rate: 0.00466577
	LOSS [training: 1.0386713934163994 | validation: 1.225231057686877]
	TIME [epoch: 7.15 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8701640393965141		[learning rate: 0.0046548]
	Learning Rate: 0.00465476
	LOSS [training: 0.8701640393965141 | validation: 1.229412201065926]
	TIME [epoch: 7.16 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7608628649560211		[learning rate: 0.0046438]
	Learning Rate: 0.00464378
	LOSS [training: 0.7608628649560211 | validation: 1.0295848171282913]
	TIME [epoch: 7.14 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0254645404877012		[learning rate: 0.0046328]
	Learning Rate: 0.00463283
	LOSS [training: 1.0254645404877012 | validation: 0.9800266435919478]
	TIME [epoch: 7.14 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.12401206155877		[learning rate: 0.0046219]
	Learning Rate: 0.0046219
	LOSS [training: 1.12401206155877 | validation: 1.3694589575540057]
	TIME [epoch: 7.13 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8617677693591077		[learning rate: 0.004611]
	Learning Rate: 0.004611
	LOSS [training: 0.8617677693591077 | validation: 0.6349237067326363]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6561578427353172		[learning rate: 0.0046001]
	Learning Rate: 0.00460012
	LOSS [training: 0.6561578427353172 | validation: 1.860554372146134]
	TIME [epoch: 7.17 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2590473604608434		[learning rate: 0.0045893]
	Learning Rate: 0.00458927
	LOSS [training: 1.2590473604608434 | validation: 1.3943388551245879]
	TIME [epoch: 7.14 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9956302927951747		[learning rate: 0.0045784]
	Learning Rate: 0.00457844
	LOSS [training: 0.9956302927951747 | validation: 0.5873153717924136]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.742023341131405		[learning rate: 0.0045676]
	Learning Rate: 0.00456765
	LOSS [training: 0.742023341131405 | validation: 0.7170064565936894]
	TIME [epoch: 7.13 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6865090131504334		[learning rate: 0.0045569]
	Learning Rate: 0.00455687
	LOSS [training: 0.6865090131504334 | validation: 0.8933774458288632]
	TIME [epoch: 7.13 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9030484908489088		[learning rate: 0.0045461]
	Learning Rate: 0.00454612
	LOSS [training: 0.9030484908489088 | validation: 1.3617003240416192]
	TIME [epoch: 7.12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.960560709488892		[learning rate: 0.0045354]
	Learning Rate: 0.0045354
	LOSS [training: 0.960560709488892 | validation: 0.9091078870164497]
	TIME [epoch: 7.16 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9762176719628244		[learning rate: 0.0045247]
	Learning Rate: 0.0045247
	LOSS [training: 0.9762176719628244 | validation: 0.7367858487635454]
	TIME [epoch: 7.13 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1871738227032367		[learning rate: 0.004514]
	Learning Rate: 0.00451403
	LOSS [training: 1.1871738227032367 | validation: 1.7118501935074697]
	TIME [epoch: 7.13 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1338682092494912		[learning rate: 0.0045034]
	Learning Rate: 0.00450338
	LOSS [training: 1.1338682092494912 | validation: 1.5112502269639214]
	TIME [epoch: 7.12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2225005952049421		[learning rate: 0.0044928]
	Learning Rate: 0.00449276
	LOSS [training: 1.2225005952049421 | validation: 1.0078639331795674]
	TIME [epoch: 7.13 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.840414810048405		[learning rate: 0.0044822]
	Learning Rate: 0.00448216
	LOSS [training: 0.840414810048405 | validation: 0.8046889837202422]
	TIME [epoch: 7.12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6563950576250902		[learning rate: 0.0044716]
	Learning Rate: 0.00447159
	LOSS [training: 0.6563950576250902 | validation: 1.6730281662883635]
	TIME [epoch: 7.16 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0938233630082663		[learning rate: 0.004461]
	Learning Rate: 0.00446104
	LOSS [training: 1.0938233630082663 | validation: 0.8134443576677205]
	TIME [epoch: 7.13 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8040781714388607		[learning rate: 0.0044505]
	Learning Rate: 0.00445051
	LOSS [training: 0.8040781714388607 | validation: 0.6615927601003435]
	TIME [epoch: 7.13 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8234373998940843		[learning rate: 0.00444]
	Learning Rate: 0.00444002
	LOSS [training: 0.8234373998940843 | validation: 0.6684061291372643]
	TIME [epoch: 7.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1724511385594663		[learning rate: 0.0044295]
	Learning Rate: 0.00442954
	LOSS [training: 1.1724511385594663 | validation: 0.8020840069287711]
	TIME [epoch: 7.12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.022091380108167		[learning rate: 0.0044191]
	Learning Rate: 0.00441909
	LOSS [training: 1.022091380108167 | validation: 1.4396295389688705]
	TIME [epoch: 7.13 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9268187662530691		[learning rate: 0.0044087]
	Learning Rate: 0.00440867
	LOSS [training: 0.9268187662530691 | validation: 1.208967443609384]
	TIME [epoch: 7.15 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9871223346995498		[learning rate: 0.0043983]
	Learning Rate: 0.00439827
	LOSS [training: 0.9871223346995498 | validation: 0.9098029721363519]
	TIME [epoch: 7.13 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8464281764782896		[learning rate: 0.0043879]
	Learning Rate: 0.0043879
	LOSS [training: 0.8464281764782896 | validation: 1.3414434032414229]
	TIME [epoch: 7.12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.065539886726176		[learning rate: 0.0043775]
	Learning Rate: 0.00437755
	LOSS [training: 1.065539886726176 | validation: 0.8840408536976472]
	TIME [epoch: 7.12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8630812311200676		[learning rate: 0.0043672]
	Learning Rate: 0.00436722
	LOSS [training: 0.8630812311200676 | validation: 1.2220082881949295]
	TIME [epoch: 7.12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9539408466016652		[learning rate: 0.0043569]
	Learning Rate: 0.00435692
	LOSS [training: 0.9539408466016652 | validation: 0.6160712553771576]
	TIME [epoch: 7.12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7443947248186397		[learning rate: 0.0043466]
	Learning Rate: 0.00434664
	LOSS [training: 0.7443947248186397 | validation: 1.0400715917708772]
	TIME [epoch: 7.15 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953906730336022		[learning rate: 0.0043364]
	Learning Rate: 0.00433639
	LOSS [training: 0.6953906730336022 | validation: 1.049651131129697]
	TIME [epoch: 7.14 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8833538677138607		[learning rate: 0.0043262]
	Learning Rate: 0.00432616
	LOSS [training: 0.8833538677138607 | validation: 1.5425590956343633]
	TIME [epoch: 7.12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9913344948615673		[learning rate: 0.004316]
	Learning Rate: 0.00431596
	LOSS [training: 0.9913344948615673 | validation: 0.8482925298040447]
	TIME [epoch: 7.13 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.829812780664137		[learning rate: 0.0043058]
	Learning Rate: 0.00430577
	LOSS [training: 0.829812780664137 | validation: 1.069437976129568]
	TIME [epoch: 7.13 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9529960374901194		[learning rate: 0.0042956]
	Learning Rate: 0.00429562
	LOSS [training: 0.9529960374901194 | validation: 0.9100122961667266]
	TIME [epoch: 7.13 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8069776234105012		[learning rate: 0.0042855]
	Learning Rate: 0.00428549
	LOSS [training: 0.8069776234105012 | validation: 0.8085159606251235]
	TIME [epoch: 7.16 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7548127255885623		[learning rate: 0.0042754]
	Learning Rate: 0.00427538
	LOSS [training: 0.7548127255885623 | validation: 0.6403153798965715]
	TIME [epoch: 7.14 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9951291320458839		[learning rate: 0.0042653]
	Learning Rate: 0.00426529
	LOSS [training: 0.9951291320458839 | validation: 1.0897254153553884]
	TIME [epoch: 7.12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7717948445096485		[learning rate: 0.0042552]
	Learning Rate: 0.00425523
	LOSS [training: 0.7717948445096485 | validation: 0.524290634165883]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7159110611541367		[learning rate: 0.0042452]
	Learning Rate: 0.00424519
	LOSS [training: 0.7159110611541367 | validation: 0.6556215976282076]
	TIME [epoch: 7.12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6170678973907308		[learning rate: 0.0042352]
	Learning Rate: 0.00423518
	LOSS [training: 0.6170678973907308 | validation: 0.6330351191286612]
	TIME [epoch: 7.13 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6278506252670624		[learning rate: 0.0042252]
	Learning Rate: 0.00422519
	LOSS [training: 0.6278506252670624 | validation: 0.7130117193156981]
	TIME [epoch: 7.16 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9928623272657351		[learning rate: 0.0042152]
	Learning Rate: 0.00421522
	LOSS [training: 0.9928623272657351 | validation: 0.7932250787457251]
	TIME [epoch: 7.14 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.899099055276689		[learning rate: 0.0042053]
	Learning Rate: 0.00420528
	LOSS [training: 0.899099055276689 | validation: 0.9445206417899145]
	TIME [epoch: 7.13 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8411152854269209		[learning rate: 0.0041954]
	Learning Rate: 0.00419536
	LOSS [training: 0.8411152854269209 | validation: 0.997375630583278]
	TIME [epoch: 7.12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6737633624507009		[learning rate: 0.0041855]
	Learning Rate: 0.00418546
	LOSS [training: 0.6737633624507009 | validation: 0.7633736230738392]
	TIME [epoch: 7.12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7804559785179725		[learning rate: 0.0041756]
	Learning Rate: 0.00417559
	LOSS [training: 0.7804559785179725 | validation: 1.1785250285111535]
	TIME [epoch: 7.16 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7223354905491057		[learning rate: 0.0041657]
	Learning Rate: 0.00416574
	LOSS [training: 1.7223354905491057 | validation: 1.362633470903678]
	TIME [epoch: 7.16 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9561565098986907		[learning rate: 0.0041559]
	Learning Rate: 0.00415592
	LOSS [training: 0.9561565098986907 | validation: 0.690752527871493]
	TIME [epoch: 7.13 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9333955230949134		[learning rate: 0.0041461]
	Learning Rate: 0.00414611
	LOSS [training: 0.9333955230949134 | validation: 0.9050968214981983]
	TIME [epoch: 7.13 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8210972231544282		[learning rate: 0.0041363]
	Learning Rate: 0.00413633
	LOSS [training: 0.8210972231544282 | validation: 0.664354997401615]
	TIME [epoch: 7.13 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7722183691792626		[learning rate: 0.0041266]
	Learning Rate: 0.00412657
	LOSS [training: 0.7722183691792626 | validation: 1.0224844420642933]
	TIME [epoch: 7.13 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8713504743987321		[learning rate: 0.0041168]
	Learning Rate: 0.00411684
	LOSS [training: 0.8713504743987321 | validation: 0.591829903893184]
	TIME [epoch: 7.13 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7337571977364717		[learning rate: 0.0041071]
	Learning Rate: 0.00410713
	LOSS [training: 0.7337571977364717 | validation: 0.9911405498691271]
	TIME [epoch: 7.16 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8853785645830261		[learning rate: 0.0040974]
	Learning Rate: 0.00409744
	LOSS [training: 0.8853785645830261 | validation: 0.763482843455247]
	TIME [epoch: 7.12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9268889413426429		[learning rate: 0.0040878]
	Learning Rate: 0.00408778
	LOSS [training: 0.9268889413426429 | validation: 0.7748461673067943]
	TIME [epoch: 7.12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7708597642286048		[learning rate: 0.0040781]
	Learning Rate: 0.00407813
	LOSS [training: 0.7708597642286048 | validation: 1.0295616002905654]
	TIME [epoch: 7.12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.865465743763224		[learning rate: 0.0040685]
	Learning Rate: 0.00406851
	LOSS [training: 0.865465743763224 | validation: 1.52595537608694]
	TIME [epoch: 7.13 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8963786594403744		[learning rate: 0.0040589]
	Learning Rate: 0.00405892
	LOSS [training: 0.8963786594403744 | validation: 0.666515962651439]
	TIME [epoch: 7.12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6536210501570843		[learning rate: 0.0040493]
	Learning Rate: 0.00404934
	LOSS [training: 0.6536210501570843 | validation: 0.5503701070262906]
	TIME [epoch: 7.16 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7661477052497065		[learning rate: 0.0040398]
	Learning Rate: 0.00403979
	LOSS [training: 0.7661477052497065 | validation: 0.709144986370634]
	TIME [epoch: 7.13 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.671864238019942		[learning rate: 0.0040303]
	Learning Rate: 0.00403026
	LOSS [training: 0.671864238019942 | validation: 0.6019286668717432]
	TIME [epoch: 7.12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5323514208320146		[learning rate: 0.0040208]
	Learning Rate: 0.00402076
	LOSS [training: 0.5323514208320146 | validation: 0.8240231245576715]
	TIME [epoch: 7.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5170244546949775		[learning rate: 0.0040113]
	Learning Rate: 0.00401127
	LOSS [training: 0.5170244546949775 | validation: 0.7017846026736982]
	TIME [epoch: 9.72 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6434457415347677		[learning rate: 0.0040018]
	Learning Rate: 0.00400181
	LOSS [training: 0.6434457415347677 | validation: 0.49873460417614546]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8936363287026807		[learning rate: 0.0039924]
	Learning Rate: 0.00399237
	LOSS [training: 0.8936363287026807 | validation: 0.9511066774232518]
	TIME [epoch: 7.17 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7371882951431312		[learning rate: 0.003983]
	Learning Rate: 0.00398295
	LOSS [training: 0.7371882951431312 | validation: 0.5419066689572876]
	TIME [epoch: 7.13 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7217321193767319		[learning rate: 0.0039736]
	Learning Rate: 0.00397356
	LOSS [training: 0.7217321193767319 | validation: 0.5022813449905303]
	TIME [epoch: 7.12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6007705894567967		[learning rate: 0.0039642]
	Learning Rate: 0.00396418
	LOSS [training: 0.6007705894567967 | validation: 0.5223841708938854]
	TIME [epoch: 7.12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5789194824021255		[learning rate: 0.0039548]
	Learning Rate: 0.00395483
	LOSS [training: 0.5789194824021255 | validation: 0.555058563106876]
	TIME [epoch: 7.12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0813753592480317		[learning rate: 0.0039455]
	Learning Rate: 0.0039455
	LOSS [training: 1.0813753592480317 | validation: 0.7843123694473572]
	TIME [epoch: 7.12 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7056660080340669		[learning rate: 0.0039362]
	Learning Rate: 0.0039362
	LOSS [training: 0.7056660080340669 | validation: 1.3004953690780454]
	TIME [epoch: 7.15 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7309398244839095		[learning rate: 0.0039269]
	Learning Rate: 0.00392691
	LOSS [training: 0.7309398244839095 | validation: 0.5709821783116187]
	TIME [epoch: 7.12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.650831129841455		[learning rate: 0.0039176]
	Learning Rate: 0.00391765
	LOSS [training: 0.650831129841455 | validation: 0.724866666009331]
	TIME [epoch: 7.12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7138463245266309		[learning rate: 0.0039084]
	Learning Rate: 0.00390841
	LOSS [training: 0.7138463245266309 | validation: 0.5339517818500104]
	TIME [epoch: 7.12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7122614680614164		[learning rate: 0.0038992]
	Learning Rate: 0.00389919
	LOSS [training: 0.7122614680614164 | validation: 0.7033602177112879]
	TIME [epoch: 7.12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6147359127647987		[learning rate: 0.00389]
	Learning Rate: 0.00388999
	LOSS [training: 0.6147359127647987 | validation: 0.7826945694631946]
	TIME [epoch: 7.12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8196675143738006		[learning rate: 0.0038808]
	Learning Rate: 0.00388082
	LOSS [training: 0.8196675143738006 | validation: 0.6255938349583934]
	TIME [epoch: 7.15 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3154783474354996		[learning rate: 0.0038717]
	Learning Rate: 0.00387166
	LOSS [training: 1.3154783474354996 | validation: 0.7347141390626711]
	TIME [epoch: 7.12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1465062637427819		[learning rate: 0.0038625]
	Learning Rate: 0.00386253
	LOSS [training: 1.1465062637427819 | validation: 1.9277192562515832]
	TIME [epoch: 7.12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.076912682317118		[learning rate: 0.0038534]
	Learning Rate: 0.00385342
	LOSS [training: 1.076912682317118 | validation: 0.579751354101604]
	TIME [epoch: 7.12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0654310649137593		[learning rate: 0.0038443]
	Learning Rate: 0.00384433
	LOSS [training: 1.0654310649137593 | validation: 1.429558856957111]
	TIME [epoch: 7.11 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8892158843744218		[learning rate: 0.0038353]
	Learning Rate: 0.00383526
	LOSS [training: 0.8892158843744218 | validation: 0.5415069273488595]
	TIME [epoch: 7.12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0082585668266462		[learning rate: 0.0038262]
	Learning Rate: 0.00382621
	LOSS [training: 1.0082585668266462 | validation: 0.624316838086249]
	TIME [epoch: 7.16 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7437490259874424		[learning rate: 0.0038172]
	Learning Rate: 0.00381719
	LOSS [training: 0.7437490259874424 | validation: 0.6957247164913793]
	TIME [epoch: 7.14 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926166000091583		[learning rate: 0.0038082]
	Learning Rate: 0.00380818
	LOSS [training: 0.6926166000091583 | validation: 0.5257903668692722]
	TIME [epoch: 7.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0584776338067168		[learning rate: 0.0037992]
	Learning Rate: 0.0037992
	LOSS [training: 1.0584776338067168 | validation: 0.8505275859013333]
	TIME [epoch: 7.13 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8426250319091565		[learning rate: 0.0037902]
	Learning Rate: 0.00379024
	LOSS [training: 0.8426250319091565 | validation: 1.1333488791086468]
	TIME [epoch: 7.11 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7258894993493954		[learning rate: 0.0037813]
	Learning Rate: 0.0037813
	LOSS [training: 0.7258894993493954 | validation: 1.8801664314745499]
	TIME [epoch: 7.12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1343226231912529		[learning rate: 0.0037724]
	Learning Rate: 0.00377238
	LOSS [training: 1.1343226231912529 | validation: 0.6236404744226152]
	TIME [epoch: 7.15 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7122410122032122		[learning rate: 0.0037635]
	Learning Rate: 0.00376348
	LOSS [training: 0.7122410122032122 | validation: 0.8194709170375414]
	TIME [epoch: 7.13 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6661569653812407		[learning rate: 0.0037546]
	Learning Rate: 0.0037546
	LOSS [training: 0.6661569653812407 | validation: 0.5921190997852779]
	TIME [epoch: 7.11 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0468433477513028		[learning rate: 0.0037457]
	Learning Rate: 0.00374575
	LOSS [training: 1.0468433477513028 | validation: 2.1149473555462817]
	TIME [epoch: 7.12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.155298302317623		[learning rate: 0.0037369]
	Learning Rate: 0.00373691
	LOSS [training: 1.155298302317623 | validation: 0.5484157206785552]
	TIME [epoch: 7.12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7691280709307206		[learning rate: 0.0037281]
	Learning Rate: 0.0037281
	LOSS [training: 0.7691280709307206 | validation: 0.5751519515595649]
	TIME [epoch: 7.12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0480818425084264		[learning rate: 0.0037193]
	Learning Rate: 0.0037193
	LOSS [training: 1.0480818425084264 | validation: 0.8247173508047203]
	TIME [epoch: 7.15 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7246958362244152		[learning rate: 0.0037105]
	Learning Rate: 0.00371053
	LOSS [training: 0.7246958362244152 | validation: 0.5905576909211081]
	TIME [epoch: 7.13 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6607582479876107		[learning rate: 0.0037018]
	Learning Rate: 0.00370178
	LOSS [training: 0.6607582479876107 | validation: 0.8544358916112862]
	TIME [epoch: 7.11 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.784854013958544		[learning rate: 0.003693]
	Learning Rate: 0.00369304
	LOSS [training: 0.784854013958544 | validation: 0.6506686300321918]
	TIME [epoch: 7.11 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7398955556378606		[learning rate: 0.0036843]
	Learning Rate: 0.00368433
	LOSS [training: 0.7398955556378606 | validation: 1.1831492968876889]
	TIME [epoch: 7.12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.184576158964938		[learning rate: 0.0036756]
	Learning Rate: 0.00367564
	LOSS [training: 1.184576158964938 | validation: 1.2743987683081]
	TIME [epoch: 7.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7475199771747586		[learning rate: 0.003667]
	Learning Rate: 0.00366697
	LOSS [training: 0.7475199771747586 | validation: 0.6898634403636388]
	TIME [epoch: 7.16 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7138590770126958		[learning rate: 0.0036583]
	Learning Rate: 0.00365832
	LOSS [training: 0.7138590770126958 | validation: 0.9292043965703739]
	TIME [epoch: 7.13 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.719723501684743		[learning rate: 0.0036497]
	Learning Rate: 0.00364969
	LOSS [training: 0.719723501684743 | validation: 0.8689135999449111]
	TIME [epoch: 7.12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9410450366131853		[learning rate: 0.0036411]
	Learning Rate: 0.00364108
	LOSS [training: 0.9410450366131853 | validation: 1.2264202460606635]
	TIME [epoch: 7.12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9991004338355189		[learning rate: 0.0036325]
	Learning Rate: 0.0036325
	LOSS [training: 0.9991004338355189 | validation: 1.071583848195258]
	TIME [epoch: 7.12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8254500449553369		[learning rate: 0.0036239]
	Learning Rate: 0.00362393
	LOSS [training: 0.8254500449553369 | validation: 0.8965491655172755]
	TIME [epoch: 7.11 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7308524675392231		[learning rate: 0.0036154]
	Learning Rate: 0.00361538
	LOSS [training: 0.7308524675392231 | validation: 0.7014329485823387]
	TIME [epoch: 7.16 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.622522091910086		[learning rate: 0.0036069]
	Learning Rate: 0.00360685
	LOSS [training: 0.622522091910086 | validation: 0.6225207472657659]
	TIME [epoch: 7.12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6258797602042445		[learning rate: 0.0035983]
	Learning Rate: 0.00359834
	LOSS [training: 0.6258797602042445 | validation: 1.0331187648994995]
	TIME [epoch: 7.13 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8006561545444504		[learning rate: 0.0035899]
	Learning Rate: 0.00358986
	LOSS [training: 0.8006561545444504 | validation: 0.689661176011616]
	TIME [epoch: 7.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8545326139049509		[learning rate: 0.0035814]
	Learning Rate: 0.00358139
	LOSS [training: 0.8545326139049509 | validation: 0.5712622119244031]
	TIME [epoch: 7.13 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6387510474778473		[learning rate: 0.0035729]
	Learning Rate: 0.00357294
	LOSS [training: 0.6387510474778473 | validation: 0.6372716267266602]
	TIME [epoch: 7.12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5614629041242198		[learning rate: 0.0035645]
	Learning Rate: 0.00356451
	LOSS [training: 0.5614629041242198 | validation: 0.4378579108029511]
	TIME [epoch: 7.16 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6042912955653849		[learning rate: 0.0035561]
	Learning Rate: 0.0035561
	LOSS [training: 0.6042912955653849 | validation: 1.6083151666642954]
	TIME [epoch: 7.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0742347862595814		[learning rate: 0.0035477]
	Learning Rate: 0.00354771
	LOSS [training: 1.0742347862595814 | validation: 0.4772761924599504]
	TIME [epoch: 7.13 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6075177017867354		[learning rate: 0.0035393]
	Learning Rate: 0.00353935
	LOSS [training: 0.6075177017867354 | validation: 0.7675217300743545]
	TIME [epoch: 7.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7767552943500081		[learning rate: 0.003531]
	Learning Rate: 0.003531
	LOSS [training: 0.7767552943500081 | validation: 0.9321567312152026]
	TIME [epoch: 7.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6858251710922476		[learning rate: 0.0035227]
	Learning Rate: 0.00352267
	LOSS [training: 0.6858251710922476 | validation: 0.6829685418693519]
	TIME [epoch: 7.13 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1057632506230464		[learning rate: 0.0035144]
	Learning Rate: 0.00351436
	LOSS [training: 1.1057632506230464 | validation: 1.3274244260968706]
	TIME [epoch: 7.16 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7364119625510193		[learning rate: 0.0035061]
	Learning Rate: 0.00350607
	LOSS [training: 0.7364119625510193 | validation: 1.1363099405900743]
	TIME [epoch: 7.13 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7743204115988148		[learning rate: 0.0034978]
	Learning Rate: 0.0034978
	LOSS [training: 0.7743204115988148 | validation: 0.8453618020494592]
	TIME [epoch: 7.12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7318471554283373		[learning rate: 0.0034895]
	Learning Rate: 0.00348955
	LOSS [training: 0.7318471554283373 | validation: 0.549635869214276]
	TIME [epoch: 7.12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.618604024856528		[learning rate: 0.0034813]
	Learning Rate: 0.00348132
	LOSS [training: 0.618604024856528 | validation: 0.7941249861292701]
	TIME [epoch: 7.12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8423055413060764		[learning rate: 0.0034731]
	Learning Rate: 0.00347311
	LOSS [training: 0.8423055413060764 | validation: 0.8026209836681116]
	TIME [epoch: 7.12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8355085815677883		[learning rate: 0.0034649]
	Learning Rate: 0.00346491
	LOSS [training: 0.8355085815677883 | validation: 0.715423036316585]
	TIME [epoch: 7.15 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5471219372411411		[learning rate: 0.0034567]
	Learning Rate: 0.00345674
	LOSS [training: 0.5471219372411411 | validation: 0.9079934762629549]
	TIME [epoch: 7.12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6746057872733999		[learning rate: 0.0034486]
	Learning Rate: 0.00344859
	LOSS [training: 0.6746057872733999 | validation: 0.6099788112277587]
	TIME [epoch: 7.12 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5788932176520113		[learning rate: 0.0034405]
	Learning Rate: 0.00344045
	LOSS [training: 0.5788932176520113 | validation: 0.8902558021412261]
	TIME [epoch: 7.12 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.880708582315451		[learning rate: 0.0034323]
	Learning Rate: 0.00343234
	LOSS [training: 0.880708582315451 | validation: 1.2914879828744517]
	TIME [epoch: 7.12 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9690206438966668		[learning rate: 0.0034242]
	Learning Rate: 0.00342424
	LOSS [training: 0.9690206438966668 | validation: 0.6399160648418323]
	TIME [epoch: 7.12 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7958865711658887		[learning rate: 0.0034162]
	Learning Rate: 0.00341616
	LOSS [training: 0.7958865711658887 | validation: 0.5238398758102443]
	TIME [epoch: 7.15 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6628576293219443		[learning rate: 0.0034081]
	Learning Rate: 0.0034081
	LOSS [training: 0.6628576293219443 | validation: 0.8057364231893307]
	TIME [epoch: 7.12 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7600973803665864		[learning rate: 0.0034001]
	Learning Rate: 0.00340006
	LOSS [training: 0.7600973803665864 | validation: 0.5846145678355192]
	TIME [epoch: 7.11 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5759310554457352		[learning rate: 0.003392]
	Learning Rate: 0.00339204
	LOSS [training: 0.5759310554457352 | validation: 0.5823169605568916]
	TIME [epoch: 7.12 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998241430209088		[learning rate: 0.003384]
	Learning Rate: 0.00338404
	LOSS [training: 0.6998241430209088 | validation: 0.376662028752053]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6806181535665488		[learning rate: 0.0033761]
	Learning Rate: 0.00337606
	LOSS [training: 0.6806181535665488 | validation: 1.101500569606814]
	TIME [epoch: 7.14 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7464172192309929		[learning rate: 0.0033681]
	Learning Rate: 0.0033681
	LOSS [training: 0.7464172192309929 | validation: 0.6865269969551332]
	TIME [epoch: 7.16 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7061386624519203		[learning rate: 0.0033602]
	Learning Rate: 0.00336015
	LOSS [training: 0.7061386624519203 | validation: 0.6086050206503412]
	TIME [epoch: 7.13 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.673863396362385		[learning rate: 0.0033522]
	Learning Rate: 0.00335223
	LOSS [training: 0.673863396362385 | validation: 0.7705662173191902]
	TIME [epoch: 7.12 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6371572526718776		[learning rate: 0.0033443]
	Learning Rate: 0.00334432
	LOSS [training: 0.6371572526718776 | validation: 0.8404693044375477]
	TIME [epoch: 7.12 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7386718729469456		[learning rate: 0.0033364]
	Learning Rate: 0.00333643
	LOSS [training: 0.7386718729469456 | validation: 0.5908890296164533]
	TIME [epoch: 7.12 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6415774394892481		[learning rate: 0.0033286]
	Learning Rate: 0.00332856
	LOSS [training: 0.6415774394892481 | validation: 0.4988007548582474]
	TIME [epoch: 7.12 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5415597359925656		[learning rate: 0.0033207]
	Learning Rate: 0.00332071
	LOSS [training: 0.5415597359925656 | validation: 0.4428630426093584]
	TIME [epoch: 7.16 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1511653744150052		[learning rate: 0.0033129]
	Learning Rate: 0.00331288
	LOSS [training: 1.1511653744150052 | validation: 0.9047843235612821]
	TIME [epoch: 7.12 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5711936662244721		[learning rate: 0.0033051]
	Learning Rate: 0.00330506
	LOSS [training: 0.5711936662244721 | validation: 0.42065951931308654]
	TIME [epoch: 7.12 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.525714893898139		[learning rate: 0.0032973]
	Learning Rate: 0.00329727
	LOSS [training: 0.525714893898139 | validation: 0.7527370454611193]
	TIME [epoch: 7.13 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7064124304115088		[learning rate: 0.0032895]
	Learning Rate: 0.00328949
	LOSS [training: 0.7064124304115088 | validation: 0.5106629249729034]
	TIME [epoch: 7.13 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5635526182251125		[learning rate: 0.0032817]
	Learning Rate: 0.00328173
	LOSS [training: 0.5635526182251125 | validation: 0.4075775688856191]
	TIME [epoch: 7.14 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4686497236982291		[learning rate: 0.003274]
	Learning Rate: 0.00327399
	LOSS [training: 0.4686497236982291 | validation: 0.7076247361149923]
	TIME [epoch: 7.16 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8825915986071914		[learning rate: 0.0032663]
	Learning Rate: 0.00326626
	LOSS [training: 0.8825915986071914 | validation: 0.6795181816638474]
	TIME [epoch: 7.13 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5091035615771426		[learning rate: 0.0032586]
	Learning Rate: 0.00325856
	LOSS [training: 0.5091035615771426 | validation: 0.5522579843890176]
	TIME [epoch: 7.14 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7170706296006873		[learning rate: 0.0032509]
	Learning Rate: 0.00325087
	LOSS [training: 0.7170706296006873 | validation: 0.6499521481371325]
	TIME [epoch: 7.13 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7887463891185933		[learning rate: 0.0032432]
	Learning Rate: 0.0032432
	LOSS [training: 0.7887463891185933 | validation: 0.8282338687868609]
	TIME [epoch: 7.13 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8665825840578524		[learning rate: 0.0032356]
	Learning Rate: 0.00323555
	LOSS [training: 0.8665825840578524 | validation: 1.075825700792584]
	TIME [epoch: 7.15 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7565653117961499		[learning rate: 0.0032279]
	Learning Rate: 0.00322792
	LOSS [training: 0.7565653117961499 | validation: 0.7644007178029506]
	TIME [epoch: 7.16 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5765780493222421		[learning rate: 0.0032203]
	Learning Rate: 0.00322031
	LOSS [training: 0.5765780493222421 | validation: 0.5344008804373482]
	TIME [epoch: 7.13 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7442190660589016		[learning rate: 0.0032127]
	Learning Rate: 0.00321271
	LOSS [training: 0.7442190660589016 | validation: 0.9832268613613468]
	TIME [epoch: 7.13 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8029635487580474		[learning rate: 0.0032051]
	Learning Rate: 0.00320513
	LOSS [training: 0.8029635487580474 | validation: 0.64370425830299]
	TIME [epoch: 7.13 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6551807196419398		[learning rate: 0.0031976]
	Learning Rate: 0.00319757
	LOSS [training: 0.6551807196419398 | validation: 0.533208249690554]
	TIME [epoch: 7.13 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5688228221388475		[learning rate: 0.00319]
	Learning Rate: 0.00319003
	LOSS [training: 0.5688228221388475 | validation: 0.5856015525016073]
	TIME [epoch: 7.14 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6732680267497863		[learning rate: 0.0031825]
	Learning Rate: 0.00318251
	LOSS [training: 0.6732680267497863 | validation: 0.6560932965372872]
	TIME [epoch: 7.16 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8278240388168158		[learning rate: 0.003175]
	Learning Rate: 0.003175
	LOSS [training: 0.8278240388168158 | validation: 0.5842973835602867]
	TIME [epoch: 7.13 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5227801180527928		[learning rate: 0.0031675]
	Learning Rate: 0.00316751
	LOSS [training: 0.5227801180527928 | validation: 2.592486782178944]
	TIME [epoch: 7.12 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1286104225346232		[learning rate: 0.00316]
	Learning Rate: 0.00316004
	LOSS [training: 1.1286104225346232 | validation: 1.0539506597908221]
	TIME [epoch: 7.13 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8371898347248681		[learning rate: 0.0031526]
	Learning Rate: 0.00315258
	LOSS [training: 0.8371898347248681 | validation: 0.7905243152769608]
	TIME [epoch: 7.13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6759043014624637		[learning rate: 0.0031451]
	Learning Rate: 0.00314515
	LOSS [training: 0.6759043014624637 | validation: 0.8562833073481553]
	TIME [epoch: 7.14 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5290171756756015		[learning rate: 0.0031377]
	Learning Rate: 0.00313773
	LOSS [training: 0.5290171756756015 | validation: 0.5312363868286141]
	TIME [epoch: 7.16 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8275777790744098		[learning rate: 0.0031303]
	Learning Rate: 0.00313033
	LOSS [training: 0.8275777790744098 | validation: 0.8659758146518125]
	TIME [epoch: 7.14 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7165044806158747		[learning rate: 0.0031229]
	Learning Rate: 0.00312294
	LOSS [training: 0.7165044806158747 | validation: 0.6437734342652359]
	TIME [epoch: 7.13 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5937116889067791		[learning rate: 0.0031156]
	Learning Rate: 0.00311558
	LOSS [training: 0.5937116889067791 | validation: 0.48608054138073736]
	TIME [epoch: 7.13 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4826709028541048		[learning rate: 0.0031082]
	Learning Rate: 0.00310823
	LOSS [training: 0.4826709028541048 | validation: 0.9637865728393895]
	TIME [epoch: 7.12 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5897769894583161		[learning rate: 0.0031009]
	Learning Rate: 0.0031009
	LOSS [training: 0.5897769894583161 | validation: 0.6628561790254517]
	TIME [epoch: 7.15 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8887994300838379		[learning rate: 0.0030936]
	Learning Rate: 0.00309358
	LOSS [training: 0.8887994300838379 | validation: 0.5856505434181436]
	TIME [epoch: 7.15 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6176643598472632		[learning rate: 0.0030863]
	Learning Rate: 0.00308628
	LOSS [training: 0.6176643598472632 | validation: 0.46480974352742027]
	TIME [epoch: 7.13 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5149666477726448		[learning rate: 0.003079]
	Learning Rate: 0.003079
	LOSS [training: 0.5149666477726448 | validation: 0.6904234925655972]
	TIME [epoch: 7.12 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6267196495278025		[learning rate: 0.0030717]
	Learning Rate: 0.00307174
	LOSS [training: 0.6267196495278025 | validation: 0.6582588499212829]
	TIME [epoch: 7.14 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6636237503657196		[learning rate: 0.0030645]
	Learning Rate: 0.0030645
	LOSS [training: 0.6636237503657196 | validation: 0.5406882540360851]
	TIME [epoch: 7.13 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6005501772614137		[learning rate: 0.0030573]
	Learning Rate: 0.00305727
	LOSS [training: 0.6005501772614137 | validation: 0.609645060681608]
	TIME [epoch: 7.14 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7026213260060763		[learning rate: 0.0030501]
	Learning Rate: 0.00305006
	LOSS [training: 0.7026213260060763 | validation: 1.9305686671184548]
	TIME [epoch: 7.15 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9239174238152412		[learning rate: 0.0030429]
	Learning Rate: 0.00304286
	LOSS [training: 0.9239174238152412 | validation: 0.5978175840106428]
	TIME [epoch: 7.13 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44354953533389335		[learning rate: 0.0030357]
	Learning Rate: 0.00303568
	LOSS [training: 0.44354953533389335 | validation: 0.48265973055919364]
	TIME [epoch: 7.12 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6008939285990338		[learning rate: 0.0030285]
	Learning Rate: 0.00302852
	LOSS [training: 0.6008939285990338 | validation: 1.0131667460654312]
	TIME [epoch: 7.13 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6634187577788891		[learning rate: 0.0030214]
	Learning Rate: 0.00302138
	LOSS [training: 0.6634187577788891 | validation: 0.4953109795000622]
	TIME [epoch: 7.13 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9836727579587047		[learning rate: 0.0030143]
	Learning Rate: 0.00301425
	LOSS [training: 0.9836727579587047 | validation: 0.7797641776636786]
	TIME [epoch: 7.15 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6032530207485032		[learning rate: 0.0030071]
	Learning Rate: 0.00300714
	LOSS [training: 0.6032530207485032 | validation: 0.8605627232413924]
	TIME [epoch: 7.16 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6851096573071334		[learning rate: 0.003]
	Learning Rate: 0.00300005
	LOSS [training: 0.6851096573071334 | validation: 0.7165515031101606]
	TIME [epoch: 7.14 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5262304577277533		[learning rate: 0.002993]
	Learning Rate: 0.00299297
	LOSS [training: 0.5262304577277533 | validation: 0.5616613480458681]
	TIME [epoch: 7.12 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.632264266034177		[learning rate: 0.0029859]
	Learning Rate: 0.00298591
	LOSS [training: 0.632264266034177 | validation: 0.9006511939422989]
	TIME [epoch: 7.13 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6435314772851699		[learning rate: 0.0029789]
	Learning Rate: 0.00297887
	LOSS [training: 0.6435314772851699 | validation: 0.5299775828396216]
	TIME [epoch: 7.13 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6149192087924834		[learning rate: 0.0029718]
	Learning Rate: 0.00297184
	LOSS [training: 0.6149192087924834 | validation: 0.7602638071367829]
	TIME [epoch: 7.14 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6028270597563934		[learning rate: 0.0029648]
	Learning Rate: 0.00296483
	LOSS [training: 0.6028270597563934 | validation: 0.7767780778670195]
	TIME [epoch: 7.15 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6040386851492477		[learning rate: 0.0029578]
	Learning Rate: 0.00295784
	LOSS [training: 0.6040386851492477 | validation: 0.6570324846993777]
	TIME [epoch: 7.13 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4682941094959481		[learning rate: 0.0029509]
	Learning Rate: 0.00295086
	LOSS [training: 0.4682941094959481 | validation: 0.5128830195051067]
	TIME [epoch: 7.13 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7862016777469251		[learning rate: 0.0029439]
	Learning Rate: 0.0029439
	LOSS [training: 0.7862016777469251 | validation: 0.648493558722314]
	TIME [epoch: 7.13 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4857030808460969		[learning rate: 0.002937]
	Learning Rate: 0.00293696
	LOSS [training: 0.4857030808460969 | validation: 0.6179747896342509]
	TIME [epoch: 7.13 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7058266605582336		[learning rate: 0.00293]
	Learning Rate: 0.00293003
	LOSS [training: 0.7058266605582336 | validation: 0.8618693147018373]
	TIME [epoch: 7.14 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6548732578857956		[learning rate: 0.0029231]
	Learning Rate: 0.00292312
	LOSS [training: 0.6548732578857956 | validation: 0.6666259053707028]
	TIME [epoch: 7.16 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7614146944955292		[learning rate: 0.0029162]
	Learning Rate: 0.00291622
	LOSS [training: 0.7614146944955292 | validation: 0.4994128305699974]
	TIME [epoch: 7.13 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4772562910391381		[learning rate: 0.0029093]
	Learning Rate: 0.00290934
	LOSS [training: 0.4772562910391381 | validation: 0.49809550995696594]
	TIME [epoch: 7.13 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.555565876554794		[learning rate: 0.0029025]
	Learning Rate: 0.00290248
	LOSS [training: 0.555565876554794 | validation: 0.44745220906528105]
	TIME [epoch: 7.13 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47359579848558464		[learning rate: 0.0028956]
	Learning Rate: 0.00289563
	LOSS [training: 0.47359579848558464 | validation: 0.7061793541325458]
	TIME [epoch: 7.13 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6043933371445614		[learning rate: 0.0028888]
	Learning Rate: 0.0028888
	LOSS [training: 0.6043933371445614 | validation: 0.4584390983494216]
	TIME [epoch: 7.14 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8197968783178992		[learning rate: 0.002882]
	Learning Rate: 0.00288199
	LOSS [training: 0.8197968783178992 | validation: 0.5990878420478085]
	TIME [epoch: 7.15 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912454841530062		[learning rate: 0.0028752]
	Learning Rate: 0.00287519
	LOSS [training: 0.6912454841530062 | validation: 0.6104194505121145]
	TIME [epoch: 7.12 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.738110398131387		[learning rate: 0.0028684]
	Learning Rate: 0.00286841
	LOSS [training: 0.738110398131387 | validation: 0.5518849151540517]
	TIME [epoch: 7.12 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5539407629547399		[learning rate: 0.0028616]
	Learning Rate: 0.00286164
	LOSS [training: 0.5539407629547399 | validation: 0.3957224735184142]
	TIME [epoch: 7.13 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7191446871757944		[learning rate: 0.0028549]
	Learning Rate: 0.00285489
	LOSS [training: 0.7191446871757944 | validation: 0.7346959687954264]
	TIME [epoch: 7.12 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6262864612776209		[learning rate: 0.0028482]
	Learning Rate: 0.00284816
	LOSS [training: 0.6262864612776209 | validation: 0.6536787994350164]
	TIME [epoch: 7.13 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5587075705104173		[learning rate: 0.0028414]
	Learning Rate: 0.00284144
	LOSS [training: 0.5587075705104173 | validation: 0.6096276689924975]
	TIME [epoch: 7.15 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8478301056169117		[learning rate: 0.0028347]
	Learning Rate: 0.00283474
	LOSS [training: 0.8478301056169117 | validation: 0.9152604969115565]
	TIME [epoch: 7.13 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970181489072647		[learning rate: 0.0028281]
	Learning Rate: 0.00282805
	LOSS [training: 0.6970181489072647 | validation: 0.3854641549921286]
	TIME [epoch: 7.12 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.418881062312449		[learning rate: 0.0028214]
	Learning Rate: 0.00282138
	LOSS [training: 0.418881062312449 | validation: 0.42272582909376955]
	TIME [epoch: 7.13 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6482251654648941		[learning rate: 0.0028147]
	Learning Rate: 0.00281472
	LOSS [training: 0.6482251654648941 | validation: 0.7124331052362077]
	TIME [epoch: 7.12 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5266230547050862		[learning rate: 0.0028081]
	Learning Rate: 0.00280808
	LOSS [training: 0.5266230547050862 | validation: 0.6589403381734289]
	TIME [epoch: 7.14 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6115438977416122		[learning rate: 0.0028015]
	Learning Rate: 0.00280146
	LOSS [training: 0.6115438977416122 | validation: 0.5073695624152805]
	TIME [epoch: 7.15 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8609124572816823		[learning rate: 0.0027949]
	Learning Rate: 0.00279485
	LOSS [training: 0.8609124572816823 | validation: 0.6675966840548381]
	TIME [epoch: 7.13 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7090043211317536		[learning rate: 0.0027883]
	Learning Rate: 0.00278826
	LOSS [training: 0.7090043211317536 | validation: 1.1573098638942523]
	TIME [epoch: 7.12 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9151289876805402		[learning rate: 0.0027817]
	Learning Rate: 0.00278168
	LOSS [training: 0.9151289876805402 | validation: 0.575134650121536]
	TIME [epoch: 7.13 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5886889511907883		[learning rate: 0.0027751]
	Learning Rate: 0.00277512
	LOSS [training: 0.5886889511907883 | validation: 0.9176593131862574]
	TIME [epoch: 7.12 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6306953322372284		[learning rate: 0.0027686]
	Learning Rate: 0.00276858
	LOSS [training: 0.6306953322372284 | validation: 0.5273580900961969]
	TIME [epoch: 7.14 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7490622691502207		[learning rate: 0.002762]
	Learning Rate: 0.00276205
	LOSS [training: 0.7490622691502207 | validation: 0.9635989020103997]
	TIME [epoch: 7.14 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7444728053164813		[learning rate: 0.0027555]
	Learning Rate: 0.00275553
	LOSS [training: 0.7444728053164813 | validation: 0.732471019040067]
	TIME [epoch: 7.13 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6114526621292129		[learning rate: 0.002749]
	Learning Rate: 0.00274903
	LOSS [training: 0.6114526621292129 | validation: 0.5060131644329582]
	TIME [epoch: 7.12 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4739919190655092		[learning rate: 0.0027425]
	Learning Rate: 0.00274255
	LOSS [training: 0.4739919190655092 | validation: 0.559163982681363]
	TIME [epoch: 7.13 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48087581900482557		[learning rate: 0.0027361]
	Learning Rate: 0.00273608
	LOSS [training: 0.48087581900482557 | validation: 0.9217105665381515]
	TIME [epoch: 7.13 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6085593046246803		[learning rate: 0.0027296]
	Learning Rate: 0.00272962
	LOSS [training: 0.6085593046246803 | validation: 0.41881883828054733]
	TIME [epoch: 7.15 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6246193870637532		[learning rate: 0.0027232]
	Learning Rate: 0.00272318
	LOSS [training: 0.6246193870637532 | validation: 0.5148234009265332]
	TIME [epoch: 7.15 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5486215441930945		[learning rate: 0.0027168]
	Learning Rate: 0.00271676
	LOSS [training: 0.5486215441930945 | validation: 0.7465952286705557]
	TIME [epoch: 7.13 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8170722607804837		[learning rate: 0.0027104]
	Learning Rate: 0.00271035
	LOSS [training: 0.8170722607804837 | validation: 0.705495811178259]
	TIME [epoch: 7.13 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.63567162757989		[learning rate: 0.002704]
	Learning Rate: 0.00270396
	LOSS [training: 0.63567162757989 | validation: 0.5604287056868671]
	TIME [epoch: 7.13 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4375887309263863		[learning rate: 0.0026976]
	Learning Rate: 0.00269758
	LOSS [training: 0.4375887309263863 | validation: 0.6120591591458048]
	TIME [epoch: 7.13 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5480404103138421		[learning rate: 0.0026912]
	Learning Rate: 0.00269122
	LOSS [training: 0.5480404103138421 | validation: 0.536180868390636]
	TIME [epoch: 7.15 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5097400156699436		[learning rate: 0.0026849]
	Learning Rate: 0.00268487
	LOSS [training: 0.5097400156699436 | validation: 0.5527464074873614]
	TIME [epoch: 7.15 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4913313005558323		[learning rate: 0.0026785]
	Learning Rate: 0.00267854
	LOSS [training: 0.4913313005558323 | validation: 0.4164352863644045]
	TIME [epoch: 7.14 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48253095034972315		[learning rate: 0.0026722]
	Learning Rate: 0.00267222
	LOSS [training: 0.48253095034972315 | validation: 0.7320819445613821]
	TIME [epoch: 7.14 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5386848985030419		[learning rate: 0.0026659]
	Learning Rate: 0.00266591
	LOSS [training: 0.5386848985030419 | validation: 0.45987096357182383]
	TIME [epoch: 7.14 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5085023509921924		[learning rate: 0.0026596]
	Learning Rate: 0.00265963
	LOSS [training: 0.5085023509921924 | validation: 0.7998518863192718]
	TIME [epoch: 7.13 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9337652032721907		[learning rate: 0.0026534]
	Learning Rate: 0.00265335
	LOSS [training: 0.9337652032721907 | validation: 0.9432765884907522]
	TIME [epoch: 7.16 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7359145483038437		[learning rate: 0.0026471]
	Learning Rate: 0.00264709
	LOSS [training: 0.7359145483038437 | validation: 0.5974290913737721]
	TIME [epoch: 7.15 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5664678832433317		[learning rate: 0.0026408]
	Learning Rate: 0.00264085
	LOSS [training: 0.5664678832433317 | validation: 0.5271718871837145]
	TIME [epoch: 7.13 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5680048338921158		[learning rate: 0.0026346]
	Learning Rate: 0.00263462
	LOSS [training: 0.5680048338921158 | validation: 1.0840899562283766]
	TIME [epoch: 7.13 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7142909823058619		[learning rate: 0.0026284]
	Learning Rate: 0.00262841
	LOSS [training: 0.7142909823058619 | validation: 0.38355174375110945]
	TIME [epoch: 7.13 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49675501772598774		[learning rate: 0.0026222]
	Learning Rate: 0.00262221
	LOSS [training: 0.49675501772598774 | validation: 0.40317822364788947]
	TIME [epoch: 7.14 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5286784834930895		[learning rate: 0.002616]
	Learning Rate: 0.00261602
	LOSS [training: 0.5286784834930895 | validation: 0.8530785200737327]
	TIME [epoch: 7.16 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6125132631364985		[learning rate: 0.0026098]
	Learning Rate: 0.00260985
	LOSS [training: 0.6125132631364985 | validation: 0.8349150068707183]
	TIME [epoch: 7.15 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6715555792931877		[learning rate: 0.0026037]
	Learning Rate: 0.00260369
	LOSS [training: 0.6715555792931877 | validation: 0.5424083354305973]
	TIME [epoch: 7.13 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4993694981105817		[learning rate: 0.0025976]
	Learning Rate: 0.00259755
	LOSS [training: 0.4993694981105817 | validation: 0.6098603995226362]
	TIME [epoch: 7.13 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6235455497961679		[learning rate: 0.0025914]
	Learning Rate: 0.00259142
	LOSS [training: 0.6235455497961679 | validation: 0.6329223490783562]
	TIME [epoch: 7.13 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.625105696288925		[learning rate: 0.0025853]
	Learning Rate: 0.00258531
	LOSS [training: 0.625105696288925 | validation: 0.84618414136982]
	TIME [epoch: 7.13 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5203760576879798		[learning rate: 0.0025792]
	Learning Rate: 0.00257921
	LOSS [training: 0.5203760576879798 | validation: 0.5425906583257054]
	TIME [epoch: 7.16 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4946010003269251		[learning rate: 0.0025731]
	Learning Rate: 0.00257313
	LOSS [training: 0.4946010003269251 | validation: 0.758583519567539]
	TIME [epoch: 7.14 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5798420572257524		[learning rate: 0.0025671]
	Learning Rate: 0.00256706
	LOSS [training: 0.5798420572257524 | validation: 0.43182915039871966]
	TIME [epoch: 7.14 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5293627084673107		[learning rate: 0.002561]
	Learning Rate: 0.002561
	LOSS [training: 0.5293627084673107 | validation: 0.6798192258197019]
	TIME [epoch: 7.14 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6097983361704367		[learning rate: 0.002555]
	Learning Rate: 0.00255496
	LOSS [training: 0.6097983361704367 | validation: 0.557563859555322]
	TIME [epoch: 7.13 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5044960976545456		[learning rate: 0.0025489]
	Learning Rate: 0.00254894
	LOSS [training: 0.5044960976545456 | validation: 0.5180575816560308]
	TIME [epoch: 7.13 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47473816841252703		[learning rate: 0.0025429]
	Learning Rate: 0.00254292
	LOSS [training: 0.47473816841252703 | validation: 0.6953318780550244]
	TIME [epoch: 7.16 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7367293251867054		[learning rate: 0.0025369]
	Learning Rate: 0.00253693
	LOSS [training: 0.7367293251867054 | validation: 0.6699898648672087]
	TIME [epoch: 7.13 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6259083013707919		[learning rate: 0.0025309]
	Learning Rate: 0.00253094
	LOSS [training: 0.6259083013707919 | validation: 0.6103025345730129]
	TIME [epoch: 7.12 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4289499272612356		[learning rate: 0.002525]
	Learning Rate: 0.00252497
	LOSS [training: 0.4289499272612356 | validation: 0.3798297059114578]
	TIME [epoch: 7.12 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46177833940881047		[learning rate: 0.002519]
	Learning Rate: 0.00251901
	LOSS [training: 0.46177833940881047 | validation: 0.4306442288188339]
	TIME [epoch: 7.13 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4093206136651616		[learning rate: 0.0025131]
	Learning Rate: 0.00251307
	LOSS [training: 0.4093206136651616 | validation: 0.6434570780095057]
	TIME [epoch: 7.12 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7107630367593225		[learning rate: 0.0025071]
	Learning Rate: 0.00250715
	LOSS [training: 0.7107630367593225 | validation: 0.9350623581679327]
	TIME [epoch: 7.15 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6236094318361136		[learning rate: 0.0025012]
	Learning Rate: 0.00250123
	LOSS [training: 0.6236094318361136 | validation: 0.4941270242546505]
	TIME [epoch: 7.13 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.538443955478973		[learning rate: 0.0024953]
	Learning Rate: 0.00249533
	LOSS [training: 0.538443955478973 | validation: 0.41249376821399064]
	TIME [epoch: 7.13 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4784806293147791		[learning rate: 0.0024894]
	Learning Rate: 0.00248945
	LOSS [training: 0.4784806293147791 | validation: 0.46181308847239494]
	TIME [epoch: 7.12 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6477754766538288		[learning rate: 0.0024836]
	Learning Rate: 0.00248357
	LOSS [training: 0.6477754766538288 | validation: 0.4225929747410967]
	TIME [epoch: 7.14 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48398764820777307		[learning rate: 0.0024777]
	Learning Rate: 0.00247771
	LOSS [training: 0.48398764820777307 | validation: 0.5133823027766173]
	TIME [epoch: 7.13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6179375471327838		[learning rate: 0.0024719]
	Learning Rate: 0.00247187
	LOSS [training: 0.6179375471327838 | validation: 0.8699165796308101]
	TIME [epoch: 7.17 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5573477707761798		[learning rate: 0.002466]
	Learning Rate: 0.00246604
	LOSS [training: 0.5573477707761798 | validation: 0.4148013477738334]
	TIME [epoch: 7.14 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5476261632118753		[learning rate: 0.0024602]
	Learning Rate: 0.00246022
	LOSS [training: 0.5476261632118753 | validation: 0.3475551554574575]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4681199547584141		[learning rate: 0.0024544]
	Learning Rate: 0.00245442
	LOSS [training: 0.4681199547584141 | validation: 0.6633053585680857]
	TIME [epoch: 7.12 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6412126781571843		[learning rate: 0.0024486]
	Learning Rate: 0.00244863
	LOSS [training: 0.6412126781571843 | validation: 0.7323371408318528]
	TIME [epoch: 7.13 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.695707549473258		[learning rate: 0.0024429]
	Learning Rate: 0.00244285
	LOSS [training: 0.695707549473258 | validation: 1.0241727614849436]
	TIME [epoch: 7.12 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6045555051447129		[learning rate: 0.0024371]
	Learning Rate: 0.00243709
	LOSS [training: 0.6045555051447129 | validation: 0.767267367903524]
	TIME [epoch: 7.16 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6187192816847377		[learning rate: 0.0024313]
	Learning Rate: 0.00243134
	LOSS [training: 0.6187192816847377 | validation: 0.8262669355191796]
	TIME [epoch: 7.13 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.640513987534065		[learning rate: 0.0024256]
	Learning Rate: 0.00242561
	LOSS [training: 0.640513987534065 | validation: 0.6302387477894693]
	TIME [epoch: 7.13 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49929454927765493		[learning rate: 0.0024199]
	Learning Rate: 0.00241989
	LOSS [training: 0.49929454927765493 | validation: 0.402275837911794]
	TIME [epoch: 7.12 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4104180963263289		[learning rate: 0.0024142]
	Learning Rate: 0.00241418
	LOSS [training: 0.4104180963263289 | validation: 0.2813769303806035]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4886843941879812		[learning rate: 0.0024085]
	Learning Rate: 0.00240848
	LOSS [training: 0.4886843941879812 | validation: 0.6182066652507536]
	TIME [epoch: 7.12 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4575279690909334		[learning rate: 0.0024028]
	Learning Rate: 0.0024028
	LOSS [training: 0.4575279690909334 | validation: 0.3314924234833829]
	TIME [epoch: 7.15 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46790473180248415		[learning rate: 0.0023971]
	Learning Rate: 0.00239713
	LOSS [training: 0.46790473180248415 | validation: 0.9657522622029429]
	TIME [epoch: 7.13 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9886709023277731		[learning rate: 0.0023915]
	Learning Rate: 0.00239148
	LOSS [training: 0.9886709023277731 | validation: 0.7148620053682431]
	TIME [epoch: 7.12 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5858595288406281		[learning rate: 0.0023858]
	Learning Rate: 0.00238584
	LOSS [training: 0.5858595288406281 | validation: 0.9530390316010728]
	TIME [epoch: 7.13 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.644222362102911		[learning rate: 0.0023802]
	Learning Rate: 0.00238021
	LOSS [training: 0.644222362102911 | validation: 0.5063202333655398]
	TIME [epoch: 7.13 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5134132803190348		[learning rate: 0.0023746]
	Learning Rate: 0.0023746
	LOSS [training: 0.5134132803190348 | validation: 1.3385744582535606]
	TIME [epoch: 7.13 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4364753782151154		[learning rate: 0.002369]
	Learning Rate: 0.00236899
	LOSS [training: 1.4364753782151154 | validation: 0.677530351314528]
	TIME [epoch: 7.16 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47170815487872997		[learning rate: 0.0023634]
	Learning Rate: 0.00236341
	LOSS [training: 0.47170815487872997 | validation: 0.4640854170152257]
	TIME [epoch: 7.13 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4907532485043249		[learning rate: 0.0023578]
	Learning Rate: 0.00235783
	LOSS [training: 0.4907532485043249 | validation: 0.5401544423144407]
	TIME [epoch: 7.12 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4661958567772812		[learning rate: 0.0023523]
	Learning Rate: 0.00235227
	LOSS [training: 0.4661958567772812 | validation: 0.5197204997046027]
	TIME [epoch: 7.12 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4382587686139391		[learning rate: 0.0023467]
	Learning Rate: 0.00234672
	LOSS [training: 0.4382587686139391 | validation: 0.3485001812632552]
	TIME [epoch: 7.12 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36128758914795994		[learning rate: 0.0023412]
	Learning Rate: 0.00234119
	LOSS [training: 0.36128758914795994 | validation: 0.9085138302427496]
	TIME [epoch: 7.12 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5331516205670488		[learning rate: 0.0023357]
	Learning Rate: 0.00233566
	LOSS [training: 0.5331516205670488 | validation: 0.4087848322068648]
	TIME [epoch: 7.15 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3808463092212784		[learning rate: 0.0023302]
	Learning Rate: 0.00233015
	LOSS [training: 0.3808463092212784 | validation: 0.4959258743874637]
	TIME [epoch: 7.13 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6041821138520366		[learning rate: 0.0023247]
	Learning Rate: 0.00232466
	LOSS [training: 0.6041821138520366 | validation: 0.464011686210944]
	TIME [epoch: 7.12 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5170598958489794		[learning rate: 0.0023192]
	Learning Rate: 0.00231917
	LOSS [training: 0.5170598958489794 | validation: 0.4563402760434933]
	TIME [epoch: 7.12 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5327564211121383		[learning rate: 0.0023137]
	Learning Rate: 0.0023137
	LOSS [training: 0.5327564211121383 | validation: 0.49477954285965464]
	TIME [epoch: 7.12 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4717349989012227		[learning rate: 0.0023082]
	Learning Rate: 0.00230825
	LOSS [training: 0.4717349989012227 | validation: 0.43345808566376465]
	TIME [epoch: 7.12 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49062867499142904		[learning rate: 0.0023028]
	Learning Rate: 0.0023028
	LOSS [training: 0.49062867499142904 | validation: 0.8978557234531653]
	TIME [epoch: 7.16 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6581149355405618		[learning rate: 0.0022974]
	Learning Rate: 0.00229737
	LOSS [training: 0.6581149355405618 | validation: 0.4587307256205815]
	TIME [epoch: 7.13 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41191219899769116		[learning rate: 0.0022919]
	Learning Rate: 0.00229195
	LOSS [training: 0.41191219899769116 | validation: 0.4044036035013279]
	TIME [epoch: 7.12 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48858615958220647		[learning rate: 0.0022865]
	Learning Rate: 0.00228654
	LOSS [training: 0.48858615958220647 | validation: 0.4277565931137974]
	TIME [epoch: 7.13 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5512748725930121		[learning rate: 0.0022811]
	Learning Rate: 0.00228115
	LOSS [training: 0.5512748725930121 | validation: 0.8636315565691274]
	TIME [epoch: 7.13 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6129699811515515		[learning rate: 0.0022758]
	Learning Rate: 0.00227577
	LOSS [training: 0.6129699811515515 | validation: 0.7442929352268142]
	TIME [epoch: 7.12 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5323392809535725		[learning rate: 0.0022704]
	Learning Rate: 0.0022704
	LOSS [training: 0.5323392809535725 | validation: 0.557672614221841]
	TIME [epoch: 7.16 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4497035935676461		[learning rate: 0.002265]
	Learning Rate: 0.00226505
	LOSS [training: 0.4497035935676461 | validation: 0.5607305178796962]
	TIME [epoch: 7.13 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48060714032936575		[learning rate: 0.0022597]
	Learning Rate: 0.0022597
	LOSS [training: 0.48060714032936575 | validation: 0.6287920812407575]
	TIME [epoch: 7.12 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45063540818948356		[learning rate: 0.0022544]
	Learning Rate: 0.00225437
	LOSS [training: 0.45063540818948356 | validation: 0.5307614164999778]
	TIME [epoch: 7.13 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4874141346098989		[learning rate: 0.0022491]
	Learning Rate: 0.00224905
	LOSS [training: 0.4874141346098989 | validation: 0.5644089448823821]
	TIME [epoch: 7.12 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.526038376324106		[learning rate: 0.0022437]
	Learning Rate: 0.00224375
	LOSS [training: 0.526038376324106 | validation: 0.35902456757339235]
	TIME [epoch: 7.11 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5134910407288062		[learning rate: 0.0022385]
	Learning Rate: 0.00223846
	LOSS [training: 0.5134910407288062 | validation: 0.3547189913244041]
	TIME [epoch: 7.16 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011178142637287		[learning rate: 0.0022332]
	Learning Rate: 0.00223318
	LOSS [training: 0.7011178142637287 | validation: 0.5694719398387316]
	TIME [epoch: 7.12 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.480284830845899		[learning rate: 0.0022279]
	Learning Rate: 0.00222791
	LOSS [training: 0.480284830845899 | validation: 0.8992349617915345]
	TIME [epoch: 7.12 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4455490871660784		[learning rate: 0.0022227]
	Learning Rate: 0.00222265
	LOSS [training: 0.4455490871660784 | validation: 0.39844325629332444]
	TIME [epoch: 7.11 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4985742879783488		[learning rate: 0.0022174]
	Learning Rate: 0.00221741
	LOSS [training: 0.4985742879783488 | validation: 0.3583415290206604]
	TIME [epoch: 7.13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36556867294811746		[learning rate: 0.0022122]
	Learning Rate: 0.00221218
	LOSS [training: 0.36556867294811746 | validation: 0.5504830773383546]
	TIME [epoch: 7.12 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5902925670566197		[learning rate: 0.002207]
	Learning Rate: 0.00220696
	LOSS [training: 0.5902925670566197 | validation: 0.840497775392261]
	TIME [epoch: 7.16 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5099373642695336		[learning rate: 0.0022018]
	Learning Rate: 0.00220176
	LOSS [training: 0.5099373642695336 | validation: 0.5240957261575376]
	TIME [epoch: 7.13 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4515859613401861		[learning rate: 0.0021966]
	Learning Rate: 0.00219656
	LOSS [training: 0.4515859613401861 | validation: 0.4370437670452778]
	TIME [epoch: 7.13 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44015502291097447		[learning rate: 0.0021914]
	Learning Rate: 0.00219138
	LOSS [training: 0.44015502291097447 | validation: 0.3141481607932453]
	TIME [epoch: 7.13 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44065270321858757		[learning rate: 0.0021862]
	Learning Rate: 0.00218621
	LOSS [training: 0.44065270321858757 | validation: 0.5814422116655495]
	TIME [epoch: 7.13 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45810902246387525		[learning rate: 0.0021811]
	Learning Rate: 0.00218106
	LOSS [training: 0.45810902246387525 | validation: 0.37538805513489204]
	TIME [epoch: 7.12 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5653228008538113		[learning rate: 0.0021759]
	Learning Rate: 0.00217591
	LOSS [training: 0.5653228008538113 | validation: 0.4703464261878789]
	TIME [epoch: 7.16 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5041545297597636		[learning rate: 0.0021708]
	Learning Rate: 0.00217078
	LOSS [training: 0.5041545297597636 | validation: 0.8203346104973167]
	TIME [epoch: 7.13 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8920645726387069		[learning rate: 0.0021657]
	Learning Rate: 0.00216566
	LOSS [training: 0.8920645726387069 | validation: 0.7144549893916198]
	TIME [epoch: 7.12 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7844544616508786		[learning rate: 0.0021605]
	Learning Rate: 0.00216055
	LOSS [training: 0.7844544616508786 | validation: 0.5932669923082665]
	TIME [epoch: 7.12 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.591105083711436		[learning rate: 0.0021555]
	Learning Rate: 0.00215545
	LOSS [training: 0.591105083711436 | validation: 0.931943706731402]
	TIME [epoch: 7.13 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6197385842334953		[learning rate: 0.0021504]
	Learning Rate: 0.00215037
	LOSS [training: 0.6197385842334953 | validation: 0.6539487636140047]
	TIME [epoch: 7.13 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5534086527735391		[learning rate: 0.0021453]
	Learning Rate: 0.0021453
	LOSS [training: 0.5534086527735391 | validation: 0.6438772531433107]
	TIME [epoch: 7.16 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5299477586216294		[learning rate: 0.0021402]
	Learning Rate: 0.00214024
	LOSS [training: 0.5299477586216294 | validation: 0.6850613736556392]
	TIME [epoch: 7.13 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5322045456224312		[learning rate: 0.0021352]
	Learning Rate: 0.00213519
	LOSS [training: 0.5322045456224312 | validation: 0.4199461656435205]
	TIME [epoch: 7.12 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5442274795340281		[learning rate: 0.0021302]
	Learning Rate: 0.00213015
	LOSS [training: 0.5442274795340281 | validation: 0.8411371115911286]
	TIME [epoch: 7.13 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5829161840490147		[learning rate: 0.0021251]
	Learning Rate: 0.00212513
	LOSS [training: 0.5829161840490147 | validation: 0.6168627990600135]
	TIME [epoch: 7.12 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5115731882535176		[learning rate: 0.0021201]
	Learning Rate: 0.00212011
	LOSS [training: 0.5115731882535176 | validation: 0.3207673513060866]
	TIME [epoch: 7.13 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5320624429065451		[learning rate: 0.0021151]
	Learning Rate: 0.00211511
	LOSS [training: 0.5320624429065451 | validation: 0.6502234309359469]
	TIME [epoch: 7.17 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42991859970637		[learning rate: 0.0021101]
	Learning Rate: 0.00211012
	LOSS [training: 0.42991859970637 | validation: 0.6767164200572814]
	TIME [epoch: 7.14 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4309526451763005		[learning rate: 0.0021051]
	Learning Rate: 0.00210515
	LOSS [training: 0.4309526451763005 | validation: 0.6348439816116349]
	TIME [epoch: 7.13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4886865297345981		[learning rate: 0.0021002]
	Learning Rate: 0.00210018
	LOSS [training: 0.4886865297345981 | validation: 0.5463383238330732]
	TIME [epoch: 7.13 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.619978390551414		[learning rate: 0.0020952]
	Learning Rate: 0.00209523
	LOSS [training: 0.619978390551414 | validation: 0.6782452642167385]
	TIME [epoch: 7.12 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287861393100552		[learning rate: 0.0020903]
	Learning Rate: 0.00209028
	LOSS [training: 0.6287861393100552 | validation: 0.47631618429335876]
	TIME [epoch: 7.12 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5391971286856589		[learning rate: 0.0020854]
	Learning Rate: 0.00208535
	LOSS [training: 0.5391971286856589 | validation: 0.7225716272724209]
	TIME [epoch: 7.16 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4772867385877559		[learning rate: 0.0020804]
	Learning Rate: 0.00208043
	LOSS [training: 0.4772867385877559 | validation: 0.47060824065145]
	TIME [epoch: 7.13 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5086669648741884		[learning rate: 0.0020755]
	Learning Rate: 0.00207553
	LOSS [training: 0.5086669648741884 | validation: 0.5273965485057621]
	TIME [epoch: 7.13 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5672685399142143		[learning rate: 0.0020706]
	Learning Rate: 0.00207063
	LOSS [training: 0.5672685399142143 | validation: 0.4008511742263342]
	TIME [epoch: 7.13 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.363465434897257		[learning rate: 0.0020657]
	Learning Rate: 0.00206575
	LOSS [training: 0.363465434897257 | validation: 0.5121055121606333]
	TIME [epoch: 7.13 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4517451029624617		[learning rate: 0.0020609]
	Learning Rate: 0.00206087
	LOSS [training: 0.4517451029624617 | validation: 0.5605110437716544]
	TIME [epoch: 7.13 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8141001402116433		[learning rate: 0.002056]
	Learning Rate: 0.00205601
	LOSS [training: 0.8141001402116433 | validation: 0.7681987287032924]
	TIME [epoch: 7.16 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5495065766388986		[learning rate: 0.0020512]
	Learning Rate: 0.00205116
	LOSS [training: 0.5495065766388986 | validation: 0.46457330292777876]
	TIME [epoch: 7.13 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39137063543511363		[learning rate: 0.0020463]
	Learning Rate: 0.00204632
	LOSS [training: 0.39137063543511363 | validation: 0.44230732826199737]
	TIME [epoch: 7.12 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41249075123963463		[learning rate: 0.0020415]
	Learning Rate: 0.0020415
	LOSS [training: 0.41249075123963463 | validation: 0.41258350241373126]
	TIME [epoch: 7.11 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35041231266655065		[learning rate: 0.0020367]
	Learning Rate: 0.00203668
	LOSS [training: 0.35041231266655065 | validation: 0.5139472449364333]
	TIME [epoch: 7.12 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5950190834270879		[learning rate: 0.0020319]
	Learning Rate: 0.00203188
	LOSS [training: 0.5950190834270879 | validation: 0.5597658705172619]
	TIME [epoch: 7.12 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4134456893553978		[learning rate: 0.0020271]
	Learning Rate: 0.00202708
	LOSS [training: 0.4134456893553978 | validation: 0.9112938012084347]
	TIME [epoch: 7.16 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5679886800494673		[learning rate: 0.0020223]
	Learning Rate: 0.0020223
	LOSS [training: 0.5679886800494673 | validation: 0.760388291449758]
	TIME [epoch: 7.12 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4423709429712381		[learning rate: 0.0020175]
	Learning Rate: 0.00201753
	LOSS [training: 0.4423709429712381 | validation: 0.47929605703435213]
	TIME [epoch: 7.13 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.527333521171088		[learning rate: 0.0020128]
	Learning Rate: 0.00201277
	LOSS [training: 0.527333521171088 | validation: 1.1689452478578184]
	TIME [epoch: 7.12 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7355784279621067		[learning rate: 0.002008]
	Learning Rate: 0.00200803
	LOSS [training: 0.7355784279621067 | validation: 0.3813116094911746]
	TIME [epoch: 7.12 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49799171193068237		[learning rate: 0.0020033]
	Learning Rate: 0.00200329
	LOSS [training: 0.49799171193068237 | validation: 0.44665426680512343]
	TIME [epoch: 7.12 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48890018030890187		[learning rate: 0.0019986]
	Learning Rate: 0.00199856
	LOSS [training: 0.48890018030890187 | validation: 0.6082911640475448]
	TIME [epoch: 7.16 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4195498537921402		[learning rate: 0.0019938]
	Learning Rate: 0.00199385
	LOSS [training: 0.4195498537921402 | validation: 0.39750356656977803]
	TIME [epoch: 7.12 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4541814186614019		[learning rate: 0.0019891]
	Learning Rate: 0.00198915
	LOSS [training: 0.4541814186614019 | validation: 0.5219583398284613]
	TIME [epoch: 7.13 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42488392795248614		[learning rate: 0.0019845]
	Learning Rate: 0.00198445
	LOSS [training: 0.42488392795248614 | validation: 0.7266819049246542]
	TIME [epoch: 7.12 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4600075336911937		[learning rate: 0.0019798]
	Learning Rate: 0.00197977
	LOSS [training: 0.4600075336911937 | validation: 0.4259797287996234]
	TIME [epoch: 7.13 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3456509535981702		[learning rate: 0.0019751]
	Learning Rate: 0.0019751
	LOSS [training: 0.3456509535981702 | validation: 0.5277495781233017]
	TIME [epoch: 7.12 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48019087922018777		[learning rate: 0.0019704]
	Learning Rate: 0.00197044
	LOSS [training: 0.48019087922018777 | validation: 0.6222223324459775]
	TIME [epoch: 7.16 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4770040416977336		[learning rate: 0.0019658]
	Learning Rate: 0.0019658
	LOSS [training: 0.4770040416977336 | validation: 0.35477625150429604]
	TIME [epoch: 7.13 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.601248127922842		[learning rate: 0.0019612]
	Learning Rate: 0.00196116
	LOSS [training: 0.601248127922842 | validation: 0.37526332339289575]
	TIME [epoch: 7.12 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43276168010987853		[learning rate: 0.0019565]
	Learning Rate: 0.00195653
	LOSS [training: 0.43276168010987853 | validation: 0.4674376108879757]
	TIME [epoch: 7.12 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3946779222754051		[learning rate: 0.0019519]
	Learning Rate: 0.00195192
	LOSS [training: 0.3946779222754051 | validation: 0.4176601551393002]
	TIME [epoch: 7.13 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31125009298497974		[learning rate: 0.0019473]
	Learning Rate: 0.00194731
	LOSS [training: 0.31125009298497974 | validation: 0.9642164558126518]
	TIME [epoch: 7.13 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49792894078587285		[learning rate: 0.0019427]
	Learning Rate: 0.00194272
	LOSS [training: 0.49792894078587285 | validation: 0.3018730889969327]
	TIME [epoch: 7.16 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36942408267782756		[learning rate: 0.0019381]
	Learning Rate: 0.00193814
	LOSS [training: 0.36942408267782756 | validation: 0.6761193072367166]
	TIME [epoch: 7.12 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5331733914705005		[learning rate: 0.0019336]
	Learning Rate: 0.00193357
	LOSS [training: 0.5331733914705005 | validation: 0.39545262907110384]
	TIME [epoch: 7.13 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.494408075650442		[learning rate: 0.001929]
	Learning Rate: 0.001929
	LOSS [training: 0.494408075650442 | validation: 0.5036415002542206]
	TIME [epoch: 7.12 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36488983422285015		[learning rate: 0.0019245]
	Learning Rate: 0.00192445
	LOSS [training: 0.36488983422285015 | validation: 0.42915506113737456]
	TIME [epoch: 7.12 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5976984195280102		[learning rate: 0.0019199]
	Learning Rate: 0.00191992
	LOSS [training: 0.5976984195280102 | validation: 0.37999214017035204]
	TIME [epoch: 7.12 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5756619069590639		[learning rate: 0.0019154]
	Learning Rate: 0.00191539
	LOSS [training: 0.5756619069590639 | validation: 0.39940186143634854]
	TIME [epoch: 7.15 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6753950724088937		[learning rate: 0.0019109]
	Learning Rate: 0.00191087
	LOSS [training: 0.6753950724088937 | validation: 0.6315390701646206]
	TIME [epoch: 7.12 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6278272399689557		[learning rate: 0.0019064]
	Learning Rate: 0.00190636
	LOSS [training: 0.6278272399689557 | validation: 0.8943122780761081]
	TIME [epoch: 7.19 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8850476886153013		[learning rate: 0.0019019]
	Learning Rate: 0.00190186
	LOSS [training: 0.8850476886153013 | validation: 1.1470676129164792]
	TIME [epoch: 7.12 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5905228471597993		[learning rate: 0.0018974]
	Learning Rate: 0.00189738
	LOSS [training: 0.5905228471597993 | validation: 0.5076582500532376]
	TIME [epoch: 7.13 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44809166844278836		[learning rate: 0.0018929]
	Learning Rate: 0.0018929
	LOSS [training: 0.44809166844278836 | validation: 0.3469923219081497]
	TIME [epoch: 7.12 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5074914959121601		[learning rate: 0.0018884]
	Learning Rate: 0.00188844
	LOSS [training: 0.5074914959121601 | validation: 0.413965729463872]
	TIME [epoch: 7.15 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4494676238674791		[learning rate: 0.001884]
	Learning Rate: 0.00188398
	LOSS [training: 0.4494676238674791 | validation: 0.37499463651098325]
	TIME [epoch: 7.12 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42058753086440753		[learning rate: 0.0018795]
	Learning Rate: 0.00187954
	LOSS [training: 0.42058753086440753 | validation: 0.6750352443472535]
	TIME [epoch: 7.13 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4712256361608179		[learning rate: 0.0018751]
	Learning Rate: 0.00187511
	LOSS [training: 0.4712256361608179 | validation: 0.44437085598672876]
	TIME [epoch: 7.12 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4016827797675253		[learning rate: 0.0018707]
	Learning Rate: 0.00187068
	LOSS [training: 0.4016827797675253 | validation: 0.247837923199767]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4122865986377394		[learning rate: 0.0018663]
	Learning Rate: 0.00186627
	LOSS [training: 0.4122865986377394 | validation: 0.4424813413230444]
	TIME [epoch: 7.13 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6469717138995534		[learning rate: 0.0018619]
	Learning Rate: 0.00186187
	LOSS [training: 0.6469717138995534 | validation: 1.6182733443361785]
	TIME [epoch: 7.14 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9764566873397816		[learning rate: 0.0018575]
	Learning Rate: 0.00185748
	LOSS [training: 0.9764566873397816 | validation: 0.5648012885528261]
	TIME [epoch: 7.12 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46628952413481334		[learning rate: 0.0018531]
	Learning Rate: 0.00185309
	LOSS [training: 0.46628952413481334 | validation: 0.3642387642873912]
	TIME [epoch: 7.12 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32761375402341275		[learning rate: 0.0018487]
	Learning Rate: 0.00184872
	LOSS [training: 0.32761375402341275 | validation: 0.519985236920858]
	TIME [epoch: 7.11 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4659719535891945		[learning rate: 0.0018444]
	Learning Rate: 0.00184436
	LOSS [training: 0.4659719535891945 | validation: 0.5366951825314131]
	TIME [epoch: 7.12 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46771160446123206		[learning rate: 0.00184]
	Learning Rate: 0.00184001
	LOSS [training: 0.46771160446123206 | validation: 0.5408159469154609]
	TIME [epoch: 7.13 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6569965670271566		[learning rate: 0.0018357]
	Learning Rate: 0.00183567
	LOSS [training: 0.6569965670271566 | validation: 0.469876689564943]
	TIME [epoch: 7.14 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5597260955230245		[learning rate: 0.0018313]
	Learning Rate: 0.00183134
	LOSS [training: 0.5597260955230245 | validation: 0.7368551424845944]
	TIME [epoch: 7.12 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5389887378549701		[learning rate: 0.001827]
	Learning Rate: 0.00182702
	LOSS [training: 0.5389887378549701 | validation: 0.5316900064053187]
	TIME [epoch: 7.12 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891050477659893		[learning rate: 0.0018227]
	Learning Rate: 0.00182271
	LOSS [training: 0.3891050477659893 | validation: 0.8901415564672461]
	TIME [epoch: 7.11 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7103436851555031		[learning rate: 0.0018184]
	Learning Rate: 0.00181841
	LOSS [training: 0.7103436851555031 | validation: 0.39840309180043254]
	TIME [epoch: 7.12 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3769546569333645		[learning rate: 0.0018141]
	Learning Rate: 0.00181412
	LOSS [training: 0.3769546569333645 | validation: 0.28167468811204455]
	TIME [epoch: 7.14 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41349536352477756		[learning rate: 0.0018098]
	Learning Rate: 0.00180984
	LOSS [training: 0.41349536352477756 | validation: 0.7239393141329618]
	TIME [epoch: 7.15 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5529337580027806		[learning rate: 0.0018056]
	Learning Rate: 0.00180557
	LOSS [training: 0.5529337580027806 | validation: 1.319995485295494]
	TIME [epoch: 7.13 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.752095888769308		[learning rate: 0.0018013]
	Learning Rate: 0.00180132
	LOSS [training: 0.752095888769308 | validation: 0.7366565904816048]
	TIME [epoch: 7.12 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5196044559616698		[learning rate: 0.0017971]
	Learning Rate: 0.00179707
	LOSS [training: 0.5196044559616698 | validation: 0.47189736001300026]
	TIME [epoch: 7.12 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.387596087964075		[learning rate: 0.0017928]
	Learning Rate: 0.00179283
	LOSS [training: 0.387596087964075 | validation: 0.48092471890210864]
	TIME [epoch: 7.11 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42146109111037966		[learning rate: 0.0017886]
	Learning Rate: 0.0017886
	LOSS [training: 0.42146109111037966 | validation: 0.4072333643211814]
	TIME [epoch: 7.13 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47347039079044106		[learning rate: 0.0017844]
	Learning Rate: 0.00178438
	LOSS [training: 0.47347039079044106 | validation: 0.6538935409083111]
	TIME [epoch: 7.14 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6216246195125488		[learning rate: 0.0017802]
	Learning Rate: 0.00178017
	LOSS [training: 0.6216246195125488 | validation: 0.9666867204600207]
	TIME [epoch: 7.12 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5869650319922781		[learning rate: 0.001776]
	Learning Rate: 0.00177597
	LOSS [training: 0.5869650319922781 | validation: 0.4560868482973667]
	TIME [epoch: 7.11 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4075406535028628		[learning rate: 0.0017718]
	Learning Rate: 0.00177178
	LOSS [training: 0.4075406535028628 | validation: 0.7171629504371173]
	TIME [epoch: 7.11 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41120812804186446		[learning rate: 0.0017676]
	Learning Rate: 0.0017676
	LOSS [training: 0.41120812804186446 | validation: 0.5671882362789284]
	TIME [epoch: 7.11 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6033166694661333		[learning rate: 0.0017634]
	Learning Rate: 0.00176343
	LOSS [training: 0.6033166694661333 | validation: 0.3734986161795798]
	TIME [epoch: 7.13 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4744049005134589		[learning rate: 0.0017593]
	Learning Rate: 0.00175927
	LOSS [training: 0.4744049005134589 | validation: 0.5690360892587208]
	TIME [epoch: 7.14 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4977520194837254		[learning rate: 0.0017551]
	Learning Rate: 0.00175512
	LOSS [training: 0.4977520194837254 | validation: 0.5883903401514272]
	TIME [epoch: 7.11 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5220272890702137		[learning rate: 0.001751]
	Learning Rate: 0.00175098
	LOSS [training: 0.5220272890702137 | validation: 0.5750520718993486]
	TIME [epoch: 7.11 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.425610701311333		[learning rate: 0.0017469]
	Learning Rate: 0.00174685
	LOSS [training: 0.425610701311333 | validation: 0.3856137489504513]
	TIME [epoch: 7.11 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5297043602366747		[learning rate: 0.0017427]
	Learning Rate: 0.00174273
	LOSS [training: 0.5297043602366747 | validation: 0.32105711243799634]
	TIME [epoch: 7.11 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4239307092188346		[learning rate: 0.0017386]
	Learning Rate: 0.00173862
	LOSS [training: 0.4239307092188346 | validation: 0.773180638478564]
	TIME [epoch: 7.13 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4609812187817943		[learning rate: 0.0017345]
	Learning Rate: 0.00173452
	LOSS [training: 0.4609812187817943 | validation: 0.5403016243727824]
	TIME [epoch: 7.15 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8793722652435356		[learning rate: 0.0017304]
	Learning Rate: 0.00173043
	LOSS [training: 0.8793722652435356 | validation: 0.57273970052408]
	TIME [epoch: 7.12 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5598367195177824		[learning rate: 0.0017263]
	Learning Rate: 0.00172635
	LOSS [training: 0.5598367195177824 | validation: 0.4366809577761676]
	TIME [epoch: 7.12 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4759537238392326		[learning rate: 0.0017223]
	Learning Rate: 0.00172228
	LOSS [training: 0.4759537238392326 | validation: 0.4254701721697518]
	TIME [epoch: 7.12 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6802833762933479		[learning rate: 0.0017182]
	Learning Rate: 0.00171821
	LOSS [training: 0.6802833762933479 | validation: 0.7564382322951386]
	TIME [epoch: 7.12 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4618545190972433		[learning rate: 0.0017142]
	Learning Rate: 0.00171416
	LOSS [training: 0.4618545190972433 | validation: 0.5426417863094934]
	TIME [epoch: 7.13 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41647188794902734		[learning rate: 0.0017101]
	Learning Rate: 0.00171012
	LOSS [training: 0.41647188794902734 | validation: 0.2786851859247972]
	TIME [epoch: 7.15 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4424128981454455		[learning rate: 0.0017061]
	Learning Rate: 0.00170608
	LOSS [training: 0.4424128981454455 | validation: 0.668367987692932]
	TIME [epoch: 7.12 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4863908996263267		[learning rate: 0.0017021]
	Learning Rate: 0.00170206
	LOSS [training: 0.4863908996263267 | validation: 0.44352151458005895]
	TIME [epoch: 7.12 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.512687817541902		[learning rate: 0.001698]
	Learning Rate: 0.00169804
	LOSS [training: 0.512687817541902 | validation: 0.4023361230853175]
	TIME [epoch: 7.11 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4013190211968568		[learning rate: 0.001694]
	Learning Rate: 0.00169404
	LOSS [training: 0.4013190211968568 | validation: 0.48975648734630095]
	TIME [epoch: 7.11 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4088623192644195		[learning rate: 0.00169]
	Learning Rate: 0.00169004
	LOSS [training: 0.4088623192644195 | validation: 0.27451248929453914]
	TIME [epoch: 7.12 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3702778759788854		[learning rate: 0.0016861]
	Learning Rate: 0.00168606
	LOSS [training: 0.3702778759788854 | validation: 0.6300749723400724]
	TIME [epoch: 7.14 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4329615135083045		[learning rate: 0.0016821]
	Learning Rate: 0.00168208
	LOSS [training: 0.4329615135083045 | validation: 0.40952543744566394]
	TIME [epoch: 7.11 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4167610606087626		[learning rate: 0.0016781]
	Learning Rate: 0.00167811
	LOSS [training: 0.4167610606087626 | validation: 0.35367159847031104]
	TIME [epoch: 7.11 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6371687394464157		[learning rate: 0.0016742]
	Learning Rate: 0.00167415
	LOSS [training: 0.6371687394464157 | validation: 0.36993816341603025]
	TIME [epoch: 7.11 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4765267001445067		[learning rate: 0.0016702]
	Learning Rate: 0.0016702
	LOSS [training: 0.4765267001445067 | validation: 0.5050152558129268]
	TIME [epoch: 7.11 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43482458457895196		[learning rate: 0.0016663]
	Learning Rate: 0.00166626
	LOSS [training: 0.43482458457895196 | validation: 0.2838734612430121]
	TIME [epoch: 7.12 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2842683343661658		[learning rate: 0.0016623]
	Learning Rate: 0.00166233
	LOSS [training: 0.2842683343661658 | validation: 0.3872555626466638]
	TIME [epoch: 7.15 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34757837015889914		[learning rate: 0.0016584]
	Learning Rate: 0.00165841
	LOSS [training: 0.34757837015889914 | validation: 0.4659338275183219]
	TIME [epoch: 7.11 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7409705427187754		[learning rate: 0.0016545]
	Learning Rate: 0.0016545
	LOSS [training: 0.7409705427187754 | validation: 0.7560691467825218]
	TIME [epoch: 7.11 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4952397564953383		[learning rate: 0.0016506]
	Learning Rate: 0.0016506
	LOSS [training: 0.4952397564953383 | validation: 0.3726815607483154]
	TIME [epoch: 7.11 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4273253475752873		[learning rate: 0.0016467]
	Learning Rate: 0.0016467
	LOSS [training: 0.4273253475752873 | validation: 0.3886529187982762]
	TIME [epoch: 7.11 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3217760730919794		[learning rate: 0.0016428]
	Learning Rate: 0.00164282
	LOSS [training: 0.3217760730919794 | validation: 0.3883692132413546]
	TIME [epoch: 7.13 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3519210575802627		[learning rate: 0.0016389]
	Learning Rate: 0.00163894
	LOSS [training: 0.3519210575802627 | validation: 0.6019822570306311]
	TIME [epoch: 7.14 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4548085017790683		[learning rate: 0.0016351]
	Learning Rate: 0.00163508
	LOSS [training: 0.4548085017790683 | validation: 0.6526079608254067]
	TIME [epoch: 7.12 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49519985475035994		[learning rate: 0.0016312]
	Learning Rate: 0.00163122
	LOSS [training: 0.49519985475035994 | validation: 0.3174020906522105]
	TIME [epoch: 7.11 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5614724629693446		[learning rate: 0.0016274]
	Learning Rate: 0.00162737
	LOSS [training: 0.5614724629693446 | validation: 0.6527833726499492]
	TIME [epoch: 7.12 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3945701489783183		[learning rate: 0.0016235]
	Learning Rate: 0.00162353
	LOSS [training: 0.3945701489783183 | validation: 0.7808167001578278]
	TIME [epoch: 7.11 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45779594767712295		[learning rate: 0.0016197]
	Learning Rate: 0.0016197
	LOSS [training: 0.45779594767712295 | validation: 0.42040639861760565]
	TIME [epoch: 7.13 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39052765402567535		[learning rate: 0.0016159]
	Learning Rate: 0.00161588
	LOSS [training: 0.39052765402567535 | validation: 0.5199564154819021]
	TIME [epoch: 7.14 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.431148322999565		[learning rate: 0.0016121]
	Learning Rate: 0.00161207
	LOSS [training: 0.431148322999565 | validation: 0.4627694578071534]
	TIME [epoch: 7.12 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3400918095401894		[learning rate: 0.0016083]
	Learning Rate: 0.00160827
	LOSS [training: 0.3400918095401894 | validation: 0.39440465303705]
	TIME [epoch: 7.11 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38386606882848084		[learning rate: 0.0016045]
	Learning Rate: 0.00160448
	LOSS [training: 0.38386606882848084 | validation: 0.5801863938678383]
	TIME [epoch: 7.11 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45264899468156433		[learning rate: 0.0016007]
	Learning Rate: 0.00160069
	LOSS [training: 0.45264899468156433 | validation: 0.5862298797977348]
	TIME [epoch: 7.11 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43414751164306226		[learning rate: 0.0015969]
	Learning Rate: 0.00159692
	LOSS [training: 0.43414751164306226 | validation: 0.31981120056931417]
	TIME [epoch: 7.13 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39830720411286924		[learning rate: 0.0015931]
	Learning Rate: 0.00159315
	LOSS [training: 0.39830720411286924 | validation: 0.38510408414568187]
	TIME [epoch: 7.14 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41602183304126666		[learning rate: 0.0015894]
	Learning Rate: 0.00158939
	LOSS [training: 0.41602183304126666 | validation: 0.39168532930242905]
	TIME [epoch: 7.11 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4845008125967512		[learning rate: 0.0015856]
	Learning Rate: 0.00158564
	LOSS [training: 0.4845008125967512 | validation: 0.5371128267755005]
	TIME [epoch: 7.11 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3775729449532895		[learning rate: 0.0015819]
	Learning Rate: 0.0015819
	LOSS [training: 0.3775729449532895 | validation: 0.296923574458376]
	TIME [epoch: 7.11 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3381387874452252		[learning rate: 0.0015782]
	Learning Rate: 0.00157817
	LOSS [training: 0.3381387874452252 | validation: 0.41365295016786524]
	TIME [epoch: 7.11 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4353858240655093		[learning rate: 0.0015744]
	Learning Rate: 0.00157445
	LOSS [training: 0.4353858240655093 | validation: 0.3623689281360292]
	TIME [epoch: 7.12 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4429592169851559		[learning rate: 0.0015707]
	Learning Rate: 0.00157073
	LOSS [training: 0.4429592169851559 | validation: 0.4068180537833858]
	TIME [epoch: 7.14 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36524948343075037		[learning rate: 0.001567]
	Learning Rate: 0.00156703
	LOSS [training: 0.36524948343075037 | validation: 0.3291611200680076]
	TIME [epoch: 7.12 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3453511966254042		[learning rate: 0.0015633]
	Learning Rate: 0.00156333
	LOSS [training: 0.3453511966254042 | validation: 0.3300996414353291]
	TIME [epoch: 7.12 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5339356030513637		[learning rate: 0.0015596]
	Learning Rate: 0.00155964
	LOSS [training: 0.5339356030513637 | validation: 0.7399996893368972]
	TIME [epoch: 7.11 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5261411201859676		[learning rate: 0.001556]
	Learning Rate: 0.00155597
	LOSS [training: 0.5261411201859676 | validation: 0.3929005280840926]
	TIME [epoch: 7.12 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5034033736147612		[learning rate: 0.0015523]
	Learning Rate: 0.0015523
	LOSS [training: 0.5034033736147612 | validation: 0.47008781544393924]
	TIME [epoch: 7.13 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49724593329935846		[learning rate: 0.0015486]
	Learning Rate: 0.00154863
	LOSS [training: 0.49724593329935846 | validation: 0.48070523726457914]
	TIME [epoch: 7.15 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6317347418258159		[learning rate: 0.001545]
	Learning Rate: 0.00154498
	LOSS [training: 0.6317347418258159 | validation: 0.4794904027669149]
	TIME [epoch: 7.12 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36488304213272793		[learning rate: 0.0015413]
	Learning Rate: 0.00154134
	LOSS [training: 0.36488304213272793 | validation: 0.4712165921899639]
	TIME [epoch: 7.12 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5694442824422292		[learning rate: 0.0015377]
	Learning Rate: 0.0015377
	LOSS [training: 0.5694442824422292 | validation: 0.528127008418142]
	TIME [epoch: 7.12 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38716099895982337		[learning rate: 0.0015341]
	Learning Rate: 0.00153407
	LOSS [training: 0.38716099895982337 | validation: 0.3006465350426292]
	TIME [epoch: 7.12 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3202689755679694		[learning rate: 0.0015305]
	Learning Rate: 0.00153045
	LOSS [training: 0.3202689755679694 | validation: 0.32665744176792766]
	TIME [epoch: 7.12 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3419887642961903		[learning rate: 0.0015268]
	Learning Rate: 0.00152684
	LOSS [training: 0.3419887642961903 | validation: 0.3830154887063873]
	TIME [epoch: 7.14 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40154133754109395		[learning rate: 0.0015232]
	Learning Rate: 0.00152324
	LOSS [training: 0.40154133754109395 | validation: 0.418798993417452]
	TIME [epoch: 7.11 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3820157908859666		[learning rate: 0.0015196]
	Learning Rate: 0.00151965
	LOSS [training: 0.3820157908859666 | validation: 0.7517221778722174]
	TIME [epoch: 7.12 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49156748009580004		[learning rate: 0.0015161]
	Learning Rate: 0.00151607
	LOSS [training: 0.49156748009580004 | validation: 0.5322992888606922]
	TIME [epoch: 7.11 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4161394959938634		[learning rate: 0.0015125]
	Learning Rate: 0.00151249
	LOSS [training: 0.4161394959938634 | validation: 0.4437477981029499]
	TIME [epoch: 7.12 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31388853111528725		[learning rate: 0.0015089]
	Learning Rate: 0.00150892
	LOSS [training: 0.31388853111528725 | validation: 0.2693378112159632]
	TIME [epoch: 7.12 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3969736626926851		[learning rate: 0.0015054]
	Learning Rate: 0.00150536
	LOSS [training: 0.3969736626926851 | validation: 0.3724577694363963]
	TIME [epoch: 7.14 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37655418909210725		[learning rate: 0.0015018]
	Learning Rate: 0.00150181
	LOSS [training: 0.37655418909210725 | validation: 0.44472756660701207]
	TIME [epoch: 7.11 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3697134248167419		[learning rate: 0.0014983]
	Learning Rate: 0.00149827
	LOSS [training: 0.3697134248167419 | validation: 0.4267649653324414]
	TIME [epoch: 7.11 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4098676200433822		[learning rate: 0.0014947]
	Learning Rate: 0.00149473
	LOSS [training: 0.4098676200433822 | validation: 0.5625532581010299]
	TIME [epoch: 7.11 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3973500997783832		[learning rate: 0.0014912]
	Learning Rate: 0.00149121
	LOSS [training: 0.3973500997783832 | validation: 0.3108585366950389]
	TIME [epoch: 7.12 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3163934614515274		[learning rate: 0.0014877]
	Learning Rate: 0.00148769
	LOSS [training: 0.3163934614515274 | validation: 0.37079229831163046]
	TIME [epoch: 7.13 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39093446567639245		[learning rate: 0.0014842]
	Learning Rate: 0.00148418
	LOSS [training: 0.39093446567639245 | validation: 0.3219581255570433]
	TIME [epoch: 7.14 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3331359977423257		[learning rate: 0.0014807]
	Learning Rate: 0.00148068
	LOSS [training: 0.3331359977423257 | validation: 0.516402756080983]
	TIME [epoch: 7.13 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4443830683901931		[learning rate: 0.0014772]
	Learning Rate: 0.00147719
	LOSS [training: 0.4443830683901931 | validation: 0.5645668682770545]
	TIME [epoch: 7.11 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38157644443623784		[learning rate: 0.0014737]
	Learning Rate: 0.0014737
	LOSS [training: 0.38157644443623784 | validation: 0.7631267655612831]
	TIME [epoch: 7.12 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4039485477135062		[learning rate: 0.0014702]
	Learning Rate: 0.00147023
	LOSS [training: 0.4039485477135062 | validation: 0.7765414272785538]
	TIME [epoch: 7.11 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4546706891628062		[learning rate: 0.0014668]
	Learning Rate: 0.00146676
	LOSS [training: 0.4546706891628062 | validation: 0.30115195878525136]
	TIME [epoch: 7.13 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.618398108137326		[learning rate: 0.0014633]
	Learning Rate: 0.0014633
	LOSS [training: 0.618398108137326 | validation: 0.436274025886464]
	TIME [epoch: 7.14 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45284174111300934		[learning rate: 0.0014598]
	Learning Rate: 0.00145985
	LOSS [training: 0.45284174111300934 | validation: 0.5153676036080821]
	TIME [epoch: 7.12 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32475354850003607		[learning rate: 0.0014564]
	Learning Rate: 0.0014564
	LOSS [training: 0.32475354850003607 | validation: 0.5328206639477214]
	TIME [epoch: 7.11 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3778967525556282		[learning rate: 0.001453]
	Learning Rate: 0.00145297
	LOSS [training: 0.3778967525556282 | validation: 0.259128551829397]
	TIME [epoch: 7.11 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6005954062363322		[learning rate: 0.0014495]
	Learning Rate: 0.00144954
	LOSS [training: 0.6005954062363322 | validation: 0.48119219708062]
	TIME [epoch: 7.11 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5654887531388846		[learning rate: 0.0014461]
	Learning Rate: 0.00144612
	LOSS [training: 0.5654887531388846 | validation: 0.43927372883910487]
	TIME [epoch: 7.13 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41479076466341774		[learning rate: 0.0014427]
	Learning Rate: 0.00144271
	LOSS [training: 0.41479076466341774 | validation: 0.30139888772574874]
	TIME [epoch: 7.14 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3513211114970337		[learning rate: 0.0014393]
	Learning Rate: 0.00143931
	LOSS [training: 0.3513211114970337 | validation: 0.8045788145360477]
	TIME [epoch: 7.11 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49854021124118963		[learning rate: 0.0014359]
	Learning Rate: 0.00143591
	LOSS [training: 0.49854021124118963 | validation: 0.39172434270464784]
	TIME [epoch: 7.11 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5359797704947876		[learning rate: 0.0014325]
	Learning Rate: 0.00143253
	LOSS [training: 0.5359797704947876 | validation: 0.9413570202993722]
	TIME [epoch: 7.11 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5766346435808349		[learning rate: 0.0014291]
	Learning Rate: 0.00142915
	LOSS [training: 0.5766346435808349 | validation: 0.6244642992488326]
	TIME [epoch: 7.11 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5607889528855355		[learning rate: 0.0014258]
	Learning Rate: 0.00142578
	LOSS [training: 0.5607889528855355 | validation: 0.32968695973942386]
	TIME [epoch: 7.13 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.319394042223921		[learning rate: 0.0014224]
	Learning Rate: 0.00142241
	LOSS [training: 0.319394042223921 | validation: 0.31935775197441535]
	TIME [epoch: 7.14 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2760590414223435		[learning rate: 0.0014191]
	Learning Rate: 0.00141906
	LOSS [training: 0.2760590414223435 | validation: 0.41071278017962465]
	TIME [epoch: 7.12 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36561023825482103		[learning rate: 0.0014157]
	Learning Rate: 0.00141571
	LOSS [training: 0.36561023825482103 | validation: 0.5245833947486985]
	TIME [epoch: 7.11 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3676594407285524		[learning rate: 0.0014124]
	Learning Rate: 0.00141237
	LOSS [training: 0.3676594407285524 | validation: 0.45874482823296714]
	TIME [epoch: 7.11 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33698050085791925		[learning rate: 0.001409]
	Learning Rate: 0.00140904
	LOSS [training: 0.33698050085791925 | validation: 0.5078643360160308]
	TIME [epoch: 7.11 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40781056195272863		[learning rate: 0.0014057]
	Learning Rate: 0.00140572
	LOSS [training: 0.40781056195272863 | validation: 0.3675535567663238]
	TIME [epoch: 7.13 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41705411887673804		[learning rate: 0.0014024]
	Learning Rate: 0.0014024
	LOSS [training: 0.41705411887673804 | validation: 0.4169668209322084]
	TIME [epoch: 7.13 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5643783481813137		[learning rate: 0.0013991]
	Learning Rate: 0.00139909
	LOSS [training: 0.5643783481813137 | validation: 0.39579945878660505]
	TIME [epoch: 7.11 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4149563886600312		[learning rate: 0.0013958]
	Learning Rate: 0.00139579
	LOSS [training: 0.4149563886600312 | validation: 0.5769492158065619]
	TIME [epoch: 7.12 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3501327486761975		[learning rate: 0.0013925]
	Learning Rate: 0.0013925
	LOSS [training: 0.3501327486761975 | validation: 0.28999387803455207]
	TIME [epoch: 7.11 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.362833748414502		[learning rate: 0.0013892]
	Learning Rate: 0.00138921
	LOSS [training: 0.362833748414502 | validation: 0.48691563415957717]
	TIME [epoch: 7.11 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4016356784120726		[learning rate: 0.0013859]
	Learning Rate: 0.00138594
	LOSS [training: 0.4016356784120726 | validation: 0.2699788488456587]
	TIME [epoch: 7.13 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4756597071306439		[learning rate: 0.0013827]
	Learning Rate: 0.00138267
	LOSS [training: 0.4756597071306439 | validation: 0.3628753749377741]
	TIME [epoch: 7.14 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3801223288209986		[learning rate: 0.0013794]
	Learning Rate: 0.00137941
	LOSS [training: 0.3801223288209986 | validation: 0.41400939920801816]
	TIME [epoch: 7.11 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45964089164174055		[learning rate: 0.0013762]
	Learning Rate: 0.00137615
	LOSS [training: 0.45964089164174055 | validation: 0.8648017747548173]
	TIME [epoch: 7.12 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49047868731150784		[learning rate: 0.0013729]
	Learning Rate: 0.00137291
	LOSS [training: 0.49047868731150784 | validation: 0.39103956854670985]
	TIME [epoch: 7.12 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2930174145468253		[learning rate: 0.0013697]
	Learning Rate: 0.00136967
	LOSS [training: 0.2930174145468253 | validation: 0.45883694335584324]
	TIME [epoch: 7.12 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40882924749286		[learning rate: 0.0013664]
	Learning Rate: 0.00136644
	LOSS [training: 0.40882924749286 | validation: 0.4306183152285611]
	TIME [epoch: 7.15 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37964354300892916		[learning rate: 0.0013632]
	Learning Rate: 0.00136321
	LOSS [training: 0.37964354300892916 | validation: 0.28246708596140413]
	TIME [epoch: 7.13 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5277228655436106		[learning rate: 0.00136]
	Learning Rate: 0.00136
	LOSS [training: 0.5277228655436106 | validation: 0.4269107896076727]
	TIME [epoch: 7.11 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35866920021583504		[learning rate: 0.0013568]
	Learning Rate: 0.00135679
	LOSS [training: 0.35866920021583504 | validation: 0.3179110878837085]
	TIME [epoch: 7.12 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3664171508663826		[learning rate: 0.0013536]
	Learning Rate: 0.00135359
	LOSS [training: 0.3664171508663826 | validation: 0.4065778637628409]
	TIME [epoch: 7.11 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37543832624782236		[learning rate: 0.0013504]
	Learning Rate: 0.0013504
	LOSS [training: 0.37543832624782236 | validation: 0.7158332903795468]
	TIME [epoch: 7.12 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5562052822800462		[learning rate: 0.0013472]
	Learning Rate: 0.00134721
	LOSS [training: 0.5562052822800462 | validation: 0.7495611992204425]
	TIME [epoch: 7.14 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3627739925619556		[learning rate: 0.001344]
	Learning Rate: 0.00134403
	LOSS [training: 0.3627739925619556 | validation: 0.2711220590050086]
	TIME [epoch: 7.13 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32891849971190407		[learning rate: 0.0013409]
	Learning Rate: 0.00134086
	LOSS [training: 0.32891849971190407 | validation: 0.40454833271910645]
	TIME [epoch: 7.14 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3851351630699981		[learning rate: 0.0013377]
	Learning Rate: 0.0013377
	LOSS [training: 0.3851351630699981 | validation: 0.3842036174262198]
	TIME [epoch: 7.12 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3117427243144582		[learning rate: 0.0013345]
	Learning Rate: 0.00133455
	LOSS [training: 0.3117427243144582 | validation: 0.2684672631947414]
	TIME [epoch: 7.11 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2961315110910635		[learning rate: 0.0013314]
	Learning Rate: 0.0013314
	LOSS [training: 0.2961315110910635 | validation: 0.4925991975223152]
	TIME [epoch: 7.11 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.550169655573721		[learning rate: 0.0013283]
	Learning Rate: 0.00132826
	LOSS [training: 0.550169655573721 | validation: 0.6358826805949024]
	TIME [epoch: 7.15 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39101580931199376		[learning rate: 0.0013251]
	Learning Rate: 0.00132512
	LOSS [training: 0.39101580931199376 | validation: 0.5053620588223087]
	TIME [epoch: 7.12 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40247426058285046		[learning rate: 0.001322]
	Learning Rate: 0.001322
	LOSS [training: 0.40247426058285046 | validation: 0.6931140476277289]
	TIME [epoch: 7.12 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45328737629347826		[learning rate: 0.0013189]
	Learning Rate: 0.00131888
	LOSS [training: 0.45328737629347826 | validation: 0.47442286555325053]
	TIME [epoch: 7.12 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4104202135058695		[learning rate: 0.0013158]
	Learning Rate: 0.00131577
	LOSS [training: 0.4104202135058695 | validation: 0.4012356211381389]
	TIME [epoch: 7.12 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3084651866841967		[learning rate: 0.0013127]
	Learning Rate: 0.00131266
	LOSS [training: 0.3084651866841967 | validation: 0.3022609258045715]
	TIME [epoch: 7.12 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3124460959915361		[learning rate: 0.0013096]
	Learning Rate: 0.00130957
	LOSS [training: 0.3124460959915361 | validation: 0.3392755655225223]
	TIME [epoch: 7.14 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38114237013174657		[learning rate: 0.0013065]
	Learning Rate: 0.00130648
	LOSS [training: 0.38114237013174657 | validation: 0.4075012317737901]
	TIME [epoch: 7.12 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32489250666742975		[learning rate: 0.0013034]
	Learning Rate: 0.0013034
	LOSS [training: 0.32489250666742975 | validation: 0.355195273587162]
	TIME [epoch: 7.11 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3380456420845013		[learning rate: 0.0013003]
	Learning Rate: 0.00130032
	LOSS [training: 0.3380456420845013 | validation: 0.24044493284929888]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31649697137668703		[learning rate: 0.0012973]
	Learning Rate: 0.00129726
	LOSS [training: 0.31649697137668703 | validation: 0.417770963639143]
	TIME [epoch: 7.12 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4698164888637306		[learning rate: 0.0012942]
	Learning Rate: 0.0012942
	LOSS [training: 0.4698164888637306 | validation: 0.36340085115029275]
	TIME [epoch: 7.11 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29594115709559504		[learning rate: 0.0012911]
	Learning Rate: 0.00129114
	LOSS [training: 0.29594115709559504 | validation: 0.38859841500584247]
	TIME [epoch: 7.14 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4023608246259675		[learning rate: 0.0012881]
	Learning Rate: 0.0012881
	LOSS [training: 0.4023608246259675 | validation: 0.5433008180540276]
	TIME [epoch: 7.12 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3616921459072071		[learning rate: 0.0012851]
	Learning Rate: 0.00128506
	LOSS [training: 0.3616921459072071 | validation: 0.32818442105651163]
	TIME [epoch: 7.11 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31339070569622396		[learning rate: 0.001282]
	Learning Rate: 0.00128203
	LOSS [training: 0.31339070569622396 | validation: 0.3293945290573387]
	TIME [epoch: 7.12 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42066766106617487		[learning rate: 0.001279]
	Learning Rate: 0.001279
	LOSS [training: 0.42066766106617487 | validation: 0.5154174671790799]
	TIME [epoch: 7.11 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3876340755854706		[learning rate: 0.001276]
	Learning Rate: 0.00127599
	LOSS [training: 0.3876340755854706 | validation: 0.5058206983130292]
	TIME [epoch: 7.11 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.722168372049145		[learning rate: 0.001273]
	Learning Rate: 0.00127298
	LOSS [training: 0.722168372049145 | validation: 0.2736889209604991]
	TIME [epoch: 7.15 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3523011532344361		[learning rate: 0.00127]
	Learning Rate: 0.00126997
	LOSS [training: 0.3523011532344361 | validation: 0.6527445830918189]
	TIME [epoch: 7.13 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3997512951633823		[learning rate: 0.001267]
	Learning Rate: 0.00126698
	LOSS [training: 0.3997512951633823 | validation: 0.33467880114349824]
	TIME [epoch: 7.11 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2612051846686512		[learning rate: 0.001264]
	Learning Rate: 0.00126399
	LOSS [training: 0.2612051846686512 | validation: 0.3885299668743067]
	TIME [epoch: 7.12 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3927208679272524		[learning rate: 0.001261]
	Learning Rate: 0.00126101
	LOSS [training: 0.3927208679272524 | validation: 0.40895248334110934]
	TIME [epoch: 7.11 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3688992209850045		[learning rate: 0.001258]
	Learning Rate: 0.00125803
	LOSS [training: 0.3688992209850045 | validation: 0.38384022099044196]
	TIME [epoch: 7.11 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34870771835414904		[learning rate: 0.0012551]
	Learning Rate: 0.00125507
	LOSS [training: 0.34870771835414904 | validation: 0.381927804354733]
	TIME [epoch: 7.14 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3518299867570321		[learning rate: 0.0012521]
	Learning Rate: 0.00125211
	LOSS [training: 0.3518299867570321 | validation: 0.27998363718728225]
	TIME [epoch: 7.12 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36081031678310316		[learning rate: 0.0012492]
	Learning Rate: 0.00124915
	LOSS [training: 0.36081031678310316 | validation: 0.39783834823138975]
	TIME [epoch: 7.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5646502316491679		[learning rate: 0.0012462]
	Learning Rate: 0.00124621
	LOSS [training: 0.5646502316491679 | validation: 0.37545374682674937]
	TIME [epoch: 7.11 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3835553474063427		[learning rate: 0.0012433]
	Learning Rate: 0.00124327
	LOSS [training: 0.3835553474063427 | validation: 0.3216290839271414]
	TIME [epoch: 7.11 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30943796213350433		[learning rate: 0.0012403]
	Learning Rate: 0.00124033
	LOSS [training: 0.30943796213350433 | validation: 0.3470662202746107]
	TIME [epoch: 7.11 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43133619639182286		[learning rate: 0.0012374]
	Learning Rate: 0.00123741
	LOSS [training: 0.43133619639182286 | validation: 0.3400830103155609]
	TIME [epoch: 7.14 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28072101502072333		[learning rate: 0.0012345]
	Learning Rate: 0.00123449
	LOSS [training: 0.28072101502072333 | validation: 0.42860176462685423]
	TIME [epoch: 7.13 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2951726757767027		[learning rate: 0.0012316]
	Learning Rate: 0.00123158
	LOSS [training: 0.2951726757767027 | validation: 0.341143567612915]
	TIME [epoch: 7.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4184686634108458		[learning rate: 0.0012287]
	Learning Rate: 0.00122867
	LOSS [training: 0.4184686634108458 | validation: 0.413758686565081]
	TIME [epoch: 7.11 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3415165424654712		[learning rate: 0.0012258]
	Learning Rate: 0.00122577
	LOSS [training: 0.3415165424654712 | validation: 0.26787332143907283]
	TIME [epoch: 7.11 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2769746490638837		[learning rate: 0.0012229]
	Learning Rate: 0.00122288
	LOSS [training: 0.2769746490638837 | validation: 0.38401794469647665]
	TIME [epoch: 7.11 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40666330286074626		[learning rate: 0.00122]
	Learning Rate: 0.00122
	LOSS [training: 0.40666330286074626 | validation: 0.896421204563113]
	TIME [epoch: 7.14 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5145846514327711		[learning rate: 0.0012171]
	Learning Rate: 0.00121712
	LOSS [training: 0.5145846514327711 | validation: 0.33748431798096534]
	TIME [epoch: 7.13 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3494295242206741		[learning rate: 0.0012142]
	Learning Rate: 0.00121425
	LOSS [training: 0.3494295242206741 | validation: 0.7524487358572096]
	TIME [epoch: 7.12 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4916908646415057		[learning rate: 0.0012114]
	Learning Rate: 0.00121138
	LOSS [training: 0.4916908646415057 | validation: 0.4165249899000579]
	TIME [epoch: 7.12 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4491051501500415		[learning rate: 0.0012085]
	Learning Rate: 0.00120853
	LOSS [training: 0.4491051501500415 | validation: 0.7742825654898225]
	TIME [epoch: 7.12 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6505800343405298		[learning rate: 0.0012057]
	Learning Rate: 0.00120568
	LOSS [training: 0.6505800343405298 | validation: 0.40679610110902054]
	TIME [epoch: 7.11 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3205984765914923		[learning rate: 0.0012028]
	Learning Rate: 0.00120283
	LOSS [training: 0.3205984765914923 | validation: 0.25877793847690345]
	TIME [epoch: 7.14 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3386895352178951		[learning rate: 0.0012]
	Learning Rate: 0.0012
	LOSS [training: 0.3386895352178951 | validation: 0.47218706958157913]
	TIME [epoch: 7.11 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35657294855792887		[learning rate: 0.0011972]
	Learning Rate: 0.00119716
	LOSS [training: 0.35657294855792887 | validation: 0.3861327956671494]
	TIME [epoch: 7.11 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3177103423920542		[learning rate: 0.0011943]
	Learning Rate: 0.00119434
	LOSS [training: 0.3177103423920542 | validation: 0.3910139300171729]
	TIME [epoch: 7.11 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36053700652425624		[learning rate: 0.0011915]
	Learning Rate: 0.00119152
	LOSS [training: 0.36053700652425624 | validation: 0.27449862006554426]
	TIME [epoch: 7.11 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28394289480176477		[learning rate: 0.0011887]
	Learning Rate: 0.00118871
	LOSS [training: 0.28394289480176477 | validation: 0.3996310506115126]
	TIME [epoch: 7.11 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34326342180806985		[learning rate: 0.0011859]
	Learning Rate: 0.00118591
	LOSS [training: 0.34326342180806985 | validation: 0.28332109883624096]
	TIME [epoch: 7.14 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3199658886425761		[learning rate: 0.0011831]
	Learning Rate: 0.00118311
	LOSS [training: 0.3199658886425761 | validation: 0.48592077723094507]
	TIME [epoch: 7.11 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31898813263273146		[learning rate: 0.0011803]
	Learning Rate: 0.00118032
	LOSS [training: 0.31898813263273146 | validation: 0.46282573094511187]
	TIME [epoch: 7.11 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3659524320079796		[learning rate: 0.0011775]
	Learning Rate: 0.00117754
	LOSS [training: 0.3659524320079796 | validation: 0.3017183426406834]
	TIME [epoch: 7.11 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3383592512296586		[learning rate: 0.0011748]
	Learning Rate: 0.00117476
	LOSS [training: 0.3383592512296586 | validation: 0.29341294117236427]
	TIME [epoch: 7.11 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2704330548967648		[learning rate: 0.001172]
	Learning Rate: 0.00117199
	LOSS [training: 0.2704330548967648 | validation: 0.4742515412541658]
	TIME [epoch: 7.11 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4907646238378427		[learning rate: 0.0011692]
	Learning Rate: 0.00116922
	LOSS [training: 0.4907646238378427 | validation: 0.6014712356826828]
	TIME [epoch: 7.15 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3889731071760081		[learning rate: 0.0011665]
	Learning Rate: 0.00116646
	LOSS [training: 0.3889731071760081 | validation: 0.7096947150857593]
	TIME [epoch: 7.12 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48507596797673386		[learning rate: 0.0011637]
	Learning Rate: 0.00116371
	LOSS [training: 0.48507596797673386 | validation: 0.3846911407749856]
	TIME [epoch: 7.12 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29139015234327803		[learning rate: 0.001161]
	Learning Rate: 0.00116097
	LOSS [training: 0.29139015234327803 | validation: 0.5042651057754914]
	TIME [epoch: 7.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34555132370487285		[learning rate: 0.0011582]
	Learning Rate: 0.00115823
	LOSS [training: 0.34555132370487285 | validation: 0.3295807319884391]
	TIME [epoch: 7.12 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2905258950904168		[learning rate: 0.0011555]
	Learning Rate: 0.0011555
	LOSS [training: 0.2905258950904168 | validation: 0.34645268035413945]
	TIME [epoch: 7.11 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32922340005151857		[learning rate: 0.0011528]
	Learning Rate: 0.00115277
	LOSS [training: 0.32922340005151857 | validation: 0.352083614918037]
	TIME [epoch: 7.15 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32825965324431416		[learning rate: 0.0011501]
	Learning Rate: 0.00115005
	LOSS [training: 0.32825965324431416 | validation: 0.5019671952319286]
	TIME [epoch: 7.12 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4053410124029412		[learning rate: 0.0011473]
	Learning Rate: 0.00114734
	LOSS [training: 0.4053410124029412 | validation: 0.2661110003966698]
	TIME [epoch: 7.11 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27925888094659673		[learning rate: 0.0011446]
	Learning Rate: 0.00114463
	LOSS [training: 0.27925888094659673 | validation: 0.3023489734627646]
	TIME [epoch: 7.11 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3531673179439528		[learning rate: 0.0011419]
	Learning Rate: 0.00114193
	LOSS [training: 0.3531673179439528 | validation: 0.5935673519583586]
	TIME [epoch: 7.11 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4342637845443411		[learning rate: 0.0011392]
	Learning Rate: 0.00113924
	LOSS [training: 0.4342637845443411 | validation: 0.5915990905758496]
	TIME [epoch: 7.11 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39229598813834077		[learning rate: 0.0011366]
	Learning Rate: 0.00113655
	LOSS [training: 0.39229598813834077 | validation: 0.41337571146894847]
	TIME [epoch: 7.14 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3924117415433003		[learning rate: 0.0011339]
	Learning Rate: 0.00113387
	LOSS [training: 0.3924117415433003 | validation: 0.28620233950275237]
	TIME [epoch: 7.12 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46751260070882117		[learning rate: 0.0011312]
	Learning Rate: 0.0011312
	LOSS [training: 0.46751260070882117 | validation: 0.46670311509736745]
	TIME [epoch: 7.11 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5131671619194649		[learning rate: 0.0011285]
	Learning Rate: 0.00112853
	LOSS [training: 0.5131671619194649 | validation: 0.5100618480006518]
	TIME [epoch: 7.12 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3444567389283635		[learning rate: 0.0011259]
	Learning Rate: 0.00112587
	LOSS [training: 0.3444567389283635 | validation: 0.321427509186191]
	TIME [epoch: 7.12 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3610068913986895		[learning rate: 0.0011232]
	Learning Rate: 0.00112321
	LOSS [training: 0.3610068913986895 | validation: 0.3275754361533175]
	TIME [epoch: 7.13 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26489107468302053		[learning rate: 0.0011206]
	Learning Rate: 0.00112056
	LOSS [training: 0.26489107468302053 | validation: 0.2818126091716818]
	TIME [epoch: 7.16 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.294060498745099		[learning rate: 0.0011179]
	Learning Rate: 0.00111792
	LOSS [training: 0.294060498745099 | validation: 0.7288656827653742]
	TIME [epoch: 7.13 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.454379749637935		[learning rate: 0.0011153]
	Learning Rate: 0.00111528
	LOSS [training: 0.454379749637935 | validation: 0.3720436116199394]
	TIME [epoch: 7.12 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2870772274837357		[learning rate: 0.0011127]
	Learning Rate: 0.00111265
	LOSS [training: 0.2870772274837357 | validation: 0.30142762531032474]
	TIME [epoch: 7.12 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2795815378068395		[learning rate: 0.00111]
	Learning Rate: 0.00111003
	LOSS [training: 0.2795815378068395 | validation: 0.2769643005287997]
	TIME [epoch: 7.12 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2995787475449994		[learning rate: 0.0011074]
	Learning Rate: 0.00110741
	LOSS [training: 0.2995787475449994 | validation: 0.2866146769660858]
	TIME [epoch: 7.11 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26107950963925475		[learning rate: 0.0011048]
	Learning Rate: 0.0011048
	LOSS [training: 0.26107950963925475 | validation: 0.30592011421304843]
	TIME [epoch: 7.15 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38214102653146104		[learning rate: 0.0011022]
	Learning Rate: 0.00110219
	LOSS [training: 0.38214102653146104 | validation: 0.326468021337202]
	TIME [epoch: 7.13 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33050574177697767		[learning rate: 0.0010996]
	Learning Rate: 0.00109959
	LOSS [training: 0.33050574177697767 | validation: 0.40196882022747743]
	TIME [epoch: 7.12 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32694575471620557		[learning rate: 0.001097]
	Learning Rate: 0.001097
	LOSS [training: 0.32694575471620557 | validation: 0.4833778070075532]
	TIME [epoch: 7.12 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33530225996401547		[learning rate: 0.0010944]
	Learning Rate: 0.00109441
	LOSS [training: 0.33530225996401547 | validation: 0.4606733833791456]
	TIME [epoch: 7.12 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4379963789964795		[learning rate: 0.0010918]
	Learning Rate: 0.00109183
	LOSS [training: 0.4379963789964795 | validation: 0.28372313198324606]
	TIME [epoch: 7.12 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28319590052208854		[learning rate: 0.0010893]
	Learning Rate: 0.00108925
	LOSS [training: 0.28319590052208854 | validation: 0.43099475752846417]
	TIME [epoch: 7.16 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32444532746589905		[learning rate: 0.0010867]
	Learning Rate: 0.00108668
	LOSS [training: 0.32444532746589905 | validation: 0.44222032381983356]
	TIME [epoch: 7.12 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44613820325635045		[learning rate: 0.0010841]
	Learning Rate: 0.00108412
	LOSS [training: 0.44613820325635045 | validation: 0.29661014069735375]
	TIME [epoch: 7.12 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2774164154747879		[learning rate: 0.0010816]
	Learning Rate: 0.00108156
	LOSS [training: 0.2774164154747879 | validation: 0.2924980871655883]
	TIME [epoch: 7.11 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3452068025932546		[learning rate: 0.001079]
	Learning Rate: 0.00107901
	LOSS [training: 0.3452068025932546 | validation: 0.25032436856266516]
	TIME [epoch: 7.12 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3313035906041179		[learning rate: 0.0010765]
	Learning Rate: 0.00107647
	LOSS [training: 0.3313035906041179 | validation: 0.4142792531447517]
	TIME [epoch: 7.11 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38505417343793036		[learning rate: 0.0010739]
	Learning Rate: 0.00107393
	LOSS [training: 0.38505417343793036 | validation: 0.32860772318780995]
	TIME [epoch: 7.15 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2751967824157943		[learning rate: 0.0010714]
	Learning Rate: 0.00107139
	LOSS [training: 0.2751967824157943 | validation: 0.49313917012291875]
	TIME [epoch: 7.12 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33710783721105		[learning rate: 0.0010689]
	Learning Rate: 0.00106887
	LOSS [training: 0.33710783721105 | validation: 0.346859831352516]
	TIME [epoch: 7.13 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33708380537740856		[learning rate: 0.0010663]
	Learning Rate: 0.00106634
	LOSS [training: 0.33708380537740856 | validation: 0.35905840621231144]
	TIME [epoch: 7.12 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.269218717460511		[learning rate: 0.0010638]
	Learning Rate: 0.00106383
	LOSS [training: 0.269218717460511 | validation: 0.4842389965130286]
	TIME [epoch: 7.12 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3793224729208031		[learning rate: 0.0010613]
	Learning Rate: 0.00106132
	LOSS [training: 0.3793224729208031 | validation: 0.4017118132773645]
	TIME [epoch: 7.11 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32305003202839283		[learning rate: 0.0010588]
	Learning Rate: 0.00105882
	LOSS [training: 0.32305003202839283 | validation: 0.3776990486727291]
	TIME [epoch: 7.15 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31664608514243653		[learning rate: 0.0010563]
	Learning Rate: 0.00105632
	LOSS [training: 0.31664608514243653 | validation: 0.2608247482998136]
	TIME [epoch: 7.12 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28455610334229964		[learning rate: 0.0010538]
	Learning Rate: 0.00105383
	LOSS [training: 0.28455610334229964 | validation: 0.26033072205162566]
	TIME [epoch: 7.12 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.317930700677252		[learning rate: 0.0010513]
	Learning Rate: 0.00105134
	LOSS [training: 0.317930700677252 | validation: 0.49063019477082925]
	TIME [epoch: 7.11 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4668012399023791		[learning rate: 0.0010489]
	Learning Rate: 0.00104886
	LOSS [training: 0.4668012399023791 | validation: 0.656748391095795]
	TIME [epoch: 7.11 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44687896150134304		[learning rate: 0.0010464]
	Learning Rate: 0.00104639
	LOSS [training: 0.44687896150134304 | validation: 0.6638689713183754]
	TIME [epoch: 7.11 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4842965835502998		[learning rate: 0.0010439]
	Learning Rate: 0.00104392
	LOSS [training: 0.4842965835502998 | validation: 0.27262764777067033]
	TIME [epoch: 7.15 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3081179355085347		[learning rate: 0.0010415]
	Learning Rate: 0.00104146
	LOSS [training: 0.3081179355085347 | validation: 0.39466542995278786]
	TIME [epoch: 7.13 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3348622273224312		[learning rate: 0.001039]
	Learning Rate: 0.001039
	LOSS [training: 0.3348622273224312 | validation: 0.3541346533297185]
	TIME [epoch: 7.12 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3776429637717357		[learning rate: 0.0010365]
	Learning Rate: 0.00103655
	LOSS [training: 0.3776429637717357 | validation: 0.32820852507000803]
	TIME [epoch: 7.13 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2870363389800731		[learning rate: 0.0010341]
	Learning Rate: 0.0010341
	LOSS [training: 0.2870363389800731 | validation: 0.5213188566388487]
	TIME [epoch: 7.11 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3221847482436978		[learning rate: 0.0010317]
	Learning Rate: 0.00103166
	LOSS [training: 0.3221847482436978 | validation: 0.3126889110087047]
	TIME [epoch: 7.13 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31054794805490654		[learning rate: 0.0010292]
	Learning Rate: 0.00102923
	LOSS [training: 0.31054794805490654 | validation: 0.3977436159866933]
	TIME [epoch: 7.15 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41591522551592597		[learning rate: 0.0010268]
	Learning Rate: 0.0010268
	LOSS [training: 0.41591522551592597 | validation: 0.4072677368374533]
	TIME [epoch: 7.12 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3107554510843844		[learning rate: 0.0010244]
	Learning Rate: 0.00102438
	LOSS [training: 0.3107554510843844 | validation: 0.35903871605371707]
	TIME [epoch: 7.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2711234463455784		[learning rate: 0.001022]
	Learning Rate: 0.00102196
	LOSS [training: 0.2711234463455784 | validation: 0.6641708865773581]
	TIME [epoch: 7.11 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090682973098986		[learning rate: 0.0010196]
	Learning Rate: 0.00101955
	LOSS [training: 0.5090682973098986 | validation: 0.2503654536260347]
	TIME [epoch: 7.11 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2731914994902407		[learning rate: 0.0010171]
	Learning Rate: 0.00101715
	LOSS [training: 0.2731914994902407 | validation: 0.36996334046040946]
	TIME [epoch: 7.12 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4936125413653928		[learning rate: 0.0010147]
	Learning Rate: 0.00101475
	LOSS [training: 0.4936125413653928 | validation: 0.4548589503024883]
	TIME [epoch: 7.15 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40653825609449756		[learning rate: 0.0010124]
	Learning Rate: 0.00101236
	LOSS [training: 0.40653825609449756 | validation: 0.5407814027409819]
	TIME [epoch: 7.12 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35337900306599734		[learning rate: 0.00101]
	Learning Rate: 0.00100997
	LOSS [training: 0.35337900306599734 | validation: 0.3915924939873915]
	TIME [epoch: 7.11 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3768174452579687		[learning rate: 0.0010076]
	Learning Rate: 0.00100759
	LOSS [training: 0.3768174452579687 | validation: 0.30366258184563594]
	TIME [epoch: 7.12 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3283243112705162		[learning rate: 0.0010052]
	Learning Rate: 0.00100521
	LOSS [training: 0.3283243112705162 | validation: 0.2418988553426391]
	TIME [epoch: 7.11 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30320617875722056		[learning rate: 0.0010028]
	Learning Rate: 0.00100284
	LOSS [training: 0.30320617875722056 | validation: 0.3723950784950265]
	TIME [epoch: 7.11 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3299316905633562		[learning rate: 0.0010005]
	Learning Rate: 0.00100047
	LOSS [training: 0.3299316905633562 | validation: 0.4817105041116754]
	TIME [epoch: 7.16 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3360426537785309		[learning rate: 0.00099811]
	Learning Rate: 0.000998112
	LOSS [training: 0.3360426537785309 | validation: 0.26806102900513673]
	TIME [epoch: 7.13 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3401274569235535		[learning rate: 0.00099576]
	Learning Rate: 0.000995758
	LOSS [training: 0.3401274569235535 | validation: 0.5900966437610471]
	TIME [epoch: 7.12 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34314128726576576		[learning rate: 0.00099341]
	Learning Rate: 0.000993409
	LOSS [training: 0.34314128726576576 | validation: 0.2403946273622969]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26933545139429516		[learning rate: 0.00099107]
	Learning Rate: 0.000991066
	LOSS [training: 0.26933545139429516 | validation: 0.2582750746720952]
	TIME [epoch: 7.12 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37363831759796035		[learning rate: 0.00098873]
	Learning Rate: 0.000988728
	LOSS [training: 0.37363831759796035 | validation: 0.4825055155069767]
	TIME [epoch: 7.12 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.349658573357473		[learning rate: 0.0009864]
	Learning Rate: 0.000986396
	LOSS [training: 0.349658573357473 | validation: 0.5069615553541172]
	TIME [epoch: 7.15 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31456506430199493		[learning rate: 0.00098407]
	Learning Rate: 0.000984069
	LOSS [training: 0.31456506430199493 | validation: 0.24494332467480529]
	TIME [epoch: 7.12 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26814689456397195		[learning rate: 0.00098175]
	Learning Rate: 0.000981748
	LOSS [training: 0.26814689456397195 | validation: 0.35452748643409937]
	TIME [epoch: 7.11 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36092622668729507		[learning rate: 0.00097943]
	Learning Rate: 0.000979432
	LOSS [training: 0.36092622668729507 | validation: 0.3705610399156076]
	TIME [epoch: 7.11 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3192127915268693		[learning rate: 0.00097712]
	Learning Rate: 0.000977122
	LOSS [training: 0.3192127915268693 | validation: 0.31548296103596096]
	TIME [epoch: 7.11 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3431037817907946		[learning rate: 0.00097482]
	Learning Rate: 0.000974817
	LOSS [training: 0.3431037817907946 | validation: 0.5113924549004469]
	TIME [epoch: 7.11 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34851628517288924		[learning rate: 0.00097252]
	Learning Rate: 0.000972517
	LOSS [training: 0.34851628517288924 | validation: 0.38423768744220654]
	TIME [epoch: 7.15 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39185493810947086		[learning rate: 0.00097022]
	Learning Rate: 0.000970224
	LOSS [training: 0.39185493810947086 | validation: 0.5242644913021839]
	TIME [epoch: 7.11 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3476498919474494		[learning rate: 0.00096793]
	Learning Rate: 0.000967935
	LOSS [training: 0.3476498919474494 | validation: 0.38168599599756753]
	TIME [epoch: 7.11 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3189283213203912		[learning rate: 0.00096565]
	Learning Rate: 0.000965652
	LOSS [training: 0.3189283213203912 | validation: 0.2846640542994469]
	TIME [epoch: 7.11 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2881316148731338		[learning rate: 0.00096337]
	Learning Rate: 0.000963374
	LOSS [training: 0.2881316148731338 | validation: 0.3925932647278936]
	TIME [epoch: 7.11 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2904877924366992		[learning rate: 0.0009611]
	Learning Rate: 0.000961101
	LOSS [training: 0.2904877924366992 | validation: 0.2631872135133641]
	TIME [epoch: 7.12 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25502583867725337		[learning rate: 0.00095883]
	Learning Rate: 0.000958834
	LOSS [training: 0.25502583867725337 | validation: 0.29317836922310525]
	TIME [epoch: 7.15 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2871773415065143		[learning rate: 0.00095657]
	Learning Rate: 0.000956572
	LOSS [training: 0.2871773415065143 | validation: 0.3458990430877412]
	TIME [epoch: 7.12 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34768329647239987		[learning rate: 0.00095432]
	Learning Rate: 0.000954316
	LOSS [training: 0.34768329647239987 | validation: 0.3081339251964474]
	TIME [epoch: 7.12 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29237935684198585		[learning rate: 0.00095207]
	Learning Rate: 0.000952065
	LOSS [training: 0.29237935684198585 | validation: 0.35849072343278543]
	TIME [epoch: 7.11 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3010421323852413		[learning rate: 0.00094982]
	Learning Rate: 0.000949819
	LOSS [training: 0.3010421323852413 | validation: 0.26132027435803384]
	TIME [epoch: 7.11 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2282742061881727		[learning rate: 0.00094758]
	Learning Rate: 0.000947579
	LOSS [training: 0.2282742061881727 | validation: 0.2471886463390203]
	TIME [epoch: 7.11 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3188794295071949		[learning rate: 0.00094534]
	Learning Rate: 0.000945344
	LOSS [training: 0.3188794295071949 | validation: 0.40403850785914214]
	TIME [epoch: 7.15 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3296610809992513		[learning rate: 0.00094311]
	Learning Rate: 0.000943114
	LOSS [training: 0.3296610809992513 | validation: 0.38674408493334655]
	TIME [epoch: 7.11 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32074772709417265		[learning rate: 0.00094089]
	Learning Rate: 0.000940889
	LOSS [training: 0.32074772709417265 | validation: 0.28769912571952316]
	TIME [epoch: 7.11 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3175717052904718		[learning rate: 0.00093867]
	Learning Rate: 0.00093867
	LOSS [training: 0.3175717052904718 | validation: 0.22494541622952585]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1052.pth
	Model improved!!!
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26605173165550255		[learning rate: 0.00093646]
	Learning Rate: 0.000936456
	LOSS [training: 0.26605173165550255 | validation: 0.24827388111616078]
	TIME [epoch: 7.12 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35914723383110553		[learning rate: 0.00093425]
	Learning Rate: 0.000934246
	LOSS [training: 0.35914723383110553 | validation: 0.35110655001395236]
	TIME [epoch: 7.12 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3218228144417369		[learning rate: 0.00093204]
	Learning Rate: 0.000932043
	LOSS [training: 0.3218228144417369 | validation: 0.3118760693887581]
	TIME [epoch: 7.15 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2848875578458204		[learning rate: 0.00092984]
	Learning Rate: 0.000929844
	LOSS [training: 0.2848875578458204 | validation: 0.302130379654558]
	TIME [epoch: 7.11 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2762614021795454		[learning rate: 0.00092765]
	Learning Rate: 0.000927651
	LOSS [training: 0.2762614021795454 | validation: 0.25607248466150934]
	TIME [epoch: 7.11 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23189976014694094		[learning rate: 0.00092546]
	Learning Rate: 0.000925463
	LOSS [training: 0.23189976014694094 | validation: 0.3368403304379729]
	TIME [epoch: 7.12 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2479290071214865		[learning rate: 0.00092328]
	Learning Rate: 0.00092328
	LOSS [training: 0.2479290071214865 | validation: 0.23772112635387022]
	TIME [epoch: 7.11 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25003891908172504		[learning rate: 0.0009211]
	Learning Rate: 0.000921102
	LOSS [training: 0.25003891908172504 | validation: 0.26781239813228275]
	TIME [epoch: 7.13 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28502043236403696		[learning rate: 0.00091893]
	Learning Rate: 0.000918929
	LOSS [training: 0.28502043236403696 | validation: 0.37915618997887607]
	TIME [epoch: 7.15 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31746906647407946		[learning rate: 0.00091676]
	Learning Rate: 0.000916762
	LOSS [training: 0.31746906647407946 | validation: 0.3724240936907069]
	TIME [epoch: 7.12 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2706432765999297		[learning rate: 0.0009146]
	Learning Rate: 0.000914599
	LOSS [training: 0.2706432765999297 | validation: 0.24477209035185382]
	TIME [epoch: 7.11 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2461347272298454		[learning rate: 0.00091244]
	Learning Rate: 0.000912442
	LOSS [training: 0.2461347272298454 | validation: 0.31374562953798035]
	TIME [epoch: 7.12 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30841639560317435		[learning rate: 0.00091029]
	Learning Rate: 0.000910289
	LOSS [training: 0.30841639560317435 | validation: 0.2484179504739279]
	TIME [epoch: 7.11 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26262435066688167		[learning rate: 0.00090814]
	Learning Rate: 0.000908142
	LOSS [training: 0.26262435066688167 | validation: 0.4166093493861293]
	TIME [epoch: 7.12 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24376446846406746		[learning rate: 0.000906]
	Learning Rate: 0.000906
	LOSS [training: 0.24376446846406746 | validation: 0.2872007242150838]
	TIME [epoch: 7.14 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27644678372301384		[learning rate: 0.00090386]
	Learning Rate: 0.000903863
	LOSS [training: 0.27644678372301384 | validation: 0.3839979186707356]
	TIME [epoch: 7.12 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32501012983171373		[learning rate: 0.00090173]
	Learning Rate: 0.000901731
	LOSS [training: 0.32501012983171373 | validation: 0.2948715824072693]
	TIME [epoch: 7.11 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29890892840682926		[learning rate: 0.0008996]
	Learning Rate: 0.000899604
	LOSS [training: 0.29890892840682926 | validation: 0.22530808159553953]
	TIME [epoch: 7.12 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24065918241972434		[learning rate: 0.00089748]
	Learning Rate: 0.000897482
	LOSS [training: 0.24065918241972434 | validation: 0.34334120977275695]
	TIME [epoch: 7.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2321262193650769		[learning rate: 0.00089536]
	Learning Rate: 0.000895365
	LOSS [training: 0.2321262193650769 | validation: 0.24278143237934185]
	TIME [epoch: 7.12 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24429698417129847		[learning rate: 0.00089325]
	Learning Rate: 0.000893253
	LOSS [training: 0.24429698417129847 | validation: 0.6366215929342215]
	TIME [epoch: 7.14 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4142108786979895		[learning rate: 0.00089115]
	Learning Rate: 0.000891146
	LOSS [training: 0.4142108786979895 | validation: 0.3590461344058342]
	TIME [epoch: 7.12 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3215127755984223		[learning rate: 0.00088904]
	Learning Rate: 0.000889044
	LOSS [training: 0.3215127755984223 | validation: 0.3332359242574058]
	TIME [epoch: 7.12 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.352886963643828		[learning rate: 0.00088695]
	Learning Rate: 0.000886946
	LOSS [training: 0.352886963643828 | validation: 0.3831838313732532]
	TIME [epoch: 7.12 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30052310562427137		[learning rate: 0.00088485]
	Learning Rate: 0.000884854
	LOSS [training: 0.30052310562427137 | validation: 0.7109728714146923]
	TIME [epoch: 7.12 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5069358965769706		[learning rate: 0.00088277]
	Learning Rate: 0.000882767
	LOSS [training: 0.5069358965769706 | validation: 0.3933413874673283]
	TIME [epoch: 7.14 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2664420420774487		[learning rate: 0.00088068]
	Learning Rate: 0.000880685
	LOSS [training: 0.2664420420774487 | validation: 0.39578417629836277]
	TIME [epoch: 7.14 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.365971619207995		[learning rate: 0.00087861]
	Learning Rate: 0.000878607
	LOSS [training: 0.365971619207995 | validation: 0.343351515664828]
	TIME [epoch: 7.11 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2875759790692926		[learning rate: 0.00087653]
	Learning Rate: 0.000876535
	LOSS [training: 0.2875759790692926 | validation: 0.5249289507631346]
	TIME [epoch: 7.12 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4478966707590999		[learning rate: 0.00087447]
	Learning Rate: 0.000874467
	LOSS [training: 0.4478966707590999 | validation: 0.5289847833967143]
	TIME [epoch: 7.11 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3730291482787077		[learning rate: 0.0008724]
	Learning Rate: 0.000872405
	LOSS [training: 0.3730291482787077 | validation: 0.2977775513952783]
	TIME [epoch: 7.11 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25344643792348504		[learning rate: 0.00087035]
	Learning Rate: 0.000870346
	LOSS [training: 0.25344643792348504 | validation: 0.2619012357246354]
	TIME [epoch: 7.13 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33205693984764423		[learning rate: 0.00086829]
	Learning Rate: 0.000868294
	LOSS [training: 0.33205693984764423 | validation: 0.32111510976793606]
	TIME [epoch: 7.15 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21506985068576318		[learning rate: 0.00086625]
	Learning Rate: 0.000866246
	LOSS [training: 0.21506985068576318 | validation: 0.3974578321306683]
	TIME [epoch: 7.11 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2887096077582662		[learning rate: 0.0008642]
	Learning Rate: 0.000864202
	LOSS [training: 0.2887096077582662 | validation: 0.3607152716066719]
	TIME [epoch: 7.11 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24743829011247107		[learning rate: 0.00086216]
	Learning Rate: 0.000862164
	LOSS [training: 0.24743829011247107 | validation: 0.3144527036007919]
	TIME [epoch: 7.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2371456764841044		[learning rate: 0.00086013]
	Learning Rate: 0.00086013
	LOSS [training: 0.2371456764841044 | validation: 0.359591076776006]
	TIME [epoch: 7.12 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32855480000002585		[learning rate: 0.0008581]
	Learning Rate: 0.000858101
	LOSS [training: 0.32855480000002585 | validation: 0.2857316543484405]
	TIME [epoch: 7.12 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23118448331015923		[learning rate: 0.00085608]
	Learning Rate: 0.000856077
	LOSS [training: 0.23118448331015923 | validation: 0.3834509674221652]
	TIME [epoch: 7.14 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3444631495116723		[learning rate: 0.00085406]
	Learning Rate: 0.000854058
	LOSS [training: 0.3444631495116723 | validation: 0.4707446100762244]
	TIME [epoch: 7.12 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3628332056684639		[learning rate: 0.00085204]
	Learning Rate: 0.000852043
	LOSS [training: 0.3628332056684639 | validation: 0.2320151951147652]
	TIME [epoch: 7.12 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3018421452743957		[learning rate: 0.00085003]
	Learning Rate: 0.000850033
	LOSS [training: 0.3018421452743957 | validation: 0.34830730179862573]
	TIME [epoch: 7.11 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2846792846738443		[learning rate: 0.00084803]
	Learning Rate: 0.000848028
	LOSS [training: 0.2846792846738443 | validation: 0.38124138230050275]
	TIME [epoch: 7.12 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150245261472304		[learning rate: 0.00084603]
	Learning Rate: 0.000846028
	LOSS [training: 0.6150245261472304 | validation: 0.752390855245806]
	TIME [epoch: 7.12 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.515627844978188		[learning rate: 0.00084403]
	Learning Rate: 0.000844032
	LOSS [training: 0.515627844978188 | validation: 0.41864584694877316]
	TIME [epoch: 7.13 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29039578755528295		[learning rate: 0.00084204]
	Learning Rate: 0.000842041
	LOSS [training: 0.29039578755528295 | validation: 0.29219948673571217]
	TIME [epoch: 7.11 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40823180830026384		[learning rate: 0.00084005]
	Learning Rate: 0.000840055
	LOSS [training: 0.40823180830026384 | validation: 0.4629357927442438]
	TIME [epoch: 7.11 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3618315641895863		[learning rate: 0.00083807]
	Learning Rate: 0.000838073
	LOSS [training: 0.3618315641895863 | validation: 0.3899060482726132]
	TIME [epoch: 7.11 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2590627765068339		[learning rate: 0.0008361]
	Learning Rate: 0.000836096
	LOSS [training: 0.2590627765068339 | validation: 0.31804373066483527]
	TIME [epoch: 7.11 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2594707767375975		[learning rate: 0.00083412]
	Learning Rate: 0.000834124
	LOSS [training: 0.2594707767375975 | validation: 0.21794977635065022]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2756931350260556		[learning rate: 0.00083216]
	Learning Rate: 0.000832157
	LOSS [training: 0.2756931350260556 | validation: 0.3765273220484146]
	TIME [epoch: 7.14 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38046676151694425		[learning rate: 0.00083019]
	Learning Rate: 0.000830194
	LOSS [training: 0.38046676151694425 | validation: 0.6721039709375636]
	TIME [epoch: 7.11 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.431477920233462		[learning rate: 0.00082824]
	Learning Rate: 0.000828236
	LOSS [training: 0.431477920233462 | validation: 0.41946628883495063]
	TIME [epoch: 7.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3963350882288727		[learning rate: 0.00082628]
	Learning Rate: 0.000826282
	LOSS [training: 0.3963350882288727 | validation: 0.3931749969404]
	TIME [epoch: 7.11 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3578957512224423		[learning rate: 0.00082433]
	Learning Rate: 0.000824333
	LOSS [training: 0.3578957512224423 | validation: 0.3035671349205293]
	TIME [epoch: 7.11 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3072308128799742		[learning rate: 0.00082239]
	Learning Rate: 0.000822388
	LOSS [training: 0.3072308128799742 | validation: 0.4162587649198626]
	TIME [epoch: 7.12 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2823946783074726		[learning rate: 0.00082045]
	Learning Rate: 0.000820448
	LOSS [training: 0.2823946783074726 | validation: 0.4531765888723849]
	TIME [epoch: 7.14 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3143174363576934		[learning rate: 0.00081851]
	Learning Rate: 0.000818513
	LOSS [training: 0.3143174363576934 | validation: 0.39368407091340285]
	TIME [epoch: 7.12 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25937973259201164		[learning rate: 0.00081658]
	Learning Rate: 0.000816582
	LOSS [training: 0.25937973259201164 | validation: 0.21025929459872333]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24906514260419202		[learning rate: 0.00081466]
	Learning Rate: 0.000814656
	LOSS [training: 0.24906514260419202 | validation: 0.2956185931820615]
	TIME [epoch: 7.12 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2813019460226755		[learning rate: 0.00081273]
	Learning Rate: 0.000812734
	LOSS [training: 0.2813019460226755 | validation: 0.5482897280824399]
	TIME [epoch: 7.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4068551861587294		[learning rate: 0.00081082]
	Learning Rate: 0.000810817
	LOSS [training: 0.4068551861587294 | validation: 0.31004975829222003]
	TIME [epoch: 7.12 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3700100470425426		[learning rate: 0.0008089]
	Learning Rate: 0.000808905
	LOSS [training: 0.3700100470425426 | validation: 0.38317406679596144]
	TIME [epoch: 7.12 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30426542245732213		[learning rate: 0.000807]
	Learning Rate: 0.000806997
	LOSS [training: 0.30426542245732213 | validation: 0.2525369790772413]
	TIME [epoch: 7.11 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.373908518804947		[learning rate: 0.00080509]
	Learning Rate: 0.000805093
	LOSS [training: 0.373908518804947 | validation: 0.2588343081600708]
	TIME [epoch: 7.1 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28213319832636397		[learning rate: 0.00080319]
	Learning Rate: 0.000803194
	LOSS [training: 0.28213319832636397 | validation: 0.2919598908969695]
	TIME [epoch: 7.11 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30876350201772584		[learning rate: 0.0008013]
	Learning Rate: 0.000801299
	LOSS [training: 0.30876350201772584 | validation: 0.5005889423650387]
	TIME [epoch: 7.11 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3653985280187052		[learning rate: 0.00079941]
	Learning Rate: 0.000799409
	LOSS [training: 0.3653985280187052 | validation: 0.26913597497711506]
	TIME [epoch: 7.12 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.342349892970833		[learning rate: 0.00079752]
	Learning Rate: 0.000797524
	LOSS [training: 0.342349892970833 | validation: 0.32532615162150913]
	TIME [epoch: 7.13 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28120819373101946		[learning rate: 0.00079564]
	Learning Rate: 0.000795642
	LOSS [training: 0.28120819373101946 | validation: 0.2925889605491133]
	TIME [epoch: 7.11 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.250772250326836		[learning rate: 0.00079377]
	Learning Rate: 0.000793766
	LOSS [training: 0.250772250326836 | validation: 0.26170753579556477]
	TIME [epoch: 7.11 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2656981346799176		[learning rate: 0.00079189]
	Learning Rate: 0.000791893
	LOSS [training: 0.2656981346799176 | validation: 0.22565324207461748]
	TIME [epoch: 7.12 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2711900749967323		[learning rate: 0.00079003]
	Learning Rate: 0.000790025
	LOSS [training: 0.2711900749967323 | validation: 0.3951675402636853]
	TIME [epoch: 7.11 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3547832760292227		[learning rate: 0.00078816]
	Learning Rate: 0.000788162
	LOSS [training: 0.3547832760292227 | validation: 0.4104407833180629]
	TIME [epoch: 7.13 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25660667066072185		[learning rate: 0.0007863]
	Learning Rate: 0.000786302
	LOSS [training: 0.25660667066072185 | validation: 0.3121811824994625]
	TIME [epoch: 7.14 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4341480746016394		[learning rate: 0.00078445]
	Learning Rate: 0.000784448
	LOSS [training: 0.4341480746016394 | validation: 0.5501277929087723]
	TIME [epoch: 7.12 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31697443589136676		[learning rate: 0.0007826]
	Learning Rate: 0.000782598
	LOSS [training: 0.31697443589136676 | validation: 0.2849063969603161]
	TIME [epoch: 7.12 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31733821587434524		[learning rate: 0.00078075]
	Learning Rate: 0.000780751
	LOSS [training: 0.31733821587434524 | validation: 0.3223961817528278]
	TIME [epoch: 7.11 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3116202013022621		[learning rate: 0.00077891]
	Learning Rate: 0.00077891
	LOSS [training: 0.3116202013022621 | validation: 0.38098070146595886]
	TIME [epoch: 7.11 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3503191556845612		[learning rate: 0.00077707]
	Learning Rate: 0.000777073
	LOSS [training: 0.3503191556845612 | validation: 0.3313015911347219]
	TIME [epoch: 7.13 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32215911205158254		[learning rate: 0.00077524]
	Learning Rate: 0.000775239
	LOSS [training: 0.32215911205158254 | validation: 0.47994281338247285]
	TIME [epoch: 7.14 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3348864643407628		[learning rate: 0.00077341]
	Learning Rate: 0.000773411
	LOSS [training: 0.3348864643407628 | validation: 0.37335183979447606]
	TIME [epoch: 7.11 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26844071857616064		[learning rate: 0.00077159]
	Learning Rate: 0.000771586
	LOSS [training: 0.26844071857616064 | validation: 0.23051736642952445]
	TIME [epoch: 7.12 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.265755391293896		[learning rate: 0.00076977]
	Learning Rate: 0.000769766
	LOSS [training: 0.265755391293896 | validation: 0.4338657980531734]
	TIME [epoch: 7.11 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3605869780116364		[learning rate: 0.00076795]
	Learning Rate: 0.000767951
	LOSS [training: 0.3605869780116364 | validation: 0.2623119036778356]
	TIME [epoch: 7.11 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26656506446300865		[learning rate: 0.00076614]
	Learning Rate: 0.000766139
	LOSS [training: 0.26656506446300865 | validation: 0.2740602949788514]
	TIME [epoch: 7.14 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2509657252777755		[learning rate: 0.00076433]
	Learning Rate: 0.000764332
	LOSS [training: 0.2509657252777755 | validation: 0.4060750624648808]
	TIME [epoch: 7.12 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29933421758687506		[learning rate: 0.00076253]
	Learning Rate: 0.000762529
	LOSS [training: 0.29933421758687506 | validation: 0.29816188494499407]
	TIME [epoch: 7.11 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23771429573143976		[learning rate: 0.00076073]
	Learning Rate: 0.00076073
	LOSS [training: 0.23771429573143976 | validation: 0.23001625830675781]
	TIME [epoch: 7.12 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28069253779457315		[learning rate: 0.00075894]
	Learning Rate: 0.000758936
	LOSS [training: 0.28069253779457315 | validation: 0.6323108244412521]
	TIME [epoch: 7.12 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43066588091892494		[learning rate: 0.00075715]
	Learning Rate: 0.000757146
	LOSS [training: 0.43066588091892494 | validation: 0.2221810852461117]
	TIME [epoch: 7.11 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26378757761774285		[learning rate: 0.00075536]
	Learning Rate: 0.00075536
	LOSS [training: 0.26378757761774285 | validation: 0.26635217554678936]
	TIME [epoch: 7.16 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3298339842927827		[learning rate: 0.00075358]
	Learning Rate: 0.000753578
	LOSS [training: 0.3298339842927827 | validation: 0.229155860539291]
	TIME [epoch: 7.13 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.272150271601451		[learning rate: 0.0007518]
	Learning Rate: 0.0007518
	LOSS [training: 0.272150271601451 | validation: 0.22743014589781918]
	TIME [epoch: 7.11 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24113412357036573		[learning rate: 0.00075003]
	Learning Rate: 0.000750027
	LOSS [training: 0.24113412357036573 | validation: 0.2346059709464094]
	TIME [epoch: 7.11 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24000654739186436		[learning rate: 0.00074826]
	Learning Rate: 0.000748258
	LOSS [training: 0.24000654739186436 | validation: 0.35429931900722406]
	TIME [epoch: 7.12 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2527179518700804		[learning rate: 0.00074649]
	Learning Rate: 0.000746493
	LOSS [training: 0.2527179518700804 | validation: 0.4784273139068753]
	TIME [epoch: 7.12 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4080560013805137		[learning rate: 0.00074473]
	Learning Rate: 0.000744732
	LOSS [training: 0.4080560013805137 | validation: 0.4075641652163738]
	TIME [epoch: 7.15 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2988002570838711		[learning rate: 0.00074298]
	Learning Rate: 0.000742975
	LOSS [training: 0.2988002570838711 | validation: 0.3398124886989202]
	TIME [epoch: 7.12 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2973965976264804		[learning rate: 0.00074122]
	Learning Rate: 0.000741223
	LOSS [training: 0.2973965976264804 | validation: 0.24789257661542294]
	TIME [epoch: 7.12 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2944976403127201		[learning rate: 0.00073947]
	Learning Rate: 0.000739474
	LOSS [training: 0.2944976403127201 | validation: 0.2891421062379807]
	TIME [epoch: 7.11 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2569864398819243		[learning rate: 0.00073773]
	Learning Rate: 0.00073773
	LOSS [training: 0.2569864398819243 | validation: 0.38769791346426286]
	TIME [epoch: 7.12 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39556492279017963		[learning rate: 0.00073599]
	Learning Rate: 0.00073599
	LOSS [training: 0.39556492279017963 | validation: 0.3830803988334238]
	TIME [epoch: 7.11 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28257078801205493		[learning rate: 0.00073425]
	Learning Rate: 0.000734254
	LOSS [training: 0.28257078801205493 | validation: 0.24605804873338066]
	TIME [epoch: 7.15 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4002948229556408		[learning rate: 0.00073252]
	Learning Rate: 0.000732522
	LOSS [training: 0.4002948229556408 | validation: 0.5602022785020782]
	TIME [epoch: 7.12 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3625934401460537		[learning rate: 0.00073079]
	Learning Rate: 0.000730794
	LOSS [training: 0.3625934401460537 | validation: 0.4472468562251134]
	TIME [epoch: 7.12 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27644056713159293		[learning rate: 0.00072907]
	Learning Rate: 0.00072907
	LOSS [training: 0.27644056713159293 | validation: 0.2822198428534464]
	TIME [epoch: 7.12 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21822458919086127		[learning rate: 0.00072735]
	Learning Rate: 0.00072735
	LOSS [training: 0.21822458919086127 | validation: 0.21799958845776202]
	TIME [epoch: 7.13 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29225552487772105		[learning rate: 0.00072563]
	Learning Rate: 0.000725634
	LOSS [training: 0.29225552487772105 | validation: 0.2522191980883216]
	TIME [epoch: 7.11 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23321769695997213		[learning rate: 0.00072392]
	Learning Rate: 0.000723923
	LOSS [training: 0.23321769695997213 | validation: 0.47705148224699157]
	TIME [epoch: 7.15 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3269680800069293		[learning rate: 0.00072222]
	Learning Rate: 0.000722215
	LOSS [training: 0.3269680800069293 | validation: 0.25350759413885393]
	TIME [epoch: 7.11 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2091227711312058		[learning rate: 0.00072051]
	Learning Rate: 0.000720512
	LOSS [training: 0.2091227711312058 | validation: 0.24324031569886914]
	TIME [epoch: 7.11 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22788096064753968		[learning rate: 0.00071881]
	Learning Rate: 0.000718812
	LOSS [training: 0.22788096064753968 | validation: 0.339683927097847]
	TIME [epoch: 7.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28897320239054736		[learning rate: 0.00071712]
	Learning Rate: 0.000717117
	LOSS [training: 0.28897320239054736 | validation: 0.3108376001091178]
	TIME [epoch: 7.12 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22460136599365016		[learning rate: 0.00071542]
	Learning Rate: 0.000715425
	LOSS [training: 0.22460136599365016 | validation: 0.23639989125311434]
	TIME [epoch: 7.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42548253229466093		[learning rate: 0.00071374]
	Learning Rate: 0.000713738
	LOSS [training: 0.42548253229466093 | validation: 0.6564806808287892]
	TIME [epoch: 7.13 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47526048953892985		[learning rate: 0.00071205]
	Learning Rate: 0.000712054
	LOSS [training: 0.47526048953892985 | validation: 0.27915702539846393]
	TIME [epoch: 7.12 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2735713231888067		[learning rate: 0.00071037]
	Learning Rate: 0.000710374
	LOSS [training: 0.2735713231888067 | validation: 0.3219353293019744]
	TIME [epoch: 7.11 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2571257956323713		[learning rate: 0.0007087]
	Learning Rate: 0.000708698
	LOSS [training: 0.2571257956323713 | validation: 0.39239177410592585]
	TIME [epoch: 7.11 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33506386951595796		[learning rate: 0.00070703]
	Learning Rate: 0.000707027
	LOSS [training: 0.33506386951595796 | validation: 0.34943208118230884]
	TIME [epoch: 7.11 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25919642233104984		[learning rate: 0.00070536]
	Learning Rate: 0.000705359
	LOSS [training: 0.25919642233104984 | validation: 0.22676547388810497]
	TIME [epoch: 7.11 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2147483775978814		[learning rate: 0.0007037]
	Learning Rate: 0.000703695
	LOSS [training: 0.2147483775978814 | validation: 0.2892464103464442]
	TIME [epoch: 7.14 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2803925262153961		[learning rate: 0.00070204]
	Learning Rate: 0.000702035
	LOSS [training: 0.2803925262153961 | validation: 0.2933887001833619]
	TIME [epoch: 7.12 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23996378260451748		[learning rate: 0.00070038]
	Learning Rate: 0.000700379
	LOSS [training: 0.23996378260451748 | validation: 0.22668955593249396]
	TIME [epoch: 7.11 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25684607854360836		[learning rate: 0.00069873]
	Learning Rate: 0.000698727
	LOSS [training: 0.25684607854360836 | validation: 0.2996986709544577]
	TIME [epoch: 7.12 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2713470764702125		[learning rate: 0.00069708]
	Learning Rate: 0.000697079
	LOSS [training: 0.2713470764702125 | validation: 0.31593875955223694]
	TIME [epoch: 7.11 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24586717782917766		[learning rate: 0.00069543]
	Learning Rate: 0.000695435
	LOSS [training: 0.24586717782917766 | validation: 0.2739149711360213]
	TIME [epoch: 7.12 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25260696244969283		[learning rate: 0.00069379]
	Learning Rate: 0.000693794
	LOSS [training: 0.25260696244969283 | validation: 0.28578947615382194]
	TIME [epoch: 7.14 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2307844845943099		[learning rate: 0.00069216]
	Learning Rate: 0.000692158
	LOSS [training: 0.2307844845943099 | validation: 0.26515037366474864]
	TIME [epoch: 7.12 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2177179972001956		[learning rate: 0.00069053]
	Learning Rate: 0.000690525
	LOSS [training: 0.2177179972001956 | validation: 0.25760737125259514]
	TIME [epoch: 7.11 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29281078298654994		[learning rate: 0.0006889]
	Learning Rate: 0.000688896
	LOSS [training: 0.29281078298654994 | validation: 0.48898815283827957]
	TIME [epoch: 7.11 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6027509542245683		[learning rate: 0.00068727]
	Learning Rate: 0.000687271
	LOSS [training: 0.6027509542245683 | validation: 0.4197226314820056]
	TIME [epoch: 7.12 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34886150906564617		[learning rate: 0.00068565]
	Learning Rate: 0.00068565
	LOSS [training: 0.34886150906564617 | validation: 0.42206542289746424]
	TIME [epoch: 7.11 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35199440409492677		[learning rate: 0.00068403]
	Learning Rate: 0.000684033
	LOSS [training: 0.35199440409492677 | validation: 0.2634348262332725]
	TIME [epoch: 7.14 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24733300882605755		[learning rate: 0.00068242]
	Learning Rate: 0.000682419
	LOSS [training: 0.24733300882605755 | validation: 0.28509286630726827]
	TIME [epoch: 7.12 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21492987431729876		[learning rate: 0.00068081]
	Learning Rate: 0.00068081
	LOSS [training: 0.21492987431729876 | validation: 0.2462104685046278]
	TIME [epoch: 7.11 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2691634777454313		[learning rate: 0.0006792]
	Learning Rate: 0.000679204
	LOSS [training: 0.2691634777454313 | validation: 0.24503008114103203]
	TIME [epoch: 7.11 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3237617030116767		[learning rate: 0.0006776]
	Learning Rate: 0.000677601
	LOSS [training: 0.3237617030116767 | validation: 0.24558980136426725]
	TIME [epoch: 7.12 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22223313920191856		[learning rate: 0.000676]
	Learning Rate: 0.000676003
	LOSS [training: 0.22223313920191856 | validation: 0.39759499445821245]
	TIME [epoch: 7.11 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31588625489720845		[learning rate: 0.00067441]
	Learning Rate: 0.000674409
	LOSS [training: 0.31588625489720845 | validation: 0.37027526536016714]
	TIME [epoch: 7.15 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24985526523189053		[learning rate: 0.00067282]
	Learning Rate: 0.000672818
	LOSS [training: 0.24985526523189053 | validation: 0.35414419702013833]
	TIME [epoch: 7.12 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27229553372712834		[learning rate: 0.00067123]
	Learning Rate: 0.000671231
	LOSS [training: 0.27229553372712834 | validation: 0.26187064953269373]
	TIME [epoch: 7.12 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23477580248970287		[learning rate: 0.00066965]
	Learning Rate: 0.000669647
	LOSS [training: 0.23477580248970287 | validation: 0.3256260121921358]
	TIME [epoch: 7.12 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3065811264658191		[learning rate: 0.00066807]
	Learning Rate: 0.000668068
	LOSS [training: 0.3065811264658191 | validation: 0.34401333802404965]
	TIME [epoch: 7.12 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25080942043998367		[learning rate: 0.00066649]
	Learning Rate: 0.000666492
	LOSS [training: 0.25080942043998367 | validation: 0.39029248886436885]
	TIME [epoch: 7.11 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.254314257261686		[learning rate: 0.00066492]
	Learning Rate: 0.00066492
	LOSS [training: 0.254314257261686 | validation: 0.37080095087503206]
	TIME [epoch: 7.15 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2568396526121934		[learning rate: 0.00066335]
	Learning Rate: 0.000663351
	LOSS [training: 0.2568396526121934 | validation: 0.2803941628585577]
	TIME [epoch: 7.11 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31562316009357894		[learning rate: 0.00066179]
	Learning Rate: 0.000661786
	LOSS [training: 0.31562316009357894 | validation: 0.3641735596373663]
	TIME [epoch: 7.11 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30288866153208377		[learning rate: 0.00066023]
	Learning Rate: 0.000660225
	LOSS [training: 0.30288866153208377 | validation: 0.274768255346287]
	TIME [epoch: 7.11 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22076992469688334		[learning rate: 0.00065867]
	Learning Rate: 0.000658668
	LOSS [training: 0.22076992469688334 | validation: 0.2137966576160574]
	TIME [epoch: 7.12 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26925975936711566		[learning rate: 0.00065711]
	Learning Rate: 0.000657114
	LOSS [training: 0.26925975936711566 | validation: 0.24806923917457524]
	TIME [epoch: 7.1 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25361934276998815		[learning rate: 0.00065556]
	Learning Rate: 0.000655564
	LOSS [training: 0.25361934276998815 | validation: 0.35935305541235885]
	TIME [epoch: 7.15 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34051220915812885		[learning rate: 0.00065402]
	Learning Rate: 0.000654018
	LOSS [training: 0.34051220915812885 | validation: 0.3274473654001391]
	TIME [epoch: 7.12 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3006828947534996		[learning rate: 0.00065248]
	Learning Rate: 0.000652475
	LOSS [training: 0.3006828947534996 | validation: 0.3316344059339357]
	TIME [epoch: 7.11 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2727626243272717		[learning rate: 0.00065094]
	Learning Rate: 0.000650936
	LOSS [training: 0.2727626243272717 | validation: 0.2941271113795161]
	TIME [epoch: 7.1 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2515918445853612		[learning rate: 0.0006494]
	Learning Rate: 0.000649401
	LOSS [training: 0.2515918445853612 | validation: 0.3281586874564339]
	TIME [epoch: 7.11 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2771712896388814		[learning rate: 0.00064787]
	Learning Rate: 0.000647869
	LOSS [training: 0.2771712896388814 | validation: 0.3977407182866715]
	TIME [epoch: 7.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2719096608268975		[learning rate: 0.00064634]
	Learning Rate: 0.000646341
	LOSS [training: 0.2719096608268975 | validation: 0.3057294870999768]
	TIME [epoch: 7.15 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21072129527438052		[learning rate: 0.00064482]
	Learning Rate: 0.000644816
	LOSS [training: 0.21072129527438052 | validation: 0.2518352845299654]
	TIME [epoch: 7.12 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22484356207694		[learning rate: 0.0006433]
	Learning Rate: 0.000643295
	LOSS [training: 0.22484356207694 | validation: 0.2509151852903791]
	TIME [epoch: 7.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2642047526321919		[learning rate: 0.00064178]
	Learning Rate: 0.000641778
	LOSS [training: 0.2642047526321919 | validation: 0.5652195375331126]
	TIME [epoch: 7.11 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3812031018043138		[learning rate: 0.00064026]
	Learning Rate: 0.000640264
	LOSS [training: 0.3812031018043138 | validation: 0.3676242981306458]
	TIME [epoch: 7.11 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2712951426312342		[learning rate: 0.00063875]
	Learning Rate: 0.000638754
	LOSS [training: 0.2712951426312342 | validation: 0.2580765584953254]
	TIME [epoch: 7.12 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2320293945198805		[learning rate: 0.00063725]
	Learning Rate: 0.000637247
	LOSS [training: 0.2320293945198805 | validation: 0.2629524425105011]
	TIME [epoch: 7.14 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24948201833490372		[learning rate: 0.00063574]
	Learning Rate: 0.000635744
	LOSS [training: 0.24948201833490372 | validation: 0.3909597543691664]
	TIME [epoch: 7.12 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2698995164984662		[learning rate: 0.00063424]
	Learning Rate: 0.000634244
	LOSS [training: 0.2698995164984662 | validation: 0.275155710063809]
	TIME [epoch: 7.11 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21838532381294282		[learning rate: 0.00063275]
	Learning Rate: 0.000632748
	LOSS [training: 0.21838532381294282 | validation: 0.38368376648369074]
	TIME [epoch: 7.12 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31105188143242246		[learning rate: 0.00063126]
	Learning Rate: 0.000631255
	LOSS [training: 0.31105188143242246 | validation: 0.2197291125525809]
	TIME [epoch: 7.11 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1972078747392435		[learning rate: 0.00062977]
	Learning Rate: 0.000629766
	LOSS [training: 0.1972078747392435 | validation: 0.26776262271555856]
	TIME [epoch: 7.11 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2062285297644912		[learning rate: 0.00062828]
	Learning Rate: 0.000628281
	LOSS [training: 0.2062285297644912 | validation: 0.22891888898227428]
	TIME [epoch: 7.14 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20792676831331444		[learning rate: 0.0006268]
	Learning Rate: 0.000626799
	LOSS [training: 0.20792676831331444 | validation: 0.2648368810471148]
	TIME [epoch: 7.11 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38314883162969077		[learning rate: 0.00062532]
	Learning Rate: 0.00062532
	LOSS [training: 0.38314883162969077 | validation: 0.295953722370827]
	TIME [epoch: 7.11 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2646664480150568		[learning rate: 0.00062385]
	Learning Rate: 0.000623845
	LOSS [training: 0.2646664480150568 | validation: 0.2562815450335719]
	TIME [epoch: 7.11 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24105461174758352		[learning rate: 0.00062237]
	Learning Rate: 0.000622374
	LOSS [training: 0.24105461174758352 | validation: 0.2898833252374024]
	TIME [epoch: 7.11 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2638186357315264		[learning rate: 0.00062091]
	Learning Rate: 0.000620906
	LOSS [training: 0.2638186357315264 | validation: 0.38756438137320737]
	TIME [epoch: 7.11 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29707127806440564		[learning rate: 0.00061944]
	Learning Rate: 0.000619441
	LOSS [training: 0.29707127806440564 | validation: 0.2067488484832584]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1228.pth
	Model improved!!!
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22798498865571193		[learning rate: 0.00061798]
	Learning Rate: 0.00061798
	LOSS [training: 0.22798498865571193 | validation: 0.26792650003261453]
	TIME [epoch: 7.13 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25172584710666246		[learning rate: 0.00061652]
	Learning Rate: 0.000616522
	LOSS [training: 0.25172584710666246 | validation: 0.28196207581533606]
	TIME [epoch: 7.1 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21496207809060736		[learning rate: 0.00061507]
	Learning Rate: 0.000615068
	LOSS [training: 0.21496207809060736 | validation: 0.31587045484546894]
	TIME [epoch: 7.11 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2380268655674604		[learning rate: 0.00061362]
	Learning Rate: 0.000613617
	LOSS [training: 0.2380268655674604 | validation: 0.30232268666903217]
	TIME [epoch: 7.1 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24639921269322734		[learning rate: 0.00061217]
	Learning Rate: 0.00061217
	LOSS [training: 0.24639921269322734 | validation: 0.22015828805253768]
	TIME [epoch: 7.11 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21637434628699337		[learning rate: 0.00061073]
	Learning Rate: 0.000610726
	LOSS [training: 0.21637434628699337 | validation: 0.22232987264151877]
	TIME [epoch: 7.14 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24760934877806426		[learning rate: 0.00060929]
	Learning Rate: 0.000609285
	LOSS [training: 0.24760934877806426 | validation: 0.2237320517382363]
	TIME [epoch: 7.11 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21620526804919388		[learning rate: 0.00060785]
	Learning Rate: 0.000607848
	LOSS [training: 0.21620526804919388 | validation: 0.20914595116943896]
	TIME [epoch: 7.11 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20948734830309945		[learning rate: 0.00060641]
	Learning Rate: 0.000606414
	LOSS [training: 0.20948734830309945 | validation: 0.2577563747401509]
	TIME [epoch: 7.1 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22606256138287123		[learning rate: 0.00060498]
	Learning Rate: 0.000604983
	LOSS [training: 0.22606256138287123 | validation: 0.2169555336385291]
	TIME [epoch: 7.1 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24421336856635306		[learning rate: 0.00060356]
	Learning Rate: 0.000603557
	LOSS [training: 0.24421336856635306 | validation: 0.28888315009103044]
	TIME [epoch: 7.1 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24413628669478799		[learning rate: 0.00060213]
	Learning Rate: 0.000602133
	LOSS [training: 0.24413628669478799 | validation: 0.294668002427039]
	TIME [epoch: 7.15 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22411291817467172		[learning rate: 0.00060071]
	Learning Rate: 0.000600712
	LOSS [training: 0.22411291817467172 | validation: 0.31868125072897546]
	TIME [epoch: 7.11 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3137240481588718		[learning rate: 0.0005993]
	Learning Rate: 0.000599296
	LOSS [training: 0.3137240481588718 | validation: 0.2756061853061587]
	TIME [epoch: 7.12 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22694045135579222		[learning rate: 0.00059788]
	Learning Rate: 0.000597882
	LOSS [training: 0.22694045135579222 | validation: 0.2463633658621478]
	TIME [epoch: 7.11 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20345368964953958		[learning rate: 0.00059647]
	Learning Rate: 0.000596471
	LOSS [training: 0.20345368964953958 | validation: 0.29534473060789274]
	TIME [epoch: 7.12 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2553518125013185		[learning rate: 0.00059506]
	Learning Rate: 0.000595065
	LOSS [training: 0.2553518125013185 | validation: 0.2239026986717343]
	TIME [epoch: 7.11 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22288203973202098		[learning rate: 0.00059366]
	Learning Rate: 0.000593661
	LOSS [training: 0.22288203973202098 | validation: 0.34680180926964593]
	TIME [epoch: 7.15 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33455034232584063		[learning rate: 0.00059226]
	Learning Rate: 0.000592261
	LOSS [training: 0.33455034232584063 | validation: 0.3582873626192007]
	TIME [epoch: 7.12 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2932904525308647		[learning rate: 0.00059086]
	Learning Rate: 0.000590863
	LOSS [training: 0.2932904525308647 | validation: 0.28356015708620996]
	TIME [epoch: 7.11 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30120427106028813		[learning rate: 0.00058947]
	Learning Rate: 0.00058947
	LOSS [training: 0.30120427106028813 | validation: 0.27379676234081707]
	TIME [epoch: 7.1 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2778476875954702		[learning rate: 0.00058808]
	Learning Rate: 0.000588079
	LOSS [training: 0.2778476875954702 | validation: 0.35501318500445495]
	TIME [epoch: 7.11 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2977427355001271		[learning rate: 0.00058669]
	Learning Rate: 0.000586692
	LOSS [training: 0.2977427355001271 | validation: 0.32758539503312034]
	TIME [epoch: 7.11 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22698175588763103		[learning rate: 0.00058531]
	Learning Rate: 0.000585308
	LOSS [training: 0.22698175588763103 | validation: 0.21425225400725428]
	TIME [epoch: 7.15 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23200309854862877		[learning rate: 0.00058393]
	Learning Rate: 0.000583927
	LOSS [training: 0.23200309854862877 | validation: 0.2700545994800856]
	TIME [epoch: 7.11 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2548703565888913		[learning rate: 0.00058255]
	Learning Rate: 0.00058255
	LOSS [training: 0.2548703565888913 | validation: 0.31586037615952295]
	TIME [epoch: 7.1 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38834136390501833		[learning rate: 0.00058118]
	Learning Rate: 0.000581176
	LOSS [training: 0.38834136390501833 | validation: 0.34432806492544366]
	TIME [epoch: 7.11 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27096407022892494		[learning rate: 0.00057981]
	Learning Rate: 0.000579805
	LOSS [training: 0.27096407022892494 | validation: 0.2811193877749666]
	TIME [epoch: 7.11 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26051064230600324		[learning rate: 0.00057844]
	Learning Rate: 0.000578437
	LOSS [training: 0.26051064230600324 | validation: 0.2534849472497019]
	TIME [epoch: 7.11 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23419185410416982		[learning rate: 0.00057707]
	Learning Rate: 0.000577073
	LOSS [training: 0.23419185410416982 | validation: 0.37682179111680997]
	TIME [epoch: 7.14 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.285351766820812		[learning rate: 0.00057571]
	Learning Rate: 0.000575712
	LOSS [training: 0.285351766820812 | validation: 0.27463182944043574]
	TIME [epoch: 7.11 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2949477491570211		[learning rate: 0.00057435]
	Learning Rate: 0.000574354
	LOSS [training: 0.2949477491570211 | validation: 0.2527905699518761]
	TIME [epoch: 7.11 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27224715135821775		[learning rate: 0.000573]
	Learning Rate: 0.000572999
	LOSS [training: 0.27224715135821775 | validation: 0.23260151310073024]
	TIME [epoch: 7.11 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30451888364658786		[learning rate: 0.00057165]
	Learning Rate: 0.000571647
	LOSS [training: 0.30451888364658786 | validation: 0.2701323111766286]
	TIME [epoch: 7.11 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28621902706427393		[learning rate: 0.0005703]
	Learning Rate: 0.000570299
	LOSS [training: 0.28621902706427393 | validation: 0.2688093214988524]
	TIME [epoch: 7.11 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2580180656636648		[learning rate: 0.00056895]
	Learning Rate: 0.000568954
	LOSS [training: 0.2580180656636648 | validation: 0.256103390714404]
	TIME [epoch: 7.14 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.227034370822048		[learning rate: 0.00056761]
	Learning Rate: 0.000567612
	LOSS [training: 0.227034370822048 | validation: 0.20772291022670242]
	TIME [epoch: 7.12 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23618070790012302		[learning rate: 0.00056627]
	Learning Rate: 0.000566273
	LOSS [training: 0.23618070790012302 | validation: 0.24351846979225983]
	TIME [epoch: 7.11 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21910612953477684		[learning rate: 0.00056494]
	Learning Rate: 0.000564937
	LOSS [training: 0.21910612953477684 | validation: 0.26715263558121916]
	TIME [epoch: 7.11 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19431019032239966		[learning rate: 0.0005636]
	Learning Rate: 0.000563604
	LOSS [training: 0.19431019032239966 | validation: 0.20637335880868263]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1268.pth
	Model improved!!!
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2018454539867339		[learning rate: 0.00056227]
	Learning Rate: 0.000562275
	LOSS [training: 0.2018454539867339 | validation: 0.22053066510306946]
	TIME [epoch: 7.13 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19517997387938738		[learning rate: 0.00056095]
	Learning Rate: 0.000560949
	LOSS [training: 0.19517997387938738 | validation: 0.255675870738138]
	TIME [epoch: 7.16 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23158332876853271		[learning rate: 0.00055963]
	Learning Rate: 0.000559625
	LOSS [training: 0.23158332876853271 | validation: 0.304041391595847]
	TIME [epoch: 7.13 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27732188696469035		[learning rate: 0.00055831]
	Learning Rate: 0.000558305
	LOSS [training: 0.27732188696469035 | validation: 0.29619376457997826]
	TIME [epoch: 7.12 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3168832813285592		[learning rate: 0.00055699]
	Learning Rate: 0.000556988
	LOSS [training: 0.3168832813285592 | validation: 0.3903305492650578]
	TIME [epoch: 7.13 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28270938634642506		[learning rate: 0.00055567]
	Learning Rate: 0.000555674
	LOSS [training: 0.28270938634642506 | validation: 0.2783465132746722]
	TIME [epoch: 7.12 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23059734097495346		[learning rate: 0.00055436]
	Learning Rate: 0.000554364
	LOSS [training: 0.23059734097495346 | validation: 0.28442407074870296]
	TIME [epoch: 7.13 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22444960264041236		[learning rate: 0.00055306]
	Learning Rate: 0.000553056
	LOSS [training: 0.22444960264041236 | validation: 0.21696689226307705]
	TIME [epoch: 7.16 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2324288574932014		[learning rate: 0.00055175]
	Learning Rate: 0.000551752
	LOSS [training: 0.2324288574932014 | validation: 0.342720485301721]
	TIME [epoch: 7.13 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21250177878617107		[learning rate: 0.00055045]
	Learning Rate: 0.00055045
	LOSS [training: 0.21250177878617107 | validation: 0.21672920623675895]
	TIME [epoch: 7.12 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20492488902327635		[learning rate: 0.00054915]
	Learning Rate: 0.000549152
	LOSS [training: 0.20492488902327635 | validation: 0.20352619411544093]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1279.pth
	Model improved!!!
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21820218964800367		[learning rate: 0.00054786]
	Learning Rate: 0.000547856
	LOSS [training: 0.21820218964800367 | validation: 0.21304659310193375]
	TIME [epoch: 7.13 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27192353747390696		[learning rate: 0.00054656]
	Learning Rate: 0.000546564
	LOSS [training: 0.27192353747390696 | validation: 0.2930312886829528]
	TIME [epoch: 7.14 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22489868474382774		[learning rate: 0.00054527]
	Learning Rate: 0.000545275
	LOSS [training: 0.22489868474382774 | validation: 0.21477982773825655]
	TIME [epoch: 7.15 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20357603641269312		[learning rate: 0.00054399]
	Learning Rate: 0.000543988
	LOSS [training: 0.20357603641269312 | validation: 0.27968095068410415]
	TIME [epoch: 7.12 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2741034176352572		[learning rate: 0.00054271]
	Learning Rate: 0.000542705
	LOSS [training: 0.2741034176352572 | validation: 0.3701821924354505]
	TIME [epoch: 7.13 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2654564061932613		[learning rate: 0.00054143]
	Learning Rate: 0.000541425
	LOSS [training: 0.2654564061932613 | validation: 0.39473059440986624]
	TIME [epoch: 7.12 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31117273712456905		[learning rate: 0.00054015]
	Learning Rate: 0.000540148
	LOSS [training: 0.31117273712456905 | validation: 0.31821582369707285]
	TIME [epoch: 7.13 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23679363681352145		[learning rate: 0.00053887]
	Learning Rate: 0.000538874
	LOSS [training: 0.23679363681352145 | validation: 0.2657051117406781]
	TIME [epoch: 7.14 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29994152064641727		[learning rate: 0.0005376]
	Learning Rate: 0.000537603
	LOSS [training: 0.29994152064641727 | validation: 0.3180853255974987]
	TIME [epoch: 7.16 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32993144177103584		[learning rate: 0.00053633]
	Learning Rate: 0.000536335
	LOSS [training: 0.32993144177103584 | validation: 0.25683561229320556]
	TIME [epoch: 7.12 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21966762848569926		[learning rate: 0.00053507]
	Learning Rate: 0.00053507
	LOSS [training: 0.21966762848569926 | validation: 0.224407569854355]
	TIME [epoch: 7.13 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2500767118793144		[learning rate: 0.00053381]
	Learning Rate: 0.000533807
	LOSS [training: 0.2500767118793144 | validation: 0.27194271539526027]
	TIME [epoch: 7.12 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22967857363354222		[learning rate: 0.00053255]
	Learning Rate: 0.000532548
	LOSS [training: 0.22967857363354222 | validation: 0.20877158815115532]
	TIME [epoch: 7.13 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22723612357698494		[learning rate: 0.00053129]
	Learning Rate: 0.000531292
	LOSS [training: 0.22723612357698494 | validation: 0.4029417598776556]
	TIME [epoch: 7.14 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25370068644879196		[learning rate: 0.00053004]
	Learning Rate: 0.000530039
	LOSS [training: 0.25370068644879196 | validation: 0.223000444342542]
	TIME [epoch: 7.16 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2202927277657257		[learning rate: 0.00052879]
	Learning Rate: 0.000528789
	LOSS [training: 0.2202927277657257 | validation: 0.2364802378354304]
	TIME [epoch: 7.13 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23722909478008652		[learning rate: 0.00052754]
	Learning Rate: 0.000527541
	LOSS [training: 0.23722909478008652 | validation: 0.28816933303397985]
	TIME [epoch: 7.13 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2599845396308085		[learning rate: 0.0005263]
	Learning Rate: 0.000526297
	LOSS [training: 0.2599845396308085 | validation: 0.252017533819992]
	TIME [epoch: 7.12 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3230961905826939		[learning rate: 0.00052506]
	Learning Rate: 0.000525055
	LOSS [training: 0.3230961905826939 | validation: 0.2673016185801952]
	TIME [epoch: 7.12 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23271461285759681		[learning rate: 0.00052382]
	Learning Rate: 0.000523817
	LOSS [training: 0.23271461285759681 | validation: 0.279332131295]
	TIME [epoch: 7.13 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28644982960410015		[learning rate: 0.00052258]
	Learning Rate: 0.000522581
	LOSS [training: 0.28644982960410015 | validation: 0.2230536686547816]
	TIME [epoch: 7.15 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22829313892264008		[learning rate: 0.00052135]
	Learning Rate: 0.000521348
	LOSS [training: 0.22829313892264008 | validation: 0.3933804055216702]
	TIME [epoch: 7.13 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2518304463786084		[learning rate: 0.00052012]
	Learning Rate: 0.000520119
	LOSS [training: 0.2518304463786084 | validation: 0.21710939783115704]
	TIME [epoch: 7.13 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2333372240901011		[learning rate: 0.00051889]
	Learning Rate: 0.000518892
	LOSS [training: 0.2333372240901011 | validation: 0.24267603847111296]
	TIME [epoch: 7.13 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2684747957700099		[learning rate: 0.00051767]
	Learning Rate: 0.000517668
	LOSS [training: 0.2684747957700099 | validation: 0.25957034479926744]
	TIME [epoch: 7.12 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2211909980718227		[learning rate: 0.00051645]
	Learning Rate: 0.000516447
	LOSS [training: 0.2211909980718227 | validation: 0.1987245193992679]
	TIME [epoch: 7.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1305.pth
	Model improved!!!
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22650347212545605		[learning rate: 0.00051523]
	Learning Rate: 0.000515229
	LOSS [training: 0.22650347212545605 | validation: 0.2807208499281101]
	TIME [epoch: 7.15 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2702789834969378		[learning rate: 0.00051401]
	Learning Rate: 0.000514013
	LOSS [training: 0.2702789834969378 | validation: 0.3622803404020565]
	TIME [epoch: 7.12 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26612426488794483		[learning rate: 0.0005128]
	Learning Rate: 0.000512801
	LOSS [training: 0.26612426488794483 | validation: 0.27333228040149704]
	TIME [epoch: 7.12 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22523573546073955		[learning rate: 0.00051159]
	Learning Rate: 0.000511591
	LOSS [training: 0.22523573546073955 | validation: 0.30303066894078895]
	TIME [epoch: 7.13 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2547278922616232		[learning rate: 0.00051038]
	Learning Rate: 0.000510384
	LOSS [training: 0.2547278922616232 | validation: 0.2792683339053771]
	TIME [epoch: 7.12 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20355013201103506		[learning rate: 0.00050918]
	Learning Rate: 0.00050918
	LOSS [training: 0.20355013201103506 | validation: 0.22389897095442557]
	TIME [epoch: 7.14 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18877392358844514		[learning rate: 0.00050798]
	Learning Rate: 0.000507979
	LOSS [training: 0.18877392358844514 | validation: 0.24093788195389165]
	TIME [epoch: 7.15 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19772907707555748		[learning rate: 0.00050678]
	Learning Rate: 0.000506781
	LOSS [training: 0.19772907707555748 | validation: 0.2802897220367122]
	TIME [epoch: 7.13 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2728321617883759		[learning rate: 0.00050559]
	Learning Rate: 0.000505586
	LOSS [training: 0.2728321617883759 | validation: 0.4047658536510379]
	TIME [epoch: 7.12 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3514628852836083		[learning rate: 0.00050439]
	Learning Rate: 0.000504393
	LOSS [training: 0.3514628852836083 | validation: 0.2492387957113517]
	TIME [epoch: 7.13 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2384513717224661		[learning rate: 0.0005032]
	Learning Rate: 0.000503203
	LOSS [training: 0.2384513717224661 | validation: 0.3701638313935864]
	TIME [epoch: 7.12 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.391990661826061		[learning rate: 0.00050202]
	Learning Rate: 0.000502016
	LOSS [training: 0.391990661826061 | validation: 0.36383251910244285]
	TIME [epoch: 7.14 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2947560513418884		[learning rate: 0.00050083]
	Learning Rate: 0.000500832
	LOSS [training: 0.2947560513418884 | validation: 0.300229643947281]
	TIME [epoch: 7.15 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3238588470010445		[learning rate: 0.00049965]
	Learning Rate: 0.000499651
	LOSS [training: 0.3238588470010445 | validation: 0.25057689021048923]
	TIME [epoch: 7.13 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23463820305050037		[learning rate: 0.00049847]
	Learning Rate: 0.000498472
	LOSS [training: 0.23463820305050037 | validation: 0.26434946056046804]
	TIME [epoch: 7.12 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26315294489653485		[learning rate: 0.0004973]
	Learning Rate: 0.000497296
	LOSS [training: 0.26315294489653485 | validation: 0.2279418141261132]
	TIME [epoch: 7.13 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19761944572985013		[learning rate: 0.00049612]
	Learning Rate: 0.000496123
	LOSS [training: 0.19761944572985013 | validation: 0.2608572732190577]
	TIME [epoch: 7.12 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25492496261723563		[learning rate: 0.00049495]
	Learning Rate: 0.000494953
	LOSS [training: 0.25492496261723563 | validation: 0.271196718753552]
	TIME [epoch: 7.14 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2539102660071036		[learning rate: 0.00049379]
	Learning Rate: 0.000493786
	LOSS [training: 0.2539102660071036 | validation: 0.32385966892003215]
	TIME [epoch: 7.14 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23721420494279633		[learning rate: 0.00049262]
	Learning Rate: 0.000492621
	LOSS [training: 0.23721420494279633 | validation: 0.30018789746834146]
	TIME [epoch: 7.13 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25225848019162356		[learning rate: 0.00049146]
	Learning Rate: 0.000491459
	LOSS [training: 0.25225848019162356 | validation: 0.43240464616528196]
	TIME [epoch: 7.13 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2799225107384058		[learning rate: 0.0004903]
	Learning Rate: 0.0004903
	LOSS [training: 0.2799225107384058 | validation: 0.24771609611278375]
	TIME [epoch: 7.13 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2577165913909123		[learning rate: 0.00048914]
	Learning Rate: 0.000489143
	LOSS [training: 0.2577165913909123 | validation: 0.3570830018683926]
	TIME [epoch: 7.13 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2639101422533328		[learning rate: 0.00048799]
	Learning Rate: 0.000487989
	LOSS [training: 0.2639101422533328 | validation: 0.23510825008279784]
	TIME [epoch: 7.16 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20114113872694767		[learning rate: 0.00048684]
	Learning Rate: 0.000486838
	LOSS [training: 0.20114113872694767 | validation: 0.2774827885878305]
	TIME [epoch: 7.13 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2475184943665308		[learning rate: 0.00048569]
	Learning Rate: 0.00048569
	LOSS [training: 0.2475184943665308 | validation: 0.22710306795333765]
	TIME [epoch: 7.12 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22480523018740964		[learning rate: 0.00048454]
	Learning Rate: 0.000484544
	LOSS [training: 0.22480523018740964 | validation: 0.21479993882481654]
	TIME [epoch: 7.13 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2295570539736		[learning rate: 0.0004834]
	Learning Rate: 0.000483401
	LOSS [training: 0.2295570539736 | validation: 0.43044895625745155]
	TIME [epoch: 7.12 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3001558376716882		[learning rate: 0.00048226]
	Learning Rate: 0.000482261
	LOSS [training: 0.3001558376716882 | validation: 0.1984801739689211]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21797755742512498		[learning rate: 0.00048112]
	Learning Rate: 0.000481123
	LOSS [training: 0.21797755742512498 | validation: 0.2334316119595306]
	TIME [epoch: 7.15 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29204271109895774		[learning rate: 0.00047999]
	Learning Rate: 0.000479988
	LOSS [training: 0.29204271109895774 | validation: 0.24929697756401104]
	TIME [epoch: 7.13 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21973584930452333		[learning rate: 0.00047886]
	Learning Rate: 0.000478856
	LOSS [training: 0.21973584930452333 | validation: 0.21397667292094474]
	TIME [epoch: 7.12 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21361370499584295		[learning rate: 0.00047773]
	Learning Rate: 0.000477727
	LOSS [training: 0.21361370499584295 | validation: 0.26081229619956725]
	TIME [epoch: 7.12 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22280151414958654		[learning rate: 0.0004766]
	Learning Rate: 0.0004766
	LOSS [training: 0.22280151414958654 | validation: 0.2335435551392659]
	TIME [epoch: 7.12 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20537505885801605		[learning rate: 0.00047548]
	Learning Rate: 0.000475476
	LOSS [training: 0.20537505885801605 | validation: 0.26023784357236907]
	TIME [epoch: 7.12 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3638789247149548		[learning rate: 0.00047435]
	Learning Rate: 0.000474354
	LOSS [training: 0.3638789247149548 | validation: 0.4384151189927958]
	TIME [epoch: 7.14 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3519214753821501		[learning rate: 0.00047324]
	Learning Rate: 0.000473235
	LOSS [training: 0.3519214753821501 | validation: 0.29498517748334985]
	TIME [epoch: 7.13 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3070549973845102		[learning rate: 0.00047212]
	Learning Rate: 0.000472119
	LOSS [training: 0.3070549973845102 | validation: 0.31253714522447823]
	TIME [epoch: 7.12 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28999779589382674		[learning rate: 0.00047101]
	Learning Rate: 0.000471005
	LOSS [training: 0.28999779589382674 | validation: 0.24967141329784642]
	TIME [epoch: 7.12 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28677962557537434		[learning rate: 0.00046989]
	Learning Rate: 0.000469894
	LOSS [training: 0.28677962557537434 | validation: 0.35900573244070066]
	TIME [epoch: 7.12 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30278342125801344		[learning rate: 0.00046879]
	Learning Rate: 0.000468786
	LOSS [training: 0.30278342125801344 | validation: 0.2992946747034821]
	TIME [epoch: 7.12 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27160583063568894		[learning rate: 0.00046768]
	Learning Rate: 0.00046768
	LOSS [training: 0.27160583063568894 | validation: 0.31122287790336395]
	TIME [epoch: 7.14 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22674488799731907		[learning rate: 0.00046658]
	Learning Rate: 0.000466577
	LOSS [training: 0.22674488799731907 | validation: 0.2574341447888583]
	TIME [epoch: 7.13 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23407770117903892		[learning rate: 0.00046548]
	Learning Rate: 0.000465476
	LOSS [training: 0.23407770117903892 | validation: 0.23445632955341883]
	TIME [epoch: 7.11 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20292434519455704		[learning rate: 0.00046438]
	Learning Rate: 0.000464378
	LOSS [training: 0.20292434519455704 | validation: 0.2539235182991696]
	TIME [epoch: 7.11 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2257994341749059		[learning rate: 0.00046328]
	Learning Rate: 0.000463283
	LOSS [training: 0.2257994341749059 | validation: 0.27636622843346204]
	TIME [epoch: 7.12 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24875173098233602		[learning rate: 0.00046219]
	Learning Rate: 0.00046219
	LOSS [training: 0.24875173098233602 | validation: 0.2853052553121148]
	TIME [epoch: 7.12 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23504791076090698		[learning rate: 0.0004611]
	Learning Rate: 0.0004611
	LOSS [training: 0.23504791076090698 | validation: 0.19575243668824285]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1353.pth
	Model improved!!!
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22458916057262077		[learning rate: 0.00046001]
	Learning Rate: 0.000460012
	LOSS [training: 0.22458916057262077 | validation: 0.2379203279310545]
	TIME [epoch: 7.13 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29674107296702235		[learning rate: 0.00045893]
	Learning Rate: 0.000458927
	LOSS [training: 0.29674107296702235 | validation: 0.31795136973608995]
	TIME [epoch: 7.11 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25326466523686825		[learning rate: 0.00045784]
	Learning Rate: 0.000457844
	LOSS [training: 0.25326466523686825 | validation: 0.24322969256642338]
	TIME [epoch: 7.11 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19769472110698222		[learning rate: 0.00045676]
	Learning Rate: 0.000456764
	LOSS [training: 0.19769472110698222 | validation: 0.2391770235013556]
	TIME [epoch: 7.11 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18814681562869442		[learning rate: 0.00045569]
	Learning Rate: 0.000455687
	LOSS [training: 0.18814681562869442 | validation: 0.21176616543956078]
	TIME [epoch: 7.11 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21598655997395558		[learning rate: 0.00045461]
	Learning Rate: 0.000454612
	LOSS [training: 0.21598655997395558 | validation: 0.24348442869674225]
	TIME [epoch: 7.15 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.253572105259694		[learning rate: 0.00045354]
	Learning Rate: 0.00045354
	LOSS [training: 0.253572105259694 | validation: 0.3113291291039145]
	TIME [epoch: 7.12 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34630222171960395		[learning rate: 0.00045247]
	Learning Rate: 0.00045247
	LOSS [training: 0.34630222171960395 | validation: 0.40806562565888904]
	TIME [epoch: 7.12 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2990595880185022		[learning rate: 0.0004514]
	Learning Rate: 0.000451403
	LOSS [training: 0.2990595880185022 | validation: 0.24342112858949416]
	TIME [epoch: 7.12 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1986833335481079		[learning rate: 0.00045034]
	Learning Rate: 0.000450338
	LOSS [training: 0.1986833335481079 | validation: 0.2317092177538533]
	TIME [epoch: 7.12 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22850278174883298		[learning rate: 0.00044928]
	Learning Rate: 0.000449276
	LOSS [training: 0.22850278174883298 | validation: 0.24017641728018113]
	TIME [epoch: 7.11 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2179940584616717		[learning rate: 0.00044822]
	Learning Rate: 0.000448216
	LOSS [training: 0.2179940584616717 | validation: 0.23001123589588507]
	TIME [epoch: 7.15 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21362657184803338		[learning rate: 0.00044716]
	Learning Rate: 0.000447159
	LOSS [training: 0.21362657184803338 | validation: 0.22843696329158986]
	TIME [epoch: 7.12 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21931372452731468		[learning rate: 0.0004461]
	Learning Rate: 0.000446104
	LOSS [training: 0.21931372452731468 | validation: 0.2051660660152902]
	TIME [epoch: 7.12 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20659374061883443		[learning rate: 0.00044505]
	Learning Rate: 0.000445051
	LOSS [training: 0.20659374061883443 | validation: 0.2056699162208253]
	TIME [epoch: 7.11 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2017571180073619		[learning rate: 0.000444]
	Learning Rate: 0.000444002
	LOSS [training: 0.2017571180073619 | validation: 0.3159338669847518]
	TIME [epoch: 7.11 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23120315574124745		[learning rate: 0.00044295]
	Learning Rate: 0.000442954
	LOSS [training: 0.23120315574124745 | validation: 0.21451076931493512]
	TIME [epoch: 7.11 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2134037279171958		[learning rate: 0.00044191]
	Learning Rate: 0.000441909
	LOSS [training: 0.2134037279171958 | validation: 0.3683163342654384]
	TIME [epoch: 7.15 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35951876101784047		[learning rate: 0.00044087]
	Learning Rate: 0.000440867
	LOSS [training: 0.35951876101784047 | validation: 0.34212082645287334]
	TIME [epoch: 7.12 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26349459313709955		[learning rate: 0.00043983]
	Learning Rate: 0.000439827
	LOSS [training: 0.26349459313709955 | validation: 0.26133807341946624]
	TIME [epoch: 7.11 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2177078642214684		[learning rate: 0.00043879]
	Learning Rate: 0.00043879
	LOSS [training: 0.2177078642214684 | validation: 0.2847485220020623]
	TIME [epoch: 7.11 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2686397847716715		[learning rate: 0.00043775]
	Learning Rate: 0.000437755
	LOSS [training: 0.2686397847716715 | validation: 0.28923229074106405]
	TIME [epoch: 7.11 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24767667559458872		[learning rate: 0.00043672]
	Learning Rate: 0.000436722
	LOSS [training: 0.24767667559458872 | validation: 0.27089981327525464]
	TIME [epoch: 7.12 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2655449557029711		[learning rate: 0.00043569]
	Learning Rate: 0.000435692
	LOSS [training: 0.2655449557029711 | validation: 0.2442773218801238]
	TIME [epoch: 7.15 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19954000943764288		[learning rate: 0.00043466]
	Learning Rate: 0.000434664
	LOSS [training: 0.19954000943764288 | validation: 0.2328439133541174]
	TIME [epoch: 7.12 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23060617870557878		[learning rate: 0.00043364]
	Learning Rate: 0.000433639
	LOSS [training: 0.23060617870557878 | validation: 0.29791245548380474]
	TIME [epoch: 7.11 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24497454800128793		[learning rate: 0.00043262]
	Learning Rate: 0.000432616
	LOSS [training: 0.24497454800128793 | validation: 0.20364933597984275]
	TIME [epoch: 7.11 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22447727691013233		[learning rate: 0.0004316]
	Learning Rate: 0.000431595
	LOSS [training: 0.22447727691013233 | validation: 0.3132168191952157]
	TIME [epoch: 7.11 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.371510921418695		[learning rate: 0.00043058]
	Learning Rate: 0.000430577
	LOSS [training: 0.371510921418695 | validation: 0.37198841955431006]
	TIME [epoch: 7.11 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2814100937508225		[learning rate: 0.00042956]
	Learning Rate: 0.000429562
	LOSS [training: 0.2814100937508225 | validation: 0.25339607287030186]
	TIME [epoch: 7.14 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25688303197560247		[learning rate: 0.00042855]
	Learning Rate: 0.000428548
	LOSS [training: 0.25688303197560247 | validation: 0.2237762198033897]
	TIME [epoch: 7.12 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19387302065253692		[learning rate: 0.00042754]
	Learning Rate: 0.000427538
	LOSS [training: 0.19387302065253692 | validation: 0.19857331248854077]
	TIME [epoch: 7.11 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20967145025840078		[learning rate: 0.00042653]
	Learning Rate: 0.000426529
	LOSS [training: 0.20967145025840078 | validation: 0.3007564310474395]
	TIME [epoch: 7.11 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19616400871220463		[learning rate: 0.00042552]
	Learning Rate: 0.000425523
	LOSS [training: 0.19616400871220463 | validation: 0.2071466533667768]
	TIME [epoch: 7.1 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19263183706343012		[learning rate: 0.00042452]
	Learning Rate: 0.000424519
	LOSS [training: 0.19263183706343012 | validation: 0.2826589079701187]
	TIME [epoch: 7.12 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21782102706503204		[learning rate: 0.00042352]
	Learning Rate: 0.000423518
	LOSS [training: 0.21782102706503204 | validation: 0.23649385833496786]
	TIME [epoch: 7.14 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20762964945436968		[learning rate: 0.00042252]
	Learning Rate: 0.000422519
	LOSS [training: 0.20762964945436968 | validation: 0.2637167286162317]
	TIME [epoch: 7.12 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18987053408204602		[learning rate: 0.00042152]
	Learning Rate: 0.000421522
	LOSS [training: 0.18987053408204602 | validation: 0.25178561725304066]
	TIME [epoch: 7.12 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2155254751628873		[learning rate: 0.00042053]
	Learning Rate: 0.000420528
	LOSS [training: 0.2155254751628873 | validation: 0.2724340069968407]
	TIME [epoch: 7.12 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2052843170031411		[learning rate: 0.00041954]
	Learning Rate: 0.000419536
	LOSS [training: 0.2052843170031411 | validation: 0.19995438389643178]
	TIME [epoch: 7.11 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18469875992639181		[learning rate: 0.00041855]
	Learning Rate: 0.000418546
	LOSS [training: 0.18469875992639181 | validation: 0.19309308583843526]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1394.pth
	Model improved!!!
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18913712133721605		[learning rate: 0.00041756]
	Learning Rate: 0.000417559
	LOSS [training: 0.18913712133721605 | validation: 0.2172305399873736]
	TIME [epoch: 7.15 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18538804868172393		[learning rate: 0.00041657]
	Learning Rate: 0.000416574
	LOSS [training: 0.18538804868172393 | validation: 0.2810965913789744]
	TIME [epoch: 7.12 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19711790285931247		[learning rate: 0.00041559]
	Learning Rate: 0.000415591
	LOSS [training: 0.19711790285931247 | validation: 0.2693880954030762]
	TIME [epoch: 7.12 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.244024484638493		[learning rate: 0.00041461]
	Learning Rate: 0.000414611
	LOSS [training: 0.244024484638493 | validation: 0.21240231896795092]
	TIME [epoch: 7.12 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19085244165590415		[learning rate: 0.00041363]
	Learning Rate: 0.000413633
	LOSS [training: 0.19085244165590415 | validation: 0.2563052568504635]
	TIME [epoch: 7.11 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1961211137448164		[learning rate: 0.00041266]
	Learning Rate: 0.000412657
	LOSS [training: 0.1961211137448164 | validation: 0.31586337693240363]
	TIME [epoch: 7.11 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24163099825001363		[learning rate: 0.00041168]
	Learning Rate: 0.000411684
	LOSS [training: 0.24163099825001363 | validation: 0.27770429960603477]
	TIME [epoch: 7.15 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2488276739911643		[learning rate: 0.00041071]
	Learning Rate: 0.000410713
	LOSS [training: 0.2488276739911643 | validation: 0.33345664202352066]
	TIME [epoch: 7.11 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32850857719617416		[learning rate: 0.00040974]
	Learning Rate: 0.000409744
	LOSS [training: 0.32850857719617416 | validation: 0.2714333805330031]
	TIME [epoch: 7.11 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26276855054350395		[learning rate: 0.00040878]
	Learning Rate: 0.000408778
	LOSS [training: 0.26276855054350395 | validation: 0.3945847768676155]
	TIME [epoch: 7.11 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.379399395338268		[learning rate: 0.00040781]
	Learning Rate: 0.000407813
	LOSS [training: 0.379399395338268 | validation: 0.30085086985692117]
	TIME [epoch: 7.13 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25312485747462704		[learning rate: 0.00040685]
	Learning Rate: 0.000406851
	LOSS [training: 0.25312485747462704 | validation: 0.22491139162094637]
	TIME [epoch: 7.11 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19705202724874732		[learning rate: 0.00040589]
	Learning Rate: 0.000405892
	LOSS [training: 0.19705202724874732 | validation: 0.25442487396210445]
	TIME [epoch: 7.15 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24410808869344397		[learning rate: 0.00040493]
	Learning Rate: 0.000404934
	LOSS [training: 0.24410808869344397 | validation: 0.28378692610973233]
	TIME [epoch: 7.11 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2583989966293328		[learning rate: 0.00040398]
	Learning Rate: 0.000403979
	LOSS [training: 0.2583989966293328 | validation: 0.266155691567947]
	TIME [epoch: 7.11 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2776634110087558		[learning rate: 0.00040303]
	Learning Rate: 0.000403026
	LOSS [training: 0.2776634110087558 | validation: 0.36159728723823903]
	TIME [epoch: 7.12 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3465036360577506		[learning rate: 0.00040208]
	Learning Rate: 0.000402076
	LOSS [training: 0.3465036360577506 | validation: 0.2720248257003586]
	TIME [epoch: 7.12 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23886478457926494		[learning rate: 0.00040113]
	Learning Rate: 0.000401127
	LOSS [training: 0.23886478457926494 | validation: 0.263008631068497]
	TIME [epoch: 7.12 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24968533731844142		[learning rate: 0.00040018]
	Learning Rate: 0.000400181
	LOSS [training: 0.24968533731844142 | validation: 0.24802590416155618]
	TIME [epoch: 7.16 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2543415125169892		[learning rate: 0.00039924]
	Learning Rate: 0.000399237
	LOSS [training: 0.2543415125169892 | validation: 0.3251111024619536]
	TIME [epoch: 7.11 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20835068493900324		[learning rate: 0.0003983]
	Learning Rate: 0.000398295
	LOSS [training: 0.20835068493900324 | validation: 0.22324383004472606]
	TIME [epoch: 7.11 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2230288572639118		[learning rate: 0.00039736]
	Learning Rate: 0.000397356
	LOSS [training: 0.2230288572639118 | validation: 0.40160262185328344]
	TIME [epoch: 7.11 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2824096801399647		[learning rate: 0.00039642]
	Learning Rate: 0.000396418
	LOSS [training: 0.2824096801399647 | validation: 0.28563339460713943]
	TIME [epoch: 7.11 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21577700301913838		[learning rate: 0.00039548]
	Learning Rate: 0.000395483
	LOSS [training: 0.21577700301913838 | validation: 0.26050817411728644]
	TIME [epoch: 7.11 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20451049756720083		[learning rate: 0.00039455]
	Learning Rate: 0.00039455
	LOSS [training: 0.20451049756720083 | validation: 0.24601463059027417]
	TIME [epoch: 7.15 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19679808954533595		[learning rate: 0.00039362]
	Learning Rate: 0.00039362
	LOSS [training: 0.19679808954533595 | validation: 0.23791847733469385]
	TIME [epoch: 7.11 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21277028950378923		[learning rate: 0.00039269]
	Learning Rate: 0.000392691
	LOSS [training: 0.21277028950378923 | validation: 0.22264703013476825]
	TIME [epoch: 7.11 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20398742570302528		[learning rate: 0.00039177]
	Learning Rate: 0.000391765
	LOSS [training: 0.20398742570302528 | validation: 0.2435499130420115]
	TIME [epoch: 7.11 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1884451559028893		[learning rate: 0.00039084]
	Learning Rate: 0.000390841
	LOSS [training: 0.1884451559028893 | validation: 0.24669842570086745]
	TIME [epoch: 7.1 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20219357832068868		[learning rate: 0.00038992]
	Learning Rate: 0.000389919
	LOSS [training: 0.20219357832068868 | validation: 0.2466113668046787]
	TIME [epoch: 7.12 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19322303437821794		[learning rate: 0.000389]
	Learning Rate: 0.000388999
	LOSS [training: 0.19322303437821794 | validation: 0.1907718683124639]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1425.pth
	Model improved!!!
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17153507579002114		[learning rate: 0.00038808]
	Learning Rate: 0.000388082
	LOSS [training: 0.17153507579002114 | validation: 0.24352296256980216]
	TIME [epoch: 7.12 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22314677341624956		[learning rate: 0.00038717]
	Learning Rate: 0.000387166
	LOSS [training: 0.22314677341624956 | validation: 0.24335134587487206]
	TIME [epoch: 7.11 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20432706731084832		[learning rate: 0.00038625]
	Learning Rate: 0.000386253
	LOSS [training: 0.20432706731084832 | validation: 0.269254683795694]
	TIME [epoch: 7.11 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2143125194202203		[learning rate: 0.00038534]
	Learning Rate: 0.000385342
	LOSS [training: 0.2143125194202203 | validation: 0.20465978176551616]
	TIME [epoch: 7.11 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22009152637083879		[learning rate: 0.00038443]
	Learning Rate: 0.000384433
	LOSS [training: 0.22009152637083879 | validation: 0.2823014981124929]
	TIME [epoch: 7.12 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2606871753390411		[learning rate: 0.00038353]
	Learning Rate: 0.000383526
	LOSS [training: 0.2606871753390411 | validation: 0.3518074187509749]
	TIME [epoch: 7.14 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24327762537358208		[learning rate: 0.00038262]
	Learning Rate: 0.000382621
	LOSS [training: 0.24327762537358208 | validation: 0.2885266164620896]
	TIME [epoch: 7.12 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2540346728229734		[learning rate: 0.00038172]
	Learning Rate: 0.000381719
	LOSS [training: 0.2540346728229734 | validation: 0.3038080057023331]
	TIME [epoch: 7.1 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28795872442026493		[learning rate: 0.00038082]
	Learning Rate: 0.000380818
	LOSS [training: 0.28795872442026493 | validation: 0.33137602755231743]
	TIME [epoch: 7.11 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23842645164924975		[learning rate: 0.00037992]
	Learning Rate: 0.00037992
	LOSS [training: 0.23842645164924975 | validation: 0.21633405110142617]
	TIME [epoch: 7.11 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20223280338744248		[learning rate: 0.00037902]
	Learning Rate: 0.000379024
	LOSS [training: 0.20223280338744248 | validation: 0.20286564433083729]
	TIME [epoch: 7.13 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2087116212457611		[learning rate: 0.00037813]
	Learning Rate: 0.00037813
	LOSS [training: 0.2087116212457611 | validation: 0.21258673422766072]
	TIME [epoch: 7.14 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19853370238614526		[learning rate: 0.00037724]
	Learning Rate: 0.000377238
	LOSS [training: 0.19853370238614526 | validation: 0.18962010553915498]
	TIME [epoch: 7.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1438.pth
	Model improved!!!
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18701170946516546		[learning rate: 0.00037635]
	Learning Rate: 0.000376348
	LOSS [training: 0.18701170946516546 | validation: 0.23578172429359917]
	TIME [epoch: 7.11 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2148015836210413		[learning rate: 0.00037546]
	Learning Rate: 0.00037546
	LOSS [training: 0.2148015836210413 | validation: 0.21298938386338329]
	TIME [epoch: 7.11 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20623997690464466		[learning rate: 0.00037457]
	Learning Rate: 0.000374575
	LOSS [training: 0.20623997690464466 | validation: 0.21496727477114397]
	TIME [epoch: 7.11 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23080722834816297		[learning rate: 0.00037369]
	Learning Rate: 0.000373691
	LOSS [training: 0.23080722834816297 | validation: 0.23677602613678095]
	TIME [epoch: 7.13 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2158899473234622		[learning rate: 0.00037281]
	Learning Rate: 0.00037281
	LOSS [training: 0.2158899473234622 | validation: 0.21229810973053503]
	TIME [epoch: 7.14 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20173329554811698		[learning rate: 0.00037193]
	Learning Rate: 0.00037193
	LOSS [training: 0.20173329554811698 | validation: 0.29955646205931713]
	TIME [epoch: 7.12 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22514591079819338		[learning rate: 0.00037105]
	Learning Rate: 0.000371053
	LOSS [training: 0.22514591079819338 | validation: 0.21356005415009738]
	TIME [epoch: 7.12 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.184026515754957		[learning rate: 0.00037018]
	Learning Rate: 0.000370178
	LOSS [training: 0.184026515754957 | validation: 0.20227705183576955]
	TIME [epoch: 7.12 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19750143658054076		[learning rate: 0.0003693]
	Learning Rate: 0.000369305
	LOSS [training: 0.19750143658054076 | validation: 0.21559648167541373]
	TIME [epoch: 7.11 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18923948278348218		[learning rate: 0.00036843]
	Learning Rate: 0.000368433
	LOSS [training: 0.18923948278348218 | validation: 0.24180031958287054]
	TIME [epoch: 7.12 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22205382796771161		[learning rate: 0.00036756]
	Learning Rate: 0.000367564
	LOSS [training: 0.22205382796771161 | validation: 0.2544993547946375]
	TIME [epoch: 7.15 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2453967462934946		[learning rate: 0.0003667]
	Learning Rate: 0.000366697
	LOSS [training: 0.2453967462934946 | validation: 0.2751460842818538]
	TIME [epoch: 7.11 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.241595607687971		[learning rate: 0.00036583]
	Learning Rate: 0.000365832
	LOSS [training: 0.241595607687971 | validation: 0.2658202297465725]
	TIME [epoch: 7.11 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20647633311638067		[learning rate: 0.00036497]
	Learning Rate: 0.000364969
	LOSS [training: 0.20647633311638067 | validation: 0.19524295017539173]
	TIME [epoch: 7.11 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21326647274379246		[learning rate: 0.00036411]
	Learning Rate: 0.000364108
	LOSS [training: 0.21326647274379246 | validation: 0.18783891714670098]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1453.pth
	Model improved!!!
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22218341312032025		[learning rate: 0.00036325]
	Learning Rate: 0.00036325
	LOSS [training: 0.22218341312032025 | validation: 0.23962606218194527]
	TIME [epoch: 7.12 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2083533306703093		[learning rate: 0.00036239]
	Learning Rate: 0.000362393
	LOSS [training: 0.2083533306703093 | validation: 0.21023032022597948]
	TIME [epoch: 7.14 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21321690898333467		[learning rate: 0.00036154]
	Learning Rate: 0.000361538
	LOSS [training: 0.21321690898333467 | validation: 0.2320146626467967]
	TIME [epoch: 7.11 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21402208908412348		[learning rate: 0.00036069]
	Learning Rate: 0.000360685
	LOSS [training: 0.21402208908412348 | validation: 0.3260891897453666]
	TIME [epoch: 7.11 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22680299955126987		[learning rate: 0.00035983]
	Learning Rate: 0.000359834
	LOSS [training: 0.22680299955126987 | validation: 0.21746423165459172]
	TIME [epoch: 7.11 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1938637635706722		[learning rate: 0.00035899]
	Learning Rate: 0.000358986
	LOSS [training: 0.1938637635706722 | validation: 0.21676301990367597]
	TIME [epoch: 7.12 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2137406146467166		[learning rate: 0.00035814]
	Learning Rate: 0.000358139
	LOSS [training: 0.2137406146467166 | validation: 0.28914846393374605]
	TIME [epoch: 7.13 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2450160450388244		[learning rate: 0.00035729]
	Learning Rate: 0.000357294
	LOSS [training: 0.2450160450388244 | validation: 0.24904612647798643]
	TIME [epoch: 7.15 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19564460203479173		[learning rate: 0.00035645]
	Learning Rate: 0.000356451
	LOSS [training: 0.19564460203479173 | validation: 0.20944238992587488]
	TIME [epoch: 7.12 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22089445731677587		[learning rate: 0.00035561]
	Learning Rate: 0.00035561
	LOSS [training: 0.22089445731677587 | validation: 0.19672590498778822]
	TIME [epoch: 7.12 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2146314326636405		[learning rate: 0.00035477]
	Learning Rate: 0.000354771
	LOSS [training: 0.2146314326636405 | validation: 0.2243397991254344]
	TIME [epoch: 7.11 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20453850100255214		[learning rate: 0.00035393]
	Learning Rate: 0.000353935
	LOSS [training: 0.20453850100255214 | validation: 0.2740751245513749]
	TIME [epoch: 7.11 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20080097516696052		[learning rate: 0.0003531]
	Learning Rate: 0.0003531
	LOSS [training: 0.20080097516696052 | validation: 0.2118605819804829]
	TIME [epoch: 7.12 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23040148473041508		[learning rate: 0.00035227]
	Learning Rate: 0.000352267
	LOSS [training: 0.23040148473041508 | validation: 0.3250023440315829]
	TIME [epoch: 7.14 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27367938241069417		[learning rate: 0.00035144]
	Learning Rate: 0.000351436
	LOSS [training: 0.27367938241069417 | validation: 0.300041482127791]
	TIME [epoch: 7.11 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3001235244179519		[learning rate: 0.00035061]
	Learning Rate: 0.000350607
	LOSS [training: 0.3001235244179519 | validation: 0.3113533539573109]
	TIME [epoch: 7.11 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26222027155616867		[learning rate: 0.00034978]
	Learning Rate: 0.00034978
	LOSS [training: 0.26222027155616867 | validation: 0.23657149704168007]
	TIME [epoch: 7.11 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21296524856133647		[learning rate: 0.00034895]
	Learning Rate: 0.000348955
	LOSS [training: 0.21296524856133647 | validation: 0.19265377319741023]
	TIME [epoch: 7.11 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1854279138684424		[learning rate: 0.00034813]
	Learning Rate: 0.000348132
	LOSS [training: 0.1854279138684424 | validation: 0.1948212095057414]
	TIME [epoch: 7.13 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1816031911368293		[learning rate: 0.00034731]
	Learning Rate: 0.000347311
	LOSS [training: 0.1816031911368293 | validation: 0.2124609441691412]
	TIME [epoch: 7.13 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20935762511007622		[learning rate: 0.00034649]
	Learning Rate: 0.000346491
	LOSS [training: 0.20935762511007622 | validation: 0.2802153964385594]
	TIME [epoch: 7.11 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21342043920013182		[learning rate: 0.00034567]
	Learning Rate: 0.000345674
	LOSS [training: 0.21342043920013182 | validation: 0.22188004926960972]
	TIME [epoch: 7.11 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2072502433155826		[learning rate: 0.00034486]
	Learning Rate: 0.000344859
	LOSS [training: 0.2072502433155826 | validation: 0.27281844615267237]
	TIME [epoch: 7.12 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20660672560846347		[learning rate: 0.00034405]
	Learning Rate: 0.000344045
	LOSS [training: 0.20660672560846347 | validation: 0.1849013552720841]
	TIME [epoch: 7.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1477.pth
	Model improved!!!
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21298990939788354		[learning rate: 0.00034323]
	Learning Rate: 0.000343233
	LOSS [training: 0.21298990939788354 | validation: 0.2266809512688247]
	TIME [epoch: 7.15 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23175683409867634		[learning rate: 0.00034242]
	Learning Rate: 0.000342424
	LOSS [training: 0.23175683409867634 | validation: 0.23747679244186837]
	TIME [epoch: 7.13 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2249911249380836		[learning rate: 0.00034162]
	Learning Rate: 0.000341616
	LOSS [training: 0.2249911249380836 | validation: 0.22298837596759014]
	TIME [epoch: 7.11 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19209505685755057		[learning rate: 0.00034081]
	Learning Rate: 0.00034081
	LOSS [training: 0.19209505685755057 | validation: 0.2100794905611243]
	TIME [epoch: 7.11 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18556595892894734		[learning rate: 0.00034001]
	Learning Rate: 0.000340006
	LOSS [training: 0.18556595892894734 | validation: 0.2127450605481979]
	TIME [epoch: 7.11 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2078634176055373		[learning rate: 0.0003392]
	Learning Rate: 0.000339204
	LOSS [training: 0.2078634176055373 | validation: 0.25149495306341496]
	TIME [epoch: 7.11 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19415533979352267		[learning rate: 0.0003384]
	Learning Rate: 0.000338404
	LOSS [training: 0.19415533979352267 | validation: 0.2373962451294814]
	TIME [epoch: 7.14 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18663071751438956		[learning rate: 0.00033761]
	Learning Rate: 0.000337606
	LOSS [training: 0.18663071751438956 | validation: 0.21483781772436666]
	TIME [epoch: 7.12 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2097921792708596		[learning rate: 0.00033681]
	Learning Rate: 0.00033681
	LOSS [training: 0.2097921792708596 | validation: 0.32466514481586783]
	TIME [epoch: 7.11 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23387129784024152		[learning rate: 0.00033602]
	Learning Rate: 0.000336015
	LOSS [training: 0.23387129784024152 | validation: 0.3779202139823096]
	TIME [epoch: 7.11 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3061430707632454		[learning rate: 0.00033522]
	Learning Rate: 0.000335223
	LOSS [training: 0.3061430707632454 | validation: 0.40232047140525473]
	TIME [epoch: 7.11 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34395059239018083		[learning rate: 0.00033443]
	Learning Rate: 0.000334432
	LOSS [training: 0.34395059239018083 | validation: 0.3435869420280513]
	TIME [epoch: 7.11 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23972760482718244		[learning rate: 0.00033364]
	Learning Rate: 0.000333643
	LOSS [training: 0.23972760482718244 | validation: 0.21879552811267416]
	TIME [epoch: 7.14 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19290542175676895		[learning rate: 0.00033286]
	Learning Rate: 0.000332856
	LOSS [training: 0.19290542175676895 | validation: 0.20568703264719826]
	TIME [epoch: 7.12 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18387354957922833		[learning rate: 0.00033207]
	Learning Rate: 0.000332071
	LOSS [training: 0.18387354957922833 | validation: 0.225356995851757]
	TIME [epoch: 7.11 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23975958355768695		[learning rate: 0.00033129]
	Learning Rate: 0.000331288
	LOSS [training: 0.23975958355768695 | validation: 0.22759381111431723]
	TIME [epoch: 7.12 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20196526569953194		[learning rate: 0.00033051]
	Learning Rate: 0.000330506
	LOSS [training: 0.20196526569953194 | validation: 0.21108542097889016]
	TIME [epoch: 7.12 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1779050783542801		[learning rate: 0.00032973]
	Learning Rate: 0.000329726
	LOSS [training: 0.1779050783542801 | validation: 0.2092192443853878]
	TIME [epoch: 7.11 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17953651247600988		[learning rate: 0.00032895]
	Learning Rate: 0.000328949
	LOSS [training: 0.17953651247600988 | validation: 0.21896831660548272]
	TIME [epoch: 7.14 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17590181290176013		[learning rate: 0.00032817]
	Learning Rate: 0.000328173
	LOSS [training: 0.17590181290176013 | validation: 0.24905727552235513]
	TIME [epoch: 7.13 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25131783260916374		[learning rate: 0.0003274]
	Learning Rate: 0.000327399
	LOSS [training: 0.25131783260916374 | validation: 0.2253218469814781]
	TIME [epoch: 7.11 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19783029255875728		[learning rate: 0.00032663]
	Learning Rate: 0.000326626
	LOSS [training: 0.19783029255875728 | validation: 0.20885432430147394]
	TIME [epoch: 7.11 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18431749795448488		[learning rate: 0.00032586]
	Learning Rate: 0.000325856
	LOSS [training: 0.18431749795448488 | validation: 0.20416295920976343]
	TIME [epoch: 7.11 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18497152004668974		[learning rate: 0.00032509]
	Learning Rate: 0.000325087
	LOSS [training: 0.18497152004668974 | validation: 0.282503816595288]
	TIME [epoch: 7.12 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20992187511001553		[learning rate: 0.00032432]
	Learning Rate: 0.00032432
	LOSS [training: 0.20992187511001553 | validation: 0.26913853306080016]
	TIME [epoch: 7.16 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20355596453582425		[learning rate: 0.00032356]
	Learning Rate: 0.000323555
	LOSS [training: 0.20355596453582425 | validation: 0.22407473707671222]
	TIME [epoch: 7.13 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19586068139850618		[learning rate: 0.00032279]
	Learning Rate: 0.000322792
	LOSS [training: 0.19586068139850618 | validation: 0.2357417108016191]
	TIME [epoch: 7.11 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18830702801007923		[learning rate: 0.00032203]
	Learning Rate: 0.000322031
	LOSS [training: 0.18830702801007923 | validation: 0.2659879238696222]
	TIME [epoch: 7.11 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19256744161333406		[learning rate: 0.00032127]
	Learning Rate: 0.000321271
	LOSS [training: 0.19256744161333406 | validation: 0.2136038975971576]
	TIME [epoch: 7.11 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18283290900342108		[learning rate: 0.00032051]
	Learning Rate: 0.000320513
	LOSS [training: 0.18283290900342108 | validation: 0.21032926350170547]
	TIME [epoch: 7.11 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20910484516626857		[learning rate: 0.00031976]
	Learning Rate: 0.000319757
	LOSS [training: 0.20910484516626857 | validation: 0.2549369942829438]
	TIME [epoch: 7.14 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19260583032397063		[learning rate: 0.000319]
	Learning Rate: 0.000319003
	LOSS [training: 0.19260583032397063 | validation: 0.1967363567173904]
	TIME [epoch: 7.13 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1710862943914204		[learning rate: 0.00031825]
	Learning Rate: 0.000318251
	LOSS [training: 0.1710862943914204 | validation: 0.20524687769290018]
	TIME [epoch: 7.12 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22461460251739726		[learning rate: 0.0003175]
	Learning Rate: 0.0003175
	LOSS [training: 0.22461460251739726 | validation: 0.2452805796444108]
	TIME [epoch: 7.12 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22764066495194654		[learning rate: 0.00031675]
	Learning Rate: 0.000316751
	LOSS [training: 0.22764066495194654 | validation: 0.2090880402255407]
	TIME [epoch: 7.11 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21568217721637364		[learning rate: 0.000316]
	Learning Rate: 0.000316004
	LOSS [training: 0.21568217721637364 | validation: 0.2267585603533071]
	TIME [epoch: 7.12 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20528218562895403		[learning rate: 0.00031526]
	Learning Rate: 0.000315258
	LOSS [training: 0.20528218562895403 | validation: 0.20058937809177593]
	TIME [epoch: 7.16 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21310791096754658		[learning rate: 0.00031451]
	Learning Rate: 0.000314515
	LOSS [training: 0.21310791096754658 | validation: 0.2450712033095089]
	TIME [epoch: 7.12 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25672091737421643		[learning rate: 0.00031377]
	Learning Rate: 0.000313773
	LOSS [training: 0.25672091737421643 | validation: 0.21799258142417205]
	TIME [epoch: 7.11 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2491292870154589		[learning rate: 0.00031303]
	Learning Rate: 0.000313033
	LOSS [training: 0.2491292870154589 | validation: 0.2514281177277773]
	TIME [epoch: 7.12 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19490182603992573		[learning rate: 0.00031229]
	Learning Rate: 0.000312294
	LOSS [training: 0.19490182603992573 | validation: 0.2112373398438146]
	TIME [epoch: 7.11 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21754409482854378		[learning rate: 0.00031156]
	Learning Rate: 0.000311558
	LOSS [training: 0.21754409482854378 | validation: 0.24726349956362403]
	TIME [epoch: 7.11 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2740928325158341		[learning rate: 0.00031082]
	Learning Rate: 0.000310823
	LOSS [training: 0.2740928325158341 | validation: 0.3633541740123265]
	TIME [epoch: 7.15 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.368513718003305		[learning rate: 0.00031009]
	Learning Rate: 0.00031009
	LOSS [training: 0.368513718003305 | validation: 0.25363110636577246]
	TIME [epoch: 7.12 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22802031155820734		[learning rate: 0.00030936]
	Learning Rate: 0.000309358
	LOSS [training: 0.22802031155820734 | validation: 0.20441806696382225]
	TIME [epoch: 7.11 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18192728533261088		[learning rate: 0.00030863]
	Learning Rate: 0.000308628
	LOSS [training: 0.18192728533261088 | validation: 0.23592250248073376]
	TIME [epoch: 7.1 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19347583703300775		[learning rate: 0.0003079]
	Learning Rate: 0.0003079
	LOSS [training: 0.19347583703300775 | validation: 0.19788234079594602]
	TIME [epoch: 7.11 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2171827759600152		[learning rate: 0.00030717]
	Learning Rate: 0.000307174
	LOSS [training: 0.2171827759600152 | validation: 0.2537273662959614]
	TIME [epoch: 7.1 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22328222086500887		[learning rate: 0.00030645]
	Learning Rate: 0.00030645
	LOSS [training: 0.22328222086500887 | validation: 0.19952395978576198]
	TIME [epoch: 7.14 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1873425031760844		[learning rate: 0.00030573]
	Learning Rate: 0.000305727
	LOSS [training: 0.1873425031760844 | validation: 0.20170902513740585]
	TIME [epoch: 7.12 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1871725881988444		[learning rate: 0.00030501]
	Learning Rate: 0.000305005
	LOSS [training: 0.1871725881988444 | validation: 0.18892999837492286]
	TIME [epoch: 7.12 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17159806575167996		[learning rate: 0.00030429]
	Learning Rate: 0.000304286
	LOSS [training: 0.17159806575167996 | validation: 0.2007638909533614]
	TIME [epoch: 7.11 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16767654871849236		[learning rate: 0.00030357]
	Learning Rate: 0.000303568
	LOSS [training: 0.16767654871849236 | validation: 0.1901905489866216]
	TIME [epoch: 7.12 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17186913073360643		[learning rate: 0.00030285]
	Learning Rate: 0.000302852
	LOSS [training: 0.17186913073360643 | validation: 0.18807904484254007]
	TIME [epoch: 7.11 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18472404621785654		[learning rate: 0.00030214]
	Learning Rate: 0.000302138
	LOSS [training: 0.18472404621785654 | validation: 0.17624700833135964]
	TIME [epoch: 7.15 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1532.pth
	Model improved!!!
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1607046130677008		[learning rate: 0.00030143]
	Learning Rate: 0.000301425
	LOSS [training: 0.1607046130677008 | validation: 0.20165026929106059]
	TIME [epoch: 7.12 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16685883385128966		[learning rate: 0.00030071]
	Learning Rate: 0.000300714
	LOSS [training: 0.16685883385128966 | validation: 0.21297961008548208]
	TIME [epoch: 7.11 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18059326497905848		[learning rate: 0.0003]
	Learning Rate: 0.000300005
	LOSS [training: 0.18059326497905848 | validation: 0.20502579610256047]
	TIME [epoch: 7.12 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1974568293058657		[learning rate: 0.0002993]
	Learning Rate: 0.000299297
	LOSS [training: 0.1974568293058657 | validation: 0.27797465358688017]
	TIME [epoch: 7.11 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24905316472261338		[learning rate: 0.00029859]
	Learning Rate: 0.000298591
	LOSS [training: 0.24905316472261338 | validation: 0.2470589486776175]
	TIME [epoch: 7.11 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2186997057591526		[learning rate: 0.00029789]
	Learning Rate: 0.000297887
	LOSS [training: 0.2186997057591526 | validation: 0.21974868778495105]
	TIME [epoch: 7.15 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18601086011959364		[learning rate: 0.00029718]
	Learning Rate: 0.000297184
	LOSS [training: 0.18601086011959364 | validation: 0.18883492224368573]
	TIME [epoch: 7.12 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1810060102511523		[learning rate: 0.00029648]
	Learning Rate: 0.000296483
	LOSS [training: 0.1810060102511523 | validation: 0.2011147120903371]
	TIME [epoch: 7.12 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21377548429735643		[learning rate: 0.00029578]
	Learning Rate: 0.000295784
	LOSS [training: 0.21377548429735643 | validation: 0.22176745553704005]
	TIME [epoch: 7.11 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1915207915176819		[learning rate: 0.00029509]
	Learning Rate: 0.000295086
	LOSS [training: 0.1915207915176819 | validation: 0.2762847002452942]
	TIME [epoch: 7.11 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24179839228002065		[learning rate: 0.00029439]
	Learning Rate: 0.00029439
	LOSS [training: 0.24179839228002065 | validation: 0.21205523970281023]
	TIME [epoch: 7.12 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19816680710632		[learning rate: 0.0002937]
	Learning Rate: 0.000293696
	LOSS [training: 0.19816680710632 | validation: 0.21369733014106085]
	TIME [epoch: 7.16 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19495300295396628		[learning rate: 0.000293]
	Learning Rate: 0.000293003
	LOSS [training: 0.19495300295396628 | validation: 0.2191709261967052]
	TIME [epoch: 7.14 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1907705196934269		[learning rate: 0.00029231]
	Learning Rate: 0.000292312
	LOSS [training: 0.1907705196934269 | validation: 0.1957136135498308]
	TIME [epoch: 7.12 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17588545597426813		[learning rate: 0.00029162]
	Learning Rate: 0.000291622
	LOSS [training: 0.17588545597426813 | validation: 0.20050320150862794]
	TIME [epoch: 7.13 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2658536141503899		[learning rate: 0.00029093]
	Learning Rate: 0.000290934
	LOSS [training: 0.2658536141503899 | validation: 0.21147321868778726]
	TIME [epoch: 7.11 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1820179405683044		[learning rate: 0.00029025]
	Learning Rate: 0.000290248
	LOSS [training: 0.1820179405683044 | validation: 0.1880716835221412]
	TIME [epoch: 7.12 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1810315174602563		[learning rate: 0.00028956]
	Learning Rate: 0.000289563
	LOSS [training: 0.1810315174602563 | validation: 0.20470236051814114]
	TIME [epoch: 7.15 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17945498236917318		[learning rate: 0.00028888]
	Learning Rate: 0.00028888
	LOSS [training: 0.17945498236917318 | validation: 0.20846242866152312]
	TIME [epoch: 7.13 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19623450364127146		[learning rate: 0.0002882]
	Learning Rate: 0.000288199
	LOSS [training: 0.19623450364127146 | validation: 0.23307906317957278]
	TIME [epoch: 7.11 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2075894187443888		[learning rate: 0.00028752]
	Learning Rate: 0.000287519
	LOSS [training: 0.2075894187443888 | validation: 0.1771798482428226]
	TIME [epoch: 7.12 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19413312685800915		[learning rate: 0.00028684]
	Learning Rate: 0.000286841
	LOSS [training: 0.19413312685800915 | validation: 0.20916700372160418]
	TIME [epoch: 7.12 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25173491832497		[learning rate: 0.00028616]
	Learning Rate: 0.000286164
	LOSS [training: 0.25173491832497 | validation: 0.2875970291231912]
	TIME [epoch: 7.12 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24934185733771774		[learning rate: 0.00028549]
	Learning Rate: 0.000285489
	LOSS [training: 0.24934185733771774 | validation: 0.24136734726803238]
	TIME [epoch: 7.15 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18777401968738974		[learning rate: 0.00028482]
	Learning Rate: 0.000284816
	LOSS [training: 0.18777401968738974 | validation: 0.2157945531936546]
	TIME [epoch: 7.12 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1829601409615366		[learning rate: 0.00028414]
	Learning Rate: 0.000284144
	LOSS [training: 0.1829601409615366 | validation: 0.3182580650010849]
	TIME [epoch: 7.11 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2207417234578993		[learning rate: 0.00028347]
	Learning Rate: 0.000283474
	LOSS [training: 0.2207417234578993 | validation: 0.2529678591539657]
	TIME [epoch: 7.12 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18242788039061617		[learning rate: 0.00028281]
	Learning Rate: 0.000282805
	LOSS [training: 0.18242788039061617 | validation: 0.19034835690692786]
	TIME [epoch: 7.12 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19364759092956133		[learning rate: 0.00028214]
	Learning Rate: 0.000282138
	LOSS [training: 0.19364759092956133 | validation: 0.19218599878545767]
	TIME [epoch: 7.12 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19401790520420578		[learning rate: 0.00028147]
	Learning Rate: 0.000281472
	LOSS [training: 0.19401790520420578 | validation: 0.2099944441010641]
	TIME [epoch: 7.15 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19875175530312247		[learning rate: 0.00028081]
	Learning Rate: 0.000280808
	LOSS [training: 0.19875175530312247 | validation: 0.24022252462622956]
	TIME [epoch: 7.12 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1820453263779474		[learning rate: 0.00028015]
	Learning Rate: 0.000280146
	LOSS [training: 0.1820453263779474 | validation: 0.22703047366944845]
	TIME [epoch: 7.11 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21654493772604858		[learning rate: 0.00027949]
	Learning Rate: 0.000279485
	LOSS [training: 0.21654493772604858 | validation: 0.28699631012943505]
	TIME [epoch: 7.12 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23606652535464248		[learning rate: 0.00027883]
	Learning Rate: 0.000278826
	LOSS [training: 0.23606652535464248 | validation: 0.21332366488468862]
	TIME [epoch: 7.16 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19756763272900296		[learning rate: 0.00027817]
	Learning Rate: 0.000278168
	LOSS [training: 0.19756763272900296 | validation: 0.1840311845551007]
	TIME [epoch: 7.16 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18821650242613117		[learning rate: 0.00027751]
	Learning Rate: 0.000277512
	LOSS [training: 0.18821650242613117 | validation: 0.18144863641802583]
	TIME [epoch: 7.2 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1736211741589398		[learning rate: 0.00027686]
	Learning Rate: 0.000276858
	LOSS [training: 0.1736211741589398 | validation: 0.2567539720082229]
	TIME [epoch: 7.17 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22883671964149088		[learning rate: 0.0002762]
	Learning Rate: 0.000276204
	LOSS [training: 0.22883671964149088 | validation: 0.40760050212096044]
	TIME [epoch: 7.17 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30138263301130247		[learning rate: 0.00027555]
	Learning Rate: 0.000275553
	LOSS [training: 0.30138263301130247 | validation: 0.2647251138989455]
	TIME [epoch: 7.16 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19719478089105075		[learning rate: 0.0002749]
	Learning Rate: 0.000274903
	LOSS [training: 0.19719478089105075 | validation: 0.2006115513429226]
	TIME [epoch: 7.16 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1899455982171982		[learning rate: 0.00027425]
	Learning Rate: 0.000274255
	LOSS [training: 0.1899455982171982 | validation: 0.2156299808073306]
	TIME [epoch: 7.17 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1859919740759522		[learning rate: 0.00027361]
	Learning Rate: 0.000273608
	LOSS [training: 0.1859919740759522 | validation: 0.2007916112837334]
	TIME [epoch: 7.2 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19277507769657556		[learning rate: 0.00027296]
	Learning Rate: 0.000272962
	LOSS [training: 0.19277507769657556 | validation: 0.21893532406265925]
	TIME [epoch: 7.16 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18219332511925307		[learning rate: 0.00027232]
	Learning Rate: 0.000272318
	LOSS [training: 0.18219332511925307 | validation: 0.19364387847095674]
	TIME [epoch: 7.16 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17140455358423648		[learning rate: 0.00027168]
	Learning Rate: 0.000271676
	LOSS [training: 0.17140455358423648 | validation: 0.21798234844719916]
	TIME [epoch: 7.16 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22641000695064983		[learning rate: 0.00027104]
	Learning Rate: 0.000271035
	LOSS [training: 0.22641000695064983 | validation: 0.23838957466106087]
	TIME [epoch: 7.16 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.218743851782288		[learning rate: 0.0002704]
	Learning Rate: 0.000270396
	LOSS [training: 0.218743851782288 | validation: 0.2117971999144893]
	TIME [epoch: 7.17 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19176852770012776		[learning rate: 0.00026976]
	Learning Rate: 0.000269758
	LOSS [training: 0.19176852770012776 | validation: 0.21961353661225602]
	TIME [epoch: 7.21 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20124549669090858		[learning rate: 0.00026912]
	Learning Rate: 0.000269122
	LOSS [training: 0.20124549669090858 | validation: 0.28164277041801256]
	TIME [epoch: 7.16 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2317594929565586		[learning rate: 0.00026849]
	Learning Rate: 0.000268487
	LOSS [training: 0.2317594929565586 | validation: 0.3056858328154098]
	TIME [epoch: 7.16 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20636370608100685		[learning rate: 0.00026785]
	Learning Rate: 0.000267854
	LOSS [training: 0.20636370608100685 | validation: 0.2203749118451733]
	TIME [epoch: 7.15 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18630347034492137		[learning rate: 0.00026722]
	Learning Rate: 0.000267222
	LOSS [training: 0.18630347034492137 | validation: 0.2269748511047256]
	TIME [epoch: 7.15 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1836734472662266		[learning rate: 0.00026659]
	Learning Rate: 0.000266591
	LOSS [training: 0.1836734472662266 | validation: 0.1750921396947022]
	TIME [epoch: 7.17 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1585.pth
	Model improved!!!
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16876548419665696		[learning rate: 0.00026596]
	Learning Rate: 0.000265963
	LOSS [training: 0.16876548419665696 | validation: 0.20706035400184813]
	TIME [epoch: 7.17 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18312595208032312		[learning rate: 0.00026534]
	Learning Rate: 0.000265335
	LOSS [training: 0.18312595208032312 | validation: 0.31191135738448483]
	TIME [epoch: 7.14 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24118527034248255		[learning rate: 0.00026471]
	Learning Rate: 0.000264709
	LOSS [training: 0.24118527034248255 | validation: 0.20872781671500412]
	TIME [epoch: 7.13 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19993831192423678		[learning rate: 0.00026408]
	Learning Rate: 0.000264085
	LOSS [training: 0.19993831192423678 | validation: 0.19673454055581777]
	TIME [epoch: 7.14 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.171016917787297		[learning rate: 0.00026346]
	Learning Rate: 0.000263462
	LOSS [training: 0.171016917787297 | validation: 0.21531609634922858]
	TIME [epoch: 7.14 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1831580221217271		[learning rate: 0.00026284]
	Learning Rate: 0.00026284
	LOSS [training: 0.1831580221217271 | validation: 0.23458734372021756]
	TIME [epoch: 7.15 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17486183685064619		[learning rate: 0.00026222]
	Learning Rate: 0.00026222
	LOSS [training: 0.17486183685064619 | validation: 0.2283712404963016]
	TIME [epoch: 7.18 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18396798519455232		[learning rate: 0.0002616]
	Learning Rate: 0.000261602
	LOSS [training: 0.18396798519455232 | validation: 0.2015081422783257]
	TIME [epoch: 7.14 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20710944399527528		[learning rate: 0.00026098]
	Learning Rate: 0.000260985
	LOSS [training: 0.20710944399527528 | validation: 0.2572498583829812]
	TIME [epoch: 7.14 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2365345476533609		[learning rate: 0.00026037]
	Learning Rate: 0.000260369
	LOSS [training: 0.2365345476533609 | validation: 0.2175810536965212]
	TIME [epoch: 7.13 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23057700501296535		[learning rate: 0.00025976]
	Learning Rate: 0.000259755
	LOSS [training: 0.23057700501296535 | validation: 0.21295920029521725]
	TIME [epoch: 7.13 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1860758934536753		[learning rate: 0.00025914]
	Learning Rate: 0.000259142
	LOSS [training: 0.1860758934536753 | validation: 0.18892813644967646]
	TIME [epoch: 7.15 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1636213869322503		[learning rate: 0.00025853]
	Learning Rate: 0.000258531
	LOSS [training: 0.1636213869322503 | validation: 0.1954759227368837]
	TIME [epoch: 7.14 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17771433734941117		[learning rate: 0.00025792]
	Learning Rate: 0.000257921
	LOSS [training: 0.17771433734941117 | validation: 0.17604537716423066]
	TIME [epoch: 7.14 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1831660759941009		[learning rate: 0.00025731]
	Learning Rate: 0.000257313
	LOSS [training: 0.1831660759941009 | validation: 0.18577304601179456]
	TIME [epoch: 7.12 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1664089596876971		[learning rate: 0.00025671]
	Learning Rate: 0.000256706
	LOSS [training: 0.1664089596876971 | validation: 0.21995282872267813]
	TIME [epoch: 7.13 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21554445223564342		[learning rate: 0.0002561]
	Learning Rate: 0.0002561
	LOSS [training: 0.21554445223564342 | validation: 0.28233635864022066]
	TIME [epoch: 7.12 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19384743539786708		[learning rate: 0.0002555]
	Learning Rate: 0.000255496
	LOSS [training: 0.19384743539786708 | validation: 0.2105870281640208]
	TIME [epoch: 7.15 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1957062186353588		[learning rate: 0.00025489]
	Learning Rate: 0.000254894
	LOSS [training: 0.1957062186353588 | validation: 0.22565602123998263]
	TIME [epoch: 7.15 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1731943533340935		[learning rate: 0.00025429]
	Learning Rate: 0.000254292
	LOSS [training: 0.1731943533340935 | validation: 0.23837321222523644]
	TIME [epoch: 7.13 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17980059480879654		[learning rate: 0.00025369]
	Learning Rate: 0.000253693
	LOSS [training: 0.17980059480879654 | validation: 0.25737844292180645]
	TIME [epoch: 7.12 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17646252725455697		[learning rate: 0.00025309]
	Learning Rate: 0.000253094
	LOSS [training: 0.17646252725455697 | validation: 0.19570985309019195]
	TIME [epoch: 7.13 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1980611170830338		[learning rate: 0.0002525]
	Learning Rate: 0.000252497
	LOSS [training: 0.1980611170830338 | validation: 0.18716968544307894]
	TIME [epoch: 7.12 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17364054817097813		[learning rate: 0.0002519]
	Learning Rate: 0.000251902
	LOSS [training: 0.17364054817097813 | validation: 0.20760247187132633]
	TIME [epoch: 7.16 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20239613189368155		[learning rate: 0.00025131]
	Learning Rate: 0.000251307
	LOSS [training: 0.20239613189368155 | validation: 0.18618424736890854]
	TIME [epoch: 7.15 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17662842425563913		[learning rate: 0.00025071]
	Learning Rate: 0.000250714
	LOSS [training: 0.17662842425563913 | validation: 0.20732992631622096]
	TIME [epoch: 7.14 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16989322471311108		[learning rate: 0.00025012]
	Learning Rate: 0.000250123
	LOSS [training: 0.16989322471311108 | validation: 0.19570454066744492]
	TIME [epoch: 7.13 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17573041575891082		[learning rate: 0.00024953]
	Learning Rate: 0.000249533
	LOSS [training: 0.17573041575891082 | validation: 0.19635005831470198]
	TIME [epoch: 7.14 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21942353458925462		[learning rate: 0.00024894]
	Learning Rate: 0.000248945
	LOSS [training: 0.21942353458925462 | validation: 0.19448256280398304]
	TIME [epoch: 7.13 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16525943400014848		[learning rate: 0.00024836]
	Learning Rate: 0.000248357
	LOSS [training: 0.16525943400014848 | validation: 0.19313456632325737]
	TIME [epoch: 7.16 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18294370733117798		[learning rate: 0.00024777]
	Learning Rate: 0.000247771
	LOSS [training: 0.18294370733117798 | validation: 0.20875327526389897]
	TIME [epoch: 7.14 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18169289084399162		[learning rate: 0.00024719]
	Learning Rate: 0.000247187
	LOSS [training: 0.18169289084399162 | validation: 0.1973302720574131]
	TIME [epoch: 7.13 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1725272623269543		[learning rate: 0.0002466]
	Learning Rate: 0.000246604
	LOSS [training: 0.1725272623269543 | validation: 0.19154432979681812]
	TIME [epoch: 7.13 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16799608068787447		[learning rate: 0.00024602]
	Learning Rate: 0.000246022
	LOSS [training: 0.16799608068787447 | validation: 0.21337750286085178]
	TIME [epoch: 7.12 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17414198675777606		[learning rate: 0.00024544]
	Learning Rate: 0.000245442
	LOSS [training: 0.17414198675777606 | validation: 0.18977670166436011]
	TIME [epoch: 7.12 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1660794604154196		[learning rate: 0.00024486]
	Learning Rate: 0.000244863
	LOSS [training: 0.1660794604154196 | validation: 0.19927612619995297]
	TIME [epoch: 7.16 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1684691538339895		[learning rate: 0.00024429]
	Learning Rate: 0.000244285
	LOSS [training: 0.1684691538339895 | validation: 0.18464334755161502]
	TIME [epoch: 7.14 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17538235864635943		[learning rate: 0.00024371]
	Learning Rate: 0.000243709
	LOSS [training: 0.17538235864635943 | validation: 0.21557271386662222]
	TIME [epoch: 7.12 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1768456570029512		[learning rate: 0.00024313]
	Learning Rate: 0.000243134
	LOSS [training: 0.1768456570029512 | validation: 0.18870851166146846]
	TIME [epoch: 7.13 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19184016141293098		[learning rate: 0.00024256]
	Learning Rate: 0.000242561
	LOSS [training: 0.19184016141293098 | validation: 0.21120238611798484]
	TIME [epoch: 7.13 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18540083584010766		[learning rate: 0.00024199]
	Learning Rate: 0.000241989
	LOSS [training: 0.18540083584010766 | validation: 0.2090835317345086]
	TIME [epoch: 7.13 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16534730884092855		[learning rate: 0.00024142]
	Learning Rate: 0.000241418
	LOSS [training: 0.16534730884092855 | validation: 0.24016541699527094]
	TIME [epoch: 7.16 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18156183150024238		[learning rate: 0.00024085]
	Learning Rate: 0.000240848
	LOSS [training: 0.18156183150024238 | validation: 0.22461630573151004]
	TIME [epoch: 7.15 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18799874235259717		[learning rate: 0.00024028]
	Learning Rate: 0.00024028
	LOSS [training: 0.18799874235259717 | validation: 0.35025297371018826]
	TIME [epoch: 7.14 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25810655089018997		[learning rate: 0.00023971]
	Learning Rate: 0.000239713
	LOSS [training: 0.25810655089018997 | validation: 0.2886119262674916]
	TIME [epoch: 7.14 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2042898996872366		[learning rate: 0.00023915]
	Learning Rate: 0.000239148
	LOSS [training: 0.2042898996872366 | validation: 0.2509769976888542]
	TIME [epoch: 7.13 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17625889014188603		[learning rate: 0.00023858]
	Learning Rate: 0.000238584
	LOSS [training: 0.17625889014188603 | validation: 0.18601798184107615]
	TIME [epoch: 7.13 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15974948075950834		[learning rate: 0.00023802]
	Learning Rate: 0.000238021
	LOSS [training: 0.15974948075950834 | validation: 0.20714196406996066]
	TIME [epoch: 7.17 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17016090196775913		[learning rate: 0.00023746]
	Learning Rate: 0.00023746
	LOSS [training: 0.17016090196775913 | validation: 0.20406759124917917]
	TIME [epoch: 7.14 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1777398687171213		[learning rate: 0.0002369]
	Learning Rate: 0.000236899
	LOSS [training: 0.1777398687171213 | validation: 0.2988327887586691]
	TIME [epoch: 7.13 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22562667257665817		[learning rate: 0.00023634]
	Learning Rate: 0.000236341
	LOSS [training: 0.22562667257665817 | validation: 0.33511767172265494]
	TIME [epoch: 7.13 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2722496852154239		[learning rate: 0.00023578]
	Learning Rate: 0.000235783
	LOSS [training: 0.2722496852154239 | validation: 0.34298193673187927]
	TIME [epoch: 7.13 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24976291096827508		[learning rate: 0.00023523]
	Learning Rate: 0.000235227
	LOSS [training: 0.24976291096827508 | validation: 0.31635479974064545]
	TIME [epoch: 7.13 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23703199749163356		[learning rate: 0.00023467]
	Learning Rate: 0.000234672
	LOSS [training: 0.23703199749163356 | validation: 0.23566662785148632]
	TIME [epoch: 7.16 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20709993874785101		[learning rate: 0.00023412]
	Learning Rate: 0.000234119
	LOSS [training: 0.20709993874785101 | validation: 0.21407330970811878]
	TIME [epoch: 7.14 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2125090031661167		[learning rate: 0.00023357]
	Learning Rate: 0.000233566
	LOSS [training: 0.2125090031661167 | validation: 0.2012469064017393]
	TIME [epoch: 7.12 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22573404230510236		[learning rate: 0.00023302]
	Learning Rate: 0.000233015
	LOSS [training: 0.22573404230510236 | validation: 0.23420240109833396]
	TIME [epoch: 7.12 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1925164131039739		[learning rate: 0.00023247]
	Learning Rate: 0.000232466
	LOSS [training: 0.1925164131039739 | validation: 0.18253771164698646]
	TIME [epoch: 7.13 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16679193146753518		[learning rate: 0.00023192]
	Learning Rate: 0.000231917
	LOSS [training: 0.16679193146753518 | validation: 0.19368042434151492]
	TIME [epoch: 7.12 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17976492555647766		[learning rate: 0.00023137]
	Learning Rate: 0.00023137
	LOSS [training: 0.17976492555647766 | validation: 0.24234485319049087]
	TIME [epoch: 7.17 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1735892801372982		[learning rate: 0.00023082]
	Learning Rate: 0.000230825
	LOSS [training: 0.1735892801372982 | validation: 0.18072794456039135]
	TIME [epoch: 7.13 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16718935495235882		[learning rate: 0.00023028]
	Learning Rate: 0.00023028
	LOSS [training: 0.16718935495235882 | validation: 0.22086715754398079]
	TIME [epoch: 7.13 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18646619216900487		[learning rate: 0.00022974]
	Learning Rate: 0.000229737
	LOSS [training: 0.18646619216900487 | validation: 0.21698652766885923]
	TIME [epoch: 7.12 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20044668885934974		[learning rate: 0.00022919]
	Learning Rate: 0.000229195
	LOSS [training: 0.20044668885934974 | validation: 0.23171272171024776]
	TIME [epoch: 7.13 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20710182436334743		[learning rate: 0.00022865]
	Learning Rate: 0.000228654
	LOSS [training: 0.20710182436334743 | validation: 0.23330231846742802]
	TIME [epoch: 7.12 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17734281063383958		[learning rate: 0.00022811]
	Learning Rate: 0.000228115
	LOSS [training: 0.17734281063383958 | validation: 0.18894165455675727]
	TIME [epoch: 7.17 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16299503389372405		[learning rate: 0.00022758]
	Learning Rate: 0.000227577
	LOSS [training: 0.16299503389372405 | validation: 0.20191134021440815]
	TIME [epoch: 7.13 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1702325116102788		[learning rate: 0.00022704]
	Learning Rate: 0.00022704
	LOSS [training: 0.1702325116102788 | validation: 0.18907506400471316]
	TIME [epoch: 7.13 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16691485587130384		[learning rate: 0.0002265]
	Learning Rate: 0.000226505
	LOSS [training: 0.16691485587130384 | validation: 0.23734909318802744]
	TIME [epoch: 7.12 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17436262202448205		[learning rate: 0.00022597]
	Learning Rate: 0.00022597
	LOSS [training: 0.17436262202448205 | validation: 0.1854326901296344]
	TIME [epoch: 7.13 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16125828935551034		[learning rate: 0.00022544]
	Learning Rate: 0.000225437
	LOSS [training: 0.16125828935551034 | validation: 0.19331401508978413]
	TIME [epoch: 7.12 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16296839588389359		[learning rate: 0.00022491]
	Learning Rate: 0.000224905
	LOSS [training: 0.16296839588389359 | validation: 0.189611951839818]
	TIME [epoch: 7.16 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16037799344329087		[learning rate: 0.00022437]
	Learning Rate: 0.000224375
	LOSS [training: 0.16037799344329087 | validation: 0.20484585190419768]
	TIME [epoch: 7.13 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16962593118359967		[learning rate: 0.00022385]
	Learning Rate: 0.000223846
	LOSS [training: 0.16962593118359967 | validation: 0.21319140563800723]
	TIME [epoch: 7.13 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17104891265071517		[learning rate: 0.00022332]
	Learning Rate: 0.000223318
	LOSS [training: 0.17104891265071517 | validation: 0.21346533355057593]
	TIME [epoch: 7.13 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1623110014520628		[learning rate: 0.00022279]
	Learning Rate: 0.000222791
	LOSS [training: 0.1623110014520628 | validation: 0.19344398819872266]
	TIME [epoch: 7.13 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18479587084193638		[learning rate: 0.00022227]
	Learning Rate: 0.000222265
	LOSS [training: 0.18479587084193638 | validation: 0.18008782078261937]
	TIME [epoch: 7.12 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1813150767000078		[learning rate: 0.00022174]
	Learning Rate: 0.000221741
	LOSS [training: 0.1813150767000078 | validation: 0.1856217454978364]
	TIME [epoch: 7.17 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1870064218295036		[learning rate: 0.00022122]
	Learning Rate: 0.000221218
	LOSS [training: 0.1870064218295036 | validation: 0.20506020756012724]
	TIME [epoch: 7.13 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1861912729765544		[learning rate: 0.0002207]
	Learning Rate: 0.000220696
	LOSS [training: 0.1861912729765544 | validation: 0.19744105562275632]
	TIME [epoch: 7.12 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1744348024508327		[learning rate: 0.00022018]
	Learning Rate: 0.000220176
	LOSS [training: 0.1744348024508327 | validation: 0.1892802590944449]
	TIME [epoch: 7.13 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17970239146248773		[learning rate: 0.00021966]
	Learning Rate: 0.000219656
	LOSS [training: 0.17970239146248773 | validation: 0.20150774811780736]
	TIME [epoch: 7.13 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18754339115503196		[learning rate: 0.00021914]
	Learning Rate: 0.000219138
	LOSS [training: 0.18754339115503196 | validation: 0.19625609810155836]
	TIME [epoch: 7.12 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1722934220823469		[learning rate: 0.00021862]
	Learning Rate: 0.000218621
	LOSS [training: 0.1722934220823469 | validation: 0.19482279077789988]
	TIME [epoch: 7.16 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16442328039324572		[learning rate: 0.00021811]
	Learning Rate: 0.000218106
	LOSS [training: 0.16442328039324572 | validation: 0.20360683399256663]
	TIME [epoch: 7.13 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1714291441344923		[learning rate: 0.00021759]
	Learning Rate: 0.000217591
	LOSS [training: 0.1714291441344923 | validation: 0.20869235488466265]
	TIME [epoch: 7.12 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17385694681544775		[learning rate: 0.00021708]
	Learning Rate: 0.000217078
	LOSS [training: 0.17385694681544775 | validation: 0.2124582115624783]
	TIME [epoch: 7.12 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17974811050222494		[learning rate: 0.00021657]
	Learning Rate: 0.000216566
	LOSS [training: 0.17974811050222494 | validation: 0.24471998326206348]
	TIME [epoch: 7.12 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20517026205667604		[learning rate: 0.00021605]
	Learning Rate: 0.000216055
	LOSS [training: 0.20517026205667604 | validation: 0.23020844200348878]
	TIME [epoch: 7.12 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17260079058154273		[learning rate: 0.00021555]
	Learning Rate: 0.000215545
	LOSS [training: 0.17260079058154273 | validation: 0.1931244261945903]
	TIME [epoch: 7.16 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1611320127066114		[learning rate: 0.00021504]
	Learning Rate: 0.000215037
	LOSS [training: 0.1611320127066114 | validation: 0.1906868326392332]
	TIME [epoch: 7.13 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16860216675624518		[learning rate: 0.00021453]
	Learning Rate: 0.00021453
	LOSS [training: 0.16860216675624518 | validation: 0.20068445231902243]
	TIME [epoch: 7.13 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17002696321133143		[learning rate: 0.00021402]
	Learning Rate: 0.000214024
	LOSS [training: 0.17002696321133143 | validation: 0.2015644739865482]
	TIME [epoch: 7.13 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19369801110223994		[learning rate: 0.00021352]
	Learning Rate: 0.000213519
	LOSS [training: 0.19369801110223994 | validation: 0.18359557322466094]
	TIME [epoch: 7.12 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17722090423280434		[learning rate: 0.00021302]
	Learning Rate: 0.000213015
	LOSS [training: 0.17722090423280434 | validation: 0.2170235406769338]
	TIME [epoch: 7.13 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18089062201066977		[learning rate: 0.00021251]
	Learning Rate: 0.000212513
	LOSS [training: 0.18089062201066977 | validation: 0.18151326622303465]
	TIME [epoch: 7.16 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1681160904031903		[learning rate: 0.00021201]
	Learning Rate: 0.000212011
	LOSS [training: 0.1681160904031903 | validation: 0.1801973488363563]
	TIME [epoch: 7.13 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17695703116813624		[learning rate: 0.00021151]
	Learning Rate: 0.000211511
	LOSS [training: 0.17695703116813624 | validation: 0.22574206595185323]
	TIME [epoch: 7.12 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16920958742191816		[learning rate: 0.00021101]
	Learning Rate: 0.000211012
	LOSS [training: 0.16920958742191816 | validation: 0.20219106661310088]
	TIME [epoch: 7.13 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18418758381957567		[learning rate: 0.00021051]
	Learning Rate: 0.000210514
	LOSS [training: 0.18418758381957567 | validation: 0.29617683916854415]
	TIME [epoch: 7.12 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2593858930020712		[learning rate: 0.00021002]
	Learning Rate: 0.000210018
	LOSS [training: 0.2593858930020712 | validation: 0.29346364223051663]
	TIME [epoch: 7.13 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2036061115472035		[learning rate: 0.00020952]
	Learning Rate: 0.000209523
	LOSS [training: 0.2036061115472035 | validation: 0.2064660144792684]
	TIME [epoch: 7.16 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17261569724853382		[learning rate: 0.00020903]
	Learning Rate: 0.000209028
	LOSS [training: 0.17261569724853382 | validation: 0.1816106495555685]
	TIME [epoch: 7.12 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17181793513225874		[learning rate: 0.00020854]
	Learning Rate: 0.000208535
	LOSS [training: 0.17181793513225874 | validation: 0.22133479860505495]
	TIME [epoch: 7.13 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17613107621496502		[learning rate: 0.00020804]
	Learning Rate: 0.000208043
	LOSS [training: 0.17613107621496502 | validation: 0.18797888129078114]
	TIME [epoch: 7.12 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16088703009128782		[learning rate: 0.00020755]
	Learning Rate: 0.000207553
	LOSS [training: 0.16088703009128782 | validation: 0.1616202651617475]
	TIME [epoch: 7.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v1_20240222_153948/states/model_phi1_1a_v1_1691.pth
	Model improved!!!
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1636157928816217		[learning rate: 0.00020706]
	Learning Rate: 0.000207063
	LOSS [training: 0.1636157928816217 | validation: 0.19496252691354346]
	TIME [epoch: 7.12 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19875046683443248		[learning rate: 0.00020657]
	Learning Rate: 0.000206575
	LOSS [training: 0.19875046683443248 | validation: 0.20674357200001256]
	TIME [epoch: 7.16 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18489307307473649		[learning rate: 0.00020609]
	Learning Rate: 0.000206087
	LOSS [training: 0.18489307307473649 | validation: 0.17457511961368502]
	TIME [epoch: 7.12 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1649360961378428		[learning rate: 0.0002056]
	Learning Rate: 0.000205601
	LOSS [training: 0.1649360961378428 | validation: 0.1702137217733633]
	TIME [epoch: 7.13 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17275962203181647		[learning rate: 0.00020512]
	Learning Rate: 0.000205116
	LOSS [training: 0.17275962203181647 | validation: 0.17267428674123522]
	TIME [epoch: 7.12 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17112295164802768		[learning rate: 0.00020463]
	Learning Rate: 0.000204632
	LOSS [training: 0.17112295164802768 | validation: 0.18725039053609427]
	TIME [epoch: 7.12 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16543586654698733		[learning rate: 0.00020415]
	Learning Rate: 0.00020415
	LOSS [training: 0.16543586654698733 | validation: 0.21040939593905228]
	TIME [epoch: 7.12 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17217069601204865		[learning rate: 0.00020367]
	Learning Rate: 0.000203668
	LOSS [training: 0.17217069601204865 | validation: 0.23085566492457438]
	TIME [epoch: 7.16 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17699338197637146		[learning rate: 0.00020319]
	Learning Rate: 0.000203188
	LOSS [training: 0.17699338197637146 | validation: 0.23436592129983314]
	TIME [epoch: 7.11 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18080899638569228		[learning rate: 0.00020271]
	Learning Rate: 0.000202708
	LOSS [training: 0.18080899638569228 | validation: 0.23670027696861998]
	TIME [epoch: 7.12 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17506801517079645		[learning rate: 0.00020223]
	Learning Rate: 0.00020223
	LOSS [training: 0.17506801517079645 | validation: 0.19354132294405446]
	TIME [epoch: 7.11 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17927526444062067		[learning rate: 0.00020175]
	Learning Rate: 0.000201753
	LOSS [training: 0.17927526444062067 | validation: 0.23078756173957457]
	TIME [epoch: 7.12 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21737332480358312		[learning rate: 0.00020128]
	Learning Rate: 0.000201277
	LOSS [training: 0.21737332480358312 | validation: 0.24594096664692344]
	TIME [epoch: 7.12 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20543303710050145		[learning rate: 0.0002008]
	Learning Rate: 0.000200803
	LOSS [training: 0.20543303710050145 | validation: 0.2233776451459415]
	TIME [epoch: 7.14 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.183248197536204		[learning rate: 0.00020033]
	Learning Rate: 0.000200329
	LOSS [training: 0.183248197536204 | validation: 0.20279347072810436]
	TIME [epoch: 7.11 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18242555764438867		[learning rate: 0.00019986]
	Learning Rate: 0.000199856
	LOSS [training: 0.18242555764438867 | validation: 0.24017369683219575]
	TIME [epoch: 7.11 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2581455248967847		[learning rate: 0.00019938]
	Learning Rate: 0.000199385
	LOSS [training: 0.2581455248967847 | validation: 0.2719563328172595]
	TIME [epoch: 7.11 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.211907700376484		[learning rate: 0.00019891]
	Learning Rate: 0.000198915
	LOSS [training: 0.211907700376484 | validation: 0.1901615330478962]
	TIME [epoch: 7.11 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18126004581406796		[learning rate: 0.00019845]
	Learning Rate: 0.000198445
	LOSS [training: 0.18126004581406796 | validation: 0.19561565229457784]
	TIME [epoch: 7.13 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17011710545294653		[learning rate: 0.00019798]
	Learning Rate: 0.000197977
	LOSS [training: 0.17011710545294653 | validation: 0.20773371157342063]
	TIME [epoch: 7.14 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1628847064008682		[learning rate: 0.00019751]
	Learning Rate: 0.00019751
	LOSS [training: 0.1628847064008682 | validation: 0.20050151086682694]
	TIME [epoch: 7.12 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16475756242171893		[learning rate: 0.00019704]
	Learning Rate: 0.000197044
	LOSS [training: 0.16475756242171893 | validation: 0.22712791211077848]
	TIME [epoch: 7.11 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17706837776791445		[learning rate: 0.00019658]
	Learning Rate: 0.00019658
	LOSS [training: 0.17706837776791445 | validation: 0.2150393150722653]
	TIME [epoch: 7.12 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15995407153965158		[learning rate: 0.00019612]
	Learning Rate: 0.000196116
	LOSS [training: 0.15995407153965158 | validation: 0.24689571703321495]
	TIME [epoch: 7.11 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19839097159305946		[learning rate: 0.00019565]
	Learning Rate: 0.000195653
	LOSS [training: 0.19839097159305946 | validation: 0.26673721909893056]
	TIME [epoch: 7.12 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1854041418731764		[learning rate: 0.00019519]
	Learning Rate: 0.000195192
	LOSS [training: 0.1854041418731764 | validation: 0.21156983036879576]
	TIME [epoch: 7.14 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15701729956000815		[learning rate: 0.00019473]
	Learning Rate: 0.000194731
	LOSS [training: 0.15701729956000815 | validation: 0.1924640163023808]
	TIME [epoch: 7.11 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15458934775459646		[learning rate: 0.00019427]
	Learning Rate: 0.000194272
	LOSS [training: 0.15458934775459646 | validation: 0.17520171989657146]
	TIME [epoch: 7.11 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16996912196781416		[learning rate: 0.00019381]
	Learning Rate: 0.000193814
	LOSS [training: 0.16996912196781416 | validation: 0.18492883655075315]
	TIME [epoch: 7.11 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17551667172608534		[learning rate: 0.00019336]
	Learning Rate: 0.000193357
	LOSS [training: 0.17551667172608534 | validation: 0.17734561118097547]
	TIME [epoch: 7.11 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16118972165545084		[learning rate: 0.0001929]
	Learning Rate: 0.0001929
	LOSS [training: 0.16118972165545084 | validation: 0.20069826739520763]
	TIME [epoch: 7.12 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16900090264954706		[learning rate: 0.00019245]
	Learning Rate: 0.000192445
	LOSS [training: 0.16900090264954706 | validation: 0.23843491133361638]
	TIME [epoch: 7.14 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17353610137741807		[learning rate: 0.00019199]
	Learning Rate: 0.000191992
	LOSS [training: 0.17353610137741807 | validation: 0.22299738664414598]
	TIME [epoch: 7.11 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17871916526752163		[learning rate: 0.00019154]
	Learning Rate: 0.000191539
	LOSS [training: 0.17871916526752163 | validation: 0.21159557224722036]
	TIME [epoch: 7.11 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16772681702096026		[learning rate: 0.00019109]
	Learning Rate: 0.000191087
	LOSS [training: 0.16772681702096026 | validation: 0.20207856717950684]
	TIME [epoch: 7.11 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19250908277093984		[learning rate: 0.00019064]
	Learning Rate: 0.000190636
	LOSS [training: 0.19250908277093984 | validation: 0.2742782219717389]
	TIME [epoch: 7.11 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2019467049463945		[learning rate: 0.00019019]
	Learning Rate: 0.000190186
	LOSS [training: 0.2019467049463945 | validation: 0.17466724198563824]
	TIME [epoch: 7.13 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18939544979400277		[learning rate: 0.00018974]
	Learning Rate: 0.000189738
	LOSS [training: 0.18939544979400277 | validation: 0.17718519540927985]
	TIME [epoch: 7.15 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16615873088245547		[learning rate: 0.00018929]
	Learning Rate: 0.00018929
	LOSS [training: 0.16615873088245547 | validation: 0.18855061473466792]
	TIME [epoch: 7.12 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1589793881190376		[learning rate: 0.00018884]
	Learning Rate: 0.000188844
	LOSS [training: 0.1589793881190376 | validation: 0.19327986052604879]
	TIME [epoch: 7.11 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17041507869369915		[learning rate: 0.0001884]
	Learning Rate: 0.000188398
	LOSS [training: 0.17041507869369915 | validation: 0.23582928818443127]
	TIME [epoch: 7.11 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1904672015359844		[learning rate: 0.00018795]
	Learning Rate: 0.000187954
	LOSS [training: 0.1904672015359844 | validation: 0.19514002728907812]
	TIME [epoch: 7.11 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17061668869875962		[learning rate: 0.00018751]
	Learning Rate: 0.000187511
	LOSS [training: 0.17061668869875962 | validation: 0.1717780911625172]
	TIME [epoch: 7.12 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16605009151855937		[learning rate: 0.00018707]
	Learning Rate: 0.000187068
	LOSS [training: 0.16605009151855937 | validation: 0.1736259992656975]
	TIME [epoch: 7.15 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16518779908726233		[learning rate: 0.00018663]
	Learning Rate: 0.000186627
	LOSS [training: 0.16518779908726233 | validation: 0.18064149860237325]
	TIME [epoch: 7.11 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16454165090769352		[learning rate: 0.00018619]
	Learning Rate: 0.000186187
	LOSS [training: 0.16454165090769352 | validation: 0.2174089348686311]
	TIME [epoch: 7.11 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1936242338132921		[learning rate: 0.00018575]
	Learning Rate: 0.000185748
	LOSS [training: 0.1936242338132921 | validation: 0.23544154782663743]
	TIME [epoch: 7.11 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19724649510800008		[learning rate: 0.00018531]
	Learning Rate: 0.000185309
	LOSS [training: 0.19724649510800008 | validation: 0.2512289392409986]
	TIME [epoch: 7.12 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20169446583362127		[learning rate: 0.00018487]
	Learning Rate: 0.000184872
	LOSS [training: 0.20169446583362127 | validation: 0.22002807701610355]
	TIME [epoch: 7.12 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1844211719642383		[learning rate: 0.00018444]
	Learning Rate: 0.000184436
	LOSS [training: 0.1844211719642383 | validation: 0.207085737106036]
	TIME [epoch: 7.14 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18140162606259438		[learning rate: 0.000184]
	Learning Rate: 0.000184001
	LOSS [training: 0.18140162606259438 | validation: 0.1792591110114325]
	TIME [epoch: 7.11 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1645117622911311		[learning rate: 0.00018357]
	Learning Rate: 0.000183567
	LOSS [training: 0.1645117622911311 | validation: 0.17604763434649995]
	TIME [epoch: 7.11 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16374520964160505		[learning rate: 0.00018313]
	Learning Rate: 0.000183134
	LOSS [training: 0.16374520964160505 | validation: 0.171742511416677]
	TIME [epoch: 7.12 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16730622043211602		[learning rate: 0.0001827]
	Learning Rate: 0.000182702
	LOSS [training: 0.16730622043211602 | validation: 0.1888306360250097]
	TIME [epoch: 7.13 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18172741173379736		[learning rate: 0.00018227]
	Learning Rate: 0.000182271
	LOSS [training: 0.18172741173379736 | validation: 0.19935276961691403]
	TIME [epoch: 7.12 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2027030114588701		[learning rate: 0.00018184]
	Learning Rate: 0.000181841
	LOSS [training: 0.2027030114588701 | validation: 0.21059976277873105]
	TIME [epoch: 7.15 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2025121409754503		[learning rate: 0.00018141]
	Learning Rate: 0.000181412
	LOSS [training: 0.2025121409754503 | validation: 0.19192370475093512]
	TIME [epoch: 7.12 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1771988577223959		[learning rate: 0.00018098]
	Learning Rate: 0.000180984
	LOSS [training: 0.1771988577223959 | validation: 0.17698641352099187]
	TIME [epoch: 7.12 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15796296778991042		[learning rate: 0.00018056]
	Learning Rate: 0.000180557
	LOSS [training: 0.15796296778991042 | validation: 0.18135725374386163]
	TIME [epoch: 7.11 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1612632313973839		[learning rate: 0.00018013]
	Learning Rate: 0.000180132
	LOSS [training: 0.1612632313973839 | validation: 0.19626978771654335]
	TIME [epoch: 7.12 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18305473459299393		[learning rate: 0.00017971]
	Learning Rate: 0.000179707
	LOSS [training: 0.18305473459299393 | validation: 0.18976227668742843]
	TIME [epoch: 7.12 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1661764486265778		[learning rate: 0.00017928]
	Learning Rate: 0.000179283
	LOSS [training: 0.1661764486265778 | validation: 0.1869996295700882]
	TIME [epoch: 7.14 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16914557741141917		[learning rate: 0.00017886]
	Learning Rate: 0.00017886
	LOSS [training: 0.16914557741141917 | validation: 0.2045433892998208]
	TIME [epoch: 7.11 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16434850622911365		[learning rate: 0.00017844]
	Learning Rate: 0.000178438
	LOSS [training: 0.16434850622911365 | validation: 0.17556010912389017]
	TIME [epoch: 7.12 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17006718953032912		[learning rate: 0.00017802]
	Learning Rate: 0.000178017
	LOSS [training: 0.17006718953032912 | validation: 0.20925752271427017]
	TIME [epoch: 7.11 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19870907372130314		[learning rate: 0.0001776]
	Learning Rate: 0.000177597
	LOSS [training: 0.19870907372130314 | validation: 0.20330616276546565]
	TIME [epoch: 7.11 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.204825646173976		[learning rate: 0.00017718]
	Learning Rate: 0.000177178
	LOSS [training: 0.204825646173976 | validation: 0.19446732561059243]
	TIME [epoch: 7.13 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16784681700401033		[learning rate: 0.00017676]
	Learning Rate: 0.00017676
	LOSS [training: 0.16784681700401033 | validation: 0.17591365966368927]
	TIME [epoch: 7.13 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16373782382180485		[learning rate: 0.00017634]
	Learning Rate: 0.000176343
	LOSS [training: 0.16373782382180485 | validation: 0.2029199916932224]
	TIME [epoch: 7.12 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16919186863738972		[learning rate: 0.00017593]
	Learning Rate: 0.000175927
	LOSS [training: 0.16919186863738972 | validation: 0.1851391199773968]
	TIME [epoch: 7.12 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16405830094322144		[learning rate: 0.00017551]
	Learning Rate: 0.000175512
	LOSS [training: 0.16405830094322144 | validation: 0.17158735168536243]
	TIME [epoch: 7.12 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1596517740914806		[learning rate: 0.0001751]
	Learning Rate: 0.000175098
	LOSS [training: 0.1596517740914806 | validation: 0.18354221599181053]
	TIME [epoch: 7.12 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17210944792422592		[learning rate: 0.00017469]
	Learning Rate: 0.000174685
	LOSS [training: 0.17210944792422592 | validation: 0.199453954969731]
	TIME [epoch: 7.15 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17793452991435563		[learning rate: 0.00017427]
	Learning Rate: 0.000174273
	LOSS [training: 0.17793452991435563 | validation: 0.17683077552686657]
	TIME [epoch: 7.13 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16788932771744958		[learning rate: 0.00017386]
	Learning Rate: 0.000173862
	LOSS [training: 0.16788932771744958 | validation: 0.20711239193467196]
	TIME [epoch: 7.12 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16146010712297507		[learning rate: 0.00017345]
	Learning Rate: 0.000173452
	LOSS [training: 0.16146010712297507 | validation: 0.18674544498953718]
	TIME [epoch: 7.11 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1778176048472683		[learning rate: 0.00017304]
	Learning Rate: 0.000173043
	LOSS [training: 0.1778176048472683 | validation: 0.2151432184511355]
	TIME [epoch: 7.11 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20254655579538478		[learning rate: 0.00017263]
	Learning Rate: 0.000172635
	LOSS [training: 0.20254655579538478 | validation: 0.1854520840444165]
	TIME [epoch: 7.11 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1972703468612286		[learning rate: 0.00017223]
	Learning Rate: 0.000172228
	LOSS [training: 0.1972703468612286 | validation: 0.19493434154626088]
	TIME [epoch: 7.15 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21223412630665295		[learning rate: 0.00017182]
	Learning Rate: 0.000171821
	LOSS [training: 0.21223412630665295 | validation: 0.2406993012840527]
	TIME [epoch: 7.12 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22559580441267496		[learning rate: 0.00017142]
	Learning Rate: 0.000171416
	LOSS [training: 0.22559580441267496 | validation: 0.21573513735231767]
	TIME [epoch: 7.11 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2024515207039041		[learning rate: 0.00017101]
	Learning Rate: 0.000171012
	LOSS [training: 0.2024515207039041 | validation: 0.2023384691895896]
	TIME [epoch: 7.11 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.181289779159514		[learning rate: 0.00017061]
	Learning Rate: 0.000170608
	LOSS [training: 0.181289779159514 | validation: 0.1960064836304124]
	TIME [epoch: 7.11 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17784496582678203		[learning rate: 0.00017021]
	Learning Rate: 0.000170206
	LOSS [training: 0.17784496582678203 | validation: 0.23695376715837196]
	TIME [epoch: 7.11 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18580171605559617		[learning rate: 0.0001698]
	Learning Rate: 0.000169804
	LOSS [training: 0.18580171605559617 | validation: 0.22078487076560513]
	TIME [epoch: 7.14 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1681151962325737		[learning rate: 0.0001694]
	Learning Rate: 0.000169404
	LOSS [training: 0.1681151962325737 | validation: 0.17758605506024808]
	TIME [epoch: 7.13 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15537649400155568		[learning rate: 0.000169]
	Learning Rate: 0.000169004
	LOSS [training: 0.15537649400155568 | validation: 0.17374437861005385]
	TIME [epoch: 7.11 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15481747060718642		[learning rate: 0.00016861]
	Learning Rate: 0.000168606
	LOSS [training: 0.15481747060718642 | validation: 0.18180818162680518]
	TIME [epoch: 7.12 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15758239832454302		[learning rate: 0.00016821]
	Learning Rate: 0.000168208
	LOSS [training: 0.15758239832454302 | validation: 0.19982070375857436]
	TIME [epoch: 7.12 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17272559991695666		[learning rate: 0.00016781]
	Learning Rate: 0.000167811
	LOSS [training: 0.17272559991695666 | validation: 0.22370995413746084]
	TIME [epoch: 7.12 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1654985413367032		[learning rate: 0.00016742]
	Learning Rate: 0.000167415
	LOSS [training: 0.1654985413367032 | validation: 0.19092713279661516]
	TIME [epoch: 7.15 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1506364962172979		[learning rate: 0.00016702]
	Learning Rate: 0.00016702
	LOSS [training: 0.1506364962172979 | validation: 0.19466616092754457]
	TIME [epoch: 7.13 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.167333311686643		[learning rate: 0.00016663]
	Learning Rate: 0.000166626
	LOSS [training: 0.167333311686643 | validation: 0.22375671999996877]
	TIME [epoch: 7.11 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17233145489980745		[learning rate: 0.00016623]
	Learning Rate: 0.000166233
	LOSS [training: 0.17233145489980745 | validation: 0.21519123750350627]
	TIME [epoch: 7.12 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1556982251469594		[learning rate: 0.00016584]
	Learning Rate: 0.000165841
	LOSS [training: 0.1556982251469594 | validation: 0.2055231406326915]
	TIME [epoch: 7.11 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17481059037384294		[learning rate: 0.00016545]
	Learning Rate: 0.00016545
	LOSS [training: 0.17481059037384294 | validation: 0.21300856324331668]
	TIME [epoch: 7.12 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19177168332346373		[learning rate: 0.00016506]
	Learning Rate: 0.00016506
	LOSS [training: 0.19177168332346373 | validation: 0.24936371603384383]
	TIME [epoch: 7.14 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1829574332588983		[learning rate: 0.00016467]
	Learning Rate: 0.00016467
	LOSS [training: 0.1829574332588983 | validation: 0.21078138984860922]
	TIME [epoch: 7.13 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17120284809243727		[learning rate: 0.00016428]
	Learning Rate: 0.000164282
	LOSS [training: 0.17120284809243727 | validation: 0.21218971338284068]
	TIME [epoch: 7.11 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16384208384613547		[learning rate: 0.00016389]
	Learning Rate: 0.000163894
	LOSS [training: 0.16384208384613547 | validation: 0.18469907095008747]
	TIME [epoch: 7.12 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15067040804242796		[learning rate: 0.00016351]
	Learning Rate: 0.000163508
	LOSS [training: 0.15067040804242796 | validation: 0.1840093282951179]
	TIME [epoch: 7.11 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18126951459965007		[learning rate: 0.00016312]
	Learning Rate: 0.000163122
	LOSS [training: 0.18126951459965007 | validation: 0.24433554808373564]
	TIME [epoch: 7.12 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1755105136451005		[learning rate: 0.00016274]
	Learning Rate: 0.000162737
	LOSS [training: 0.1755105136451005 | validation: 0.1942413408538206]
	TIME [epoch: 7.15 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1603894105176582		[learning rate: 0.00016235]
	Learning Rate: 0.000162353
	LOSS [training: 0.1603894105176582 | validation: 0.19065204715406295]
	TIME [epoch: 7.13 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15415972217142365		[learning rate: 0.00016197]
	Learning Rate: 0.00016197
	LOSS [training: 0.15415972217142365 | validation: 0.17527473814457756]
	TIME [epoch: 7.12 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16654876315035985		[learning rate: 0.00016159]
	Learning Rate: 0.000161588
	LOSS [training: 0.16654876315035985 | validation: 0.18769653122808796]
	TIME [epoch: 7.11 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1723791461684224		[learning rate: 0.00016121]
	Learning Rate: 0.000161207
	LOSS [training: 0.1723791461684224 | validation: 0.21967766888148987]
	TIME [epoch: 7.11 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18601355849856457		[learning rate: 0.00016083]
	Learning Rate: 0.000160827
	LOSS [training: 0.18601355849856457 | validation: 0.18842347063013007]
	TIME [epoch: 7.12 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16455611188425945		[learning rate: 0.00016045]
	Learning Rate: 0.000160448
	LOSS [training: 0.16455611188425945 | validation: 0.2247634647289341]
	TIME [epoch: 7.14 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16578401326089126		[learning rate: 0.00016007]
	Learning Rate: 0.000160069
	LOSS [training: 0.16578401326089126 | validation: 0.20500252427701943]
	TIME [epoch: 7.13 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16157093777562562		[learning rate: 0.00015969]
	Learning Rate: 0.000159692
	LOSS [training: 0.16157093777562562 | validation: 0.1890793675531801]
	TIME [epoch: 7.11 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15582244520214678		[learning rate: 0.00015931]
	Learning Rate: 0.000159315
	LOSS [training: 0.15582244520214678 | validation: 0.17165018075532837]
	TIME [epoch: 7.11 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17651801704811668		[learning rate: 0.00015894]
	Learning Rate: 0.000158939
	LOSS [training: 0.17651801704811668 | validation: 0.17663993279145832]
	TIME [epoch: 7.12 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16519645773554553		[learning rate: 0.00015856]
	Learning Rate: 0.000158564
	LOSS [training: 0.16519645773554553 | validation: 0.16229545217142577]
	TIME [epoch: 7.12 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16175740990613013		[learning rate: 0.00015819]
	Learning Rate: 0.00015819
	LOSS [training: 0.16175740990613013 | validation: 0.1842885240976067]
	TIME [epoch: 7.16 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16865269973442204		[learning rate: 0.00015782]
	Learning Rate: 0.000157817
	LOSS [training: 0.16865269973442204 | validation: 0.17955860619982647]
	TIME [epoch: 7.13 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16494077515121236		[learning rate: 0.00015744]
	Learning Rate: 0.000157445
	LOSS [training: 0.16494077515121236 | validation: 0.17032483572778845]
	TIME [epoch: 7.12 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16285380216501416		[learning rate: 0.00015707]
	Learning Rate: 0.000157073
	LOSS [training: 0.16285380216501416 | validation: 0.17252115605909757]
	TIME [epoch: 7.12 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1619647136827314		[learning rate: 0.0001567]
	Learning Rate: 0.000156703
	LOSS [training: 0.1619647136827314 | validation: 0.1869405750141034]
	TIME [epoch: 7.12 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1528508746863841		[learning rate: 0.00015633]
	Learning Rate: 0.000156333
	LOSS [training: 0.1528508746863841 | validation: 0.17313534001753905]
	TIME [epoch: 7.11 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.158411235967931		[learning rate: 0.00015596]
	Learning Rate: 0.000155964
	LOSS [training: 0.158411235967931 | validation: 0.1725458055763055]
	TIME [epoch: 7.15 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17578094125669835		[learning rate: 0.0001556]
	Learning Rate: 0.000155597
	LOSS [training: 0.17578094125669835 | validation: 0.18128698442249808]
	TIME [epoch: 7.12 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16257191077536726		[learning rate: 0.00015523]
	Learning Rate: 0.00015523
	LOSS [training: 0.16257191077536726 | validation: 0.17498078912742418]
	TIME [epoch: 7.12 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15642848792367403		[learning rate: 0.00015486]
	Learning Rate: 0.000154863
	LOSS [training: 0.15642848792367403 | validation: 0.18716396112652]
	TIME [epoch: 7.11 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1560151441697516		[learning rate: 0.0001545]
	Learning Rate: 0.000154498
	LOSS [training: 0.1560151441697516 | validation: 0.17432745483706136]
	TIME [epoch: 7.12 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16087641281875503		[learning rate: 0.00015413]
	Learning Rate: 0.000154134
	LOSS [training: 0.16087641281875503 | validation: 0.17133330892453058]
	TIME [epoch: 7.11 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17300529755237054		[learning rate: 0.00015377]
	Learning Rate: 0.00015377
	LOSS [training: 0.17300529755237054 | validation: 0.18890706445776723]
	TIME [epoch: 7.15 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16502023643451208		[learning rate: 0.00015341]
	Learning Rate: 0.000153407
	LOSS [training: 0.16502023643451208 | validation: 0.1890655843434763]
	TIME [epoch: 7.12 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15635205704745184		[learning rate: 0.00015305]
	Learning Rate: 0.000153045
	LOSS [training: 0.15635205704745184 | validation: 0.1680180909166655]
	TIME [epoch: 7.12 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15466345995196515		[learning rate: 0.00015268]
	Learning Rate: 0.000152684
	LOSS [training: 0.15466345995196515 | validation: 0.18025041576997403]
	TIME [epoch: 7.11 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15256836219701303		[learning rate: 0.00015232]
	Learning Rate: 0.000152324
	LOSS [training: 0.15256836219701303 | validation: 0.18013971212236143]
	TIME [epoch: 7.11 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1685034502801053		[learning rate: 0.00015196]
	Learning Rate: 0.000151965
	LOSS [training: 0.1685034502801053 | validation: 0.18357557311190045]
	TIME [epoch: 7.11 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15654958584060832		[learning rate: 0.00015161]
	Learning Rate: 0.000151607
	LOSS [training: 0.15654958584060832 | validation: 0.17818571451321538]
	TIME [epoch: 7.14 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16663060552757042		[learning rate: 0.00015125]
	Learning Rate: 0.000151249
	LOSS [training: 0.16663060552757042 | validation: 0.19421425228813338]
	TIME [epoch: 7.12 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1771073845500377		[learning rate: 0.00015089]
	Learning Rate: 0.000150892
	LOSS [training: 0.1771073845500377 | validation: 0.17836736172276046]
	TIME [epoch: 7.11 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16274753756516475		[learning rate: 0.00015054]
	Learning Rate: 0.000150536
	LOSS [training: 0.16274753756516475 | validation: 0.2006673332319084]
	TIME [epoch: 7.11 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1986303052146742		[learning rate: 0.00015018]
	Learning Rate: 0.000150181
	LOSS [training: 0.1986303052146742 | validation: 0.20780977467658918]
	TIME [epoch: 7.11 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17409639007288447		[learning rate: 0.00014983]
	Learning Rate: 0.000149827
	LOSS [training: 0.17409639007288447 | validation: 0.1801675794881929]
	TIME [epoch: 7.12 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16239003472331298		[learning rate: 0.00014947]
	Learning Rate: 0.000149473
	LOSS [training: 0.16239003472331298 | validation: 0.18653214165734736]
	TIME [epoch: 7.14 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1574799617379357		[learning rate: 0.00014912]
	Learning Rate: 0.000149121
	LOSS [training: 0.1574799617379357 | validation: 0.17428617211997985]
	TIME [epoch: 7.12 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15354598263378036		[learning rate: 0.00014877]
	Learning Rate: 0.000148769
	LOSS [training: 0.15354598263378036 | validation: 0.1908218252186515]
	TIME [epoch: 7.11 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14504760330150268		[learning rate: 0.00014842]
	Learning Rate: 0.000148418
	LOSS [training: 0.14504760330150268 | validation: 0.17425311263735094]
	TIME [epoch: 7.11 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1679545889390182		[learning rate: 0.00014807]
	Learning Rate: 0.000148068
	LOSS [training: 0.1679545889390182 | validation: 0.18844540884232658]
	TIME [epoch: 7.1 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17618263735034295		[learning rate: 0.00014772]
	Learning Rate: 0.000147719
	LOSS [training: 0.17618263735034295 | validation: 0.1861335982198034]
	TIME [epoch: 7.11 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1715739464254316		[learning rate: 0.00014737]
	Learning Rate: 0.00014737
	LOSS [training: 0.1715739464254316 | validation: 0.18143330636674251]
	TIME [epoch: 7.14 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18104200740327187		[learning rate: 0.00014702]
	Learning Rate: 0.000147023
	LOSS [training: 0.18104200740327187 | validation: 0.17707466136808403]
	TIME [epoch: 7.12 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15395904862349868		[learning rate: 0.00014668]
	Learning Rate: 0.000146676
	LOSS [training: 0.15395904862349868 | validation: 0.19497824059772784]
	TIME [epoch: 7.1 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15155330054075603		[learning rate: 0.00014633]
	Learning Rate: 0.00014633
	LOSS [training: 0.15155330054075603 | validation: 0.19141929167541333]
	TIME [epoch: 7.11 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16166822945509937		[learning rate: 0.00014598]
	Learning Rate: 0.000145985
	LOSS [training: 0.16166822945509937 | validation: 0.20826026842833606]
	TIME [epoch: 7.11 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18063746126752037		[learning rate: 0.00014564]
	Learning Rate: 0.00014564
	LOSS [training: 0.18063746126752037 | validation: 0.20337027278129638]
	TIME [epoch: 7.11 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19133881901691474		[learning rate: 0.0001453]
	Learning Rate: 0.000145297
	LOSS [training: 0.19133881901691474 | validation: 0.20985452391039972]
	TIME [epoch: 7.14 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16552749724650745		[learning rate: 0.00014495]
	Learning Rate: 0.000144954
	LOSS [training: 0.16552749724650745 | validation: 0.1805970514343682]
	TIME [epoch: 7.11 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15509690183751954		[learning rate: 0.00014461]
	Learning Rate: 0.000144612
	LOSS [training: 0.15509690183751954 | validation: 0.2002613805908316]
	TIME [epoch: 7.1 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18275955432110433		[learning rate: 0.00014427]
	Learning Rate: 0.000144271
	LOSS [training: 0.18275955432110433 | validation: 0.24925869072129206]
	TIME [epoch: 7.11 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21164029216497218		[learning rate: 0.00014393]
	Learning Rate: 0.000143931
	LOSS [training: 0.21164029216497218 | validation: 0.2365417406132364]
	TIME [epoch: 7.1 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1861412711949442		[learning rate: 0.00014359]
	Learning Rate: 0.000143591
	LOSS [training: 0.1861412711949442 | validation: 0.2129967913712632]
	TIME [epoch: 7.11 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1699888877184963		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.1699888877184963 | validation: 0.2018832026235895]
	TIME [epoch: 7.14 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16684070848804755		[learning rate: 0.00014291]
	Learning Rate: 0.000142915
	LOSS [training: 0.16684070848804755 | validation: 0.1819442936010808]
	TIME [epoch: 7.12 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16413270019400916		[learning rate: 0.00014258]
	Learning Rate: 0.000142578
	LOSS [training: 0.16413270019400916 | validation: 0.17629157142657706]
	TIME [epoch: 7.11 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15601538974665488		[learning rate: 0.00014224]
	Learning Rate: 0.000142241
	LOSS [training: 0.15601538974665488 | validation: 0.18122347806198316]
	TIME [epoch: 7.1 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16157756059678458		[learning rate: 0.00014191]
	Learning Rate: 0.000141906
	LOSS [training: 0.16157756059678458 | validation: 0.2065398891131295]
	TIME [epoch: 7.11 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18690154444563561		[learning rate: 0.00014157]
	Learning Rate: 0.000141571
	LOSS [training: 0.18690154444563561 | validation: 0.18896485607108482]
	TIME [epoch: 7.1 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17135284964450886		[learning rate: 0.00014124]
	Learning Rate: 0.000141237
	LOSS [training: 0.17135284964450886 | validation: 0.19236314403589883]
	TIME [epoch: 7.15 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16885557358137632		[learning rate: 0.0001409]
	Learning Rate: 0.000140904
	LOSS [training: 0.16885557358137632 | validation: 0.2028899157548974]
	TIME [epoch: 7.11 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19421294814865084		[learning rate: 0.00014057]
	Learning Rate: 0.000140572
	LOSS [training: 0.19421294814865084 | validation: 0.2048684048626132]
	TIME [epoch: 7.11 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1762920388875165		[learning rate: 0.00014024]
	Learning Rate: 0.00014024
	LOSS [training: 0.1762920388875165 | validation: 0.1840020875190666]
	TIME [epoch: 7.11 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15835372367466705		[learning rate: 0.00013991]
	Learning Rate: 0.000139909
	LOSS [training: 0.15835372367466705 | validation: 0.1841222923213049]
	TIME [epoch: 7.11 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16461977241898068		[learning rate: 0.00013958]
	Learning Rate: 0.000139579
	LOSS [training: 0.16461977241898068 | validation: 0.1877293072777812]
	TIME [epoch: 7.11 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.158780184856961		[learning rate: 0.00013925]
	Learning Rate: 0.00013925
	LOSS [training: 0.158780184856961 | validation: 0.1905761808906063]
	TIME [epoch: 7.15 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16329142961723309		[learning rate: 0.00013892]
	Learning Rate: 0.000138921
	LOSS [training: 0.16329142961723309 | validation: 0.1875145428793512]
	TIME [epoch: 7.11 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16033433386135915		[learning rate: 0.00013859]
	Learning Rate: 0.000138594
	LOSS [training: 0.16033433386135915 | validation: 0.203035564303865]
	TIME [epoch: 7.12 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1845492642713382		[learning rate: 0.00013827]
	Learning Rate: 0.000138267
	LOSS [training: 0.1845492642713382 | validation: 0.21750712630123542]
	TIME [epoch: 7.11 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16275763648689545		[learning rate: 0.00013794]
	Learning Rate: 0.000137941
	LOSS [training: 0.16275763648689545 | validation: 0.19794295115606136]
	TIME [epoch: 7.12 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.161676864697279		[learning rate: 0.00013762]
	Learning Rate: 0.000137615
	LOSS [training: 0.161676864697279 | validation: 0.1802969567200865]
	TIME [epoch: 7.11 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1651962489285133		[learning rate: 0.00013729]
	Learning Rate: 0.000137291
	LOSS [training: 0.1651962489285133 | validation: 0.16887538528326831]
	TIME [epoch: 7.15 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1641952674387813		[learning rate: 0.00013697]
	Learning Rate: 0.000136967
	LOSS [training: 0.1641952674387813 | validation: 0.17642102530286127]
	TIME [epoch: 7.1 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15602480906982488		[learning rate: 0.00013664]
	Learning Rate: 0.000136644
	LOSS [training: 0.15602480906982488 | validation: 0.1642635552272903]
	TIME [epoch: 7.11 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15151804659855966		[learning rate: 0.00013632]
	Learning Rate: 0.000136321
	LOSS [training: 0.15151804659855966 | validation: 0.19524211201696257]
	TIME [epoch: 7.11 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15448239249899645		[learning rate: 0.000136]
	Learning Rate: 0.000136
	LOSS [training: 0.15448239249899645 | validation: 0.17963793855057764]
	TIME [epoch: 7.11 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1474656730676954		[learning rate: 0.00013568]
	Learning Rate: 0.000135679
	LOSS [training: 0.1474656730676954 | validation: 0.18460341165398247]
	TIME [epoch: 7.11 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1669313833784403		[learning rate: 0.00013536]
	Learning Rate: 0.000135359
	LOSS [training: 0.1669313833784403 | validation: 0.2406134186560137]
	TIME [epoch: 7.15 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18378548381935175		[learning rate: 0.00013504]
	Learning Rate: 0.00013504
	LOSS [training: 0.18378548381935175 | validation: 0.24351511484365151]
	TIME [epoch: 7.11 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20659738155140173		[learning rate: 0.00013472]
	Learning Rate: 0.000134721
	LOSS [training: 0.20659738155140173 | validation: 0.25150641895617426]
	TIME [epoch: 7.11 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19551156924732702		[learning rate: 0.0001344]
	Learning Rate: 0.000134403
	LOSS [training: 0.19551156924732702 | validation: 0.2338046013826618]
	TIME [epoch: 7.11 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1823766708268241		[learning rate: 0.00013409]
	Learning Rate: 0.000134086
	LOSS [training: 0.1823766708268241 | validation: 0.20547921209737902]
	TIME [epoch: 7.11 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16180193626907197		[learning rate: 0.00013377]
	Learning Rate: 0.00013377
	LOSS [training: 0.16180193626907197 | validation: 0.20052836152623604]
	TIME [epoch: 7.12 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16267759812210533		[learning rate: 0.00013345]
	Learning Rate: 0.000133455
	LOSS [training: 0.16267759812210533 | validation: 0.22918051009155938]
	TIME [epoch: 7.16 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17616275457688627		[learning rate: 0.00013314]
	Learning Rate: 0.00013314
	LOSS [training: 0.17616275457688627 | validation: 0.2365163109526998]
	TIME [epoch: 7.12 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18175819244395988		[learning rate: 0.00013283]
	Learning Rate: 0.000132826
	LOSS [training: 0.18175819244395988 | validation: 0.21933567426729678]
	TIME [epoch: 7.1 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18016125210739603		[learning rate: 0.00013251]
	Learning Rate: 0.000132512
	LOSS [training: 0.18016125210739603 | validation: 0.22296035770653305]
	TIME [epoch: 7.12 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1782805101771661		[learning rate: 0.0001322]
	Learning Rate: 0.0001322
	LOSS [training: 0.1782805101771661 | validation: 0.22105006580348555]
	TIME [epoch: 7.11 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1665557545212549		[learning rate: 0.00013189]
	Learning Rate: 0.000131888
	LOSS [training: 0.1665557545212549 | validation: 0.20818198193537424]
	TIME [epoch: 7.12 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17548936843413987		[learning rate: 0.00013158]
	Learning Rate: 0.000131577
	LOSS [training: 0.17548936843413987 | validation: 0.2277483453955551]
	TIME [epoch: 7.15 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17579027809364964		[learning rate: 0.00013127]
	Learning Rate: 0.000131266
	LOSS [training: 0.17579027809364964 | validation: 0.20403687099021112]
	TIME [epoch: 7.12 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16549480565303412		[learning rate: 0.00013096]
	Learning Rate: 0.000130957
	LOSS [training: 0.16549480565303412 | validation: 0.20931301480824163]
	TIME [epoch: 7.11 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16255804259064913		[learning rate: 0.00013065]
	Learning Rate: 0.000130648
	LOSS [training: 0.16255804259064913 | validation: 0.19564719131806033]
	TIME [epoch: 7.12 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16139450106854242		[learning rate: 0.00013034]
	Learning Rate: 0.00013034
	LOSS [training: 0.16139450106854242 | validation: 0.181985120774146]
	TIME [epoch: 7.12 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1624394008045865		[learning rate: 0.00013003]
	Learning Rate: 0.000130032
	LOSS [training: 0.1624394008045865 | validation: 0.21083022466762727]
	TIME [epoch: 7.12 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15847225242917973		[learning rate: 0.00012973]
	Learning Rate: 0.000129726
	LOSS [training: 0.15847225242917973 | validation: 0.2208836953206237]
	TIME [epoch: 7.16 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17641979686247772		[learning rate: 0.00012942]
	Learning Rate: 0.00012942
	LOSS [training: 0.17641979686247772 | validation: 0.20364919286618205]
	TIME [epoch: 7.12 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15818888717646068		[learning rate: 0.00012911]
	Learning Rate: 0.000129114
	LOSS [training: 0.15818888717646068 | validation: 0.1911344064329431]
	TIME [epoch: 7.12 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15511510319340166		[learning rate: 0.00012881]
	Learning Rate: 0.00012881
	LOSS [training: 0.15511510319340166 | validation: 0.19115926529851576]
	TIME [epoch: 7.12 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1608863431695089		[learning rate: 0.00012851]
	Learning Rate: 0.000128506
	LOSS [training: 0.1608863431695089 | validation: 0.19709760556796385]
	TIME [epoch: 7.12 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15453177583610786		[learning rate: 0.0001282]
	Learning Rate: 0.000128203
	LOSS [training: 0.15453177583610786 | validation: 0.18895415636515678]
	TIME [epoch: 7.13 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1525736034914714		[learning rate: 0.0001279]
	Learning Rate: 0.0001279
	LOSS [training: 0.1525736034914714 | validation: 0.1821881715334301]
	TIME [epoch: 7.16 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15152114782944592		[learning rate: 0.0001276]
	Learning Rate: 0.000127599
	LOSS [training: 0.15152114782944592 | validation: 0.20202602256510882]
	TIME [epoch: 7.12 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16674247099557746		[learning rate: 0.0001273]
	Learning Rate: 0.000127298
	LOSS [training: 0.16674247099557746 | validation: 0.1679593199452529]
	TIME [epoch: 7.12 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16526387213697025		[learning rate: 0.000127]
	Learning Rate: 0.000126997
	LOSS [training: 0.16526387213697025 | validation: 0.19074356228173545]
	TIME [epoch: 7.12 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1727993980165466		[learning rate: 0.0001267]
	Learning Rate: 0.000126698
	LOSS [training: 0.1727993980165466 | validation: 0.1861473360682746]
	TIME [epoch: 7.12 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17538452807788987		[learning rate: 0.0001264]
	Learning Rate: 0.000126399
	LOSS [training: 0.17538452807788987 | validation: 0.17696838903479226]
	TIME [epoch: 7.12 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1783945797110117		[learning rate: 0.0001261]
	Learning Rate: 0.000126101
	LOSS [training: 0.1783945797110117 | validation: 0.19602672810234195]
	TIME [epoch: 7.14 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18300125600415834		[learning rate: 0.0001258]
	Learning Rate: 0.000125803
	LOSS [training: 0.18300125600415834 | validation: 0.19689656021226754]
	TIME [epoch: 7.11 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16486547382351907		[learning rate: 0.00012551]
	Learning Rate: 0.000125507
	LOSS [training: 0.16486547382351907 | validation: 0.17624226536455623]
	TIME [epoch: 7.12 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1554718444222035		[learning rate: 0.00012521]
	Learning Rate: 0.000125211
	LOSS [training: 0.1554718444222035 | validation: 0.17213243640665443]
	TIME [epoch: 7.11 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16733751429006843		[learning rate: 0.00012492]
	Learning Rate: 0.000124915
	LOSS [training: 0.16733751429006843 | validation: 0.18749347249541548]
	TIME [epoch: 7.11 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16293629538127383		[learning rate: 0.00012462]
	Learning Rate: 0.000124621
	LOSS [training: 0.16293629538127383 | validation: 0.1864625854786163]
	TIME [epoch: 7.13 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16716466662083376		[learning rate: 0.00012433]
	Learning Rate: 0.000124327
	LOSS [training: 0.16716466662083376 | validation: 0.17859751198560012]
	TIME [epoch: 7.13 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17236987080660376		[learning rate: 0.00012403]
	Learning Rate: 0.000124033
	LOSS [training: 0.17236987080660376 | validation: 0.2179735477559741]
	TIME [epoch: 7.12 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17535113023789023		[learning rate: 0.00012374]
	Learning Rate: 0.000123741
	LOSS [training: 0.17535113023789023 | validation: 0.1890754160970473]
	TIME [epoch: 7.11 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1861815387930788		[learning rate: 0.00012345]
	Learning Rate: 0.000123449
	LOSS [training: 0.1861815387930788 | validation: 0.2691393911915243]
	TIME [epoch: 7.13 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21460442367064686		[learning rate: 0.00012316]
	Learning Rate: 0.000123158
	LOSS [training: 0.21460442367064686 | validation: 0.2394433049724862]
	TIME [epoch: 7.12 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21435991424605824		[learning rate: 0.00012287]
	Learning Rate: 0.000122867
	LOSS [training: 0.21435991424605824 | validation: 0.24589015284622723]
	TIME [epoch: 7.13 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19233856854099196		[learning rate: 0.00012258]
	Learning Rate: 0.000122577
	LOSS [training: 0.19233856854099196 | validation: 0.24568055418882245]
	TIME [epoch: 7.15 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2056302328732726		[learning rate: 0.00012229]
	Learning Rate: 0.000122288
	LOSS [training: 0.2056302328732726 | validation: 0.26946321477307794]
	TIME [epoch: 7.12 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2238009745230613		[learning rate: 0.000122]
	Learning Rate: 0.000122
	LOSS [training: 0.2238009745230613 | validation: 0.254083913215125]
	TIME [epoch: 7.12 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21584624606418323		[learning rate: 0.00012171]
	Learning Rate: 0.000121712
	LOSS [training: 0.21584624606418323 | validation: 0.23316214703280547]
	TIME [epoch: 7.12 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21992149455008653		[learning rate: 0.00012142]
	Learning Rate: 0.000121425
	LOSS [training: 0.21992149455008653 | validation: 0.2223131674821449]
	TIME [epoch: 7.1 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19898394533551686		[learning rate: 0.00012114]
	Learning Rate: 0.000121138
	LOSS [training: 0.19898394533551686 | validation: 0.2562881867155835]
	TIME [epoch: 7.12 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2000684669864703		[learning rate: 0.00012085]
	Learning Rate: 0.000120853
	LOSS [training: 0.2000684669864703 | validation: 0.22043971729174733]
	TIME [epoch: 7.13 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17330856578772877		[learning rate: 0.00012057]
	Learning Rate: 0.000120568
	LOSS [training: 0.17330856578772877 | validation: 0.18567672041794425]
	TIME [epoch: 7.11 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16147749130736672		[learning rate: 0.00012028]
	Learning Rate: 0.000120283
	LOSS [training: 0.16147749130736672 | validation: 0.18481984114886746]
	TIME [epoch: 7.11 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17942909073022098		[learning rate: 0.00012]
	Learning Rate: 0.00012
	LOSS [training: 0.17942909073022098 | validation: 0.2076593708423233]
	TIME [epoch: 7.11 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1646700453648767		[learning rate: 0.00011972]
	Learning Rate: 0.000119716
	LOSS [training: 0.1646700453648767 | validation: 0.18995673198985813]
	TIME [epoch: 7.11 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1570319686857086		[learning rate: 0.00011943]
	Learning Rate: 0.000119434
	LOSS [training: 0.1570319686857086 | validation: 0.1825304571931205]
	TIME [epoch: 7.12 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16798109276183631		[learning rate: 0.00011915]
	Learning Rate: 0.000119152
	LOSS [training: 0.16798109276183631 | validation: 0.18549268640976518]
	TIME [epoch: 7.14 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1762868724585512		[learning rate: 0.00011887]
	Learning Rate: 0.000118871
	LOSS [training: 0.1762868724585512 | validation: 0.2123322602796573]
	TIME [epoch: 7.12 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1835341111273605		[learning rate: 0.00011859]
	Learning Rate: 0.000118591
	LOSS [training: 0.1835341111273605 | validation: 0.22142174537368986]
	TIME [epoch: 7.11 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2037138130627349		[learning rate: 0.00011831]
	Learning Rate: 0.000118311
	LOSS [training: 0.2037138130627349 | validation: 0.1812429255541513]
	TIME [epoch: 7.12 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19422341631932435		[learning rate: 0.00011803]
	Learning Rate: 0.000118032
	LOSS [training: 0.19422341631932435 | validation: 0.20973709362055373]
	TIME [epoch: 7.11 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20122725426936716		[learning rate: 0.00011775]
	Learning Rate: 0.000117754
	LOSS [training: 0.20122725426936716 | validation: 0.17312075687065515]
	TIME [epoch: 7.13 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1734347200489083		[learning rate: 0.00011748]
	Learning Rate: 0.000117476
	LOSS [training: 0.1734347200489083 | validation: 0.17410528891921753]
	TIME [epoch: 7.14 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16055097264478488		[learning rate: 0.0001172]
	Learning Rate: 0.000117199
	LOSS [training: 0.16055097264478488 | validation: 0.1803831537162017]
	TIME [epoch: 7.12 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16633846275991376		[learning rate: 0.00011692]
	Learning Rate: 0.000116922
	LOSS [training: 0.16633846275991376 | validation: 0.17948690306861503]
	TIME [epoch: 7.11 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16561247935567797		[learning rate: 0.00011665]
	Learning Rate: 0.000116647
	LOSS [training: 0.16561247935567797 | validation: 0.17333243868076725]
	TIME [epoch: 7.11 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15667094456646957		[learning rate: 0.00011637]
	Learning Rate: 0.000116371
	LOSS [training: 0.15667094456646957 | validation: 0.17507865631620456]
	TIME [epoch: 7.11 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16149777623794281		[learning rate: 0.0001161]
	Learning Rate: 0.000116097
	LOSS [training: 0.16149777623794281 | validation: 0.1756567749889575]
	TIME [epoch: 7.13 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15541885419499726		[learning rate: 0.00011582]
	Learning Rate: 0.000115823
	LOSS [training: 0.15541885419499726 | validation: 0.18634046513875113]
	TIME [epoch: 7.14 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15093625060802163		[learning rate: 0.00011555]
	Learning Rate: 0.00011555
	LOSS [training: 0.15093625060802163 | validation: 0.17695220772427855]
	TIME [epoch: 7.12 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1599172748593822		[learning rate: 0.00011528]
	Learning Rate: 0.000115277
	LOSS [training: 0.1599172748593822 | validation: 0.17546990557440847]
	TIME [epoch: 7.12 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15231717889874305		[learning rate: 0.00011501]
	Learning Rate: 0.000115005
	LOSS [training: 0.15231717889874305 | validation: 0.179003530569672]
	TIME [epoch: 7.12 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14979768551672057		[learning rate: 0.00011473]
	Learning Rate: 0.000114734
	LOSS [training: 0.14979768551672057 | validation: 0.20120622508493863]
	TIME [epoch: 7.12 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16994179063680423		[learning rate: 0.00011446]
	Learning Rate: 0.000114463
	LOSS [training: 0.16994179063680423 | validation: 0.19101078153697815]
	TIME [epoch: 7.12 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15745307517299112		[learning rate: 0.00011419]
	Learning Rate: 0.000114193
	LOSS [training: 0.15745307517299112 | validation: 0.17437113586229644]
	TIME [epoch: 7.15 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1557565893686946		[learning rate: 0.00011392]
	Learning Rate: 0.000113924
	LOSS [training: 0.1557565893686946 | validation: 0.17279418021929369]
	TIME [epoch: 7.12 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16427773742338908		[learning rate: 0.00011366]
	Learning Rate: 0.000113655
	LOSS [training: 0.16427773742338908 | validation: 0.17050045578778203]
	TIME [epoch: 7.12 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14975634390425874		[learning rate: 0.00011339]
	Learning Rate: 0.000113387
	LOSS [training: 0.14975634390425874 | validation: 0.18513936017118102]
	TIME [epoch: 7.12 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15844381502297125		[learning rate: 0.00011312]
	Learning Rate: 0.00011312
	LOSS [training: 0.15844381502297125 | validation: 0.20525401688196288]
	TIME [epoch: 7.13 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17564552073280457		[learning rate: 0.00011285]
	Learning Rate: 0.000112853
	LOSS [training: 0.17564552073280457 | validation: 0.21578312253071702]
	TIME [epoch: 7.14 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16689517063821516		[learning rate: 0.00011259]
	Learning Rate: 0.000112587
	LOSS [training: 0.16689517063821516 | validation: 0.19339767946157368]
	TIME [epoch: 7.15 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17084653463650074		[learning rate: 0.00011232]
	Learning Rate: 0.000112321
	LOSS [training: 0.17084653463650074 | validation: 0.22106068582398836]
	TIME [epoch: 7.12 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1673220027207732		[learning rate: 0.00011206]
	Learning Rate: 0.000112056
	LOSS [training: 0.1673220027207732 | validation: 0.17852504309792722]
	TIME [epoch: 7.13 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15608209663612224		[learning rate: 0.00011179]
	Learning Rate: 0.000111792
	LOSS [training: 0.15608209663612224 | validation: 0.1812114914230067]
	TIME [epoch: 7.13 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15432055029603975		[learning rate: 0.00011153]
	Learning Rate: 0.000111528
	LOSS [training: 0.15432055029603975 | validation: 0.17007704581823627]
	TIME [epoch: 7.12 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15663888236340234		[learning rate: 0.00011127]
	Learning Rate: 0.000111265
	LOSS [training: 0.15663888236340234 | validation: 0.18213429594228933]
	TIME [epoch: 7.15 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16372122354430213		[learning rate: 0.000111]
	Learning Rate: 0.000111003
	LOSS [training: 0.16372122354430213 | validation: 0.17247757698439156]
	TIME [epoch: 7.13 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1555897589397557		[learning rate: 0.00011074]
	Learning Rate: 0.000110741
	LOSS [training: 0.1555897589397557 | validation: 0.16704595198203126]
	TIME [epoch: 7.12 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1597668751056178		[learning rate: 0.00011048]
	Learning Rate: 0.00011048
	LOSS [training: 0.1597668751056178 | validation: 0.17811008113048057]
	TIME [epoch: 7.11 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1544674198647949		[learning rate: 0.00011022]
	Learning Rate: 0.000110219
	LOSS [training: 0.1544674198647949 | validation: 0.1735520066012236]
	TIME [epoch: 7.11 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16250304304683916		[learning rate: 0.00010996]
	Learning Rate: 0.000109959
	LOSS [training: 0.16250304304683916 | validation: 0.18706455230360491]
	TIME [epoch: 7.11 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1679532826060338		[learning rate: 0.0001097]
	Learning Rate: 0.0001097
	LOSS [training: 0.1679532826060338 | validation: 0.16587833472801258]
	TIME [epoch: 7.16 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1623375342196734		[learning rate: 0.00010944]
	Learning Rate: 0.000109441
	LOSS [training: 0.1623375342196734 | validation: 0.17758651906568113]
	TIME [epoch: 7.13 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16483109835387552		[learning rate: 0.00010918]
	Learning Rate: 0.000109183
	LOSS [training: 0.16483109835387552 | validation: 0.19897108523574927]
	TIME [epoch: 7.12 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1788573376658369		[learning rate: 0.00010893]
	Learning Rate: 0.000108925
	LOSS [training: 0.1788573376658369 | validation: 0.21485968599306238]
	TIME [epoch: 7.12 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15974856888367014		[learning rate: 0.00010867]
	Learning Rate: 0.000108668
	LOSS [training: 0.15974856888367014 | validation: 0.1842769263064979]
	TIME [epoch: 7.12 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15946190346697137		[learning rate: 0.00010841]
	Learning Rate: 0.000108412
	LOSS [training: 0.15946190346697137 | validation: 0.17184698071872775]
	TIME [epoch: 7.11 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15388177724957902		[learning rate: 0.00010816]
	Learning Rate: 0.000108156
	LOSS [training: 0.15388177724957902 | validation: 0.19288011884964387]
	TIME [epoch: 7.15 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1750747251940679		[learning rate: 0.0001079]
	Learning Rate: 0.000107901
	LOSS [training: 0.1750747251940679 | validation: 0.21301577193248233]
	TIME [epoch: 7.12 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18306802377659612		[learning rate: 0.00010765]
	Learning Rate: 0.000107647
	LOSS [training: 0.18306802377659612 | validation: 0.19100318825194781]
	TIME [epoch: 7.12 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16616459602185557		[learning rate: 0.00010739]
	Learning Rate: 0.000107393
	LOSS [training: 0.16616459602185557 | validation: 0.19726470937406093]
	TIME [epoch: 7.11 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16834083200059555		[learning rate: 0.00010714]
	Learning Rate: 0.000107139
	LOSS [training: 0.16834083200059555 | validation: 0.19454725891759772]
	TIME [epoch: 7.12 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1637988166617914		[learning rate: 0.00010689]
	Learning Rate: 0.000106887
	LOSS [training: 0.1637988166617914 | validation: 0.179971424367393]
	TIME [epoch: 7.11 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1596834845140584		[learning rate: 0.00010663]
	Learning Rate: 0.000106634
	LOSS [training: 0.1596834845140584 | validation: 0.18491444005021668]
	TIME [epoch: 7.15 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.189525576040674		[learning rate: 0.00010638]
	Learning Rate: 0.000106383
	LOSS [training: 0.189525576040674 | validation: 0.20559333304883365]
	TIME [epoch: 7.12 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19206385312083496		[learning rate: 0.00010613]
	Learning Rate: 0.000106132
	LOSS [training: 0.19206385312083496 | validation: 0.18418719667349312]
	TIME [epoch: 7.12 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1762302735915804		[learning rate: 0.00010588]
	Learning Rate: 0.000105882
	LOSS [training: 0.1762302735915804 | validation: 0.1870936240785535]
	TIME [epoch: 7.11 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1688349840627818		[learning rate: 0.00010563]
	Learning Rate: 0.000105632
	LOSS [training: 0.1688349840627818 | validation: 0.1919208357833035]
	TIME [epoch: 7.12 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15446581562370038		[learning rate: 0.00010538]
	Learning Rate: 0.000105383
	LOSS [training: 0.15446581562370038 | validation: 0.17963330459773058]
	TIME [epoch: 7.11 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.160084694952828		[learning rate: 0.00010513]
	Learning Rate: 0.000105134
	LOSS [training: 0.160084694952828 | validation: 0.18072281980984817]
	TIME [epoch: 7.15 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15098997199760716		[learning rate: 0.00010489]
	Learning Rate: 0.000104886
	LOSS [training: 0.15098997199760716 | validation: 0.17993642970046125]
	TIME [epoch: 7.12 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15001869449819547		[learning rate: 0.00010464]
	Learning Rate: 0.000104639
	LOSS [training: 0.15001869449819547 | validation: 0.17537902215661297]
	TIME [epoch: 7.11 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17220614279481833		[learning rate: 0.00010439]
	Learning Rate: 0.000104392
	LOSS [training: 0.17220614279481833 | validation: 0.1774628896212585]
	TIME [epoch: 7.11 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16276562184194449		[learning rate: 0.00010415]
	Learning Rate: 0.000104146
	LOSS [training: 0.16276562184194449 | validation: 0.19175632582573446]
	TIME [epoch: 7.11 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16342259657129504		[learning rate: 0.0001039]
	Learning Rate: 0.0001039
	LOSS [training: 0.16342259657129504 | validation: 0.17589675863317114]
	TIME [epoch: 7.11 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15535484613118494		[learning rate: 0.00010365]
	Learning Rate: 0.000103655
	LOSS [training: 0.15535484613118494 | validation: 0.1898585921796973]
	TIME [epoch: 7.15 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15423006640651885		[learning rate: 0.00010341]
	Learning Rate: 0.00010341
	LOSS [training: 0.15423006640651885 | validation: 0.1913888799751044]
	TIME [epoch: 7.11 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15048641941093505		[learning rate: 0.00010317]
	Learning Rate: 0.000103166
	LOSS [training: 0.15048641941093505 | validation: 0.2062268286343569]
	TIME [epoch: 7.11 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1602268424510513		[learning rate: 0.00010292]
	Learning Rate: 0.000102923
	LOSS [training: 0.1602268424510513 | validation: 0.1926130359214226]
	TIME [epoch: 7.11 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16229331335728697		[learning rate: 0.00010268]
	Learning Rate: 0.00010268
	LOSS [training: 0.16229331335728697 | validation: 0.21982762462981498]
	TIME [epoch: 7.11 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17187471252586337		[learning rate: 0.00010244]
	Learning Rate: 0.000102438
	LOSS [training: 0.17187471252586337 | validation: 0.21109379753681634]
	TIME [epoch: 7.11 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1694455484597665		[learning rate: 0.0001022]
	Learning Rate: 0.000102196
	LOSS [training: 0.1694455484597665 | validation: 0.18683844383016104]
	TIME [epoch: 7.15 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1570089517316311		[learning rate: 0.00010196]
	Learning Rate: 0.000101955
	LOSS [training: 0.1570089517316311 | validation: 0.1773983863522584]
	TIME [epoch: 7.12 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1587466776796797		[learning rate: 0.00010171]
	Learning Rate: 0.000101715
	LOSS [training: 0.1587466776796797 | validation: 0.1951435617645338]
	TIME [epoch: 7.11 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1637452736285158		[learning rate: 0.00010147]
	Learning Rate: 0.000101475
	LOSS [training: 0.1637452736285158 | validation: 0.1851061201951753]
	TIME [epoch: 7.12 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17731215228382818		[learning rate: 0.00010124]
	Learning Rate: 0.000101236
	LOSS [training: 0.17731215228382818 | validation: 0.19659301077251717]
	TIME [epoch: 7.12 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18007756879260642		[learning rate: 0.000101]
	Learning Rate: 0.000100997
	LOSS [training: 0.18007756879260642 | validation: 0.19941752697162385]
	TIME [epoch: 7.12 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17257262425037206		[learning rate: 0.00010076]
	Learning Rate: 0.000100759
	LOSS [training: 0.17257262425037206 | validation: 0.21232570477784873]
	TIME [epoch: 7.14 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16520541070490766		[learning rate: 0.00010052]
	Learning Rate: 0.000100521
	LOSS [training: 0.16520541070490766 | validation: 0.19431452380168032]
	TIME [epoch: 7.13 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16571994550141905		[learning rate: 0.00010028]
	Learning Rate: 0.000100284
	LOSS [training: 0.16571994550141905 | validation: 0.20858825973165618]
	TIME [epoch: 7.11 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17992922293029145		[learning rate: 0.00010005]
	Learning Rate: 0.000100047
	LOSS [training: 0.17992922293029145 | validation: 0.19680079084264096]
	TIME [epoch: 7.12 sec]
Finished training in 14452.458 seconds.
