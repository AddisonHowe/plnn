Args:
Namespace(name='model_phiq_1a_v1', outdir='out/model_training/model_phiq_1a_v1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3485329059

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.06675448662814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.06675448662814 | validation: 7.9538455686026195]
	TIME [epoch: 100 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.4839878955895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.4839878955895 | validation: 7.5641114221417105]
	TIME [epoch: 6.87 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.194986686507193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.194986686507193 | validation: 7.35405968893449]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.080380927906742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.080380927906742 | validation: 7.2223311938104615]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.9415493006359075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.9415493006359075 | validation: 7.080088846992936]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.888434701460319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.888434701460319 | validation: 7.055048100298713]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.842748528768829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.842748528768829 | validation: 6.966453928206176]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.755832136892624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.755832136892624 | validation: 6.88162932147235]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.670325690047912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.670325690047912 | validation: 6.81897052503978]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.607447070352878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.607447070352878 | validation: 6.763247122598934]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.756081742638681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.756081742638681 | validation: 6.857883857924541]
	TIME [epoch: 6.75 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.575658679591256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.575658679591256 | validation: 7.2884211361606415]
	TIME [epoch: 6.74 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.916477156607578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.916477156607578 | validation: 6.923401990860183]
	TIME [epoch: 6.74 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.535355339159026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.535355339159026 | validation: 6.750209430762924]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.358481410563591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.358481410563591 | validation: 6.467840092864597]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.17674650672907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.17674650672907 | validation: 6.239449751006461]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.00183701696375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.00183701696375 | validation: 6.113730881721223]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.824912399476983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.824912399476983 | validation: 5.991282588534055]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.642222872604832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.642222872604832 | validation: 5.83858970995156]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.493761644199962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.493761644199962 | validation: 5.459166903874256]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.466529054615984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.466529054615984 | validation: 5.424453477393021]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.3697422190839905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3697422190839905 | validation: 5.50846995084421]
	TIME [epoch: 6.8 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.351654174806001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.351654174806001 | validation: 5.685876525512901]
	TIME [epoch: 6.76 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.965866463387607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.965866463387607 | validation: 4.925713498499917]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.8658274522073235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8658274522073235 | validation: 4.95863017821382]
	TIME [epoch: 6.75 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.543530649504772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.543530649504772 | validation: 5.3906720812856594]
	TIME [epoch: 6.74 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.057490857571715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.057490857571715 | validation: 5.887400218186494]
	TIME [epoch: 6.74 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.092619630565929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.092619630565929 | validation: 6.3065594049470235]
	TIME [epoch: 6.74 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.392858687164462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.392858687164462 | validation: 6.086541212688781]
	TIME [epoch: 6.79 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.019767178568808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.019767178568808 | validation: 5.971162221393474]
	TIME [epoch: 6.75 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.437985939197059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.437985939197059 | validation: 5.887495126617184]
	TIME [epoch: 6.74 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.186764800159374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.186764800159374 | validation: 5.82407312232978]
	TIME [epoch: 6.74 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.122404363584512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.122404363584512 | validation: 5.377290492315748]
	TIME [epoch: 6.74 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.78646125230263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.78646125230263 | validation: 4.902335455737916]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.514785936953685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.514785936953685 | validation: 4.712589283378717]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.425571138441906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.425571138441906 | validation: 4.5956940459884255]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_36.pth
	Model improved!!!
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330646057942807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.330646057942807 | validation: 4.45185054704757]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.236197515062979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.236197515062979 | validation: 4.368946774861355]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.089612352749758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.089612352749758 | validation: 4.149217886892073]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.903784204742172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.903784204742172 | validation: 4.17152249205229]
	TIME [epoch: 6.75 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.008374055159785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008374055159785 | validation: 3.7107007274249493]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.666145404517121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.666145404517121 | validation: 3.5914079647219603]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.706182537735789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.706182537735789 | validation: 3.8987536078516696]
	TIME [epoch: 6.77 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.539134233265682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539134233265682 | validation: 3.5437499793044833]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5355218785593934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5355218785593934 | validation: 3.452440490866193]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4505828371404133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4505828371404133 | validation: 3.2596895930622454]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.370873657548259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.370873657548259 | validation: 3.2318558887828175]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6378975781075606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6378975781075606 | validation: 3.4089525018238227]
	TIME [epoch: 6.75 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9347858714188737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9347858714188737 | validation: 4.734205637991586]
	TIME [epoch: 6.79 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.984598412083653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.984598412083653 | validation: 4.320743373325328]
	TIME [epoch: 6.75 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8388372469776577		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 3.8388372469776577 | validation: 4.080671935790476]
	TIME [epoch: 6.75 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6771808363500034		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 3.6771808363500034 | validation: 3.644049369653044]
	TIME [epoch: 6.74 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6293144311069896		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 3.6293144311069896 | validation: 3.629939449384762]
	TIME [epoch: 6.76 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.498989728181831		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 3.498989728181831 | validation: 3.294132136946029]
	TIME [epoch: 6.77 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.368309029378141		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.368309029378141 | validation: 3.3353324906364485]
	TIME [epoch: 6.76 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.306655293251336		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 3.306655293251336 | validation: 3.140670642833545]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.205159602065231		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 3.205159602065231 | validation: 3.025546383260607]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1215126475540136		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 3.1215126475540136 | validation: 2.9210269528404735]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_58.pth
	Model improved!!!
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.247430647237962		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 3.247430647237962 | validation: 3.161944205546261]
	TIME [epoch: 6.74 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0627975113646952		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 3.0627975113646952 | validation: 2.9558739366218703]
	TIME [epoch: 6.74 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.039745660383617		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 3.039745660383617 | validation: 4.189198738609111]
	TIME [epoch: 6.74 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.110512812648096		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 4.110512812648096 | validation: 3.3404966658530304]
	TIME [epoch: 6.75 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.223535174261214		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 3.223535174261214 | validation: 2.984314634585864]
	TIME [epoch: 6.77 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.038181744873076		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 3.038181744873076 | validation: 2.805223191377081]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_64.pth
	Model improved!!!
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9491344469512595		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 2.9491344469512595 | validation: 2.835850340863641]
	TIME [epoch: 6.74 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8472043729643834		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 2.8472043729643834 | validation: 2.7675466064452916]
	TIME [epoch: 6.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_66.pth
	Model improved!!!
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.11335488047258		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 3.11335488047258 | validation: 3.8043468461383285]
	TIME [epoch: 6.74 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.307958594989583		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 3.307958594989583 | validation: 3.242427367368064]
	TIME [epoch: 6.73 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.170046870151153		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 3.170046870151153 | validation: 2.7845976151162404]
	TIME [epoch: 6.78 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0156817935295477		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 3.0156817935295477 | validation: 2.8843941211753563]
	TIME [epoch: 6.75 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9363606718959305		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 2.9363606718959305 | validation: 2.912514391205322]
	TIME [epoch: 6.76 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9539695104859724		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 2.9539695104859724 | validation: 2.910832877808511]
	TIME [epoch: 6.76 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8600330703764367		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 2.8600330703764367 | validation: 3.392004479824444]
	TIME [epoch: 6.74 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3248652708613777		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.3248652708613777 | validation: 4.904279674337375]
	TIME [epoch: 6.74 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.345731866786567		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 5.345731866786567 | validation: 4.256863320310651]
	TIME [epoch: 6.74 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7215341998985747		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 3.7215341998985747 | validation: 3.7289542285709856]
	TIME [epoch: 6.78 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2102994227839425		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 3.2102994227839425 | validation: 2.826881088584758]
	TIME [epoch: 6.75 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.894691553468398		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 2.894691553468398 | validation: 2.7185190398873456]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_78.pth
	Model improved!!!
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0441461808285624		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 3.0441461808285624 | validation: 3.2151000118581594]
	TIME [epoch: 6.74 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8943629784921616		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 2.8943629784921616 | validation: 2.6126880643784114]
	TIME [epoch: 6.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_80.pth
	Model improved!!!
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9101527460029373		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 2.9101527460029373 | validation: 2.9053122920525603]
	TIME [epoch: 6.74 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9385342791485987		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 2.9385342791485987 | validation: 2.7555548279201085]
	TIME [epoch: 6.74 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.744765018136965		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 2.744765018136965 | validation: 2.493186309258084]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_83.pth
	Model improved!!!
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.653663305641071		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 2.653663305641071 | validation: 2.4702777337389557]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_84.pth
	Model improved!!!
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5483022473354198		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 2.5483022473354198 | validation: 2.461772655858856]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_85.pth
	Model improved!!!
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.46274301005671		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 2.46274301005671 | validation: 2.257896666267567]
	TIME [epoch: 7.06 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_86.pth
	Model improved!!!
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.408343598057659		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 2.408343598057659 | validation: 2.8272647484676394]
	TIME [epoch: 6.75 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.441608270524656		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 2.441608270524656 | validation: 2.1890940699338826]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_88.pth
	Model improved!!!
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2910550890418553		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 2.2910550890418553 | validation: 2.915530520403503]
	TIME [epoch: 6.82 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.547643434321404		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 2.547643434321404 | validation: 2.3538408354349443]
	TIME [epoch: 6.77 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5836751407136953		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 2.5836751407136953 | validation: 3.641587530812546]
	TIME [epoch: 6.75 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9032018658247267		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 2.9032018658247267 | validation: 2.4729095338286786]
	TIME [epoch: 6.75 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.271781097235708		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 2.271781097235708 | validation: 2.167144077207637]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_93.pth
	Model improved!!!
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.237745159704623		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 2.237745159704623 | validation: 2.14509360125749]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_94.pth
	Model improved!!!
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1904457661298005		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 2.1904457661298005 | validation: 2.075034187243508]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5845532487185414		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 2.5845532487185414 | validation: 3.248748710755827]
	TIME [epoch: 6.79 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.239635500835827		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 3.239635500835827 | validation: 3.6247972389472793]
	TIME [epoch: 6.76 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7545211549431348		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 3.7545211549431348 | validation: 4.178525124045618]
	TIME [epoch: 6.74 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.73613369854643		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 4.73613369854643 | validation: 4.6462203389810295]
	TIME [epoch: 6.74 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.955677236015222		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 3.955677236015222 | validation: 3.594195232538357]
	TIME [epoch: 6.74 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8978334555938083		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 3.8978334555938083 | validation: 3.7812420418114057]
	TIME [epoch: 6.74 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.39681618771791		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 4.39681618771791 | validation: 3.9110725127256467]
	TIME [epoch: 6.74 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.284784355046029		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 4.284784355046029 | validation: 3.4772656984879635]
	TIME [epoch: 6.79 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4189767815774696		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 3.4189767815774696 | validation: 2.7198569605179297]
	TIME [epoch: 6.75 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8707628490374177		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 2.8707628490374177 | validation: 2.6446352319763475]
	TIME [epoch: 6.74 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.648450113981575		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 2.648450113981575 | validation: 2.6949242926069443]
	TIME [epoch: 6.76 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6980965968507364		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 2.6980965968507364 | validation: 2.41244617083445]
	TIME [epoch: 6.76 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4173607301054405		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 2.4173607301054405 | validation: 2.5629346131098147]
	TIME [epoch: 6.74 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3945393635180934		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 2.3945393635180934 | validation: 2.6406824874595625]
	TIME [epoch: 6.75 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7261351935204816		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 2.7261351935204816 | validation: 3.164209284240398]
	TIME [epoch: 6.78 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070420985398753		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 3.2070420985398753 | validation: 3.187626880234516]
	TIME [epoch: 6.74 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1422573314783473		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 3.1422573314783473 | validation: 2.935594582999557]
	TIME [epoch: 6.74 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0731247143600076		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 3.0731247143600076 | validation: 3.230822651940968]
	TIME [epoch: 6.74 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.431663335002089		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 3.431663335002089 | validation: 3.1934650481387425]
	TIME [epoch: 6.73 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.152885491639953		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 3.152885491639953 | validation: 2.7203819894180468]
	TIME [epoch: 6.74 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.506143600381252		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 2.506143600381252 | validation: 2.392964273689938]
	TIME [epoch: 6.74 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.76098339258518		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 2.76098339258518 | validation: 2.47459993986252]
	TIME [epoch: 6.78 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.350776877908131		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 2.350776877908131 | validation: 2.188130935364296]
	TIME [epoch: 6.75 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2174997321353183		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 2.2174997321353183 | validation: 2.154127551130631]
	TIME [epoch: 6.75 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0563107935584553		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 2.0563107935584553 | validation: 2.4375795141203396]
	TIME [epoch: 6.74 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.477619918747294		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 2.477619918747294 | validation: 2.7738949734184257]
	TIME [epoch: 6.74 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.301887854157358		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 2.301887854157358 | validation: 2.0129149230321195]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_122.pth
	Model improved!!!
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9765334807210913		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 1.9765334807210913 | validation: 1.91462911678116]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_123.pth
	Model improved!!!
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0784259463619907		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 2.0784259463619907 | validation: 1.9858284148173588]
	TIME [epoch: 6.8 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1960400297914013		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 2.1960400297914013 | validation: 2.4268313849346352]
	TIME [epoch: 6.77 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6912970823201583		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 2.6912970823201583 | validation: 2.2166784472074657]
	TIME [epoch: 6.74 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0698844749007232		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 2.0698844749007232 | validation: 1.7998258925659472]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_127.pth
	Model improved!!!
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.860155156054343		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 1.860155156054343 | validation: 1.8332792565486462]
	TIME [epoch: 6.76 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8433154766296063		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 1.8433154766296063 | validation: 1.8849675416251457]
	TIME [epoch: 6.76 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7425430353958873		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 1.7425430353958873 | validation: 1.971961737512212]
	TIME [epoch: 6.8 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2966158868305584		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 2.2966158868305584 | validation: 2.280165015105381]
	TIME [epoch: 6.77 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4408116731811527		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 2.4408116731811527 | validation: 2.6418302835885563]
	TIME [epoch: 6.75 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.457562522541442		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 2.457562522541442 | validation: 2.695906152028325]
	TIME [epoch: 6.75 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.752965138467891		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 2.752965138467891 | validation: 3.8345349815891767]
	TIME [epoch: 6.75 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.61537228947714		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 3.61537228947714 | validation: 4.6004888040632]
	TIME [epoch: 6.75 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.955676533062295		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 3.955676533062295 | validation: 3.9843454496830146]
	TIME [epoch: 6.75 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.587628066993233		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 3.587628066993233 | validation: 2.831812451812396]
	TIME [epoch: 6.79 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6830183968591816		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 2.6830183968591816 | validation: 2.225652927351403]
	TIME [epoch: 6.77 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9914506487813723		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 1.9914506487813723 | validation: 2.1145753964018636]
	TIME [epoch: 6.76 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1745868643801782		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 2.1745868643801782 | validation: 3.1536254913217876]
	TIME [epoch: 6.75 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2172014919255765		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 3.2172014919255765 | validation: 2.745963368319756]
	TIME [epoch: 6.75 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.445910042529862		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 2.445910042529862 | validation: 2.23486351260449]
	TIME [epoch: 6.77 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.983296511651299		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 1.983296511651299 | validation: 2.1805190779494006]
	TIME [epoch: 6.76 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1747475809408385		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 2.1747475809408385 | validation: 2.0052514244109814]
	TIME [epoch: 6.79 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8124422351664289		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 1.8124422351664289 | validation: 1.8119122259854104]
	TIME [epoch: 6.77 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8724432927968688		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 1.8724432927968688 | validation: 2.322055724281186]
	TIME [epoch: 6.75 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.879947422052152		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 1.879947422052152 | validation: 1.8043637703581237]
	TIME [epoch: 6.75 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7359061387778996		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 1.7359061387778996 | validation: 2.100841704355074]
	TIME [epoch: 6.75 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1597913872318326		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 2.1597913872318326 | validation: 2.1055693533033866]
	TIME [epoch: 6.74 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0956473144460643		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 2.0956473144460643 | validation: 1.84223005015753]
	TIME [epoch: 6.74 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6184642260409927		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 1.6184642260409927 | validation: 1.6743053593858113]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_151.pth
	Model improved!!!
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8013279179176545		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 1.8013279179176545 | validation: 1.770512488342937]
	TIME [epoch: 6.76 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6406516832252713		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 1.6406516832252713 | validation: 1.679322700816488]
	TIME [epoch: 6.75 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6200262674811285		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 1.6200262674811285 | validation: 1.9513028193517128]
	TIME [epoch: 6.76 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6067504540624762		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 1.6067504540624762 | validation: 2.084318799398319]
	TIME [epoch: 6.74 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8757600401613606		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 2.8757600401613606 | validation: 3.673825498471621]
	TIME [epoch: 6.74 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.183245422524541		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 3.183245422524541 | validation: 2.4369396957190155]
	TIME [epoch: 6.75 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0983615335541943		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 2.0983615335541943 | validation: 2.6927523577168575]
	TIME [epoch: 6.78 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8724242089809626		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 1.8724242089809626 | validation: 1.8546834443446318]
	TIME [epoch: 6.76 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6422470374869826		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 1.6422470374869826 | validation: 2.7184438028364695]
	TIME [epoch: 6.77 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.876189423919475		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 2.876189423919475 | validation: 3.355859749997182]
	TIME [epoch: 6.76 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.209612674840954		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 3.209612674840954 | validation: 3.5194142976871206]
	TIME [epoch: 6.75 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2032761740848654		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 3.2032761740848654 | validation: 2.919133478694552]
	TIME [epoch: 6.75 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.346916819431256		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 2.346916819431256 | validation: 1.856235069793618]
	TIME [epoch: 6.76 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.839936431087808		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 1.839936431087808 | validation: 1.8572172055278888]
	TIME [epoch: 6.81 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7272199618306519		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 1.7272199618306519 | validation: 1.8585546555201269]
	TIME [epoch: 6.77 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.739877996048718		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 1.739877996048718 | validation: 1.9968179354774647]
	TIME [epoch: 6.77 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0804794793408465		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 2.0804794793408465 | validation: 2.127883892580716]
	TIME [epoch: 6.76 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.699706352407964		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 1.699706352407964 | validation: 1.7038336787629764]
	TIME [epoch: 6.76 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6176088468264025		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 1.6176088468264025 | validation: 2.670986543289203]
	TIME [epoch: 6.76 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.990252395615158		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 1.990252395615158 | validation: 1.8968695954554466]
	TIME [epoch: 6.79 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.741685069827358		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 1.741685069827358 | validation: 2.0762663640733083]
	TIME [epoch: 6.81 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6880129522965628		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 1.6880129522965628 | validation: 2.213689908134926]
	TIME [epoch: 6.77 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8059655431776227		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 1.8059655431776227 | validation: 2.1319499688418926]
	TIME [epoch: 6.77 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.499262236231166		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 1.499262236231166 | validation: 1.7112569565582687]
	TIME [epoch: 6.76 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4941651914744682		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 1.4941651914744682 | validation: 1.647035269652713]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_176.pth
	Model improved!!!
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7874402536620302		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 1.7874402536620302 | validation: 1.6528274150946893]
	TIME [epoch: 6.76 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5241260864252928		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 1.5241260864252928 | validation: 1.9252122720538463]
	TIME [epoch: 6.81 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6703136122842994		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 1.6703136122842994 | validation: 1.833474433084959]
	TIME [epoch: 6.79 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.571659458648669		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 1.571659458648669 | validation: 1.6558639416939869]
	TIME [epoch: 6.75 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1902676325178545		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 2.1902676325178545 | validation: 3.0335205861040144]
	TIME [epoch: 6.75 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.025471969539915		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 2.025471969539915 | validation: 1.8796556071777446]
	TIME [epoch: 6.74 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5163731334318298		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 1.5163731334318298 | validation: 2.2679344856949077]
	TIME [epoch: 6.75 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5160198665357063		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 1.5160198665357063 | validation: 1.4685158423892215]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_184.pth
	Model improved!!!
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.407588962766835		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 1.407588962766835 | validation: 1.375221710259816]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_185.pth
	Model improved!!!
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5828505233250754		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 1.5828505233250754 | validation: 1.473534134452267]
	TIME [epoch: 6.77 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3720471640039094		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 1.3720471640039094 | validation: 2.407892784798208]
	TIME [epoch: 6.75 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7819514007208141		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 1.7819514007208141 | validation: 1.6314747641331768]
	TIME [epoch: 6.75 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.002282039281573		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 2.002282039281573 | validation: 1.7593126378231712]
	TIME [epoch: 6.75 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7377476651081223		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 1.7377476651081223 | validation: 1.8640476071947503]
	TIME [epoch: 6.74 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8048328474298814		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 1.8048328474298814 | validation: 1.9315638539501276]
	TIME [epoch: 6.74 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5518881486841676		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 1.5518881486841676 | validation: 1.9165739926461627]
	TIME [epoch: 6.79 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7279878204820038		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 1.7279878204820038 | validation: 2.4652072989126426]
	TIME [epoch: 6.75 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.649312729786054		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 1.649312729786054 | validation: 2.565961053563412]
	TIME [epoch: 6.75 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.296983115825769		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 2.296983115825769 | validation: 2.158520738118571]
	TIME [epoch: 6.77 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0133219094753243		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 2.0133219094753243 | validation: 1.7329693416543768]
	TIME [epoch: 6.77 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7917135965177429		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 1.7917135965177429 | validation: 1.8545559833919856]
	TIME [epoch: 6.75 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5416160240291075		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 1.5416160240291075 | validation: 1.622754106577006]
	TIME [epoch: 6.76 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.501654299107075		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 1.501654299107075 | validation: 2.382666351590715]
	TIME [epoch: 6.79 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9727846789016907		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 1.9727846789016907 | validation: 1.6500993486720428]
	TIME [epoch: 6.75 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5335422864085957		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 1.5335422864085957 | validation: 1.578110829618697]
	TIME [epoch: 6.75 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.944180914041504		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 1.944180914041504 | validation: 2.0946467947401515]
	TIME [epoch: 6.75 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9288095930973728		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 1.9288095930973728 | validation: 1.612117681929705]
	TIME [epoch: 6.75 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5648827839195742		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 1.5648827839195742 | validation: 1.4637613193974115]
	TIME [epoch: 6.74 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5870040240205583		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 1.5870040240205583 | validation: 1.7783815272993282]
	TIME [epoch: 6.75 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5496171332622042		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 1.5496171332622042 | validation: 1.478690023408568]
	TIME [epoch: 6.79 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4555759637436387		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 1.4555759637436387 | validation: 2.0510860585147475]
	TIME [epoch: 6.76 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4337079689499785		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 1.4337079689499785 | validation: 1.6013744695622365]
	TIME [epoch: 6.76 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.401578617009829		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 1.401578617009829 | validation: 1.7561037815110336]
	TIME [epoch: 6.75 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7136999363597538		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 1.7136999363597538 | validation: 1.5959948429574051]
	TIME [epoch: 6.75 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5859328987105132		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 1.5859328987105132 | validation: 1.701939083257629]
	TIME [epoch: 6.76 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5592456604347835		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 1.5592456604347835 | validation: 1.583546891775082]
	TIME [epoch: 6.78 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3518322033341403		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 1.3518322033341403 | validation: 1.6240254500976719]
	TIME [epoch: 6.81 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5633246940534333		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 1.5633246940534333 | validation: 1.458883739844858]
	TIME [epoch: 6.79 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3146567967835283		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 1.3146567967835283 | validation: 1.3933132491251774]
	TIME [epoch: 6.78 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8397596883483123		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 1.8397596883483123 | validation: 3.223484327255921]
	TIME [epoch: 6.76 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3814117867447537		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 2.3814117867447537 | validation: 3.6283885297411524]
	TIME [epoch: 6.75 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5887555814375385		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 2.5887555814375385 | validation: 2.6661091163172466]
	TIME [epoch: 6.75 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8037805920404322		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 1.8037805920404322 | validation: 1.6367165582879482]
	TIME [epoch: 6.78 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.406607449860208		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 1.406607449860208 | validation: 1.3435461516443592]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_220.pth
	Model improved!!!
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.251495454344165		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 1.251495454344165 | validation: 1.3390966947296183]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_221.pth
	Model improved!!!
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2862090293004012		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 1.2862090293004012 | validation: 1.3641860627683644]
	TIME [epoch: 6.77 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3678017428423115		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 1.3678017428423115 | validation: 2.175379526484848]
	TIME [epoch: 6.77 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9928911759804186		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 1.9928911759804186 | validation: 3.417866643966434]
	TIME [epoch: 6.75 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.652991335635879		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 2.652991335635879 | validation: 3.102701632497365]
	TIME [epoch: 6.75 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4590200753433984		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 2.4590200753433984 | validation: 2.613947475815145]
	TIME [epoch: 6.8 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.856492592584		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 1.856492592584 | validation: 1.4422884151605984]
	TIME [epoch: 6.78 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4004341178615836		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 1.4004341178615836 | validation: 1.6065140398407278]
	TIME [epoch: 6.77 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3640292908548215		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 1.3640292908548215 | validation: 1.3504386155117265]
	TIME [epoch: 6.77 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3832638950870937		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 1.3832638950870937 | validation: 2.7003683716622664]
	TIME [epoch: 6.76 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9644920647363782		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 1.9644920647363782 | validation: 1.577158037441066]
	TIME [epoch: 6.78 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2914800314317696		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 1.2914800314317696 | validation: 1.3592923557570207]
	TIME [epoch: 6.78 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.199697148555714		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 1.199697148555714 | validation: 1.5648370935275155]
	TIME [epoch: 6.82 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2496839835598585		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 1.2496839835598585 | validation: 1.3992874443756338]
	TIME [epoch: 6.78 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3335515614897622		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 1.3335515614897622 | validation: 1.765696413158377]
	TIME [epoch: 6.76 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.264189495059719		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 1.264189495059719 | validation: 1.406557087680971]
	TIME [epoch: 6.77 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1933291063599798		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 1.1933291063599798 | validation: 2.1233156192776823]
	TIME [epoch: 6.76 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7516955060890536		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 1.7516955060890536 | validation: 1.4254059463988122]
	TIME [epoch: 6.76 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3250704082671045		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 1.3250704082671045 | validation: 1.7565934539612456]
	TIME [epoch: 6.77 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3338424063855094		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 1.3338424063855094 | validation: 1.362532925309073]
	TIME [epoch: 6.81 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3208181968057648		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 1.3208181968057648 | validation: 1.5471361230487153]
	TIME [epoch: 6.78 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3923889576860664		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 1.3923889576860664 | validation: 1.503578595883677]
	TIME [epoch: 6.76 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2509026496552569		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 1.2509026496552569 | validation: 1.2387325532794757]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_243.pth
	Model improved!!!
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2446657182682068		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 1.2446657182682068 | validation: 1.3876389740360984]
	TIME [epoch: 6.77 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.47190682429976		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 1.47190682429976 | validation: 1.762958962918091]
	TIME [epoch: 6.75 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2130122897617623		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 1.2130122897617623 | validation: 1.343316986252411]
	TIME [epoch: 6.76 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2514912739391553		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 1.2514912739391553 | validation: 1.2704168332911034]
	TIME [epoch: 6.8 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1278933650372855		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 1.1278933650372855 | validation: 1.2389048371445677]
	TIME [epoch: 6.75 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3304358529500309		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 1.3304358529500309 | validation: 1.2206007314417078]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_249.pth
	Model improved!!!
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.068918375948892		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 1.068918375948892 | validation: 1.1924464051678108]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_250.pth
	Model improved!!!
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.183906936739616		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 1.183906936739616 | validation: 1.3646603516354627]
	TIME [epoch: 6.75 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0936136935276626		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 1.0936136935276626 | validation: 1.2364019533121617]
	TIME [epoch: 6.74 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1945201972811996		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 1.1945201972811996 | validation: 1.7874664298998866]
	TIME [epoch: 6.78 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.833012301451338		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 1.833012301451338 | validation: 2.981202995239518]
	TIME [epoch: 6.78 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9934196145247212		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 1.9934196145247212 | validation: 2.567668862554024]
	TIME [epoch: 6.75 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5807476348070002		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 1.5807476348070002 | validation: 1.3719507060387923]
	TIME [epoch: 6.75 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2170096135004802		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 1.2170096135004802 | validation: 1.2068032320696995]
	TIME [epoch: 6.74 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5564248174186184		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 1.5564248174186184 | validation: 2.4774502398833045]
	TIME [epoch: 6.75 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3520614868120695		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 1.3520614868120695 | validation: 1.2434968579808532]
	TIME [epoch: 6.74 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0840690777413768		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 1.0840690777413768 | validation: 1.1633114069194064]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_260.pth
	Model improved!!!
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0239919613536719		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 1.0239919613536719 | validation: 1.2355663763886935]
	TIME [epoch: 6.78 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0545488039996824		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 1.0545488039996824 | validation: 1.3219684305370265]
	TIME [epoch: 6.75 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.122380787387424		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 1.122380787387424 | validation: 1.0990484871568285]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_263.pth
	Model improved!!!
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0122312847006827		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 1.0122312847006827 | validation: 1.265060624247778]
	TIME [epoch: 6.75 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1196771521601085		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 1.1196771521601085 | validation: 1.3757004558249017]
	TIME [epoch: 6.74 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.141668915724402		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 1.141668915724402 | validation: 1.2189114514230215]
	TIME [epoch: 6.76 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0196736269543925		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 1.0196736269543925 | validation: 1.208416471186658]
	TIME [epoch: 6.82 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1247699404815616		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 1.1247699404815616 | validation: 1.451697063400721]
	TIME [epoch: 6.78 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2154971907669074		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 1.2154971907669074 | validation: 1.3226199035785102]
	TIME [epoch: 6.76 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2219883607500255		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 1.2219883607500255 | validation: 1.278504334756129]
	TIME [epoch: 6.75 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0583355991817067		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 1.0583355991817067 | validation: 1.2455220956848607]
	TIME [epoch: 6.76 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2631143461663261		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 1.2631143461663261 | validation: 1.9789672428898348]
	TIME [epoch: 6.76 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3254265299052506		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 1.3254265299052506 | validation: 1.2578134941794612]
	TIME [epoch: 6.75 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.200731158725094		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 1.200731158725094 | validation: 1.1923518175176506]
	TIME [epoch: 6.81 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0279957484496234		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 1.0279957484496234 | validation: 1.4178857530640823]
	TIME [epoch: 6.77 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0576964278264156		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 1.0576964278264156 | validation: 1.9989683860090008]
	TIME [epoch: 6.76 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3650962800601318		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 1.3650962800601318 | validation: 1.2628693610838921]
	TIME [epoch: 6.76 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1016876119077332		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 1.1016876119077332 | validation: 1.405314402571379]
	TIME [epoch: 6.76 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.264561902501527		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 1.264561902501527 | validation: 1.7025352659879562]
	TIME [epoch: 6.76 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.192744904369211		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 1.192744904369211 | validation: 1.2261724406173369]
	TIME [epoch: 6.77 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.155567485476659		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 1.155567485476659 | validation: 1.3956570728787385]
	TIME [epoch: 6.86 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0792194958324166		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 1.0792194958324166 | validation: 1.1479782336220279]
	TIME [epoch: 6.76 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0402908294475655		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 1.0402908294475655 | validation: 1.1042132244689409]
	TIME [epoch: 6.75 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1224090694857252		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 1.1224090694857252 | validation: 1.7073401793702336]
	TIME [epoch: 6.76 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.149477007931546		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 1.149477007931546 | validation: 1.2781665445747274]
	TIME [epoch: 6.77 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.061537541219737		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 1.061537541219737 | validation: 1.0735338324431158]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_286.pth
	Model improved!!!
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9719424215554477		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.9719424215554477 | validation: 1.1221214620111828]
	TIME [epoch: 6.76 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9867111408595461		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.9867111408595461 | validation: 1.7247500831434897]
	TIME [epoch: 6.8 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.292789612931028		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 1.292789612931028 | validation: 1.1142314666370363]
	TIME [epoch: 6.76 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.323542978447438		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 1.323542978447438 | validation: 1.5508562336644203]
	TIME [epoch: 6.76 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1331740163116184		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 1.1331740163116184 | validation: 1.6507420703757583]
	TIME [epoch: 6.76 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.580593242046173		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 1.580593242046173 | validation: 1.772174851598995]
	TIME [epoch: 6.75 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.221512251248407		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 1.221512251248407 | validation: 1.1440823515020235]
	TIME [epoch: 6.76 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0548838423016358		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 1.0548838423016358 | validation: 1.1999903537508845]
	TIME [epoch: 6.78 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1123127093598106		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 1.1123127093598106 | validation: 2.3330889579083007]
	TIME [epoch: 6.8 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4232055622483193		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 2.4232055622483193 | validation: 3.220845357858077]
	TIME [epoch: 6.75 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6835233349869267		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 2.6835233349869267 | validation: 2.1765984962039857]
	TIME [epoch: 6.75 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7074042618626444		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 1.7074042618626444 | validation: 2.2828558998794457]
	TIME [epoch: 6.76 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.574848258088295		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 1.574848258088295 | validation: 1.3372697496897203]
	TIME [epoch: 6.75 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2020786494495157		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 1.2020786494495157 | validation: 1.3772903556499678]
	TIME [epoch: 6.75 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2459429297768183		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 1.2459429297768183 | validation: 1.8852452293613922]
	TIME [epoch: 6.77 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4470050367194736		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 1.4470050367194736 | validation: 1.5924377830655163]
	TIME [epoch: 6.81 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2595355931342955		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 1.2595355931342955 | validation: 1.282428671463283]
	TIME [epoch: 6.78 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1157635628055838		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 1.1157635628055838 | validation: 1.1995541844736288]
	TIME [epoch: 6.75 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.22945217605624		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 1.22945217605624 | validation: 1.3701502516548851]
	TIME [epoch: 6.75 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1386353701073655		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 1.1386353701073655 | validation: 1.1686758616248283]
	TIME [epoch: 6.75 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0452407594367972		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 1.0452407594367972 | validation: 1.1377576638576468]
	TIME [epoch: 6.75 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0033162011557377		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 1.0033162011557377 | validation: 1.127076571858246]
	TIME [epoch: 6.79 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0127134083389577		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 1.0127134083389577 | validation: 1.0780914299189608]
	TIME [epoch: 6.76 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.106129520109192		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 1.106129520109192 | validation: 1.2647286570417275]
	TIME [epoch: 6.75 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.028861211031939		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 1.028861211031939 | validation: 1.2256191355482793]
	TIME [epoch: 6.75 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2496765478881624		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 1.2496765478881624 | validation: 1.4342192140226973]
	TIME [epoch: 6.75 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1963796347664652		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 1.1963796347664652 | validation: 1.1760536642660997]
	TIME [epoch: 6.75 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0495173618515798		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 1.0495173618515798 | validation: 1.3337550561673583]
	TIME [epoch: 6.74 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1358114543655629		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 1.1358114543655629 | validation: 1.241758234092301]
	TIME [epoch: 6.79 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9942415320855716		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.9942415320855716 | validation: 1.1077662627422815]
	TIME [epoch: 6.76 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0503029493294849		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 1.0503029493294849 | validation: 1.219295479676636]
	TIME [epoch: 6.75 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0587100501561355		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 1.0587100501561355 | validation: 1.2304969910476535]
	TIME [epoch: 6.75 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9702450909342111		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.9702450909342111 | validation: 1.3298230391841732]
	TIME [epoch: 6.74 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.050459308909373		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 1.050459308909373 | validation: 1.4521022189117072]
	TIME [epoch: 6.77 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.172076313098755		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 1.172076313098755 | validation: 1.3010898798515085]
	TIME [epoch: 6.77 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0805830423443923		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 1.0805830423443923 | validation: 1.129670891382109]
	TIME [epoch: 6.8 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0319585792520014		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 1.0319585792520014 | validation: 1.3641065828860746]
	TIME [epoch: 6.76 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.314393321549553		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 1.314393321549553 | validation: 1.4516730737336796]
	TIME [epoch: 6.75 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0133455987994562		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 1.0133455987994562 | validation: 1.065251338010981]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_325.pth
	Model improved!!!
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9290698382689517		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.9290698382689517 | validation: 1.0589333923584907]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_326.pth
	Model improved!!!
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9196953817977678		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.9196953817977678 | validation: 1.412132789455641]
	TIME [epoch: 6.74 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0664183853439761		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 1.0664183853439761 | validation: 1.2556467303340513]
	TIME [epoch: 6.76 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9620006188481313		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.9620006188481313 | validation: 1.025289306733219]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_329.pth
	Model improved!!!
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8468535079973051		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.8468535079973051 | validation: 0.9940621179252722]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_330.pth
	Model improved!!!
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8569828631172954		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.8569828631172954 | validation: 1.0615892719822035]
	TIME [epoch: 6.78 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9362198335943626		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.9362198335943626 | validation: 1.025110226271739]
	TIME [epoch: 6.77 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8681979814064925		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.8681979814064925 | validation: 2.072609873356394]
	TIME [epoch: 6.76 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2238581498432355		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 2.2238581498432355 | validation: 3.2411873966898677]
	TIME [epoch: 6.76 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.413592459150764		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 2.413592459150764 | validation: 2.934740374124142]
	TIME [epoch: 6.77 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5247908087546502		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 2.5247908087546502 | validation: 1.716221829088783]
	TIME [epoch: 6.78 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2003908693123		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 1.2003908693123 | validation: 1.1517417544148605]
	TIME [epoch: 6.77 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3266083490075375		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 1.3266083490075375 | validation: 1.333867075208489]
	TIME [epoch: 6.8 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0944483981859305		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 1.0944483981859305 | validation: 1.0956495294669923]
	TIME [epoch: 6.78 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8841952126991818		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.8841952126991818 | validation: 1.0949683894434679]
	TIME [epoch: 6.76 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0505693641990153		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 1.0505693641990153 | validation: 1.4125769543159832]
	TIME [epoch: 6.76 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0798447900872876		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 1.0798447900872876 | validation: 1.0818176067463066]
	TIME [epoch: 6.78 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3815435611508275		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 1.3815435611508275 | validation: 1.6735003054322304]
	TIME [epoch: 6.79 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4021448386223316		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 1.4021448386223316 | validation: 1.2061640405068932]
	TIME [epoch: 6.76 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0547144564116653		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 1.0547144564116653 | validation: 1.239932518310916]
	TIME [epoch: 6.76 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.013846885370022		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 1.013846885370022 | validation: 1.4463526615822773]
	TIME [epoch: 6.76 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1490098804243976		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 1.1490098804243976 | validation: 1.1076095840939477]
	TIME [epoch: 6.76 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9228299954132199		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.9228299954132199 | validation: 1.0859001844818463]
	TIME [epoch: 6.76 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8925120386312329		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.8925120386312329 | validation: 1.1020910626785272]
	TIME [epoch: 6.8 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.929805336445864		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.929805336445864 | validation: 1.0826529293842149]
	TIME [epoch: 6.78 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0014318974750633		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 1.0014318974750633 | validation: 1.0445027148096933]
	TIME [epoch: 6.77 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8879368810117825		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.8879368810117825 | validation: 1.094195579812796]
	TIME [epoch: 6.76 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.931526259816801		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.931526259816801 | validation: 1.0112933959116666]
	TIME [epoch: 6.76 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8551321844925505		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.8551321844925505 | validation: 1.0027520246177832]
	TIME [epoch: 6.76 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9594779241653213		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.9594779241653213 | validation: 1.0464414619517015]
	TIME [epoch: 6.76 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0208392843764889		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 1.0208392843764889 | validation: 1.5312499453427684]
	TIME [epoch: 6.84 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.052570032702526		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 1.052570032702526 | validation: 1.0347908388132294]
	TIME [epoch: 6.79 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9242258638373314		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.9242258638373314 | validation: 1.2488610894831211]
	TIME [epoch: 6.76 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9152975832783058		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.9152975832783058 | validation: 1.0084013655552337]
	TIME [epoch: 6.76 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9178863395834066		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.9178863395834066 | validation: 1.0403195882522716]
	TIME [epoch: 6.76 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8856814504620549		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.8856814504620549 | validation: 1.1308243693903282]
	TIME [epoch: 6.75 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9292705219339523		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.9292705219339523 | validation: 1.5415718614230354]
	TIME [epoch: 6.75 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1690269375090765		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 1.1690269375090765 | validation: 0.9453453213151033]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_363.pth
	Model improved!!!
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.85389018352222		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.85389018352222 | validation: 1.1779114518645137]
	TIME [epoch: 6.76 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.974409152625902		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.974409152625902 | validation: 1.2077183990046971]
	TIME [epoch: 6.76 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9002543453694076		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.9002543453694076 | validation: 1.0474395223529285]
	TIME [epoch: 6.75 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.819532923752045		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.819532923752045 | validation: 0.9785764971121718]
	TIME [epoch: 6.75 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9237265446795597		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.9237265446795597 | validation: 1.0561974147722557]
	TIME [epoch: 6.75 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8337312415395106		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.8337312415395106 | validation: 0.9954883198967142]
	TIME [epoch: 6.75 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.007014253032086		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 1.007014253032086 | validation: 1.0025562709939442]
	TIME [epoch: 6.79 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8674803340440385		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.8674803340440385 | validation: 0.986162282157478]
	TIME [epoch: 6.76 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.837933685658357		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.837933685658357 | validation: 1.1071001140788945]
	TIME [epoch: 6.75 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.076714167058857		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 1.076714167058857 | validation: 1.0572075619995487]
	TIME [epoch: 6.76 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9170241167206956		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.9170241167206956 | validation: 1.0344963235215001]
	TIME [epoch: 6.78 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0063728268391472		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 1.0063728268391472 | validation: 1.0433594631968055]
	TIME [epoch: 6.76 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8561416819421594		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.8561416819421594 | validation: 0.9439377252120937]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_376.pth
	Model improved!!!
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.86086296521179		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.86086296521179 | validation: 1.043276639237812]
	TIME [epoch: 6.8 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.849273645069734		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.849273645069734 | validation: 1.071636457935399]
	TIME [epoch: 6.76 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9453307277354475		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.9453307277354475 | validation: 1.1822582989228931]
	TIME [epoch: 6.75 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8909506358898511		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.8909506358898511 | validation: 1.3561257517200338]
	TIME [epoch: 6.75 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9542948061286807		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.9542948061286807 | validation: 1.027789802000164]
	TIME [epoch: 6.75 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8806471252390444		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 0.8806471252390444 | validation: 1.2071175838857346]
	TIME [epoch: 6.75 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.868821435746071		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 0.868821435746071 | validation: 1.0196447216150337]
	TIME [epoch: 6.77 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8862201533557308		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 0.8862201533557308 | validation: 1.0528855502177055]
	TIME [epoch: 6.78 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9364453139758763		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 0.9364453139758763 | validation: 1.06931597672523]
	TIME [epoch: 6.75 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8946408097310072		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 0.8946408097310072 | validation: 1.029455996149754]
	TIME [epoch: 6.75 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9034515785731384		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 0.9034515785731384 | validation: 1.1766283841229965]
	TIME [epoch: 6.75 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9580295406652042		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 0.9580295406652042 | validation: 1.4518440725242747]
	TIME [epoch: 6.75 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.01914218303413		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 1.01914218303413 | validation: 1.0957223403390386]
	TIME [epoch: 6.75 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9525892254259274		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 0.9525892254259274 | validation: 0.9471971105106493]
	TIME [epoch: 6.78 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8031814872380195		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 0.8031814872380195 | validation: 0.9816694892445631]
	TIME [epoch: 6.79 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.828722078607838		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 0.828722078607838 | validation: 0.9793750817173965]
	TIME [epoch: 6.79 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8709408443925708		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 0.8709408443925708 | validation: 1.0727361457887419]
	TIME [epoch: 6.77 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9460477293806968		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 0.9460477293806968 | validation: 0.9458155496522767]
	TIME [epoch: 6.75 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8653819438565364		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 0.8653819438565364 | validation: 1.0916311643327339]
	TIME [epoch: 6.75 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9409454608250991		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 0.9409454608250991 | validation: 0.9374119415083668]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_396.pth
	Model improved!!!
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7839602148514175		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.7839602148514175 | validation: 0.8945950696952325]
	TIME [epoch: 6.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_397.pth
	Model improved!!!
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7546928215056087		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 0.7546928215056087 | validation: 0.8736063163886012]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_398.pth
	Model improved!!!
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8656167375805028		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 0.8656167375805028 | validation: 0.9289831386838003]
	TIME [epoch: 6.77 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8637388671992767		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 0.8637388671992767 | validation: 0.9155341704557758]
	TIME [epoch: 6.76 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.812981031340789		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 0.812981031340789 | validation: 1.1269159754784164]
	TIME [epoch: 6.76 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8902106969261881		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 0.8902106969261881 | validation: 0.906307823688856]
	TIME [epoch: 6.75 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7667708559704045		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 0.7667708559704045 | validation: 0.9718335870807615]
	TIME [epoch: 6.76 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8233661503163385		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 0.8233661503163385 | validation: 0.9853625612845214]
	TIME [epoch: 6.81 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8031966452712367		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 0.8031966452712367 | validation: 1.0375603579220252]
	TIME [epoch: 6.77 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8109470444508496		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 0.8109470444508496 | validation: 0.9709259846034379]
	TIME [epoch: 6.75 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7963665651889384		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 0.7963665651889384 | validation: 0.8735130820245182]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_407.pth
	Model improved!!!
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.755777583092663		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 0.755777583092663 | validation: 1.0124321047953726]
	TIME [epoch: 6.76 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8562460425576001		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 0.8562460425576001 | validation: 1.033879177297329]
	TIME [epoch: 6.77 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8122040512830382		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 0.8122040512830382 | validation: 0.9829843970646479]
	TIME [epoch: 6.79 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7674436678214548		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 0.7674436678214548 | validation: 1.0637783310729574]
	TIME [epoch: 6.81 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8847705116817288		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 0.8847705116817288 | validation: 1.0890526426811822]
	TIME [epoch: 6.76 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.819930389870301		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 0.819930389870301 | validation: 0.9000879502725754]
	TIME [epoch: 6.76 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8054358239765146		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 0.8054358239765146 | validation: 1.1038093056041616]
	TIME [epoch: 6.76 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8545698839382387		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 0.8545698839382387 | validation: 0.9557253677439523]
	TIME [epoch: 6.75 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7825534498446507		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.7825534498446507 | validation: 0.9131893723818272]
	TIME [epoch: 6.76 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7746428039168121		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 0.7746428039168121 | validation: 0.9294894258158569]
	TIME [epoch: 6.78 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7421957302988127		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 0.7421957302988127 | validation: 0.9600175093927092]
	TIME [epoch: 6.79 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7497066961397088		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 0.7497066961397088 | validation: 0.8809000332082171]
	TIME [epoch: 6.76 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7492151475634992		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 0.7492151475634992 | validation: 0.872817930986265]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_420.pth
	Model improved!!!
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7606271790758582		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 0.7606271790758582 | validation: 0.8792053911393644]
	TIME [epoch: 6.75 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7342531067934439		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 0.7342531067934439 | validation: 0.9723182498742142]
	TIME [epoch: 6.75 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0142426299756802		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 1.0142426299756802 | validation: 1.056905200945672]
	TIME [epoch: 6.75 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9118422097784955		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 0.9118422097784955 | validation: 0.9193979423876989]
	TIME [epoch: 6.77 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7901144745338127		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 0.7901144745338127 | validation: 1.0102569305418632]
	TIME [epoch: 6.78 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7900984212440006		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 0.7900984212440006 | validation: 1.0781277534245821]
	TIME [epoch: 6.76 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8895224640026289		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 0.8895224640026289 | validation: 1.2971582949646199]
	TIME [epoch: 6.78 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8519323330298327		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 0.8519323330298327 | validation: 0.9745822856556743]
	TIME [epoch: 6.77 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7589082144382366		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 0.7589082144382366 | validation: 0.8911836779848692]
	TIME [epoch: 6.76 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0212749529775338		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 1.0212749529775338 | validation: 1.2868144172827112]
	TIME [epoch: 6.75 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9407894136725657		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 0.9407894136725657 | validation: 1.0582877933444383]
	TIME [epoch: 6.79 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7921549038773208		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 0.7921549038773208 | validation: 0.9405390276963547]
	TIME [epoch: 6.77 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8010346906445804		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 0.8010346906445804 | validation: 0.9147218032027451]
	TIME [epoch: 6.75 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7406504860309557		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 0.7406504860309557 | validation: 1.0045482784932227]
	TIME [epoch: 6.76 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7595183012697238		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.7595183012697238 | validation: 0.9163148091768407]
	TIME [epoch: 6.75 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7472843867755533		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 0.7472843867755533 | validation: 1.0489212844705682]
	TIME [epoch: 6.76 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7672481903450276		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 0.7672481903450276 | validation: 1.0254893638048084]
	TIME [epoch: 6.75 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0401258740195836		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 1.0401258740195836 | validation: 0.9779031377188757]
	TIME [epoch: 6.8 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8101667645388508		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 0.8101667645388508 | validation: 1.0012188744581811]
	TIME [epoch: 6.77 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9780072624157188		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 0.9780072624157188 | validation: 1.1900409556830192]
	TIME [epoch: 6.76 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8815158445923843		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 0.8815158445923843 | validation: 0.886484203931744]
	TIME [epoch: 6.75 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7266226365912529		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 0.7266226365912529 | validation: 0.8866945617538016]
	TIME [epoch: 6.75 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7078345814304652		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 0.7078345814304652 | validation: 0.9767578291113586]
	TIME [epoch: 6.75 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7545169416309957		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 0.7545169416309957 | validation: 0.8746282267211227]
	TIME [epoch: 6.76 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7504882565149018		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 0.7504882565149018 | validation: 0.9497899131490279]
	TIME [epoch: 6.83 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7772871007820228		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 0.7772871007820228 | validation: 0.9078820675330912]
	TIME [epoch: 6.79 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8142610362527782		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 0.8142610362527782 | validation: 1.0795569762846833]
	TIME [epoch: 6.76 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8058327948683359		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 0.8058327948683359 | validation: 0.916307804290138]
	TIME [epoch: 6.75 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7342543983525833		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 0.7342543983525833 | validation: 1.1885174629814763]
	TIME [epoch: 6.75 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9483098832074783		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 0.9483098832074783 | validation: 0.8731471775868571]
	TIME [epoch: 6.75 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.730924779267341		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 0.730924779267341 | validation: 0.8865550510558684]
	TIME [epoch: 6.76 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8102028901848914		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 0.8102028901848914 | validation: 0.8930811273921038]
	TIME [epoch: 6.8 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7600255587545521		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 0.7600255587545521 | validation: 1.026955336069478]
	TIME [epoch: 6.78 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.107416581237333		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 1.107416581237333 | validation: 1.0145775375409167]
	TIME [epoch: 6.76 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7659070036102844		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 0.7659070036102844 | validation: 0.9464966249199976]
	TIME [epoch: 6.75 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7876717406030415		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 0.7876717406030415 | validation: 0.8850572351127349]
	TIME [epoch: 6.75 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8618923008303598		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 0.8618923008303598 | validation: 0.9735049523505498]
	TIME [epoch: 6.75 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7497627366309857		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 0.7497627366309857 | validation: 0.8712607226720699]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_458.pth
	Model improved!!!
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7578609002061534		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 0.7578609002061534 | validation: 0.9423922904943594]
	TIME [epoch: 6.79 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7404160596490899		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 0.7404160596490899 | validation: 0.8316337310303141]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_460.pth
	Model improved!!!
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7220160630702716		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 0.7220160630702716 | validation: 0.9671199659436439]
	TIME [epoch: 6.76 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8645061289460797		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 0.8645061289460797 | validation: 0.9075065165563919]
	TIME [epoch: 6.75 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7274235128615767		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 0.7274235128615767 | validation: 0.8544121237976163]
	TIME [epoch: 6.77 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.877187853475251		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 0.877187853475251 | validation: 0.9790055414864436]
	TIME [epoch: 6.77 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8506706778719049		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 0.8506706778719049 | validation: 1.262266015405337]
	TIME [epoch: 6.78 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8553286050206323		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 0.8553286050206323 | validation: 0.9470917757250397]
	TIME [epoch: 6.79 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0976478500854905		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 1.0976478500854905 | validation: 1.2009896257145174]
	TIME [epoch: 6.75 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9135614045750715		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 0.9135614045750715 | validation: 0.9106371591855321]
	TIME [epoch: 6.76 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.732591663478671		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 0.732591663478671 | validation: 0.9535733731287905]
	TIME [epoch: 6.75 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7704442358910336		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 0.7704442358910336 | validation: 1.118091516019425]
	TIME [epoch: 6.75 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8933664465996708		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 0.8933664465996708 | validation: 0.9596186159155786]
	TIME [epoch: 6.76 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7483782269173154		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 0.7483782269173154 | validation: 1.0166195060851584]
	TIME [epoch: 6.78 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8042829795769342		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.8042829795769342 | validation: 1.0034516640922788]
	TIME [epoch: 6.79 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7958853317175951		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 0.7958853317175951 | validation: 0.8777475024511079]
	TIME [epoch: 6.76 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7446802062118537		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 0.7446802062118537 | validation: 1.0118417082890456]
	TIME [epoch: 6.79 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8057646298865009		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 0.8057646298865009 | validation: 0.95641892097826]
	TIME [epoch: 6.75 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7871178926016749		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 0.7871178926016749 | validation: 0.8596551739684615]
	TIME [epoch: 6.76 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.744277666568161		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 0.744277666568161 | validation: 0.8585934430156315]
	TIME [epoch: 6.75 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8507129900340868		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 0.8507129900340868 | validation: 1.0064843548456324]
	TIME [epoch: 6.8 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7640821629911915		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 0.7640821629911915 | validation: 0.8523679627633496]
	TIME [epoch: 6.79 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7085980804083347		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 0.7085980804083347 | validation: 0.9629262554827943]
	TIME [epoch: 6.79 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8968056630858461		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 0.8968056630858461 | validation: 0.9318527458187111]
	TIME [epoch: 6.77 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8029327376634274		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 0.8029327376634274 | validation: 0.9558955524290818]
	TIME [epoch: 6.75 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7986752735837714		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 0.7986752735837714 | validation: 0.8998982595344973]
	TIME [epoch: 6.76 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8586972850069363		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 0.8586972850069363 | validation: 1.183844784347988]
	TIME [epoch: 6.75 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8542943844281116		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 0.8542943844281116 | validation: 0.8690690342913643]
	TIME [epoch: 6.79 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7808338954086897		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 0.7808338954086897 | validation: 0.8632640697073788]
	TIME [epoch: 6.77 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7579933574135977		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 0.7579933574135977 | validation: 0.8614045738261907]
	TIME [epoch: 6.76 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7353424617424761		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 0.7353424617424761 | validation: 0.8446344474839564]
	TIME [epoch: 6.76 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7797908813943007		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 0.7797908813943007 | validation: 0.8243925582738353]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_490.pth
	Model improved!!!
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7807633463010533		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 0.7807633463010533 | validation: 0.8990515948743947]
	TIME [epoch: 6.77 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7111622930083752		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.7111622930083752 | validation: 0.8264498360202278]
	TIME [epoch: 6.76 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6826418506218046		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 0.6826418506218046 | validation: 0.8321015334417554]
	TIME [epoch: 6.81 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6932498461754021		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 0.6932498461754021 | validation: 0.9250577175278936]
	TIME [epoch: 6.77 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6946648346271078		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 0.6946648346271078 | validation: 0.869305303210455]
	TIME [epoch: 6.76 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.823663373425914		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 0.823663373425914 | validation: 0.9693280541286102]
	TIME [epoch: 6.76 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8060985893185133		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 0.8060985893185133 | validation: 0.801380645871534]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_497.pth
	Model improved!!!
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8205199525523283		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 0.8205199525523283 | validation: 0.8889750687324106]
	TIME [epoch: 6.77 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7607393233173689		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 0.7607393233173689 | validation: 0.8481522357974655]
	TIME [epoch: 6.78 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7046572920007778		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 0.7046572920007778 | validation: 0.8286140112042524]
	TIME [epoch: 6.81 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6835025968585322		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 0.6835025968585322 | validation: 0.8284288848809465]
	TIME [epoch: 6.77 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7716389517485602		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 0.7716389517485602 | validation: 0.8159047954938503]
	TIME [epoch: 6.76 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6739052987622931		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 0.6739052987622931 | validation: 0.8278618238586266]
	TIME [epoch: 6.75 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6987051694398232		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 0.6987051694398232 | validation: 0.9295646886502152]
	TIME [epoch: 6.76 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7267179474383839		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 0.7267179474383839 | validation: 0.9254667061569514]
	TIME [epoch: 6.76 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7556343380385808		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 0.7556343380385808 | validation: 0.8447644778699118]
	TIME [epoch: 6.78 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7025987488447578		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 0.7025987488447578 | validation: 0.8854528096408503]
	TIME [epoch: 6.79 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7135455281475989		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 0.7135455281475989 | validation: 0.8910125132308306]
	TIME [epoch: 6.77 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6811783064997825		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 0.6811783064997825 | validation: 0.8603538631313428]
	TIME [epoch: 6.77 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8661825779011472		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 0.8661825779011472 | validation: 0.8080847616340363]
	TIME [epoch: 6.77 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6953129956921225		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.6953129956921225 | validation: 0.8265819549371574]
	TIME [epoch: 6.76 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6999606676206593		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 0.6999606676206593 | validation: 0.828128598906719]
	TIME [epoch: 6.76 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7126754029497919		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 0.7126754029497919 | validation: 0.8403806504694802]
	TIME [epoch: 6.79 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7392171364175805		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 0.7392171364175805 | validation: 0.8175674503041673]
	TIME [epoch: 6.8 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.688585613976181		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 0.688585613976181 | validation: 0.8320097178982746]
	TIME [epoch: 6.77 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6788188677284515		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 0.6788188677284515 | validation: 0.8519512489683989]
	TIME [epoch: 6.79 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6988185041650196		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 0.6988185041650196 | validation: 0.8224393408490404]
	TIME [epoch: 6.79 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7025177997624046		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 0.7025177997624046 | validation: 0.8053789625762113]
	TIME [epoch: 6.77 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6682598182501991		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 0.6682598182501991 | validation: 0.8212918579915504]
	TIME [epoch: 6.76 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7153799367793213		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 0.7153799367793213 | validation: 0.8252822015720431]
	TIME [epoch: 6.8 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6702365187323414		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 0.6702365187323414 | validation: 0.8405817498354166]
	TIME [epoch: 6.78 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6769633667791528		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 0.6769633667791528 | validation: 0.899065233393809]
	TIME [epoch: 6.76 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6999488013461536		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 0.6999488013461536 | validation: 0.8830236514574044]
	TIME [epoch: 6.77 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7338446714897492		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 0.7338446714897492 | validation: 0.8279847634198885]
	TIME [epoch: 6.77 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6918285135213911		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 0.6918285135213911 | validation: 0.9066978163692259]
	TIME [epoch: 6.76 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.694452254140669		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 0.694452254140669 | validation: 0.8262837958569549]
	TIME [epoch: 6.77 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6813250176960138		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 0.6813250176960138 | validation: 0.8040578385916888]
	TIME [epoch: 6.81 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6657627426775246		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 0.6657627426775246 | validation: 0.840633305583831]
	TIME [epoch: 6.79 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6791314678931049		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 0.6791314678931049 | validation: 0.8315465390467272]
	TIME [epoch: 6.77 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6961633792506783		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.6961633792506783 | validation: 0.89165198064285]
	TIME [epoch: 6.76 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6926347662356716		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 0.6926347662356716 | validation: 0.7958061502563096]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_531.pth
	Model improved!!!
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6491951048485807		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 0.6491951048485807 | validation: 0.7905740691024952]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_532.pth
	Model improved!!!
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7413864145345057		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 0.7413864145345057 | validation: 0.8466495945819386]
	TIME [epoch: 6.77 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7220592762465241		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 0.7220592762465241 | validation: 0.9451262244933013]
	TIME [epoch: 6.84 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8040226673426287		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 0.8040226673426287 | validation: 0.8957699760150899]
	TIME [epoch: 6.8 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.702077733143811		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 0.702077733143811 | validation: 0.8330410176468876]
	TIME [epoch: 6.76 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.678039481420281		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 0.678039481420281 | validation: 0.8297915443696755]
	TIME [epoch: 6.76 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6943548480991585		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 0.6943548480991585 | validation: 0.8632972974224066]
	TIME [epoch: 6.76 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6803761474764249		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 0.6803761474764249 | validation: 0.8460681301713675]
	TIME [epoch: 6.76 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.698787534935215		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 0.698787534935215 | validation: 0.8080054574594364]
	TIME [epoch: 6.77 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6588736757391095		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 0.6588736757391095 | validation: 0.7843664297705011]
	TIME [epoch: 6.81 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_541.pth
	Model improved!!!
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6381148079951888		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 0.6381148079951888 | validation: 0.7995459890288938]
	TIME [epoch: 6.77 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.692487711693514		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 0.692487711693514 | validation: 0.832862408541137]
	TIME [epoch: 6.76 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.672651680206658		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 0.672651680206658 | validation: 0.7943002262267873]
	TIME [epoch: 6.75 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6662521798493559		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 0.6662521798493559 | validation: 0.824305145132546]
	TIME [epoch: 6.76 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7273633258738296		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 0.7273633258738296 | validation: 0.8192811387296199]
	TIME [epoch: 6.76 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6555855942759559		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 0.6555855942759559 | validation: 0.8158851481102407]
	TIME [epoch: 6.77 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.734818649045954		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 0.734818649045954 | validation: 0.9753821595584518]
	TIME [epoch: 6.79 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7380971700599497		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.7380971700599497 | validation: 0.8366231710550524]
	TIME [epoch: 6.76 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6587038121676106		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 0.6587038121676106 | validation: 0.8036871307516464]
	TIME [epoch: 6.76 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.728773730030304		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 0.728773730030304 | validation: 0.8916199028152836]
	TIME [epoch: 6.76 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7072627780175491		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 0.7072627780175491 | validation: 0.8327431549188427]
	TIME [epoch: 6.78 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6623585210823822		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 0.6623585210823822 | validation: 0.8050929656972096]
	TIME [epoch: 6.77 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6644731246829123		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 0.6644731246829123 | validation: 0.8348089968720841]
	TIME [epoch: 6.78 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6486364590780261		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 0.6486364590780261 | validation: 0.7855877153684057]
	TIME [epoch: 6.79 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6550281244078231		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 0.6550281244078231 | validation: 0.824044045467321]
	TIME [epoch: 6.75 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6804283250471932		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 0.6804283250471932 | validation: 0.7902298296318151]
	TIME [epoch: 6.75 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6423750239828603		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 0.6423750239828603 | validation: 0.7942782899623784]
	TIME [epoch: 6.75 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6487102002436953		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 0.6487102002436953 | validation: 0.7967699240485954]
	TIME [epoch: 6.75 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6938698756067891		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 0.6938698756067891 | validation: 0.7931206257685612]
	TIME [epoch: 6.75 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.704066703601018		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 0.704066703601018 | validation: 0.7885027198898369]
	TIME [epoch: 6.8 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6631188896155548		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 0.6631188896155548 | validation: 0.8170890235701014]
	TIME [epoch: 6.77 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6677488297890987		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 0.6677488297890987 | validation: 0.7866036047642344]
	TIME [epoch: 6.75 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6333575120786455		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 0.6333575120786455 | validation: 0.7693172365255425]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_564.pth
	Model improved!!!
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6422007502398698		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 0.6422007502398698 | validation: 0.7897889473760922]
	TIME [epoch: 6.75 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6367043387757025		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 0.6367043387757025 | validation: 0.8067884515014685]
	TIME [epoch: 6.76 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7664178229384369		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 0.7664178229384369 | validation: 0.8048394388114447]
	TIME [epoch: 6.77 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6524222041518744		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.6524222041518744 | validation: 0.7694293994833851]
	TIME [epoch: 6.8 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6425981887710619		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 0.6425981887710619 | validation: 0.801536213830774]
	TIME [epoch: 6.78 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6607425228522265		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 0.6607425228522265 | validation: 0.82160943598214]
	TIME [epoch: 6.78 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6761442745725497		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 0.6761442745725497 | validation: 0.7665450504412189]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_571.pth
	Model improved!!!
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.682082916653376		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 0.682082916653376 | validation: 0.7736296253920429]
	TIME [epoch: 6.76 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7128720474521137		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 0.7128720474521137 | validation: 0.8178719616722218]
	TIME [epoch: 6.75 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6592295107160584		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 0.6592295107160584 | validation: 0.8022936398646203]
	TIME [epoch: 6.76 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6417332980574824		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 0.6417332980574824 | validation: 0.8077562467034956]
	TIME [epoch: 6.81 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6407428716345828		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 0.6407428716345828 | validation: 0.798282768392317]
	TIME [epoch: 6.77 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6600312656075276		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 0.6600312656075276 | validation: 0.7731744955975794]
	TIME [epoch: 6.77 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6484185912707956		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 0.6484185912707956 | validation: 0.8183889089520167]
	TIME [epoch: 6.77 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7634137042854281		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 0.7634137042854281 | validation: 0.956838676114402]
	TIME [epoch: 6.76 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7295990091560165		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 0.7295990091560165 | validation: 0.7907781572992705]
	TIME [epoch: 6.75 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6647423265928668		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 0.6647423265928668 | validation: 0.8133321346122088]
	TIME [epoch: 6.77 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6957171825015649		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 0.6957171825015649 | validation: 0.8072675115558967]
	TIME [epoch: 6.81 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6487571452440005		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 0.6487571452440005 | validation: 0.7802473931063972]
	TIME [epoch: 6.77 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6396664283526574		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 0.6396664283526574 | validation: 0.7544511230041901]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_584.pth
	Model improved!!!
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6464615677584571		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 0.6464615677584571 | validation: 0.7688887606562431]
	TIME [epoch: 6.78 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.635219751256314		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 0.635219751256314 | validation: 0.780369182208354]
	TIME [epoch: 6.76 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6629660961997025		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.6629660961997025 | validation: 0.7585615491049529]
	TIME [epoch: 6.77 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6593929150075322		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 0.6593929150075322 | validation: 0.7721838192305583]
	TIME [epoch: 6.8 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6534518467666526		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 0.6534518467666526 | validation: 0.7874485036813998]
	TIME [epoch: 6.8 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6435656016551397		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 0.6435656016551397 | validation: 0.7877054238809106]
	TIME [epoch: 6.76 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6376747416167499		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 0.6376747416167499 | validation: 0.870442471330183]
	TIME [epoch: 6.76 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.784011992277607		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 0.784011992277607 | validation: 0.8476453467529188]
	TIME [epoch: 6.76 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6709695568544412		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 0.6709695568544412 | validation: 0.8320482442001731]
	TIME [epoch: 6.76 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6728631224192002		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 0.6728631224192002 | validation: 0.8236072312305304]
	TIME [epoch: 6.76 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6479826583994401		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 0.6479826583994401 | validation: 0.7914213804984547]
	TIME [epoch: 6.78 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6397689995825444		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 0.6397689995825444 | validation: 0.7760391642406257]
	TIME [epoch: 6.8 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6331710910230246		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 0.6331710910230246 | validation: 0.7765524994022843]
	TIME [epoch: 6.76 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6373206625906077		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 0.6373206625906077 | validation: 0.7676454160009807]
	TIME [epoch: 6.77 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6437099577729635		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 0.6437099577729635 | validation: 0.7641453219344667]
	TIME [epoch: 6.76 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6496730008561082		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 0.6496730008561082 | validation: 0.8065389650446837]
	TIME [epoch: 6.76 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.66055393964162		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 0.66055393964162 | validation: 0.815673393040004]
	TIME [epoch: 6.76 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.657557062991417		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 0.657557062991417 | validation: 0.8217305266954079]
	TIME [epoch: 6.81 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6468991461052166		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 0.6468991461052166 | validation: 0.7889646030271433]
	TIME [epoch: 6.77 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.640936899570218		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 0.640936899570218 | validation: 0.8066065815283028]
	TIME [epoch: 6.76 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6610484138505358		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 0.6610484138505358 | validation: 0.7981786903380291]
	TIME [epoch: 6.78 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6664941712160342		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.6664941712160342 | validation: 0.7736144895485102]
	TIME [epoch: 6.78 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6430513490719131		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 0.6430513490719131 | validation: 0.8062981681449262]
	TIME [epoch: 6.76 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6339735111733793		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 0.6339735111733793 | validation: 0.7970558370913527]
	TIME [epoch: 6.75 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6503843634807467		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 0.6503843634807467 | validation: 0.7789780246191639]
	TIME [epoch: 6.8 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6432019728667616		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 0.6432019728667616 | validation: 0.7870024891141689]
	TIME [epoch: 6.77 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428367989053941		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 0.6428367989053941 | validation: 0.7798613068180066]
	TIME [epoch: 6.77 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6518320889894194		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 0.6518320889894194 | validation: 0.7547053302956406]
	TIME [epoch: 6.76 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6388823158598138		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 0.6388823158598138 | validation: 0.7693403556346339]
	TIME [epoch: 6.76 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6265094936180675		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 0.6265094936180675 | validation: 0.7640222733884277]
	TIME [epoch: 6.76 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6209168176345811		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 0.6209168176345811 | validation: 0.7530134830225104]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_615.pth
	Model improved!!!
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6610388719981144		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 0.6610388719981144 | validation: 0.7592534933747768]
	TIME [epoch: 6.82 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6321911539424563		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 0.6321911539424563 | validation: 0.7569799821203567]
	TIME [epoch: 6.77 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6320596834806502		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 0.6320596834806502 | validation: 0.7611020058362576]
	TIME [epoch: 6.76 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.626420080417464		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 0.626420080417464 | validation: 0.7519366955330433]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_619.pth
	Model improved!!!
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6221563462095603		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 0.6221563462095603 | validation: 0.7979600596456187]
	TIME [epoch: 6.76 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6875966575468293		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 0.6875966575468293 | validation: 0.8662910124677434]
	TIME [epoch: 6.75 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6615610251608768		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 0.6615610251608768 | validation: 0.7745769838287723]
	TIME [epoch: 6.75 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6386757062121939		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 0.6386757062121939 | validation: 0.7816403956636213]
	TIME [epoch: 6.82 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.653133693020806		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 0.653133693020806 | validation: 0.7552816777562029]
	TIME [epoch: 6.78 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6202415443583533		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.6202415443583533 | validation: 0.733943489178855]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_625.pth
	Model improved!!!
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6257261203773192		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 0.6257261203773192 | validation: 0.7683933485549168]
	TIME [epoch: 6.75 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6358405907107914		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 0.6358405907107914 | validation: 0.7612956437445197]
	TIME [epoch: 6.75 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6381805539546023		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 0.6381805539546023 | validation: 0.7693198306197543]
	TIME [epoch: 6.75 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6317664832381544		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 0.6317664832381544 | validation: 0.7543636085427552]
	TIME [epoch: 6.78 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6191885090760921		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 0.6191885090760921 | validation: 0.7551524174842243]
	TIME [epoch: 6.79 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6150948948193931		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 0.6150948948193931 | validation: 0.7540255055846778]
	TIME [epoch: 6.76 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6150097203224574		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 0.6150097203224574 | validation: 0.7806079464445774]
	TIME [epoch: 6.76 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6783088410407228		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 0.6783088410407228 | validation: 0.7554653667206412]
	TIME [epoch: 6.76 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6204935697348739		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 0.6204935697348739 | validation: 0.7496248205941642]
	TIME [epoch: 6.75 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6179462382186464		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 0.6179462382186464 | validation: 0.7622885309187704]
	TIME [epoch: 6.75 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6350082058017528		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 0.6350082058017528 | validation: 0.7648071009275041]
	TIME [epoch: 6.78 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6176879849541468		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 0.6176879849541468 | validation: 0.8073508811430133]
	TIME [epoch: 6.8 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6412992848423068		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 0.6412992848423068 | validation: 0.7669393378528476]
	TIME [epoch: 6.77 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6416852309780734		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 0.6416852309780734 | validation: 0.7623515823973845]
	TIME [epoch: 6.76 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6616489553907704		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 0.6616489553907704 | validation: 0.8108771900097713]
	TIME [epoch: 6.76 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7232485155348882		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 0.7232485155348882 | validation: 0.8412736746996803]
	TIME [epoch: 6.77 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.676276352540858		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 0.676276352540858 | validation: 0.7636634128273028]
	TIME [epoch: 6.78 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6267771713521271		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 0.6267771713521271 | validation: 0.752165152142572]
	TIME [epoch: 6.79 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6352758761223695		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.6352758761223695 | validation: 0.7684996115011037]
	TIME [epoch: 6.76 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6307374659918183		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 0.6307374659918183 | validation: 0.7939591959813281]
	TIME [epoch: 6.74 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6656992421232155		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 0.6656992421232155 | validation: 0.7707792019080091]
	TIME [epoch: 6.75 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6359220214633152		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 0.6359220214633152 | validation: 0.730167169639192]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_647.pth
	Model improved!!!
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6165654311373736		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 0.6165654311373736 | validation: 0.7486902324023358]
	TIME [epoch: 6.75 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6279592709339528		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 0.6279592709339528 | validation: 0.75293576305637]
	TIME [epoch: 6.74 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6309113511490256		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 0.6309113511490256 | validation: 0.7585011040442546]
	TIME [epoch: 6.79 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428198337691688		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 0.6428198337691688 | validation: 0.7839967581529637]
	TIME [epoch: 6.76 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.726304160272484		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 0.726304160272484 | validation: 0.9923577141752968]
	TIME [epoch: 6.75 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7508330122157341		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 0.7508330122157341 | validation: 0.7846786164445138]
	TIME [epoch: 6.74 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6299178585585576		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 0.6299178585585576 | validation: 0.7821007367066846]
	TIME [epoch: 6.75 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6325526804796346		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 0.6325526804796346 | validation: 0.7795812817065286]
	TIME [epoch: 6.75 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6278582378221349		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 0.6278582378221349 | validation: 0.7934812053132936]
	TIME [epoch: 6.75 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6536644096678531		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 0.6536644096678531 | validation: 0.8267091084175509]
	TIME [epoch: 6.8 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6658096383773376		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 0.6658096383773376 | validation: 0.8219183347328676]
	TIME [epoch: 6.76 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6587458942464335		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 0.6587458942464335 | validation: 0.7886946676318187]
	TIME [epoch: 6.79 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6329484847617644		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 0.6329484847617644 | validation: 0.8061586160731149]
	TIME [epoch: 6.78 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6334459346854278		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 0.6334459346854278 | validation: 0.7774490237994893]
	TIME [epoch: 6.76 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6524302947134368		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 0.6524302947134368 | validation: 0.7749945036214123]
	TIME [epoch: 6.76 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6333098512816657		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.6333098512816657 | validation: 0.7780674448227182]
	TIME [epoch: 6.77 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6284288115324872		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 0.6284288115324872 | validation: 0.7554848382377299]
	TIME [epoch: 6.82 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6417941129424798		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 0.6417941129424798 | validation: 0.7466852385011269]
	TIME [epoch: 6.78 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6230400983187235		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 0.6230400983187235 | validation: 0.7889282015009067]
	TIME [epoch: 6.78 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7193224468792467		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 0.7193224468792467 | validation: 0.8809935846546529]
	TIME [epoch: 6.77 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6878852806612747		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 0.6878852806612747 | validation: 0.773066680871382]
	TIME [epoch: 6.77 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6313813482266468		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 0.6313813482266468 | validation: 0.7483633096803934]
	TIME [epoch: 6.77 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6300009065714786		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 0.6300009065714786 | validation: 0.7591528440185441]
	TIME [epoch: 6.77 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6293056287318438		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 0.6293056287318438 | validation: 0.7567120213043999]
	TIME [epoch: 6.81 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6182135186216267		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 0.6182135186216267 | validation: 0.7405531358289086]
	TIME [epoch: 6.76 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6476245540901944		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 0.6476245540901944 | validation: 0.7919081995031971]
	TIME [epoch: 6.78 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7015116827183486		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 0.7015116827183486 | validation: 0.8400432092950773]
	TIME [epoch: 6.76 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6848176961308393		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 0.6848176961308393 | validation: 0.7853616904674053]
	TIME [epoch: 6.76 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6351640749263542		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 0.6351640749263542 | validation: 0.7838265774781679]
	TIME [epoch: 6.75 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6337043204434255		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 0.6337043204434255 | validation: 0.7715304573021371]
	TIME [epoch: 6.82 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.635273581090883		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 0.635273581090883 | validation: 0.7630623538275754]
	TIME [epoch: 6.8 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6287407434554717		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 0.6287407434554717 | validation: 0.7905870177786483]
	TIME [epoch: 6.76 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6422963935109677		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 0.6422963935109677 | validation: 0.7575952165296028]
	TIME [epoch: 6.76 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6262373010632885		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 0.6262373010632885 | validation: 0.7786333549987978]
	TIME [epoch: 6.76 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.631986529132027		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.631986529132027 | validation: 0.8021926980582144]
	TIME [epoch: 6.76 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6394605408436161		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 0.6394605408436161 | validation: 0.7718933232068435]
	TIME [epoch: 6.76 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6290248297010963		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 0.6290248297010963 | validation: 0.7713942539785688]
	TIME [epoch: 6.78 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6200032720192775		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 0.6200032720192775 | validation: 0.7526202159795157]
	TIME [epoch: 6.79 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6241117247621012		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 0.6241117247621012 | validation: 0.7843541215836729]
	TIME [epoch: 6.76 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231104070344717		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 0.6231104070344717 | validation: 0.785994898912964]
	TIME [epoch: 6.76 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6362824699415669		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 0.6362824699415669 | validation: 0.7842613857665117]
	TIME [epoch: 6.76 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6331018382070079		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 0.6331018382070079 | validation: 0.7645474679395182]
	TIME [epoch: 6.76 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6252772677470299		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 0.6252772677470299 | validation: 0.7554738768519534]
	TIME [epoch: 6.76 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6402290846133472		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 0.6402290846133472 | validation: 0.7893911020957081]
	TIME [epoch: 6.81 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6902446857834267		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 0.6902446857834267 | validation: 0.7838735610927516]
	TIME [epoch: 6.78 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6515791030170024		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 0.6515791030170024 | validation: 0.7480754837635791]
	TIME [epoch: 6.77 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6190421652212428		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 0.6190421652212428 | validation: 0.7399886233842808]
	TIME [epoch: 6.78 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6181830142852308		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 0.6181830142852308 | validation: 0.7459429434196115]
	TIME [epoch: 6.81 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6181331704223896		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 0.6181331704223896 | validation: 0.7807074818314552]
	TIME [epoch: 6.78 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6479585412507313		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 0.6479585412507313 | validation: 0.7697246976684787]
	TIME [epoch: 6.77 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6250273460469602		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 0.6250273460469602 | validation: 0.7538076677886487]
	TIME [epoch: 6.81 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6185156920733509		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 0.6185156920733509 | validation: 0.7548588107624696]
	TIME [epoch: 6.78 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6160179316328804		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 0.6160179316328804 | validation: 0.7388699090668036]
	TIME [epoch: 6.77 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6470226038500861		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.6470226038500861 | validation: 0.7980443880562339]
	TIME [epoch: 6.76 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6369267177741104		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 0.6369267177741104 | validation: 0.7266947685061473]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_702.pth
	Model improved!!!
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6150472522824957		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 0.6150472522824957 | validation: 0.7402662338942922]
	TIME [epoch: 6.76 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6471927638301148		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 0.6471927638301148 | validation: 0.7936892908285391]
	TIME [epoch: 6.76 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6366297566622977		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 0.6366297566622977 | validation: 0.7705811981604007]
	TIME [epoch: 6.81 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6357733655070883		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 0.6357733655070883 | validation: 0.7705806069587837]
	TIME [epoch: 6.77 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6127324979771069		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 0.6127324979771069 | validation: 0.7564081140224359]
	TIME [epoch: 6.76 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6146955767401128		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 0.6146955767401128 | validation: 0.757042611183179]
	TIME [epoch: 6.77 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6164114396614089		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 0.6164114396614089 | validation: 0.7484072684545054]
	TIME [epoch: 6.76 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.60477507182078		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 0.60477507182078 | validation: 0.7437525233469564]
	TIME [epoch: 6.76 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6151172438256935		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 0.6151172438256935 | validation: 0.781599630424207]
	TIME [epoch: 6.77 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6316310566656875		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 0.6316310566656875 | validation: 0.7694659744834457]
	TIME [epoch: 6.82 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6273152427870774		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 0.6273152427870774 | validation: 0.7789141059971723]
	TIME [epoch: 6.8 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6304533880935518		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 0.6304533880935518 | validation: 0.7658066619302534]
	TIME [epoch: 6.78 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6244916654224015		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 0.6244916654224015 | validation: 0.7696421584994739]
	TIME [epoch: 6.76 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6224417861437841		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 0.6224417861437841 | validation: 0.7453432444338264]
	TIME [epoch: 6.76 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6282236050904879		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 0.6282236050904879 | validation: 0.7477399079372063]
	TIME [epoch: 6.76 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6262317300734654		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 0.6262317300734654 | validation: 0.759818121836631]
	TIME [epoch: 6.77 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.617015467251757		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 0.617015467251757 | validation: 0.7704927599987583]
	TIME [epoch: 6.81 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6163718688115338		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.6163718688115338 | validation: 0.7697609630039873]
	TIME [epoch: 6.77 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6242476520838678		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 0.6242476520838678 | validation: 0.7418619801016908]
	TIME [epoch: 6.77 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6153154867389804		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 0.6153154867389804 | validation: 0.7561704490531399]
	TIME [epoch: 6.76 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6099220377550385		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 0.6099220377550385 | validation: 0.7211211662434848]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_723.pth
	Model improved!!!
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6119905655017277		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 0.6119905655017277 | validation: 0.7559054389049955]
	TIME [epoch: 6.76 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6204728182603132		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 0.6204728182603132 | validation: 0.7955427323858288]
	TIME [epoch: 6.78 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6345909828352034		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 0.6345909828352034 | validation: 0.7613576395785816]
	TIME [epoch: 6.79 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6211430733970096		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 0.6211430733970096 | validation: 0.7644938033849085]
	TIME [epoch: 6.76 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6192092794640166		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 0.6192092794640166 | validation: 0.7478496308259943]
	TIME [epoch: 6.77 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6215113566723198		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 0.6215113566723198 | validation: 0.7510581809049663]
	TIME [epoch: 6.76 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6081591940763145		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 0.6081591940763145 | validation: 0.7582944698060615]
	TIME [epoch: 6.78 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6135090794585674		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 0.6135090794585674 | validation: 0.7534964712477688]
	TIME [epoch: 6.79 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6104381088100327		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 0.6104381088100327 | validation: 0.764294229073426]
	TIME [epoch: 6.81 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6327329342591951		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 0.6327329342591951 | validation: 0.7460905388519948]
	TIME [epoch: 6.77 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6297333592978256		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 0.6297333592978256 | validation: 0.7782755348500107]
	TIME [epoch: 6.76 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6835562278840629		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 0.6835562278840629 | validation: 0.8071422311881159]
	TIME [epoch: 6.77 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6472040022763882		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 0.6472040022763882 | validation: 0.7634143140724508]
	TIME [epoch: 6.77 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6094292644650673		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 0.6094292644650673 | validation: 0.748932548177552]
	TIME [epoch: 6.77 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6089098347405713		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 0.6089098347405713 | validation: 0.7507437568790205]
	TIME [epoch: 6.77 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6144930865884626		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.6144930865884626 | validation: 0.760895759265398]
	TIME [epoch: 6.85 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6220646697743906		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 0.6220646697743906 | validation: 0.7587309507983526]
	TIME [epoch: 6.78 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.646266444653385		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 0.646266444653385 | validation: 0.7721271926073139]
	TIME [epoch: 6.77 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6618485347190028		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 0.6618485347190028 | validation: 0.7503594283122461]
	TIME [epoch: 6.77 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6100960717386322		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 0.6100960717386322 | validation: 0.7532381193201312]
	TIME [epoch: 6.77 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6133380520036686		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 0.6133380520036686 | validation: 0.7362404185411827]
	TIME [epoch: 6.77 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.605780812923953		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 0.605780812923953 | validation: 0.7450330921197468]
	TIME [epoch: 6.76 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6016037320857626		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 0.6016037320857626 | validation: 0.7497816650215504]
	TIME [epoch: 6.81 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6102007451013304		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 0.6102007451013304 | validation: 0.7479474024158304]
	TIME [epoch: 6.78 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6164919346417553		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 0.6164919346417553 | validation: 0.7509698334189263]
	TIME [epoch: 6.8 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6254051990089501		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 0.6254051990089501 | validation: 0.7314824778737363]
	TIME [epoch: 6.79 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.613295990348834		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 0.613295990348834 | validation: 0.7626354289107236]
	TIME [epoch: 6.78 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6211769599627134		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 0.6211769599627134 | validation: 0.7399377543242749]
	TIME [epoch: 6.76 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6136179626060149		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 0.6136179626060149 | validation: 0.74942671491247]
	TIME [epoch: 6.77 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6091415759446569		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 0.6091415759446569 | validation: 0.7417489928116358]
	TIME [epoch: 6.8 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6239705101636016		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 0.6239705101636016 | validation: 0.7531986555021879]
	TIME [epoch: 6.77 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6196753017301331		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 0.6196753017301331 | validation: 0.7456192051749355]
	TIME [epoch: 6.76 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6159582264422935		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 0.6159582264422935 | validation: 0.7489152572013432]
	TIME [epoch: 6.75 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6192064673025056		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 0.6192064673025056 | validation: 0.7651742825403098]
	TIME [epoch: 6.75 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231324497139011		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.6231324497139011 | validation: 0.7579047614082923]
	TIME [epoch: 6.75 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6372973752315462		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 0.6372973752315462 | validation: 0.7723595297986161]
	TIME [epoch: 6.76 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6202914007013522		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 0.6202914007013522 | validation: 0.7478711893255899]
	TIME [epoch: 6.8 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6055625705684193		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 0.6055625705684193 | validation: 0.7491772618567732]
	TIME [epoch: 6.77 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6063214292217083		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 0.6063214292217083 | validation: 0.7481363575632347]
	TIME [epoch: 6.76 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.605714599451151		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 0.605714599451151 | validation: 0.7446754657763299]
	TIME [epoch: 6.76 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6171030543672669		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 0.6171030543672669 | validation: 0.7277410589186571]
	TIME [epoch: 6.75 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6097296395443355		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 0.6097296395443355 | validation: 0.7550390052628244]
	TIME [epoch: 6.76 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6101969442300097		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 0.6101969442300097 | validation: 0.7465408436147926]
	TIME [epoch: 6.79 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6064457610295672		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 0.6064457610295672 | validation: 0.7403322263275913]
	TIME [epoch: 6.83 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6080355157291047		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 0.6080355157291047 | validation: 0.7511766482263127]
	TIME [epoch: 6.77 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6201458369922307		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 0.6201458369922307 | validation: 0.7404342737435133]
	TIME [epoch: 6.76 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6074121307937129		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 0.6074121307937129 | validation: 0.7374137186438343]
	TIME [epoch: 6.76 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6118509742090638		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 0.6118509742090638 | validation: 0.7378622271334527]
	TIME [epoch: 6.75 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6140968159906623		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 0.6140968159906623 | validation: 0.7535653941977111]
	TIME [epoch: 6.75 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6042607209831293		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 0.6042607209831293 | validation: 0.7176982819474648]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_773.pth
	Model improved!!!
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6080614247830164		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 0.6080614247830164 | validation: 0.7368727371116413]
	TIME [epoch: 6.79 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6125778072998379		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 0.6125778072998379 | validation: 0.7353807591091144]
	TIME [epoch: 6.75 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6054834501747295		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 0.6054834501747295 | validation: 0.7488041439161761]
	TIME [epoch: 6.76 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6092525948625301		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.6092525948625301 | validation: 0.7375677603012252]
	TIME [epoch: 6.75 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6142569263601361		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 0.6142569263601361 | validation: 0.7324929085798692]
	TIME [epoch: 6.75 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6079209631114251		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 0.6079209631114251 | validation: 0.728229960235431]
	TIME [epoch: 6.76 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6154946059639251		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 0.6154946059639251 | validation: 0.763927600267815]
	TIME [epoch: 6.8 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6349132663813863		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 0.6349132663813863 | validation: 0.7327353504942151]
	TIME [epoch: 6.77 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6053222489948357		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 0.6053222489948357 | validation: 0.7243188044769197]
	TIME [epoch: 6.75 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6064904623544947		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 0.6064904623544947 | validation: 0.7522612066037646]
	TIME [epoch: 6.76 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6020006396702442		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 0.6020006396702442 | validation: 0.7505770218047692]
	TIME [epoch: 6.78 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6124026137405195		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 0.6124026137405195 | validation: 0.747032555059185]
	TIME [epoch: 6.77 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6047863683341288		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 0.6047863683341288 | validation: 0.7574745140980779]
	TIME [epoch: 6.75 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6099191948109172		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 0.6099191948109172 | validation: 0.7548103399559317]
	TIME [epoch: 6.8 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6138260240380315		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 0.6138260240380315 | validation: 0.7237641930863529]
	TIME [epoch: 6.76 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6153369136010371		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 0.6153369136010371 | validation: 0.7502638517702797]
	TIME [epoch: 6.75 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6139958139092406		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 0.6139958139092406 | validation: 0.7286344345601834]
	TIME [epoch: 6.75 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6122726089153852		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 0.6122726089153852 | validation: 0.7263534416069961]
	TIME [epoch: 6.75 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.61285629369543		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 0.61285629369543 | validation: 0.7482921799415965]
	TIME [epoch: 6.75 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6131031921032564		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 0.6131031921032564 | validation: 0.7348162028779925]
	TIME [epoch: 6.74 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.609600017836903		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 0.609600017836903 | validation: 0.7236015580173503]
	TIME [epoch: 6.79 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6081679206655222		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 0.6081679206655222 | validation: 0.7313555553165545]
	TIME [epoch: 6.76 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5976184526608698		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.5976184526608698 | validation: 0.7417684425582232]
	TIME [epoch: 6.75 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5992682473496377		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 0.5992682473496377 | validation: 0.7344810578669334]
	TIME [epoch: 6.75 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5952734139140411		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 0.5952734139140411 | validation: 0.7267968950791035]
	TIME [epoch: 6.75 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6112637286285107		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 0.6112637286285107 | validation: 0.7436803039538342]
	TIME [epoch: 6.75 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6019295395941919		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 0.6019295395941919 | validation: 0.7223643752413298]
	TIME [epoch: 6.74 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6049466421044022		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 0.6049466421044022 | validation: 0.7362566999545732]
	TIME [epoch: 6.79 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6039179384489093		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 0.6039179384489093 | validation: 0.72875724705864]
	TIME [epoch: 6.79 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6009437067589547		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 0.6009437067589547 | validation: 0.7421087476137419]
	TIME [epoch: 6.76 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6139245397684852		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 0.6139245397684852 | validation: 0.7423071712433921]
	TIME [epoch: 6.75 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.608307402251008		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 0.608307402251008 | validation: 0.751873094305108]
	TIME [epoch: 6.75 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6101787570427543		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 0.6101787570427543 | validation: 0.7522721042869671]
	TIME [epoch: 6.74 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6084091396410314		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 0.6084091396410314 | validation: 0.724818454350612]
	TIME [epoch: 6.76 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6043491991630827		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 0.6043491991630827 | validation: 0.7320698318092164]
	TIME [epoch: 6.79 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6095036225654081		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 0.6095036225654081 | validation: 0.7315761211468498]
	TIME [epoch: 6.76 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6076892187646507		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 0.6076892187646507 | validation: 0.7253059056115216]
	TIME [epoch: 6.76 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6043300003768599		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 0.6043300003768599 | validation: 0.7294642514418055]
	TIME [epoch: 6.75 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6083545311514236		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 0.6083545311514236 | validation: 0.7287533959805983]
	TIME [epoch: 6.75 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.603410925750338		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 0.603410925750338 | validation: 0.743052584265379]
	TIME [epoch: 6.75 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6012587719044155		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 0.6012587719044155 | validation: 0.7161951259062774]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_814.pth
	Model improved!!!
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5989790388963653		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.5989790388963653 | validation: 0.7163587850053407]
	TIME [epoch: 6.8 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5978715359095874		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 0.5978715359095874 | validation: 0.732672867288521]
	TIME [epoch: 6.76 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5980012263191642		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 0.5980012263191642 | validation: 0.7354444943028577]
	TIME [epoch: 6.77 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5993364622980222		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 0.5993364622980222 | validation: 0.7283814085375485]
	TIME [epoch: 6.76 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6037868361954468		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 0.6037868361954468 | validation: 0.7173329187346063]
	TIME [epoch: 6.76 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.602303837040825		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 0.602303837040825 | validation: 0.7193609345728846]
	TIME [epoch: 6.78 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027851240038057		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 0.6027851240038057 | validation: 0.709269979432787]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_821.pth
	Model improved!!!
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5972130851639091		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 0.5972130851639091 | validation: 0.7260205970769731]
	TIME [epoch: 6.79 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6053751987844533		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 0.6053751987844533 | validation: 0.7262315865296574]
	TIME [epoch: 6.76 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6025190141366944		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 0.6025190141366944 | validation: 0.7163918990101075]
	TIME [epoch: 6.75 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6068335190353675		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 0.6068335190353675 | validation: 0.7251536878828262]
	TIME [epoch: 6.76 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.600992877946442		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 0.600992877946442 | validation: 0.7390581944173973]
	TIME [epoch: 6.75 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6013168537697273		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 0.6013168537697273 | validation: 0.6911072813523989]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v1_20240503_101337/states/model_phiq_1a_v1_827.pth
	Model improved!!!
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6029173390233189		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 0.6029173390233189 | validation: 0.7185606769483311]
	TIME [epoch: 6.79 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6114167271513549		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 0.6114167271513549 | validation: 0.7212315091128727]
	TIME [epoch: 6.76 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6030994305675885		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 0.6030994305675885 | validation: 0.7211597932978132]
	TIME [epoch: 6.75 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6044076572111358		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 0.6044076572111358 | validation: 0.71567405207502]
	TIME [epoch: 6.76 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128843967891782		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 0.6128843967891782 | validation: 0.7272844343414615]
	TIME [epoch: 6.76 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6171320830570051		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 0.6171320830570051 | validation: 0.7251904953283226]
	TIME [epoch: 6.75 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6162623031383516		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.6162623031383516 | validation: 0.7192651294995853]
	TIME [epoch: 6.76 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.598929350310759		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 0.598929350310759 | validation: 0.7363023665287053]
	TIME [epoch: 6.8 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040693343212177		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 0.6040693343212177 | validation: 0.7271203782243304]
	TIME [epoch: 6.77 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5988820253827505		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 0.5988820253827505 | validation: 0.7289952619611002]
	TIME [epoch: 6.78 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5980545926378309		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 0.5980545926378309 | validation: 0.7344858547906272]
	TIME [epoch: 6.77 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5953158510081571		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 0.5953158510081571 | validation: 0.738797231213209]
	TIME [epoch: 6.75 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6054182780521135		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 0.6054182780521135 | validation: 0.7294115661706906]
	TIME [epoch: 6.76 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6111591743539977		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 0.6111591743539977 | validation: 0.7359912238618123]
	TIME [epoch: 6.75 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5942024325517442		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 0.5942024325517442 | validation: 0.7361793442313157]
	TIME [epoch: 6.8 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5900547247915756		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 0.5900547247915756 | validation: 0.7190676729625886]
	TIME [epoch: 6.76 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.599113050035626		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 0.599113050035626 | validation: 0.7228590001866878]
	TIME [epoch: 6.76 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5998480415814929		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 0.5998480415814929 | validation: 0.7270948768948439]
	TIME [epoch: 6.75 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.605784345481782		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 0.605784345481782 | validation: 0.7332825038952027]
	TIME [epoch: 6.75 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6043457528580609		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 0.6043457528580609 | validation: 0.7054667384425315]
	TIME [epoch: 6.75 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6014960231627453		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 0.6014960231627453 | validation: 0.7280684713865264]
	TIME [epoch: 6.76 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5993909339925896		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 0.5993909339925896 | validation: 0.7380218520571697]
	TIME [epoch: 6.79 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.600333593161501		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 0.600333593161501 | validation: 0.7212541210887247]
	TIME [epoch: 6.77 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6222228212467968		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 0.6222228212467968 | validation: 0.7489653998495662]
	TIME [epoch: 6.75 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.616203816916325		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 0.616203816916325 | validation: 0.719512553457768]
	TIME [epoch: 6.74 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6076257393087399		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.6076257393087399 | validation: 0.7238501129543349]
	TIME [epoch: 6.75 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6013157987759085		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 0.6013157987759085 | validation: 0.7240573736957288]
	TIME [epoch: 6.76 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6064328732776371		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 0.6064328732776371 | validation: 0.7321818627924317]
	TIME [epoch: 6.78 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6036445814446996		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 0.6036445814446996 | validation: 0.7102997409725201]
	TIME [epoch: 6.82 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040459681162826		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 0.6040459681162826 | validation: 0.7168475597140312]
	TIME [epoch: 6.76 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077428345959939		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 0.6077428345959939 | validation: 0.7220685760467322]
	TIME [epoch: 6.76 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6123944463791792		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 0.6123944463791792 | validation: 0.7336264042384234]
	TIME [epoch: 6.75 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6095833077108066		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 0.6095833077108066 | validation: 0.730622531628069]
	TIME [epoch: 6.77 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6102756504594289		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 0.6102756504594289 | validation: 0.7429574776677135]
	TIME [epoch: 6.76 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.611555441940165		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 0.611555441940165 | validation: 0.7381935811674083]
	TIME [epoch: 6.79 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6179455228609927		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 0.6179455228609927 | validation: 0.7383046305796207]
	TIME [epoch: 6.8 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6233010722658806		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 0.6233010722658806 | validation: 0.7429645404581151]
	TIME [epoch: 6.77 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6042263161281765		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 0.6042263161281765 | validation: 0.7230614492337681]
	TIME [epoch: 6.77 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005515052807366		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 0.6005515052807366 | validation: 0.7188695717093885]
	TIME [epoch: 6.77 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5975281923556532		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 0.5975281923556532 | validation: 0.7213444705403129]
	TIME [epoch: 6.76 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5917657808511064		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 0.5917657808511064 | validation: 0.7179230991331476]
	TIME [epoch: 6.76 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6022347367540136		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 0.6022347367540136 | validation: 0.7409905663897658]
	TIME [epoch: 6.78 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5962600654873083		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 0.5962600654873083 | validation: 0.7178030344906228]
	TIME [epoch: 6.8 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.594258241788088		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 0.594258241788088 | validation: 0.7201629371106248]
	TIME [epoch: 6.76 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6010661296967604		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.6010661296967604 | validation: 0.7230545210413541]
	TIME [epoch: 6.76 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5893787533299105		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 0.5893787533299105 | validation: 0.7134484336929425]
	TIME [epoch: 6.78 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6028698076748498		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 0.6028698076748498 | validation: 0.716862868668011]
	TIME [epoch: 6.78 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6029487993851372		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 0.6029487993851372 | validation: 0.7225533059708269]
	TIME [epoch: 6.76 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5912967079117133		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 0.5912967079117133 | validation: 0.7167449033752027]
	TIME [epoch: 6.8 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5929633680530446		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 0.5929633680530446 | validation: 0.7204748006824832]
	TIME [epoch: 6.77 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.591405113801736		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 0.591405113801736 | validation: 0.7346123129373526]
	TIME [epoch: 6.76 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5910229259374894		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 0.5910229259374894 | validation: 0.7317045110475688]
	TIME [epoch: 6.75 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5957992300372191		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 0.5957992300372191 | validation: 0.7349849681700742]
	TIME [epoch: 6.76 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5990095178533412		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 0.5990095178533412 | validation: 0.730309817769427]
	TIME [epoch: 6.75 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5981373082865257		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 0.5981373082865257 | validation: 0.7304984175275762]
	TIME [epoch: 6.75 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6000966872333007		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 0.6000966872333007 | validation: 0.7285706885108854]
	TIME [epoch: 6.8 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6033111288228779		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 0.6033111288228779 | validation: 0.7260221022501177]
	TIME [epoch: 6.77 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6003060880759074		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 0.6003060880759074 | validation: 0.7115969887470972]
	TIME [epoch: 6.76 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007994073621041		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 0.6007994073621041 | validation: 0.7377859083415264]
	TIME [epoch: 6.77 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.594540214606442		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 0.594540214606442 | validation: 0.713955676337373]
	TIME [epoch: 6.76 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5991133315388399		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 0.5991133315388399 | validation: 0.7366897009595619]
	TIME [epoch: 6.75 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5919250183477704		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 0.5919250183477704 | validation: 0.7157400123673253]
	TIME [epoch: 6.76 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6052021478849127		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 0.6052021478849127 | validation: 0.7216996173403302]
	TIME [epoch: 6.8 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5953603122257918		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 0.5953603122257918 | validation: 0.719366093695244]
	TIME [epoch: 6.78 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6033390305619178		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 0.6033390305619178 | validation: 0.7229790658807989]
	TIME [epoch: 6.78 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5998091175570472		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 0.5998091175570472 | validation: 0.7138097369378011]
	TIME [epoch: 6.79 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5978794013644622		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 0.5978794013644622 | validation: 0.7110669178022071]
	TIME [epoch: 6.76 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.595536756987607		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 0.595536756987607 | validation: 0.7331013567716498]
	TIME [epoch: 6.76 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5904664294545219		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 0.5904664294545219 | validation: 0.71124773186182]
	TIME [epoch: 6.76 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6019165692794101		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 0.6019165692794101 | validation: 0.7162722795726888]
	TIME [epoch: 6.79 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5936184934171381		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 0.5936184934171381 | validation: 0.7254121738086079]
	TIME [epoch: 6.77 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6038830128496411		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 0.6038830128496411 | validation: 0.7411394720266868]
	TIME [epoch: 6.75 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6075456140095655		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 0.6075456140095655 | validation: 0.7178226527652292]
	TIME [epoch: 6.75 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6020690779623703		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 0.6020690779623703 | validation: 0.7237425066570288]
	TIME [epoch: 6.76 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6033755941631643		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 0.6033755941631643 | validation: 0.7099336075387983]
	TIME [epoch: 6.76 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5993174180128494		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 0.5993174180128494 | validation: 0.7179645741495465]
	TIME [epoch: 6.77 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5941605827716637		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 0.5941605827716637 | validation: 0.7144593709673864]
	TIME [epoch: 6.81 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5871174718140872		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 0.5871174718140872 | validation: 0.7269037636584071]
	TIME [epoch: 6.76 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5957820523744952		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 0.5957820523744952 | validation: 0.7292582776461765]
	TIME [epoch: 6.76 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5934269777518909		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 0.5934269777518909 | validation: 0.72077946540237]
	TIME [epoch: 6.76 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5930429754201841		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 0.5930429754201841 | validation: 0.73682370099412]
	TIME [epoch: 6.76 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5996794915771785		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 0.5996794915771785 | validation: 0.7142458115437689]
	TIME [epoch: 6.79 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005787273154782		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.6005787273154782 | validation: 0.7251800545943785]
	TIME [epoch: 6.81 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6037877647331455		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 0.6037877647331455 | validation: 0.742247673817759]
	TIME [epoch: 6.8 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5997527408628337		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 0.5997527408628337 | validation: 0.7263196736808668]
	TIME [epoch: 6.77 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005502165337332		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 0.6005502165337332 | validation: 0.717586143292686]
	TIME [epoch: 6.77 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6052172281922452		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 0.6052172281922452 | validation: 0.7399903894810953]
	TIME [epoch: 6.76 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5981476866965443		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 0.5981476866965443 | validation: 0.7263713222348774]
	TIME [epoch: 6.76 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5935908916361693		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 0.5935908916361693 | validation: 0.7400807891643517]
	TIME [epoch: 6.77 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5920888194593096		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 0.5920888194593096 | validation: 0.7134946756410292]
	TIME [epoch: 6.79 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.601346327061233		[learning rate: 1.8186e-05]
	Learning Rate: 1.8186e-05
	LOSS [training: 0.601346327061233 | validation: 0.7091113383586536]
	TIME [epoch: 6.8 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6015378618066846		[learning rate: 1.8054e-05]
	Learning Rate: 1.80542e-05
	LOSS [training: 0.6015378618066846 | validation: 0.7110271832852211]
	TIME [epoch: 6.76 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5961904893924166		[learning rate: 1.7923e-05]
	Learning Rate: 1.79234e-05
	LOSS [training: 0.5961904893924166 | validation: 0.7116989233298411]
	TIME [epoch: 6.77 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5980157874948502		[learning rate: 1.7794e-05]
	Learning Rate: 1.77936e-05
	LOSS [training: 0.5980157874948502 | validation: 0.7153835477747816]
	TIME [epoch: 6.76 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5974312526140761		[learning rate: 1.7665e-05]
	Learning Rate: 1.76647e-05
	LOSS [training: 0.5974312526140761 | validation: 0.7373622028774975]
	TIME [epoch: 6.76 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5981879403049515		[learning rate: 1.7537e-05]
	Learning Rate: 1.75367e-05
	LOSS [training: 0.5981879403049515 | validation: 0.7460077620761312]
	TIME [epoch: 6.75 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5949707741331167		[learning rate: 1.741e-05]
	Learning Rate: 1.74096e-05
	LOSS [training: 0.5949707741331167 | validation: 0.7238053532582539]
	TIME [epoch: 6.79 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6038810272357356		[learning rate: 1.7284e-05]
	Learning Rate: 1.72835e-05
	LOSS [training: 0.6038810272357356 | validation: 0.7238014076572765]
	TIME [epoch: 6.78 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6120508974505502		[learning rate: 1.7158e-05]
	Learning Rate: 1.71583e-05
	LOSS [training: 0.6120508974505502 | validation: 0.7249148609182873]
	TIME [epoch: 6.75 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007630966474676		[learning rate: 1.7034e-05]
	Learning Rate: 1.7034e-05
	LOSS [training: 0.6007630966474676 | validation: 0.737086766035598]
	TIME [epoch: 6.77 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.601250360421046		[learning rate: 1.6911e-05]
	Learning Rate: 1.69106e-05
	LOSS [training: 0.601250360421046 | validation: 0.7243016700807705]
	TIME [epoch: 6.78 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6050708610574249		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 0.6050708610574249 | validation: 0.7225990108101146]
	TIME [epoch: 6.76 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5969197355913576		[learning rate: 1.6666e-05]
	Learning Rate: 1.66664e-05
	LOSS [training: 0.5969197355913576 | validation: 0.7212888623466291]
	TIME [epoch: 6.75 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6059791480817179		[learning rate: 1.6546e-05]
	Learning Rate: 1.65457e-05
	LOSS [training: 0.6059791480817179 | validation: 0.7218048268064303]
	TIME [epoch: 6.8 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007597226391158		[learning rate: 1.6426e-05]
	Learning Rate: 1.64258e-05
	LOSS [training: 0.6007597226391158 | validation: 0.7486384498872196]
	TIME [epoch: 6.77 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6022014362880809		[learning rate: 1.6307e-05]
	Learning Rate: 1.63068e-05
	LOSS [training: 0.6022014362880809 | validation: 0.7224613201902921]
	TIME [epoch: 6.76 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5955347711452998		[learning rate: 1.6189e-05]
	Learning Rate: 1.61887e-05
	LOSS [training: 0.5955347711452998 | validation: 0.728561339310533]
	TIME [epoch: 6.76 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5959690702853575		[learning rate: 1.6071e-05]
	Learning Rate: 1.60714e-05
	LOSS [training: 0.5959690702853575 | validation: 0.7261086524902618]
	TIME [epoch: 6.76 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5993526436828674		[learning rate: 1.5955e-05]
	Learning Rate: 1.59549e-05
	LOSS [training: 0.5993526436828674 | validation: 0.72800667813933]
	TIME [epoch: 6.76 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.601636071670918		[learning rate: 1.5839e-05]
	Learning Rate: 1.58393e-05
	LOSS [training: 0.601636071670918 | validation: 0.7259907891024685]
	TIME [epoch: 6.77 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6003926025405635		[learning rate: 1.5725e-05]
	Learning Rate: 1.57246e-05
	LOSS [training: 0.6003926025405635 | validation: 0.7321601881935084]
	TIME [epoch: 6.81 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6066449599988057		[learning rate: 1.5611e-05]
	Learning Rate: 1.56107e-05
	LOSS [training: 0.6066449599988057 | validation: 0.7141505647609939]
	TIME [epoch: 6.77 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077713163330264		[learning rate: 1.5498e-05]
	Learning Rate: 1.54976e-05
	LOSS [training: 0.6077713163330264 | validation: 0.7119903984802876]
	TIME [epoch: 6.76 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5963637340383043		[learning rate: 1.5385e-05]
	Learning Rate: 1.53853e-05
	LOSS [training: 0.5963637340383043 | validation: 0.7205091013130649]
	TIME [epoch: 6.76 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5951433596597542		[learning rate: 1.5274e-05]
	Learning Rate: 1.52738e-05
	LOSS [training: 0.5951433596597542 | validation: 0.7242527715234854]
	TIME [epoch: 6.76 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034696428434757		[learning rate: 1.5163e-05]
	Learning Rate: 1.51632e-05
	LOSS [training: 0.6034696428434757 | validation: 0.7299435340307019]
	TIME [epoch: 6.76 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5931728354840506		[learning rate: 1.5053e-05]
	Learning Rate: 1.50533e-05
	LOSS [training: 0.5931728354840506 | validation: 0.7196611177936448]
	TIME [epoch: 6.76 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5980305076128426		[learning rate: 1.4944e-05]
	Learning Rate: 1.49442e-05
	LOSS [training: 0.5980305076128426 | validation: 0.7214451544486816]
	TIME [epoch: 6.82 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018805001001251		[learning rate: 1.4836e-05]
	Learning Rate: 1.4836e-05
	LOSS [training: 0.6018805001001251 | validation: 0.7141761009895087]
	TIME [epoch: 6.79 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5949519877520618		[learning rate: 1.4728e-05]
	Learning Rate: 1.47285e-05
	LOSS [training: 0.5949519877520618 | validation: 0.7120558706011675]
	TIME [epoch: 6.76 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5957728779190314		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 0.5957728779190314 | validation: 0.711538975289911]
	TIME [epoch: 6.75 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040814471037543		[learning rate: 1.4516e-05]
	Learning Rate: 1.45158e-05
	LOSS [training: 0.6040814471037543 | validation: 0.7290189888954391]
	TIME [epoch: 6.75 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6056553665258402		[learning rate: 1.4411e-05]
	Learning Rate: 1.44107e-05
	LOSS [training: 0.6056553665258402 | validation: 0.7093015379068603]
	TIME [epoch: 6.76 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5922393530395145		[learning rate: 1.4306e-05]
	Learning Rate: 1.43063e-05
	LOSS [training: 0.5922393530395145 | validation: 0.7266748201554474]
	TIME [epoch: 6.76 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5948670053422576		[learning rate: 1.4203e-05]
	Learning Rate: 1.42026e-05
	LOSS [training: 0.5948670053422576 | validation: 0.7210516545972515]
	TIME [epoch: 6.8 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5968459107781092		[learning rate: 1.41e-05]
	Learning Rate: 1.40997e-05
	LOSS [training: 0.5968459107781092 | validation: 0.716653570194181]
	TIME [epoch: 6.76 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5976581728149661		[learning rate: 1.3998e-05]
	Learning Rate: 1.39976e-05
	LOSS [training: 0.5976581728149661 | validation: 0.7146578937181587]
	TIME [epoch: 6.76 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6006644713516892		[learning rate: 1.3896e-05]
	Learning Rate: 1.38962e-05
	LOSS [training: 0.6006644713516892 | validation: 0.7270216190671676]
	TIME [epoch: 6.76 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.599804524601886		[learning rate: 1.3795e-05]
	Learning Rate: 1.37955e-05
	LOSS [training: 0.599804524601886 | validation: 0.7198616195320122]
	TIME [epoch: 6.75 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5966403096956104		[learning rate: 1.3696e-05]
	Learning Rate: 1.36955e-05
	LOSS [training: 0.5966403096956104 | validation: 0.7155051534653104]
	TIME [epoch: 6.75 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034526223636935		[learning rate: 1.3596e-05]
	Learning Rate: 1.35963e-05
	LOSS [training: 0.6034526223636935 | validation: 0.7079755879584618]
	TIME [epoch: 6.76 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5978849033009161		[learning rate: 1.3498e-05]
	Learning Rate: 1.34978e-05
	LOSS [training: 0.5978849033009161 | validation: 0.7108453324600289]
	TIME [epoch: 6.8 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5995371759962314		[learning rate: 1.34e-05]
	Learning Rate: 1.34e-05
	LOSS [training: 0.5995371759962314 | validation: 0.7302428892036206]
	TIME [epoch: 6.76 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.600180174629382		[learning rate: 1.3303e-05]
	Learning Rate: 1.33029e-05
	LOSS [training: 0.600180174629382 | validation: 0.7259912105758772]
	TIME [epoch: 6.75 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6011533604371087		[learning rate: 1.3207e-05]
	Learning Rate: 1.32066e-05
	LOSS [training: 0.6011533604371087 | validation: 0.721536059539379]
	TIME [epoch: 6.76 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034425720193197		[learning rate: 1.3111e-05]
	Learning Rate: 1.31109e-05
	LOSS [training: 0.6034425720193197 | validation: 0.7264026866532542]
	TIME [epoch: 6.77 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5962425972897437		[learning rate: 1.3016e-05]
	Learning Rate: 1.30159e-05
	LOSS [training: 0.5962425972897437 | validation: 0.7278175849705626]
	TIME [epoch: 6.77 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.594529043184733		[learning rate: 1.2922e-05]
	Learning Rate: 1.29216e-05
	LOSS [training: 0.594529043184733 | validation: 0.7206437019150833]
	TIME [epoch: 6.78 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5987644611607267		[learning rate: 1.2828e-05]
	Learning Rate: 1.2828e-05
	LOSS [training: 0.5987644611607267 | validation: 0.7299905886973437]
	TIME [epoch: 6.79 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5983320505068138		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 0.5983320505068138 | validation: 0.7165289173456839]
	TIME [epoch: 6.76 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6003558013303306		[learning rate: 1.2643e-05]
	Learning Rate: 1.26428e-05
	LOSS [training: 0.6003558013303306 | validation: 0.716503640093789]
	TIME [epoch: 6.76 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5978890743786061		[learning rate: 1.2551e-05]
	Learning Rate: 1.25512e-05
	LOSS [training: 0.5978890743786061 | validation: 0.7234678353121543]
	TIME [epoch: 6.76 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5939124535943208		[learning rate: 1.246e-05]
	Learning Rate: 1.24602e-05
	LOSS [training: 0.5939124535943208 | validation: 0.7277055752833761]
	TIME [epoch: 6.76 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.593970832711174		[learning rate: 1.237e-05]
	Learning Rate: 1.237e-05
	LOSS [training: 0.593970832711174 | validation: 0.7278501987653153]
	TIME [epoch: 6.76 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5948636845591615		[learning rate: 1.228e-05]
	Learning Rate: 1.22803e-05
	LOSS [training: 0.5948636845591615 | validation: 0.7124057474497757]
	TIME [epoch: 6.8 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5977931054012199		[learning rate: 1.2191e-05]
	Learning Rate: 1.21914e-05
	LOSS [training: 0.5977931054012199 | validation: 0.7291591945200413]
	TIME [epoch: 6.79 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5953843484981052		[learning rate: 1.2103e-05]
	Learning Rate: 1.21031e-05
	LOSS [training: 0.5953843484981052 | validation: 0.7194914236439314]
	TIME [epoch: 6.77 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5918494276487958		[learning rate: 1.2015e-05]
	Learning Rate: 1.20154e-05
	LOSS [training: 0.5918494276487958 | validation: 0.717920793443654]
	TIME [epoch: 6.77 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5968684909925571		[learning rate: 1.1928e-05]
	Learning Rate: 1.19283e-05
	LOSS [training: 0.5968684909925571 | validation: 0.7343869621028002]
	TIME [epoch: 6.76 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6025454208162998		[learning rate: 1.1842e-05]
	Learning Rate: 1.18419e-05
	LOSS [training: 0.6025454208162998 | validation: 0.7255997752924133]
	TIME [epoch: 6.76 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5896554519868875		[learning rate: 1.1756e-05]
	Learning Rate: 1.17561e-05
	LOSS [training: 0.5896554519868875 | validation: 0.7294621450489978]
	TIME [epoch: 6.75 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007283494096713		[learning rate: 1.1671e-05]
	Learning Rate: 1.16709e-05
	LOSS [training: 0.6007283494096713 | validation: 0.7257416990717588]
	TIME [epoch: 6.81 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5971558304170279		[learning rate: 1.1586e-05]
	Learning Rate: 1.15864e-05
	LOSS [training: 0.5971558304170279 | validation: 0.7041509058068205]
	TIME [epoch: 6.78 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.593226576165905		[learning rate: 1.1502e-05]
	Learning Rate: 1.15024e-05
	LOSS [training: 0.593226576165905 | validation: 0.7299847885036339]
	TIME [epoch: 6.79 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.592661824502272		[learning rate: 1.1419e-05]
	Learning Rate: 1.14191e-05
	LOSS [training: 0.592661824502272 | validation: 0.7140513331290814]
	TIME [epoch: 6.77 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5994977264147751		[learning rate: 1.1336e-05]
	Learning Rate: 1.13364e-05
	LOSS [training: 0.5994977264147751 | validation: 0.7304432457922481]
	TIME [epoch: 6.76 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6036846498398046		[learning rate: 1.1254e-05]
	Learning Rate: 1.12542e-05
	LOSS [training: 0.6036846498398046 | validation: 0.7217146111672708]
	TIME [epoch: 6.76 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.602771035286898		[learning rate: 1.1173e-05]
	Learning Rate: 1.11727e-05
	LOSS [training: 0.602771035286898 | validation: 0.7142187458211273]
	TIME [epoch: 6.76 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6016374981513867		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 0.6016374981513867 | validation: 0.7143554696949714]
	TIME [epoch: 6.81 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5956474463979807		[learning rate: 1.1011e-05]
	Learning Rate: 1.10114e-05
	LOSS [training: 0.5956474463979807 | validation: 0.7245841339127395]
	TIME [epoch: 6.78 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5893939632618219		[learning rate: 1.0932e-05]
	Learning Rate: 1.09316e-05
	LOSS [training: 0.5893939632618219 | validation: 0.7286112394099784]
	TIME [epoch: 6.77 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5969787644462171		[learning rate: 1.0852e-05]
	Learning Rate: 1.08524e-05
	LOSS [training: 0.5969787644462171 | validation: 0.7340866632305398]
	TIME [epoch: 6.76 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5999488152141637		[learning rate: 1.0774e-05]
	Learning Rate: 1.07738e-05
	LOSS [training: 0.5999488152141637 | validation: 0.7188960572591481]
	TIME [epoch: 6.76 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6008253245565208		[learning rate: 1.0696e-05]
	Learning Rate: 1.06957e-05
	LOSS [training: 0.6008253245565208 | validation: 0.7238785520620237]
	TIME [epoch: 6.76 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5969572414503123		[learning rate: 1.0618e-05]
	Learning Rate: 1.06182e-05
	LOSS [training: 0.5969572414503123 | validation: 0.7259037698965034]
	TIME [epoch: 6.76 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6003249314981489		[learning rate: 1.0541e-05]
	Learning Rate: 1.05413e-05
	LOSS [training: 0.6003249314981489 | validation: 0.7380978905791086]
	TIME [epoch: 6.8 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5971036399210778		[learning rate: 1.0465e-05]
	Learning Rate: 1.04649e-05
	LOSS [training: 0.5971036399210778 | validation: 0.7268750902647167]
	TIME [epoch: 6.76 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5936517446033085		[learning rate: 1.0389e-05]
	Learning Rate: 1.03891e-05
	LOSS [training: 0.5936517446033085 | validation: 0.7272101150371033]
	TIME [epoch: 6.76 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5924334370440096		[learning rate: 1.0314e-05]
	Learning Rate: 1.03139e-05
	LOSS [training: 0.5924334370440096 | validation: 0.7299022159884327]
	TIME [epoch: 6.76 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5944198674734157		[learning rate: 1.0239e-05]
	Learning Rate: 1.02391e-05
	LOSS [training: 0.5944198674734157 | validation: 0.7375788168265066]
	TIME [epoch: 6.76 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5916349380817024		[learning rate: 1.0165e-05]
	Learning Rate: 1.0165e-05
	LOSS [training: 0.5916349380817024 | validation: 0.7156163641573547]
	TIME [epoch: 6.76 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5889404035745768		[learning rate: 1.0091e-05]
	Learning Rate: 1.00913e-05
	LOSS [training: 0.5889404035745768 | validation: 0.732940136856702]
	TIME [epoch: 6.8 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.589873268077943		[learning rate: 1.0018e-05]
	Learning Rate: 1.00182e-05
	LOSS [training: 0.589873268077943 | validation: 0.7212843311933737]
	TIME [epoch: 6.82 sec]
Finished training in 6932.951 seconds.
