Args:
Namespace(name='model_algphi1_1a_v_klv2', outdir='out/model_training/model_algphi1_1a_v_klv2', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='binary_choice', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='klv2', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1604407552

Training model...

Saving initial model state to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 10.564820612083881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.564820612083881 | validation: 10.713587076643229]
	TIME [epoch: 94.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 10.683979670894482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.683979670894482 | validation: 10.722569260249914]
	TIME [epoch: 4.36 sec]
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 10.67864121507803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.67864121507803 | validation: 10.711926798214066]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 10.679636435259148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.679636435259148 | validation: 10.725870247591345]
	TIME [epoch: 4.3 sec]
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 10.674290538406627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.674290538406627 | validation: 10.720052775569417]
	TIME [epoch: 4.34 sec]
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 10.65862351904591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.65862351904591 | validation: 10.781151782889852]
	TIME [epoch: 4.32 sec]
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 10.716213313663594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.716213313663594 | validation: 10.803250745230212]
	TIME [epoch: 4.3 sec]
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 10.733212533210564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.733212533210564 | validation: 10.771377254915443]
	TIME [epoch: 4.31 sec]
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 10.725745182257562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.725745182257562 | validation: 10.764904359696027]
	TIME [epoch: 4.3 sec]
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 10.715847285134101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.715847285134101 | validation: 10.776221073809591]
	TIME [epoch: 4.31 sec]
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 10.687489672445459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.687489672445459 | validation: 10.699691461991266]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 10.584148595401098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.584148595401098 | validation: 10.650876232802021]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 10.587116261316275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.587116261316275 | validation: 10.586798853942739]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 10.517721210601643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.517721210601643 | validation: 10.5715591839335]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 10.524440606515402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.524440606515402 | validation: 10.537212549312082]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 10.528183692468458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.528183692468458 | validation: 10.605353981119642]
	TIME [epoch: 4.34 sec]
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 10.523264607413292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.523264607413292 | validation: 10.59412321784463]
	TIME [epoch: 4.32 sec]
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 10.587006814151145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.587006814151145 | validation: 10.641372352031844]
	TIME [epoch: 4.31 sec]
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 10.632167990624477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.632167990624477 | validation: 10.73567555826453]
	TIME [epoch: 4.3 sec]
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 10.674346751361751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.674346751361751 | validation: 10.674425988808096]
	TIME [epoch: 4.31 sec]
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 10.672439312811466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.672439312811466 | validation: 10.718462657385246]
	TIME [epoch: 4.3 sec]
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 10.659564520057712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.659564520057712 | validation: 10.688261035398575]
	TIME [epoch: 4.31 sec]
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 10.628635332770529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.628635332770529 | validation: 10.732061567575602]
	TIME [epoch: 4.3 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 10.667379192151206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.667379192151206 | validation: 10.774272350795545]
	TIME [epoch: 4.3 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 10.680199527558756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.680199527558756 | validation: 10.760756828166354]
	TIME [epoch: 4.3 sec]
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 10.652931488557789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.652931488557789 | validation: 10.737563133102835]
	TIME [epoch: 4.31 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 10.633955923823704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.633955923823704 | validation: 10.723604964753669]
	TIME [epoch: 4.34 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 10.584503576860394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.584503576860394 | validation: 10.589082776867503]
	TIME [epoch: 4.31 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 10.531416877334369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.531416877334369 | validation: 10.635349230089075]
	TIME [epoch: 4.3 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 10.538837707835187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.538837707835187 | validation: 10.576281228842777]
	TIME [epoch: 4.3 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 10.506624106621087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.506624106621087 | validation: 10.562298505904995]
	TIME [epoch: 4.3 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 10.47369266641348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.47369266641348 | validation: 10.541677330978146]
	TIME [epoch: 4.3 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 10.489700767487234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.489700767487234 | validation: 10.5356551786351]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 10.57286610905706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.57286610905706 | validation: 10.563077346051887]
	TIME [epoch: 4.29 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 10.629572229260678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.629572229260678 | validation: 10.611053483837267]
	TIME [epoch: 4.28 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 10.641748132934373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.641748132934373 | validation: 10.634383099455995]
	TIME [epoch: 4.3 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 10.63474746880084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.63474746880084 | validation: 10.630405649721316]
	TIME [epoch: 4.32 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 10.658149049879984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.658149049879984 | validation: 10.624599707899566]
	TIME [epoch: 4.29 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 10.633159619266904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.633159619266904 | validation: 10.589829552445853]
	TIME [epoch: 4.29 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 10.625522752328875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.625522752328875 | validation: 10.605079627172227]
	TIME [epoch: 4.29 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 10.634984653786688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.634984653786688 | validation: 10.616169617595029]
	TIME [epoch: 4.28 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 10.650700383957837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.650700383957837 | validation: 10.600946477186149]
	TIME [epoch: 4.28 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 10.626069084949268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.626069084949268 | validation: 10.577992271774487]
	TIME [epoch: 4.29 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 10.618711607691688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.618711607691688 | validation: 10.562731779047095]
	TIME [epoch: 4.29 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 10.618543991859246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.618543991859246 | validation: 10.561301908437574]
	TIME [epoch: 4.29 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 10.624111973720991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.624111973720991 | validation: 10.582175933297673]
	TIME [epoch: 4.3 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 10.662234007965674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.662234007965674 | validation: 10.59461678334624]
	TIME [epoch: 4.31 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 10.651615766528263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.651615766528263 | validation: 10.602138978008112]
	TIME [epoch: 4.29 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 10.671583319104933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.671583319104933 | validation: 10.603982497778782]
	TIME [epoch: 4.29 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 10.687275338100841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.687275338100841 | validation: 10.63445976986159]
	TIME [epoch: 4.29 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 10.675517084815844		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 10.675517084815844 | validation: 10.615689348157574]
	TIME [epoch: 4.28 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 10.676756652615952		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 10.676756652615952 | validation: 10.599279206836247]
	TIME [epoch: 4.28 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 10.665123888062046		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 10.665123888062046 | validation: 10.58890423602709]
	TIME [epoch: 4.28 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 10.646924580995815		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 10.646924580995815 | validation: 10.564575480687774]
	TIME [epoch: 4.28 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 10.632479825280122		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 10.632479825280122 | validation: 10.552917911956241]
	TIME [epoch: 4.28 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 10.622507140675038		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 10.622507140675038 | validation: 10.57381525857696]
	TIME [epoch: 4.3 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 10.637463569507915		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 10.637463569507915 | validation: 10.562491579165133]
	TIME [epoch: 4.32 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 10.624617780016349		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 10.624617780016349 | validation: 10.537444947456839]
	TIME [epoch: 4.29 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 10.619222453410625		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 10.619222453410625 | validation: 10.531541895523851]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_59.pth
	Model improved!!!
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 10.605217153739002		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 10.605217153739002 | validation: 10.506812429284361]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 10.589813544379378		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 10.589813544379378 | validation: 10.515656532887864]
	TIME [epoch: 4.29 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 10.569863460886621		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 10.569863460886621 | validation: 10.49505021872065]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_62.pth
	Model improved!!!
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 10.564160747230176		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 10.564160747230176 | validation: 10.499277616518459]
	TIME [epoch: 4.29 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 10.55236105312015		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 10.55236105312015 | validation: 10.46702133217947]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_64.pth
	Model improved!!!
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 10.55233175909143		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 10.55233175909143 | validation: 10.47362474917146]
	TIME [epoch: 4.29 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 10.573628200194513		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 10.573628200194513 | validation: 10.508367173581401]
	TIME [epoch: 4.31 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 10.582877807363916		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 10.582877807363916 | validation: 10.518313921637562]
	TIME [epoch: 4.3 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 10.59238352556816		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 10.59238352556816 | validation: 10.489748548979316]
	TIME [epoch: 4.29 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 10.564639842720227		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 10.564639842720227 | validation: 10.48157081838428]
	TIME [epoch: 4.29 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 10.561352453881778		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 10.561352453881778 | validation: 10.46353355839296]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_70.pth
	Model improved!!!
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 10.54578857258845		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 10.54578857258845 | validation: 10.45577371630558]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 10.529389435591996		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 10.529389435591996 | validation: 10.448098577996648]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 10.530516223417676		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 10.530516223417676 | validation: 10.45291099082587]
	TIME [epoch: 4.3 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 10.52049008756897		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 10.52049008756897 | validation: 10.412977901834788]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_74.pth
	Model improved!!!
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 10.504633964704968		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 10.504633964704968 | validation: 10.41672252034544]
	TIME [epoch: 4.31 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 10.500950000695525		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 10.500950000695525 | validation: 10.432054397943375]
	TIME [epoch: 4.32 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 10.514341753477531		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 10.514341753477531 | validation: 10.403354304248321]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_77.pth
	Model improved!!!
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 10.505932773255008		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 10.505932773255008 | validation: 10.408167118660955]
	TIME [epoch: 4.3 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 10.488718337227311		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 10.488718337227311 | validation: 10.413022085313516]
	TIME [epoch: 4.29 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 10.496174490579584		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 10.496174490579584 | validation: 10.378633012585665]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_80.pth
	Model improved!!!
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 10.46145861035977		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 10.46145861035977 | validation: 10.386271994740348]
	TIME [epoch: 4.29 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 10.469620951999143		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 10.469620951999143 | validation: 10.400283872954752]
	TIME [epoch: 4.28 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 10.489274356218028		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 10.489274356218028 | validation: 10.364717340819414]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_83.pth
	Model improved!!!
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 10.455435506480484		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 10.455435506480484 | validation: 10.359982468933524]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_84.pth
	Model improved!!!
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 10.43796130231593		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 10.43796130231593 | validation: 10.370853608300173]
	TIME [epoch: 4.28 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 10.44776543663107		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 10.44776543663107 | validation: 10.353804197847525]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_86.pth
	Model improved!!!
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 10.448865396807733		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 10.448865396807733 | validation: 10.35966215083003]
	TIME [epoch: 4.31 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 10.448782139264491		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 10.448782139264491 | validation: 10.364555708977822]
	TIME [epoch: 4.28 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 10.43451480183301		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 10.43451480183301 | validation: 10.326176778598649]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_89.pth
	Model improved!!!
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 10.413183891377727		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 10.413183891377727 | validation: 10.273438010678685]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_90.pth
	Model improved!!!
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 10.366697188270747		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 10.366697188270747 | validation: 10.256808568501476]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_91.pth
	Model improved!!!
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 10.34655356663417		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 10.34655356663417 | validation: 10.269853227749154]
	TIME [epoch: 4.29 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 10.326333878558003		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 10.326333878558003 | validation: 10.23734388421898]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_93.pth
	Model improved!!!
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 10.327712822041013		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 10.327712822041013 | validation: 10.222875453859562]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_94.pth
	Model improved!!!
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 10.295334207815309		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 10.295334207815309 | validation: 10.205759573771845]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_95.pth
	Model improved!!!
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 10.285362173097754		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 10.285362173097754 | validation: 10.178175740233918]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_96.pth
	Model improved!!!
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 10.294518877364176		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 10.294518877364176 | validation: 10.20580597831028]
	TIME [epoch: 4.29 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 10.279582401642886		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 10.279582401642886 | validation: 10.19521937302629]
	TIME [epoch: 4.28 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 10.28405906333721		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 10.28405906333721 | validation: 10.213400689017275]
	TIME [epoch: 4.28 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 10.307306415312365		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 10.307306415312365 | validation: 10.186140226821013]
	TIME [epoch: 4.28 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 10.277755754669743		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 10.277755754669743 | validation: 10.20489344707085]
	TIME [epoch: 4.28 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 10.280615867877453		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 10.280615867877453 | validation: 10.197239519022647]
	TIME [epoch: 4.28 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 10.287848687031802		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 10.287848687031802 | validation: 10.210584517916699]
	TIME [epoch: 4.28 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 10.303528159429414		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 10.303528159429414 | validation: 10.214886262975027]
	TIME [epoch: 4.3 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 10.3331336353582		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 10.3331336353582 | validation: 10.230152360888331]
	TIME [epoch: 4.29 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 10.29884666528305		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 10.29884666528305 | validation: 10.194295173021207]
	TIME [epoch: 4.31 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 10.29960993483909		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 10.29960993483909 | validation: 10.198135066149272]
	TIME [epoch: 4.28 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 10.290745980592218		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 10.290745980592218 | validation: 10.19478805259427]
	TIME [epoch: 4.28 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 10.306451329081494		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 10.306451329081494 | validation: 10.231020577830533]
	TIME [epoch: 4.28 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 10.318585306792578		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 10.318585306792578 | validation: 10.226037019381318]
	TIME [epoch: 4.28 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 10.306765268519175		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 10.306765268519175 | validation: 10.223659007497048]
	TIME [epoch: 4.28 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 10.321545577659041		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 10.321545577659041 | validation: 10.250501245448614]
	TIME [epoch: 4.28 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 10.320181989247686		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 10.320181989247686 | validation: 10.240388613396192]
	TIME [epoch: 4.28 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 10.332157535185223		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 10.332157535185223 | validation: 10.230902407657254]
	TIME [epoch: 4.29 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 10.308806374931688		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 10.308806374931688 | validation: 10.242940370290118]
	TIME [epoch: 4.29 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 10.336841971060807		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 10.336841971060807 | validation: 10.253622166949603]
	TIME [epoch: 4.32 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 10.315939231758572		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 10.315939231758572 | validation: 10.233457826660747]
	TIME [epoch: 4.28 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 10.297724539648708		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 10.297724539648708 | validation: 10.21013845076912]
	TIME [epoch: 4.28 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 10.310403493607573		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 10.310403493607573 | validation: 10.214506599801148]
	TIME [epoch: 4.28 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 10.281910706664227		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 10.281910706664227 | validation: 10.198849952843599]
	TIME [epoch: 4.27 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 10.295411856220502		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 10.295411856220502 | validation: 10.20986182169985]
	TIME [epoch: 4.28 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 10.299431175135435		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 10.299431175135435 | validation: 10.218248633856938]
	TIME [epoch: 4.27 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 10.304755240141716		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 10.304755240141716 | validation: 10.206768338369127]
	TIME [epoch: 4.27 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 10.29935323712742		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 10.29935323712742 | validation: 10.188194365172368]
	TIME [epoch: 4.28 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 10.308093844287294		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 10.308093844287294 | validation: 10.205558681422048]
	TIME [epoch: 4.29 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 10.32023659014765		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 10.32023659014765 | validation: 10.255999597651677]
	TIME [epoch: 4.31 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 10.327638945844368		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 10.327638945844368 | validation: 10.241630967776233]
	TIME [epoch: 4.28 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 10.307435363663153		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 10.307435363663153 | validation: 10.238988205315461]
	TIME [epoch: 4.28 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 10.33040163192834		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 10.33040163192834 | validation: 10.225426307427439]
	TIME [epoch: 4.28 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 10.321102986393624		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 10.321102986393624 | validation: 10.220285198120088]
	TIME [epoch: 4.28 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 10.319581180772554		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 10.319581180772554 | validation: 10.254163131798903]
	TIME [epoch: 4.27 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 10.332384206174602		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 10.332384206174602 | validation: 10.248468560407684]
	TIME [epoch: 4.28 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 10.332040607145757		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 10.332040607145757 | validation: 10.23670725104962]
	TIME [epoch: 4.27 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 10.331114030300679		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 10.331114030300679 | validation: 10.27268513065561]
	TIME [epoch: 4.28 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 10.347524364074221		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 10.347524364074221 | validation: 10.244543910214485]
	TIME [epoch: 4.28 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 10.328073311214977		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 10.328073311214977 | validation: 10.225796620346513]
	TIME [epoch: 4.31 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 10.320510789668422		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 10.320510789668422 | validation: 10.218271825635163]
	TIME [epoch: 4.28 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 10.311915803780336		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 10.311915803780336 | validation: 10.213763850629437]
	TIME [epoch: 4.28 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 10.310399445262604		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 10.310399445262604 | validation: 10.229775822949914]
	TIME [epoch: 4.28 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 10.292120888289151		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 10.292120888289151 | validation: 10.225594417960647]
	TIME [epoch: 4.28 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 10.324782268006839		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 10.324782268006839 | validation: 10.22147687674757]
	TIME [epoch: 4.28 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 10.312940193452562		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 10.312940193452562 | validation: 10.253236649283952]
	TIME [epoch: 4.27 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 10.342980547293225		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 10.342980547293225 | validation: 10.267083937485014]
	TIME [epoch: 4.28 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 10.354173065795557		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 10.354173065795557 | validation: 10.258888022777352]
	TIME [epoch: 4.27 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 10.32792278749039		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 10.32792278749039 | validation: 10.228988049154815]
	TIME [epoch: 4.28 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 10.352539447865317		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 10.352539447865317 | validation: 10.277706262026438]
	TIME [epoch: 4.31 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 10.34704897455472		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 10.34704897455472 | validation: 10.260817581889484]
	TIME [epoch: 4.28 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 10.343109917863618		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 10.343109917863618 | validation: 10.26642304925452]
	TIME [epoch: 4.28 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 10.343361075449724		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 10.343361075449724 | validation: 10.253219394212522]
	TIME [epoch: 4.28 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 10.350250959240984		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 10.350250959240984 | validation: 10.230636498682196]
	TIME [epoch: 4.28 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 10.335955334369935		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 10.335955334369935 | validation: 10.279518394250065]
	TIME [epoch: 4.28 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 10.349021645416737		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 10.349021645416737 | validation: 10.251179020874712]
	TIME [epoch: 4.28 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 10.346094231778073		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 10.346094231778073 | validation: 10.257481632457964]
	TIME [epoch: 4.28 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 10.332561603396055		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 10.332561603396055 | validation: 10.263552904939083]
	TIME [epoch: 4.28 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 10.341850428101404		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 10.341850428101404 | validation: 10.265631428537846]
	TIME [epoch: 4.29 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 10.332890839806844		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 10.332890839806844 | validation: 10.260028691752384]
	TIME [epoch: 4.32 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 10.327118823104126		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 10.327118823104126 | validation: 10.229989690947761]
	TIME [epoch: 4.29 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 10.31567476029056		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 10.31567476029056 | validation: 10.207909385953919]
	TIME [epoch: 4.28 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 10.306058505335933		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 10.306058505335933 | validation: 10.211062810366627]
	TIME [epoch: 4.28 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 10.308894529765265		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 10.308894529765265 | validation: 10.190331876714513]
	TIME [epoch: 4.28 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 10.311477978759852		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 10.311477978759852 | validation: 10.212850530022138]
	TIME [epoch: 4.28 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 10.303666934567051		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 10.303666934567051 | validation: 10.214442661884348]
	TIME [epoch: 4.28 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 10.303627043625447		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 10.303627043625447 | validation: 10.180539514686215]
	TIME [epoch: 4.28 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 10.282446047700228		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 10.282446047700228 | validation: 10.195724615573134]
	TIME [epoch: 4.28 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 10.30861759778918		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 10.30861759778918 | validation: 10.217084245217237]
	TIME [epoch: 4.29 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 10.296485259816066		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 10.296485259816066 | validation: 10.2380879977594]
	TIME [epoch: 4.32 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 10.307153470791961		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 10.307153470791961 | validation: 10.20996672602465]
	TIME [epoch: 4.29 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 10.314775461665073		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 10.314775461665073 | validation: 10.247432392991424]
	TIME [epoch: 4.29 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 10.294816153149387		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 10.294816153149387 | validation: 10.213587818869318]
	TIME [epoch: 4.29 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 10.278827543570202		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 10.278827543570202 | validation: 10.195933322114346]
	TIME [epoch: 4.28 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 10.295464437115214		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 10.295464437115214 | validation: 10.190843099199409]
	TIME [epoch: 4.28 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 10.287945583487193		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 10.287945583487193 | validation: 10.186302896989432]
	TIME [epoch: 4.28 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 10.291810780916114		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 10.291810780916114 | validation: 10.207169302924918]
	TIME [epoch: 4.29 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 10.29250253710986		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 10.29250253710986 | validation: 10.217362501706798]
	TIME [epoch: 4.28 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 10.299864840962316		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 10.299864840962316 | validation: 10.217261377055802]
	TIME [epoch: 4.28 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 10.296750580691397		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 10.296750580691397 | validation: 10.194164480875871]
	TIME [epoch: 4.31 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 10.301326594847339		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 10.301326594847339 | validation: 10.210663667134881]
	TIME [epoch: 4.3 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 10.294552251487524		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 10.294552251487524 | validation: 10.216140202138863]
	TIME [epoch: 4.28 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 10.281974049295828		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 10.281974049295828 | validation: 10.197106356055986]
	TIME [epoch: 4.29 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 10.28915392349644		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 10.28915392349644 | validation: 10.20958708497611]
	TIME [epoch: 4.28 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 10.273824773711361		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 10.273824773711361 | validation: 10.183286726241953]
	TIME [epoch: 4.28 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 10.269324519340701		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 10.269324519340701 | validation: 10.16460650949254]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_182.pth
	Model improved!!!
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 10.260997007314227		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 10.260997007314227 | validation: 10.194232248664537]
	TIME [epoch: 4.29 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 10.267733922566228		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 10.267733922566228 | validation: 10.18523349710344]
	TIME [epoch: 4.3 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 10.266764705482426		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 10.266764705482426 | validation: 10.199749104614408]
	TIME [epoch: 4.29 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 10.270830147837563		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 10.270830147837563 | validation: 10.161042397088284]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_186.pth
	Model improved!!!
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 10.273865365553059		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 10.273865365553059 | validation: 10.137277899711986]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_187.pth
	Model improved!!!
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 10.269227847190004		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 10.269227847190004 | validation: 10.206799395846293]
	TIME [epoch: 4.29 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 10.275614319324442		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 10.275614319324442 | validation: 10.219163397054757]
	TIME [epoch: 4.28 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 10.256305122965546		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 10.256305122965546 | validation: 10.157421731006167]
	TIME [epoch: 4.28 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 10.264540460127751		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 10.264540460127751 | validation: 10.205254554419575]
	TIME [epoch: 4.28 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 10.263495580441884		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 10.263495580441884 | validation: 10.191709843886924]
	TIME [epoch: 4.28 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 10.260312915226741		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 10.260312915226741 | validation: 10.190381224878323]
	TIME [epoch: 4.28 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244042198694023		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 10.244042198694023 | validation: 10.175158777809958]
	TIME [epoch: 4.28 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 10.277810709539935		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 10.277810709539935 | validation: 10.196790042494348]
	TIME [epoch: 4.28 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 10.290007276773093		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 10.290007276773093 | validation: 10.199116909024855]
	TIME [epoch: 4.3 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 10.27334902194902		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 10.27334902194902 | validation: 10.187711493784535]
	TIME [epoch: 4.3 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 10.27682899691293		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 10.27682899691293 | validation: 10.214670418122292]
	TIME [epoch: 4.29 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 10.281692590251861		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 10.281692590251861 | validation: 10.190896773558265]
	TIME [epoch: 4.29 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 10.27915086601957		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 10.27915086601957 | validation: 10.205181482750238]
	TIME [epoch: 4.28 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 10.275804887730384		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 10.275804887730384 | validation: 10.186989258468449]
	TIME [epoch: 4.29 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 10.279464104434412		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 10.279464104434412 | validation: 10.216597260603418]
	TIME [epoch: 4.28 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 10.272637960729353		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 10.272637960729353 | validation: 10.217422583221495]
	TIME [epoch: 4.28 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 10.267907144418233		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 10.267907144418233 | validation: 10.187487272514158]
	TIME [epoch: 4.28 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 10.272818184962883		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 10.272818184962883 | validation: 10.169458532824258]
	TIME [epoch: 4.28 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 10.276430642784726		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 10.276430642784726 | validation: 10.14567036038681]
	TIME [epoch: 4.3 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 10.287714475393098		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 10.287714475393098 | validation: 10.198638009723233]
	TIME [epoch: 4.3 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 10.276622996747996		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 10.276622996747996 | validation: 10.180813153546703]
	TIME [epoch: 4.29 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 10.27510472488014		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 10.27510472488014 | validation: 10.196784512903022]
	TIME [epoch: 4.28 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 10.2696827305968		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 10.2696827305968 | validation: 10.20294169466954]
	TIME [epoch: 4.28 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 10.261394000557814		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 10.261394000557814 | validation: 10.176794817072565]
	TIME [epoch: 4.28 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 10.265207392539539		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 10.265207392539539 | validation: 10.201864454614352]
	TIME [epoch: 4.28 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 10.252856033735453		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 10.252856033735453 | validation: 10.152793168292765]
	TIME [epoch: 4.28 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 10.26288249997516		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 10.26288249997516 | validation: 10.189022356042102]
	TIME [epoch: 4.28 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 10.252418752260471		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 10.252418752260471 | validation: 10.186096012945946]
	TIME [epoch: 4.28 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 10.24725939966007		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 10.24725939966007 | validation: 10.176941384073631]
	TIME [epoch: 4.31 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 10.25082838779458		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 10.25082838779458 | validation: 10.161416224335284]
	TIME [epoch: 4.31 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 10.256475125404027		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 10.256475125404027 | validation: 10.161038982856262]
	TIME [epoch: 4.29 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 10.246644385742744		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 10.246644385742744 | validation: 10.186339026506282]
	TIME [epoch: 4.28 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 10.252425226242451		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 10.252425226242451 | validation: 10.16815445433247]
	TIME [epoch: 4.28 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 10.239304107425742		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 10.239304107425742 | validation: 10.164341224828409]
	TIME [epoch: 4.28 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 10.24176671747966		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 10.24176671747966 | validation: 10.161818031031412]
	TIME [epoch: 4.28 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 10.26290751902215		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 10.26290751902215 | validation: 10.143606215666566]
	TIME [epoch: 4.28 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 10.251256887861516		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 10.251256887861516 | validation: 10.181288565676105]
	TIME [epoch: 4.28 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 10.232394507288118		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 10.232394507288118 | validation: 10.1813902311834]
	TIME [epoch: 4.28 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 10.246215356371668		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 10.246215356371668 | validation: 10.140090987019338]
	TIME [epoch: 4.3 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 10.248355242778894		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 10.248355242778894 | validation: 10.121217878266838]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_227.pth
	Model improved!!!
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 10.235816831822255		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 10.235816831822255 | validation: 10.145161328671692]
	TIME [epoch: 4.29 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 10.23378954409792		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 10.23378954409792 | validation: 10.15179769131668]
	TIME [epoch: 4.29 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 10.235428310136093		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 10.235428310136093 | validation: 10.18706813051277]
	TIME [epoch: 4.27 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 10.254051472329756		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 10.254051472329756 | validation: 10.157815101703303]
	TIME [epoch: 4.27 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 10.232510793065071		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 10.232510793065071 | validation: 10.147069530015727]
	TIME [epoch: 4.27 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 10.255953705885942		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 10.255953705885942 | validation: 10.148656597465624]
	TIME [epoch: 4.27 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 10.23565034108802		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 10.23565034108802 | validation: 10.151938202801247]
	TIME [epoch: 4.28 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 10.248551099778311		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 10.248551099778311 | validation: 10.1718899821287]
	TIME [epoch: 4.28 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 10.261049570934276		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 10.261049570934276 | validation: 10.138145278988924]
	TIME [epoch: 4.3 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22718994530252		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 10.22718994530252 | validation: 10.170615503071879]
	TIME [epoch: 4.3 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244974550681862		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 10.244974550681862 | validation: 10.173922612616472]
	TIME [epoch: 4.28 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 10.256821248322273		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 10.256821248322273 | validation: 10.167432164242289]
	TIME [epoch: 4.28 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 10.256924105188915		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 10.256924105188915 | validation: 10.172408683882537]
	TIME [epoch: 4.28 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 10.252647899497351		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 10.252647899497351 | validation: 10.18088836757301]
	TIME [epoch: 4.28 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 10.233170479577876		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 10.233170479577876 | validation: 10.16639246937684]
	TIME [epoch: 4.28 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244057002312966		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 10.244057002312966 | validation: 10.154337766821289]
	TIME [epoch: 4.28 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 10.264254309721792		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 10.264254309721792 | validation: 10.161526057466094]
	TIME [epoch: 4.27 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 10.247969698256615		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 10.247969698256615 | validation: 10.17033564402122]
	TIME [epoch: 4.28 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 10.24277028448164		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 10.24277028448164 | validation: 10.164193641516832]
	TIME [epoch: 4.3 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 10.242417240577861		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 10.242417240577861 | validation: 10.161147174274484]
	TIME [epoch: 4.3 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 10.23987736627467		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 10.23987736627467 | validation: 10.177648231663802]
	TIME [epoch: 4.28 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 10.242383506173768		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 10.242383506173768 | validation: 10.136573961207567]
	TIME [epoch: 4.28 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 10.232848130244461		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 10.232848130244461 | validation: 10.15486722390083]
	TIME [epoch: 4.27 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244824872881285		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 10.244824872881285 | validation: 10.127648609940174]
	TIME [epoch: 4.27 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 10.248173977776107		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 10.248173977776107 | validation: 10.16231307303957]
	TIME [epoch: 4.28 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 10.252344883247915		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 10.252344883247915 | validation: 10.15981739915878]
	TIME [epoch: 4.28 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 10.228604002569092		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 10.228604002569092 | validation: 10.166227828417341]
	TIME [epoch: 4.27 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244705909969634		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 10.244705909969634 | validation: 10.1590831805219]
	TIME [epoch: 4.28 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 10.238833056040644		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 10.238833056040644 | validation: 10.15122443646182]
	TIME [epoch: 4.3 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22859434951816		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 10.22859434951816 | validation: 10.164024226795345]
	TIME [epoch: 4.29 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 10.229902791785776		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 10.229902791785776 | validation: 10.168719783949658]
	TIME [epoch: 4.29 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 10.232488006094172		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 10.232488006094172 | validation: 10.123257128167301]
	TIME [epoch: 4.28 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 10.229672470418897		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 10.229672470418897 | validation: 10.160456103745936]
	TIME [epoch: 4.28 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 10.248125148234148		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 10.248125148234148 | validation: 10.14276386871123]
	TIME [epoch: 4.28 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 10.240005647369085		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 10.240005647369085 | validation: 10.173017607812774]
	TIME [epoch: 4.27 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244473758841812		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 10.244473758841812 | validation: 10.175523927006317]
	TIME [epoch: 4.27 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 10.239464563655668		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 10.239464563655668 | validation: 10.143944194008196]
	TIME [epoch: 4.27 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 10.238548771536454		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 10.238548771536454 | validation: 10.166345785724298]
	TIME [epoch: 4.28 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 10.249420896228218		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 10.249420896228218 | validation: 10.1651714391419]
	TIME [epoch: 4.3 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 10.228969859237532		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 10.228969859237532 | validation: 10.152515183520869]
	TIME [epoch: 4.3 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 10.223119347233673		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 10.223119347233673 | validation: 10.15824955412231]
	TIME [epoch: 4.28 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 10.235459961745061		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 10.235459961745061 | validation: 10.136709069558554]
	TIME [epoch: 4.28 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244983658933762		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 10.244983658933762 | validation: 10.16038098292407]
	TIME [epoch: 4.27 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 10.236677428296826		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 10.236677428296826 | validation: 10.129097241802196]
	TIME [epoch: 4.28 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 10.251925510826066		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 10.251925510826066 | validation: 10.187352894616525]
	TIME [epoch: 4.27 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 10.254086644328204		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 10.254086644328204 | validation: 10.15050009691997]
	TIME [epoch: 4.27 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 10.230684324292852		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 10.230684324292852 | validation: 10.158923377398963]
	TIME [epoch: 4.27 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 10.238138498421648		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 10.238138498421648 | validation: 10.135710022257648]
	TIME [epoch: 4.27 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211643205320675		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 10.211643205320675 | validation: 10.145452885044993]
	TIME [epoch: 4.3 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 10.228029316918818		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 10.228029316918818 | validation: 10.142744036127178]
	TIME [epoch: 4.3 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213280346162279		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 10.213280346162279 | validation: 10.172817897736355]
	TIME [epoch: 4.28 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21938801745442		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 10.21938801745442 | validation: 10.136362517037778]
	TIME [epoch: 4.28 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 10.229419523902799		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 10.229419523902799 | validation: 10.173044394402813]
	TIME [epoch: 4.28 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22631702408724		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 10.22631702408724 | validation: 10.168951745795518]
	TIME [epoch: 4.27 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 10.249855998926408		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 10.249855998926408 | validation: 10.134243450498744]
	TIME [epoch: 4.28 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22309940118579		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 10.22309940118579 | validation: 10.131700092366113]
	TIME [epoch: 4.28 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 10.241958772296611		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 10.241958772296611 | validation: 10.171137615765243]
	TIME [epoch: 4.28 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 10.231122196148856		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 10.231122196148856 | validation: 10.117915368140274]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_285.pth
	Model improved!!!
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210831564154113		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 10.210831564154113 | validation: 10.12321844943104]
	TIME [epoch: 4.31 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 10.232100019912892		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 10.232100019912892 | validation: 10.126113762034844]
	TIME [epoch: 4.3 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219931195683655		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 10.219931195683655 | validation: 10.166844808008014]
	TIME [epoch: 4.28 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 10.223892142728683		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 10.223892142728683 | validation: 10.158775340928457]
	TIME [epoch: 4.28 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22746645499554		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 10.22746645499554 | validation: 10.162932091129562]
	TIME [epoch: 4.28 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219568617386546		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 10.219568617386546 | validation: 10.142958847297578]
	TIME [epoch: 4.28 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 10.228244420995134		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 10.228244420995134 | validation: 10.076905812375589]
	TIME [epoch: 4.27 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_292.pth
	Model improved!!!
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 10.2065449634929		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 10.2065449634929 | validation: 10.136709676601757]
	TIME [epoch: 4.28 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 10.202161784788386		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 10.202161784788386 | validation: 10.192038636416376]
	TIME [epoch: 4.27 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 10.223390434216615		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 10.223390434216615 | validation: 10.143452082504222]
	TIME [epoch: 4.28 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 10.228729604653129		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 10.228729604653129 | validation: 10.144362375727628]
	TIME [epoch: 4.3 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221187871099374		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 10.221187871099374 | validation: 10.131083464961044]
	TIME [epoch: 4.3 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 10.237663628616279		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 10.237663628616279 | validation: 10.143468089603553]
	TIME [epoch: 4.28 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212177244997186		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 10.212177244997186 | validation: 10.169947376129901]
	TIME [epoch: 4.28 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21854545249756		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 10.21854545249756 | validation: 10.17276075366603]
	TIME [epoch: 4.29 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 10.224002809614024		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 10.224002809614024 | validation: 10.144260583314976]
	TIME [epoch: 4.29 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209031435254312		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 10.209031435254312 | validation: 10.174096516809332]
	TIME [epoch: 4.28 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22865010793521		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 10.22865010793521 | validation: 10.143874551652015]
	TIME [epoch: 4.28 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 10.236080257413652		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 10.236080257413652 | validation: 10.139380524448395]
	TIME [epoch: 4.28 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 10.215381059009882		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 10.215381059009882 | validation: 10.116303087912435]
	TIME [epoch: 4.28 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219852730679495		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 10.219852730679495 | validation: 10.120955664853295]
	TIME [epoch: 4.31 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 10.225688827853233		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 10.225688827853233 | validation: 10.142335033678448]
	TIME [epoch: 4.59 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 10.224340298094027		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 10.224340298094027 | validation: 10.136994715896087]
	TIME [epoch: 4.28 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 10.214574666491771		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 10.214574666491771 | validation: 10.16044153632846]
	TIME [epoch: 4.28 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 10.230290512312186		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 10.230290512312186 | validation: 10.126176062160429]
	TIME [epoch: 4.29 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 10.201464980796935		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 10.201464980796935 | validation: 10.164874660590732]
	TIME [epoch: 4.29 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 10.23154702522256		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 10.23154702522256 | validation: 10.15642916267453]
	TIME [epoch: 4.29 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221982644805593		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 10.221982644805593 | validation: 10.12110105921775]
	TIME [epoch: 4.29 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 10.244239524530665		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 10.244239524530665 | validation: 10.146367600790828]
	TIME [epoch: 4.29 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 10.222144186981048		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 10.222144186981048 | validation: 10.136013197719993]
	TIME [epoch: 4.29 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 10.214493529507386		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 10.214493529507386 | validation: 10.13400428247165]
	TIME [epoch: 4.29 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 10.215550838055231		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 10.215550838055231 | validation: 10.119615012721034]
	TIME [epoch: 4.33 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 10.237098609007194		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 10.237098609007194 | validation: 10.093699018909566]
	TIME [epoch: 4.3 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209130548652135		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 10.209130548652135 | validation: 10.131347996505793]
	TIME [epoch: 4.29 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213087761338759		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 10.213087761338759 | validation: 10.1533186325513]
	TIME [epoch: 4.3 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 10.220551161275		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 10.220551161275 | validation: 10.102850389581985]
	TIME [epoch: 4.29 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 10.225474225047485		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 10.225474225047485 | validation: 10.127027622673328]
	TIME [epoch: 4.29 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 10.238176681697835		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 10.238176681697835 | validation: 10.134185951698246]
	TIME [epoch: 4.29 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217463820978193		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 10.217463820978193 | validation: 10.114408390638445]
	TIME [epoch: 4.29 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212950164024392		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 10.212950164024392 | validation: 10.120923804572538]
	TIME [epoch: 4.29 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221093399622248		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 10.221093399622248 | validation: 10.146353802372937]
	TIME [epoch: 4.29 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210109837302124		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 10.210109837302124 | validation: 10.134557815598932]
	TIME [epoch: 4.33 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 10.194995312843892		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 10.194995312843892 | validation: 10.135111497394517]
	TIME [epoch: 4.3 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 10.233556538884539		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 10.233556538884539 | validation: 10.100504632277726]
	TIME [epoch: 4.3 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221594697615746		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 10.221594697615746 | validation: 10.127471736341912]
	TIME [epoch: 4.29 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 10.233575764613313		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 10.233575764613313 | validation: 10.158966328611333]
	TIME [epoch: 4.29 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 10.202270870392814		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 10.202270870392814 | validation: 10.129958443115358]
	TIME [epoch: 4.29 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 10.229859529805726		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 10.229859529805726 | validation: 10.105361850509412]
	TIME [epoch: 4.29 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213391996519096		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 10.213391996519096 | validation: 10.157067617179607]
	TIME [epoch: 4.29 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20322492575946		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 10.20322492575946 | validation: 10.166544370625608]
	TIME [epoch: 4.29 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212562130798872		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 10.212562130798872 | validation: 10.13666996567221]
	TIME [epoch: 4.29 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 10.214466108279183		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 10.214466108279183 | validation: 10.162200808275418]
	TIME [epoch: 4.33 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21282736832805		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 10.21282736832805 | validation: 10.115802171114364]
	TIME [epoch: 4.3 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211131949880047		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 10.211131949880047 | validation: 10.143784878387379]
	TIME [epoch: 4.29 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219271095500876		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 10.219271095500876 | validation: 10.136293090122756]
	TIME [epoch: 4.29 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206393560994234		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 10.206393560994234 | validation: 10.149358299782154]
	TIME [epoch: 4.29 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212908566238166		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 10.212908566238166 | validation: 10.15652678272139]
	TIME [epoch: 5.71 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209467532381716		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 10.209467532381716 | validation: 10.15551433119416]
	TIME [epoch: 4.29 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 10.230054146509882		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 10.230054146509882 | validation: 10.141907198476343]
	TIME [epoch: 4.29 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 10.207776545009903		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 10.207776545009903 | validation: 10.124641491079078]
	TIME [epoch: 4.29 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210672996521627		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 10.210672996521627 | validation: 10.17498056209525]
	TIME [epoch: 4.29 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219041480361136		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 10.219041480361136 | validation: 10.125087566438447]
	TIME [epoch: 4.33 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 10.205177954771084		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 10.205177954771084 | validation: 10.121738763855003]
	TIME [epoch: 4.3 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 10.199286187385628		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 10.199286187385628 | validation: 10.162777982489859]
	TIME [epoch: 4.29 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217937735929754		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 10.217937735929754 | validation: 10.0913502269485]
	TIME [epoch: 4.29 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20459084669039		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 10.20459084669039 | validation: 10.167420476388198]
	TIME [epoch: 4.29 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21758572336581		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 10.21758572336581 | validation: 10.1180551997669]
	TIME [epoch: 4.29 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20750968155869		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 10.20750968155869 | validation: 10.07611660962732]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_353.pth
	Model improved!!!
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: 10.196778866676636		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: 10.196778866676636 | validation: 10.108296394083586]
	TIME [epoch: 4.3 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 10.224949765235333		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 10.224949765235333 | validation: 10.142501755411033]
	TIME [epoch: 4.29 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20486321832975		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 10.20486321832975 | validation: 10.13918635493712]
	TIME [epoch: 4.3 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21019324380049		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 10.21019324380049 | validation: 10.099689582173335]
	TIME [epoch: 4.32 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212864856176514		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 10.212864856176514 | validation: 10.119361578241953]
	TIME [epoch: 4.29 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203760807153483		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 10.203760807153483 | validation: 10.176985060882561]
	TIME [epoch: 4.29 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217108309037759		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 10.217108309037759 | validation: 10.124996127724316]
	TIME [epoch: 4.29 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21129278536145		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: 10.21129278536145 | validation: 10.074571859809696]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_361.pth
	Model improved!!!
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22182260489075		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 10.22182260489075 | validation: 10.151822228315247]
	TIME [epoch: 4.87 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221173383831255		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 10.221173383831255 | validation: 10.136674047770033]
	TIME [epoch: 4.28 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203475183680542		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 10.203475183680542 | validation: 10.119834498641378]
	TIME [epoch: 4.28 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203424908434396		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 10.203424908434396 | validation: 10.107378655658763]
	TIME [epoch: 4.28 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213720610446277		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 10.213720610446277 | validation: 10.150984891194295]
	TIME [epoch: 4.29 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206011871511873		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 10.206011871511873 | validation: 10.113448506716542]
	TIME [epoch: 4.32 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206899555336197		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 10.206899555336197 | validation: 10.146470849668646]
	TIME [epoch: 4.3 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21963060506413		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 10.21963060506413 | validation: 10.116937098240513]
	TIME [epoch: 4.35 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 10.229231700689382		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 10.229231700689382 | validation: 10.107104671166182]
	TIME [epoch: 4.29 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20792579755331		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 10.20792579755331 | validation: 10.109050574073272]
	TIME [epoch: 4.28 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209236449441665		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 10.209236449441665 | validation: 10.123244402187044]
	TIME [epoch: 4.28 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 10.205450180939248		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 10.205450180939248 | validation: 10.114590932418752]
	TIME [epoch: 4.28 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21532272040528		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 10.21532272040528 | validation: 10.106915133621285]
	TIME [epoch: 4.28 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 10.221642443216513		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 10.221642443216513 | validation: 10.145429407767548]
	TIME [epoch: 4.29 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21689147726546		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 10.21689147726546 | validation: 10.127827053197805]
	TIME [epoch: 4.29 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210562350378593		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 10.210562350378593 | validation: 10.143437511981247]
	TIME [epoch: 4.32 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21943041799789		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: 10.21943041799789 | validation: 10.130991834784288]
	TIME [epoch: 4.29 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21839786579979		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 10.21839786579979 | validation: 10.142261931327116]
	TIME [epoch: 4.29 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203403648372381		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 10.203403648372381 | validation: 10.115240753807665]
	TIME [epoch: 4.3 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 10.222233025572802		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 10.222233025572802 | validation: 10.155617200406398]
	TIME [epoch: 4.29 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 10.200590856280218		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 10.200590856280218 | validation: 10.140927278351896]
	TIME [epoch: 4.29 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 10.195191134959922		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 10.195191134959922 | validation: 10.117369511939764]
	TIME [epoch: 4.29 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 10.215742962355522		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 10.215742962355522 | validation: 10.135482393537679]
	TIME [epoch: 4.29 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206455763917916		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 10.206455763917916 | validation: 10.165075065549704]
	TIME [epoch: 4.29 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: 10.205473770134603		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: 10.205473770134603 | validation: 10.133612541558527]
	TIME [epoch: 4.31 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 10.225975924816112		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 10.225975924816112 | validation: 10.135669795011271]
	TIME [epoch: 4.33 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 10.204071453304838		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 10.204071453304838 | validation: 10.112407316016922]
	TIME [epoch: 4.3 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 10.207921887638816		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 10.207921887638816 | validation: 10.110991954702376]
	TIME [epoch: 4.3 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 10.208001299456475		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 10.208001299456475 | validation: 10.110438245623468]
	TIME [epoch: 4.3 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217855951657691		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 10.217855951657691 | validation: 10.1410634016143]
	TIME [epoch: 4.29 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 10.195231027053836		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 10.195231027053836 | validation: 10.14675417878151]
	TIME [epoch: 4.41 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210529248999752		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 10.210529248999752 | validation: 10.138648326801217]
	TIME [epoch: 4.29 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 10.199953565514203		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 10.199953565514203 | validation: 10.137046738372817]
	TIME [epoch: 4.29 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20319652453598		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 10.20319652453598 | validation: 10.140212595540842]
	TIME [epoch: 4.29 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217757846476124		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 10.217757846476124 | validation: 10.144665964911592]
	TIME [epoch: 4.3 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213285453426511		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 10.213285453426511 | validation: 10.125273013431208]
	TIME [epoch: 4.32 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 10.199633443175502		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 10.199633443175502 | validation: 10.141604421703514]
	TIME [epoch: 4.3 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21282422416922		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 10.21282422416922 | validation: 10.117136061120085]
	TIME [epoch: 4.3 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 10.208129845804331		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 10.208129845804331 | validation: 10.134271397288408]
	TIME [epoch: 4.29 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 10.223543513294318		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 10.223543513294318 | validation: 10.145095826973913]
	TIME [epoch: 4.29 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 10.218267651857126		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 10.218267651857126 | validation: 10.131609864625059]
	TIME [epoch: 4.28 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 10.19948102061222		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 10.19948102061222 | validation: 10.137812873385109]
	TIME [epoch: 4.28 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213651623224619		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 10.213651623224619 | validation: 10.12594222683547]
	TIME [epoch: 4.29 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206702299095461		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 10.206702299095461 | validation: 10.095644475311769]
	TIME [epoch: 4.28 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 10.219839271382668		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 10.219839271382668 | validation: 10.132857378460251]
	TIME [epoch: 4.31 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 10.214194559433594		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 10.214194559433594 | validation: 10.117883073351479]
	TIME [epoch: 4.3 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 10.207299893991664		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 10.207299893991664 | validation: 10.126125946548289]
	TIME [epoch: 4.28 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 10.217715733602569		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 10.217715733602569 | validation: 10.15386742162369]
	TIME [epoch: 4.29 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213712666063358		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 10.213712666063358 | validation: 10.141027570730392]
	TIME [epoch: 4.28 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 10.216875717485443		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 10.216875717485443 | validation: 10.080075416056205]
	TIME [epoch: 4.28 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203229893789251		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 10.203229893789251 | validation: 10.131275989793988]
	TIME [epoch: 4.28 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 10.204863214952205		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 10.204863214952205 | validation: 10.137502321606942]
	TIME [epoch: 4.28 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206741763634328		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 10.206741763634328 | validation: 10.128086365228544]
	TIME [epoch: 4.27 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206391865868124		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 10.206391865868124 | validation: 10.126458614174108]
	TIME [epoch: 4.27 sec]
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213947993628286		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: 10.213947993628286 | validation: 10.107230585156417]
	TIME [epoch: 4.3 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: 10.201352104887011		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: 10.201352104887011 | validation: 10.154842909286181]
	TIME [epoch: 4.3 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 10.210197439012033		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 10.210197439012033 | validation: 10.125834480899329]
	TIME [epoch: 4.28 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 10.200101873599266		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 10.200101873599266 | validation: 10.14172787955108]
	TIME [epoch: 4.28 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 10.200374334359925		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 10.200374334359925 | validation: 10.083928273203018]
	TIME [epoch: 4.28 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206504876048257		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 10.206504876048257 | validation: 10.1330282860925]
	TIME [epoch: 4.28 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: 10.218694674023906		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: 10.218694674023906 | validation: 10.116161888340343]
	TIME [epoch: 4.28 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: 10.187835428309455		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: 10.187835428309455 | validation: 10.129124368979108]
	TIME [epoch: 4.28 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209861287821404		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 10.209861287821404 | validation: 10.134545123676194]
	TIME [epoch: 4.28 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212376005165993		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: 10.212376005165993 | validation: 10.087603038690355]
	TIME [epoch: 4.28 sec]
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 10.201616770785076		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 10.201616770785076 | validation: 10.134616447201548]
	TIME [epoch: 4.3 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 10.209006742275765		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 10.209006742275765 | validation: 10.128794470851197]
	TIME [epoch: 4.3 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 10.225324969063056		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 10.225324969063056 | validation: 10.143688431336262]
	TIME [epoch: 4.28 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 10.197493945785396		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 10.197493945785396 | validation: 10.125593767409622]
	TIME [epoch: 4.29 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211572621820526		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 10.211572621820526 | validation: 10.117718617896921]
	TIME [epoch: 4.28 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 10.204733011232271		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 10.204733011232271 | validation: 10.109347308052039]
	TIME [epoch: 4.27 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 10.212550072477562		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 10.212550072477562 | validation: 10.11997311479991]
	TIME [epoch: 4.28 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 10.196163817949504		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 10.196163817949504 | validation: 10.103143693329535]
	TIME [epoch: 4.28 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 10.191123045719756		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 10.191123045719756 | validation: 10.12580689244437]
	TIME [epoch: 4.28 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 10.22464667670599		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 10.22464667670599 | validation: 10.121554565415579]
	TIME [epoch: 4.28 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 10.204447359190722		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 10.204447359190722 | validation: 10.133288388952328]
	TIME [epoch: 4.29 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 10.200455038263868		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 10.200455038263868 | validation: 10.101182467034125]
	TIME [epoch: 4.31 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211618233687288		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 10.211618233687288 | validation: 10.144960734494834]
	TIME [epoch: 4.28 sec]
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21657055188287		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: 10.21657055188287 | validation: 10.146012532733504]
	TIME [epoch: 4.28 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 10.215481372152535		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 10.215481372152535 | validation: 10.147449867466685]
	TIME [epoch: 4.27 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203303518810309		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 10.203303518810309 | validation: 10.13867527824435]
	TIME [epoch: 4.27 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 10.19879048111345		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 10.19879048111345 | validation: 10.127308385221616]
	TIME [epoch: 4.28 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: 10.206013987614117		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: 10.206013987614117 | validation: 10.093326551005603]
	TIME [epoch: 4.28 sec]
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20102668523289		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 10.20102668523289 | validation: 10.113678337026727]
	TIME [epoch: 4.28 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 10.198637813360492		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 10.198637813360492 | validation: 10.131856876290268]
	TIME [epoch: 4.28 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 10.214207336823623		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 10.214207336823623 | validation: 10.107941710633495]
	TIME [epoch: 4.29 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: 10.222923103665554		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: 10.222923103665554 | validation: 10.13348698505821]
	TIME [epoch: 4.31 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: 10.197465250869872		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: 10.197465250869872 | validation: 10.14230417959855]
	TIME [epoch: 4.28 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: 10.220561977761612		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: 10.220561977761612 | validation: 10.15412421129551]
	TIME [epoch: 4.29 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: 10.202842108938283		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: 10.202842108938283 | validation: 10.116697366285095]
	TIME [epoch: 4.28 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21984338361311		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: 10.21984338361311 | validation: 10.136855635215898]
	TIME [epoch: 4.28 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 10.23614622451009		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 10.23614622451009 | validation: 10.101523848583343]
	TIME [epoch: 4.28 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 10.203388380558266		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 10.203388380558266 | validation: 10.140131608060692]
	TIME [epoch: 4.28 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213025719918303		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 10.213025719918303 | validation: 10.092835015392458]
	TIME [epoch: 4.28 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211852464735026		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 10.211852464735026 | validation: 10.132613819714116]
	TIME [epoch: 4.29 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 10.21159769466377		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 10.21159769466377 | validation: 10.101391562674102]
	TIME [epoch: 4.3 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: 10.20130330164482		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: 10.20130330164482 | validation: 10.126463163941409]
	TIME [epoch: 4.32 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213440640905194		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: 10.213440640905194 | validation: 10.133200856122484]
	TIME [epoch: 4.28 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 10.211849476011398		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 10.211849476011398 | validation: 10.12345717872158]
	TIME [epoch: 4.28 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 10.195925808085352		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 10.195925808085352 | validation: 10.099847641558089]
	TIME [epoch: 4.29 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 10.208720661568023		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 10.208720661568023 | validation: 10.113029047424726]
	TIME [epoch: 4.28 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 10.213395569964227		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 10.213395569964227 | validation: 10.10080861774922]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_klv2_20240703_153539/states/model_algphi1_1a_v_klv2_462.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2110.582 seconds.
