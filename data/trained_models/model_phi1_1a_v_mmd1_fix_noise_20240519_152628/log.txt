Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.5, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1556894357

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.235624929503921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.235624929503921 | validation: 5.189581870039369]
	TIME [epoch: 145 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86060271914903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.86060271914903 | validation: 4.4422602119518615]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5120758239091066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5120758239091066 | validation: 4.359099024619506]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.393112009763999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.393112009763999 | validation: 4.202949601759575]
	TIME [epoch: 8.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.286852841694476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.286852841694476 | validation: 4.130484763586891]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.204280020464253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.204280020464253 | validation: 3.992710169892989]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.316111180447261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.316111180447261 | validation: 4.13874093484575]
	TIME [epoch: 8.33 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.191975816632679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.191975816632679 | validation: 4.039764433148173]
	TIME [epoch: 8.32 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.099924569417905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.099924569417905 | validation: 3.895696375153343]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.385684545152716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.385684545152716 | validation: 4.026481653778893]
	TIME [epoch: 8.38 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130093069105867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.130093069105867 | validation: 3.9744866310674425]
	TIME [epoch: 8.34 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0567120871221425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0567120871221425 | validation: 3.855209816603491]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.221512641848434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221512641848434 | validation: 4.091280372528806]
	TIME [epoch: 8.32 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1305316185192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1305316185192 | validation: 3.938848153126481]
	TIME [epoch: 8.32 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.015703193637111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015703193637111 | validation: 3.8219602341243015]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.111737749187158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.111737749187158 | validation: 3.937303744387764]
	TIME [epoch: 8.34 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.006510624649661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.006510624649661 | validation: 3.784163526682688]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9447309889988458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9447309889988458 | validation: 4.218072832902859]
	TIME [epoch: 8.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.074016879786355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074016879786355 | validation: 3.782144407460745]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9370787728253553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9370787728253553 | validation: 3.7932207182641777]
	TIME [epoch: 8.34 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130474011006564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.130474011006564 | validation: 3.928152806339685]
	TIME [epoch: 8.36 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.983379916887393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.983379916887393 | validation: 3.730608575095414]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.913664901510167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.913664901510167 | validation: 3.9512883821038436]
	TIME [epoch: 8.64 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9761027838953806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9761027838953806 | validation: 3.7354587955539387]
	TIME [epoch: 8.33 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.961920311674273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.961920311674273 | validation: 3.7360340630176014]
	TIME [epoch: 8.34 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9028046486823067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9028046486823067 | validation: 3.670756950452364]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8678332410455725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8678332410455725 | validation: 3.842522557246075]
	TIME [epoch: 8.47 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9514156603240735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9514156603240735 | validation: 3.7977796581004615]
	TIME [epoch: 8.34 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9242077038761867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9242077038761867 | validation: 3.7157127847814513]
	TIME [epoch: 8.35 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8777211313489386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8777211313489386 | validation: 3.6483145221721385]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824163347210346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.824163347210346 | validation: 3.6451144844781656]
	TIME [epoch: 8.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.90558979651568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.90558979651568 | validation: 3.76533309615687]
	TIME [epoch: 8.5 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8527108242209764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8527108242209764 | validation: 3.6633613129193794]
	TIME [epoch: 8.34 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.864287969912305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.864287969912305 | validation: 3.633395731948959]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8457801675448806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8457801675448806 | validation: 3.6372418891789415]
	TIME [epoch: 8.33 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.822500179547568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.822500179547568 | validation: 3.6761555374861983]
	TIME [epoch: 8.33 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8993620889070186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8993620889070186 | validation: 3.65475951201412]
	TIME [epoch: 8.37 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.859363109040464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859363109040464 | validation: 3.663898123152736]
	TIME [epoch: 8.35 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.818151751798665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.818151751798665 | validation: 3.6211934679958593]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8092041558673593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8092041558673593 | validation: 3.633962038696994]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819226921724857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819226921724857 | validation: 3.6105011634939377]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.809236185678622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.809236185678622 | validation: 3.6651624921147667]
	TIME [epoch: 8.34 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8171804119669543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8171804119669543 | validation: 3.6344762085044735]
	TIME [epoch: 8.37 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8079177146352885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8079177146352885 | validation: 3.644964434605892]
	TIME [epoch: 8.33 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8145499355531967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8145499355531967 | validation: 3.6189591061040387]
	TIME [epoch: 8.34 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7972017617039278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7972017617039278 | validation: 3.6364449823857594]
	TIME [epoch: 8.31 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7760426517162777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7760426517162777 | validation: 3.601822206747852]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8120860465158426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8120860465158426 | validation: 3.597916338480231]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.807594601226312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.807594601226312 | validation: 3.682643215877188]
	TIME [epoch: 8.35 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8041522163937787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8041522163937787 | validation: 3.6281875049253642]
	TIME [epoch: 8.33 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.772674203781002		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 3.772674203781002 | validation: 3.650749689236906]
	TIME [epoch: 8.65 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7709014153007323		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 3.7709014153007323 | validation: 3.596135869763191]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776870843918788		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 3.776870843918788 | validation: 3.6244238416626495]
	TIME [epoch: 8.35 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.749193224858116		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 3.749193224858116 | validation: 3.5810941741172027]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9744336133203633		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 3.9744336133203633 | validation: 4.112194382202917]
	TIME [epoch: 8.34 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7310596851050164		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 3.7310596851050164 | validation: 3.549160183947058]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.462354450354059		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.462354450354059 | validation: 3.4260376779392483]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3992340508872623		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.3992340508872623 | validation: 3.399032636286056]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3913206346005196		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.3913206346005196 | validation: 3.3977628220063436]
	TIME [epoch: 8.39 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.400260186170703		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.400260186170703 | validation: 3.48941877954008]
	TIME [epoch: 8.34 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.430014006825359		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.430014006825359 | validation: 3.3791853954243947]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352058864677715		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.352058864677715 | validation: 3.3541281600939943]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394095927723862		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.394095927723862 | validation: 3.409034317559673]
	TIME [epoch: 8.33 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4477315658882977		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 3.4477315658882977 | validation: 3.4752021765354297]
	TIME [epoch: 8.34 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4951549023160355		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 3.4951549023160355 | validation: 3.389594078413035]
	TIME [epoch: 8.35 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.364229912963562		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 3.364229912963562 | validation: 3.3461895195005438]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3496546222630297		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.3496546222630297 | validation: 3.3389095975959266]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6225597060258288		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.6225597060258288 | validation: 3.8962425868774733]
	TIME [epoch: 8.35 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6938525584189565		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.6938525584189565 | validation: 3.495713026011662]
	TIME [epoch: 8.34 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.47408372319197		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.47408372319197 | validation: 3.3627656166722115]
	TIME [epoch: 8.39 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342685306898239		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 3.342685306898239 | validation: 3.3408347138596817]
	TIME [epoch: 8.35 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325796059971837		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.325796059971837 | validation: 3.336303333739626]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3310176659033033		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.3310176659033033 | validation: 3.323200398981339]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.317299296734542		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.317299296734542 | validation: 3.3944445222208905]
	TIME [epoch: 8.33 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320620063088038		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.320620063088038 | validation: 3.3208775677458613]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3349848294588997		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 3.3349848294588997 | validation: 3.357163191740322]
	TIME [epoch: 8.36 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3245338429583664		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 3.3245338429583664 | validation: 3.310860921073953]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308052583567431		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 3.308052583567431 | validation: 3.304370832827665]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.314634517078162		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.314634517078162 | validation: 3.3496893172722233]
	TIME [epoch: 8.33 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331217379677714		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 3.331217379677714 | validation: 3.3429645552557616]
	TIME [epoch: 8.33 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324507356973027		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 3.324507356973027 | validation: 3.3184815522187003]
	TIME [epoch: 8.38 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3110960597833308		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.3110960597833308 | validation: 3.3723087938652805]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4079968439927315		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 3.4079968439927315 | validation: 3.375589775780385]
	TIME [epoch: 8.33 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323505188261719		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.323505188261719 | validation: 3.3202970907356937]
	TIME [epoch: 8.33 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3032594971431406		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.3032594971431406 | validation: 3.3045242107101593]
	TIME [epoch: 8.33 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3020692386557027		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.3020692386557027 | validation: 3.3297052029154504]
	TIME [epoch: 8.36 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3101204319186746		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.3101204319186746 | validation: 4.013462016602127]
	TIME [epoch: 8.36 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.725893001820379		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 3.725893001820379 | validation: 3.472163085468835]
	TIME [epoch: 8.33 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.377546989176944		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.377546989176944 | validation: 3.3116808432041402]
	TIME [epoch: 8.32 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298566458532664		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 3.298566458532664 | validation: 3.3086175587423785]
	TIME [epoch: 8.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3849604025766857		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 3.3849604025766857 | validation: 3.484799054221287]
	TIME [epoch: 8.32 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.602421456429594		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 3.602421456429594 | validation: 3.4635533266636216]
	TIME [epoch: 8.37 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366501929957838		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 3.366501929957838 | validation: 3.3224657378720686]
	TIME [epoch: 8.34 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3054559357082		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 3.3054559357082 | validation: 3.53851892624007]
	TIME [epoch: 8.32 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5247478999479203		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 3.5247478999479203 | validation: 3.369128424298376]
	TIME [epoch: 8.32 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329640352086158		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 3.329640352086158 | validation: 3.307372960808155]
	TIME [epoch: 8.32 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2952018835290318		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 3.2952018835290318 | validation: 3.306123286899018]
	TIME [epoch: 8.33 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2922197890544203		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 3.2922197890544203 | validation: 3.2977031766637115]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2861106721415725		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.2861106721415725 | validation: 3.301348167819807]
	TIME [epoch: 8.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2926089515977215		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 3.2926089515977215 | validation: 3.315350236278186]
	TIME [epoch: 8.34 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3008173263189535		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 3.3008173263189535 | validation: 3.3033103492041116]
	TIME [epoch: 8.34 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2971670803919766		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 3.2971670803919766 | validation: 3.328427255933488]
	TIME [epoch: 8.33 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.307007250206504		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 3.307007250206504 | validation: 3.296640499242213]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3025396329333896		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 3.3025396329333896 | validation: 3.3209150082509096]
	TIME [epoch: 8.35 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.291090603768432		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.291090603768432 | validation: 3.3728330436762843]
	TIME [epoch: 8.34 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351025954458528		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 3.351025954458528 | validation: 3.342055059812627]
	TIME [epoch: 8.34 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2994900625755186		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 3.2994900625755186 | validation: 3.2945558070280425]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285097136587037		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.285097136587037 | validation: 3.292588810392317]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295026624564577		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 3.295026624564577 | validation: 3.3023203745277065]
	TIME [epoch: 8.37 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294353840311904		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 3.294353840311904 | validation: 3.3768037539609495]
	TIME [epoch: 8.33 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.701154682047437		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.701154682047437 | validation: 3.481319601035299]
	TIME [epoch: 8.32 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5933599148028414		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 3.5933599148028414 | validation: 3.5752767696548995]
	TIME [epoch: 8.33 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7101251699613615		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 3.7101251699613615 | validation: 3.53301563051768]
	TIME [epoch: 8.33 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6817501149170337		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 3.6817501149170337 | validation: 3.524898927810911]
	TIME [epoch: 8.36 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6683324078226733		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 3.6683324078226733 | validation: 3.5226412304485866]
	TIME [epoch: 8.35 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6526925184086445		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 3.6526925184086445 | validation: 3.5214904125765414]
	TIME [epoch: 8.32 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6339263677162594		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.6339263677162594 | validation: 3.49283118806723]
	TIME [epoch: 8.32 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.587536451437568		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 3.587536451437568 | validation: 3.469095660451783]
	TIME [epoch: 8.33 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5369102684163085		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.5369102684163085 | validation: 3.4496950403033173]
	TIME [epoch: 8.33 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.493321081580216		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 3.493321081580216 | validation: 3.4124600447180047]
	TIME [epoch: 8.38 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4308604588328455		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 3.4308604588328455 | validation: 3.383244106692775]
	TIME [epoch: 8.34 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4317382835635755		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 3.4317382835635755 | validation: 3.3963066160392037]
	TIME [epoch: 8.33 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351009770299909		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 3.351009770299909 | validation: 3.3485409139075446]
	TIME [epoch: 8.32 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3262183964623393		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 3.3262183964623393 | validation: 3.338920907832583]
	TIME [epoch: 8.32 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.357156734967007		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 3.357156734967007 | validation: 3.337338222146865]
	TIME [epoch: 8.33 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3322502199240533		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 3.3322502199240533 | validation: 3.335419638855023]
	TIME [epoch: 8.37 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323937084056906		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 3.323937084056906 | validation: 3.3319370019708283]
	TIME [epoch: 8.32 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329688439270668		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 3.329688439270668 | validation: 3.3186013238440575]
	TIME [epoch: 8.33 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3066149264859814		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 3.3066149264859814 | validation: 3.32029921059661]
	TIME [epoch: 8.33 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320147937945354		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 3.320147937945354 | validation: 3.311648653288581]
	TIME [epoch: 8.33 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.307868787634325		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 3.307868787634325 | validation: 3.338555445878314]
	TIME [epoch: 8.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302081413936452		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 3.302081413936452 | validation: 3.3344684833083242]
	TIME [epoch: 8.34 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320722076578049		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 3.320722076578049 | validation: 3.3200289835493013]
	TIME [epoch: 8.37 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3001022957176587		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 3.3001022957176587 | validation: 3.3192970899422143]
	TIME [epoch: 8.39 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299933232062198		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 3.299933232062198 | validation: 3.335189452061301]
	TIME [epoch: 8.34 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320280168840745		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 3.320280168840745 | validation: 3.3038374606238143]
	TIME [epoch: 8.32 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3020697677361253		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 3.3020697677361253 | validation: 3.322473506090515]
	TIME [epoch: 8.36 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3241644331750755		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 3.3241644331750755 | validation: 3.3380662255972897]
	TIME [epoch: 8.33 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299521253072074		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 3.299521253072074 | validation: 3.3043370823578533]
	TIME [epoch: 8.33 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2967669643271886		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 3.2967669643271886 | validation: 3.2985323797825465]
	TIME [epoch: 8.34 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2900061807669903		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 3.2900061807669903 | validation: 3.3097156560043737]
	TIME [epoch: 8.34 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3009211315743934		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 3.3009211315743934 | validation: 3.3018313705904885]
	TIME [epoch: 8.48 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2999993164656827		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 3.2999993164656827 | validation: 3.302302437369015]
	TIME [epoch: 8.42 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2966887876665703		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 3.2966887876665703 | validation: 3.3257514372982504]
	TIME [epoch: 8.35 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295398299423886		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 3.295398299423886 | validation: 3.3093690366286266]
	TIME [epoch: 8.33 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31044555646226		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 3.31044555646226 | validation: 3.3122859811262586]
	TIME [epoch: 8.33 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298785237163996		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 3.298785237163996 | validation: 3.3000469684616522]
	TIME [epoch: 8.34 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286594346350572		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 3.286594346350572 | validation: 3.301816088971562]
	TIME [epoch: 8.38 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2923838044099654		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 3.2923838044099654 | validation: 3.3051776987522623]
	TIME [epoch: 8.34 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29509164461259		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 3.29509164461259 | validation: 3.307610005269031]
	TIME [epoch: 8.33 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2908883901059824		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 3.2908883901059824 | validation: 3.3019630730455716]
	TIME [epoch: 8.36 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2846494479784165		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 3.2846494479784165 | validation: 3.3086029374164867]
	TIME [epoch: 8.36 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2942195578305777		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 3.2942195578305777 | validation: 3.2902132579147665]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28193649753854		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 3.28193649753854 | validation: 3.3462952106151485]
	TIME [epoch: 8.53 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3134273216835397		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 3.3134273216835397 | validation: 3.3023132563158466]
	TIME [epoch: 8.37 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2899476309780717		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 3.2899476309780717 | validation: 3.3013503603838688]
	TIME [epoch: 8.33 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2851473323640272		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 3.2851473323640272 | validation: 3.310080333629558]
	TIME [epoch: 8.32 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2853281734562576		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 3.2853281734562576 | validation: 3.289622670610579]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2766259039653094		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 3.2766259039653094 | validation: 3.3143566545892686]
	TIME [epoch: 8.49 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295061614078305		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 3.295061614078305 | validation: 3.286799634679231]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284297170847421		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 3.284297170847421 | validation: 3.3439386148822168]
	TIME [epoch: 8.45 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2980853232905805		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 3.2980853232905805 | validation: 3.3058053047734193]
	TIME [epoch: 8.35 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2827666113403273		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 3.2827666113403273 | validation: 3.317447964276962]
	TIME [epoch: 8.34 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299928101899855		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 3.299928101899855 | validation: 3.3042950582753177]
	TIME [epoch: 8.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275851056922229		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 3.275851056922229 | validation: 3.3202840761530528]
	TIME [epoch: 8.45 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2960389750507835		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 3.2960389750507835 | validation: 3.308758054398502]
	TIME [epoch: 8.4 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2768956279123302		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 3.2768956279123302 | validation: 3.279491365898651]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284297161492802		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 3.284297161492802 | validation: 3.2842916008062324]
	TIME [epoch: 8.42 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.268498654624623		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 3.268498654624623 | validation: 3.281733207945803]
	TIME [epoch: 8.34 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2758920148378885		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 3.2758920148378885 | validation: 3.4386689593962765]
	TIME [epoch: 8.37 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3234605736023095		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 3.3234605736023095 | validation: 3.284846062684787]
	TIME [epoch: 8.34 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.26722407136254		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 3.26722407136254 | validation: 3.3031218421949005]
	TIME [epoch: 8.33 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271908937529997		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 3.271908937529997 | validation: 3.2782261367581462]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2663009826309697		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 3.2663009826309697 | validation: 3.2947947419030346]
	TIME [epoch: 8.33 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2719000060356467		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 3.2719000060356467 | validation: 3.2938822281433113]
	TIME [epoch: 8.34 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2706108758464016		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 3.2706108758464016 | validation: 3.318253714191358]
	TIME [epoch: 8.37 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2692900946240906		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 3.2692900946240906 | validation: 3.2821915754098545]
	TIME [epoch: 8.33 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.277665683335594		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 3.277665683335594 | validation: 3.2870762717697946]
	TIME [epoch: 8.33 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2723180930931193		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 3.2723180930931193 | validation: 3.27147029754303]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4122197151151172		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 3.4122197151151172 | validation: 3.337448351119239]
	TIME [epoch: 8.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.310439426437269		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 3.310439426437269 | validation: 3.2841123930934906]
	TIME [epoch: 8.36 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2619359251167332		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 3.2619359251167332 | validation: 3.2687176491636105]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295685645865598		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 3.295685645865598 | validation: 3.6146714519344765]
	TIME [epoch: 8.32 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363809459057604		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 3.363809459057604 | validation: 3.3092275437608425]
	TIME [epoch: 8.33 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288104620207213		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 3.288104620207213 | validation: 3.4006402884053832]
	TIME [epoch: 8.34 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.301319619863838		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 3.301319619863838 | validation: 3.328693267900751]
	TIME [epoch: 8.34 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3007303884127053		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 3.3007303884127053 | validation: 3.3108701585378926]
	TIME [epoch: 8.38 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2758299527235573		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 3.2758299527235573 | validation: 3.3257714212480423]
	TIME [epoch: 8.34 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2759124311169523		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 3.2759124311169523 | validation: 3.304831519038597]
	TIME [epoch: 8.34 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2834033243000564		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 3.2834033243000564 | validation: 3.2935133162780623]
	TIME [epoch: 8.32 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2720015500882482		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 3.2720015500882482 | validation: 3.294979545637993]
	TIME [epoch: 8.32 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2701481921769435		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 3.2701481921769435 | validation: 3.3024181698155517]
	TIME [epoch: 8.36 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2726971454274167		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 3.2726971454274167 | validation: 3.2869288298356647]
	TIME [epoch: 8.35 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2716102847121253		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 3.2716102847121253 | validation: 3.342848927584326]
	TIME [epoch: 8.33 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3075594929814205		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 3.3075594929814205 | validation: 3.2998064167788357]
	TIME [epoch: 8.33 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.600975279167968		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 3.600975279167968 | validation: 3.6311850078137606]
	TIME [epoch: 8.33 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4227233251098976		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 3.4227233251098976 | validation: 3.288039715089217]
	TIME [epoch: 8.32 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258454624122229		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 3.258454624122229 | validation: 3.2682731521710124]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252479085904632		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 3.252479085904632 | validation: 3.267903144937516]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2461259824982167		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 3.2461259824982167 | validation: 3.265828285993099]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2553901987280387		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 3.2553901987280387 | validation: 3.2608933563591984]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2483380469022434		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 3.2483380469022434 | validation: 3.271300289070881]
	TIME [epoch: 8.34 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2490360739240063		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 3.2490360739240063 | validation: 3.2701306957595344]
	TIME [epoch: 8.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2425449899548853		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 3.2425449899548853 | validation: 3.270317824872785]
	TIME [epoch: 8.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.244502807447264		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 3.244502807447264 | validation: 3.255472475299225]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2394827940722744		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 3.2394827940722744 | validation: 3.302714533712268]
	TIME [epoch: 8.34 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2655761399540792		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 3.2655761399540792 | validation: 3.2647531971097195]
	TIME [epoch: 8.32 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252490329465635		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 3.252490329465635 | validation: 3.275622784128446]
	TIME [epoch: 8.34 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2374055596085873		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 3.2374055596085873 | validation: 3.2824457848114266]
	TIME [epoch: 8.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269852708620127		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 3.269852708620127 | validation: 3.314239408248454]
	TIME [epoch: 8.33 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2719819007321536		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 3.2719819007321536 | validation: 3.268391055116199]
	TIME [epoch: 8.33 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2937203871514127		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 3.2937203871514127 | validation: 3.3330297355068463]
	TIME [epoch: 8.33 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3009653840226334		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 3.3009653840226334 | validation: 3.292903096973002]
	TIME [epoch: 8.33 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2824282371660907		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 3.2824282371660907 | validation: 3.2912760391815294]
	TIME [epoch: 8.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.266478778248663		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 3.266478778248663 | validation: 3.2832846946382297]
	TIME [epoch: 8.37 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.301281165877323		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 3.301281165877323 | validation: 3.317951050649741]
	TIME [epoch: 8.33 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2961575368886193		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 3.2961575368886193 | validation: 3.298943372151456]
	TIME [epoch: 8.33 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2907208344026797		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 3.2907208344026797 | validation: 3.279126464718428]
	TIME [epoch: 8.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2728103444005896		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 3.2728103444005896 | validation: 3.3738316862926614]
	TIME [epoch: 8.33 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3067191351610883		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 3.3067191351610883 | validation: 3.2889792087467673]
	TIME [epoch: 8.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2735919147119157		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 3.2735919147119157 | validation: 3.2923144565485183]
	TIME [epoch: 8.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2755693616305295		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 3.2755693616305295 | validation: 3.2940089126078034]
	TIME [epoch: 8.33 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.309618613090819		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 3.309618613090819 | validation: 3.4037191085352347]
	TIME [epoch: 8.34 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3023550492050475		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 3.3023550492050475 | validation: 3.286953297633377]
	TIME [epoch: 8.33 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269782219530234		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 3.269782219530234 | validation: 3.276956118726739]
	TIME [epoch: 8.34 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2697636332218587		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 3.2697636332218587 | validation: 3.2948110939959063]
	TIME [epoch: 8.37 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265966056021738		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 3.265966056021738 | validation: 3.2855956063970737]
	TIME [epoch: 8.33 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2879226265902886		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 3.2879226265902886 | validation: 3.291155693035786]
	TIME [epoch: 8.33 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2752048096929256		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 3.2752048096929256 | validation: 3.282140829345387]
	TIME [epoch: 8.33 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261803238087193		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 3.261803238087193 | validation: 3.3195360797497164]
	TIME [epoch: 8.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285569735022196		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 3.285569735022196 | validation: 3.2872358716424324]
	TIME [epoch: 8.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2707580496468602		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 3.2707580496468602 | validation: 3.293803268829844]
	TIME [epoch: 8.37 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2888378260808		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 3.2888378260808 | validation: 3.2912835805445155]
	TIME [epoch: 8.34 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2639903735170126		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 3.2639903735170126 | validation: 3.2608128353309445]
	TIME [epoch: 8.32 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2425116173535784		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 3.2425116173535784 | validation: 3.313031507549736]
	TIME [epoch: 8.32 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2688141792394383		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 3.2688141792394383 | validation: 3.2931543007249524]
	TIME [epoch: 8.33 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263572668820011		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 3.263572668820011 | validation: 3.273053282197872]
	TIME [epoch: 8.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281869095435864		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 3.281869095435864 | validation: 3.262806932505308]
	TIME [epoch: 8.34 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2348077613341575		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 3.2348077613341575 | validation: 3.263544019122955]
	TIME [epoch: 8.32 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2811829945131965		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 3.2811829945131965 | validation: 3.285251324731739]
	TIME [epoch: 8.32 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2540815193380315		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 3.2540815193380315 | validation: 3.3188974752313007]
	TIME [epoch: 8.32 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2625102611558128		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 3.2625102611558128 | validation: 3.7249613502626127]
	TIME [epoch: 8.33 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367766343508324		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 3.367766343508324 | validation: 3.2700627780436307]
	TIME [epoch: 8.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2520520042653356		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 3.2520520042653356 | validation: 3.255801772997203]
	TIME [epoch: 8.33 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2590591930395965		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 3.2590591930395965 | validation: 3.2415368101816546]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2348578627366003		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 3.2348578627366003 | validation: 3.2411081617967428]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2644118134134117		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 3.2644118134134117 | validation: 3.2757258049610884]
	TIME [epoch: 8.33 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3181746611998206		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 3.3181746611998206 | validation: 3.3104235769420325]
	TIME [epoch: 8.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2495467844237202		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 3.2495467844237202 | validation: 3.2668561466482533]
	TIME [epoch: 8.68 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2730781672983853		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 3.2730781672983853 | validation: 3.276923380698655]
	TIME [epoch: 8.34 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2449415963394888		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 3.2449415963394888 | validation: 3.266312263611365]
	TIME [epoch: 8.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.279568504621399		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 3.279568504621399 | validation: 3.324085929823771]
	TIME [epoch: 8.34 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273249231201904		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 3.273249231201904 | validation: 3.5496686432129243]
	TIME [epoch: 8.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402288129000967		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 3.402288129000967 | validation: 3.266481549191925]
	TIME [epoch: 8.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2359295629261124		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 3.2359295629261124 | validation: 3.2481138394856903]
	TIME [epoch: 8.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255088971909863		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 3.255088971909863 | validation: 3.2802230951776754]
	TIME [epoch: 8.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2373605138614368		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 3.2373605138614368 | validation: 3.244120381997277]
	TIME [epoch: 8.34 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2301577507173884		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 3.2301577507173884 | validation: 3.2410871930908796]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2629527286277082		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 3.2629527286277082 | validation: 3.2564768022976223]
	TIME [epoch: 8.37 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2934143232369784		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 3.2934143232369784 | validation: 3.4909587124724117]
	TIME [epoch: 8.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.406431602384453		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 3.406431602384453 | validation: 3.294323011037039]
	TIME [epoch: 8.34 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2459911074121948		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 3.2459911074121948 | validation: 3.2588501889308232]
	TIME [epoch: 8.33 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3921726310751614		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 3.3921726310751614 | validation: 3.294372124259791]
	TIME [epoch: 8.33 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2663549752369523		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 3.2663549752369523 | validation: 3.266546388935306]
	TIME [epoch: 8.33 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237531626428781		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 3.237531626428781 | validation: 3.278859383830176]
	TIME [epoch: 8.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2433788620724227		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 3.2433788620724227 | validation: 3.337647375975613]
	TIME [epoch: 8.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2863321369679688		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 3.2863321369679688 | validation: 3.2723652191601573]
	TIME [epoch: 8.33 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2395784352640122		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 3.2395784352640122 | validation: 3.2670220459144357]
	TIME [epoch: 8.33 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2249807389563543		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 3.2249807389563543 | validation: 3.2574058036665834]
	TIME [epoch: 8.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2603637585139973		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 3.2603637585139973 | validation: 3.377096344991751]
	TIME [epoch: 8.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.369968230932675		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 3.369968230932675 | validation: 3.3386163075537705]
	TIME [epoch: 8.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3016427494103096		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 3.3016427494103096 | validation: 3.278547413199881]
	TIME [epoch: 8.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2652080121536375		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 3.2652080121536375 | validation: 3.311996251425718]
	TIME [epoch: 8.33 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2506449913676536		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 3.2506449913676536 | validation: 3.262437806616747]
	TIME [epoch: 8.33 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.249289328137494		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 3.249289328137494 | validation: 3.3155408994305993]
	TIME [epoch: 8.33 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2692356437794383		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 3.2692356437794383 | validation: 3.2846950268185964]
	TIME [epoch: 8.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2516299755786187		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 3.2516299755786187 | validation: 3.2635600679088546]
	TIME [epoch: 8.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.256130631988242		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 3.256130631988242 | validation: 3.355420852737814]
	TIME [epoch: 8.33 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2749432888209116		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 3.2749432888209116 | validation: 3.316918786655668]
	TIME [epoch: 8.33 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2906046344950877		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 3.2906046344950877 | validation: 3.2954502603436273]
	TIME [epoch: 8.33 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.270764330054682		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 3.270764330054682 | validation: 3.2803940606057287]
	TIME [epoch: 8.33 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2490298363587953		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 3.2490298363587953 | validation: 3.256786334030675]
	TIME [epoch: 8.37 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255148869893635		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 3.255148869893635 | validation: 3.277144649161163]
	TIME [epoch: 8.33 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252497517574636		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 3.252497517574636 | validation: 3.268586190481921]
	TIME [epoch: 8.32 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263588497439471		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 3.263588497439471 | validation: 3.282632337434463]
	TIME [epoch: 8.33 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2755845080244823		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 3.2755845080244823 | validation: 3.2863505101097976]
	TIME [epoch: 8.34 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2431771488548806		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 3.2431771488548806 | validation: 3.310576154624691]
	TIME [epoch: 8.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2373140969150715		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 3.2373140969150715 | validation: 3.3455869638384588]
	TIME [epoch: 8.37 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.279391250016857		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 3.279391250016857 | validation: 3.234612773084265]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2145437949277866		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 3.2145437949277866 | validation: 3.2273745870995407]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073055999248643		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 3.2073055999248643 | validation: 3.2432009135730793]
	TIME [epoch: 8.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2116560714373916		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 3.2116560714373916 | validation: 3.3067985623042295]
	TIME [epoch: 8.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227309417141762		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 3.227309417141762 | validation: 3.2385796389002772]
	TIME [epoch: 8.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2383167602365064		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 3.2383167602365064 | validation: 3.237121423242007]
	TIME [epoch: 8.33 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2817886475335203		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 3.2817886475335203 | validation: 3.2849679164058916]
	TIME [epoch: 8.32 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232269405000495		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 3.232269405000495 | validation: 3.245588906028906]
	TIME [epoch: 8.32 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2534375222821823		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 3.2534375222821823 | validation: 3.2227289399857346]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2031704490187387		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 3.2031704490187387 | validation: 3.2449686145661287]
	TIME [epoch: 8.34 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318035586130522		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 3.318035586130522 | validation: 3.5678587017104397]
	TIME [epoch: 8.37 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3472512548546334		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 3.3472512548546334 | validation: 3.2413043933460584]
	TIME [epoch: 8.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2521018788960863		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 3.2521018788960863 | validation: 3.357069044157409]
	TIME [epoch: 8.34 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2365256148605708		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 3.2365256148605708 | validation: 3.2535957672945113]
	TIME [epoch: 8.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2127872893477214		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 3.2127872893477214 | validation: 3.248413214118391]
	TIME [epoch: 8.32 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1911442738662354		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 3.1911442738662354 | validation: 3.2681020504913576]
	TIME [epoch: 8.37 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2146137563155923		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 3.2146137563155923 | validation: 3.20617574017874]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.182654865826472		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 3.182654865826472 | validation: 3.263329537454508]
	TIME [epoch: 8.66 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088120807100937		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 3.2088120807100937 | validation: 3.2640197643372684]
	TIME [epoch: 8.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2189213329188364		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 3.2189213329188364 | validation: 3.2504906374700493]
	TIME [epoch: 8.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209897226570166		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 3.209897226570166 | validation: 3.2223898939011297]
	TIME [epoch: 8.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101548866049323		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 3.2101548866049323 | validation: 3.2509498229440505]
	TIME [epoch: 8.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252001584309428		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 3.252001584309428 | validation: 3.230847082830339]
	TIME [epoch: 8.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218806230658446		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 3.218806230658446 | validation: 3.2163563133957114]
	TIME [epoch: 8.34 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2181562673921253		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 3.2181562673921253 | validation: 3.227105108784682]
	TIME [epoch: 8.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1880253684411834		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 3.1880253684411834 | validation: 3.219116751640934]
	TIME [epoch: 8.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.194929604394444		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 3.194929604394444 | validation: 3.233387461637665]
	TIME [epoch: 8.37 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2451664800323083		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 3.2451664800323083 | validation: 3.226357798861122]
	TIME [epoch: 8.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2012223476676342		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 3.2012223476676342 | validation: 3.2749855169328526]
	TIME [epoch: 8.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4772135226870478		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 3.4772135226870478 | validation: 3.3019599859452624]
	TIME [epoch: 8.33 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274023389021259		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 3.274023389021259 | validation: 3.276111107261762]
	TIME [epoch: 8.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247236381217684		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 3.247236381217684 | validation: 3.267875654058738]
	TIME [epoch: 8.34 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2736030071334383		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 3.2736030071334383 | validation: 3.353654685568576]
	TIME [epoch: 8.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3723587300693194		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 3.3723587300693194 | validation: 3.385699839577794]
	TIME [epoch: 8.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5881151908733155		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 3.5881151908733155 | validation: 3.6385164923687023]
	TIME [epoch: 8.33 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852865328892295		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 3.852865328892295 | validation: 3.61206761871829]
	TIME [epoch: 8.34 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.818232913546284		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 3.818232913546284 | validation: 3.590177774681668]
	TIME [epoch: 8.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8317965690004776		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 3.8317965690004776 | validation: 3.8213275235964312]
	TIME [epoch: 8.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.247032555866889		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 4.247032555866889 | validation: 3.9651717510949442]
	TIME [epoch: 8.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.96597808328562		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 3.96597808328562 | validation: 3.577581015615218]
	TIME [epoch: 8.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8257370949131406		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 3.8257370949131406 | validation: 3.6667933455384625]
	TIME [epoch: 8.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1030930373981835		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 4.1030930373981835 | validation: 4.47800830450983]
	TIME [epoch: 8.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.603636844624042		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 4.603636844624042 | validation: 4.603756419564969]
	TIME [epoch: 8.34 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.645125445945206		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 4.645125445945206 | validation: 4.596050198776881]
	TIME [epoch: 8.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6222054622379805		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 4.6222054622379805 | validation: 4.521281031841747]
	TIME [epoch: 8.37 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.559982721931277		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 4.559982721931277 | validation: 4.457723771353256]
	TIME [epoch: 8.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.497640032015296		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 4.497640032015296 | validation: 4.262592866536808]
	TIME [epoch: 8.34 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.328542971320078		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 4.328542971320078 | validation: 3.9034285698466507]
	TIME [epoch: 8.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131386526255925		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 4.131386526255925 | validation: 3.764743068811555]
	TIME [epoch: 8.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8741787358827793		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 3.8741787358827793 | validation: 3.342090993360708]
	TIME [epoch: 8.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2857458532608397		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 3.2857458532608397 | validation: 3.2569603767887685]
	TIME [epoch: 8.34 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2349570029777945		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 3.2349570029777945 | validation: 3.250497518336676]
	TIME [epoch: 8.34 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230933518156848		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 3.230933518156848 | validation: 3.2481047554827596]
	TIME [epoch: 8.34 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225471010404504		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 3.225471010404504 | validation: 3.2433267586152605]
	TIME [epoch: 8.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2518555362274606		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 3.2518555362274606 | validation: 3.309805080549684]
	TIME [epoch: 8.37 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2282153117463968		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 3.2282153117463968 | validation: 3.260467375874481]
	TIME [epoch: 8.37 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247205196876185		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 3.247205196876185 | validation: 3.262277256916649]
	TIME [epoch: 8.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2418230654705		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 3.2418230654705 | validation: 3.2697391623192615]
	TIME [epoch: 8.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2253572575054568		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 3.2253572575054568 | validation: 3.238079485837214]
	TIME [epoch: 8.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.267202796128409		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 3.267202796128409 | validation: 3.3302568650670734]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3003970736506703		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 3.3003970736506703 | validation: 3.324005126117268]
	TIME [epoch: 8.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257750867392126		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 3.257750867392126 | validation: 3.2410888796959356]
	TIME [epoch: 8.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20911831285177		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 3.20911831285177 | validation: 3.2233542375022406]
	TIME [epoch: 8.32 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.199987313489518		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 3.199987313489518 | validation: 3.229546525063871]
	TIME [epoch: 8.33 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.18851723375636		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 3.18851723375636 | validation: 3.2176873139399467]
	TIME [epoch: 8.32 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29780578210843		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 3.29780578210843 | validation: 3.3923107120283853]
	TIME [epoch: 8.33 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366123416051669		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 3.366123416051669 | validation: 3.30411081098703]
	TIME [epoch: 8.37 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2858318951818495		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 3.2858318951818495 | validation: 3.27722793723889]
	TIME [epoch: 8.33 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2528875437062617		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 3.2528875437062617 | validation: 3.2523540163706244]
	TIME [epoch: 8.34 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2509251861907105		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 3.2509251861907105 | validation: 3.2391480217424267]
	TIME [epoch: 8.33 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2209707594394024		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 3.2209707594394024 | validation: 3.2276784509854135]
	TIME [epoch: 8.32 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.234202166006584		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 3.234202166006584 | validation: 3.247765421571623]
	TIME [epoch: 8.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226338361393153		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 3.226338361393153 | validation: 3.235631097724477]
	TIME [epoch: 8.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2148855561953527		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 3.2148855561953527 | validation: 3.2435649477914374]
	TIME [epoch: 8.33 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224903698070556		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 3.224903698070556 | validation: 3.2393532746254694]
	TIME [epoch: 8.33 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342002921858681		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 3.342002921858681 | validation: 3.302978545553109]
	TIME [epoch: 8.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274067583833557		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 3.274067583833557 | validation: 3.2355245219213495]
	TIME [epoch: 8.32 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2205256990161812		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 3.2205256990161812 | validation: 3.245619513364904]
	TIME [epoch: 8.37 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240890195591275		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 3.240890195591275 | validation: 3.293818828320581]
	TIME [epoch: 8.33 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2340250087438753		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 3.2340250087438753 | validation: 3.2421669691600195]
	TIME [epoch: 8.33 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222312440721521		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 3.222312440721521 | validation: 3.2340295956006946]
	TIME [epoch: 8.33 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2158389792271085		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 3.2158389792271085 | validation: 3.3165604230714774]
	TIME [epoch: 8.32 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237067084471583		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 3.237067084471583 | validation: 3.233262534731089]
	TIME [epoch: 8.33 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2290468045625516		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 3.2290468045625516 | validation: 3.229907438912282]
	TIME [epoch: 8.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2253364765495087		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 3.2253364765495087 | validation: 3.2604305245039926]
	TIME [epoch: 8.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2203662793887595		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 3.2203662793887595 | validation: 3.3225327061995493]
	TIME [epoch: 8.33 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2501747120772455		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 3.2501747120772455 | validation: 3.212762897151534]
	TIME [epoch: 8.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1928191312837986		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 3.1928191312837986 | validation: 3.2620134013924416]
	TIME [epoch: 8.31 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2536951401856564		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 3.2536951401856564 | validation: 3.275907022338172]
	TIME [epoch: 8.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2304831449961906		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 3.2304831449961906 | validation: 3.260912423380308]
	TIME [epoch: 8.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2942927450795945		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 3.2942927450795945 | validation: 3.3695740869021815]
	TIME [epoch: 8.33 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4503049895277456		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 3.4503049895277456 | validation: 3.5563125528797457]
	TIME [epoch: 8.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4553740678417038		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 3.4553740678417038 | validation: 3.3587671960991425]
	TIME [epoch: 8.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.399567276104136		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 3.399567276104136 | validation: 3.3512727321707576]
	TIME [epoch: 8.33 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3676920900623584		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 3.3676920900623584 | validation: 3.324162449001704]
	TIME [epoch: 8.37 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3466521090720653		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 3.3466521090720653 | validation: 3.3114482478846874]
	TIME [epoch: 8.32 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3167646746670356		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 3.3167646746670356 | validation: 3.2894107909472607]
	TIME [epoch: 8.32 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2920347815803366		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 3.2920347815803366 | validation: 3.384714744435535]
	TIME [epoch: 8.33 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.472472779503243		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 3.472472779503243 | validation: 3.512451415672136]
	TIME [epoch: 8.32 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3609548566646836		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 3.3609548566646836 | validation: 3.330642840731756]
	TIME [epoch: 8.33 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.27172327342777		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 3.27172327342777 | validation: 3.2823225051551135]
	TIME [epoch: 8.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2519483314854813		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 3.2519483314854813 | validation: 3.3064388063181696]
	TIME [epoch: 8.32 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238923746932479		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 3.238923746932479 | validation: 3.27568476619423]
	TIME [epoch: 8.32 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2317007436204293		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 3.2317007436204293 | validation: 3.254685614477343]
	TIME [epoch: 8.33 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219726778462136		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 3.219726778462136 | validation: 3.2463933251907844]
	TIME [epoch: 8.32 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226475790532638		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 3.226475790532638 | validation: 3.2334155501069874]
	TIME [epoch: 8.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2128890798357927		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 3.2128890798357927 | validation: 3.2598424915085795]
	TIME [epoch: 8.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228453806847437		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 3.228453806847437 | validation: 3.267329684869897]
	TIME [epoch: 8.32 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2206989710553415		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 3.2206989710553415 | validation: 3.235508778471468]
	TIME [epoch: 8.33 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4581055913268592		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 3.4581055913268592 | validation: 3.7942674302189654]
	TIME [epoch: 8.32 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4910936114923605		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 3.4910936114923605 | validation: 3.7128497347217335]
	TIME [epoch: 8.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4485997086543447		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 3.4485997086543447 | validation: 3.6287362127294838]
	TIME [epoch: 8.37 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427634118732676		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 3.427634118732676 | validation: 3.7351496804680675]
	TIME [epoch: 8.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.41934719753127		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 3.41934719753127 | validation: 3.4286916720037817]
	TIME [epoch: 8.32 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271035299920453		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 3.271035299920453 | validation: 3.2603894202492163]
	TIME [epoch: 8.31 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230783653656374		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 3.230783653656374 | validation: 3.2807494410903963]
	TIME [epoch: 8.32 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2340002847951563		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 3.2340002847951563 | validation: 3.2478601615731835]
	TIME [epoch: 8.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214520989487374		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 3.214520989487374 | validation: 3.2390056665181435]
	TIME [epoch: 8.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236334248830434		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 3.236334248830434 | validation: 3.277325240849945]
	TIME [epoch: 8.31 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221872678763252		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 3.221872678763252 | validation: 3.3053116355612775]
	TIME [epoch: 8.32 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2405378210504474		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 3.2405378210504474 | validation: 3.242955156500246]
	TIME [epoch: 8.32 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2333858925484957		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 3.2333858925484957 | validation: 3.2403609440172536]
	TIME [epoch: 8.32 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225487133768556		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 3.225487133768556 | validation: 3.229345948537019]
	TIME [epoch: 8.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105540069049145		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 3.2105540069049145 | validation: 3.2290091586005576]
	TIME [epoch: 8.34 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2240181385854716		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 3.2240181385854716 | validation: 3.2506458069714474]
	TIME [epoch: 8.31 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223550987817605		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 3.223550987817605 | validation: 3.226162083934393]
	TIME [epoch: 8.32 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2130927147675177		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 3.2130927147675177 | validation: 3.354684038589223]
	TIME [epoch: 8.32 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.464614591981318		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 3.464614591981318 | validation: 3.3457493009368484]
	TIME [epoch: 8.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376315146589113		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 3.376315146589113 | validation: 3.595728962297452]
	TIME [epoch: 8.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.968855911667254		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 3.968855911667254 | validation: 4.302023570197033]
	TIME [epoch: 8.32 sec]
EPOCH 419/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 5.152162941997739		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 5.152162941997739 | validation: 6.3196889697313114]
	TIME [epoch: 103 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.867953176585253		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 5.867953176585253 | validation: 5.500064182057333]
	TIME [epoch: 8.41 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.313226169857865		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 5.313226169857865 | validation: 5.380068967881133]
	TIME [epoch: 8.34 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.349730014836212		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 5.349730014836212 | validation: 5.248156845444406]
	TIME [epoch: 8.32 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.268169840054643		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 5.268169840054643 | validation: 5.0937455000263245]
	TIME [epoch: 8.32 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.172537109177441		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 5.172537109177441 | validation: 5.012685428670821]
	TIME [epoch: 8.31 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.088348414512167		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 5.088348414512167 | validation: 4.950637432733988]
	TIME [epoch: 8.32 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.204806950150703		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 5.204806950150703 | validation: 5.014539452411617]
	TIME [epoch: 8.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.143133225493438		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 5.143133225493438 | validation: 4.822083848039305]
	TIME [epoch: 8.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.043466133569497		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 5.043466133569497 | validation: 4.8054068231735805]
	TIME [epoch: 8.32 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.93181847365752		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 4.93181847365752 | validation: 4.675497367311461]
	TIME [epoch: 8.31 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.86256511659031		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 4.86256511659031 | validation: 4.655407378879507]
	TIME [epoch: 8.31 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.848357080613762		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 4.848357080613762 | validation: 4.691170257368828]
	TIME [epoch: 8.32 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.853429850562582		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 4.853429850562582 | validation: 4.629453173102693]
	TIME [epoch: 8.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7864658328220315		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 4.7864658328220315 | validation: 4.721913757074827]
	TIME [epoch: 8.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.230264331407494		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 5.230264331407494 | validation: 4.760076444627336]
	TIME [epoch: 8.32 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.786208555063526		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 4.786208555063526 | validation: 4.436408424680458]
	TIME [epoch: 8.32 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.825239981454752		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 4.825239981454752 | validation: 4.489116525145409]
	TIME [epoch: 8.32 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.715733152291926		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 4.715733152291926 | validation: 4.45288347230713]
	TIME [epoch: 8.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.666205350756401		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 4.666205350756401 | validation: 4.565177157868211]
	TIME [epoch: 8.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.681747797956957		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 4.681747797956957 | validation: 4.626631885173465]
	TIME [epoch: 8.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.729258749359312		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 4.729258749359312 | validation: 4.645278373412347]
	TIME [epoch: 8.32 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7074407161540055		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 4.7074407161540055 | validation: 4.475818132682081]
	TIME [epoch: 8.31 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.694242486148683		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 4.694242486148683 | validation: 4.479974519494903]
	TIME [epoch: 8.32 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.659048789989064		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 4.659048789989064 | validation: 4.355024294218433]
	TIME [epoch: 8.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.574524998208383		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 4.574524998208383 | validation: 4.328322936659425]
	TIME [epoch: 8.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.557360684506787		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 4.557360684506787 | validation: 4.342958537936305]
	TIME [epoch: 8.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.529069394828443		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 4.529069394828443 | validation: 4.2144448738560225]
	TIME [epoch: 8.32 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.393329516002654		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 4.393329516002654 | validation: 4.148051151843228]
	TIME [epoch: 8.33 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.319410397500478		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 4.319410397500478 | validation: 4.028426710253022]
	TIME [epoch: 8.33 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.179979583277055		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 4.179979583277055 | validation: 4.034717399829423]
	TIME [epoch: 8.33 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127046969197476		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 4.127046969197476 | validation: 4.008075146701886]
	TIME [epoch: 8.34 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2599523367051555		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 4.2599523367051555 | validation: 4.353102209339104]
	TIME [epoch: 8.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21440599431477		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 4.21440599431477 | validation: 4.336073043054352]
	TIME [epoch: 8.32 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.238041577323108		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 4.238041577323108 | validation: 4.201955693483978]
	TIME [epoch: 8.32 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.314731156306443		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 4.314731156306443 | validation: 4.05633174770089]
	TIME [epoch: 8.32 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139581209849006		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 4.139581209849006 | validation: 3.9187271859073007]
	TIME [epoch: 8.32 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.984561819280015		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 3.984561819280015 | validation: 3.821885342692326]
	TIME [epoch: 8.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9244088430148225		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 3.9244088430148225 | validation: 3.769957410241687]
	TIME [epoch: 8.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9181244476187667		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 3.9181244476187667 | validation: 3.799773277715583]
	TIME [epoch: 8.33 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9067816336143495		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 3.9067816336143495 | validation: 3.8042385008193333]
	TIME [epoch: 8.32 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.934371728089233		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 3.934371728089233 | validation: 3.820965140604098]
	TIME [epoch: 8.33 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923886230649063		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 3.923886230649063 | validation: 3.827157020934407]
	TIME [epoch: 8.32 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.979399608179395		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 3.979399608179395 | validation: 3.763282415882691]
	TIME [epoch: 8.34 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.865260199448713		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 3.865260199448713 | validation: 3.7636312526023312]
	TIME [epoch: 8.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9033292564402506		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 3.9033292564402506 | validation: 3.835932427790684]
	TIME [epoch: 8.32 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9294616310864963		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 3.9294616310864963 | validation: 3.8135547283745677]
	TIME [epoch: 8.32 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9594345661990387		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 3.9594345661990387 | validation: 3.923538213065754]
	TIME [epoch: 8.32 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.997973042361704		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 3.997973042361704 | validation: 3.9254414215599374]
	TIME [epoch: 8.32 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.99748332436951		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 3.99748332436951 | validation: 3.927815946023996]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.026099320321858		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 4.026099320321858 | validation: 3.928260972375395]
	TIME [epoch: 8.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.027410729177726		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 4.027410729177726 | validation: 3.9207670121163436]
	TIME [epoch: 8.32 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.98684991360152		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 3.98684991360152 | validation: 3.889068499818843]
	TIME [epoch: 8.32 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.972012440848188		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 3.972012440848188 | validation: 3.890220430416756]
	TIME [epoch: 8.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9854853915889183		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 3.9854853915889183 | validation: 3.9175794031163607]
	TIME [epoch: 8.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9860607387122684		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 3.9860607387122684 | validation: 3.9005519846686667]
	TIME [epoch: 8.32 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.963683313299031		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 3.963683313299031 | validation: 3.883408499327188]
	TIME [epoch: 8.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.953085047541694		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 3.953085047541694 | validation: 3.8876824120142723]
	TIME [epoch: 8.34 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.948425842483903		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 3.948425842483903 | validation: 3.874816409369009]
	TIME [epoch: 8.32 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.892523866081884		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 3.892523866081884 | validation: 3.7524081687331137]
	TIME [epoch: 8.33 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7692731976858824		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 3.7692731976858824 | validation: 3.7138347645418293]
	TIME [epoch: 8.32 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7683674788729586		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 3.7683674788729586 | validation: 3.6821156071806493]
	TIME [epoch: 8.33 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7146531806991625		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 3.7146531806991625 | validation: 3.7206756899863205]
	TIME [epoch: 8.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7666194326561184		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 3.7666194326561184 | validation: 3.9544559781999338]
	TIME [epoch: 8.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9450525377392287		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 3.9450525377392287 | validation: 3.869825267650525]
	TIME [epoch: 8.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8133503119537075		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 3.8133503119537075 | validation: 3.78856274359737]
	TIME [epoch: 8.32 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7937838798032226		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 3.7937838798032226 | validation: 3.735189465982051]
	TIME [epoch: 8.32 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7379648292084697		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 3.7379648292084697 | validation: 3.644338197297924]
	TIME [epoch: 8.33 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6414030378872013		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 3.6414030378872013 | validation: 3.6189968762057116]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.617187592818887		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 3.617187592818887 | validation: 3.598747687566548]
	TIME [epoch: 8.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5846013428572157		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 3.5846013428572157 | validation: 3.5715651977229337]
	TIME [epoch: 8.32 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5630983919938926		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 3.5630983919938926 | validation: 3.5568255083647067]
	TIME [epoch: 8.33 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.556564244516959		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 3.556564244516959 | validation: 3.673050661991999]
	TIME [epoch: 8.33 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6882686056674		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 3.6882686056674 | validation: 3.61471143593892]
	TIME [epoch: 8.33 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.555632262143062		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 3.555632262143062 | validation: 3.5236487153758214]
	TIME [epoch: 8.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5056603317648545		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 3.5056603317648545 | validation: 3.5379625736381675]
	TIME [epoch: 8.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.509263204867955		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 3.509263204867955 | validation: 3.5626457377247185]
	TIME [epoch: 8.32 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5394774759872645		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 3.5394774759872645 | validation: 3.5747884483811863]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5328448813543054		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 3.5328448813543054 | validation: 3.5430514432955174]
	TIME [epoch: 8.33 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.493497260611454		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 3.493497260611454 | validation: 3.5058783337053896]
	TIME [epoch: 8.32 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5121331128446545		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 3.5121331128446545 | validation: 3.5830185659356015]
	TIME [epoch: 8.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5348468844577443		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 3.5348468844577443 | validation: 3.564630742004269]
	TIME [epoch: 8.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5459784059942128		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 3.5459784059942128 | validation: 3.5732354747038206]
	TIME [epoch: 8.33 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.53483257539577		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 3.53483257539577 | validation: 3.529574703548496]
	TIME [epoch: 8.33 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5026181012227973		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 3.5026181012227973 | validation: 3.510313605083417]
	TIME [epoch: 8.32 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4785831527011535		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 3.4785831527011535 | validation: 3.482750938233978]
	TIME [epoch: 8.33 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.456063746634496		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 3.456063746634496 | validation: 3.4654134621064134]
	TIME [epoch: 8.35 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4341552811522753		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 3.4341552811522753 | validation: 3.4459473086382584]
	TIME [epoch: 8.36 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4063699677526107		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 3.4063699677526107 | validation: 3.411647388166715]
	TIME [epoch: 8.33 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3845935604601376		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 3.3845935604601376 | validation: 3.389342227969]
	TIME [epoch: 8.33 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.365577532370753		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 3.365577532370753 | validation: 3.4108263043103406]
	TIME [epoch: 8.32 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354264139240615		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 3.354264139240615 | validation: 3.364830578086093]
	TIME [epoch: 8.32 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330105156842226		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 3.330105156842226 | validation: 3.3469480979538444]
	TIME [epoch: 8.34 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.314048646732119		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 3.314048646732119 | validation: 3.3322563744598437]
	TIME [epoch: 8.36 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302127194034559		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 3.302127194034559 | validation: 3.32225986671455]
	TIME [epoch: 8.32 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2965418575388794		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 3.2965418575388794 | validation: 3.311845709283954]
	TIME [epoch: 8.32 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.290062067658047		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 3.290062067658047 | validation: 3.3076451946409966]
	TIME [epoch: 8.31 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2781562677187694		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 3.2781562677187694 | validation: 3.2989876447469424]
	TIME [epoch: 8.32 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263061026951269		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 3.263061026951269 | validation: 3.2882957384916773]
	TIME [epoch: 8.33 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2612658728410455		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 3.2612658728410455 | validation: 3.2923210768763]
	TIME [epoch: 8.36 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2529746428676938		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 3.2529746428676938 | validation: 3.2768922778393548]
	TIME [epoch: 8.33 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2427066682357726		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 3.2427066682357726 | validation: 3.2606001293753137]
	TIME [epoch: 8.32 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2359923427711736		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 3.2359923427711736 | validation: 3.2510399276389537]
	TIME [epoch: 8.33 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2288088317341472		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 3.2288088317341472 | validation: 3.255812417423934]
	TIME [epoch: 8.33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2288154099565656		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 3.2288154099565656 | validation: 3.2425390405135373]
	TIME [epoch: 8.33 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2363250757660644		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 3.2363250757660644 | validation: 3.260593738889063]
	TIME [epoch: 8.35 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2286659491976737		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 3.2286659491976737 | validation: 3.229123373693456]
	TIME [epoch: 8.32 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212890749708172		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 3.212890749708172 | validation: 3.23831969780714]
	TIME [epoch: 8.32 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093335679125694		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 3.2093335679125694 | validation: 3.218994998119982]
	TIME [epoch: 8.32 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208491281141752		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 3.208491281141752 | validation: 3.2353379724560307]
	TIME [epoch: 8.32 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1982054635950434		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 3.1982054635950434 | validation: 3.2013337499496943]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196625249578773		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 3.196625249578773 | validation: 3.22453930770055]
	TIME [epoch: 8.36 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1871356223001563		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 3.1871356223001563 | validation: 3.1969661046321445]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1718788376180798		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 3.1718788376180798 | validation: 3.1864599083314173]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.180156098096634		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 3.180156098096634 | validation: 3.1840953947379553]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.163774423352592		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 3.163774423352592 | validation: 3.178415414683591]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.172823149090388		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 3.172823149090388 | validation: 3.1817580938668084]
	TIME [epoch: 8.34 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1625511947642004		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 3.1625511947642004 | validation: 3.17356798717582]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1573371619322668		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 3.1573371619322668 | validation: 3.173720439663948]
	TIME [epoch: 8.32 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166112623227727		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 3.166112623227727 | validation: 3.1861463094818108]
	TIME [epoch: 8.31 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1646207148457046		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 3.1646207148457046 | validation: 3.1702626816560495]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1501031768866556		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 3.1501031768866556 | validation: 3.1640221702554467]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1494815855238585		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 3.1494815855238585 | validation: 3.172118496071688]
	TIME [epoch: 8.34 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.14516220557133		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 3.14516220557133 | validation: 3.1527144815190615]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1429018429375004		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 3.1429018429375004 | validation: 3.153993762421016]
	TIME [epoch: 8.33 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.143143935729208		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 3.143143935729208 | validation: 3.152989770290383]
	TIME [epoch: 8.33 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.151683500426704		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 3.151683500426704 | validation: 3.2634272617041287]
	TIME [epoch: 8.32 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1924595327616094		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 3.1924595327616094 | validation: 3.1857976278033253]
	TIME [epoch: 8.33 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.149047002146092		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 3.149047002146092 | validation: 3.16259164626858]
	TIME [epoch: 8.38 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1993425423806388		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 3.1993425423806388 | validation: 3.2150640321211026]
	TIME [epoch: 8.33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2336692537929723		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 3.2336692537929723 | validation: 3.15251155233749]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.158912998999633		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 3.158912998999633 | validation: 3.179510583331595]
	TIME [epoch: 8.32 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1405145127023033		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 3.1405145127023033 | validation: 3.144057008933765]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.136233805072486		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 3.136233805072486 | validation: 3.1789528775584817]
	TIME [epoch: 8.34 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1780155424856704		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 3.1780155424856704 | validation: 3.1465585977173074]
	TIME [epoch: 8.36 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.135616069306005		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 3.135616069306005 | validation: 3.136552711918015]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240519_152628/states/model_phi1_1a_v_mmd1_fix_noise_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.122818881852614		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 3.122818881852614 | validation: 3.165535304724514]
	TIME [epoch: 8.33 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1661919400063137		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 3.1661919400063137 | validation: 3.149433420966323]
	TIME [epoch: 8.31 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170982991344952		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 3.170982991344952 | validation: 3.229483249416004]
	TIME [epoch: 8.32 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121888777725625		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 3.2121888777725625 | validation: 3.236373469736294]
	TIME [epoch: 8.35 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1832579177431963		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 3.1832579177431963 | validation: 3.1973786452560677]
	TIME [epoch: 8.34 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1951673345488047		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 3.1951673345488047 | validation: 3.1842907927788437]
	TIME [epoch: 8.32 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.16676181790429		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 3.16676181790429 | validation: 3.208656629936992]
	TIME [epoch: 8.33 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1571568209983063		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 3.1571568209983063 | validation: 3.1720737774741092]
	TIME [epoch: 8.32 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1419849678233005		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 3.1419849678233005 | validation: 3.2089481450570294]
	TIME [epoch: 8.32 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1751560843616113		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 3.1751560843616113 | validation: 3.20881324601529]
	TIME [epoch: 8.36 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.160347671952527		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 3.160347671952527 | validation: 3.1751448032820226]
	TIME [epoch: 8.33 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.134426667433184		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 3.134426667433184 | validation: 3.1884710720698677]
	TIME [epoch: 8.32 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1883223761222514		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 3.1883223761222514 | validation: 3.315321317421091]
	TIME [epoch: 8.32 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.430411861558477		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 3.430411861558477 | validation: 3.6273501148762186]
	TIME [epoch: 8.32 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6842699838674324		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 3.6842699838674324 | validation: 3.8996242736891547]
	TIME [epoch: 8.33 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.890868200762431		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 3.890868200762431 | validation: 3.973087888216239]
	TIME [epoch: 8.37 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8405579599034407		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 3.8405579599034407 | validation: 4.017233197521108]
	TIME [epoch: 8.32 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8242302892471374		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 3.8242302892471374 | validation: 3.999217923708522]
	TIME [epoch: 8.32 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719621239634952		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 3.719621239634952 | validation: 3.9686868199421093]
	TIME [epoch: 8.32 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.737800053251755		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 3.737800053251755 | validation: 3.9289608134942267]
	TIME [epoch: 8.32 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.66660470030532		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 3.66660470030532 | validation: 4.048967026395381]
	TIME [epoch: 8.34 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.727233749325812		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 3.727233749325812 | validation: 3.991419927577863]
	TIME [epoch: 8.36 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.680878022870207		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 3.680878022870207 | validation: 4.021286882055068]
	TIME [epoch: 8.32 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7159388283111516		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 3.7159388283111516 | validation: 3.9984791713725465]
	TIME [epoch: 8.31 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6842870166875126		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 3.6842870166875126 | validation: 3.9681036032173918]
	TIME [epoch: 8.31 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.692145656376959		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 3.692145656376959 | validation: 4.012631219284282]
	TIME [epoch: 8.31 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.723687634144458		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 3.723687634144458 | validation: 4.100721649797894]
	TIME [epoch: 8.35 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762498420101295		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 3.762498420101295 | validation: 4.069627848179538]
	TIME [epoch: 8.36 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.708621992265284		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 3.708621992265284 | validation: 3.943300292005743]
	TIME [epoch: 8.32 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7935663410140004		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 3.7935663410140004 | validation: 4.176371846771214]
	TIME [epoch: 8.32 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161005305351795		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 4.161005305351795 | validation: 4.1915230097053096]
	TIME [epoch: 8.32 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170145397064196		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 4.170145397064196 | validation: 4.197011677557404]
	TIME [epoch: 8.31 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147049919794476		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 4.147049919794476 | validation: 4.050743755704547]
	TIME [epoch: 8.35 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.088354216828034		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 4.088354216828034 | validation: 4.1017897692236]
	TIME [epoch: 8.34 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.074830423498483		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 4.074830423498483 | validation: 4.029374889340499]
	TIME [epoch: 8.32 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9796676602966046		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 3.9796676602966046 | validation: 3.8326036142323567]
	TIME [epoch: 8.33 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7927760855154924		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 3.7927760855154924 | validation: 3.812396150705931]
	TIME [epoch: 8.33 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7943488982877542		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 3.7943488982877542 | validation: 3.8383342732225385]
	TIME [epoch: 8.32 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8652984662735896		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 3.8652984662735896 | validation: 3.9148269345157445]
	TIME [epoch: 8.35 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9701065193152543		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 3.9701065193152543 | validation: 3.9414236407475514]
	TIME [epoch: 8.34 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.925980110096662		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 3.925980110096662 | validation: 3.893476227305186]
	TIME [epoch: 8.32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840714850661637		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 3.840714850661637 | validation: 3.8266709017842846]
	TIME [epoch: 8.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.810825412762182		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 3.810825412762182 | validation: 3.7592050449216745]
	TIME [epoch: 8.32 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7553068164032375		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 3.7553068164032375 | validation: 3.667625684420562]
	TIME [epoch: 8.32 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7051245971759457		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 3.7051245971759457 | validation: 3.691388239122203]
	TIME [epoch: 8.36 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681579902648663		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 3.681579902648663 | validation: 3.6376375542425112]
	TIME [epoch: 8.32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.61590032012552		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 3.61590032012552 | validation: 3.68237206926044]
	TIME [epoch: 8.31 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6356457024835764		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 3.6356457024835764 | validation: 3.7374909233095144]
	TIME [epoch: 8.32 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6989133262718963		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 3.6989133262718963 | validation: 3.769622099391575]
	TIME [epoch: 8.32 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7122632939538347		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 3.7122632939538347 | validation: 3.7835753138936523]
	TIME [epoch: 8.33 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.747913373338054		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 3.747913373338054 | validation: 3.81720495459079]
	TIME [epoch: 8.36 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.808139064523299		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 3.808139064523299 | validation: 3.763791540165645]
	TIME [epoch: 8.32 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6814384005556757		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 3.6814384005556757 | validation: 3.6890738860320305]
	TIME [epoch: 8.32 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6628062197333744		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 3.6628062197333744 | validation: 3.8021903796928993]
	TIME [epoch: 8.31 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716958930424858		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 3.716958930424858 | validation: 3.8734043429650535]
	TIME [epoch: 8.31 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716478138333022		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 3.716478138333022 | validation: 3.928037958120557]
	TIME [epoch: 8.33 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7659418522929844		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 3.7659418522929844 | validation: 4.0395173932343384]
	TIME [epoch: 8.35 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.847616018270827		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 3.847616018270827 | validation: 4.083460288708376]
	TIME [epoch: 8.32 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.877573180931801		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 3.877573180931801 | validation: 4.121852316445068]
	TIME [epoch: 8.31 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9819900683775797		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 3.9819900683775797 | validation: 4.301365497037683]
	TIME [epoch: 8.31 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.048932911798893		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 4.048932911798893 | validation: 4.264677418627323]
	TIME [epoch: 8.32 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.028398044007304		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 4.028398044007304 | validation: 4.249503973639939]
	TIME [epoch: 8.34 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.049829033324115		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 4.049829033324115 | validation: 4.348554875595058]
	TIME [epoch: 8.35 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143335517841967		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 4.143335517841967 | validation: 4.493374789236325]
	TIME [epoch: 8.33 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.211361628422261		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 4.211361628422261 | validation: 4.532616260320484]
	TIME [epoch: 8.32 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285321505786968		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 4.285321505786968 | validation: 4.62418686789464]
	TIME [epoch: 8.32 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.319133992197271		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 4.319133992197271 | validation: 4.535860893890545]
	TIME [epoch: 8.33 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.250006847884309		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 4.250006847884309 | validation: 4.525384710822222]
	TIME [epoch: 8.36 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225422520372146		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 4.225422520372146 | validation: 4.4279112174554935]
	TIME [epoch: 8.33 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195182060432259		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 4.195182060432259 | validation: 4.552865878793025]
	TIME [epoch: 8.32 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.233262794110713		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 4.233262794110713 | validation: 4.508343167036305]
	TIME [epoch: 8.32 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.196266315380813		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 4.196266315380813 | validation: 4.405850012646749]
	TIME [epoch: 8.31 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147526395107772		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 4.147526395107772 | validation: 4.427039349883385]
	TIME [epoch: 8.33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1577286079287905		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 4.1577286079287905 | validation: 4.352476331063128]
	TIME [epoch: 8.36 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.083024109318552		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 4.083024109318552 | validation: 4.273633541250748]
	TIME [epoch: 8.32 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.066120419459268		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 4.066120419459268 | validation: 4.188642269149031]
	TIME [epoch: 8.32 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.99854757330464		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 3.99854757330464 | validation: 4.158597037507395]
	TIME [epoch: 8.32 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002529733232102		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 4.002529733232102 | validation: 4.1822141507262405]
	TIME [epoch: 8.31 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9828303234895888		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 3.9828303234895888 | validation: 4.031979431197512]
	TIME [epoch: 8.34 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.880293721620074		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 3.880293721620074 | validation: 4.015338265558103]
	TIME [epoch: 8.36 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9040844286228364		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 3.9040844286228364 | validation: 4.08643450349344]
	TIME [epoch: 8.31 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944064900546499		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 3.944064900546499 | validation: 4.169053382099552]
	TIME [epoch: 8.31 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0507490342545704		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 4.0507490342545704 | validation: 4.289924212808177]
	TIME [epoch: 8.31 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12108544075405		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 4.12108544075405 | validation: 4.360688299003298]
	TIME [epoch: 8.31 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.118012587659741		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 4.118012587659741 | validation: 4.320143166154647]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13288994535947		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 4.13288994535947 | validation: 4.275856059408493]
	TIME [epoch: 8.35 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.083002575939496		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 4.083002575939496 | validation: 4.314064983258858]
	TIME [epoch: 8.32 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140708097492015		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 4.140708097492015 | validation: 4.405054706753141]
	TIME [epoch: 8.32 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22786387832753		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 4.22786387832753 | validation: 4.520522138816653]
	TIME [epoch: 8.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268170455259385		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 4.268170455259385 | validation: 4.496686722235931]
	TIME [epoch: 8.32 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.243504874915004		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 4.243504874915004 | validation: 4.386483075954143]
	TIME [epoch: 8.35 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.092378953759605		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 4.092378953759605 | validation: 4.285518380793132]
	TIME [epoch: 8.35 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.05498487523732		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 4.05498487523732 | validation: 4.242529090716249]
	TIME [epoch: 8.32 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.027201772665428		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 4.027201772665428 | validation: 4.127683950882783]
	TIME [epoch: 8.32 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.953274705115834		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 3.953274705115834 | validation: 4.130389729210053]
	TIME [epoch: 8.31 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9587237923064023		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 3.9587237923064023 | validation: 4.106063110160548]
	TIME [epoch: 8.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8764103757654835		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 3.8764103757654835 | validation: 3.9694914642079775]
	TIME [epoch: 8.34 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7706900137287436		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 3.7706900137287436 | validation: 3.941801714947978]
	TIME [epoch: 8.34 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7396832392634254		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 3.7396832392634254 | validation: 3.8413109553478266]
	TIME [epoch: 8.32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6703520922480033		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 3.6703520922480033 | validation: 3.8786963572717807]
	TIME [epoch: 8.31 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.714800306944969		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 3.714800306944969 | validation: 3.914588671350952]
	TIME [epoch: 8.32 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7237121560204685		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 3.7237121560204685 | validation: 3.898154741628413]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7515096741107095		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 3.7515096741107095 | validation: 3.9698578532233277]
	TIME [epoch: 8.42 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8045340685827336		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 3.8045340685827336 | validation: 4.083091796480083]
	TIME [epoch: 8.32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8726616397383062		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 3.8726616397383062 | validation: 4.081584620827366]
	TIME [epoch: 8.31 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8469146947879023		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 3.8469146947879023 | validation: 4.05410419913137]
	TIME [epoch: 8.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778725689324092		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 3.778725689324092 | validation: 3.9455931585439696]
	TIME [epoch: 8.32 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.769742952353645		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 3.769742952353645 | validation: 4.051135886564287]
	TIME [epoch: 8.34 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858237617303229		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 3.858237617303229 | validation: 4.1348216496779475]
	TIME [epoch: 8.35 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9021520051940324		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 3.9021520051940324 | validation: 4.100898007628436]
	TIME [epoch: 8.31 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888669857173568		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 3.888669857173568 | validation: 4.072113861292868]
	TIME [epoch: 8.31 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.865177767106194		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 3.865177767106194 | validation: 4.130573626093171]
	TIME [epoch: 8.31 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.880067144349402		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 3.880067144349402 | validation: 4.194172996701315]
	TIME [epoch: 8.31 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9859068634398507		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 3.9859068634398507 | validation: 4.323672574060456]
	TIME [epoch: 8.33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.038218290486696		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 4.038218290486696 | validation: 4.388938803033321]
	TIME [epoch: 8.36 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.094921337460081		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 4.094921337460081 | validation: 4.455058196153081]
	TIME [epoch: 8.32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.112550590840341		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 4.112550590840341 | validation: 4.402983506140817]
	TIME [epoch: 8.31 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141980803491833		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 4.141980803491833 | validation: 4.441139224209074]
	TIME [epoch: 8.32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151645611493729		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 4.151645611493729 | validation: 4.501284054486524]
	TIME [epoch: 8.32 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187696818139367		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 4.187696818139367 | validation: 4.523523105200937]
	TIME [epoch: 8.34 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.188022144331226		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 4.188022144331226 | validation: 4.516634098351792]
	TIME [epoch: 8.35 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131661523172234		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 4.131661523172234 | validation: 4.445420546869381]
	TIME [epoch: 8.32 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0737794241734475		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 4.0737794241734475 | validation: 4.39938420490161]
	TIME [epoch: 8.32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0153730758003		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 4.0153730758003 | validation: 4.295216306121049]
	TIME [epoch: 8.33 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9356320211353477		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 3.9356320211353477 | validation: 4.321371588402834]
	TIME [epoch: 8.32 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9657554471167806		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 3.9657554471167806 | validation: 4.274346875061228]
	TIME [epoch: 8.35 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.929386866678307		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 3.929386866678307 | validation: 4.357633347301321]
	TIME [epoch: 8.34 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9844589353405806		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 3.9844589353405806 | validation: 4.3420858405973775]
	TIME [epoch: 8.32 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9671800424775747		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 3.9671800424775747 | validation: 4.335407086936361]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9619011436547593		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 3.9619011436547593 | validation: 4.354361468172497]
	TIME [epoch: 8.33 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0224339176973105		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 4.0224339176973105 | validation: 4.456558380015133]
	TIME [epoch: 8.32 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.110051519739207		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 4.110051519739207 | validation: 4.439655858130752]
	TIME [epoch: 8.35 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.04972241972523		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 4.04972241972523 | validation: 4.364868919910734]
	TIME [epoch: 8.33 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.94730273514432		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 3.94730273514432 | validation: 4.17820754712503]
	TIME [epoch: 8.32 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8018768281683206		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 3.8018768281683206 | validation: 4.143942323126121]
	TIME [epoch: 8.33 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8102870666470605		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 3.8102870666470605 | validation: 4.17517427261693]
	TIME [epoch: 8.32 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.811553109795543		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 3.811553109795543 | validation: 4.200022717861272]
	TIME [epoch: 8.34 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8493892410509503		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 3.8493892410509503 | validation: 4.191646688151537]
	TIME [epoch: 8.35 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8372395602840847		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 3.8372395602840847 | validation: 4.177122012696248]
	TIME [epoch: 8.33 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8336323411343844		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 3.8336323411343844 | validation: 4.212633800545893]
	TIME [epoch: 8.31 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8513660876479054		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 3.8513660876479054 | validation: 4.192291048523523]
	TIME [epoch: 8.31 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.839606277484352		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 3.839606277484352 | validation: 4.171889573185137]
	TIME [epoch: 8.31 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872179229799607		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 3.872179229799607 | validation: 4.273741907494362]
	TIME [epoch: 8.33 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9230169961629544		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 3.9230169961629544 | validation: 4.223819195079857]
	TIME [epoch: 8.35 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8849815296222823		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 3.8849815296222823 | validation: 4.187609528588919]
	TIME [epoch: 8.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.897347187832777		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 3.897347187832777 | validation: 4.223995108915551]
	TIME [epoch: 8.31 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.943104284503258		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 3.943104284503258 | validation: 4.266532335565115]
	TIME [epoch: 8.31 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.958132860368834		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 3.958132860368834 | validation: 4.154307712272876]
	TIME [epoch: 8.33 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8945576671494657		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 3.8945576671494657 | validation: 4.156292274365761]
	TIME [epoch: 8.37 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840787957122439		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 3.840787957122439 | validation: 4.109962749685848]
	TIME [epoch: 8.35 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831723821080298		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 3.831723821080298 | validation: 4.084651208086868]
	TIME [epoch: 8.34 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8197867137675656		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 3.8197867137675656 | validation: 4.069585631008297]
	TIME [epoch: 8.33 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863913284915251		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 3.863913284915251 | validation: 4.199808897418407]
	TIME [epoch: 8.33 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.96444843562331		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 3.96444843562331 | validation: 4.241646758352148]
	TIME [epoch: 8.33 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0058411185377505		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 4.0058411185377505 | validation: 4.288289739447481]
	TIME [epoch: 8.36 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.018201078088392		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 4.018201078088392 | validation: 4.31265932298251]
	TIME [epoch: 8.36 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.060552903491441		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 4.060552903491441 | validation: 4.250558652684054]
	TIME [epoch: 8.33 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9946022134192467		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 3.9946022134192467 | validation: 4.294444866888437]
	TIME [epoch: 8.32 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016544107802387		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 4.016544107802387 | validation: 4.18657678261634]
	TIME [epoch: 8.32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9444163295232375		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 3.9444163295232375 | validation: 4.163438078222855]
	TIME [epoch: 8.33 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.955874510706201		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 3.955874510706201 | validation: 4.2127267763405385]
	TIME [epoch: 8.38 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077225114426157		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 4.077225114426157 | validation: 4.417597882226315]
	TIME [epoch: 8.33 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187008027717258		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 4.187008027717258 | validation: 4.423723173194422]
	TIME [epoch: 8.33 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.224159907431468		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 4.224159907431468 | validation: 4.388213922172971]
	TIME [epoch: 8.34 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.188398210424329		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 4.188398210424329 | validation: 4.270313499027994]
	TIME [epoch: 8.33 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.120882780404173		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 4.120882780404173 | validation: 4.27501783336718]
	TIME [epoch: 8.34 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153233161047801		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 4.153233161047801 | validation: 4.246609283174152]
	TIME [epoch: 8.36 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195585002706091		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 4.195585002706091 | validation: 4.387623912484119]
	TIME [epoch: 8.34 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.355283110658587		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 4.355283110658587 | validation: 4.499461352127695]
	TIME [epoch: 8.32 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.490504736161124		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 4.490504736161124 | validation: 4.667480168661729]
	TIME [epoch: 8.33 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.580283897348473		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 4.580283897348473 | validation: 4.608047785976018]
	TIME [epoch: 8.32 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.504333866891007		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 4.504333866891007 | validation: 4.532066079366201]
	TIME [epoch: 8.35 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4271360935275785		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 4.4271360935275785 | validation: 4.434113693352208]
	TIME [epoch: 8.36 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.459695368048534		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 4.459695368048534 | validation: 4.546931451796931]
	TIME [epoch: 8.32 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.587819162774214		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 4.587819162774214 | validation: 4.66904525790544]
	TIME [epoch: 8.32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.647379996875391		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 4.647379996875391 | validation: 4.697903266721138]
	TIME [epoch: 8.32 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.669805162772432		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 4.669805162772432 | validation: 4.745712486223518]
	TIME [epoch: 8.32 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.69747742145365		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 4.69747742145365 | validation: 4.779484719702694]
	TIME [epoch: 8.36 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.751166106677319		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 4.751166106677319 | validation: 4.8627864096418865]
	TIME [epoch: 8.34 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8789150703120825		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 4.8789150703120825 | validation: 5.019506784489231]
	TIME [epoch: 8.32 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.059728247218723		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 5.059728247218723 | validation: 5.159271861593466]
	TIME [epoch: 8.32 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.092524057778622		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 5.092524057778622 | validation: 5.075954031932131]
	TIME [epoch: 8.32 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.001007094465201		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 5.001007094465201 | validation: 4.968492707858774]
	TIME [epoch: 8.32 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.968709493661063		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 4.968709493661063 | validation: 4.987373442562589]
	TIME [epoch: 8.35 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.005489994556497		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 5.005489994556497 | validation: 5.025750002565667]
	TIME [epoch: 8.34 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.995560728017656		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 4.995560728017656 | validation: 5.0631339484676054]
	TIME [epoch: 8.32 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.086631484646768		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 5.086631484646768 | validation: 5.130000717395381]
	TIME [epoch: 8.32 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.105352530724951		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 5.105352530724951 | validation: 5.1831640987124645]
	TIME [epoch: 8.31 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.213798812889539		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 5.213798812889539 | validation: 5.300913806882427]
	TIME [epoch: 8.32 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.249575692815047		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 5.249575692815047 | validation: 5.28362175535353]
	TIME [epoch: 8.36 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.233730265182697		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 5.233730265182697 | validation: 5.237026863227119]
	TIME [epoch: 8.32 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.246021231883938		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 5.246021231883938 | validation: 5.3393486714488905]
	TIME [epoch: 8.32 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.289047144757893		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 5.289047144757893 | validation: 5.250923516305589]
	TIME [epoch: 8.31 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.229150170735537		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 5.229150170735537 | validation: 5.338273392933104]
	TIME [epoch: 8.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.293934433623012		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 5.293934433623012 | validation: 5.462016619784274]
	TIME [epoch: 8.33 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.29971308934857		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 5.29971308934857 | validation: 5.398223236848954]
	TIME [epoch: 8.35 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.257202239254944		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 5.257202239254944 | validation: 5.333736196714372]
	TIME [epoch: 8.31 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.204187634824493		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 5.204187634824493 | validation: 5.366782833895161]
	TIME [epoch: 8.32 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.195327981291777		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 5.195327981291777 | validation: 5.35889766434077]
	TIME [epoch: 8.31 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.206798563378459		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 5.206798563378459 | validation: 5.436994016826477]
	TIME [epoch: 8.31 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.254256907887776		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 5.254256907887776 | validation: 5.500076763896541]
	TIME [epoch: 8.34 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.282552122338842		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 5.282552122338842 | validation: 5.5299029239717825]
	TIME [epoch: 8.35 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.252621320097516		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 5.252621320097516 | validation: 5.276499489672176]
	TIME [epoch: 8.31 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9419873620260315		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 4.9419873620260315 | validation: 5.121268101356819]
	TIME [epoch: 8.31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.892089509477947		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 4.892089509477947 | validation: 4.994141587475351]
	TIME [epoch: 8.32 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.721615182623248		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 4.721615182623248 | validation: 4.726554773973329]
	TIME [epoch: 8.33 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.533757726744223		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 4.533757726744223 | validation: 4.704280000382296]
	TIME [epoch: 8.35 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.397362109973544		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 4.397362109973544 | validation: 4.515518040179639]
	TIME [epoch: 8.35 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.322319434554357		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 4.322319434554357 | validation: 4.524704741445897]
	TIME [epoch: 8.31 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.358183362231592		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 4.358183362231592 | validation: 4.530935312324518]
	TIME [epoch: 8.32 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.311791523543874		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 4.311791523543874 | validation: 4.43086619052689]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195226316110027		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 4.195226316110027 | validation: 4.28523675856643]
	TIME [epoch: 8.31 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.088908277785901		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 4.088908277785901 | validation: 4.2842020730610955]
	TIME [epoch: 8.35 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0624335533198055		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 4.0624335533198055 | validation: 4.1582625831511635]
	TIME [epoch: 8.33 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9179368641273387		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 3.9179368641273387 | validation: 4.109095601081824]
	TIME [epoch: 8.31 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.862225338632133		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 3.862225338632133 | validation: 4.015732283198387]
	TIME [epoch: 8.31 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850650718065499		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 3.850650718065499 | validation: 4.114567935803146]
	TIME [epoch: 8.32 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.898562571460996		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 3.898562571460996 | validation: 4.154314520471123]
	TIME [epoch: 8.32 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9399891730490446		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 3.9399891730490446 | validation: 4.139617900414236]
	TIME [epoch: 8.36 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8932340318114047		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 3.8932340318114047 | validation: 4.090706044122227]
	TIME [epoch: 8.32 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.831138636186159		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 3.831138636186159 | validation: 3.9865105949154573]
	TIME [epoch: 8.32 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7399283884571988		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 3.7399283884571988 | validation: 3.9205543824982216]
	TIME [epoch: 8.32 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682548955793395		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 3.682548955793395 | validation: 3.887820864780899]
	TIME [epoch: 8.32 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.655755782765521		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 3.655755782765521 | validation: 3.950686914498224]
	TIME [epoch: 8.33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.618892244871014		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 3.618892244871014 | validation: 3.7928112638736238]
	TIME [epoch: 8.36 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5546136140548934		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 3.5546136140548934 | validation: 3.7687967015468757]
	TIME [epoch: 8.32 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.543089133151404		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 3.543089133151404 | validation: 3.7992364872058015]
	TIME [epoch: 8.32 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.502937233304747		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 3.502937233304747 | validation: 3.7187473055039098]
	TIME [epoch: 8.32 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4802256372094953		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 3.4802256372094953 | validation: 3.701225385054295]
	TIME [epoch: 8.31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4725990227641317		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 3.4725990227641317 | validation: 3.686202918310455]
	TIME [epoch: 8.33 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4547801281693107		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 3.4547801281693107 | validation: 3.67896048235306]
	TIME [epoch: 8.35 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4739637179259577		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 3.4739637179259577 | validation: 3.7486386462369086]
	TIME [epoch: 8.31 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.462058463291032		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 3.462058463291032 | validation: 3.6562087815023325]
	TIME [epoch: 8.32 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4451707132377343		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 3.4451707132377343 | validation: 3.6464262795928906]
	TIME [epoch: 8.32 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4418509474964782		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 3.4418509474964782 | validation: 3.643710219765399]
	TIME [epoch: 8.32 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.434531274239852		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 3.434531274239852 | validation: 3.6432326461528057]
	TIME [epoch: 8.35 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4312646871053274		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 3.4312646871053274 | validation: 3.6304531474544808]
	TIME [epoch: 8.35 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4281768648693407		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 3.4281768648693407 | validation: 3.62288936154897]
	TIME [epoch: 8.32 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427529681091326		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 3.427529681091326 | validation: 3.6348298831232877]
	TIME [epoch: 8.32 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.433554985444696		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 3.433554985444696 | validation: 3.6286928353426116]
	TIME [epoch: 8.31 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.430834768743872		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 3.430834768743872 | validation: 3.6228683367099337]
	TIME [epoch: 8.32 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4370167051060583		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 3.4370167051060583 | validation: 3.669088016265083]
	TIME [epoch: 8.35 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4456950784686335		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 3.4456950784686335 | validation: 3.6097052138392147]
	TIME [epoch: 8.33 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426882307170304		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 3.426882307170304 | validation: 3.6067046991061495]
	TIME [epoch: 8.31 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4256255735545307		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 3.4256255735545307 | validation: 3.612303547666019]
	TIME [epoch: 8.31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.419234640625155		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 3.419234640625155 | validation: 3.580105312091793]
	TIME [epoch: 8.31 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402532104313156		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 3.402532104313156 | validation: 3.556473181827399]
	TIME [epoch: 8.32 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.421641256329716		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 3.421641256329716 | validation: 3.643349956847152]
	TIME [epoch: 8.37 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4284937181834714		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 3.4284937181834714 | validation: 3.6260547520679536]
	TIME [epoch: 8.32 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.41419968230949		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 3.41419968230949 | validation: 3.576253168493735]
	TIME [epoch: 8.31 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.400549540632204		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 3.400549540632204 | validation: 3.583870599503261]
	TIME [epoch: 8.31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.403852363358028		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 3.403852363358028 | validation: 3.5832262117554228]
	TIME [epoch: 8.31 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4383637128136684		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 3.4383637128136684 | validation: 3.691601307891544]
	TIME [epoch: 8.33 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4612895145783664		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 3.4612895145783664 | validation: 3.6830364823718775]
	TIME [epoch: 8.35 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4492915725578244		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 3.4492915725578244 | validation: 3.6133755551068685]
	TIME [epoch: 8.32 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4330949665586226		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 3.4330949665586226 | validation: 3.578634002135442]
	TIME [epoch: 8.32 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.414374441450156		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 3.414374441450156 | validation: 3.575441038823998]
	TIME [epoch: 8.32 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4190866288284605		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 3.4190866288284605 | validation: 3.567467976500927]
	TIME [epoch: 8.31 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.404296574958685		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 3.404296574958685 | validation: 3.571627377075888]
	TIME [epoch: 8.33 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3989415082160868		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 3.3989415082160868 | validation: 3.591466078117113]
	TIME [epoch: 8.35 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394718225827082		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 3.394718225827082 | validation: 3.560242079067004]
	TIME [epoch: 8.31 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.415268262949171		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 3.415268262949171 | validation: 3.6487714433791254]
	TIME [epoch: 8.32 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4186934142639984		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 3.4186934142639984 | validation: 3.6152775687509653]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3887218164243906		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 3.3887218164243906 | validation: 3.5382692444768775]
	TIME [epoch: 8.31 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3793245372196328		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 3.3793245372196328 | validation: 3.538954107909312]
	TIME [epoch: 8.35 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3829268309163933		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 3.3829268309163933 | validation: 3.557154234820329]
	TIME [epoch: 8.34 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.393456934095448		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 3.393456934095448 | validation: 3.5532776179894983]
	TIME [epoch: 8.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.386970022900659		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 3.386970022900659 | validation: 3.5448496308000115]
	TIME [epoch: 8.31 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.382861911335168		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 3.382861911335168 | validation: 3.529258109936574]
	TIME [epoch: 8.31 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375730637940341		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 3.375730637940341 | validation: 3.5247814391195202]
	TIME [epoch: 8.31 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.37628374022734		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 3.37628374022734 | validation: 3.5290856496262757]
	TIME [epoch: 8.35 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375650721231861		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 3.375650721231861 | validation: 3.5231201754323274]
	TIME [epoch: 8.33 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3741107981128464		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 3.3741107981128464 | validation: 3.521861972500389]
	TIME [epoch: 8.32 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373469091175796		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 3.373469091175796 | validation: 3.5283527346059875]
	TIME [epoch: 8.31 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373693913666412		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 3.373693913666412 | validation: 3.5125809831409103]
	TIME [epoch: 8.31 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3767569283586543		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 3.3767569283586543 | validation: 3.5394253397543616]
	TIME [epoch: 8.31 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3883247550753763		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 3.3883247550753763 | validation: 3.6131506473411257]
	TIME [epoch: 8.36 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3930651177446833		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 3.3930651177446833 | validation: 3.533324712442876]
	TIME [epoch: 8.33 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.377279441482345		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 3.377279441482345 | validation: 3.528181966737079]
	TIME [epoch: 8.32 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3744617050964845		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 3.3744617050964845 | validation: 3.5176326568252696]
	TIME [epoch: 8.31 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363918149456414		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 3.363918149456414 | validation: 3.494859211235066]
	TIME [epoch: 8.32 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3592510340109083		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 3.3592510340109083 | validation: 3.4924492415843247]
	TIME [epoch: 8.32 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353845142432718		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 3.353845142432718 | validation: 3.4808580247546956]
	TIME [epoch: 8.35 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352098253762964		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 3.352098253762964 | validation: 3.4866595525133786]
	TIME [epoch: 8.31 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3575865877091307		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 3.3575865877091307 | validation: 3.549765975226217]
	TIME [epoch: 8.32 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3677129139325572		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 3.3677129139325572 | validation: 3.4919522955143507]
	TIME [epoch: 8.32 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3370130276876147		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 3.3370130276876147 | validation: 3.429733342085094]
	TIME [epoch: 8.31 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3177576145597714		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 3.3177576145597714 | validation: 3.3995585519758613]
	TIME [epoch: 8.33 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.296666789671073		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 3.296666789671073 | validation: 3.3596198731103604]
	TIME [epoch: 8.35 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273691823642266		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 3.273691823642266 | validation: 3.2989818863876104]
	TIME [epoch: 8.31 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2354610127520287		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 3.2354610127520287 | validation: 3.2561013813732362]
	TIME [epoch: 8.31 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2147004851230854		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 3.2147004851230854 | validation: 3.2346059952973194]
	TIME [epoch: 8.32 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2132465101524197		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 3.2132465101524197 | validation: 3.241526213530781]
	TIME [epoch: 8.32 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2186320202890726		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 3.2186320202890726 | validation: 3.2316180207914584]
	TIME [epoch: 8.34 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210541800728004		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 3.210541800728004 | validation: 3.2295815745679275]
	TIME [epoch: 8.34 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209888415717047		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 3.209888415717047 | validation: 3.234370657520171]
	TIME [epoch: 8.32 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209173718677218		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 3.209173718677218 | validation: 3.2288890369672814]
	TIME [epoch: 8.32 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205195303189601		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 3.205195303189601 | validation: 3.2152181729626292]
	TIME [epoch: 8.31 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1979611321349886		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 3.1979611321349886 | validation: 3.2084286338838]
	TIME [epoch: 8.32 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1912586287964313		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 3.1912586287964313 | validation: 3.205767183348607]
	TIME [epoch: 8.35 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.189062896008692		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 3.189062896008692 | validation: 3.1989441314809923]
	TIME [epoch: 8.34 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1838286090411594		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 3.1838286090411594 | validation: 3.1937697054001033]
	TIME [epoch: 8.31 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1762683590282483		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 3.1762683590282483 | validation: 3.1831071216071254]
	TIME [epoch: 8.32 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.173315881564396		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 3.173315881564396 | validation: 3.1880172105804165]
	TIME [epoch: 8.32 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1730027012552116		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 3.1730027012552116 | validation: 3.186220084019012]
	TIME [epoch: 8.33 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.179927769742596		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 3.179927769742596 | validation: 3.1931974160273455]
	TIME [epoch: 8.37 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1800367409110457		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 3.1800367409110457 | validation: 3.191497062167005]
	TIME [epoch: 8.34 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1727404169788906		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 3.1727404169788906 | validation: 3.1877487820928425]
	TIME [epoch: 8.39 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.176749836260057		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 3.176749836260057 | validation: 3.190239614564117]
	TIME [epoch: 8.32 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.18032698278179		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 3.18032698278179 | validation: 3.189079822309326]
	TIME [epoch: 8.34 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1779328669192797		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 3.1779328669192797 | validation: 3.202572056063511]
	TIME [epoch: 8.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.18356289532713		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 3.18356289532713 | validation: 3.199773690359225]
	TIME [epoch: 8.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.177996486227988		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 3.177996486227988 | validation: 3.1847111314435224]
	TIME [epoch: 8.32 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.179194577238614		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 3.179194577238614 | validation: 3.2034431313887097]
	TIME [epoch: 8.32 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.178006295785824		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 3.178006295785824 | validation: 3.1909288787186663]
	TIME [epoch: 8.31 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1701718149020346		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 3.1701718149020346 | validation: 3.181149925946865]
	TIME [epoch: 8.31 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1659833878248733		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 3.1659833878248733 | validation: 3.176669216356941]
	TIME [epoch: 8.33 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.162094560554108		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 3.162094560554108 | validation: 3.172502808979323]
	TIME [epoch: 8.39 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1621191855099378		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 3.1621191855099378 | validation: 3.174688461320698]
	TIME [epoch: 8.31 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1624494048254914		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 3.1624494048254914 | validation: 3.17573758380349]
	TIME [epoch: 8.32 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1735265601756733		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 3.1735265601756733 | validation: 3.1843475107857584]
	TIME [epoch: 8.31 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1688715646165684		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 3.1688715646165684 | validation: 3.176599033620265]
	TIME [epoch: 8.32 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.165898164664048		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 3.165898164664048 | validation: 3.1775928174519485]
	TIME [epoch: 8.34 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1624688041639093		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 3.1624688041639093 | validation: 3.1725665896566593]
	TIME [epoch: 8.34 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.156902462010051		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.156902462010051 | validation: 3.167690213908715]
	TIME [epoch: 8.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.154983251658439		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 3.154983251658439 | validation: 3.165103165747084]
	TIME [epoch: 8.32 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1527855936801066		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 3.1527855936801066 | validation: 3.169322207909733]
	TIME [epoch: 8.31 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1808200342301842		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 3.1808200342301842 | validation: 3.2031738997784736]
	TIME [epoch: 8.43 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1778630770646066		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 3.1778630770646066 | validation: 3.1806939682146735]
	TIME [epoch: 8.35 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1563524906371003		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 3.1563524906371003 | validation: 3.1683459254038047]
	TIME [epoch: 8.38 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.15050199625239		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 3.15050199625239 | validation: 3.1620498097223235]
	TIME [epoch: 8.38 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1490425757031755		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 3.1490425757031755 | validation: 3.162598685274089]
	TIME [epoch: 8.34 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1492906962695857		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 3.1492906962695857 | validation: 3.17251436481673]
	TIME [epoch: 8.32 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.160081248813177		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 3.160081248813177 | validation: 3.1800253136566345]
	TIME [epoch: 8.33 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1623460570231363		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 3.1623460570231363 | validation: 3.1870773335300955]
	TIME [epoch: 8.36 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166155701049594		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 3.166155701049594 | validation: 3.187538652083962]
	TIME [epoch: 8.32 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1664653060950476		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 3.1664653060950476 | validation: 3.181642430223108]
	TIME [epoch: 8.31 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1597999754452704		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 3.1597999754452704 | validation: 3.1779666508654163]
	TIME [epoch: 8.31 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1573542063108895		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 3.1573542063108895 | validation: 3.1696541683330492]
	TIME [epoch: 8.31 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.163510193752952		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 3.163510193752952 | validation: 3.176742248197934]
	TIME [epoch: 8.32 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1664199836073874		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 3.1664199836073874 | validation: 3.1836396706807593]
	TIME [epoch: 8.35 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166697408593364		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 3.166697408593364 | validation: 3.1765144680411783]
	TIME [epoch: 8.32 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.165235185665069		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 3.165235185665069 | validation: 3.164111852366434]
	TIME [epoch: 8.31 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1551936476824816		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 3.1551936476824816 | validation: 3.1652306886892196]
	TIME [epoch: 8.32 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.154010351076064		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 3.154010351076064 | validation: 3.1598719981848813]
	TIME [epoch: 8.32 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1455881949565816		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 3.1455881949565816 | validation: 3.1517702448491605]
	TIME [epoch: 8.34 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1429736842746387		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 3.1429736842746387 | validation: 3.1531768906749944]
	TIME [epoch: 8.37 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1441471211435523		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 3.1441471211435523 | validation: 3.153272684606966]
	TIME [epoch: 8.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.145168115411363		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 3.145168115411363 | validation: 3.155986979322231]
	TIME [epoch: 8.31 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1446260495539837		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 3.1446260495539837 | validation: 3.149105174965001]
	TIME [epoch: 8.37 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.14078945568488		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 3.14078945568488 | validation: 3.1504410938780403]
	TIME [epoch: 8.39 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1385347678092916		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 3.1385347678092916 | validation: 3.149041356638522]
	TIME [epoch: 8.36 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1441057909107286		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 3.1441057909107286 | validation: 3.175162050020511]
	TIME [epoch: 8.34 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1526616951542863		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 3.1526616951542863 | validation: 3.1788980639604305]
	TIME [epoch: 8.32 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1613059190955783		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 3.1613059190955783 | validation: 3.1803353607973275]
	TIME [epoch: 8.32 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1477806312840606		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 3.1477806312840606 | validation: 3.1519014553233844]
	TIME [epoch: 8.31 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.141642414540278		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 3.141642414540278 | validation: 3.1494993890572474]
	TIME [epoch: 8.32 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1413236827528825		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 3.1413236827528825 | validation: 3.1517990517332306]
	TIME [epoch: 8.35 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1414210539490757		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 3.1414210539490757 | validation: 3.156883774424358]
	TIME [epoch: 8.35 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.150410850384498		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 3.150410850384498 | validation: 3.1753918229500755]
	TIME [epoch: 8.32 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1562367378767178		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 3.1562367378767178 | validation: 3.1820409524004534]
	TIME [epoch: 8.31 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.177408652179155		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 3.177408652179155 | validation: 3.1983955769525436]
	TIME [epoch: 8.31 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.189787241210077		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 3.189787241210077 | validation: 3.201862776330298]
	TIME [epoch: 8.32 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1875266385594587		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 3.1875266385594587 | validation: 3.1889056132748705]
	TIME [epoch: 8.36 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.197935221947585		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 3.197935221947585 | validation: 3.2035205711121533]
	TIME [epoch: 8.32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1930098455032025		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 3.1930098455032025 | validation: 3.2005506602223917]
	TIME [epoch: 8.32 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.185117309053716		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 3.185117309053716 | validation: 3.197635366517577]
	TIME [epoch: 8.31 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.177053801212025		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 3.177053801212025 | validation: 3.1888975015744463]
	TIME [epoch: 8.43 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1812781719765675		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 3.1812781719765675 | validation: 3.1853476978112347]
	TIME [epoch: 8.36 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.176090617402052		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 3.176090617402052 | validation: 3.17957180231981]
	TIME [epoch: 8.39 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170015581781213		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 3.170015581781213 | validation: 3.179785681017673]
	TIME [epoch: 8.35 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166590490460983		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 3.166590490460983 | validation: 3.1726439741421633]
	TIME [epoch: 8.34 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1638715674802937		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 3.1638715674802937 | validation: 3.171859677820838]
	TIME [epoch: 8.34 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.164050462174667		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 3.164050462174667 | validation: 3.186114597435825]
	TIME [epoch: 8.33 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1759634043078386		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 3.1759634043078386 | validation: 3.1921146831065323]
	TIME [epoch: 8.35 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1681720395503836		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 3.1681720395503836 | validation: 3.1761917094385277]
	TIME [epoch: 8.36 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.158812759446386		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 3.158812759446386 | validation: 3.168621367903812]
	TIME [epoch: 8.33 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.15823410188092		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 3.15823410188092 | validation: 3.171453435558778]
	TIME [epoch: 8.33 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1816807053097937		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 3.1816807053097937 | validation: 3.18994824735141]
	TIME [epoch: 8.34 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1814963267913914		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 3.1814963267913914 | validation: 3.172145728447319]
	TIME [epoch: 8.33 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1750058022101197		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 3.1750058022101197 | validation: 3.184430905659736]
	TIME [epoch: 8.37 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.187729918170434		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 3.187729918170434 | validation: 3.210402603571065]
	TIME [epoch: 8.36 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2163014239929453		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 3.2163014239929453 | validation: 3.2647154270952603]
	TIME [epoch: 8.33 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261067253933777		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 3.261067253933777 | validation: 3.2840896810726896]
	TIME [epoch: 8.34 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2717641266404756		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 3.2717641266404756 | validation: 3.29331434947785]
	TIME [epoch: 8.33 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2844204897859695		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 3.2844204897859695 | validation: 3.317693642153692]
	TIME [epoch: 8.34 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.300256354250261		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 3.300256354250261 | validation: 3.309693142427269]
	TIME [epoch: 8.37 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30224537022636		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 3.30224537022636 | validation: 3.345505428693973]
	TIME [epoch: 8.34 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315089509941531		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 3.315089509941531 | validation: 3.3421759671210602]
	TIME [epoch: 8.33 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28810936658997		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 3.28810936658997 | validation: 3.2992774238350453]
	TIME [epoch: 8.34 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259267035161593		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 3.259267035161593 | validation: 3.318927696231798]
	TIME [epoch: 8.32 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239557272766836		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 3.239557272766836 | validation: 3.2517068772509443]
	TIME [epoch: 8.34 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224763018207417		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 3.224763018207417 | validation: 3.3007621802870935]
	TIME [epoch: 8.37 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2466229616842823		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 3.2466229616842823 | validation: 3.287533890594192]
	TIME [epoch: 8.33 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2218967438712296		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 3.2218967438712296 | validation: 3.254937435356002]
	TIME [epoch: 8.33 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081417126254053		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 3.2081417126254053 | validation: 3.2520351338701987]
	TIME [epoch: 8.33 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213232893437035		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 3.213232893437035 | validation: 3.247425922604989]
	TIME [epoch: 8.33 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205627584183775		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 3.205627584183775 | validation: 3.2311576032292004]
	TIME [epoch: 8.35 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1947911530153377		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 3.1947911530153377 | validation: 3.2022501299836157]
	TIME [epoch: 8.37 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1737764771455392		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 3.1737764771455392 | validation: 3.207188138825109]
	TIME [epoch: 8.34 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170864580521969		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 3.170864580521969 | validation: 3.200881087201373]
	TIME [epoch: 8.32 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1711299387784457		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 3.1711299387784457 | validation: 3.1875934562684463]
	TIME [epoch: 8.32 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1674156106959996		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 3.1674156106959996 | validation: 3.193124548015432]
	TIME [epoch: 8.33 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1695284434805258		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 3.1695284434805258 | validation: 3.179347137436455]
	TIME [epoch: 8.35 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.175927048379312		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 3.175927048379312 | validation: 3.220453352552104]
	TIME [epoch: 8.36 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.193234840564366		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 3.193234840564366 | validation: 3.223336309022163]
	TIME [epoch: 8.33 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.191940199953708		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 3.191940199953708 | validation: 3.235216572217145]
	TIME [epoch: 8.32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1969326282982866		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 3.1969326282982866 | validation: 3.224336647017827]
	TIME [epoch: 8.33 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1862884906163758		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 3.1862884906163758 | validation: 3.2131167672689385]
	TIME [epoch: 8.33 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1929616530790357		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 3.1929616530790357 | validation: 3.230447775933719]
	TIME [epoch: 8.35 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1922711842715037		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 3.1922711842715037 | validation: 3.231352037068815]
	TIME [epoch: 8.34 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.193599332955211		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 3.193599332955211 | validation: 3.2270935717438274]
	TIME [epoch: 8.33 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1892032681006235		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 3.1892032681006235 | validation: 3.2091269986750888]
	TIME [epoch: 8.33 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1894330742833024		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 3.1894330742833024 | validation: 3.203061309617749]
	TIME [epoch: 8.33 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.191633359202832		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 3.191633359202832 | validation: 3.2081053152106564]
	TIME [epoch: 8.33 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1921561245430112		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 3.1921561245430112 | validation: 3.206817126067214]
	TIME [epoch: 8.37 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.191001859201946		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 3.191001859201946 | validation: 3.2083919385672184]
	TIME [epoch: 8.32 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1876452919487748		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 3.1876452919487748 | validation: 3.2055854980993406]
	TIME [epoch: 8.33 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.189398674562179		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 3.189398674562179 | validation: 3.2259016329950185]
	TIME [epoch: 8.32 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1894576962698347		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 3.1894576962698347 | validation: 3.2157098632899164]
	TIME [epoch: 8.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1905144074066514		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 3.1905144074066514 | validation: 3.2086805716556936]
	TIME [epoch: 8.34 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.179494937643579		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 3.179494937643579 | validation: 3.196847119401232]
	TIME [epoch: 8.36 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1802902436095097		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 3.1802902436095097 | validation: 3.1983036326426753]
	TIME [epoch: 8.32 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1860060055121946		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 3.1860060055121946 | validation: 3.202991133351188]
	TIME [epoch: 8.32 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1839860444514954		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 3.1839860444514954 | validation: 3.207730820823311]
	TIME [epoch: 8.32 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.181713529333519		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 3.181713529333519 | validation: 3.204123094229723]
	TIME [epoch: 8.32 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1825161007013705		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 3.1825161007013705 | validation: 3.1942954151210996]
	TIME [epoch: 8.35 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.17894308612191		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 3.17894308612191 | validation: 3.1927842658063326]
	TIME [epoch: 8.37 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1729599357361282		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 3.1729599357361282 | validation: 3.1911437037191073]
	TIME [epoch: 8.38 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1686378132816726		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 3.1686378132816726 | validation: 3.197653493216608]
	TIME [epoch: 8.38 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1741224535941726		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 3.1741224535941726 | validation: 3.208531866240577]
	TIME [epoch: 8.32 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.183005939987492		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 3.183005939987492 | validation: 3.1969227481788476]
	TIME [epoch: 8.32 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1753775945687224		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 3.1753775945687224 | validation: 3.19655357810837]
	TIME [epoch: 8.35 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.176871164692373		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 3.176871164692373 | validation: 3.200498257269823]
	TIME [epoch: 8.34 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1826237995208904		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 3.1826237995208904 | validation: 3.203625134931212]
	TIME [epoch: 8.32 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1748481072282058		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 3.1748481072282058 | validation: 3.1903392657170224]
	TIME [epoch: 8.32 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170729962360055		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 3.170729962360055 | validation: 3.193322564345426]
	TIME [epoch: 8.31 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.169602829829514		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 3.169602829829514 | validation: 3.1877315100156447]
	TIME [epoch: 8.32 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.169181292317141		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 3.169181292317141 | validation: 3.189545129975234]
	TIME [epoch: 8.35 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1705624404116577		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 3.1705624404116577 | validation: 3.187367102576759]
	TIME [epoch: 8.34 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166394110197764		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 3.166394110197764 | validation: 3.178147848703436]
	TIME [epoch: 8.32 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1648512794857115		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 3.1648512794857115 | validation: 3.1821338291559425]
	TIME [epoch: 8.33 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1642573035371337		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 3.1642573035371337 | validation: 3.167958376836791]
	TIME [epoch: 8.32 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1498645104355663		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 3.1498645104355663 | validation: 3.162387675174955]
	TIME [epoch: 8.33 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1497935905941015		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 3.1497935905941015 | validation: 3.168121650199145]
	TIME [epoch: 8.37 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1533269122512686		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 3.1533269122512686 | validation: 3.1632940910854987]
	TIME [epoch: 8.32 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.152369799150113		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 3.152369799150113 | validation: 3.159555301485608]
	TIME [epoch: 8.32 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1496828803836103		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 3.1496828803836103 | validation: 3.15814540688601]
	TIME [epoch: 8.41 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1551556589048912		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 3.1551556589048912 | validation: 3.1670423225627315]
	TIME [epoch: 8.38 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1643466004831806		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 3.1643466004831806 | validation: 3.161898226029402]
	TIME [epoch: 8.37 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.155339071165306		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 3.155339071165306 | validation: 3.1616660038964755]
	TIME [epoch: 8.36 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1572920574936774		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 3.1572920574936774 | validation: 3.1735800719960716]
	TIME [epoch: 8.32 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1563240709671883		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 3.1563240709671883 | validation: 3.174059213013394]
	TIME [epoch: 8.31 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1563282263104195		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 3.1563282263104195 | validation: 3.1738636124643906]
	TIME [epoch: 8.32 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1583323309472555		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 3.1583323309472555 | validation: 3.19444580045669]
	TIME [epoch: 8.32 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.163408997605213		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 3.163408997605213 | validation: 3.2004606116595005]
	TIME [epoch: 8.34 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1601050674958664		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 3.1601050674958664 | validation: 3.183249369662583]
	TIME [epoch: 8.36 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.15755273270706		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 3.15755273270706 | validation: 3.1740934796461593]
	TIME [epoch: 8.32 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.151773928223613		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 3.151773928223613 | validation: 3.1689150320236177]
	TIME [epoch: 8.32 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.148359834470186		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 3.148359834470186 | validation: 3.163925967493253]
	TIME [epoch: 8.32 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1461548464487548		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 3.1461548464487548 | validation: 3.1641929846021455]
	TIME [epoch: 8.33 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.147378040682209		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 3.147378040682209 | validation: 3.1582466397580973]
	TIME [epoch: 8.35 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1451920052795033		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 3.1451920052795033 | validation: 3.1532080622753753]
	TIME [epoch: 8.34 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1465824183845355		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 3.1465824183845355 | validation: 3.157276536574442]
	TIME [epoch: 8.32 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1467127366648038		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 3.1467127366648038 | validation: 3.1594513051132846]
	TIME [epoch: 8.32 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1475688444849252		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 3.1475688444849252 | validation: 3.1697636363184545]
	TIME [epoch: 8.32 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1626135082241755		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 3.1626135082241755 | validation: 3.1808832694226554]
	TIME [epoch: 8.33 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.16342900708641		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 3.16342900708641 | validation: 3.189818146502565]
	TIME [epoch: 8.47 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1718261987372203		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 3.1718261987372203 | validation: 3.198395291334698]
	TIME [epoch: 8.34 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.180905899394		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 3.180905899394 | validation: 3.210089252914834]
	TIME [epoch: 8.32 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.18806617044124		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 3.18806617044124 | validation: 3.2177945679713917]
	TIME [epoch: 8.31 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.188404413245005		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 3.188404413245005 | validation: 3.2200061015273125]
	TIME [epoch: 8.32 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.190575275067094		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 3.190575275067094 | validation: 3.2222491720919724]
	TIME [epoch: 8.33 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.191049053693048		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 3.191049053693048 | validation: 3.2165386380894576]
	TIME [epoch: 8.36 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1890999861937672		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 3.1890999861937672 | validation: 3.2070302848975905]
	TIME [epoch: 8.32 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.186219853075382		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 3.186219853075382 | validation: 3.2080441459285143]
	TIME [epoch: 8.32 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1867107040293003		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 3.1867107040293003 | validation: 3.21884272190625]
	TIME [epoch: 8.32 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1921455543994908		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 3.1921455543994908 | validation: 3.197580722125218]
	TIME [epoch: 8.32 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1739551587247408		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 3.1739551587247408 | validation: 3.175041868716912]
	TIME [epoch: 8.33 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.16136851802795		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 3.16136851802795 | validation: 3.175552354171975]
	TIME [epoch: 8.36 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1622142254133085		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 3.1622142254133085 | validation: 3.178367140885198]
	TIME [epoch: 8.32 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.163406125756022		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 3.163406125756022 | validation: 3.183393536377256]
	TIME [epoch: 8.32 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1717271762120656		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 3.1717271762120656 | validation: 3.1928501688152946]
	TIME [epoch: 8.33 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.184793792078293		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 3.184793792078293 | validation: 3.2243169087214976]
	TIME [epoch: 8.32 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196363473759198		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 3.196363473759198 | validation: 3.211966112968458]
	TIME [epoch: 8.34 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.190814124059347		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 3.190814124059347 | validation: 3.2049282793650296]
	TIME [epoch: 8.37 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1831018093559345		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 3.1831018093559345 | validation: 3.200887544563318]
	TIME [epoch: 8.33 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196798951255682		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 3.196798951255682 | validation: 3.226581303263079]
	TIME [epoch: 8.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219940906228448		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 3.219940906228448 | validation: 3.252419838858225]
	TIME [epoch: 8.32 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224773546876264		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 3.224773546876264 | validation: 3.2505559130609063]
	TIME [epoch: 8.32 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2281697270380985		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 3.2281697270380985 | validation: 3.258169268133436]
	TIME [epoch: 8.35 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2231449658897664		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 3.2231449658897664 | validation: 3.243498182678416]
	TIME [epoch: 8.35 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214841176660439		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 3.214841176660439 | validation: 3.2284443973219448]
	TIME [epoch: 8.33 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20960980495442		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 3.20960980495442 | validation: 3.2310091174920954]
	TIME [epoch: 8.33 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064549656468504		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 3.2064549656468504 | validation: 3.219860645038202]
	TIME [epoch: 8.32 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.18844778494131		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 3.18844778494131 | validation: 3.197143851685052]
	TIME [epoch: 8.32 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1796354029997165		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 3.1796354029997165 | validation: 3.1936153603932103]
	TIME [epoch: 8.36 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.174732436741928		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 3.174732436741928 | validation: 3.191720797660439]
	TIME [epoch: 8.33 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1754075128918267		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 3.1754075128918267 | validation: 3.194464810578518]
	TIME [epoch: 8.31 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1732066569920887		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 3.1732066569920887 | validation: 3.196906174682115]
	TIME [epoch: 8.32 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1784757581950203		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 3.1784757581950203 | validation: 3.2009634229816015]
	TIME [epoch: 8.32 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1758830149987722		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 3.1758830149987722 | validation: 3.19173038870184]
	TIME [epoch: 8.33 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1730316450719296		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 3.1730316450719296 | validation: 3.1896484817415676]
	TIME [epoch: 8.37 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170464175967238		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 3.170464175967238 | validation: 3.1742590439295784]
	TIME [epoch: 8.32 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.16391367727383		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 3.16391367727383 | validation: 3.1762606395836754]
	TIME [epoch: 8.32 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1649557968962694		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 3.1649557968962694 | validation: 3.173717496061302]
	TIME [epoch: 8.32 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1654081787347534		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 3.1654081787347534 | validation: 3.1709539468071974]
	TIME [epoch: 8.33 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1594887995855574		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 3.1594887995855574 | validation: 3.1620286102532598]
	TIME [epoch: 8.34 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.160982540570725		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 3.160982540570725 | validation: 3.1700401358178256]
	TIME [epoch: 8.37 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1583986605518337		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 3.1583986605518337 | validation: 3.174339654751619]
	TIME [epoch: 8.33 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1572981684916526		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 3.1572981684916526 | validation: 3.164729907816544]
	TIME [epoch: 8.32 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.15985135552619		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 3.15985135552619 | validation: 3.1791027145806945]
	TIME [epoch: 8.32 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1789468427221705		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 3.1789468427221705 | validation: 3.2123640104840128]
	TIME [epoch: 8.32 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210011515282913		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 3.210011515282913 | validation: 3.2248675236461457]
	TIME [epoch: 8.34 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2174521791977972		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 3.2174521791977972 | validation: 3.234264733533779]
	TIME [epoch: 8.35 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225339193224628		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 3.225339193224628 | validation: 3.2339791686720556]
	TIME [epoch: 8.32 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2266581852917304		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 3.2266581852917304 | validation: 3.2340907917007122]
	TIME [epoch: 8.32 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2225964774299465		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 3.2225964774299465 | validation: 3.2341886629316052]
	TIME [epoch: 8.32 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22932914474857		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 3.22932914474857 | validation: 3.2609567703524456]
	TIME [epoch: 8.32 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2533553267791033		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 3.2533553267791033 | validation: 3.2896282771410545]
	TIME [epoch: 8.34 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2643256973850097		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 3.2643256973850097 | validation: 3.3193513464382884]
	TIME [epoch: 8.34 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284646466019972		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 3.284646466019972 | validation: 3.348516520057273]
	TIME [epoch: 8.32 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.309892156674133		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 3.309892156674133 | validation: 3.397402093059095]
	TIME [epoch: 8.32 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34996210590655		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 3.34996210590655 | validation: 3.45387519610383]
	TIME [epoch: 8.32 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.391320540466065		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 3.391320540466065 | validation: 3.457631828203515]
	TIME [epoch: 8.33 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3692932730935055		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 3.3692932730935055 | validation: 3.4367310256463144]
	TIME [epoch: 8.35 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3488435609980582		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 3.3488435609980582 | validation: 3.4101310762549706]
	TIME [epoch: 8.34 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3398611410001307		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 3.3398611410001307 | validation: 3.4303217690048355]
	TIME [epoch: 8.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.362656189419665		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 3.362656189419665 | validation: 3.4524605176437735]
	TIME [epoch: 8.31 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3774652941545047		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 3.3774652941545047 | validation: 3.4427277883842624]
	TIME [epoch: 8.31 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351928948818739		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 3.351928948818739 | validation: 3.4175293490249503]
	TIME [epoch: 8.33 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34123880470862		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 3.34123880470862 | validation: 3.4232920689890163]
	TIME [epoch: 8.35 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336015488543373		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 3.336015488543373 | validation: 3.399579056377701]
	TIME [epoch: 8.32 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324386007672897		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 3.324386007672897 | validation: 3.39740576040451]
	TIME [epoch: 8.31 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3101300321048326		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 3.3101300321048326 | validation: 3.366184643900387]
	TIME [epoch: 8.32 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292843073271694		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 3.292843073271694 | validation: 3.3423120553701953]
	TIME [epoch: 8.32 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292267369029342		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 3.292267369029342 | validation: 3.3559995315416957]
	TIME [epoch: 8.34 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2931167493815363		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 3.2931167493815363 | validation: 3.359971665986895]
	TIME [epoch: 8.36 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.293007882386239		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 3.293007882386239 | validation: 3.3471391106774355]
	TIME [epoch: 8.32 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28123846666292		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 3.28123846666292 | validation: 3.34528377669248]
	TIME [epoch: 8.32 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2729700520483784		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 3.2729700520483784 | validation: 3.3445222008352777]
	TIME [epoch: 8.32 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278095932555666		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 3.278095932555666 | validation: 3.357651479791736]
	TIME [epoch: 8.32 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288318496782072		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 3.288318496782072 | validation: 3.371119384712208]
	TIME [epoch: 8.34 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.296873461653684		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 3.296873461653684 | validation: 3.3598059271425207]
	TIME [epoch: 8.35 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283585129068046		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 3.283585129068046 | validation: 3.3536997211318473]
	TIME [epoch: 8.37 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2771603630211517		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 3.2771603630211517 | validation: 3.3315562275952586]
	TIME [epoch: 8.39 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2666597184347275		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 3.2666597184347275 | validation: 3.324436728099726]
	TIME [epoch: 8.33 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269489013468266		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 3.269489013468266 | validation: 3.3633918334724298]
	TIME [epoch: 8.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3019158680865184		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 3.3019158680865184 | validation: 3.388039792549697]
	TIME [epoch: 8.35 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32138824537072		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 3.32138824537072 | validation: 3.421843221355071]
	TIME [epoch: 8.34 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3275824891113297		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 3.3275824891113297 | validation: 3.4212478901758847]
	TIME [epoch: 8.32 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3267048657306324		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 3.3267048657306324 | validation: 3.433368729957044]
	TIME [epoch: 8.31 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3417750076758024		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 3.3417750076758024 | validation: 3.43929025683635]
	TIME [epoch: 8.31 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354208498925424		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 3.354208498925424 | validation: 3.4589415473781453]
	TIME [epoch: 8.32 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3598797199198414		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 3.3598797199198414 | validation: 3.4441379524617313]
	TIME [epoch: 8.35 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.349125303317593		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 3.349125303317593 | validation: 3.4478921439253423]
	TIME [epoch: 8.33 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3519764384026107		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 3.3519764384026107 | validation: 3.4506068493795565]
	TIME [epoch: 8.31 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3470569807084987		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 3.3470569807084987 | validation: 3.4392991271004263]
	TIME [epoch: 8.31 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.350544936433426		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 3.350544936433426 | validation: 3.473101612318746]
	TIME [epoch: 8.32 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3702909326138157		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 3.3702909326138157 | validation: 3.485888373605042]
	TIME [epoch: 8.33 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3737720137938134		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 3.3737720137938134 | validation: 3.4817710621347864]
	TIME [epoch: 8.36 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3752208899289613		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 3.3752208899289613 | validation: 3.48574270478308]
	TIME [epoch: 8.32 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3731587341162803		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 3.3731587341162803 | validation: 3.4747286303329483]
	TIME [epoch: 8.32 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.371879732049859		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 3.371879732049859 | validation: 3.4840432775193033]
	TIME [epoch: 8.33 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3810872086381285		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 3.3810872086381285 | validation: 3.491733344174693]
	TIME [epoch: 8.38 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.377953286509772		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 3.377953286509772 | validation: 3.4829330494639295]
	TIME [epoch: 8.38 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3671040051554786		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 3.3671040051554786 | validation: 3.483870086754946]
	TIME [epoch: 8.35 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3656287615803264		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 3.3656287615803264 | validation: 3.476663329789072]
	TIME [epoch: 8.41 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.370002995401163		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 3.370002995401163 | validation: 3.487867422668642]
	TIME [epoch: 8.31 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379768503329082		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 3.379768503329082 | validation: 3.4975175183021205]
	TIME [epoch: 8.31 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.38452186896721		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 3.38452186896721 | validation: 3.4869742662932683]
	TIME [epoch: 8.31 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3785584879694723		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 3.3785584879694723 | validation: 3.490481044477624]
	TIME [epoch: 8.35 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3785748104943525		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 3.3785748104943525 | validation: 3.478306637107134]
	TIME [epoch: 8.35 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.372372065963625		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 3.372372065963625 | validation: 3.4868757184612065]
	TIME [epoch: 8.32 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.377950584411904		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 3.377950584411904 | validation: 3.5010966086148314]
	TIME [epoch: 8.32 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3838084217003854		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 3.3838084217003854 | validation: 3.4875235926908084]
	TIME [epoch: 8.32 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3814570887191984		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 3.3814570887191984 | validation: 3.4884221558074535]
	TIME [epoch: 8.31 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373161406937557		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 3.373161406937557 | validation: 3.467880355676712]
	TIME [epoch: 8.34 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3620214551496765		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 3.3620214551496765 | validation: 3.4707896374476377]
	TIME [epoch: 8.44 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363428950414269		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 3.363428950414269 | validation: 3.462487468979031]
	TIME [epoch: 8.31 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3456591016216692		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 3.3456591016216692 | validation: 3.428312522866892]
	TIME [epoch: 8.31 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330058937944705		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 3.330058937944705 | validation: 3.409807680572494]
	TIME [epoch: 8.32 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3260593626218866		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 3.3260593626218866 | validation: 3.429729222108181]
	TIME [epoch: 8.34 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424908626696386		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 3.3424908626696386 | validation: 3.4331362040721407]
	TIME [epoch: 8.43 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3508073985488496		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 3.3508073985488496 | validation: 3.4561862600418225]
	TIME [epoch: 8.36 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3692285935603246		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 3.3692285935603246 | validation: 3.463102914452847]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366051251894612		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 3.366051251894612 | validation: 3.4549513320346947]
	TIME [epoch: 8.32 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352858684837587		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 3.352858684837587 | validation: 3.435012962815657]
	TIME [epoch: 8.34 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3410933915878873		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 3.3410933915878873 | validation: 3.433033673183555]
	TIME [epoch: 8.35 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346440248909378		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 3.346440248909378 | validation: 3.429232977269284]
	TIME [epoch: 8.37 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3451201862359685		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 3.3451201862359685 | validation: 3.4312822174867437]
	TIME [epoch: 8.34 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3465953990173354		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 3.3465953990173354 | validation: 3.4534635027943352]
	TIME [epoch: 8.33 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354584097131697		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 3.354584097131697 | validation: 3.454643675180555]
	TIME [epoch: 8.33 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351418032679242		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 3.351418032679242 | validation: 3.4739483369537734]
	TIME [epoch: 8.34 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3618545112176954		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 3.3618545112176954 | validation: 3.4623019504121295]
	TIME [epoch: 8.36 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3613889804719794		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 3.3613889804719794 | validation: 3.4735482303866987]
	TIME [epoch: 8.36 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3621713239306015		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 3.3621713239306015 | validation: 3.4638010552234064]
	TIME [epoch: 8.34 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3491094579501404		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 3.3491094579501404 | validation: 3.449214896263917]
	TIME [epoch: 8.33 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337807543808158		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 3.337807543808158 | validation: 3.4310842627207307]
	TIME [epoch: 8.33 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345113221576485		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 3.345113221576485 | validation: 3.455719502787533]
	TIME [epoch: 8.33 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3523377166384405		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 3.3523377166384405 | validation: 3.448943447694949]
	TIME [epoch: 8.37 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3535922939381018		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 3.3535922939381018 | validation: 3.459411996285431]
	TIME [epoch: 8.35 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352269741135492		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 3.352269741135492 | validation: 3.452618388922496]
	TIME [epoch: 8.33 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367204615383284		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 3.367204615383284 | validation: 3.4769784138728843]
	TIME [epoch: 8.33 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3695698712676636		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 3.3695698712676636 | validation: 3.483120274226067]
	TIME [epoch: 8.38 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3698583889610467		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 3.3698583889610467 | validation: 3.4577205290962114]
	TIME [epoch: 8.41 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.348197827183996		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 3.348197827183996 | validation: 3.4316487832091394]
	TIME [epoch: 8.38 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377411638919225		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 3.3377411638919225 | validation: 3.4193304255433743]
	TIME [epoch: 8.41 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332342841370443		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 3.332342841370443 | validation: 3.4275618204229437]
	TIME [epoch: 8.33 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3395448105496905		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 3.3395448105496905 | validation: 3.4373607173700482]
	TIME [epoch: 8.32 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345912351961985		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 3.345912351961985 | validation: 3.4638846116813222]
	TIME [epoch: 8.32 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3588971631674864		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 3.3588971631674864 | validation: 3.4850856477954]
	TIME [epoch: 8.34 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3738218899438666		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 3.3738218899438666 | validation: 3.497458812218435]
	TIME [epoch: 8.37 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3714338703374334		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 3.3714338703374334 | validation: 3.484670113065515]
	TIME [epoch: 8.33 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378030477834125		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 3.378030477834125 | validation: 3.500879562058206]
	TIME [epoch: 8.33 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3768529672278293		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 3.3768529672278293 | validation: 3.4914925081316794]
	TIME [epoch: 8.33 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3656149823112433		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 3.3656149823112433 | validation: 3.5054391916546637]
	TIME [epoch: 8.32 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376327506842923		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 3.376327506842923 | validation: 3.5162066090940076]
	TIME [epoch: 8.34 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376023483889595		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 3.376023483889595 | validation: 3.508128906060967]
	TIME [epoch: 8.37 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3739091255182228		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 3.3739091255182228 | validation: 3.5207662347962794]
	TIME [epoch: 8.32 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3843733215655822		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 3.3843733215655822 | validation: 3.546993839446655]
	TIME [epoch: 8.33 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3921597473165006		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 3.3921597473165006 | validation: 3.563034698293987]
	TIME [epoch: 8.32 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.397306289466044		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 3.397306289466044 | validation: 3.557077466751324]
	TIME [epoch: 8.32 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4007175456364216		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 3.4007175456364216 | validation: 3.565087571980693]
	TIME [epoch: 8.37 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410156107829309		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 3.410156107829309 | validation: 3.5602392281802877]
	TIME [epoch: 8.41 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.399002158959827		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 3.399002158959827 | validation: 3.525999938662615]
	TIME [epoch: 8.37 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3797373996933735		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 3.3797373996933735 | validation: 3.519207416544324]
	TIME [epoch: 8.33 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.374231832004592		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 3.374231832004592 | validation: 3.4954195665791072]
	TIME [epoch: 8.42 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3652034712336323		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 3.3652034712336323 | validation: 3.5065143467523465]
	TIME [epoch: 8.33 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3681863967443237		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 3.3681863967443237 | validation: 3.4988781605142956]
	TIME [epoch: 8.35 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3718286545478837		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 3.3718286545478837 | validation: 3.511304537439753]
	TIME [epoch: 8.33 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3743404483335637		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 3.3743404483335637 | validation: 3.5278374092646105]
	TIME [epoch: 8.32 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.38872033131687		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 3.38872033131687 | validation: 3.550925441764746]
	TIME [epoch: 8.32 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4003270190147337		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 3.4003270190147337 | validation: 3.5608003132783907]
	TIME [epoch: 8.32 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3970696445427024		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 3.3970696445427024 | validation: 3.535840415592543]
	TIME [epoch: 8.33 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3945958603537503		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 3.3945958603537503 | validation: 3.5312035423568995]
	TIME [epoch: 8.36 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.393796169484409		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 3.393796169484409 | validation: 3.527333600836785]
	TIME [epoch: 8.32 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3867150227624796		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 3.3867150227624796 | validation: 3.5378216409122425]
	TIME [epoch: 8.32 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3885791113854724		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 3.3885791113854724 | validation: 3.526682191901928]
	TIME [epoch: 8.32 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3881962890645716		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 3.3881962890645716 | validation: 3.5524150936970704]
	TIME [epoch: 8.33 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3995114733844014		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 3.3995114733844014 | validation: 3.5539682048478065]
	TIME [epoch: 8.34 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4062996098935208		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 3.4062996098935208 | validation: 3.5619610831863824]
	TIME [epoch: 8.36 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4182286212411745		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 3.4182286212411745 | validation: 3.5794253758399126]
	TIME [epoch: 8.32 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427000708898734		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 3.427000708898734 | validation: 3.5975552747540656]
	TIME [epoch: 8.34 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4317412751403795		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 3.4317412751403795 | validation: 3.602347933938813]
	TIME [epoch: 8.38 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.436092839354921		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 3.436092839354921 | validation: 3.5978899945000347]
	TIME [epoch: 8.45 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.428076724816034		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 3.428076724816034 | validation: 3.60121627633099]
	TIME [epoch: 8.33 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4357744807446453		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 3.4357744807446453 | validation: 3.6150697943143126]
	TIME [epoch: 8.43 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.447469387277806		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 3.447469387277806 | validation: 3.6124193894745202]
	TIME [epoch: 8.31 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4406960075416917		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 3.4406960075416917 | validation: 3.6130738601271215]
	TIME [epoch: 8.32 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4402404464122736		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 3.4402404464122736 | validation: 3.6004415709995055]
	TIME [epoch: 8.32 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.439549781720957		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 3.439549781720957 | validation: 3.582460423705877]
	TIME [epoch: 8.32 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.42704415503196		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 3.42704415503196 | validation: 3.566553766696969]
	TIME [epoch: 8.36 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4120637077182248		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 3.4120637077182248 | validation: 3.55363901853236]
	TIME [epoch: 8.34 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410920882027572		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 3.410920882027572 | validation: 3.5442653763690957]
	TIME [epoch: 8.32 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.41051806340293		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 3.41051806340293 | validation: 3.5466928698875737]
	TIME [epoch: 8.32 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410733960782888		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 3.410733960782888 | validation: 3.55589513022868]
	TIME [epoch: 8.32 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4174029791967526		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 3.4174029791967526 | validation: 3.563190425557541]
	TIME [epoch: 8.32 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4236657269376276		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 3.4236657269376276 | validation: 3.5558495079322965]
	TIME [epoch: 8.35 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4135422146587864		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 3.4135422146587864 | validation: 3.540805872813798]
	TIME [epoch: 8.33 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.397141762968047		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 3.397141762968047 | validation: 3.5255275163350217]
	TIME [epoch: 8.35 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3881522065652008		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 3.3881522065652008 | validation: 3.509961995790415]
	TIME [epoch: 8.31 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3817150333579473		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 3.3817150333579473 | validation: 3.5163391343949892]
	TIME [epoch: 8.32 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.383342110538802		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 3.383342110538802 | validation: 3.5254275431869138]
	TIME [epoch: 8.39 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3883090666986018		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 3.3883090666986018 | validation: 3.5248232825890025]
	TIME [epoch: 8.43 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.389574027233496		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 3.389574027233496 | validation: 3.5348108316168245]
	TIME [epoch: 8.34 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3858959775560944		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 3.3858959775560944 | validation: 3.4999306199231954]
	TIME [epoch: 8.32 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3812657226937137		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 3.3812657226937137 | validation: 3.5115458533670907]
	TIME [epoch: 8.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.382621799232476		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 3.382621799232476 | validation: 3.5224926127215]
	TIME [epoch: 8.31 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.391322151784398		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 3.391322151784398 | validation: 3.518071912707901]
	TIME [epoch: 8.33 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.391080620840485		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 3.391080620840485 | validation: 3.5286913603103933]
	TIME [epoch: 8.35 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3934649313202643		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 3.3934649313202643 | validation: 3.5290046227272702]
	TIME [epoch: 8.32 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394854173813659		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 3.394854173813659 | validation: 3.5218419668488528]
	TIME [epoch: 8.32 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.396409369296503		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 3.396409369296503 | validation: 3.525272068767773]
	TIME [epoch: 8.32 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3890873493021294		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 3.3890873493021294 | validation: 3.501208146605494]
	TIME [epoch: 8.32 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3873487515082576		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 3.3873487515082576 | validation: 3.5037789908912416]
	TIME [epoch: 8.35 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3915170438276667		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 3.3915170438276667 | validation: 3.5175072886168146]
	TIME [epoch: 8.36 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3973144546684626		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 3.3973144546684626 | validation: 3.510618621376954]
	TIME [epoch: 8.33 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.39211671265306		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 3.39211671265306 | validation: 3.5012053524593796]
	TIME [epoch: 8.33 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379707677913796		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 3.379707677913796 | validation: 3.4969250397993124]
	TIME [epoch: 8.33 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376536740798931		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 3.376536740798931 | validation: 3.4895883069074394]
	TIME [epoch: 8.33 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.371676046008888		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 3.371676046008888 | validation: 3.4797017988217736]
	TIME [epoch: 8.35 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367802048386419		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 3.367802048386419 | validation: 3.4759909819581347]
	TIME [epoch: 8.36 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363049853842918		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 3.363049853842918 | validation: 3.4895992267448808]
	TIME [epoch: 8.38 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3737502709058123		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 3.3737502709058123 | validation: 3.4876131013635847]
	TIME [epoch: 8.37 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.371354306214641		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 3.371354306214641 | validation: 3.4912817526213544]
	TIME [epoch: 8.32 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.368412251680491		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 3.368412251680491 | validation: 3.4987160335623964]
	TIME [epoch: 8.33 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367955985657134		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 3.367955985657134 | validation: 3.489140283229228]
	TIME [epoch: 8.46 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3712435185288054		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 3.3712435185288054 | validation: 3.4983033203279144]
	TIME [epoch: 8.31 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3692391568923927		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 3.3692391568923927 | validation: 3.497797778816084]
	TIME [epoch: 8.31 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367247306069914		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 3.367247306069914 | validation: 3.4868412044771437]
	TIME [epoch: 8.33 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3632664748768706		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 3.3632664748768706 | validation: 3.486888309951718]
	TIME [epoch: 8.33 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3659644114114147		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 3.3659644114114147 | validation: 3.496353152220979]
	TIME [epoch: 8.34 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3678398625841313		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 3.3678398625841313 | validation: 3.4996624062759163]
	TIME [epoch: 8.35 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378757273013564		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 3.378757273013564 | validation: 3.513132329142673]
	TIME [epoch: 8.32 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.386838191942978		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 3.386838191942978 | validation: 3.517847981388713]
	TIME [epoch: 8.32 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.395552856179913		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 3.395552856179913 | validation: 3.529581647235371]
	TIME [epoch: 8.31 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4040117803325938		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 3.4040117803325938 | validation: 3.5409571203899937]
	TIME [epoch: 8.31 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4079792733294347		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 3.4079792733294347 | validation: 3.547643720016665]
	TIME [epoch: 8.33 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4129692195779873		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 3.4129692195779873 | validation: 3.560609819008291]
	TIME [epoch: 8.35 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4309486572868915		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 3.4309486572868915 | validation: 3.5655458336221058]
	TIME [epoch: 8.31 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4320122563563955		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 3.4320122563563955 | validation: 3.575911044423719]
	TIME [epoch: 8.31 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.440069563909719		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 3.440069563909719 | validation: 3.569903530598289]
	TIME [epoch: 8.31 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.436698986042313		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 3.436698986042313 | validation: 3.572478962633331]
	TIME [epoch: 8.32 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4308588630241683		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 3.4308588630241683 | validation: 3.5523067235738934]
	TIME [epoch: 8.35 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4228184911991058		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 3.4228184911991058 | validation: 3.539737868890171]
	TIME [epoch: 8.35 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.411219230781799		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 3.411219230781799 | validation: 3.5261602227438518]
	TIME [epoch: 8.32 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4005866953602397		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 3.4005866953602397 | validation: 3.5211038855887]
	TIME [epoch: 8.41 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4028958164116268		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 3.4028958164116268 | validation: 3.531636552529046]
	TIME [epoch: 8.31 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3991799473780633		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 3.3991799473780633 | validation: 3.5203098291855293]
	TIME [epoch: 8.31 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4001971398934763		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 3.4001971398934763 | validation: 3.521181909733381]
	TIME [epoch: 8.34 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4051258306569685		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 3.4051258306569685 | validation: 3.5245773626227725]
	TIME [epoch: 8.32 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4064532028826973		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 3.4064532028826973 | validation: 3.5250168507548914]
	TIME [epoch: 8.31 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4042114547664024		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 3.4042114547664024 | validation: 3.5154485526123582]
	TIME [epoch: 8.31 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3998717378931507		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 3.3998717378931507 | validation: 3.518751562537133]
	TIME [epoch: 8.31 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402290488317254		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 3.402290488317254 | validation: 3.5290956707914427]
	TIME [epoch: 8.32 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4040746210793413		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 3.4040746210793413 | validation: 3.5311915269166345]
	TIME [epoch: 8.35 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4096498540228097		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 3.4096498540228097 | validation: 3.543001558838042]
	TIME [epoch: 8.31 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.419271489821707		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 3.419271489821707 | validation: 3.5401969477836217]
	TIME [epoch: 8.31 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.418443861955614		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 3.418443861955614 | validation: 3.5357555009151342]
	TIME [epoch: 8.36 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426221858609299		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 3.426221858609299 | validation: 3.560571128971345]
	TIME [epoch: 8.38 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427892832485056		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 3.427892832485056 | validation: 3.5477911664600352]
	TIME [epoch: 8.35 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.415128009271031		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 3.415128009271031 | validation: 3.5360107018687543]
	TIME [epoch: 8.36 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4104881444513166		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 3.4104881444513166 | validation: 3.5267394756744608]
	TIME [epoch: 8.32 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4125394370747015		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 3.4125394370747015 | validation: 3.529963674548555]
	TIME [epoch: 8.31 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4134985890392757		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 3.4134985890392757 | validation: 3.5344500020451735]
	TIME [epoch: 8.32 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4102645261703013		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 3.4102645261703013 | validation: 3.533769093500588]
	TIME [epoch: 8.31 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.417232789050966		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 3.417232789050966 | validation: 3.5378928426231955]
	TIME [epoch: 8.45 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4132475149066224		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 3.4132475149066224 | validation: 3.5299822240268934]
	TIME [epoch: 8.35 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4121765442161007		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 3.4121765442161007 | validation: 3.5278565046162553]
	TIME [epoch: 8.32 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.414201270335973		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 3.414201270335973 | validation: 3.5258741540065266]
	TIME [epoch: 8.32 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.409952116162627		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 3.409952116162627 | validation: 3.524371868243364]
	TIME [epoch: 8.32 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.409160631272046		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 3.409160631272046 | validation: 3.5191840381562027]
	TIME [epoch: 8.32 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4061342813436886		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 3.4061342813436886 | validation: 3.5170941303465586]
	TIME [epoch: 8.36 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4097954503926915		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 3.4097954503926915 | validation: 3.522361356373234]
	TIME [epoch: 8.34 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4026142159850905		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 3.4026142159850905 | validation: 3.503154174058597]
	TIME [epoch: 8.32 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.399276367929536		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 3.399276367929536 | validation: 3.501473737222213]
	TIME [epoch: 8.31 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.388406097499279		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 3.388406097499279 | validation: 3.4972754604490675]
	TIME [epoch: 8.32 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3873735454713323		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 3.3873735454713323 | validation: 3.478089784812431]
	TIME [epoch: 8.32 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394037609246013		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 3.394037609246013 | validation: 3.505645266063296]
	TIME [epoch: 8.41 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.398418627760602		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 3.398418627760602 | validation: 3.504824049658133]
	TIME [epoch: 8.39 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.40488722632671		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 3.40488722632671 | validation: 3.5205218562435014]
	TIME [epoch: 8.32 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410906146000695		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 3.410906146000695 | validation: 3.5241918244078567]
	TIME [epoch: 8.31 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.407366480915199		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 3.407366480915199 | validation: 3.5114780317778562]
	TIME [epoch: 8.31 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.40063628472923		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 3.40063628472923 | validation: 3.4965342215548216]
	TIME [epoch: 8.32 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3856555277604503		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 3.3856555277604503 | validation: 3.4726947896396023]
	TIME [epoch: 8.35 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3797611591390773		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 3.3797611591390773 | validation: 3.4813378033583815]
	TIME [epoch: 8.32 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3815999600279536		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 3.3815999600279536 | validation: 3.4716141703748034]
	TIME [epoch: 8.42 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.379687905058942		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 3.379687905058942 | validation: 3.464035078279098]
	TIME [epoch: 8.31 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.373876831555506		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 3.373876831555506 | validation: 3.4562435857822864]
	TIME [epoch: 8.31 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3666944195288893		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 3.3666944195288893 | validation: 3.4495655937582983]
	TIME [epoch: 8.33 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3552410339497323		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 3.3552410339497323 | validation: 3.436601099673545]
	TIME [epoch: 8.34 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3483910476830014		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 3.3483910476830014 | validation: 3.4358974657930808]
	TIME [epoch: 8.31 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3531228348040907		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 3.3531228348040907 | validation: 3.4335691718577417]
	TIME [epoch: 8.31 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3510248291085505		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 3.3510248291085505 | validation: 3.437801411454293]
	TIME [epoch: 8.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354563005238985		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 3.354563005238985 | validation: 3.4406318826572733]
	TIME [epoch: 8.32 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34970028250561		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 3.34970028250561 | validation: 3.4238343079140434]
	TIME [epoch: 8.34 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3425195653457944		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 3.3425195653457944 | validation: 3.436083755776223]
	TIME [epoch: 8.34 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3517848090616043		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 3.3517848090616043 | validation: 3.427844625283799]
	TIME [epoch: 8.34 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3479594465878098		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 3.3479594465878098 | validation: 3.4402931427916483]
	TIME [epoch: 8.38 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345678613818886		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 3.345678613818886 | validation: 3.420578435696945]
	TIME [epoch: 8.36 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3378512261772544		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 3.3378512261772544 | validation: 3.418277710375484]
	TIME [epoch: 8.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3329084409197467		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 3.3329084409197467 | validation: 3.40434510742092]
	TIME [epoch: 8.34 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3351676529632384		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 3.3351676529632384 | validation: 3.4121903783499254]
	TIME [epoch: 8.33 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335034094383478		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 3.335034094383478 | validation: 3.4044762189125324]
	TIME [epoch: 8.31 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337123192452912		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 3.337123192452912 | validation: 3.418319830560364]
	TIME [epoch: 8.31 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3416911344481344		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 3.3416911344481344 | validation: 3.4273594631132456]
	TIME [epoch: 8.31 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341521538258685		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 3.341521538258685 | validation: 3.4283081076141526]
	TIME [epoch: 8.4 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341704491963752		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 3.341704491963752 | validation: 3.4167608774655633]
	TIME [epoch: 8.34 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3383162355933735		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 3.3383162355933735 | validation: 3.421783935437925]
	TIME [epoch: 8.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346919761824109		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 3.346919761824109 | validation: 3.4331095227003203]
	TIME [epoch: 8.3 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3462983992818875		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 3.3462983992818875 | validation: 3.4360762255299515]
	TIME [epoch: 8.29 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3444975719771937		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 3.3444975719771937 | validation: 3.434542708562905]
	TIME [epoch: 8.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351927019336022		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 3.351927019336022 | validation: 3.4429391734266854]
	TIME [epoch: 8.31 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351476156615676		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 3.351476156615676 | validation: 3.4341718279909506]
	TIME [epoch: 8.33 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351489515094498		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 3.351489515094498 | validation: 3.4307022344172777]
	TIME [epoch: 8.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3456461490922353		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 3.3456461490922353 | validation: 3.4281810393096763]
	TIME [epoch: 8.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3386387633071433		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 3.3386387633071433 | validation: 3.4259989491222873]
	TIME [epoch: 8.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342938572008066		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 3.342938572008066 | validation: 3.4258458201256836]
	TIME [epoch: 8.35 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347061257778197		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 3.347061257778197 | validation: 3.440842724886685]
	TIME [epoch: 8.39 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3450910472823066		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 3.3450910472823066 | validation: 3.4427898524427674]
	TIME [epoch: 8.36 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.348954424407805		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 3.348954424407805 | validation: 3.4297100167142007]
	TIME [epoch: 8.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346483187735446		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 3.346483187735446 | validation: 3.42993448779403]
	TIME [epoch: 8.29 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343838405745562		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 3.343838405745562 | validation: 3.429821928598379]
	TIME [epoch: 8.29 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3420444714518953		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 3.3420444714518953 | validation: 3.441908479435299]
	TIME [epoch: 8.29 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3510397232989755		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 3.3510397232989755 | validation: 3.436893726968597]
	TIME [epoch: 8.32 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346804829939506		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 3.346804829939506 | validation: 3.4443137134828117]
	TIME [epoch: 8.32 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.348490156436224		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 3.348490156436224 | validation: 3.444949544674368]
	TIME [epoch: 8.37 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3475320872271417		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 3.3475320872271417 | validation: 3.442066767086329]
	TIME [epoch: 8.29 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3460270558972276		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 3.3460270558972276 | validation: 3.4407268277755474]
	TIME [epoch: 8.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341968761706959		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 3.341968761706959 | validation: 3.4301076805889856]
	TIME [epoch: 8.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33790948204165		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 3.33790948204165 | validation: 3.43235766404211]
	TIME [epoch: 8.33 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3447332110118255		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 3.3447332110118255 | validation: 3.4355309473788624]
	TIME [epoch: 8.32 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343177826758504		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 3.343177826758504 | validation: 3.437822372178745]
	TIME [epoch: 8.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3407395573277427		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 3.3407395573277427 | validation: 3.430057917091802]
	TIME [epoch: 8.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341505827286356		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 3.341505827286356 | validation: 3.446821566018737]
	TIME [epoch: 8.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34226766339168		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 3.34226766339168 | validation: 3.44066426988794]
	TIME [epoch: 8.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346310068464039		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 3.346310068464039 | validation: 3.4338586349197673]
	TIME [epoch: 8.33 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342219909296032		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 3.342219909296032 | validation: 3.4292572594620214]
	TIME [epoch: 8.37 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3402688288503155		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 3.3402688288503155 | validation: 3.435256344410799]
	TIME [epoch: 8.37 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340055083488802		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 3.340055083488802 | validation: 3.435277998984197]
	TIME [epoch: 8.31 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34091518666357		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 3.34091518666357 | validation: 3.4337197102452848]
	TIME [epoch: 8.29 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341280419094396		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 3.341280419094396 | validation: 3.428912761548076]
	TIME [epoch: 8.31 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3398697718788855		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 3.3398697718788855 | validation: 3.4261040569136316]
	TIME [epoch: 8.34 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338291193819336		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 3.338291193819336 | validation: 3.4340903299021295]
	TIME [epoch: 8.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3412511810676984		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 3.3412511810676984 | validation: 3.442561895125383]
	TIME [epoch: 8.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342909051561679		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 3.342909051561679 | validation: 3.4351752616290288]
	TIME [epoch: 8.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3349140518893483		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 3.3349140518893483 | validation: 3.424698269572085]
	TIME [epoch: 8.4 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328485489474648		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 3.328485489474648 | validation: 3.41717168299843]
	TIME [epoch: 8.35 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3323549163955373		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 3.3323549163955373 | validation: 3.4327830433888282]
	TIME [epoch: 8.37 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3304956068864144		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 3.3304956068864144 | validation: 3.424856291386706]
	TIME [epoch: 8.33 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330620729360989		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 3.330620729360989 | validation: 3.4325004522069844]
	TIME [epoch: 8.33 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329882109733172		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 3.329882109733172 | validation: 3.423925793433652]
	TIME [epoch: 8.32 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327308420257946		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 3.327308420257946 | validation: 3.4251960539146316]
	TIME [epoch: 8.33 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3292818278008482		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 3.3292818278008482 | validation: 3.4217710190271537]
	TIME [epoch: 8.36 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3270142777196234		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 3.3270142777196234 | validation: 3.428265259194556]
	TIME [epoch: 8.35 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3294523558801554		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 3.3294523558801554 | validation: 3.422444243612814]
	TIME [epoch: 8.33 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3276095020321774		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 3.3276095020321774 | validation: 3.4162760635201206]
	TIME [epoch: 8.34 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328752874219883		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 3.328752874219883 | validation: 3.4269578599852486]
	TIME [epoch: 8.33 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3282743862427235		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 3.3282743862427235 | validation: 3.4278194778172026]
	TIME [epoch: 8.34 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3296026929915437		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 3.3296026929915437 | validation: 3.4216487396593873]
	TIME [epoch: 8.36 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33384787054544		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 3.33384787054544 | validation: 3.43736132034176]
	TIME [epoch: 8.35 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334726377838378		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 3.334726377838378 | validation: 3.427704025799627]
	TIME [epoch: 8.33 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326725082924361		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 3.326725082924361 | validation: 3.426615172266837]
	TIME [epoch: 8.32 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3260173212192754		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 3.3260173212192754 | validation: 3.4141029595848096]
	TIME [epoch: 8.32 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321940220214353		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 3.321940220214353 | validation: 3.4071490145508756]
	TIME [epoch: 8.33 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3136272583475765		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 3.3136272583475765 | validation: 3.401997226556165]
	TIME [epoch: 8.37 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3180646700752776		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 3.3180646700752776 | validation: 3.4056459599125803]
	TIME [epoch: 8.39 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.317105190566296		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 3.317105190566296 | validation: 3.401350991675022]
	TIME [epoch: 8.32 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3195321030002702		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 3.3195321030002702 | validation: 3.403255324011344]
	TIME [epoch: 8.31 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31836316092594		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 3.31836316092594 | validation: 3.401324143513782]
	TIME [epoch: 8.32 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318177780841206		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 3.318177780841206 | validation: 3.409756589495105]
	TIME [epoch: 8.34 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3155287427406845		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 3.3155287427406845 | validation: 3.395586989795752]
	TIME [epoch: 8.37 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315281230190111		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 3.315281230190111 | validation: 3.3933118492539647]
	TIME [epoch: 8.33 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3149479578094803		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 3.3149479578094803 | validation: 3.395712347118555]
	TIME [epoch: 8.33 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3156101830353224		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 3.3156101830353224 | validation: 3.400228471330589]
	TIME [epoch: 8.32 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315348537982066		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 3.315348537982066 | validation: 3.39112086739863]
	TIME [epoch: 8.32 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.305487517610018		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 3.305487517610018 | validation: 3.3781025760844576]
	TIME [epoch: 8.38 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2989117923436773		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 3.2989117923436773 | validation: 3.3714733517453026]
	TIME [epoch: 8.41 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2967291359871687		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 3.2967291359871687 | validation: 3.377104567909015]
	TIME [epoch: 8.36 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2995565876790742		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 3.2995565876790742 | validation: 3.3789846842379534]
	TIME [epoch: 8.33 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30108245195417		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 3.30108245195417 | validation: 3.381580495173406]
	TIME [epoch: 8.33 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3090894200686587		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 3.3090894200686587 | validation: 3.3876162571105946]
	TIME [epoch: 8.33 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.305150203007612		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 3.305150203007612 | validation: 3.3727274728721377]
	TIME [epoch: 8.35 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.303449838340102		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 3.303449838340102 | validation: 3.3698529845713603]
	TIME [epoch: 8.34 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3023069673636964		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 3.3023069673636964 | validation: 3.377361593546707]
	TIME [epoch: 8.32 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3006087960029387		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 3.3006087960029387 | validation: 3.3767438984702283]
	TIME [epoch: 8.32 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298601723059859		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 3.298601723059859 | validation: 3.366056336384497]
	TIME [epoch: 8.43 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297627502626302		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 3.297627502626302 | validation: 3.372661256305334]
	TIME [epoch: 8.33 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29757934840789		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 3.29757934840789 | validation: 3.3705097676605456]
	TIME [epoch: 8.36 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3013498258849516		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 3.3013498258849516 | validation: 3.3744128986893593]
	TIME [epoch: 8.32 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2974875332417892		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 3.2974875332417892 | validation: 3.374339042989532]
	TIME [epoch: 8.31 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297708899385327		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 3.297708899385327 | validation: 3.3733657206046277]
	TIME [epoch: 8.31 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3004799069614084		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 3.3004799069614084 | validation: 3.370023536890361]
	TIME [epoch: 8.32 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2963826358215336		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 3.2963826358215336 | validation: 3.3582475323192753]
	TIME [epoch: 8.34 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2917453219200214		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 3.2917453219200214 | validation: 3.351808808008211]
	TIME [epoch: 8.35 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287145351317554		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 3.287145351317554 | validation: 3.352670891500371]
	TIME [epoch: 8.32 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2867050759920353		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 3.2867050759920353 | validation: 3.359968387838861]
	TIME [epoch: 8.32 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2903457576301554		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 3.2903457576301554 | validation: 3.3667645187936923]
	TIME [epoch: 8.31 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294637753784394		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 3.294637753784394 | validation: 3.3718948215319564]
	TIME [epoch: 8.31 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2983285885760836		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 3.2983285885760836 | validation: 3.372041073107078]
	TIME [epoch: 8.33 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297495634102839		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 3.297495634102839 | validation: 3.376860717096835]
	TIME [epoch: 8.35 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299276201042948		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 3.299276201042948 | validation: 3.3701565318600046]
	TIME [epoch: 8.32 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.303067946162212		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 3.303067946162212 | validation: 3.3858450096817805]
	TIME [epoch: 8.32 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30451556179119		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 3.30451556179119 | validation: 3.3781988730727246]
	TIME [epoch: 8.32 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.304373048614675		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 3.304373048614675 | validation: 3.375176159923655]
	TIME [epoch: 8.32 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3032170935550553		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 3.3032170935550553 | validation: 3.3700872618562796]
	TIME [epoch: 8.34 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3051143794399267		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 3.3051143794399267 | validation: 3.3735411274211753]
	TIME [epoch: 8.43 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3045227286215244		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 3.3045227286215244 | validation: 3.382375546300082]
	TIME [epoch: 8.31 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3083603606242984		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 3.3083603606242984 | validation: 3.383011600313336]
	TIME [epoch: 8.32 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31133228869553		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 3.31133228869553 | validation: 3.383577031259804]
	TIME [epoch: 8.31 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3164092861216834		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 3.3164092861216834 | validation: 3.3988885075169826]
	TIME [epoch: 8.32 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321205959063887		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 3.321205959063887 | validation: 3.404668793743535]
	TIME [epoch: 8.36 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3299487388805984		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 3.3299487388805984 | validation: 3.4018002056499896]
	TIME [epoch: 8.34 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3296945048843902		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 3.3296945048843902 | validation: 3.4003799962630996]
	TIME [epoch: 8.32 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321662658904977		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 3.321662658904977 | validation: 3.3963723861114135]
	TIME [epoch: 8.32 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323336064895412		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 3.323336064895412 | validation: 3.3983139384975476]
	TIME [epoch: 8.33 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318779753681517		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 3.318779753681517 | validation: 3.386867651283521]
	TIME [epoch: 8.39 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3231169309198725		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 3.3231169309198725 | validation: 3.4034965897165597]
	TIME [epoch: 8.42 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3261109195558536		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 3.3261109195558536 | validation: 3.4008650388714745]
	TIME [epoch: 8.32 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3256661560840057		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 3.3256661560840057 | validation: 3.3941912122850506]
	TIME [epoch: 8.31 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3262457984692		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 3.3262457984692 | validation: 3.397790044451959]
	TIME [epoch: 8.31 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329593498950758		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 3.329593498950758 | validation: 3.4118001191703193]
	TIME [epoch: 8.32 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3380687498888744		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 3.3380687498888744 | validation: 3.4184488197342393]
	TIME [epoch: 8.33 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338770712536408		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 3.338770712536408 | validation: 3.418663603531704]
	TIME [epoch: 8.36 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34097444622242		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 3.34097444622242 | validation: 3.4229681117997117]
	TIME [epoch: 8.32 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3426801475950967		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 3.3426801475950967 | validation: 3.4186138056897084]
	TIME [epoch: 8.32 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424093229017138		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 3.3424093229017138 | validation: 3.422639243387689]
	TIME [epoch: 8.41 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3451551414087177		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 3.3451551414087177 | validation: 3.4219224531525825]
	TIME [epoch: 8.31 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347679321555522		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 3.347679321555522 | validation: 3.4288453951728712]
	TIME [epoch: 8.34 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346192831081477		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 3.346192831081477 | validation: 3.431271127099011]
	TIME [epoch: 8.34 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3525662897088195		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 3.3525662897088195 | validation: 3.435677672606454]
	TIME [epoch: 8.31 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.359273519580352		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 3.359273519580352 | validation: 3.431577924268022]
	TIME [epoch: 8.31 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.360300859745876		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 3.360300859745876 | validation: 3.4459730720815718]
	TIME [epoch: 8.31 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3639729062893977		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 3.3639729062893977 | validation: 3.439637550215868]
	TIME [epoch: 8.31 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.365326240074642		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 3.365326240074642 | validation: 3.462279915705754]
	TIME [epoch: 8.35 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.369906092046924		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 3.369906092046924 | validation: 3.460303946864319]
	TIME [epoch: 8.37 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.36818613327637		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 3.36818613327637 | validation: 3.446290012230599]
	TIME [epoch: 8.38 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3654137341007315		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 3.3654137341007315 | validation: 3.4504827085960494]
	TIME [epoch: 8.36 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3630517193147367		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 3.3630517193147367 | validation: 3.4515045062827365]
	TIME [epoch: 8.32 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3678322439745276		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 3.3678322439745276 | validation: 3.4402373945865055]
	TIME [epoch: 8.33 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366904699177384		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 3.366904699177384 | validation: 3.44992446519489]
	TIME [epoch: 8.35 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3717917554539576		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 3.3717917554539576 | validation: 3.451238499209599]
	TIME [epoch: 8.32 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3741586538453783		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 3.3741586538453783 | validation: 3.462517288305931]
	TIME [epoch: 8.31 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3702459597323964		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 3.3702459597323964 | validation: 3.4515571346356104]
	TIME [epoch: 8.31 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3680585621317984		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 3.3680585621317984 | validation: 3.4380504002932604]
	TIME [epoch: 8.31 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3662928393933518		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 3.3662928393933518 | validation: 3.4454450148880618]
	TIME [epoch: 8.31 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.362443978681044		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 3.362443978681044 | validation: 3.4329167168321817]
	TIME [epoch: 8.44 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3546287195644493		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 3.3546287195644493 | validation: 3.420897455076567]
	TIME [epoch: 8.31 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.35165792458449		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 3.35165792458449 | validation: 3.4181595671237615]
	TIME [epoch: 8.3 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3489100636896865		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 3.3489100636896865 | validation: 3.416788934067569]
	TIME [epoch: 8.31 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343130954662116		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 3.343130954662116 | validation: 3.411991483016887]
	TIME [epoch: 8.32 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3442724019059593		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 3.3442724019059593 | validation: 3.4036359534924316]
	TIME [epoch: 8.33 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3370090666231524		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 3.3370090666231524 | validation: 3.407454397846867]
	TIME [epoch: 8.36 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337436414919636		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 3.337436414919636 | validation: 3.416638941753703]
	TIME [epoch: 8.31 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3375703655256714		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 3.3375703655256714 | validation: 3.4110937679288624]
	TIME [epoch: 8.31 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3383561065037766		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 3.3383561065037766 | validation: 3.407448617147719]
	TIME [epoch: 8.36 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3369917935608435		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 3.3369917935608435 | validation: 3.415684207604139]
	TIME [epoch: 8.39 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3409518010112276		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 3.3409518010112276 | validation: 3.4203748139523897]
	TIME [epoch: 8.36 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3446024690516563		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 3.3446024690516563 | validation: 3.424442409545952]
	TIME [epoch: 8.34 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344048777573983		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 3.344048777573983 | validation: 3.4277014637825163]
	TIME [epoch: 8.31 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.348856329964032		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 3.348856329964032 | validation: 3.422391123312736]
	TIME [epoch: 8.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.350840184620454		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 3.350840184620454 | validation: 3.4217686429397154]
	TIME [epoch: 8.3 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3472676485332022		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 3.3472676485332022 | validation: 3.4258734203815657]
	TIME [epoch: 8.31 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3476904737530626		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 3.3476904737530626 | validation: 3.4211126684455175]
	TIME [epoch: 8.34 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3500999837920604		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 3.3500999837920604 | validation: 3.4217894229923767]
	TIME [epoch: 8.34 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3434766954590702		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 3.3434766954590702 | validation: 3.4172404671753185]
	TIME [epoch: 8.32 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341336524341808		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 3.341336524341808 | validation: 3.410596766268867]
	TIME [epoch: 8.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3435012662085235		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 3.3435012662085235 | validation: 3.41571080204099]
	TIME [epoch: 8.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424245323908384		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 3.3424245323908384 | validation: 3.4160176903162554]
	TIME [epoch: 8.31 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338335211791776		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 3.338335211791776 | validation: 3.419243921687233]
	TIME [epoch: 8.35 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3403662720483602		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 3.3403662720483602 | validation: 3.414236811513928]
	TIME [epoch: 8.31 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3375087239874603		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 3.3375087239874603 | validation: 3.417144200026912]
	TIME [epoch: 8.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3365526411462056		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 3.3365526411462056 | validation: 3.4088221919608825]
	TIME [epoch: 8.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3291812687889735		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 3.3291812687889735 | validation: 3.3986416252797707]
	TIME [epoch: 8.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3279606021382357		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 3.3279606021382357 | validation: 3.3914535029514106]
	TIME [epoch: 8.31 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3261976843522794		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 3.3261976843522794 | validation: 3.393716968407393]
	TIME [epoch: 8.4 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3281309454211305		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 3.3281309454211305 | validation: 3.3988351948707414]
	TIME [epoch: 8.37 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325602147393294		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 3.325602147393294 | validation: 3.399526202218794]
	TIME [epoch: 8.31 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3233180212084648		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 3.3233180212084648 | validation: 3.39847963892932]
	TIME [epoch: 8.3 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327854937541496		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 3.327854937541496 | validation: 3.3939942679258097]
	TIME [epoch: 8.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3335867270579818		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 3.3335867270579818 | validation: 3.3949732270376707]
	TIME [epoch: 8.33 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3293656005781544		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 3.3293656005781544 | validation: 3.405087492530777]
	TIME [epoch: 8.32 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331547833627711		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 3.331547833627711 | validation: 3.400726539920621]
	TIME [epoch: 8.3 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331264395866058		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 3.331264395866058 | validation: 3.3937007641899797]
	TIME [epoch: 8.3 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323371428540357		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 3.323371428540357 | validation: 3.398735125089031]
	TIME [epoch: 8.3 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3224455908712955		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 3.3224455908712955 | validation: 3.3879130168763916]
	TIME [epoch: 8.29 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32466465705369		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 3.32466465705369 | validation: 3.3965041504208546]
	TIME [epoch: 8.42 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322059094411395		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 3.322059094411395 | validation: 3.4010236776647758]
	TIME [epoch: 8.31 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3245864364515194		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 3.3245864364515194 | validation: 3.3967805713734274]
	TIME [epoch: 8.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326358546567521		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 3.326358546567521 | validation: 3.4030757299810057]
	TIME [epoch: 8.3 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325419363853939		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 3.325419363853939 | validation: 3.3977890025092927]
	TIME [epoch: 8.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3223219661643766		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 3.3223219661643766 | validation: 3.394685910084285]
	TIME [epoch: 8.31 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324064754248408		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 3.324064754248408 | validation: 3.398137428177921]
	TIME [epoch: 8.33 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3299964869060794		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 3.3299964869060794 | validation: 3.4111424771239394]
	TIME [epoch: 8.32 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3356166421221713		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 3.3356166421221713 | validation: 3.413829512146234]
	TIME [epoch: 8.32 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3329020090569883		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 3.3329020090569883 | validation: 3.4080968999500785]
	TIME [epoch: 8.37 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331502260342858		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 3.331502260342858 | validation: 3.4057807369629094]
	TIME [epoch: 8.36 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330598086305682		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 3.330598086305682 | validation: 3.407213139605073]
	TIME [epoch: 8.29 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327927634655562		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 3.327927634655562 | validation: 3.4049469954280918]
	TIME [epoch: 8.34 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3299654273901322		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 3.3299654273901322 | validation: 3.4033136437198617]
	TIME [epoch: 8.29 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3278730181250937		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 3.3278730181250937 | validation: 3.408205597708921]
	TIME [epoch: 8.3 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3277095960524834		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 3.3277095960524834 | validation: 3.4036406766127927]
	TIME [epoch: 8.29 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3272088070683403		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 3.3272088070683403 | validation: 3.404908842236125]
	TIME [epoch: 8.3 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3297607241832696		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 3.3297607241832696 | validation: 3.4033711252693157]
	TIME [epoch: 8.32 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3289088797583615		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 3.3289088797583615 | validation: 3.4018440950302318]
	TIME [epoch: 8.35 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329910539023684		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 3.329910539023684 | validation: 3.401544801751088]
	TIME [epoch: 8.31 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3264166022826567		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 3.3264166022826567 | validation: 3.3930914557024496]
	TIME [epoch: 8.37 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324060786967923		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 3.324060786967923 | validation: 3.402167926346819]
	TIME [epoch: 8.29 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3257675076244118		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 3.3257675076244118 | validation: 3.4064077989013763]
	TIME [epoch: 8.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327515291623392		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 3.327515291623392 | validation: 3.399634504837903]
	TIME [epoch: 8.32 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3326688966796647		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 3.3326688966796647 | validation: 3.4057321727157785]
	TIME [epoch: 8.34 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3333833098891574		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 3.3333833098891574 | validation: 3.4142971201100245]
	TIME [epoch: 8.3 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3324691521350127		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 3.3324691521350127 | validation: 3.421490026023147]
	TIME [epoch: 8.3 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340747890218439		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 3.340747890218439 | validation: 3.4101303184206166]
	TIME [epoch: 8.3 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335971674064899		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 3.335971674064899 | validation: 3.421285628347408]
	TIME [epoch: 8.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335893032455626		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 3.335893032455626 | validation: 3.419596359701525]
	TIME [epoch: 8.33 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337847098081176		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 3.337847098081176 | validation: 3.422846617744649]
	TIME [epoch: 8.32 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339104498311239		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 3.339104498311239 | validation: 3.4177544399437054]
	TIME [epoch: 8.3 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339461675003283		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 3.339461675003283 | validation: 3.424463230163674]
	TIME [epoch: 8.29 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340492941017401		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 3.340492941017401 | validation: 3.426884385115547]
	TIME [epoch: 8.29 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338401109317526		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 3.338401109317526 | validation: 3.4154985033395775]
	TIME [epoch: 8.3 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3429264522638023		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 3.3429264522638023 | validation: 3.4225436400872904]
	TIME [epoch: 8.33 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344949594803906		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 3.344949594803906 | validation: 3.4240394421345255]
	TIME [epoch: 8.31 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3463663238664116		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 3.3463663238664116 | validation: 3.4241115255637347]
	TIME [epoch: 8.29 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3444467972324876		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 3.3444467972324876 | validation: 3.434981544870533]
	TIME [epoch: 8.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3486859015272734		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 3.3486859015272734 | validation: 3.432410589146653]
	TIME [epoch: 8.29 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351892193177451		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 3.351892193177451 | validation: 3.428356822883689]
	TIME [epoch: 8.42 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.350371790554806		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 3.350371790554806 | validation: 3.4295221211971256]
	TIME [epoch: 8.34 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.351835640255167		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 3.351835640255167 | validation: 3.4382085654794032]
	TIME [epoch: 8.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346339824946999		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 3.346339824946999 | validation: 3.426926951929273]
	TIME [epoch: 8.3 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3451689629219628		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 3.3451689629219628 | validation: 3.428204382948981]
	TIME [epoch: 8.31 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3469227092285845		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 3.3469227092285845 | validation: 3.422645278283375]
	TIME [epoch: 8.3 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3450454525156483		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 3.3450454525156483 | validation: 3.422436418752295]
	TIME [epoch: 8.31 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3459756287322637		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 3.3459756287322637 | validation: 3.4245755230247044]
	TIME [epoch: 8.41 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345641661782294		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 3.345641661782294 | validation: 3.430234852889549]
	TIME [epoch: 8.37 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.350478973069351		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 3.350478973069351 | validation: 3.436434429459272]
	TIME [epoch: 8.31 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3536598398033592		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 3.3536598398033592 | validation: 3.443074806133292]
	TIME [epoch: 8.3 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354384692984394		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 3.354384692984394 | validation: 3.4417432487066537]
	TIME [epoch: 8.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353221593368956		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 3.353221593368956 | validation: 3.435213866026821]
	TIME [epoch: 8.32 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.352675590983532		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 3.352675590983532 | validation: 3.4344786082131327]
	TIME [epoch: 8.34 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3549854490239968		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 3.3549854490239968 | validation: 3.437998100224747]
	TIME [epoch: 8.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3553551760341773		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 3.3553551760341773 | validation: 3.4413386567670576]
	TIME [epoch: 8.3 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3547583649025032		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 3.3547583649025032 | validation: 3.439779859009482]
	TIME [epoch: 8.3 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3547422502852204		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 3.3547422502852204 | validation: 3.437992679431402]
	TIME [epoch: 8.31 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3579118077415173		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 3.3579118077415173 | validation: 3.441321891608771]
	TIME [epoch: 8.33 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3588369855481774		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 3.3588369855481774 | validation: 3.448829342292523]
	TIME [epoch: 8.33 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3593248705987673		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 3.3593248705987673 | validation: 3.439430398105486]
	TIME [epoch: 8.39 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3550982225953074		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 3.3550982225953074 | validation: 3.4356846986467637]
	TIME [epoch: 8.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3526648422775565		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 3.3526648422775565 | validation: 3.4460495878921726]
	TIME [epoch: 8.3 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3494400877988717		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 3.3494400877988717 | validation: 3.429579024907815]
	TIME [epoch: 8.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3472337010433044		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 3.3472337010433044 | validation: 3.4261599844452966]
	TIME [epoch: 8.33 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3450137120695693		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 3.3450137120695693 | validation: 3.434147225407986]
	TIME [epoch: 8.31 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3464750415227624		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 3.3464750415227624 | validation: 3.4274136783727975]
	TIME [epoch: 8.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3441891826813848		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 3.3441891826813848 | validation: 3.425344383696803]
	TIME [epoch: 8.37 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342849253455784		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 3.342849253455784 | validation: 3.4186483037444457]
	TIME [epoch: 8.36 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3400545203089806		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 3.3400545203089806 | validation: 3.4145045402040815]
	TIME [epoch: 8.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340018854721042		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 3.340018854721042 | validation: 3.4122008855979917]
	TIME [epoch: 8.35 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33936336366458		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 3.33936336366458 | validation: 3.4190150766658576]
	TIME [epoch: 8.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3445159258833663		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 3.3445159258833663 | validation: 3.420460206014278]
	TIME [epoch: 8.29 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344400263800727		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 3.344400263800727 | validation: 3.427980469627225]
	TIME [epoch: 8.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345550119954942		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 3.345550119954942 | validation: 3.428270152512063]
	TIME [epoch: 8.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3463290813708704		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 3.3463290813708704 | validation: 3.424934064884037]
	TIME [epoch: 8.31 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343753807625401		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 3.343753807625401 | validation: 3.4259937168852566]
	TIME [epoch: 8.34 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3483665423756612		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 3.3483665423756612 | validation: 3.4249277647074976]
	TIME [epoch: 8.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3448683567354207		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 3.3448683567354207 | validation: 3.4295666385714085]
	TIME [epoch: 8.29 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3475668476971814		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 3.3475668476971814 | validation: 3.4297112413425577]
	TIME [epoch: 8.29 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345310841904552		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 3.345310841904552 | validation: 3.4269483557091984]
	TIME [epoch: 8.39 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3431546441180267		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 3.3431546441180267 | validation: 3.4302081965446978]
	TIME [epoch: 8.32 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345370459101394		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 3.345370459101394 | validation: 3.4312437032060643]
	TIME [epoch: 8.33 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346508439736877		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 3.346508439736877 | validation: 3.4191889299666283]
	TIME [epoch: 8.31 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3430976755839894		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 3.3430976755839894 | validation: 3.425129954498918]
	TIME [epoch: 8.3 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339169158823885		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 3.339169158823885 | validation: 3.4280707292100328]
	TIME [epoch: 8.29 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3386518298966656		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 3.3386518298966656 | validation: 3.423062860045177]
	TIME [epoch: 8.33 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334983659605244		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 3.334983659605244 | validation: 3.420755165085925]
	TIME [epoch: 8.41 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339623278620405		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 3.339623278620405 | validation: 3.4273990575081665]
	TIME [epoch: 8.36 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336481081612714		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 3.336481081612714 | validation: 3.4246374972923403]
	TIME [epoch: 8.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340757125133817		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 3.340757125133817 | validation: 3.420923736688355]
	TIME [epoch: 8.29 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338197901463843		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 3.338197901463843 | validation: 3.420697075572025]
	TIME [epoch: 8.29 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337733784561809		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 3.337733784561809 | validation: 3.428034316397124]
	TIME [epoch: 8.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3379598360537894		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 3.3379598360537894 | validation: 3.421462815443376]
	TIME [epoch: 8.33 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3364337120522003		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 3.3364337120522003 | validation: 3.4176435558285734]
	TIME [epoch: 8.31 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338424439008296		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 3.338424439008296 | validation: 3.416768607511749]
	TIME [epoch: 8.3 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333391626959572		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 3.333391626959572 | validation: 3.421636330300739]
	TIME [epoch: 8.3 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3361656099880372		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 3.3361656099880372 | validation: 3.419012099806946]
	TIME [epoch: 8.3 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3332690639876654		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 3.3332690639876654 | validation: 3.4136873114465986]
	TIME [epoch: 8.31 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3308577951062506		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 3.3308577951062506 | validation: 3.422645219236536]
	TIME [epoch: 8.35 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332853645713411		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 3.332853645713411 | validation: 3.414229666634866]
	TIME [epoch: 8.41 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3356409208786038		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 3.3356409208786038 | validation: 3.4187738662220357]
	TIME [epoch: 8.33 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3352565005934203		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 3.3352565005934203 | validation: 3.417733634284658]
	TIME [epoch: 8.32 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3374488093107275		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 3.3374488093107275 | validation: 3.4230446781932446]
	TIME [epoch: 8.33 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3384638725940765		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 3.3384638725940765 | validation: 3.4192954541125604]
	TIME [epoch: 8.34 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3360106059721346		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 3.3360106059721346 | validation: 3.4151326054917934]
	TIME [epoch: 8.36 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3307367478666876		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 3.3307367478666876 | validation: 3.407829685120517]
	TIME [epoch: 8.37 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325546203132565		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 3.325546203132565 | validation: 3.4074906700752132]
	TIME [epoch: 8.38 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324531462155124		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 3.324531462155124 | validation: 3.406484861831941]
	TIME [epoch: 8.34 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3271430027442146		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 3.3271430027442146 | validation: 3.4067058127012104]
	TIME [epoch: 8.32 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3282837951260125		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 3.3282837951260125 | validation: 3.3996960487007217]
	TIME [epoch: 8.36 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329729349137444		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 3.329729349137444 | validation: 3.4172354274268404]
	TIME [epoch: 8.34 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3250814764244403		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 3.3250814764244403 | validation: 3.410806265439027]
	TIME [epoch: 8.32 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32419288050073		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 3.32419288050073 | validation: 3.4008694862093085]
	TIME [epoch: 8.32 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3248310157796137		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 3.3248310157796137 | validation: 3.412316642995857]
	TIME [epoch: 8.32 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326201566831279		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 3.326201566831279 | validation: 3.4065600939745373]
	TIME [epoch: 8.32 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327410258726517		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 3.327410258726517 | validation: 3.4157527464300204]
	TIME [epoch: 8.35 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3313046782667906		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 3.3313046782667906 | validation: 3.4095989391791575]
	TIME [epoch: 8.34 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328500060410033		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 3.328500060410033 | validation: 3.408827082345274]
	TIME [epoch: 8.32 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3286205611722335		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 3.3286205611722335 | validation: 3.412453362799706]
	TIME [epoch: 8.32 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328336619155415		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 3.328336619155415 | validation: 3.40861609928766]
	TIME [epoch: 8.43 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3295161065643675		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 3.3295161065643675 | validation: 3.4097393441884103]
	TIME [epoch: 8.34 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327869672129639		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 3.327869672129639 | validation: 3.4131488318743024]
	TIME [epoch: 8.37 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3275714080975454		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 3.3275714080975454 | validation: 3.4068981647430534]
	TIME [epoch: 8.32 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3260503010521427		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 3.3260503010521427 | validation: 3.414007070758248]
	TIME [epoch: 8.32 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3266343880452456		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 3.3266343880452456 | validation: 3.412284118767543]
	TIME [epoch: 8.32 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3294524440399518		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 3.3294524440399518 | validation: 3.4188217679207114]
	TIME [epoch: 8.32 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3308448591625845		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 3.3308448591625845 | validation: 3.4180652489899237]
	TIME [epoch: 8.33 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333728144589798		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 3.333728144589798 | validation: 3.419104082441031]
	TIME [epoch: 8.36 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3308447803895236		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 3.3308447803895236 | validation: 3.4144374195439613]
	TIME [epoch: 8.31 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331602597742586		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 3.331602597742586 | validation: 3.4101572098626]
	TIME [epoch: 8.32 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3311193282045277		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 3.3311193282045277 | validation: 3.416553933058832]
	TIME [epoch: 8.32 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330633835845522		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 3.330633835845522 | validation: 3.421766859938744]
	TIME [epoch: 8.32 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335325613207051		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 3.335325613207051 | validation: 3.425974823353493]
	TIME [epoch: 8.33 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3362039069294496		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 3.3362039069294496 | validation: 3.4239976982433777]
	TIME [epoch: 8.36 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3357011061826847		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 3.3357011061826847 | validation: 3.4208738430091143]
	TIME [epoch: 8.32 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3351460494971246		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 3.3351460494971246 | validation: 3.415564343084425]
	TIME [epoch: 8.32 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331279797068488		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 3.331279797068488 | validation: 3.4272291332489635]
	TIME [epoch: 8.32 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337791173459456		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 3.337791173459456 | validation: 3.424044903212637]
	TIME [epoch: 8.31 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337768001709737		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 3.337768001709737 | validation: 3.420123342123809]
	TIME [epoch: 8.35 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33942691443512		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 3.33942691443512 | validation: 3.420160164487717]
	TIME [epoch: 8.34 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3358075994051304		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 3.3358075994051304 | validation: 3.413570917418293]
	TIME [epoch: 8.32 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3341531324001785		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 3.3341531324001785 | validation: 3.412762152035638]
	TIME [epoch: 8.31 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3323423035269384		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 3.3323423035269384 | validation: 3.4146231157863483]
	TIME [epoch: 8.32 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3333949126660074		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 3.3333949126660074 | validation: 3.419299524974049]
	TIME [epoch: 8.31 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3324359428885977		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 3.3324359428885977 | validation: 3.41791548993773]
	TIME [epoch: 8.37 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3325222044009988		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 3.3325222044009988 | validation: 3.407557702321759]
	TIME [epoch: 8.39 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328384974755224		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 3.328384974755224 | validation: 3.4160933609905566]
	TIME [epoch: 8.36 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331892752976393		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 3.331892752976393 | validation: 3.4093295164968573]
	TIME [epoch: 8.31 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3318772505994136		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 3.3318772505994136 | validation: 3.412099961883145]
	TIME [epoch: 8.32 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3275149406753717		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 3.3275149406753717 | validation: 3.4046000025491185]
	TIME [epoch: 8.32 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330248321734263		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 3.330248321734263 | validation: 3.4090240978890183]
	TIME [epoch: 8.36 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330446241871861		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 3.330446241871861 | validation: 3.4075393355715358]
	TIME [epoch: 8.32 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3321423524086655		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 3.3321423524086655 | validation: 3.4020599394804147]
	TIME [epoch: 8.31 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3292194987845734		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 3.3292194987845734 | validation: 3.4129210393011746]
	TIME [epoch: 8.31 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327152775889928		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 3.327152775889928 | validation: 3.404163785559764]
	TIME [epoch: 8.32 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3304442921229134		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 3.3304442921229134 | validation: 3.4062219229573087]
	TIME [epoch: 8.33 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332582781123164		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 3.332582781123164 | validation: 3.410139621649564]
	TIME [epoch: 8.35 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332050397415892		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 3.332050397415892 | validation: 3.4074492585337692]
	TIME [epoch: 8.32 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3334404109029494		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 3.3334404109029494 | validation: 3.4105090589763662]
	TIME [epoch: 8.31 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3367086111492306		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 3.3367086111492306 | validation: 3.4136325543314774]
	TIME [epoch: 8.39 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3358799739719305		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 3.3358799739719305 | validation: 3.4109017870113236]
	TIME [epoch: 8.31 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3343633837423656		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 3.3343633837423656 | validation: 3.4091109241580404]
	TIME [epoch: 8.34 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332592551278145		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 3.332592551278145 | validation: 3.4095598312795]
	TIME [epoch: 8.34 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3349338986985435		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 3.3349338986985435 | validation: 3.4094547733241383]
	TIME [epoch: 8.32 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335249682954024		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 3.335249682954024 | validation: 3.4143469648899045]
	TIME [epoch: 8.31 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333020164694658		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 3.333020164694658 | validation: 3.414574840895647]
	TIME [epoch: 8.31 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3335170318633205		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 3.3335170318633205 | validation: 3.4092217876429625]
	TIME [epoch: 8.31 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3324298750030783		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 3.3324298750030783 | validation: 3.40215635167098]
	TIME [epoch: 8.34 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331537523342207		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 3.331537523342207 | validation: 3.4078357006918676]
	TIME [epoch: 8.33 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330322131008647		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 3.330322131008647 | validation: 3.4068560794513982]
	TIME [epoch: 8.31 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3287670034469716		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 3.3287670034469716 | validation: 3.4066890280093243]
	TIME [epoch: 8.31 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330994941902828		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 3.330994941902828 | validation: 3.407353011414793]
	TIME [epoch: 8.31 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335871985575357		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 3.335871985575357 | validation: 3.4101708816860796]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330649627901588		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 3.330649627901588 | validation: 3.402491499586166]
	TIME [epoch: 8.34 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332537037794576		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 3.332537037794576 | validation: 3.410951647664488]
	TIME [epoch: 8.32 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337858704276402		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 3.337858704276402 | validation: 3.4114785186586634]
	TIME [epoch: 8.31 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338996566241887		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 3.338996566241887 | validation: 3.4186131915945794]
	TIME [epoch: 8.31 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337536878990779		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 3.337536878990779 | validation: 3.418741507390675]
	TIME [epoch: 8.31 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33892009327992		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 3.33892009327992 | validation: 3.4199058541357967]
	TIME [epoch: 8.32 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3429975604855575		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 3.3429975604855575 | validation: 3.4266658172472146]
	TIME [epoch: 8.43 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346516536677564		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 3.346516536677564 | validation: 3.426650928940803]
	TIME [epoch: 8.31 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3462461869925053		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 3.3462461869925053 | validation: 3.428505013706178]
	TIME [epoch: 8.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3458064788484476		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 3.3458064788484476 | validation: 3.4366204178336988]
	TIME [epoch: 8.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347880354337336		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 3.347880354337336 | validation: 3.4271762004585034]
	TIME [epoch: 8.32 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3474299199512183		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 3.3474299199512183 | validation: 3.430301750884526]
	TIME [epoch: 8.4 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345533887575659		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 3.345533887575659 | validation: 3.4341342682733167]
	TIME [epoch: 8.39 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.349937942299694		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 3.349937942299694 | validation: 3.424279328599031]
	TIME [epoch: 8.31 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3488222418995433		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 3.3488222418995433 | validation: 3.4279688432517137]
	TIME [epoch: 8.3 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346174695236251		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 3.346174695236251 | validation: 3.4360362890882916]
	TIME [epoch: 8.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347757291083752		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 3.347757291083752 | validation: 3.4261974437440936]
	TIME [epoch: 8.31 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3482413385877554		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 3.3482413385877554 | validation: 3.431692560882198]
	TIME [epoch: 8.33 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3443970838269355		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 3.3443970838269355 | validation: 3.4274441254621886]
	TIME [epoch: 8.33 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345671301994744		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 3.345671301994744 | validation: 3.437791823780051]
	TIME [epoch: 8.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347309199077607		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 3.347309199077607 | validation: 3.4281601287470194]
	TIME [epoch: 8.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346104018386801		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 3.346104018386801 | validation: 3.430120793300789]
	TIME [epoch: 8.31 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3414760429184414		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 3.3414760429184414 | validation: 3.429154130681913]
	TIME [epoch: 8.31 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34462634525947		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 3.34462634525947 | validation: 3.4210517061899335]
	TIME [epoch: 8.33 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343291490818376		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 3.343291490818376 | validation: 3.418206462359895]
	TIME [epoch: 8.33 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419761166529685		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 3.3419761166529685 | validation: 3.413136577274801]
	TIME [epoch: 8.3 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377208171895782		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 3.3377208171895782 | validation: 3.416290337447354]
	TIME [epoch: 8.39 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33975594808284		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 3.33975594808284 | validation: 3.4140188632965516]
	TIME [epoch: 8.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337639480475212		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 3.337639480475212 | validation: 3.419899149399569]
	TIME [epoch: 8.31 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3411597977992042		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 3.3411597977992042 | validation: 3.420918540927962]
	TIME [epoch: 8.35 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341547899898094		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 3.341547899898094 | validation: 3.4216836418671175]
	TIME [epoch: 8.31 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3411003277814872		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 3.3411003277814872 | validation: 3.418343423903644]
	TIME [epoch: 8.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3378336239567825		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 3.3378336239567825 | validation: 3.4089211084400857]
	TIME [epoch: 8.29 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3402620211635923		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 3.3402620211635923 | validation: 3.422413063004708]
	TIME [epoch: 8.29 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3397765548620777		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 3.3397765548620777 | validation: 3.4172745296066482]
	TIME [epoch: 8.31 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338446115502322		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 3.338446115502322 | validation: 3.4110050434207095]
	TIME [epoch: 8.33 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339810998200627		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 3.339810998200627 | validation: 3.4171503079885293]
	TIME [epoch: 8.3 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3410786515567064		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 3.3410786515567064 | validation: 3.4159713556619]
	TIME [epoch: 8.3 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341027469980651		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 3.341027469980651 | validation: 3.416795791818501]
	TIME [epoch: 8.3 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337430115683854		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 3.337430115683854 | validation: 3.41246232947087]
	TIME [epoch: 8.3 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338007548820051		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 3.338007548820051 | validation: 3.4124994927041215]
	TIME [epoch: 8.32 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337505644547721		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 3.337505644547721 | validation: 3.4219811367873167]
	TIME [epoch: 8.35 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337597053671388		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 3.337597053671388 | validation: 3.4106961417298187]
	TIME [epoch: 8.31 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3418212420884386		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 3.3418212420884386 | validation: 3.4138861804802687]
	TIME [epoch: 8.31 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342778597442937		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 3.342778597442937 | validation: 3.418865507858363]
	TIME [epoch: 8.31 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342517138284562		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 3.342517138284562 | validation: 3.428946029023727]
	TIME [epoch: 8.31 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340397495299875		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 3.340397495299875 | validation: 3.4249644035733073]
	TIME [epoch: 8.34 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3428596106362725		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 3.3428596106362725 | validation: 3.410913365262368]
	TIME [epoch: 8.32 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338924373471445		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 3.338924373471445 | validation: 3.421347433771997]
	TIME [epoch: 8.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3378911570687406		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 3.3378911570687406 | validation: 3.4147094776373352]
	TIME [epoch: 8.3 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3382365056307646		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 3.3382365056307646 | validation: 3.4118843182468828]
	TIME [epoch: 8.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3372861248202335		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 3.3372861248202335 | validation: 3.4115106599131924]
	TIME [epoch: 8.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334713356461557		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 3.334713356461557 | validation: 3.4117912880824086]
	TIME [epoch: 8.34 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335408631307724		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 3.335408631307724 | validation: 3.4176426733390413]
	TIME [epoch: 8.32 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3351211877148006		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 3.3351211877148006 | validation: 3.4145261223326653]
	TIME [epoch: 8.29 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337878024129808		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 3.337878024129808 | validation: 3.4102878196577886]
	TIME [epoch: 8.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333492551628482		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 3.333492551628482 | validation: 3.407393092825054]
	TIME [epoch: 8.3 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3348351377437804		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 3.3348351377437804 | validation: 3.4189607759601515]
	TIME [epoch: 8.31 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336632289387436		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 3.336632289387436 | validation: 3.415017106764694]
	TIME [epoch: 8.35 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335980388686873		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 3.335980388686873 | validation: 3.4262390474244127]
	TIME [epoch: 8.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3392190643837143		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 3.3392190643837143 | validation: 3.417293856853207]
	TIME [epoch: 8.29 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3400106301736074		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 3.3400106301736074 | validation: 3.4292764279121455]
	TIME [epoch: 8.29 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3359966940478696		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 3.3359966940478696 | validation: 3.424632054284639]
	TIME [epoch: 8.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341123947494397		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 3.341123947494397 | validation: 3.423516581891956]
	TIME [epoch: 8.31 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340499774734758		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 3.340499774734758 | validation: 3.425908441150506]
	TIME [epoch: 8.33 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34385161388721		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 3.34385161388721 | validation: 3.4242281831734234]
	TIME [epoch: 8.3 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3421014134607		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 3.3421014134607 | validation: 3.4210626441120695]
	TIME [epoch: 8.31 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339901907743907		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 3.339901907743907 | validation: 3.4185971324292423]
	TIME [epoch: 8.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3368596804873425		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 3.3368596804873425 | validation: 3.4205232381234447]
	TIME [epoch: 8.3 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3373245748395606		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 3.3373245748395606 | validation: 3.4229691315019797]
	TIME [epoch: 8.32 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33845119211571		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 3.33845119211571 | validation: 3.4135567908152313]
	TIME [epoch: 8.34 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3378606325183102		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 3.3378606325183102 | validation: 3.415231770494584]
	TIME [epoch: 8.31 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3385699460282767		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 3.3385699460282767 | validation: 3.4185826934837564]
	TIME [epoch: 8.31 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341700897588599		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 3.341700897588599 | validation: 3.418141670375204]
	TIME [epoch: 8.29 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339799428205735		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 3.339799428205735 | validation: 3.4184885232018667]
	TIME [epoch: 8.29 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337088204435299		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 3.337088204435299 | validation: 3.41502545418954]
	TIME [epoch: 8.33 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335427427017844		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 3.335427427017844 | validation: 3.4142183039106984]
	TIME [epoch: 8.31 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3335322855309375		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 3.3335322855309375 | validation: 3.4109571501871674]
	TIME [epoch: 8.29 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3360038953975244		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 3.3360038953975244 | validation: 3.4154714543571756]
	TIME [epoch: 8.29 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3361138802471757		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 3.3361138802471757 | validation: 3.417411461157794]
	TIME [epoch: 8.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340293823749448		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 3.340293823749448 | validation: 3.4170823495887137]
	TIME [epoch: 8.29 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3395306101105966		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 3.3395306101105966 | validation: 3.4166405608241517]
	TIME [epoch: 8.32 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344795283791611		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 3.344795283791611 | validation: 3.4229776432673322]
	TIME [epoch: 8.32 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343446015268456		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 3.343446015268456 | validation: 3.4224780407659297]
	TIME [epoch: 8.31 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3441781586605606		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 3.3441781586605606 | validation: 3.423599379728265]
	TIME [epoch: 8.31 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345302344712126		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 3.345302344712126 | validation: 3.423519736583149]
	TIME [epoch: 8.31 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3423646777017506		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 3.3423646777017506 | validation: 3.422234960531688]
	TIME [epoch: 8.32 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3427435778143204		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 3.3427435778143204 | validation: 3.4169296591473555]
	TIME [epoch: 8.35 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3412734735205243		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 3.3412734735205243 | validation: 3.4245285297258192]
	TIME [epoch: 8.3 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342059707583145		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 3.342059707583145 | validation: 3.4224288971464807]
	TIME [epoch: 8.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424020163392907		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 3.3424020163392907 | validation: 3.4237928475044725]
	TIME [epoch: 8.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342766123635287		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 3.342766123635287 | validation: 3.4320434629986813]
	TIME [epoch: 8.3 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3458680041025985		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 3.3458680041025985 | validation: 3.4236680525930705]
	TIME [epoch: 8.32 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344637974294622		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 3.344637974294622 | validation: 3.4302879960494206]
	TIME [epoch: 8.34 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344712444471076		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 3.344712444471076 | validation: 3.42664269987736]
	TIME [epoch: 8.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3453364027039765		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 3.3453364027039765 | validation: 3.4120108462201033]
	TIME [epoch: 8.3 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396680744762075		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 3.3396680744762075 | validation: 3.4131663359290405]
	TIME [epoch: 8.3 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339266028805566		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 3.339266028805566 | validation: 3.4248090937294755]
	TIME [epoch: 8.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339686619397796		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 3.339686619397796 | validation: 3.4140627963362573]
	TIME [epoch: 8.33 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338600641430233		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 3.338600641430233 | validation: 3.413723270204027]
	TIME [epoch: 8.33 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337867566894028		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 3.337867566894028 | validation: 3.4152531807323987]
	TIME [epoch: 8.31 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337362011056346		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 3.337362011056346 | validation: 3.4218544571545984]
	TIME [epoch: 8.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337271940578032		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 3.337271940578032 | validation: 3.412335151647933]
	TIME [epoch: 8.3 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34164579908757		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 3.34164579908757 | validation: 3.4067868468188216]
	TIME [epoch: 8.3 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3405335907361513		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 3.3405335907361513 | validation: 3.421826759981286]
	TIME [epoch: 8.34 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3397851970085433		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 3.3397851970085433 | validation: 3.4179643173865877]
	TIME [epoch: 8.32 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3384973316921105		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 3.3384973316921105 | validation: 3.4150985043855604]
	TIME [epoch: 8.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337038237726828		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 3.337038237726828 | validation: 3.415215747181147]
	TIME [epoch: 8.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3385931198358483		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 3.3385931198358483 | validation: 3.411526184749718]
	TIME [epoch: 8.3 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3355721651096353		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 3.3355721651096353 | validation: 3.4090440317011277]
	TIME [epoch: 8.31 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333613306764151		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 3.333613306764151 | validation: 3.4076528610811563]
	TIME [epoch: 8.33 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334522457619749		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 3.334522457619749 | validation: 3.4143037964467284]
	TIME [epoch: 8.31 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3336909798160606		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 3.3336909798160606 | validation: 3.41546345206255]
	TIME [epoch: 8.3 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338979317415249		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 3.338979317415249 | validation: 3.417431041802702]
	TIME [epoch: 8.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3395035630616716		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 3.3395035630616716 | validation: 3.4215332064032307]
	TIME [epoch: 8.3 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3414693186390965		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 3.3414693186390965 | validation: 3.423920453153844]
	TIME [epoch: 8.32 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341041595923869		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 3.341041595923869 | validation: 3.422038262597243]
	TIME [epoch: 8.35 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340549431826778		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 3.340549431826778 | validation: 3.43159260705033]
	TIME [epoch: 8.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344907772181781		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 3.344907772181781 | validation: 3.429865770932534]
	TIME [epoch: 8.3 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3445542569520237		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 3.3445542569520237 | validation: 3.4244200138619587]
	TIME [epoch: 8.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341092428749293		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 3.341092428749293 | validation: 3.42736826195427]
	TIME [epoch: 8.29 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343879149895371		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 3.343879149895371 | validation: 3.4201063624051313]
	TIME [epoch: 8.32 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343897955632712		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 3.343897955632712 | validation: 3.4209254222087466]
	TIME [epoch: 8.34 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343422965821052		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 3.343422965821052 | validation: 3.4171427209253453]
	TIME [epoch: 8.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3449398896201537		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 3.3449398896201537 | validation: 3.420333835923406]
	TIME [epoch: 8.29 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3467664471252556		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 3.3467664471252556 | validation: 3.4295910764913353]
	TIME [epoch: 8.29 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342952209012681		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 3.342952209012681 | validation: 3.427099916898226]
	TIME [epoch: 8.3 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3454215486553966		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 3.3454215486553966 | validation: 3.420116768573271]
	TIME [epoch: 8.33 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3422384038971846		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 3.3422384038971846 | validation: 3.42363611589158]
	TIME [epoch: 8.33 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344188906652617		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 3.344188906652617 | validation: 3.4087226795455283]
	TIME [epoch: 8.31 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3429092930710893		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 3.3429092930710893 | validation: 3.418800246762336]
	TIME [epoch: 8.3 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34278976570067		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 3.34278976570067 | validation: 3.4210238871505982]
	TIME [epoch: 8.3 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3443109549297465		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 3.3443109549297465 | validation: 3.4239998904131266]
	TIME [epoch: 8.29 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341730713393539		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 3.341730713393539 | validation: 3.423405641494773]
	TIME [epoch: 8.33 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3388957432683326		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 3.3388957432683326 | validation: 3.412098220193412]
	TIME [epoch: 8.32 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3387161301159285		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 3.3387161301159285 | validation: 3.423987858453935]
	TIME [epoch: 8.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340732433541548		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 3.340732433541548 | validation: 3.4167785362697733]
	TIME [epoch: 8.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3394504361980664		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 3.3394504361980664 | validation: 3.4114027868845875]
	TIME [epoch: 8.3 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339737904486073		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 3.339737904486073 | validation: 3.4110479056022114]
	TIME [epoch: 8.3 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3349440175673815		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 3.3349440175673815 | validation: 3.4094904856978605]
	TIME [epoch: 8.33 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3361135952332446		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 3.3361135952332446 | validation: 3.4117686637863587]
	TIME [epoch: 8.32 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338103387369644		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 3.338103387369644 | validation: 3.416833883112986]
	TIME [epoch: 8.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3376953417841566		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 3.3376953417841566 | validation: 3.4194584201862783]
	TIME [epoch: 8.3 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3381019924887507		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 3.3381019924887507 | validation: 3.408199119054194]
	TIME [epoch: 8.31 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339673651185061		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 3.339673651185061 | validation: 3.415020232582699]
	TIME [epoch: 8.31 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3401339936834686		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 3.3401339936834686 | validation: 3.418955438835665]
	TIME [epoch: 8.34 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340818347509566		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 3.340818347509566 | validation: 3.4172677463490895]
	TIME [epoch: 8.33 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3359735217592283		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 3.3359735217592283 | validation: 3.4198001023695768]
	TIME [epoch: 8.33 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3354532084703425		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 3.3354532084703425 | validation: 3.418884172773341]
	TIME [epoch: 8.33 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3366660760937936		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 3.3366660760937936 | validation: 3.4131615629826397]
	TIME [epoch: 8.33 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3368460526162163		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 3.3368460526162163 | validation: 3.415469666293248]
	TIME [epoch: 8.34 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3390421591477937		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 3.3390421591477937 | validation: 3.4173287676083337]
	TIME [epoch: 8.37 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3415458104369895		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 3.3415458104369895 | validation: 3.4100648659616493]
	TIME [epoch: 8.32 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337830424426252		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 3.337830424426252 | validation: 3.4151816992702537]
	TIME [epoch: 8.32 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334719632364701		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 3.334719632364701 | validation: 3.41201611601105]
	TIME [epoch: 8.32 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336235844585991		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 3.336235844585991 | validation: 3.4108490933092277]
	TIME [epoch: 8.33 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3341548041399034		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 3.3341548041399034 | validation: 3.4075861273965478]
	TIME [epoch: 8.36 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332995350546823		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 3.332995350546823 | validation: 3.414090399207045]
	TIME [epoch: 8.37 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331413624940399		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 3.331413624940399 | validation: 3.4098325300231087]
	TIME [epoch: 8.33 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3351330422905185		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 3.3351330422905185 | validation: 3.4100288832266257]
	TIME [epoch: 8.32 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3342496471892424		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 3.3342496471892424 | validation: 3.4142627117853808]
	TIME [epoch: 8.33 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338340749405263		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 3.338340749405263 | validation: 3.4138979222450243]
	TIME [epoch: 8.34 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3356940825494026		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 3.3356940825494026 | validation: 3.4170237601432554]
	TIME [epoch: 8.37 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3362861116418077		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 3.3362861116418077 | validation: 3.412601601417077]
	TIME [epoch: 8.35 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33845973169554		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 3.33845973169554 | validation: 3.41803860847262]
	TIME [epoch: 8.32 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337712470918799		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 3.337712470918799 | validation: 3.4112874439737633]
	TIME [epoch: 8.32 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337269302943921		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 3.337269302943921 | validation: 3.4122356458670624]
	TIME [epoch: 8.32 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336589071272217		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 3.336589071272217 | validation: 3.4086313631936718]
	TIME [epoch: 8.33 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3353715950933776		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 3.3353715950933776 | validation: 3.406761869171963]
	TIME [epoch: 8.37 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335843689874068		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 3.335843689874068 | validation: 3.4099849328608576]
	TIME [epoch: 8.33 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3354343707314404		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 3.3354343707314404 | validation: 3.4090556250521686]
	TIME [epoch: 8.33 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3341037103314077		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 3.3341037103314077 | validation: 3.409630028992374]
	TIME [epoch: 8.32 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332824082751248		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 3.332824082751248 | validation: 3.412109223476153]
	TIME [epoch: 8.31 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336289867086429		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 3.336289867086429 | validation: 3.4142925325979943]
	TIME [epoch: 8.33 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334575537522177		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 3.334575537522177 | validation: 3.4118020995190608]
	TIME [epoch: 8.36 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334761836288246		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 3.334761836288246 | validation: 3.407919966011468]
	TIME [epoch: 8.33 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336325373779726		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 3.336325373779726 | validation: 3.4099539035411706]
	TIME [epoch: 8.33 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3332843270920716		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 3.3332843270920716 | validation: 3.4158637374239946]
	TIME [epoch: 8.34 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336005706528554		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 3.336005706528554 | validation: 3.416853512468041]
	TIME [epoch: 8.33 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377735123611396		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 3.3377735123611396 | validation: 3.4205573105168785]
	TIME [epoch: 8.34 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337578179938506		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 3.337578179938506 | validation: 3.415604959662446]
	TIME [epoch: 8.36 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3399474399206053		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 3.3399474399206053 | validation: 3.417902536822387]
	TIME [epoch: 8.32 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3386474053715745		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 3.3386474053715745 | validation: 3.4184015603075846]
	TIME [epoch: 8.32 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3399678686214185		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 3.3399678686214185 | validation: 3.420918673181661]
	TIME [epoch: 8.31 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338927500791109		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 3.338927500791109 | validation: 3.4210653502005157]
	TIME [epoch: 8.33 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339538927758957		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 3.339538927758957 | validation: 3.4110574462059224]
	TIME [epoch: 8.36 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3401467845045127		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 3.3401467845045127 | validation: 3.4199112106019984]
	TIME [epoch: 8.35 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3436723045006636		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 3.3436723045006636 | validation: 3.4229988224662806]
	TIME [epoch: 8.33 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341975789120868		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 3.341975789120868 | validation: 3.421007184481847]
	TIME [epoch: 8.32 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3442339632168756		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 3.3442339632168756 | validation: 3.4182491073562584]
	TIME [epoch: 8.32 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3427579096376423		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 3.3427579096376423 | validation: 3.421955780871982]
	TIME [epoch: 8.33 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3433769086419267		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 3.3433769086419267 | validation: 3.420551502906983]
	TIME [epoch: 8.36 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3406115653886217		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 3.3406115653886217 | validation: 3.4269647594089734]
	TIME [epoch: 8.34 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3397150673002174		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 3.3397150673002174 | validation: 3.4279722747232313]
	TIME [epoch: 8.31 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3406869283780827		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 3.3406869283780827 | validation: 3.427609971457687]
	TIME [epoch: 8.31 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345815972357161		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 3.345815972357161 | validation: 3.4223804217632074]
	TIME [epoch: 8.33 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3428359770218967		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 3.3428359770218967 | validation: 3.4327516583424176]
	TIME [epoch: 8.33 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3437298288632666		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 3.3437298288632666 | validation: 3.4332796691815304]
	TIME [epoch: 8.38 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343125687523231		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 3.343125687523231 | validation: 3.42374390983585]
	TIME [epoch: 8.32 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340748038819573		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 3.340748038819573 | validation: 3.4190576367139087]
	TIME [epoch: 8.32 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339833876111537		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 3.339833876111537 | validation: 3.421720789031627]
	TIME [epoch: 8.33 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3425695223182776		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 3.3425695223182776 | validation: 3.416225906278987]
	TIME [epoch: 8.32 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3400738377011665		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 3.3400738377011665 | validation: 3.4158644136184684]
	TIME [epoch: 8.34 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3415620301666054		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 3.3415620301666054 | validation: 3.417801624810855]
	TIME [epoch: 8.37 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3392821616695736		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 3.3392821616695736 | validation: 3.422884573417228]
	TIME [epoch: 8.33 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341561506391482		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 3.341561506391482 | validation: 3.4203107631648306]
	TIME [epoch: 8.32 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3393021217355985		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 3.3393021217355985 | validation: 3.4171339182882976]
	TIME [epoch: 8.32 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396066447347224		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 3.3396066447347224 | validation: 3.4183971218393383]
	TIME [epoch: 8.31 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33934237662597		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 3.33934237662597 | validation: 3.416838041343084]
	TIME [epoch: 8.35 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337639014044152		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 3.337639014044152 | validation: 3.422899915716835]
	TIME [epoch: 8.34 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341442876865801		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 3.341442876865801 | validation: 3.4176948301403125]
	TIME [epoch: 8.32 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339744544622627		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 3.339744544622627 | validation: 3.421590660608963]
	TIME [epoch: 8.31 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419476510021333		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 3.3419476510021333 | validation: 3.4224202941960367]
	TIME [epoch: 8.31 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3410448863769187		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 3.3410448863769187 | validation: 3.4204136893476718]
	TIME [epoch: 8.31 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3374452250736506		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 3.3374452250736506 | validation: 3.4222128890678807]
	TIME [epoch: 8.34 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3355971089652523		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 3.3355971089652523 | validation: 3.4177685237426294]
	TIME [epoch: 8.33 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336689202508645		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 3.336689202508645 | validation: 3.4151046661183457]
	TIME [epoch: 8.31 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377389632119385		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 3.3377389632119385 | validation: 3.4243447057168543]
	TIME [epoch: 8.31 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339399259100684		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 3.339399259100684 | validation: 3.425750029240513]
	TIME [epoch: 8.32 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3384788115298276		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 3.3384788115298276 | validation: 3.4227920691298337]
	TIME [epoch: 8.32 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3353701061843655		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 3.3353701061843655 | validation: 3.4214486811705687]
	TIME [epoch: 8.36 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338355417934273		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 3.338355417934273 | validation: 3.414554826047742]
	TIME [epoch: 8.32 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33735921225434		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 3.33735921225434 | validation: 3.41902658902519]
	TIME [epoch: 8.31 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3349714593555375		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 3.3349714593555375 | validation: 3.4133850341003065]
	TIME [epoch: 8.31 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337318580757561		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 3.337318580757561 | validation: 3.4208283499320986]
	TIME [epoch: 8.31 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336727734021034		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 3.336727734021034 | validation: 3.4163345586101963]
	TIME [epoch: 8.33 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3352341659511846		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 3.3352341659511846 | validation: 3.418992131380011]
	TIME [epoch: 8.36 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336919255535901		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 3.336919255535901 | validation: 3.4199491799371455]
	TIME [epoch: 8.31 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.333814899867165		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 3.333814899867165 | validation: 3.418259161570262]
	TIME [epoch: 8.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336328475918354		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 3.336328475918354 | validation: 3.4163276668199316]
	TIME [epoch: 8.31 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336626656700968		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 3.336626656700968 | validation: 3.4147955588176053]
	TIME [epoch: 8.31 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377832674831303		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 3.3377832674831303 | validation: 3.4233050271232437]
	TIME [epoch: 8.32 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33658647618316		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 3.33658647618316 | validation: 3.4221507062967023]
	TIME [epoch: 8.34 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338701009171789		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 3.338701009171789 | validation: 3.414178746090591]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337928752871786		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 3.337928752871786 | validation: 3.417874456213911]
	TIME [epoch: 8.3 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3373835770267606		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 3.3373835770267606 | validation: 3.416578637619338]
	TIME [epoch: 8.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338176599020158		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 3.338176599020158 | validation: 3.4253615067708374]
	TIME [epoch: 8.31 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3379780814874223		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 3.3379780814874223 | validation: 3.422693790274696]
	TIME [epoch: 8.32 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338495861295872		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 3.338495861295872 | validation: 3.414070069101127]
	TIME [epoch: 8.33 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396807511003566		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 3.3396807511003566 | validation: 3.4227335084916772]
	TIME [epoch: 8.31 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337027770822822		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 3.337027770822822 | validation: 3.423400708179904]
	TIME [epoch: 8.3 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33796627623363		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 3.33796627623363 | validation: 3.4200693731301683]
	TIME [epoch: 8.31 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338038171633996		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 3.338038171633996 | validation: 3.416510830899352]
	TIME [epoch: 8.31 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338225121045924		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 3.338225121045924 | validation: 3.418518470713428]
	TIME [epoch: 8.34 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3421336799107464		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 3.3421336799107464 | validation: 3.4200263204112247]
	TIME [epoch: 8.33 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3369069921818713		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 3.3369069921818713 | validation: 3.4217621639956466]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3403773314928946		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 3.3403773314928946 | validation: 3.4270516855106257]
	TIME [epoch: 8.3 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340583344734466		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 3.340583344734466 | validation: 3.422711555710514]
	TIME [epoch: 8.29 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339256806416272		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 3.339256806416272 | validation: 3.4297870655626355]
	TIME [epoch: 8.3 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342144889837683		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 3.342144889837683 | validation: 3.4269235602287718]
	TIME [epoch: 8.34 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3440119693328256		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 3.3440119693328256 | validation: 3.422354396919013]
	TIME [epoch: 8.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34181381847211		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 3.34181381847211 | validation: 3.424259117380039]
	TIME [epoch: 8.29 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341233368215009		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 3.341233368215009 | validation: 3.4284582362876863]
	TIME [epoch: 8.3 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343040852153387		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 3.343040852153387 | validation: 3.434161775535828]
	TIME [epoch: 8.29 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3422305363771008		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 3.3422305363771008 | validation: 3.426594043725633]
	TIME [epoch: 8.31 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341870569284503		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 3.341870569284503 | validation: 3.425046341652088]
	TIME [epoch: 8.34 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3432277358907236		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 3.3432277358907236 | validation: 3.424544925200298]
	TIME [epoch: 8.3 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342845145436173		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 3.342845145436173 | validation: 3.42776734961291]
	TIME [epoch: 8.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419791242213472		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 3.3419791242213472 | validation: 3.432576469330945]
	TIME [epoch: 8.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3452208437886046		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 3.3452208437886046 | validation: 3.430080991106701]
	TIME [epoch: 8.29 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342319526929625		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 3.342319526929625 | validation: 3.4277470983253635]
	TIME [epoch: 8.31 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34473908857927		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 3.34473908857927 | validation: 3.4267949917663314]
	TIME [epoch: 8.33 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342414395670117		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 3.342414395670117 | validation: 3.426019210301461]
	TIME [epoch: 8.3 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419988228491073		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 3.3419988228491073 | validation: 3.4327000178891334]
	TIME [epoch: 8.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3410165537238368		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 3.3410165537238368 | validation: 3.427309313768018]
	TIME [epoch: 8.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3439977636279448		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 3.3439977636279448 | validation: 3.4273481371561934]
	TIME [epoch: 8.3 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3440268883663022		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 3.3440268883663022 | validation: 3.4279971253495827]
	TIME [epoch: 8.36 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342619350395781		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 3.342619350395781 | validation: 3.429644524730066]
	TIME [epoch: 8.27 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3414102817016578		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 3.3414102817016578 | validation: 3.4286818154843086]
	TIME [epoch: 8.25 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3423888611945975		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 3.3423888611945975 | validation: 3.426759106796135]
	TIME [epoch: 8.26 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3458390875084634		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 3.3458390875084634 | validation: 3.4275986481371827]
	TIME [epoch: 8.25 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419091928557316		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 3.3419091928557316 | validation: 3.4185731736052425]
	TIME [epoch: 8.25 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3425591189026957		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 3.3425591189026957 | validation: 3.4202279291691067]
	TIME [epoch: 8.28 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342058318706491		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 3.342058318706491 | validation: 3.4177918612242535]
	TIME [epoch: 8.27 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342172248690013		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 3.342172248690013 | validation: 3.425790139083679]
	TIME [epoch: 8.24 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340653570040662		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 3.340653570040662 | validation: 3.429998590549622]
	TIME [epoch: 8.24 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424241384466815		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 3.3424241384466815 | validation: 3.4150431638578196]
	TIME [epoch: 8.24 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340786514375703		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 3.340786514375703 | validation: 3.427191021698455]
	TIME [epoch: 8.26 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3413901617428707		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 3.3413901617428707 | validation: 3.4260107985328276]
	TIME [epoch: 8.28 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33971430885156		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 3.33971430885156 | validation: 3.418285162883034]
	TIME [epoch: 8.26 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338221535701522		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 3.338221535701522 | validation: 3.4202870117387554]
	TIME [epoch: 8.25 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3392999057933768		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 3.3392999057933768 | validation: 3.425461915257923]
	TIME [epoch: 8.26 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3412275840881307		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 3.3412275840881307 | validation: 3.416946891497532]
	TIME [epoch: 8.26 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3389828349230806		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 3.3389828349230806 | validation: 3.427094205500815]
	TIME [epoch: 8.26 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337474772288172		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 3.337474772288172 | validation: 3.4233866677434697]
	TIME [epoch: 8.29 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3388175004441623		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 3.3388175004441623 | validation: 3.4226394135463556]
	TIME [epoch: 8.25 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340137261547406		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 3.340137261547406 | validation: 3.4213356592508672]
	TIME [epoch: 8.32 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3386401032227355		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 3.3386401032227355 | validation: 3.421103983744864]
	TIME [epoch: 8.25 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3356406202540976		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 3.3356406202540976 | validation: 3.414453258745091]
	TIME [epoch: 8.26 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3377255495598366		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 3.3377255495598366 | validation: 3.4182726167594204]
	TIME [epoch: 8.27 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3393512588159076		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 3.3393512588159076 | validation: 3.4192290066224493]
	TIME [epoch: 8.28 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340381156575168		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 3.340381156575168 | validation: 3.421123854456843]
	TIME [epoch: 8.25 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3368167596609952		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 3.3368167596609952 | validation: 3.4137440572594917]
	TIME [epoch: 8.24 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3419277855540215		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 3.3419277855540215 | validation: 3.4210740455386888]
	TIME [epoch: 8.26 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3412211190764194		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 3.3412211190764194 | validation: 3.419422765432457]
	TIME [epoch: 8.25 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3420802452861134		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 3.3420802452861134 | validation: 3.4195921154984767]
	TIME [epoch: 8.27 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396048302002805		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 3.3396048302002805 | validation: 3.4202349647620203]
	TIME [epoch: 8.27 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339120716242338		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 3.339120716242338 | validation: 3.4261117915239967]
	TIME [epoch: 8.24 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340750675143899		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 3.340750675143899 | validation: 3.4196934539105035]
	TIME [epoch: 8.25 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3445802517444205		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 3.3445802517444205 | validation: 3.4317459521029594]
	TIME [epoch: 8.25 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340820319119699		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 3.340820319119699 | validation: 3.419844760922217]
	TIME [epoch: 8.25 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396659877448513		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 3.3396659877448513 | validation: 3.4242896919278394]
	TIME [epoch: 8.29 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3408481737941393		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 3.3408481737941393 | validation: 3.417269811856502]
	TIME [epoch: 8.27 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334389922671167		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 3.334389922671167 | validation: 3.418830668849603]
	TIME [epoch: 8.27 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340484633470201		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 3.340484633470201 | validation: 3.4285089856730124]
	TIME [epoch: 8.25 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3395003792120725		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 3.3395003792120725 | validation: 3.422963024695732]
	TIME [epoch: 8.25 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340280987698881		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 3.340280987698881 | validation: 3.4241256349273104]
	TIME [epoch: 8.38 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3431700693583295		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 3.3431700693583295 | validation: 3.4220460778961295]
	TIME [epoch: 8.28 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338605896963443		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 3.338605896963443 | validation: 3.4241183075525212]
	TIME [epoch: 8.26 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343075915629661		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 3.343075915629661 | validation: 3.4230902999344917]
	TIME [epoch: 8.25 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343209519221661		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 3.343209519221661 | validation: 3.426575598819918]
	TIME [epoch: 8.26 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341285148258885		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 3.341285148258885 | validation: 3.4275685018175457]
	TIME [epoch: 8.24 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339355217468416		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 3.339355217468416 | validation: 3.415471834978411]
	TIME [epoch: 8.26 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339726480485544		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 3.339726480485544 | validation: 3.41840532407105]
	TIME [epoch: 8.29 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34036979626405		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 3.34036979626405 | validation: 3.419850713627448]
	TIME [epoch: 8.26 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.338625505410932		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 3.338625505410932 | validation: 3.4191622828410124]
	TIME [epoch: 8.25 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337764245726052		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 3.337764245726052 | validation: 3.4209252820012566]
	TIME [epoch: 8.24 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.34000288736456		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 3.34000288736456 | validation: 3.4193849837231522]
	TIME [epoch: 8.25 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.342677390067928		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 3.342677390067928 | validation: 3.4201286207720196]
	TIME [epoch: 8.26 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3396189931426536		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 3.3396189931426536 | validation: 3.4258430965231]
	TIME [epoch: 8.28 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3409734966868045		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 3.3409734966868045 | validation: 3.4179550332047803]
	TIME [epoch: 8.27 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339450325121615		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 3.339450325121615 | validation: 3.422432620506779]
	TIME [epoch: 8.26 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341121901882139		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 3.341121901882139 | validation: 3.4230996143282484]
	TIME [epoch: 8.26 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341509089326307		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 3.341509089326307 | validation: 3.425208910991055]
	TIME [epoch: 8.24 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340733368997887		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 3.340733368997887 | validation: 3.415555156667436]
	TIME [epoch: 8.27 sec]
Finished training in 17888.412 seconds.
