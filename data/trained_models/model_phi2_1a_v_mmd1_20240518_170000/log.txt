Args:
Namespace(name='model_phi2_1a_v_mmd1', outdir='out/model_training/model_phi2_1a_v_mmd1', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3162392242

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.834461765239804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.834461765239804 | validation: 5.829810699011951]
	TIME [epoch: 103 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.493348333217585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.493348333217585 | validation: 5.364702951138386]
	TIME [epoch: 8.42 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.114440809888619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.114440809888619 | validation: 4.9935700034088875]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8587537015586015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8587537015586015 | validation: 4.4970215924522225]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.620763496182738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.620763496182738 | validation: 3.9633416026328963]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4637765074611755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4637765074611755 | validation: 5.410643489865878]
	TIME [epoch: 8.38 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.054798289289082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.054798289289082 | validation: 5.085888179002764]
	TIME [epoch: 8.33 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.51854492336993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.51854492336993 | validation: 4.096707430514754]
	TIME [epoch: 8.34 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9099854082039767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9099854082039767 | validation: 3.639283854329044]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.737449794476624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737449794476624 | validation: 3.7208214617575055]
	TIME [epoch: 8.34 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5004994180222466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5004994180222466 | validation: 3.455585585939942]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196579379253551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.196579379253551 | validation: 3.358341532169299]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1590964243855897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1590964243855897 | validation: 3.2000526634573374]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.946035161575274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.946035161575274 | validation: 2.9520271714737403]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7898760067265056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7898760067265056 | validation: 2.945399986339991]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.761613248458991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.761613248458991 | validation: 2.7823571465157158]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6646783602409556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6646783602409556 | validation: 2.7635356546214345]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.600147648052933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.600147648052933 | validation: 2.7089059568566856]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6194692695151978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6194692695151978 | validation: 2.8957747514834367]
	TIME [epoch: 8.33 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6489762038805242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6489762038805242 | validation: 2.665436975331573]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.536485400539436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.536485400539436 | validation: 2.660599962644424]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5478500732399345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5478500732399345 | validation: 2.6810031034314328]
	TIME [epoch: 8.38 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5400703080894944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5400703080894944 | validation: 2.687097159325165]
	TIME [epoch: 8.34 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5526448903096353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5526448903096353 | validation: 2.6430228278578074]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.528156549791412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.528156549791412 | validation: 2.591570169239322]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.45513416368771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.45513416368771 | validation: 2.5937278695652215]
	TIME [epoch: 8.33 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5392638018727434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5392638018727434 | validation: 2.57517142198905]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4958772192587224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4958772192587224 | validation: 2.580657991924992]
	TIME [epoch: 8.34 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.458073187692711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.458073187692711 | validation: 2.6102958052185166]
	TIME [epoch: 8.33 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.47276952202073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.47276952202073 | validation: 2.659369276252863]
	TIME [epoch: 8.32 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466454346513793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.466454346513793 | validation: 2.5651993996307834]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.428235627663062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.428235627663062 | validation: 2.5573330681322117]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5100170050213713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5100170050213713 | validation: 2.933541123183793]
	TIME [epoch: 8.37 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.596032842239522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.596032842239522 | validation: 2.662500576024866]
	TIME [epoch: 8.33 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4687101997181853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4687101997181853 | validation: 2.565262669837508]
	TIME [epoch: 8.33 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.429375189947853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.429375189947853 | validation: 2.570847486463096]
	TIME [epoch: 8.33 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.475042425141016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.475042425141016 | validation: 2.5542943979583894]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4351993446777214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4351993446777214 | validation: 2.5505645429615402]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4628218865130145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4628218865130145 | validation: 2.567268366119829]
	TIME [epoch: 8.33 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4446327575071107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4446327575071107 | validation: 2.6370929872887707]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.484230865963727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.484230865963727 | validation: 2.5758573672502827]
	TIME [epoch: 8.33 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4412469694974055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4412469694974055 | validation: 2.554051103243743]
	TIME [epoch: 8.33 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4320296693618757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4320296693618757 | validation: 2.6289395633290242]
	TIME [epoch: 8.35 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.443919066520777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.443919066520777 | validation: 2.577373085606715]
	TIME [epoch: 8.37 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4424061241167245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4424061241167245 | validation: 2.598008062808856]
	TIME [epoch: 8.33 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4431719905301037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4431719905301037 | validation: 2.5420256941299737]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.425852835449354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.425852835449354 | validation: 2.6793820636406025]
	TIME [epoch: 8.33 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4621457841607612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4621457841607612 | validation: 2.5444571661325677]
	TIME [epoch: 8.33 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.416908687903692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.416908687903692 | validation: 2.6086011659112893]
	TIME [epoch: 8.37 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4386875134584396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4386875134584396 | validation: 2.540278733457244]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.445490415993244		[learning rate: 0.0099823]
	Learning Rate: 0.0099823
	LOSS [training: 2.445490415993244 | validation: 2.607437436098104]
	TIME [epoch: 8.33 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.444071813018227		[learning rate: 0.0099588]
	Learning Rate: 0.00995876
	LOSS [training: 2.444071813018227 | validation: 2.564863775837658]
	TIME [epoch: 8.33 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.431794042966511		[learning rate: 0.0099353]
	Learning Rate: 0.00993527
	LOSS [training: 2.431794042966511 | validation: 2.540993078010091]
	TIME [epoch: 8.33 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.418385581246287		[learning rate: 0.0099118]
	Learning Rate: 0.00991183
	LOSS [training: 2.418385581246287 | validation: 2.5627870651512703]
	TIME [epoch: 8.34 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.430206471293388		[learning rate: 0.0098884]
	Learning Rate: 0.00988845
	LOSS [training: 2.430206471293388 | validation: 2.5600570592251835]
	TIME [epoch: 8.36 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4214775840909666		[learning rate: 0.0098651]
	Learning Rate: 0.00986512
	LOSS [training: 2.4214775840909666 | validation: 2.5358482416556125]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.435473346321896		[learning rate: 0.0098419]
	Learning Rate: 0.00984185
	LOSS [training: 2.435473346321896 | validation: 2.55044906393144]
	TIME [epoch: 8.34 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.421545042611888		[learning rate: 0.0098186]
	Learning Rate: 0.00981864
	LOSS [training: 2.421545042611888 | validation: 2.5471998600479226]
	TIME [epoch: 8.34 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.422957168625071		[learning rate: 0.0097955]
	Learning Rate: 0.00979548
	LOSS [training: 2.422957168625071 | validation: 2.5651002023634226]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4263180548634247		[learning rate: 0.0097724]
	Learning Rate: 0.00977237
	LOSS [training: 2.4263180548634247 | validation: 2.5351639961821864]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4125254298152505		[learning rate: 0.0097493]
	Learning Rate: 0.00974932
	LOSS [training: 2.4125254298152505 | validation: 2.5476306355262306]
	TIME [epoch: 8.32 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.427453778710413		[learning rate: 0.0097263]
	Learning Rate: 0.00972632
	LOSS [training: 2.427453778710413 | validation: 2.5330082114790557]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4082601512231063		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 2.4082601512231063 | validation: 2.532800827274765]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4103149538624415		[learning rate: 0.0096805]
	Learning Rate: 0.00968049
	LOSS [training: 2.4103149538624415 | validation: 2.570631478978686]
	TIME [epoch: 8.34 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.441119366795679		[learning rate: 0.0096577]
	Learning Rate: 0.00965766
	LOSS [training: 2.441119366795679 | validation: 2.5305713112813386]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.398812205844815		[learning rate: 0.0096349]
	Learning Rate: 0.00963488
	LOSS [training: 2.398812205844815 | validation: 2.5247318055611174]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4137231456863097		[learning rate: 0.0096121]
	Learning Rate: 0.00961215
	LOSS [training: 2.4137231456863097 | validation: 2.6068256632533435]
	TIME [epoch: 8.32 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419246328611286		[learning rate: 0.0095895]
	Learning Rate: 0.00958948
	LOSS [training: 2.419246328611286 | validation: 2.5201310276423126]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3919000119329326		[learning rate: 0.0095669]
	Learning Rate: 0.00956686
	LOSS [training: 2.3919000119329326 | validation: 2.5247138795796102]
	TIME [epoch: 8.33 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4176394649844197		[learning rate: 0.0095443]
	Learning Rate: 0.00954429
	LOSS [training: 2.4176394649844197 | validation: 2.682612453586341]
	TIME [epoch: 8.34 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.451045222195816		[learning rate: 0.0095218]
	Learning Rate: 0.00952177
	LOSS [training: 2.451045222195816 | validation: 2.5383498716942428]
	TIME [epoch: 8.38 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399581142327029		[learning rate: 0.0094993]
	Learning Rate: 0.00949931
	LOSS [training: 2.399581142327029 | validation: 2.5173888773246307]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4055915320907375		[learning rate: 0.0094769]
	Learning Rate: 0.00947691
	LOSS [training: 2.4055915320907375 | validation: 2.576104045618039]
	TIME [epoch: 8.32 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.408025539566314		[learning rate: 0.0094546]
	Learning Rate: 0.00945455
	LOSS [training: 2.408025539566314 | validation: 2.5154493503663984]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.391101692478115		[learning rate: 0.0094323]
	Learning Rate: 0.00943225
	LOSS [training: 2.391101692478115 | validation: 2.52996217338377]
	TIME [epoch: 8.33 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.421087691047068		[learning rate: 0.00941]
	Learning Rate: 0.00941
	LOSS [training: 2.421087691047068 | validation: 2.5340183864404775]
	TIME [epoch: 8.38 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.396391496079546		[learning rate: 0.0093878]
	Learning Rate: 0.00938781
	LOSS [training: 2.396391496079546 | validation: 2.5210066729903953]
	TIME [epoch: 8.34 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3964439920139617		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 2.3964439920139617 | validation: 2.635247348055337]
	TIME [epoch: 8.32 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4321831693364806		[learning rate: 0.0093436]
	Learning Rate: 0.00934357
	LOSS [training: 2.4321831693364806 | validation: 2.5120603963227204]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3915297139652147		[learning rate: 0.0093215]
	Learning Rate: 0.00932153
	LOSS [training: 2.3915297139652147 | validation: 2.543976138028898]
	TIME [epoch: 8.32 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.415574949078292		[learning rate: 0.0092995]
	Learning Rate: 0.00929954
	LOSS [training: 2.415574949078292 | validation: 2.5199524480294597]
	TIME [epoch: 8.33 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399990332519654		[learning rate: 0.0092776]
	Learning Rate: 0.00927761
	LOSS [training: 2.399990332519654 | validation: 2.5246784761293464]
	TIME [epoch: 8.35 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4085189371181923		[learning rate: 0.0092557]
	Learning Rate: 0.00925572
	LOSS [training: 2.4085189371181923 | validation: 2.5212079363103355]
	TIME [epoch: 8.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3884304269125107		[learning rate: 0.0092339]
	Learning Rate: 0.00923389
	LOSS [training: 2.3884304269125107 | validation: 2.524485542771178]
	TIME [epoch: 8.31 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.403552831679264		[learning rate: 0.0092121]
	Learning Rate: 0.00921211
	LOSS [training: 2.403552831679264 | validation: 2.551386437315646]
	TIME [epoch: 8.31 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.396657327152887		[learning rate: 0.0091904]
	Learning Rate: 0.00919038
	LOSS [training: 2.396657327152887 | validation: 2.514652559304837]
	TIME [epoch: 8.31 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.388160257364528		[learning rate: 0.0091687]
	Learning Rate: 0.0091687
	LOSS [training: 2.388160257364528 | validation: 2.5291768591669657]
	TIME [epoch: 8.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.413157848776111		[learning rate: 0.0091471]
	Learning Rate: 0.00914707
	LOSS [training: 2.413157848776111 | validation: 2.575376975512383]
	TIME [epoch: 8.33 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.402983417142195		[learning rate: 0.0091255]
	Learning Rate: 0.00912549
	LOSS [training: 2.402983417142195 | validation: 2.518998205044043]
	TIME [epoch: 8.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.388622876353372		[learning rate: 0.009104]
	Learning Rate: 0.00910397
	LOSS [training: 2.388622876353372 | validation: 2.5180929104346044]
	TIME [epoch: 8.31 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3980126721239143		[learning rate: 0.0090825]
	Learning Rate: 0.00908249
	LOSS [training: 2.3980126721239143 | validation: 2.512743733934653]
	TIME [epoch: 8.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381505698202515		[learning rate: 0.0090611]
	Learning Rate: 0.00906107
	LOSS [training: 2.381505698202515 | validation: 2.5084828717741856]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3986233744616445		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 2.3986233744616445 | validation: 2.588517806842728]
	TIME [epoch: 8.37 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4065555136405776		[learning rate: 0.0090184]
	Learning Rate: 0.00901837
	LOSS [training: 2.4065555136405776 | validation: 2.514585792158808]
	TIME [epoch: 8.33 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3906721014566674		[learning rate: 0.0089971]
	Learning Rate: 0.0089971
	LOSS [training: 2.3906721014566674 | validation: 2.5110597433956223]
	TIME [epoch: 8.32 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.385624291184547		[learning rate: 0.0089759]
	Learning Rate: 0.00897588
	LOSS [training: 2.385624291184547 | validation: 2.5237938631318713]
	TIME [epoch: 8.33 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3952132872701446		[learning rate: 0.0089547]
	Learning Rate: 0.0089547
	LOSS [training: 2.3952132872701446 | validation: 2.518166822067501]
	TIME [epoch: 8.33 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.383475351806161		[learning rate: 0.0089336]
	Learning Rate: 0.00893358
	LOSS [training: 2.383475351806161 | validation: 2.5063478861819695]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3943670930442194		[learning rate: 0.0089125]
	Learning Rate: 0.00891251
	LOSS [training: 2.3943670930442194 | validation: 2.554613424544063]
	TIME [epoch: 8.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.40570528796647		[learning rate: 0.0088915]
	Learning Rate: 0.00889149
	LOSS [training: 2.40570528796647 | validation: 2.5109047551910066]
	TIME [epoch: 8.33 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.380883790174034		[learning rate: 0.0088705]
	Learning Rate: 0.00887051
	LOSS [training: 2.380883790174034 | validation: 2.5048447031574073]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3838811126184876		[learning rate: 0.0088496]
	Learning Rate: 0.00884959
	LOSS [training: 2.3838811126184876 | validation: 2.534215456562908]
	TIME [epoch: 8.33 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3878901157718855		[learning rate: 0.0088287]
	Learning Rate: 0.00882871
	LOSS [training: 2.3878901157718855 | validation: 2.5054462852396737]
	TIME [epoch: 8.33 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3769357658129713		[learning rate: 0.0088079]
	Learning Rate: 0.00880789
	LOSS [training: 2.3769357658129713 | validation: 2.506348706557451]
	TIME [epoch: 8.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3902521680398836		[learning rate: 0.0087871]
	Learning Rate: 0.00878711
	LOSS [training: 2.3902521680398836 | validation: 2.5649544529407486]
	TIME [epoch: 8.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4115379552059655		[learning rate: 0.0087664]
	Learning Rate: 0.00876638
	LOSS [training: 2.4115379552059655 | validation: 2.5148965241992465]
	TIME [epoch: 8.32 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3790928093922017		[learning rate: 0.0087457]
	Learning Rate: 0.00874571
	LOSS [training: 2.3790928093922017 | validation: 2.5019930001102524]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3769314751439548		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 2.3769314751439548 | validation: 2.50106023706965]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3808393464584663		[learning rate: 0.0087045]
	Learning Rate: 0.00870449
	LOSS [training: 2.3808393464584663 | validation: 2.513293635139564]
	TIME [epoch: 8.36 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3833390916147104		[learning rate: 0.008684]
	Learning Rate: 0.00868396
	LOSS [training: 2.3833390916147104 | validation: 2.5128255934276176]
	TIME [epoch: 8.33 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378707601314806		[learning rate: 0.0086635]
	Learning Rate: 0.00866348
	LOSS [training: 2.378707601314806 | validation: 2.5267658450400283]
	TIME [epoch: 8.32 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.385687084654706		[learning rate: 0.008643]
	Learning Rate: 0.00864304
	LOSS [training: 2.385687084654706 | validation: 2.5057049110210707]
	TIME [epoch: 8.32 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372948982653204		[learning rate: 0.0086227]
	Learning Rate: 0.00862265
	LOSS [training: 2.372948982653204 | validation: 2.4989087203809035]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3873850305733204		[learning rate: 0.0086023]
	Learning Rate: 0.00860232
	LOSS [training: 2.3873850305733204 | validation: 2.5157920927540616]
	TIME [epoch: 8.35 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379709497432219		[learning rate: 0.008582]
	Learning Rate: 0.00858202
	LOSS [training: 2.379709497432219 | validation: 2.506542014151401]
	TIME [epoch: 8.37 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3708338821873474		[learning rate: 0.0085618]
	Learning Rate: 0.00856178
	LOSS [training: 2.3708338821873474 | validation: 2.5002464662339183]
	TIME [epoch: 8.32 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372533870799897		[learning rate: 0.0085416]
	Learning Rate: 0.00854158
	LOSS [training: 2.372533870799897 | validation: 2.5006399262569943]
	TIME [epoch: 8.32 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3885805831239333		[learning rate: 0.0085214]
	Learning Rate: 0.00852144
	LOSS [training: 2.3885805831239333 | validation: 2.49973761557984]
	TIME [epoch: 8.32 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3763203624477542		[learning rate: 0.0085013]
	Learning Rate: 0.00850134
	LOSS [training: 2.3763203624477542 | validation: 2.4985969447785243]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379002740809095		[learning rate: 0.0084813]
	Learning Rate: 0.00848128
	LOSS [training: 2.379002740809095 | validation: 2.5069071286527316]
	TIME [epoch: 8.36 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.373169875208177		[learning rate: 0.0084613]
	Learning Rate: 0.00846128
	LOSS [training: 2.373169875208177 | validation: 2.505598924605151]
	TIME [epoch: 8.33 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370927397486585		[learning rate: 0.0084413]
	Learning Rate: 0.00844132
	LOSS [training: 2.370927397486585 | validation: 2.5033016424941845]
	TIME [epoch: 8.33 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3808532228554147		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 2.3808532228554147 | validation: 2.49856487800884]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3726181448630514		[learning rate: 0.0084015]
	Learning Rate: 0.00840154
	LOSS [training: 2.3726181448630514 | validation: 2.5006248452836957]
	TIME [epoch: 8.32 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3707840789566568		[learning rate: 0.0083817]
	Learning Rate: 0.00838172
	LOSS [training: 2.3707840789566568 | validation: 2.505825034405312]
	TIME [epoch: 8.33 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3759308724269506		[learning rate: 0.008362]
	Learning Rate: 0.00836195
	LOSS [training: 2.3759308724269506 | validation: 2.5239688581809605]
	TIME [epoch: 8.36 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3755162168156674		[learning rate: 0.0083422]
	Learning Rate: 0.00834223
	LOSS [training: 2.3755162168156674 | validation: 2.4941672513122497]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.366684088217705		[learning rate: 0.0083225]
	Learning Rate: 0.00832255
	LOSS [training: 2.366684088217705 | validation: 2.501036778216217]
	TIME [epoch: 8.33 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3698711582734044		[learning rate: 0.0083029]
	Learning Rate: 0.00830292
	LOSS [training: 2.3698711582734044 | validation: 2.5032822806023187]
	TIME [epoch: 8.34 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3705325482823167		[learning rate: 0.0082833]
	Learning Rate: 0.00828333
	LOSS [training: 2.3705325482823167 | validation: 2.5019345083581133]
	TIME [epoch: 8.33 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3789069510130405		[learning rate: 0.0082638]
	Learning Rate: 0.00826379
	LOSS [training: 2.3789069510130405 | validation: 2.4942724865002117]
	TIME [epoch: 8.38 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3653401871491475		[learning rate: 0.0082443]
	Learning Rate: 0.0082443
	LOSS [training: 2.3653401871491475 | validation: 2.4871053906253087]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3690841774733213		[learning rate: 0.0082249]
	Learning Rate: 0.00822485
	LOSS [training: 2.3690841774733213 | validation: 2.4908794863963424]
	TIME [epoch: 8.33 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369492295349643		[learning rate: 0.0082055]
	Learning Rate: 0.00820545
	LOSS [training: 2.369492295349643 | validation: 2.496448785414933]
	TIME [epoch: 8.34 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3634913793466663		[learning rate: 0.0081861]
	Learning Rate: 0.0081861
	LOSS [training: 2.3634913793466663 | validation: 2.4983443021703122]
	TIME [epoch: 8.33 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3787797668776864		[learning rate: 0.0081668]
	Learning Rate: 0.00816679
	LOSS [training: 2.3787797668776864 | validation: 2.498764436796198]
	TIME [epoch: 8.35 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3761722719919094		[learning rate: 0.0081475]
	Learning Rate: 0.00814752
	LOSS [training: 2.3761722719919094 | validation: 2.487876459520245]
	TIME [epoch: 8.37 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.363571603061234		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 2.363571603061234 | validation: 2.5053974870802023]
	TIME [epoch: 8.33 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3708336980738594		[learning rate: 0.0081091]
	Learning Rate: 0.00810913
	LOSS [training: 2.3708336980738594 | validation: 2.488867796467317]
	TIME [epoch: 8.33 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3593029312573357		[learning rate: 0.00809]
	Learning Rate: 0.00809
	LOSS [training: 2.3593029312573357 | validation: 2.4864484318066307]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3629742918047394		[learning rate: 0.0080709]
	Learning Rate: 0.00807092
	LOSS [training: 2.3629742918047394 | validation: 2.4868022730062256]
	TIME [epoch: 8.33 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3713635185033493		[learning rate: 0.0080519]
	Learning Rate: 0.00805188
	LOSS [training: 2.3713635185033493 | validation: 2.4974544408133896]
	TIME [epoch: 8.38 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3689000557210065		[learning rate: 0.0080329]
	Learning Rate: 0.00803289
	LOSS [training: 2.3689000557210065 | validation: 2.4875430867918364]
	TIME [epoch: 8.34 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.361008605581469		[learning rate: 0.0080139]
	Learning Rate: 0.00801394
	LOSS [training: 2.361008605581469 | validation: 2.4911631949129522]
	TIME [epoch: 8.33 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3677960021280455		[learning rate: 0.007995]
	Learning Rate: 0.00799504
	LOSS [training: 2.3677960021280455 | validation: 2.486122922609309]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360683784163659		[learning rate: 0.0079762]
	Learning Rate: 0.00797618
	LOSS [training: 2.360683784163659 | validation: 2.5199333615606196]
	TIME [epoch: 8.34 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3745529230473634		[learning rate: 0.0079574]
	Learning Rate: 0.00795736
	LOSS [training: 2.3745529230473634 | validation: 2.480562987362225]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360493186728516		[learning rate: 0.0079386]
	Learning Rate: 0.00793859
	LOSS [training: 2.360493186728516 | validation: 2.491474274409605]
	TIME [epoch: 8.36 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.377541135265772		[learning rate: 0.0079199]
	Learning Rate: 0.00791987
	LOSS [training: 2.377541135265772 | validation: 2.4904495258743857]
	TIME [epoch: 8.32 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3889652833842776		[learning rate: 0.0079012]
	Learning Rate: 0.00790119
	LOSS [training: 2.3889652833842776 | validation: 2.48901601784815]
	TIME [epoch: 8.32 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3742818377189328		[learning rate: 0.0078825]
	Learning Rate: 0.00788255
	LOSS [training: 2.3742818377189328 | validation: 2.4849851543811976]
	TIME [epoch: 8.33 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381239877700189		[learning rate: 0.007864]
	Learning Rate: 0.00786396
	LOSS [training: 2.381239877700189 | validation: 2.6200689915938624]
	TIME [epoch: 8.33 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4550036059370437		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 2.4550036059370437 | validation: 2.537178356910996]
	TIME [epoch: 8.38 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4000904470639814		[learning rate: 0.0078269]
	Learning Rate: 0.0078269
	LOSS [training: 2.4000904470639814 | validation: 2.502872990107674]
	TIME [epoch: 8.35 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.365444822198142		[learning rate: 0.0078084]
	Learning Rate: 0.00780844
	LOSS [training: 2.365444822198142 | validation: 2.482801841607724]
	TIME [epoch: 8.34 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355444705942849		[learning rate: 0.00779]
	Learning Rate: 0.00779002
	LOSS [training: 2.355444705942849 | validation: 2.4793002647726436]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.352696936298591		[learning rate: 0.0077716]
	Learning Rate: 0.00777164
	LOSS [training: 2.352696936298591 | validation: 2.4905471498860594]
	TIME [epoch: 8.34 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3696966710048892		[learning rate: 0.0077533]
	Learning Rate: 0.00775331
	LOSS [training: 2.3696966710048892 | validation: 2.511240858875222]
	TIME [epoch: 8.36 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.363952291060377		[learning rate: 0.007735]
	Learning Rate: 0.00773502
	LOSS [training: 2.363952291060377 | validation: 2.5074388333096955]
	TIME [epoch: 8.36 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3619140866321064		[learning rate: 0.0077168]
	Learning Rate: 0.00771678
	LOSS [training: 2.3619140866321064 | validation: 2.4797176024777983]
	TIME [epoch: 8.33 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3503619927737796		[learning rate: 0.0076986]
	Learning Rate: 0.00769857
	LOSS [training: 2.3503619927737796 | validation: 2.4829542510854026]
	TIME [epoch: 8.34 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3633821257752836		[learning rate: 0.0076804]
	Learning Rate: 0.00768041
	LOSS [training: 2.3633821257752836 | validation: 2.476353689453691]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3560825119819753		[learning rate: 0.0076623]
	Learning Rate: 0.0076623
	LOSS [training: 2.3560825119819753 | validation: 2.4807520856596454]
	TIME [epoch: 8.33 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.357560516254852		[learning rate: 0.0076442]
	Learning Rate: 0.00764422
	LOSS [training: 2.357560516254852 | validation: 2.47584763742546]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3528691677028		[learning rate: 0.0076262]
	Learning Rate: 0.00762619
	LOSS [training: 2.3528691677028 | validation: 2.478230855171811]
	TIME [epoch: 8.34 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.439178666414405		[learning rate: 0.0076082]
	Learning Rate: 0.0076082
	LOSS [training: 2.439178666414405 | validation: 2.590956381112069]
	TIME [epoch: 8.34 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3846282562958914		[learning rate: 0.0075903]
	Learning Rate: 0.00759025
	LOSS [training: 2.3846282562958914 | validation: 2.608420635876911]
	TIME [epoch: 8.34 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.420800161380526		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 2.420800161380526 | validation: 2.5002985957625925]
	TIME [epoch: 8.34 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.361712674585443		[learning rate: 0.0075545]
	Learning Rate: 0.00755449
	LOSS [training: 2.361712674585443 | validation: 2.4856319875369755]
	TIME [epoch: 8.36 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3549458920998463		[learning rate: 0.0075367]
	Learning Rate: 0.00753667
	LOSS [training: 2.3549458920998463 | validation: 2.4842198705726672]
	TIME [epoch: 8.37 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3592464742782298		[learning rate: 0.0075189]
	Learning Rate: 0.00751889
	LOSS [training: 2.3592464742782298 | validation: 2.4793250325076484]
	TIME [epoch: 8.34 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3526549081535295		[learning rate: 0.0075012]
	Learning Rate: 0.00750116
	LOSS [training: 2.3526549081535295 | validation: 2.475717030204219]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3468306106288725		[learning rate: 0.0074835]
	Learning Rate: 0.00748346
	LOSS [training: 2.3468306106288725 | validation: 2.4725557701093455]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359541198868887		[learning rate: 0.0074658]
	Learning Rate: 0.00746581
	LOSS [training: 2.359541198868887 | validation: 2.477384850505918]
	TIME [epoch: 8.35 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3932428338413505		[learning rate: 0.0074482]
	Learning Rate: 0.0074482
	LOSS [training: 2.3932428338413505 | validation: 2.4849626329270054]
	TIME [epoch: 8.39 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.352877482062793		[learning rate: 0.0074306]
	Learning Rate: 0.00743063
	LOSS [training: 2.352877482062793 | validation: 2.471894133780766]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.350407831883132		[learning rate: 0.0074131]
	Learning Rate: 0.0074131
	LOSS [training: 2.350407831883132 | validation: 2.5198702367165895]
	TIME [epoch: 8.35 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.577412840296236		[learning rate: 0.0073956]
	Learning Rate: 0.00739562
	LOSS [training: 2.577412840296236 | validation: 2.7554639285065736]
	TIME [epoch: 8.34 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.578471314307307		[learning rate: 0.0073782]
	Learning Rate: 0.00737817
	LOSS [training: 2.578471314307307 | validation: 2.6529498919347]
	TIME [epoch: 8.33 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4636713175855363		[learning rate: 0.0073608]
	Learning Rate: 0.00736077
	LOSS [training: 2.4636713175855363 | validation: 2.52242526155133]
	TIME [epoch: 8.37 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379907379461808		[learning rate: 0.0073434]
	Learning Rate: 0.0073434
	LOSS [training: 2.379907379461808 | validation: 2.4953996501551288]
	TIME [epoch: 8.35 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.364361691207426		[learning rate: 0.0073261]
	Learning Rate: 0.00732608
	LOSS [training: 2.364361691207426 | validation: 2.4871464594606953]
	TIME [epoch: 8.34 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3582054588556436		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 2.3582054588556436 | validation: 2.483797170578513]
	TIME [epoch: 8.34 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355495689679704		[learning rate: 0.0072916]
	Learning Rate: 0.00729156
	LOSS [training: 2.355495689679704 | validation: 2.4909055497879606]
	TIME [epoch: 8.34 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3579804467121916		[learning rate: 0.0072744]
	Learning Rate: 0.00727436
	LOSS [training: 2.3579804467121916 | validation: 2.472623797364666]
	TIME [epoch: 8.34 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349732601668754		[learning rate: 0.0072572]
	Learning Rate: 0.0072572
	LOSS [training: 2.349732601668754 | validation: 2.4731878578568622]
	TIME [epoch: 8.38 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.348826945355762		[learning rate: 0.0072401]
	Learning Rate: 0.00724008
	LOSS [training: 2.348826945355762 | validation: 2.4898469709174993]
	TIME [epoch: 8.34 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3974308976240493		[learning rate: 0.007223]
	Learning Rate: 0.007223
	LOSS [training: 2.3974308976240493 | validation: 2.5079155462404725]
	TIME [epoch: 8.33 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3752647257206307		[learning rate: 0.007206]
	Learning Rate: 0.00720597
	LOSS [training: 2.3752647257206307 | validation: 2.4753519436658307]
	TIME [epoch: 8.34 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.348679736271088		[learning rate: 0.007189]
	Learning Rate: 0.00718897
	LOSS [training: 2.348679736271088 | validation: 2.475699298767265]
	TIME [epoch: 8.34 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3503739293562105		[learning rate: 0.007172]
	Learning Rate: 0.00717201
	LOSS [training: 2.3503739293562105 | validation: 2.4760584689601792]
	TIME [epoch: 8.36 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3520718188562477		[learning rate: 0.0071551]
	Learning Rate: 0.00715509
	LOSS [training: 2.3520718188562477 | validation: 2.478478224244193]
	TIME [epoch: 8.37 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359624266770637		[learning rate: 0.0071382]
	Learning Rate: 0.00713822
	LOSS [training: 2.359624266770637 | validation: 2.472844749309834]
	TIME [epoch: 8.34 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3501523144195158		[learning rate: 0.0071214]
	Learning Rate: 0.00712138
	LOSS [training: 2.3501523144195158 | validation: 2.474260069873924]
	TIME [epoch: 8.34 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.382210485587991		[learning rate: 0.0071046]
	Learning Rate: 0.00710458
	LOSS [training: 2.382210485587991 | validation: 2.579456512468096]
	TIME [epoch: 8.34 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.389729225602067		[learning rate: 0.0070878]
	Learning Rate: 0.00708782
	LOSS [training: 2.389729225602067 | validation: 2.482092955980974]
	TIME [epoch: 8.34 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.350446128722756		[learning rate: 0.0070711]
	Learning Rate: 0.0070711
	LOSS [training: 2.350446128722756 | validation: 2.4766016424553117]
	TIME [epoch: 8.38 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3507837935408475		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 2.3507837935408475 | validation: 2.478707474865596]
	TIME [epoch: 8.35 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.347989302637056		[learning rate: 0.0070378]
	Learning Rate: 0.00703778
	LOSS [training: 2.347989302637056 | validation: 2.49110751463659]
	TIME [epoch: 8.34 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3550616403874605		[learning rate: 0.0070212]
	Learning Rate: 0.00702118
	LOSS [training: 2.3550616403874605 | validation: 2.470876889771252]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.348287053743488		[learning rate: 0.0070046]
	Learning Rate: 0.00700462
	LOSS [training: 2.348287053743488 | validation: 2.466334374594031]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3451086844522395		[learning rate: 0.0069881]
	Learning Rate: 0.0069881
	LOSS [training: 2.3451086844522395 | validation: 2.470344882940376]
	TIME [epoch: 8.35 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3444713106361537		[learning rate: 0.0069716]
	Learning Rate: 0.00697161
	LOSS [training: 2.3444713106361537 | validation: 2.50634953322459]
	TIME [epoch: 8.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.356576736300092		[learning rate: 0.0069552]
	Learning Rate: 0.00695517
	LOSS [training: 2.356576736300092 | validation: 2.4649689571601128]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339912404952657		[learning rate: 0.0069388]
	Learning Rate: 0.00693876
	LOSS [training: 2.339912404952657 | validation: 2.4685573575443662]
	TIME [epoch: 8.34 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355328034856126		[learning rate: 0.0069224]
	Learning Rate: 0.00692239
	LOSS [training: 2.355328034856126 | validation: 2.493286474901213]
	TIME [epoch: 8.34 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.358494656500064		[learning rate: 0.0069061]
	Learning Rate: 0.00690607
	LOSS [training: 2.358494656500064 | validation: 2.465746579739027]
	TIME [epoch: 8.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3407280733212166		[learning rate: 0.0068898]
	Learning Rate: 0.00688978
	LOSS [training: 2.3407280733212166 | validation: 2.4698692679712693]
	TIME [epoch: 8.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343040907464352		[learning rate: 0.0068735]
	Learning Rate: 0.00687352
	LOSS [training: 2.343040907464352 | validation: 2.469780081528848]
	TIME [epoch: 8.34 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3656839007982087		[learning rate: 0.0068573]
	Learning Rate: 0.00685731
	LOSS [training: 2.3656839007982087 | validation: 2.48639124504984]
	TIME [epoch: 8.33 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3478485170256524		[learning rate: 0.0068411]
	Learning Rate: 0.00684114
	LOSS [training: 2.3478485170256524 | validation: 2.476321692698378]
	TIME [epoch: 8.34 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.344538200215934		[learning rate: 0.006825]
	Learning Rate: 0.006825
	LOSS [training: 2.344538200215934 | validation: 2.476035670651071]
	TIME [epoch: 8.33 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3439030133204946		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 2.3439030133204946 | validation: 2.4678218301435306]
	TIME [epoch: 8.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3443608251876675		[learning rate: 0.0067928]
	Learning Rate: 0.00679284
	LOSS [training: 2.3443608251876675 | validation: 2.485296017705492]
	TIME [epoch: 8.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3604407801000873		[learning rate: 0.0067768]
	Learning Rate: 0.00677681
	LOSS [training: 2.3604407801000873 | validation: 2.481760548165949]
	TIME [epoch: 8.33 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3665549198703215		[learning rate: 0.0067608]
	Learning Rate: 0.00676083
	LOSS [training: 2.3665549198703215 | validation: 2.490837808480104]
	TIME [epoch: 8.33 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360395071918132		[learning rate: 0.0067449]
	Learning Rate: 0.00674488
	LOSS [training: 2.360395071918132 | validation: 2.468436732195575]
	TIME [epoch: 8.33 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3415045079524806		[learning rate: 0.006729]
	Learning Rate: 0.00672897
	LOSS [training: 2.3415045079524806 | validation: 2.559048412745293]
	TIME [epoch: 8.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4440411828977373		[learning rate: 0.0067131]
	Learning Rate: 0.0067131
	LOSS [training: 2.4440411828977373 | validation: 2.4887637796870905]
	TIME [epoch: 8.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.391070233218991		[learning rate: 0.0066973]
	Learning Rate: 0.00669726
	LOSS [training: 2.391070233218991 | validation: 2.5721780282708764]
	TIME [epoch: 8.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.402465289184295		[learning rate: 0.0066815]
	Learning Rate: 0.00668147
	LOSS [training: 2.402465289184295 | validation: 2.4842391509702852]
	TIME [epoch: 8.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3545153748323395		[learning rate: 0.0066657]
	Learning Rate: 0.00666571
	LOSS [training: 2.3545153748323395 | validation: 2.4644324207302404]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3803925783454307		[learning rate: 0.00665]
	Learning Rate: 0.00664998
	LOSS [training: 2.3803925783454307 | validation: 2.502285784787119]
	TIME [epoch: 8.33 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.367759661585098		[learning rate: 0.0066343]
	Learning Rate: 0.0066343
	LOSS [training: 2.367759661585098 | validation: 2.4880374140149035]
	TIME [epoch: 8.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349005854626033		[learning rate: 0.0066186]
	Learning Rate: 0.00661865
	LOSS [training: 2.349005854626033 | validation: 2.464279732878245]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3765537973336923		[learning rate: 0.006603]
	Learning Rate: 0.00660304
	LOSS [training: 2.3765537973336923 | validation: 2.6203452964417933]
	TIME [epoch: 8.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.44159742806732		[learning rate: 0.0065875]
	Learning Rate: 0.00658746
	LOSS [training: 2.44159742806732 | validation: 2.5055510927124196]
	TIME [epoch: 8.32 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3755177219677233		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 2.3755177219677233 | validation: 2.550320113820204]
	TIME [epoch: 8.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359198995213036		[learning rate: 0.0065564]
	Learning Rate: 0.00655642
	LOSS [training: 2.359198995213036 | validation: 2.4583286379079006]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359072037831494		[learning rate: 0.006541]
	Learning Rate: 0.00654095
	LOSS [training: 2.359072037831494 | validation: 2.472491782167543]
	TIME [epoch: 8.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.338595118101547		[learning rate: 0.0065255]
	Learning Rate: 0.00652552
	LOSS [training: 2.338595118101547 | validation: 2.4554244472346585]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336778573815393		[learning rate: 0.0065101]
	Learning Rate: 0.00651013
	LOSS [training: 2.336778573815393 | validation: 2.4570478875678905]
	TIME [epoch: 8.33 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343168357356159		[learning rate: 0.0064948]
	Learning Rate: 0.00649477
	LOSS [training: 2.343168357356159 | validation: 2.4553160141372636]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334527883319587		[learning rate: 0.0064795]
	Learning Rate: 0.00647945
	LOSS [training: 2.334527883319587 | validation: 2.46372959014901]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3919829259650847		[learning rate: 0.0064642]
	Learning Rate: 0.00646417
	LOSS [training: 2.3919829259650847 | validation: 2.473853075333956]
	TIME [epoch: 8.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3446060121450696		[learning rate: 0.0064489]
	Learning Rate: 0.00644892
	LOSS [training: 2.3446060121450696 | validation: 2.5085813533156163]
	TIME [epoch: 8.34 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3608759859144657		[learning rate: 0.0064337]
	Learning Rate: 0.00643371
	LOSS [training: 2.3608759859144657 | validation: 2.4583627553228236]
	TIME [epoch: 8.33 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3445893059016703		[learning rate: 0.0064185]
	Learning Rate: 0.00641853
	LOSS [training: 2.3445893059016703 | validation: 2.462850881609766]
	TIME [epoch: 8.33 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337556018919274		[learning rate: 0.0064034]
	Learning Rate: 0.00640339
	LOSS [training: 2.337556018919274 | validation: 2.4530071716518878]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331724434480922		[learning rate: 0.0063883]
	Learning Rate: 0.00638829
	LOSS [training: 2.331724434480922 | validation: 2.5791890774787776]
	TIME [epoch: 8.33 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3716050692810766		[learning rate: 0.0063732]
	Learning Rate: 0.00637322
	LOSS [training: 2.3716050692810766 | validation: 2.4586900978463815]
	TIME [epoch: 8.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3369453599389107		[learning rate: 0.0063582]
	Learning Rate: 0.00635819
	LOSS [training: 2.3369453599389107 | validation: 2.447540635552752]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4624365323371347		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 2.4624365323371347 | validation: 2.5380097488957354]
	TIME [epoch: 8.33 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390117872870457		[learning rate: 0.0063282]
	Learning Rate: 0.00632823
	LOSS [training: 2.390117872870457 | validation: 2.4981769613159033]
	TIME [epoch: 8.33 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381968882607852		[learning rate: 0.0063133]
	Learning Rate: 0.0063133
	LOSS [training: 2.381968882607852 | validation: 2.497037364328973]
	TIME [epoch: 8.33 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372018769028712		[learning rate: 0.0062984]
	Learning Rate: 0.00629841
	LOSS [training: 2.372018769028712 | validation: 2.489598345695139]
	TIME [epoch: 8.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3622946995323573		[learning rate: 0.0062836]
	Learning Rate: 0.00628355
	LOSS [training: 2.3622946995323573 | validation: 2.484382348612618]
	TIME [epoch: 8.34 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3485069590493124		[learning rate: 0.0062687]
	Learning Rate: 0.00626873
	LOSS [training: 2.3485069590493124 | validation: 2.468371572978576]
	TIME [epoch: 8.33 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3439410530815588		[learning rate: 0.0062539]
	Learning Rate: 0.00625394
	LOSS [training: 2.3439410530815588 | validation: 2.4714046992749203]
	TIME [epoch: 8.33 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.346847550629728		[learning rate: 0.0062392]
	Learning Rate: 0.00623919
	LOSS [training: 2.346847550629728 | validation: 2.4711012200811755]
	TIME [epoch: 8.33 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3362397444801424		[learning rate: 0.0062245]
	Learning Rate: 0.00622447
	LOSS [training: 2.3362397444801424 | validation: 2.4530580903135]
	TIME [epoch: 8.33 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3504430521795276		[learning rate: 0.0062098]
	Learning Rate: 0.00620979
	LOSS [training: 2.3504430521795276 | validation: 2.456713711080351]
	TIME [epoch: 8.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3363530776820607		[learning rate: 0.0061951]
	Learning Rate: 0.00619514
	LOSS [training: 2.3363530776820607 | validation: 2.4525224241823143]
	TIME [epoch: 8.34 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343862266643923		[learning rate: 0.0061805]
	Learning Rate: 0.00618053
	LOSS [training: 2.343862266643923 | validation: 2.470123054401451]
	TIME [epoch: 8.34 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3471488136895005		[learning rate: 0.0061659]
	Learning Rate: 0.00616595
	LOSS [training: 2.3471488136895005 | validation: 2.4510677817328226]
	TIME [epoch: 8.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3491536880158157		[learning rate: 0.0061514]
	Learning Rate: 0.00615141
	LOSS [training: 2.3491536880158157 | validation: 2.465762040098734]
	TIME [epoch: 8.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3399954104984655		[learning rate: 0.0061369]
	Learning Rate: 0.0061369
	LOSS [training: 2.3399954104984655 | validation: 2.459479192687124]
	TIME [epoch: 8.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339015953106016		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 2.339015953106016 | validation: 2.632749384334681]
	TIME [epoch: 8.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4478343698009395		[learning rate: 0.006108]
	Learning Rate: 0.00610798
	LOSS [training: 2.4478343698009395 | validation: 2.5130561411258867]
	TIME [epoch: 8.33 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3663552750421486		[learning rate: 0.0060936]
	Learning Rate: 0.00609357
	LOSS [training: 2.3663552750421486 | validation: 2.4846228341109975]
	TIME [epoch: 8.34 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349009506735518		[learning rate: 0.0060792]
	Learning Rate: 0.0060792
	LOSS [training: 2.349009506735518 | validation: 2.532110040436746]
	TIME [epoch: 8.32 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390265131729078		[learning rate: 0.0060649]
	Learning Rate: 0.00606486
	LOSS [training: 2.390265131729078 | validation: 2.4728836380509174]
	TIME [epoch: 8.33 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3555277292402423		[learning rate: 0.0060505]
	Learning Rate: 0.00605055
	LOSS [training: 2.3555277292402423 | validation: 2.463268661804106]
	TIME [epoch: 8.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.34491158893843		[learning rate: 0.0060363]
	Learning Rate: 0.00603628
	LOSS [training: 2.34491158893843 | validation: 2.462276669378349]
	TIME [epoch: 8.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3428738915013323		[learning rate: 0.006022]
	Learning Rate: 0.00602204
	LOSS [training: 2.3428738915013323 | validation: 2.45226121383514]
	TIME [epoch: 8.33 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335134661016379		[learning rate: 0.0060078]
	Learning Rate: 0.00600783
	LOSS [training: 2.335134661016379 | validation: 2.4586473934956405]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3749536594822542		[learning rate: 0.0059937]
	Learning Rate: 0.00599366
	LOSS [training: 2.3749536594822542 | validation: 2.456136787710003]
	TIME [epoch: 8.33 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3423135635916528		[learning rate: 0.0059795]
	Learning Rate: 0.00597952
	LOSS [training: 2.3423135635916528 | validation: 2.4562963291713915]
	TIME [epoch: 8.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3250090988607433		[learning rate: 0.0059654]
	Learning Rate: 0.00596542
	LOSS [training: 2.3250090988607433 | validation: 2.4570751316530073]
	TIME [epoch: 8.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328960759856503		[learning rate: 0.0059513]
	Learning Rate: 0.00595135
	LOSS [training: 2.328960759856503 | validation: 2.4481133828579758]
	TIME [epoch: 8.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3597714182778367		[learning rate: 0.0059373]
	Learning Rate: 0.00593731
	LOSS [training: 2.3597714182778367 | validation: 2.472759369444563]
	TIME [epoch: 8.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.346932796118134		[learning rate: 0.0059233]
	Learning Rate: 0.00592331
	LOSS [training: 2.346932796118134 | validation: 2.4498255973052068]
	TIME [epoch: 8.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322487722736614		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 2.322487722736614 | validation: 2.4386354686198812]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326531950797704		[learning rate: 0.0058954]
	Learning Rate: 0.00589539
	LOSS [training: 2.326531950797704 | validation: 2.454068329167878]
	TIME [epoch: 8.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324967767729191		[learning rate: 0.0058815]
	Learning Rate: 0.00588149
	LOSS [training: 2.324967767729191 | validation: 2.4359815013706343]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4418991612589935		[learning rate: 0.0058676]
	Learning Rate: 0.00586761
	LOSS [training: 2.4418991612589935 | validation: 2.6085738686550206]
	TIME [epoch: 8.33 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.406243322702395		[learning rate: 0.0058538]
	Learning Rate: 0.00585377
	LOSS [training: 2.406243322702395 | validation: 2.4609245288778583]
	TIME [epoch: 8.32 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339039170601889		[learning rate: 0.00584]
	Learning Rate: 0.00583996
	LOSS [training: 2.339039170601889 | validation: 2.451425827127235]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326041513980228		[learning rate: 0.0058262]
	Learning Rate: 0.00582619
	LOSS [training: 2.326041513980228 | validation: 2.442821034205692]
	TIME [epoch: 8.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3206922957297187		[learning rate: 0.0058124]
	Learning Rate: 0.00581245
	LOSS [training: 2.3206922957297187 | validation: 2.4498339127767474]
	TIME [epoch: 8.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331736488420078		[learning rate: 0.0057987]
	Learning Rate: 0.00579874
	LOSS [training: 2.331736488420078 | validation: 2.44357349338651]
	TIME [epoch: 8.34 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313377571407536		[learning rate: 0.0057851]
	Learning Rate: 0.00578506
	LOSS [training: 2.313377571407536 | validation: 2.4307248398801544]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3150237306909847		[learning rate: 0.0057714]
	Learning Rate: 0.00577141
	LOSS [training: 2.3150237306909847 | validation: 2.4255950023659376]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3531481496856155		[learning rate: 0.0057578]
	Learning Rate: 0.0057578
	LOSS [training: 2.3531481496856155 | validation: 2.486047674784252]
	TIME [epoch: 8.33 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3448868240217284		[learning rate: 0.0057442]
	Learning Rate: 0.00574422
	LOSS [training: 2.3448868240217284 | validation: 2.442118346953425]
	TIME [epoch: 8.37 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3759881382778074		[learning rate: 0.0057307]
	Learning Rate: 0.00573067
	LOSS [training: 2.3759881382778074 | validation: 2.4975373659431295]
	TIME [epoch: 8.33 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3452441196717544		[learning rate: 0.0057171]
	Learning Rate: 0.00571715
	LOSS [training: 2.3452441196717544 | validation: 2.463926286993413]
	TIME [epoch: 8.32 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.315773530966058		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 2.315773530966058 | validation: 2.4374127866656985]
	TIME [epoch: 8.33 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.397619087082127		[learning rate: 0.0056902]
	Learning Rate: 0.00569021
	LOSS [training: 2.397619087082127 | validation: 2.49551191072001]
	TIME [epoch: 8.32 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349658368194837		[learning rate: 0.0056768]
	Learning Rate: 0.00567679
	LOSS [training: 2.349658368194837 | validation: 2.4467159345564014]
	TIME [epoch: 8.34 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3192905796081438		[learning rate: 0.0056634]
	Learning Rate: 0.0056634
	LOSS [training: 2.3192905796081438 | validation: 2.429283044123535]
	TIME [epoch: 8.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31330208399957		[learning rate: 0.00565]
	Learning Rate: 0.00565004
	LOSS [training: 2.31330208399957 | validation: 2.4515475103995374]
	TIME [epoch: 8.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316077076585903		[learning rate: 0.0056367]
	Learning Rate: 0.00563671
	LOSS [training: 2.316077076585903 | validation: 2.4642488590966116]
	TIME [epoch: 8.33 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3286310194333497		[learning rate: 0.0056234]
	Learning Rate: 0.00562341
	LOSS [training: 2.3286310194333497 | validation: 2.4285922510223115]
	TIME [epoch: 8.32 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3034974268855786		[learning rate: 0.0056101]
	Learning Rate: 0.00561015
	LOSS [training: 2.3034974268855786 | validation: 2.4264519209025925]
	TIME [epoch: 8.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3193220784347206		[learning rate: 0.0055969]
	Learning Rate: 0.00559691
	LOSS [training: 2.3193220784347206 | validation: 2.526953981625188]
	TIME [epoch: 8.37 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.469109644196152		[learning rate: 0.0055837]
	Learning Rate: 0.00558371
	LOSS [training: 2.469109644196152 | validation: 2.671055223417646]
	TIME [epoch: 8.34 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5108922018950697		[learning rate: 0.0055705]
	Learning Rate: 0.00557054
	LOSS [training: 2.5108922018950697 | validation: 2.700271206339448]
	TIME [epoch: 8.33 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.498433147826451		[learning rate: 0.0055574]
	Learning Rate: 0.0055574
	LOSS [training: 2.498433147826451 | validation: 2.624763709872724]
	TIME [epoch: 8.33 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4532234637215553		[learning rate: 0.0055443]
	Learning Rate: 0.00554429
	LOSS [training: 2.4532234637215553 | validation: 2.606546214555428]
	TIME [epoch: 8.33 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.435489251728849		[learning rate: 0.0055312]
	Learning Rate: 0.00553121
	LOSS [training: 2.435489251728849 | validation: 2.5336456711899435]
	TIME [epoch: 8.33 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.384681463168579		[learning rate: 0.0055182]
	Learning Rate: 0.00551817
	LOSS [training: 2.384681463168579 | validation: 2.454239407666742]
	TIME [epoch: 8.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3259402943364074		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 2.3259402943364074 | validation: 2.4372672905796673]
	TIME [epoch: 8.33 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3126411095492516		[learning rate: 0.0054922]
	Learning Rate: 0.00549216
	LOSS [training: 2.3126411095492516 | validation: 2.463823228872454]
	TIME [epoch: 8.32 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3314512233930933		[learning rate: 0.0054792]
	Learning Rate: 0.00547921
	LOSS [training: 2.3314512233930933 | validation: 2.4384835835735297]
	TIME [epoch: 8.33 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.304351143618123		[learning rate: 0.0054663]
	Learning Rate: 0.00546629
	LOSS [training: 2.304351143618123 | validation: 2.4229017281691956]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3018498885533996		[learning rate: 0.0054534]
	Learning Rate: 0.00545339
	LOSS [training: 2.3018498885533996 | validation: 2.4400015886239417]
	TIME [epoch: 8.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319412153925454		[learning rate: 0.0054405]
	Learning Rate: 0.00544053
	LOSS [training: 2.319412153925454 | validation: 2.5472402267691607]
	TIME [epoch: 8.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4370806186473364		[learning rate: 0.0054277]
	Learning Rate: 0.00542769
	LOSS [training: 2.4370806186473364 | validation: 2.7992874449503597]
	TIME [epoch: 8.34 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6635375502532113		[learning rate: 0.0054149]
	Learning Rate: 0.00541489
	LOSS [training: 2.6635375502532113 | validation: 2.7390178123071642]
	TIME [epoch: 8.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.800072368909148		[learning rate: 0.0054021]
	Learning Rate: 0.00540212
	LOSS [training: 2.800072368909148 | validation: 3.444631097419416]
	TIME [epoch: 8.34 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2795360962722304		[learning rate: 0.0053894]
	Learning Rate: 0.00538938
	LOSS [training: 3.2795360962722304 | validation: 3.62371367587503]
	TIME [epoch: 8.34 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3033203694020816		[learning rate: 0.0053767]
	Learning Rate: 0.00537666
	LOSS [training: 3.3033203694020816 | validation: 3.569949855787556]
	TIME [epoch: 8.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3686292646545484		[learning rate: 0.005364]
	Learning Rate: 0.00536398
	LOSS [training: 3.3686292646545484 | validation: 3.779087441999814]
	TIME [epoch: 8.33 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6232908679553133		[learning rate: 0.0053513]
	Learning Rate: 0.00535133
	LOSS [training: 3.6232908679553133 | validation: 4.265542138113251]
	TIME [epoch: 8.33 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9636272521668623		[learning rate: 0.0053387]
	Learning Rate: 0.00533871
	LOSS [training: 3.9636272521668623 | validation: 4.191599057759797]
	TIME [epoch: 8.34 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7165943554159346		[learning rate: 0.0053261]
	Learning Rate: 0.00532611
	LOSS [training: 3.7165943554159346 | validation: 3.6975626820487917]
	TIME [epoch: 8.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.355512464319556		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 3.355512464319556 | validation: 3.4700735119383]
	TIME [epoch: 8.37 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1134558031380273		[learning rate: 0.005301]
	Learning Rate: 0.00530101
	LOSS [training: 3.1134558031380273 | validation: 3.209562239959107]
	TIME [epoch: 8.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8336021539413707		[learning rate: 0.0052885]
	Learning Rate: 0.00528851
	LOSS [training: 2.8336021539413707 | validation: 2.9265923688834965]
	TIME [epoch: 8.33 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6911025122926966		[learning rate: 0.005276]
	Learning Rate: 0.00527604
	LOSS [training: 2.6911025122926966 | validation: 2.81412248697046]
	TIME [epoch: 8.34 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.620410631391995		[learning rate: 0.0052636]
	Learning Rate: 0.00526359
	LOSS [training: 2.620410631391995 | validation: 2.738289430874472]
	TIME [epoch: 8.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.513154029323073		[learning rate: 0.0052512]
	Learning Rate: 0.00525117
	LOSS [training: 2.513154029323073 | validation: 2.674740060972524]
	TIME [epoch: 8.34 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4769164883074772		[learning rate: 0.0052388]
	Learning Rate: 0.00523879
	LOSS [training: 2.4769164883074772 | validation: 2.6615695049932793]
	TIME [epoch: 8.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.472446207762674		[learning rate: 0.0052264]
	Learning Rate: 0.00522643
	LOSS [training: 2.472446207762674 | validation: 2.6162069414214972]
	TIME [epoch: 8.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.449609461958757		[learning rate: 0.0052141]
	Learning Rate: 0.0052141
	LOSS [training: 2.449609461958757 | validation: 2.605449920217256]
	TIME [epoch: 8.33 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4631344291208186		[learning rate: 0.0052018]
	Learning Rate: 0.0052018
	LOSS [training: 2.4631344291208186 | validation: 2.6046485280741454]
	TIME [epoch: 8.34 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.450534829301561		[learning rate: 0.0051895]
	Learning Rate: 0.00518953
	LOSS [training: 2.450534829301561 | validation: 2.593037087062032]
	TIME [epoch: 8.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.429650071868976		[learning rate: 0.0051773]
	Learning Rate: 0.00517729
	LOSS [training: 2.429650071868976 | validation: 2.580974791855802]
	TIME [epoch: 8.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4490724728306845		[learning rate: 0.0051651]
	Learning Rate: 0.00516508
	LOSS [training: 2.4490724728306845 | validation: 2.6241216106978085]
	TIME [epoch: 8.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.47505446729103		[learning rate: 0.0051529]
	Learning Rate: 0.00515289
	LOSS [training: 2.47505446729103 | validation: 2.6520818726921984]
	TIME [epoch: 8.34 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5039270777742515		[learning rate: 0.0051407]
	Learning Rate: 0.00514074
	LOSS [training: 2.5039270777742515 | validation: 2.644828853261103]
	TIME [epoch: 8.33 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.492290176034415		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 2.492290176034415 | validation: 2.6303052217027894]
	TIME [epoch: 8.34 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.48231961501292		[learning rate: 0.0051165]
	Learning Rate: 0.00511652
	LOSS [training: 2.48231961501292 | validation: 2.625655992477019]
	TIME [epoch: 8.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4744617711333		[learning rate: 0.0051044]
	Learning Rate: 0.00510445
	LOSS [training: 2.4744617711333 | validation: 2.62258972264723]
	TIME [epoch: 8.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.460987899678523		[learning rate: 0.0050924]
	Learning Rate: 0.00509241
	LOSS [training: 2.460987899678523 | validation: 2.61140206066519]
	TIME [epoch: 8.34 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.452419310962359		[learning rate: 0.0050804]
	Learning Rate: 0.00508039
	LOSS [training: 2.452419310962359 | validation: 2.6105784322749654]
	TIME [epoch: 8.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4494630245470805		[learning rate: 0.0050684]
	Learning Rate: 0.00506841
	LOSS [training: 2.4494630245470805 | validation: 2.605853323123843]
	TIME [epoch: 8.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4856855522559873		[learning rate: 0.0050565]
	Learning Rate: 0.00505645
	LOSS [training: 2.4856855522559873 | validation: 2.628940688285476]
	TIME [epoch: 8.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4847384570810807		[learning rate: 0.0050445]
	Learning Rate: 0.00504453
	LOSS [training: 2.4847384570810807 | validation: 2.6278852110159923]
	TIME [epoch: 8.34 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4732855458753393		[learning rate: 0.0050326]
	Learning Rate: 0.00503263
	LOSS [training: 2.4732855458753393 | validation: 2.6081141267246526]
	TIME [epoch: 8.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.450562429422132		[learning rate: 0.0050208]
	Learning Rate: 0.00502076
	LOSS [training: 2.450562429422132 | validation: 2.604334385699289]
	TIME [epoch: 8.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4408405867409257		[learning rate: 0.0050089]
	Learning Rate: 0.00500891
	LOSS [training: 2.4408405867409257 | validation: 2.5968248623150703]
	TIME [epoch: 8.34 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.453328168856046		[learning rate: 0.0049971]
	Learning Rate: 0.0049971
	LOSS [training: 2.453328168856046 | validation: 2.617929968662466]
	TIME [epoch: 8.34 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.443925613040002		[learning rate: 0.0049853]
	Learning Rate: 0.00498531
	LOSS [training: 2.443925613040002 | validation: 2.5918628491118003]
	TIME [epoch: 8.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.433529637771542		[learning rate: 0.0049736]
	Learning Rate: 0.00497355
	LOSS [training: 2.433529637771542 | validation: 2.59169602765487]
	TIME [epoch: 8.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4286201168427475		[learning rate: 0.0049618]
	Learning Rate: 0.00496182
	LOSS [training: 2.4286201168427475 | validation: 2.5920595224367613]
	TIME [epoch: 8.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4308972795270325		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 2.4308972795270325 | validation: 2.5866526812467248]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4211130771065714		[learning rate: 0.0049384]
	Learning Rate: 0.00493844
	LOSS [training: 2.4211130771065714 | validation: 2.5778165181535706]
	TIME [epoch: 8.34 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.431109261916165		[learning rate: 0.0049268]
	Learning Rate: 0.00492679
	LOSS [training: 2.431109261916165 | validation: 2.597638554847757]
	TIME [epoch: 8.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4246273395822087		[learning rate: 0.0049152]
	Learning Rate: 0.00491517
	LOSS [training: 2.4246273395822087 | validation: 2.566694542553707]
	TIME [epoch: 8.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5874358788999827		[learning rate: 0.0049036]
	Learning Rate: 0.00490357
	LOSS [training: 2.5874358788999827 | validation: 2.7261713933738987]
	TIME [epoch: 8.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5062863971888443		[learning rate: 0.004892]
	Learning Rate: 0.00489201
	LOSS [training: 2.5062863971888443 | validation: 2.570524028024428]
	TIME [epoch: 8.34 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4146731169673674		[learning rate: 0.0048805]
	Learning Rate: 0.00488047
	LOSS [training: 2.4146731169673674 | validation: 2.5173396115282722]
	TIME [epoch: 8.34 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3708078351645256		[learning rate: 0.004869]
	Learning Rate: 0.00486896
	LOSS [training: 2.3708078351645256 | validation: 2.484334682225705]
	TIME [epoch: 8.34 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3513967082277523		[learning rate: 0.0048575]
	Learning Rate: 0.00485747
	LOSS [training: 2.3513967082277523 | validation: 2.467289218996397]
	TIME [epoch: 8.33 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332057221874314		[learning rate: 0.004846]
	Learning Rate: 0.00484601
	LOSS [training: 2.332057221874314 | validation: 2.464001549462239]
	TIME [epoch: 8.37 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328087770027038		[learning rate: 0.0048346]
	Learning Rate: 0.00483458
	LOSS [training: 2.328087770027038 | validation: 2.4571513142469383]
	TIME [epoch: 8.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3229475892357314		[learning rate: 0.0048232]
	Learning Rate: 0.00482318
	LOSS [training: 2.3229475892357314 | validation: 2.436692725459813]
	TIME [epoch: 8.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337859504703211		[learning rate: 0.0048118]
	Learning Rate: 0.0048118
	LOSS [training: 2.337859504703211 | validation: 2.4611418546266304]
	TIME [epoch: 8.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3327599309938765		[learning rate: 0.0048005]
	Learning Rate: 0.00480045
	LOSS [training: 2.3327599309938765 | validation: 2.45353445593203]
	TIME [epoch: 8.34 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331906422766247		[learning rate: 0.0047891]
	Learning Rate: 0.00478913
	LOSS [training: 2.331906422766247 | validation: 2.4454515227899414]
	TIME [epoch: 8.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3143169855771633		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 2.3143169855771633 | validation: 2.4366842830301554]
	TIME [epoch: 8.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3528793189127835		[learning rate: 0.0047666]
	Learning Rate: 0.00476656
	LOSS [training: 2.3528793189127835 | validation: 2.451399643818292]
	TIME [epoch: 8.34 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336853395430592		[learning rate: 0.0047553]
	Learning Rate: 0.00475532
	LOSS [training: 2.336853395430592 | validation: 2.5973704740229153]
	TIME [epoch: 8.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6066580347591004		[learning rate: 0.0047441]
	Learning Rate: 0.0047441
	LOSS [training: 2.6066580347591004 | validation: 2.6030373171892904]
	TIME [epoch: 8.34 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.481234888483282		[learning rate: 0.0047329]
	Learning Rate: 0.00473291
	LOSS [training: 2.481234888483282 | validation: 2.564313254033931]
	TIME [epoch: 8.34 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4139277158788075		[learning rate: 0.0047217]
	Learning Rate: 0.00472175
	LOSS [training: 2.4139277158788075 | validation: 2.4943721365382645]
	TIME [epoch: 8.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.37034092323288		[learning rate: 0.0047106]
	Learning Rate: 0.00471061
	LOSS [training: 2.37034092323288 | validation: 2.4820299654562596]
	TIME [epoch: 8.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.348171261058		[learning rate: 0.0046995]
	Learning Rate: 0.0046995
	LOSS [training: 2.348171261058 | validation: 2.4956260767955385]
	TIME [epoch: 8.34 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3583847128697144		[learning rate: 0.0046884]
	Learning Rate: 0.00468841
	LOSS [training: 2.3583847128697144 | validation: 2.470221295913576]
	TIME [epoch: 8.34 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3420144200456803		[learning rate: 0.0046774]
	Learning Rate: 0.00467735
	LOSS [training: 2.3420144200456803 | validation: 2.471652936502352]
	TIME [epoch: 8.34 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340205540851762		[learning rate: 0.0046663]
	Learning Rate: 0.00466632
	LOSS [training: 2.340205540851762 | validation: 2.4681661248581617]
	TIME [epoch: 8.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3400764762643114		[learning rate: 0.0046553]
	Learning Rate: 0.00465531
	LOSS [training: 2.3400764762643114 | validation: 2.4873036431682642]
	TIME [epoch: 8.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3601449789524502		[learning rate: 0.0046443]
	Learning Rate: 0.00464433
	LOSS [training: 2.3601449789524502 | validation: 2.4998161675127855]
	TIME [epoch: 8.34 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3902765328151667		[learning rate: 0.0046334]
	Learning Rate: 0.00463338
	LOSS [training: 2.3902765328151667 | validation: 2.521225511100983]
	TIME [epoch: 8.34 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.393940336170745		[learning rate: 0.0046224]
	Learning Rate: 0.00462245
	LOSS [training: 2.393940336170745 | validation: 2.5089393030334812]
	TIME [epoch: 8.34 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375089314014353		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 2.375089314014353 | validation: 2.5642652905624335]
	TIME [epoch: 8.34 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4250202222264052		[learning rate: 0.0046007]
	Learning Rate: 0.00460066
	LOSS [training: 2.4250202222264052 | validation: 2.608852991481479]
	TIME [epoch: 8.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.427659438040169		[learning rate: 0.0045898]
	Learning Rate: 0.00458981
	LOSS [training: 2.427659438040169 | validation: 2.5845954381066925]
	TIME [epoch: 8.37 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6214466879009124		[learning rate: 0.004579]
	Learning Rate: 0.00457899
	LOSS [training: 2.6214466879009124 | validation: 2.972047339252908]
	TIME [epoch: 8.34 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.689748367253812		[learning rate: 0.0045682]
	Learning Rate: 0.00456818
	LOSS [training: 2.689748367253812 | validation: 2.731333972786209]
	TIME [epoch: 8.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7492971693791954		[learning rate: 0.0045574]
	Learning Rate: 0.00455741
	LOSS [training: 2.7492971693791954 | validation: 2.8591853769998004]
	TIME [epoch: 8.34 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7341351470362847		[learning rate: 0.0045467]
	Learning Rate: 0.00454666
	LOSS [training: 2.7341351470362847 | validation: 2.8604268334678697]
	TIME [epoch: 8.34 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.727327914774836		[learning rate: 0.0045359]
	Learning Rate: 0.00453593
	LOSS [training: 2.727327914774836 | validation: 3.0688534910032024]
	TIME [epoch: 8.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.931652793994045		[learning rate: 0.0045252]
	Learning Rate: 0.00452523
	LOSS [training: 2.931652793994045 | validation: 2.856349071812579]
	TIME [epoch: 8.34 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843110188410014		[learning rate: 0.0045146]
	Learning Rate: 0.00451456
	LOSS [training: 2.843110188410014 | validation: 3.223010876409005]
	TIME [epoch: 8.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9151088024070333		[learning rate: 0.0045039]
	Learning Rate: 0.00450391
	LOSS [training: 2.9151088024070333 | validation: 2.976155377158838]
	TIME [epoch: 8.34 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.785252182339		[learning rate: 0.0044933]
	Learning Rate: 0.00449329
	LOSS [training: 2.785252182339 | validation: 3.0866549364872795]
	TIME [epoch: 8.33 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1050080920942436		[learning rate: 0.0044827]
	Learning Rate: 0.00448269
	LOSS [training: 3.1050080920942436 | validation: 3.456444904990466]
	TIME [epoch: 8.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3147506000043894		[learning rate: 0.0044721]
	Learning Rate: 0.00447211
	LOSS [training: 3.3147506000043894 | validation: 3.1102369954548337]
	TIME [epoch: 8.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229514094092771		[learning rate: 0.0044616]
	Learning Rate: 0.00446156
	LOSS [training: 3.229514094092771 | validation: 3.4117127789815163]
	TIME [epoch: 8.34 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3873146960452427		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 3.3873146960452427 | validation: 3.969821373297357]
	TIME [epoch: 8.34 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9716684678543572		[learning rate: 0.0044405]
	Learning Rate: 0.00444054
	LOSS [training: 3.9716684678543572 | validation: 3.9755566968527534]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5680528930594337		[learning rate: 0.0044301]
	Learning Rate: 0.00443007
	LOSS [training: 3.5680528930594337 | validation: 3.380195637244062]
	TIME [epoch: 8.34 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6192261334149207		[learning rate: 0.0044196]
	Learning Rate: 0.00441962
	LOSS [training: 3.6192261334149207 | validation: 3.7588583686802997]
	TIME [epoch: 8.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3751527287469165		[learning rate: 0.0044092]
	Learning Rate: 0.00440919
	LOSS [training: 3.3751527287469165 | validation: 3.0938042624625783]
	TIME [epoch: 8.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0218820704740352		[learning rate: 0.0043988]
	Learning Rate: 0.00439879
	LOSS [training: 3.0218820704740352 | validation: 3.2451587138043254]
	TIME [epoch: 8.34 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2865249654781516		[learning rate: 0.0043884]
	Learning Rate: 0.00438841
	LOSS [training: 3.2865249654781516 | validation: 3.190548906112806]
	TIME [epoch: 8.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8626045428014892		[learning rate: 0.0043781]
	Learning Rate: 0.00437806
	LOSS [training: 2.8626045428014892 | validation: 2.7954254945491916]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.687622354646836		[learning rate: 0.0043677]
	Learning Rate: 0.00436774
	LOSS [training: 2.687622354646836 | validation: 2.831823146241236]
	TIME [epoch: 8.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6684322073397646		[learning rate: 0.0043574]
	Learning Rate: 0.00435743
	LOSS [training: 2.6684322073397646 | validation: 2.7919471344523252]
	TIME [epoch: 8.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6100139261462196		[learning rate: 0.0043472]
	Learning Rate: 0.00434715
	LOSS [training: 2.6100139261462196 | validation: 2.72576200425008]
	TIME [epoch: 8.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5516101383096883		[learning rate: 0.0043369]
	Learning Rate: 0.0043369
	LOSS [training: 2.5516101383096883 | validation: 2.702781811617708]
	TIME [epoch: 8.34 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.536020809432654		[learning rate: 0.0043267]
	Learning Rate: 0.00432667
	LOSS [training: 2.536020809432654 | validation: 2.669723272111621]
	TIME [epoch: 8.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.516473474238747		[learning rate: 0.0043165]
	Learning Rate: 0.00431646
	LOSS [training: 2.516473474238747 | validation: 2.651918279285951]
	TIME [epoch: 8.34 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.494694668815375		[learning rate: 0.0043063]
	Learning Rate: 0.00430628
	LOSS [training: 2.494694668815375 | validation: 2.633428831313159]
	TIME [epoch: 8.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4733699355085395		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 2.4733699355085395 | validation: 2.6056786643061063]
	TIME [epoch: 8.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4617421847296703		[learning rate: 0.004286]
	Learning Rate: 0.00428599
	LOSS [training: 2.4617421847296703 | validation: 2.6003604527001363]
	TIME [epoch: 8.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4613283984322853		[learning rate: 0.0042759]
	Learning Rate: 0.00427588
	LOSS [training: 2.4613283984322853 | validation: 2.601653239489612]
	TIME [epoch: 8.33 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.465084579623191		[learning rate: 0.0042658]
	Learning Rate: 0.0042658
	LOSS [training: 2.465084579623191 | validation: 2.6025289804804044]
	TIME [epoch: 8.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4603522843124184		[learning rate: 0.0042557]
	Learning Rate: 0.00425573
	LOSS [training: 2.4603522843124184 | validation: 2.5894160276369336]
	TIME [epoch: 8.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4540937486571517		[learning rate: 0.0042457]
	Learning Rate: 0.00424569
	LOSS [training: 2.4540937486571517 | validation: 2.5822492473033747]
	TIME [epoch: 8.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4606576362963697		[learning rate: 0.0042357]
	Learning Rate: 0.00423568
	LOSS [training: 2.4606576362963697 | validation: 2.58340826025335]
	TIME [epoch: 8.34 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.555227523312983		[learning rate: 0.0042257]
	Learning Rate: 0.00422569
	LOSS [training: 2.555227523312983 | validation: 2.9778629752788204]
	TIME [epoch: 8.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.024450204006439		[learning rate: 0.0042157]
	Learning Rate: 0.00421572
	LOSS [training: 3.024450204006439 | validation: 2.904561075860209]
	TIME [epoch: 8.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.640308034713665		[learning rate: 0.0042058]
	Learning Rate: 0.00420578
	LOSS [training: 2.640308034713665 | validation: 2.613812911704678]
	TIME [epoch: 8.34 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.471788077025288		[learning rate: 0.0041959]
	Learning Rate: 0.00419585
	LOSS [training: 2.471788077025288 | validation: 2.5622932438545054]
	TIME [epoch: 8.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4101137272893998		[learning rate: 0.004186]
	Learning Rate: 0.00418596
	LOSS [training: 2.4101137272893998 | validation: 2.5341535588187583]
	TIME [epoch: 8.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4012615973910485		[learning rate: 0.0041761]
	Learning Rate: 0.00417608
	LOSS [training: 2.4012615973910485 | validation: 2.5297592847058423]
	TIME [epoch: 8.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3926923075567665		[learning rate: 0.0041662]
	Learning Rate: 0.00416623
	LOSS [training: 2.3926923075567665 | validation: 2.514792646267212]
	TIME [epoch: 8.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3857869620301857		[learning rate: 0.0041564]
	Learning Rate: 0.00415641
	LOSS [training: 2.3857869620301857 | validation: 2.534761538139954]
	TIME [epoch: 8.33 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39644713882416		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 2.39644713882416 | validation: 2.521524913304102]
	TIME [epoch: 8.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3731387936303117		[learning rate: 0.0041368]
	Learning Rate: 0.00413682
	LOSS [training: 2.3731387936303117 | validation: 2.504785063496128]
	TIME [epoch: 8.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3650107805087717		[learning rate: 0.0041271]
	Learning Rate: 0.00412706
	LOSS [training: 2.3650107805087717 | validation: 2.504175139490936]
	TIME [epoch: 8.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3719762030817		[learning rate: 0.0041173]
	Learning Rate: 0.00411733
	LOSS [training: 2.3719762030817 | validation: 2.5099204600869065]
	TIME [epoch: 8.33 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378779511493414		[learning rate: 0.0041076]
	Learning Rate: 0.00410762
	LOSS [training: 2.378779511493414 | validation: 2.4841854440704765]
	TIME [epoch: 8.33 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3496143603597512		[learning rate: 0.0040979]
	Learning Rate: 0.00409793
	LOSS [training: 2.3496143603597512 | validation: 2.4740945854130345]
	TIME [epoch: 8.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3911994023124516		[learning rate: 0.0040883]
	Learning Rate: 0.00408826
	LOSS [training: 2.3911994023124516 | validation: 2.515712251954934]
	TIME [epoch: 8.34 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.34985389684446		[learning rate: 0.0040786]
	Learning Rate: 0.00407862
	LOSS [training: 2.34985389684446 | validation: 2.4630831248098906]
	TIME [epoch: 8.37 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33579188094177		[learning rate: 0.004069]
	Learning Rate: 0.004069
	LOSS [training: 2.33579188094177 | validation: 2.4684021634235465]
	TIME [epoch: 8.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336547426636521		[learning rate: 0.0040594]
	Learning Rate: 0.0040594
	LOSS [training: 2.336547426636521 | validation: 2.470681299043724]
	TIME [epoch: 8.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343804934762173		[learning rate: 0.0040498]
	Learning Rate: 0.00404982
	LOSS [training: 2.343804934762173 | validation: 2.4985743194506167]
	TIME [epoch: 8.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3657362191156457		[learning rate: 0.0040403]
	Learning Rate: 0.00404027
	LOSS [training: 2.3657362191156457 | validation: 2.4980459584865544]
	TIME [epoch: 8.33 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3476368739280833		[learning rate: 0.0040307]
	Learning Rate: 0.00403074
	LOSS [training: 2.3476368739280833 | validation: 2.573697444814537]
	TIME [epoch: 8.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4161558524064253		[learning rate: 0.0040212]
	Learning Rate: 0.00402123
	LOSS [training: 2.4161558524064253 | validation: 2.5403365116479035]
	TIME [epoch: 8.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3869750635138156		[learning rate: 0.0040117]
	Learning Rate: 0.00401175
	LOSS [training: 2.3869750635138156 | validation: 2.5239061661124316]
	TIME [epoch: 8.34 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3763201274555423		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 2.3763201274555423 | validation: 2.5095120277857603]
	TIME [epoch: 8.33 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3702882266054353		[learning rate: 0.0039928]
	Learning Rate: 0.00399284
	LOSS [training: 2.3702882266054353 | validation: 2.5361161733426125]
	TIME [epoch: 8.33 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4102982958542176		[learning rate: 0.0039834]
	Learning Rate: 0.00398342
	LOSS [training: 2.4102982958542176 | validation: 2.695331144391376]
	TIME [epoch: 8.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.665188508680289		[learning rate: 0.003974]
	Learning Rate: 0.00397403
	LOSS [training: 2.665188508680289 | validation: 2.69456535524053]
	TIME [epoch: 8.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5170540060494604		[learning rate: 0.0039647]
	Learning Rate: 0.00396465
	LOSS [training: 2.5170540060494604 | validation: 2.6997124486267987]
	TIME [epoch: 8.34 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5280367629145957		[learning rate: 0.0039553]
	Learning Rate: 0.0039553
	LOSS [training: 2.5280367629145957 | validation: 2.6189922288070315]
	TIME [epoch: 8.34 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4923327344481514		[learning rate: 0.003946]
	Learning Rate: 0.00394597
	LOSS [training: 2.4923327344481514 | validation: 2.661390369900259]
	TIME [epoch: 8.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4594610435972677		[learning rate: 0.0039367]
	Learning Rate: 0.00393666
	LOSS [training: 2.4594610435972677 | validation: 2.528827607668604]
	TIME [epoch: 8.33 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.365230840135082		[learning rate: 0.0039274]
	Learning Rate: 0.00392738
	LOSS [training: 2.365230840135082 | validation: 2.483111045892957]
	TIME [epoch: 8.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3379267902727108		[learning rate: 0.0039181]
	Learning Rate: 0.00391811
	LOSS [training: 2.3379267902727108 | validation: 2.46181193669747]
	TIME [epoch: 8.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3296395537217376		[learning rate: 0.0039089]
	Learning Rate: 0.00390887
	LOSS [training: 2.3296395537217376 | validation: 2.468910124553506]
	TIME [epoch: 8.34 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3278276752926335		[learning rate: 0.0038996]
	Learning Rate: 0.00389965
	LOSS [training: 2.3278276752926335 | validation: 2.4604977959501197]
	TIME [epoch: 8.33 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3260628042645664		[learning rate: 0.0038905]
	Learning Rate: 0.00389045
	LOSS [training: 2.3260628042645664 | validation: 2.4631885618233813]
	TIME [epoch: 8.33 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328328227934585		[learning rate: 0.0038813]
	Learning Rate: 0.00388127
	LOSS [training: 2.328328227934585 | validation: 2.4655594517070236]
	TIME [epoch: 8.33 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332517832044997		[learning rate: 0.0038721]
	Learning Rate: 0.00387212
	LOSS [training: 2.332517832044997 | validation: 2.489341316111231]
	TIME [epoch: 8.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3782448376154237		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 2.3782448376154237 | validation: 2.4874971537770714]
	TIME [epoch: 8.33 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360346020505393		[learning rate: 0.0038539]
	Learning Rate: 0.00385387
	LOSS [training: 2.360346020505393 | validation: 2.467390690654235]
	TIME [epoch: 8.34 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331894963669578		[learning rate: 0.0038448]
	Learning Rate: 0.00384478
	LOSS [training: 2.331894963669578 | validation: 2.447242664457543]
	TIME [epoch: 8.34 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3222686338302667		[learning rate: 0.0038357]
	Learning Rate: 0.00383571
	LOSS [training: 2.3222686338302667 | validation: 2.4456297834515137]
	TIME [epoch: 8.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3154772568036806		[learning rate: 0.0038267]
	Learning Rate: 0.00382667
	LOSS [training: 2.3154772568036806 | validation: 2.439511792417531]
	TIME [epoch: 8.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31674167282983		[learning rate: 0.0038176]
	Learning Rate: 0.00381764
	LOSS [training: 2.31674167282983 | validation: 2.443136551136411]
	TIME [epoch: 8.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3278455533873035		[learning rate: 0.0038086]
	Learning Rate: 0.00380863
	LOSS [training: 2.3278455533873035 | validation: 2.4730522193809223]
	TIME [epoch: 8.33 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3351501326765023		[learning rate: 0.0037996]
	Learning Rate: 0.00379965
	LOSS [training: 2.3351501326765023 | validation: 2.479178079181552]
	TIME [epoch: 8.34 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339004716008994		[learning rate: 0.0037907]
	Learning Rate: 0.00379069
	LOSS [training: 2.339004716008994 | validation: 2.4652499215619175]
	TIME [epoch: 8.33 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3320936124308704		[learning rate: 0.0037817]
	Learning Rate: 0.00378175
	LOSS [training: 2.3320936124308704 | validation: 2.4750942465314223]
	TIME [epoch: 8.33 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3409169896666793		[learning rate: 0.0037728]
	Learning Rate: 0.00377283
	LOSS [training: 2.3409169896666793 | validation: 2.4776278757154175]
	TIME [epoch: 8.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3380115322224366		[learning rate: 0.0037639]
	Learning Rate: 0.00376393
	LOSS [training: 2.3380115322224366 | validation: 2.466341969058537]
	TIME [epoch: 8.34 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334868124784445		[learning rate: 0.003755]
	Learning Rate: 0.00375505
	LOSS [training: 2.334868124784445 | validation: 2.458647428390608]
	TIME [epoch: 8.34 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3177858159434246		[learning rate: 0.0037462]
	Learning Rate: 0.00374619
	LOSS [training: 2.3177858159434246 | validation: 2.4414258404194555]
	TIME [epoch: 8.33 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3138453220838517		[learning rate: 0.0037374]
	Learning Rate: 0.00373735
	LOSS [training: 2.3138453220838517 | validation: 2.4400857086160963]
	TIME [epoch: 8.33 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3183705321389505		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 2.3183705321389505 | validation: 2.4943149486633427]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4039189515675305		[learning rate: 0.0037197]
	Learning Rate: 0.00371974
	LOSS [training: 2.4039189515675305 | validation: 2.5239388456862706]
	TIME [epoch: 8.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3601308909163956		[learning rate: 0.003711]
	Learning Rate: 0.00371097
	LOSS [training: 2.3601308909163956 | validation: 2.4889939911386394]
	TIME [epoch: 8.34 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332600399116202		[learning rate: 0.0037022]
	Learning Rate: 0.00370221
	LOSS [training: 2.332600399116202 | validation: 2.4496728354322164]
	TIME [epoch: 8.33 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312969212938668		[learning rate: 0.0036935]
	Learning Rate: 0.00369348
	LOSS [training: 2.312969212938668 | validation: 2.4338832037689793]
	TIME [epoch: 8.33 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31810314787787		[learning rate: 0.0036848]
	Learning Rate: 0.00368477
	LOSS [training: 2.31810314787787 | validation: 2.464674645338871]
	TIME [epoch: 8.33 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3237623766920663		[learning rate: 0.0036761]
	Learning Rate: 0.00367608
	LOSS [training: 2.3237623766920663 | validation: 2.4458704572478034]
	TIME [epoch: 8.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.344343333280877		[learning rate: 0.0036674]
	Learning Rate: 0.00366741
	LOSS [training: 2.344343333280877 | validation: 2.489533602827719]
	TIME [epoch: 8.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3422303125663024		[learning rate: 0.0036588]
	Learning Rate: 0.00365875
	LOSS [training: 2.3422303125663024 | validation: 2.4491376068446087]
	TIME [epoch: 8.33 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.327615791205092		[learning rate: 0.0036501]
	Learning Rate: 0.00365012
	LOSS [training: 2.327615791205092 | validation: 2.516169729519282]
	TIME [epoch: 8.34 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3747678803038377		[learning rate: 0.0036415]
	Learning Rate: 0.00364151
	LOSS [training: 2.3747678803038377 | validation: 2.47623688002165]
	TIME [epoch: 8.33 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331196962397512		[learning rate: 0.0036329]
	Learning Rate: 0.00363292
	LOSS [training: 2.331196962397512 | validation: 2.4454237361364983]
	TIME [epoch: 8.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3217933184003336		[learning rate: 0.0036244]
	Learning Rate: 0.00362436
	LOSS [training: 2.3217933184003336 | validation: 2.451098370119909]
	TIME [epoch: 8.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335743099083463		[learning rate: 0.0036158]
	Learning Rate: 0.00361581
	LOSS [training: 2.335743099083463 | validation: 2.448849510513643]
	TIME [epoch: 8.34 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3183065113531227		[learning rate: 0.0036073]
	Learning Rate: 0.00360728
	LOSS [training: 2.3183065113531227 | validation: 2.4995013583961208]
	TIME [epoch: 8.34 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3524028069983065		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 2.3524028069983065 | validation: 2.451217718022079]
	TIME [epoch: 8.34 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.317651133111189		[learning rate: 0.0035903]
	Learning Rate: 0.00359028
	LOSS [training: 2.317651133111189 | validation: 2.4471365685332493]
	TIME [epoch: 8.34 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312274804421116		[learning rate: 0.0035818]
	Learning Rate: 0.00358181
	LOSS [training: 2.312274804421116 | validation: 2.4518567363435406]
	TIME [epoch: 8.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3282405789996803		[learning rate: 0.0035734]
	Learning Rate: 0.00357336
	LOSS [training: 2.3282405789996803 | validation: 2.475601132158248]
	TIME [epoch: 8.34 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3424357746458933		[learning rate: 0.0035649]
	Learning Rate: 0.00356493
	LOSS [training: 2.3424357746458933 | validation: 2.477355432337947]
	TIME [epoch: 8.33 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.357648698819463		[learning rate: 0.0035565]
	Learning Rate: 0.00355652
	LOSS [training: 2.357648698819463 | validation: 2.4835614521050626]
	TIME [epoch: 8.33 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3507338127427944		[learning rate: 0.0035481]
	Learning Rate: 0.00354813
	LOSS [training: 2.3507338127427944 | validation: 2.5359609500559674]
	TIME [epoch: 8.33 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3912753080785025		[learning rate: 0.0035398]
	Learning Rate: 0.00353976
	LOSS [training: 2.3912753080785025 | validation: 2.4990856700083546]
	TIME [epoch: 8.33 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3545244389110556		[learning rate: 0.0035314]
	Learning Rate: 0.00353141
	LOSS [training: 2.3545244389110556 | validation: 2.4590857695985155]
	TIME [epoch: 8.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3269580031504526		[learning rate: 0.0035231]
	Learning Rate: 0.00352308
	LOSS [training: 2.3269580031504526 | validation: 2.435069156682604]
	TIME [epoch: 8.34 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.315891253039819		[learning rate: 0.0035148]
	Learning Rate: 0.00351477
	LOSS [training: 2.315891253039819 | validation: 2.4324346675105746]
	TIME [epoch: 8.33 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322397314181627		[learning rate: 0.0035065]
	Learning Rate: 0.00350648
	LOSS [training: 2.322397314181627 | validation: 2.455745951359905]
	TIME [epoch: 8.34 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337545281641092		[learning rate: 0.0034982]
	Learning Rate: 0.00349821
	LOSS [training: 2.337545281641092 | validation: 2.4554188493351043]
	TIME [epoch: 8.34 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334377058470573		[learning rate: 0.00349]
	Learning Rate: 0.00348996
	LOSS [training: 2.334377058470573 | validation: 2.461381292648469]
	TIME [epoch: 8.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3320564234489636		[learning rate: 0.0034817]
	Learning Rate: 0.00348173
	LOSS [training: 2.3320564234489636 | validation: 2.437244625822946]
	TIME [epoch: 8.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3171154574335984		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 2.3171154574335984 | validation: 2.4478123656071196]
	TIME [epoch: 8.33 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3307200889168547		[learning rate: 0.0034653]
	Learning Rate: 0.00346532
	LOSS [training: 2.3307200889168547 | validation: 2.4715274444548925]
	TIME [epoch: 8.33 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3358396783510806		[learning rate: 0.0034571]
	Learning Rate: 0.00345715
	LOSS [training: 2.3358396783510806 | validation: 2.4791227048147677]
	TIME [epoch: 8.33 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328065515317836		[learning rate: 0.003449]
	Learning Rate: 0.00344899
	LOSS [training: 2.328065515317836 | validation: 2.4315643529568374]
	TIME [epoch: 8.33 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316583550050959		[learning rate: 0.0034409]
	Learning Rate: 0.00344086
	LOSS [training: 2.316583550050959 | validation: 2.480738991109848]
	TIME [epoch: 8.38 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.327773025953496		[learning rate: 0.0034327]
	Learning Rate: 0.00343274
	LOSS [training: 2.327773025953496 | validation: 2.42353881099525]
	TIME [epoch: 8.34 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.305590582433256		[learning rate: 0.0034246]
	Learning Rate: 0.00342464
	LOSS [training: 2.305590582433256 | validation: 2.450614092569632]
	TIME [epoch: 8.33 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3150027871312404		[learning rate: 0.0034166]
	Learning Rate: 0.00341657
	LOSS [training: 2.3150027871312404 | validation: 2.4160768188085675]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2982156134406995		[learning rate: 0.0034085]
	Learning Rate: 0.00340851
	LOSS [training: 2.2982156134406995 | validation: 2.411749446307058]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240518_170000/states/model_phi2_1a_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2961946185453184		[learning rate: 0.0034005]
	Learning Rate: 0.00340047
	LOSS [training: 2.2961946185453184 | validation: 2.4173158957109884]
	TIME [epoch: 8.36 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.297215142428832		[learning rate: 0.0033924]
	Learning Rate: 0.00339244
	LOSS [training: 2.297215142428832 | validation: 2.420392050119964]
	TIME [epoch: 8.35 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.301305496579352		[learning rate: 0.0033844]
	Learning Rate: 0.00338444
	LOSS [training: 2.301305496579352 | validation: 2.425252756983093]
	TIME [epoch: 8.32 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3103505571306364		[learning rate: 0.0033765]
	Learning Rate: 0.00337646
	LOSS [training: 2.3103505571306364 | validation: 2.4357145381247247]
	TIME [epoch: 8.33 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3028582730421174		[learning rate: 0.0033685]
	Learning Rate: 0.0033685
	LOSS [training: 2.3028582730421174 | validation: 2.4341928641623785]
	TIME [epoch: 8.32 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32953725725362		[learning rate: 0.0033605]
	Learning Rate: 0.00336055
	LOSS [training: 2.32953725725362 | validation: 2.468839953156107]
	TIME [epoch: 8.34 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3178134882302412		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 2.3178134882302412 | validation: 2.421768229999058]
	TIME [epoch: 8.39 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3091353892901454		[learning rate: 0.0033447]
	Learning Rate: 0.00334471
	LOSS [training: 2.3091353892901454 | validation: 2.4424910143929868]
	TIME [epoch: 8.34 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349981833934828		[learning rate: 0.0033368]
	Learning Rate: 0.00333682
	LOSS [training: 2.349981833934828 | validation: 2.456079916473316]
	TIME [epoch: 8.33 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3245478997021265		[learning rate: 0.003329]
	Learning Rate: 0.00332895
	LOSS [training: 2.3245478997021265 | validation: 2.4619740457079873]
	TIME [epoch: 8.33 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39797144620854		[learning rate: 0.0033211]
	Learning Rate: 0.0033211
	LOSS [training: 2.39797144620854 | validation: 2.4535032570009765]
	TIME [epoch: 8.33 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3177452685617257		[learning rate: 0.0033133]
	Learning Rate: 0.00331327
	LOSS [training: 2.3177452685617257 | validation: 2.4346695291534752]
	TIME [epoch: 8.35 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.308758477839141		[learning rate: 0.0033055]
	Learning Rate: 0.00330545
	LOSS [training: 2.308758477839141 | validation: 2.4272190873834267]
	TIME [epoch: 8.37 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.306293531207372		[learning rate: 0.0032977]
	Learning Rate: 0.00329765
	LOSS [training: 2.306293531207372 | validation: 2.4703948437811105]
	TIME [epoch: 8.32 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360763027176594		[learning rate: 0.0032899]
	Learning Rate: 0.00328988
	LOSS [training: 2.360763027176594 | validation: 2.44891497178181]
	TIME [epoch: 8.33 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3116288633090005		[learning rate: 0.0032821]
	Learning Rate: 0.00328212
	LOSS [training: 2.3116288633090005 | validation: 2.4351567440997144]
	TIME [epoch: 8.33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326647127470905		[learning rate: 0.0032744]
	Learning Rate: 0.00327437
	LOSS [training: 2.326647127470905 | validation: 2.4355063071769805]
	TIME [epoch: 8.33 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.306989085434943		[learning rate: 0.0032666]
	Learning Rate: 0.00326665
	LOSS [training: 2.306989085434943 | validation: 2.4432447978691854]
	TIME [epoch: 8.38 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3208289324610307		[learning rate: 0.0032589]
	Learning Rate: 0.00325894
	LOSS [training: 2.3208289324610307 | validation: 2.47185657610156]
	TIME [epoch: 8.34 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330669117046252		[learning rate: 0.0032513]
	Learning Rate: 0.00325126
	LOSS [training: 2.330669117046252 | validation: 2.4545699006673862]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3415179408966544		[learning rate: 0.0032436]
	Learning Rate: 0.00324359
	LOSS [training: 2.3415179408966544 | validation: 2.461087327013625]
	TIME [epoch: 8.34 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.325283223514712		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 2.325283223514712 | validation: 2.470550878951063]
	TIME [epoch: 8.33 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370829969459808		[learning rate: 0.0032283]
	Learning Rate: 0.0032283
	LOSS [training: 2.370829969459808 | validation: 2.456364912050266]
	TIME [epoch: 8.34 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3341131415555743		[learning rate: 0.0032207]
	Learning Rate: 0.00322069
	LOSS [training: 2.3341131415555743 | validation: 2.4407432594169265]
	TIME [epoch: 8.39 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3200406989338314		[learning rate: 0.0032131]
	Learning Rate: 0.00321309
	LOSS [training: 2.3200406989338314 | validation: 2.437425894263809]
	TIME [epoch: 8.34 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3179389279038474		[learning rate: 0.0032055]
	Learning Rate: 0.00320551
	LOSS [training: 2.3179389279038474 | validation: 2.4417589523384]
	TIME [epoch: 8.33 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.344170271801005		[learning rate: 0.003198]
	Learning Rate: 0.00319795
	LOSS [training: 2.344170271801005 | validation: 2.470972669620358]
	TIME [epoch: 8.33 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3428033181130257		[learning rate: 0.0031904]
	Learning Rate: 0.00319041
	LOSS [training: 2.3428033181130257 | validation: 2.450191758801888]
	TIME [epoch: 8.33 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336123423078305		[learning rate: 0.0031829]
	Learning Rate: 0.00318288
	LOSS [training: 2.336123423078305 | validation: 2.501607660578667]
	TIME [epoch: 8.38 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3558943370111463		[learning rate: 0.0031754]
	Learning Rate: 0.00317537
	LOSS [training: 2.3558943370111463 | validation: 2.485834876497078]
	TIME [epoch: 8.35 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3359562187053404		[learning rate: 0.0031679]
	Learning Rate: 0.00316788
	LOSS [training: 2.3359562187053404 | validation: 2.462192486266331]
	TIME [epoch: 8.34 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.317884967478992		[learning rate: 0.0031604]
	Learning Rate: 0.00316041
	LOSS [training: 2.317884967478992 | validation: 2.4285544496908225]
	TIME [epoch: 8.34 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.308112949956908		[learning rate: 0.003153]
	Learning Rate: 0.00315296
	LOSS [training: 2.308112949956908 | validation: 2.4283957532683074]
	TIME [epoch: 8.34 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31408183962864		[learning rate: 0.0031455]
	Learning Rate: 0.00314552
	LOSS [training: 2.31408183962864 | validation: 2.4296285584990205]
	TIME [epoch: 8.34 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3066032008943127		[learning rate: 0.0031381]
	Learning Rate: 0.0031381
	LOSS [training: 2.3066032008943127 | validation: 2.4317401951440063]
	TIME [epoch: 8.38 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3168295490843693		[learning rate: 0.0031307]
	Learning Rate: 0.0031307
	LOSS [training: 2.3168295490843693 | validation: 2.4601955760749057]
	TIME [epoch: 8.34 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3471475425901778		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 2.3471475425901778 | validation: 2.4452756016851374]
	TIME [epoch: 8.34 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316885997042137		[learning rate: 0.0031159]
	Learning Rate: 0.00311594
	LOSS [training: 2.316885997042137 | validation: 2.4503638018863505]
	TIME [epoch: 8.34 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342273173863119		[learning rate: 0.0031086]
	Learning Rate: 0.0031086
	LOSS [training: 2.342273173863119 | validation: 2.468633317096563]
	TIME [epoch: 8.33 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3286577507409505		[learning rate: 0.0031013]
	Learning Rate: 0.00310126
	LOSS [training: 2.3286577507409505 | validation: 2.441073403265876]
	TIME [epoch: 8.37 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3429789325772865		[learning rate: 0.0030939]
	Learning Rate: 0.00309395
	LOSS [training: 2.3429789325772865 | validation: 2.5288841358837257]
	TIME [epoch: 8.35 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.351782029679277		[learning rate: 0.0030866]
	Learning Rate: 0.00308665
	LOSS [training: 2.351782029679277 | validation: 2.4516489588344843]
	TIME [epoch: 8.33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33480336158642		[learning rate: 0.0030794]
	Learning Rate: 0.00307937
	LOSS [training: 2.33480336158642 | validation: 2.4378265506147114]
	TIME [epoch: 8.34 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3208974952547807		[learning rate: 0.0030721]
	Learning Rate: 0.0030721
	LOSS [training: 2.3208974952547807 | validation: 2.447570249347087]
	TIME [epoch: 8.33 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3396626961630496		[learning rate: 0.0030649]
	Learning Rate: 0.00306486
	LOSS [training: 2.3396626961630496 | validation: 2.5026152915508906]
	TIME [epoch: 8.33 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5255582662039857		[learning rate: 0.0030576]
	Learning Rate: 0.00305763
	LOSS [training: 2.5255582662039857 | validation: 2.6630214881439827]
	TIME [epoch: 8.38 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.443305061355802		[learning rate: 0.0030504]
	Learning Rate: 0.00305042
	LOSS [training: 2.443305061355802 | validation: 2.453458889753242]
	TIME [epoch: 8.34 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328808868676539		[learning rate: 0.0030432]
	Learning Rate: 0.00304322
	LOSS [training: 2.328808868676539 | validation: 2.444319689167721]
	TIME [epoch: 8.34 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3324099024179006		[learning rate: 0.003036]
	Learning Rate: 0.00303604
	LOSS [training: 2.3324099024179006 | validation: 2.4597251996466314]
	TIME [epoch: 8.34 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3269818715272335		[learning rate: 0.0030289]
	Learning Rate: 0.00302888
	LOSS [training: 2.3269818715272335 | validation: 2.4295806305711296]
	TIME [epoch: 8.33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3148704231088795		[learning rate: 0.0030217]
	Learning Rate: 0.00302174
	LOSS [training: 2.3148704231088795 | validation: 2.438058287023006]
	TIME [epoch: 8.36 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323957408672142		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 2.323957408672142 | validation: 2.453010822775319]
	TIME [epoch: 8.36 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3262164592431143		[learning rate: 0.0030075]
	Learning Rate: 0.0030075
	LOSS [training: 2.3262164592431143 | validation: 2.4596395346138236]
	TIME [epoch: 8.34 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3245608144644616		[learning rate: 0.0030004]
	Learning Rate: 0.0030004
	LOSS [training: 2.3245608144644616 | validation: 2.4544693203422523]
	TIME [epoch: 8.33 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3239515847976024		[learning rate: 0.0029933]
	Learning Rate: 0.00299332
	LOSS [training: 2.3239515847976024 | validation: 2.4478850216497534]
	TIME [epoch: 8.34 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3256007733876363		[learning rate: 0.0029863]
	Learning Rate: 0.00298626
	LOSS [training: 2.3256007733876363 | validation: 2.4610603419289063]
	TIME [epoch: 8.33 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3263118125646165		[learning rate: 0.0029792]
	Learning Rate: 0.00297922
	LOSS [training: 2.3263118125646165 | validation: 2.4567756119402873]
	TIME [epoch: 8.38 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3386197347116267		[learning rate: 0.0029722]
	Learning Rate: 0.00297219
	LOSS [training: 2.3386197347116267 | validation: 2.469204995874575]
	TIME [epoch: 8.34 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.341911935634577		[learning rate: 0.0029652]
	Learning Rate: 0.00296518
	LOSS [training: 2.341911935634577 | validation: 2.4701439140243444]
	TIME [epoch: 8.33 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3463391517478724		[learning rate: 0.0029582]
	Learning Rate: 0.00295819
	LOSS [training: 2.3463391517478724 | validation: 2.474556170021909]
	TIME [epoch: 8.34 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.345141095088206		[learning rate: 0.0029512]
	Learning Rate: 0.00295121
	LOSS [training: 2.345141095088206 | validation: 2.4690984370326823]
	TIME [epoch: 8.34 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340990506848108		[learning rate: 0.0029442]
	Learning Rate: 0.00294425
	LOSS [training: 2.340990506848108 | validation: 2.4480148626710587]
	TIME [epoch: 8.34 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32316795315387		[learning rate: 0.0029373]
	Learning Rate: 0.0029373
	LOSS [training: 2.32316795315387 | validation: 2.4468062004035223]
	TIME [epoch: 8.38 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3207515401320213		[learning rate: 0.0029304]
	Learning Rate: 0.00293037
	LOSS [training: 2.3207515401320213 | validation: 2.4587440484253182]
	TIME [epoch: 8.34 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3408546071480854		[learning rate: 0.0029235]
	Learning Rate: 0.00292346
	LOSS [training: 2.3408546071480854 | validation: 2.491320392306476]
	TIME [epoch: 8.34 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3454983221856294		[learning rate: 0.0029166]
	Learning Rate: 0.00291657
	LOSS [training: 2.3454983221856294 | validation: 2.4781987970594783]
	TIME [epoch: 8.33 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3556558299948023		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 2.3556558299948023 | validation: 2.535166547839301]
	TIME [epoch: 8.34 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.37477851798707		[learning rate: 0.0029028]
	Learning Rate: 0.00290282
	LOSS [training: 2.37477851798707 | validation: 2.46476063970505]
	TIME [epoch: 8.38 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3343630866353333		[learning rate: 0.002896]
	Learning Rate: 0.00289598
	LOSS [training: 2.3343630866353333 | validation: 2.4551784018736402]
	TIME [epoch: 8.34 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3236396330201696		[learning rate: 0.0028891]
	Learning Rate: 0.00288914
	LOSS [training: 2.3236396330201696 | validation: 2.457153871825729]
	TIME [epoch: 8.35 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.325010170089792		[learning rate: 0.0028823]
	Learning Rate: 0.00288233
	LOSS [training: 2.325010170089792 | validation: 2.4429610315139105]
	TIME [epoch: 8.33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.314829524876191		[learning rate: 0.0028755]
	Learning Rate: 0.00287553
	LOSS [training: 2.314829524876191 | validation: 2.4431961717864166]
	TIME [epoch: 8.33 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316935213094647		[learning rate: 0.0028687]
	Learning Rate: 0.00286875
	LOSS [training: 2.316935213094647 | validation: 2.442701424017976]
	TIME [epoch: 8.34 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319034832282373		[learning rate: 0.002862]
	Learning Rate: 0.00286198
	LOSS [training: 2.319034832282373 | validation: 2.453625837646882]
	TIME [epoch: 8.37 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3257429936318066		[learning rate: 0.0028552]
	Learning Rate: 0.00285523
	LOSS [training: 2.3257429936318066 | validation: 2.4464884956057316]
	TIME [epoch: 8.33 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321647462681095		[learning rate: 0.0028485]
	Learning Rate: 0.00284849
	LOSS [training: 2.321647462681095 | validation: 2.4531375481264677]
	TIME [epoch: 8.34 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323970764240798		[learning rate: 0.0028418]
	Learning Rate: 0.00284178
	LOSS [training: 2.323970764240798 | validation: 2.480032135847039]
	TIME [epoch: 8.34 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.345094903389171		[learning rate: 0.0028351]
	Learning Rate: 0.00283507
	LOSS [training: 2.345094903389171 | validation: 2.4932249507226394]
	TIME [epoch: 8.34 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3587460729350838		[learning rate: 0.0028284]
	Learning Rate: 0.00282838
	LOSS [training: 2.3587460729350838 | validation: 2.479763247642127]
	TIME [epoch: 8.38 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3397238275871555		[learning rate: 0.0028217]
	Learning Rate: 0.00282171
	LOSS [training: 2.3397238275871555 | validation: 2.4541882883140222]
	TIME [epoch: 8.35 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3412853656410393		[learning rate: 0.0028151]
	Learning Rate: 0.00281506
	LOSS [training: 2.3412853656410393 | validation: 2.4585543112527644]
	TIME [epoch: 8.34 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326416457931374		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 2.326416457931374 | validation: 2.464399913416523]
	TIME [epoch: 8.33 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326707174419156		[learning rate: 0.0028018]
	Learning Rate: 0.00280179
	LOSS [training: 2.326707174419156 | validation: 2.444658048103455]
	TIME [epoch: 8.33 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.327636565160993		[learning rate: 0.0027952]
	Learning Rate: 0.00279518
	LOSS [training: 2.327636565160993 | validation: 2.4794700074596676]
	TIME [epoch: 8.33 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4148087747101337		[learning rate: 0.0027886]
	Learning Rate: 0.00278859
	LOSS [training: 2.4148087747101337 | validation: 2.5176619729084813]
	TIME [epoch: 8.37 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4074284821714045		[learning rate: 0.002782]
	Learning Rate: 0.00278201
	LOSS [training: 2.4074284821714045 | validation: 2.5692548155347543]
	TIME [epoch: 8.34 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.414691048214605		[learning rate: 0.0027754]
	Learning Rate: 0.00277545
	LOSS [training: 2.414691048214605 | validation: 2.537016287827351]
	TIME [epoch: 8.33 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.451959965741776		[learning rate: 0.0027689]
	Learning Rate: 0.0027689
	LOSS [training: 2.451959965741776 | validation: 2.717891988898617]
	TIME [epoch: 8.33 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5252189426777685		[learning rate: 0.0027624]
	Learning Rate: 0.00276237
	LOSS [training: 2.5252189426777685 | validation: 2.6477637050993983]
	TIME [epoch: 8.33 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4774409536650017		[learning rate: 0.0027559]
	Learning Rate: 0.00275586
	LOSS [training: 2.4774409536650017 | validation: 2.5894318873449245]
	TIME [epoch: 8.36 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4148051916314683		[learning rate: 0.0027494]
	Learning Rate: 0.00274936
	LOSS [training: 2.4148051916314683 | validation: 2.569425100691257]
	TIME [epoch: 8.36 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4058385786208616		[learning rate: 0.0027429]
	Learning Rate: 0.00274287
	LOSS [training: 2.4058385786208616 | validation: 2.544996312280734]
	TIME [epoch: 8.33 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378052138850813		[learning rate: 0.0027364]
	Learning Rate: 0.0027364
	LOSS [training: 2.378052138850813 | validation: 2.514673511404542]
	TIME [epoch: 8.33 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3626230353846847		[learning rate: 0.0027299]
	Learning Rate: 0.00272994
	LOSS [training: 2.3626230353846847 | validation: 2.5032704757622035]
	TIME [epoch: 8.34 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3537379704204358		[learning rate: 0.0027235]
	Learning Rate: 0.00272351
	LOSS [training: 2.3537379704204358 | validation: 2.4860120737723417]
	TIME [epoch: 8.33 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342678665219221		[learning rate: 0.0027171]
	Learning Rate: 0.00271708
	LOSS [training: 2.342678665219221 | validation: 2.4811238321085574]
	TIME [epoch: 8.38 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.349155065740275		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 2.349155065740275 | validation: 2.5007928744880368]
	TIME [epoch: 8.33 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340564823538581		[learning rate: 0.0027043]
	Learning Rate: 0.00270428
	LOSS [training: 2.340564823538581 | validation: 2.4657291290457968]
	TIME [epoch: 8.33 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328684810549349		[learning rate: 0.0026979]
	Learning Rate: 0.0026979
	LOSS [training: 2.328684810549349 | validation: 2.4742256399094162]
	TIME [epoch: 8.33 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337016254650116		[learning rate: 0.0026915]
	Learning Rate: 0.00269153
	LOSS [training: 2.337016254650116 | validation: 2.4483574401997634]
	TIME [epoch: 8.33 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3183528412321044		[learning rate: 0.0026852]
	Learning Rate: 0.00268519
	LOSS [training: 2.3183528412321044 | validation: 2.4428520841073946]
	TIME [epoch: 8.35 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3206195793462747		[learning rate: 0.0026789]
	Learning Rate: 0.00267885
	LOSS [training: 2.3206195793462747 | validation: 2.4678545054906924]
	TIME [epoch: 8.37 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337841252704064		[learning rate: 0.0026725]
	Learning Rate: 0.00267253
	LOSS [training: 2.337841252704064 | validation: 2.4958221760642836]
	TIME [epoch: 8.31 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3671989714322423		[learning rate: 0.0026662]
	Learning Rate: 0.00266623
	LOSS [training: 2.3671989714322423 | validation: 2.501547996335799]
	TIME [epoch: 8.33 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3587264050918133		[learning rate: 0.0026599]
	Learning Rate: 0.00265994
	LOSS [training: 2.3587264050918133 | validation: 2.4890994550100434]
	TIME [epoch: 8.32 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.353219620937923		[learning rate: 0.0026537]
	Learning Rate: 0.00265367
	LOSS [training: 2.353219620937923 | validation: 2.4794351090919475]
	TIME [epoch: 8.33 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3354148005160282		[learning rate: 0.0026474]
	Learning Rate: 0.00264741
	LOSS [training: 2.3354148005160282 | validation: 2.4497665088225196]
	TIME [epoch: 8.38 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324665410730681		[learning rate: 0.0026412]
	Learning Rate: 0.00264116
	LOSS [training: 2.324665410730681 | validation: 2.448671575214498]
	TIME [epoch: 8.35 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3199189087551835		[learning rate: 0.0026349]
	Learning Rate: 0.00263493
	LOSS [training: 2.3199189087551835 | validation: 2.4763864538623936]
	TIME [epoch: 8.34 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342259622076245		[learning rate: 0.0026287]
	Learning Rate: 0.00262872
	LOSS [training: 2.342259622076245 | validation: 2.4550885391169253]
	TIME [epoch: 8.33 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321477831974007		[learning rate: 0.0026225]
	Learning Rate: 0.00262251
	LOSS [training: 2.321477831974007 | validation: 2.4421442027540223]
	TIME [epoch: 8.34 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323994529486359		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 2.323994529486359 | validation: 2.4584644235257542]
	TIME [epoch: 8.34 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3299262738374042		[learning rate: 0.0026102]
	Learning Rate: 0.00261016
	LOSS [training: 2.3299262738374042 | validation: 2.476962999558303]
	TIME [epoch: 8.39 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3334840501998357		[learning rate: 0.002604]
	Learning Rate: 0.002604
	LOSS [training: 2.3334840501998357 | validation: 2.4529738121190503]
	TIME [epoch: 8.34 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323466447965184		[learning rate: 0.0025979]
	Learning Rate: 0.00259786
	LOSS [training: 2.323466447965184 | validation: 2.4633269401138103]
	TIME [epoch: 8.33 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3482866888878573		[learning rate: 0.0025917]
	Learning Rate: 0.00259173
	LOSS [training: 2.3482866888878573 | validation: 2.482779716967907]
	TIME [epoch: 8.33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.346057134040028		[learning rate: 0.0025856]
	Learning Rate: 0.00258562
	LOSS [training: 2.346057134040028 | validation: 2.548694224029701]
	TIME [epoch: 8.33 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3921914014551904		[learning rate: 0.0025795]
	Learning Rate: 0.00257952
	LOSS [training: 2.3921914014551904 | validation: 2.539128979037402]
	TIME [epoch: 8.37 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.366495516322346		[learning rate: 0.0025734]
	Learning Rate: 0.00257343
	LOSS [training: 2.366495516322346 | validation: 2.5447039991751383]
	TIME [epoch: 8.34 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3884518385314317		[learning rate: 0.0025674]
	Learning Rate: 0.00256736
	LOSS [training: 2.3884518385314317 | validation: 2.5680466808260687]
	TIME [epoch: 8.33 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3813006636949687		[learning rate: 0.0025613]
	Learning Rate: 0.00256131
	LOSS [training: 2.3813006636949687 | validation: 2.5099960500869023]
	TIME [epoch: 8.33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.345723312886037		[learning rate: 0.0025553]
	Learning Rate: 0.00255526
	LOSS [training: 2.345723312886037 | validation: 2.486474147590819]
	TIME [epoch: 8.34 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3439646446170133		[learning rate: 0.0025492]
	Learning Rate: 0.00254924
	LOSS [training: 2.3439646446170133 | validation: 2.4966276364185456]
	TIME [epoch: 8.34 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324723173277361		[learning rate: 0.0025432]
	Learning Rate: 0.00254322
	LOSS [training: 2.324723173277361 | validation: 2.436892624296445]
	TIME [epoch: 8.38 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3120352487851505		[learning rate: 0.0025372]
	Learning Rate: 0.00253722
	LOSS [training: 2.3120352487851505 | validation: 2.438483608337644]
	TIME [epoch: 8.34 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324581952421526		[learning rate: 0.0025312]
	Learning Rate: 0.00253124
	LOSS [training: 2.324581952421526 | validation: 2.4259923384844573]
	TIME [epoch: 8.33 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3120873971727463		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 2.3120873971727463 | validation: 2.442359048459072]
	TIME [epoch: 8.33 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3184006358893283		[learning rate: 0.0025193]
	Learning Rate: 0.00251931
	LOSS [training: 2.3184006358893283 | validation: 2.4498535635432894]
	TIME [epoch: 8.33 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321264162807869		[learning rate: 0.0025134]
	Learning Rate: 0.00251337
	LOSS [training: 2.321264162807869 | validation: 2.4737850483012256]
	TIME [epoch: 8.38 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335488080965183		[learning rate: 0.0025074]
	Learning Rate: 0.00250744
	LOSS [training: 2.335488080965183 | validation: 2.452801759326013]
	TIME [epoch: 8.34 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3293621986861215		[learning rate: 0.0025015]
	Learning Rate: 0.00250153
	LOSS [training: 2.3293621986861215 | validation: 2.4812627821394626]
	TIME [epoch: 8.33 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.376111916425738		[learning rate: 0.0024956]
	Learning Rate: 0.00249563
	LOSS [training: 2.376111916425738 | validation: 2.7477235499080823]
	TIME [epoch: 8.33 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6218017945426673		[learning rate: 0.0024897]
	Learning Rate: 0.00248974
	LOSS [training: 2.6218017945426673 | validation: 2.706428070307008]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4884383372570644		[learning rate: 0.0024839]
	Learning Rate: 0.00248387
	LOSS [training: 2.4884383372570644 | validation: 2.655696253198363]
	TIME [epoch: 8.34 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4721687278143385		[learning rate: 0.002478]
	Learning Rate: 0.00247801
	LOSS [training: 2.4721687278143385 | validation: 2.6341922810586555]
	TIME [epoch: 8.39 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4546921077579364		[learning rate: 0.0024722]
	Learning Rate: 0.00247216
	LOSS [training: 2.4546921077579364 | validation: 2.6099169687700146]
	TIME [epoch: 8.34 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.447937940914318		[learning rate: 0.0024663]
	Learning Rate: 0.00246633
	LOSS [training: 2.447937940914318 | validation: 2.583543100470502]
	TIME [epoch: 8.34 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419113901571743		[learning rate: 0.0024605]
	Learning Rate: 0.00246051
	LOSS [training: 2.419113901571743 | validation: 2.5590415777203392]
	TIME [epoch: 8.33 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.405855229205769		[learning rate: 0.0024547]
	Learning Rate: 0.00245471
	LOSS [training: 2.405855229205769 | validation: 2.5532158155049482]
	TIME [epoch: 8.33 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3985621135742576		[learning rate: 0.0024489]
	Learning Rate: 0.00244892
	LOSS [training: 2.3985621135742576 | validation: 2.5587998546360935]
	TIME [epoch: 8.35 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4053131745120915		[learning rate: 0.0024431]
	Learning Rate: 0.00244314
	LOSS [training: 2.4053131745120915 | validation: 2.579435610461162]
	TIME [epoch: 8.36 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4394758083093544		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 2.4394758083093544 | validation: 2.617386626734103]
	TIME [epoch: 8.34 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4593116849357552		[learning rate: 0.0024316]
	Learning Rate: 0.00243163
	LOSS [training: 2.4593116849357552 | validation: 2.6907663756070237]
	TIME [epoch: 8.34 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.71715137849728		[learning rate: 0.0024259]
	Learning Rate: 0.00242589
	LOSS [training: 2.71715137849728 | validation: 3.049368443601926]
	TIME [epoch: 8.34 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.782727665561046		[learning rate: 0.0024202]
	Learning Rate: 0.00242017
	LOSS [training: 2.782727665561046 | validation: 2.9765874679798268]
	TIME [epoch: 8.33 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8586497715001538		[learning rate: 0.0024145]
	Learning Rate: 0.00241446
	LOSS [training: 2.8586497715001538 | validation: 3.10205868713476]
	TIME [epoch: 8.38 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.81131473809082		[learning rate: 0.0024088]
	Learning Rate: 0.00240877
	LOSS [training: 2.81131473809082 | validation: 2.984327577628571]
	TIME [epoch: 8.34 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7481400906138362		[learning rate: 0.0024031]
	Learning Rate: 0.00240309
	LOSS [training: 2.7481400906138362 | validation: 2.8046056654621547]
	TIME [epoch: 8.33 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5496038532330036		[learning rate: 0.0023974]
	Learning Rate: 0.00239742
	LOSS [training: 2.5496038532330036 | validation: 2.690626202419271]
	TIME [epoch: 8.34 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.497725370078879		[learning rate: 0.0023918]
	Learning Rate: 0.00239176
	LOSS [training: 2.497725370078879 | validation: 2.719422772905498]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5164744552351888		[learning rate: 0.0023861]
	Learning Rate: 0.00238612
	LOSS [training: 2.5164744552351888 | validation: 2.7002702542589825]
	TIME [epoch: 8.35 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.603624368888388		[learning rate: 0.0023805]
	Learning Rate: 0.00238049
	LOSS [training: 2.603624368888388 | validation: 2.9205659329077616]
	TIME [epoch: 8.39 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6997638239361192		[learning rate: 0.0023749]
	Learning Rate: 0.00237488
	LOSS [training: 2.6997638239361192 | validation: 2.8952105340400873]
	TIME [epoch: 8.33 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6311806552090915		[learning rate: 0.0023693]
	Learning Rate: 0.00236927
	LOSS [training: 2.6311806552090915 | validation: 2.826607020584782]
	TIME [epoch: 8.33 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5535738393886187		[learning rate: 0.0023637]
	Learning Rate: 0.00236369
	LOSS [training: 2.5535738393886187 | validation: 2.715288747806473]
	TIME [epoch: 8.34 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5522311534422477		[learning rate: 0.0023581]
	Learning Rate: 0.00235811
	LOSS [training: 2.5522311534422477 | validation: 2.9691531728052825]
	TIME [epoch: 8.33 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7479774815566342		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 2.7479774815566342 | validation: 2.9412043782999575]
	TIME [epoch: 8.37 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6898102082542534		[learning rate: 0.002347]
	Learning Rate: 0.002347
	LOSS [training: 2.6898102082542534 | validation: 2.8323432259449746]
	TIME [epoch: 8.34 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6023584031996148		[learning rate: 0.0023415]
	Learning Rate: 0.00234146
	LOSS [training: 2.6023584031996148 | validation: 2.742554539401171]
	TIME [epoch: 8.33 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5218193679813576		[learning rate: 0.0023359]
	Learning Rate: 0.00233594
	LOSS [training: 2.5218193679813576 | validation: 2.675906740349699]
	TIME [epoch: 8.33 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4811488977222216		[learning rate: 0.0023304]
	Learning Rate: 0.00233043
	LOSS [training: 2.4811488977222216 | validation: 2.650641992942399]
	TIME [epoch: 8.33 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.473487929460434		[learning rate: 0.0023249]
	Learning Rate: 0.00232493
	LOSS [training: 2.473487929460434 | validation: 2.694802159195047]
	TIME [epoch: 8.33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5126530943483765		[learning rate: 0.0023194]
	Learning Rate: 0.00231945
	LOSS [training: 2.5126530943483765 | validation: 2.673354697983494]
	TIME [epoch: 8.38 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.467183430969624		[learning rate: 0.002314]
	Learning Rate: 0.00231398
	LOSS [training: 2.467183430969624 | validation: 2.6207486541523854]
	TIME [epoch: 8.34 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.440898667068484		[learning rate: 0.0023085]
	Learning Rate: 0.00230852
	LOSS [training: 2.440898667068484 | validation: 2.6120284017157833]
	TIME [epoch: 8.33 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.438513666996581		[learning rate: 0.0023031]
	Learning Rate: 0.00230307
	LOSS [training: 2.438513666996581 | validation: 2.6063630319292095]
	TIME [epoch: 8.33 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.444648276557686		[learning rate: 0.0022976]
	Learning Rate: 0.00229764
	LOSS [training: 2.444648276557686 | validation: 2.605262572044265]
	TIME [epoch: 8.33 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437785257084485		[learning rate: 0.0022922]
	Learning Rate: 0.00229222
	LOSS [training: 2.437785257084485 | validation: 2.6081420345809434]
	TIME [epoch: 8.37 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4437263256806054		[learning rate: 0.0022868]
	Learning Rate: 0.00228681
	LOSS [training: 2.4437263256806054 | validation: 2.60342468496747]
	TIME [epoch: 8.35 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4323644215638405		[learning rate: 0.0022814]
	Learning Rate: 0.00228142
	LOSS [training: 2.4323644215638405 | validation: 2.609787036259438]
	TIME [epoch: 8.33 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.434629059751698		[learning rate: 0.002276]
	Learning Rate: 0.00227604
	LOSS [training: 2.434629059751698 | validation: 2.6195121055099992]
	TIME [epoch: 8.33 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.454222159107837		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 2.454222159107837 | validation: 2.623665793161653]
	TIME [epoch: 8.33 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4536106228866066		[learning rate: 0.0022653]
	Learning Rate: 0.00226531
	LOSS [training: 2.4536106228866066 | validation: 2.6151561476693987]
	TIME [epoch: 8.35 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4464147951921		[learning rate: 0.00226]
	Learning Rate: 0.00225997
	LOSS [training: 2.4464147951921 | validation: 2.6147723080028467]
	TIME [epoch: 8.38 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4427649898151		[learning rate: 0.0022546]
	Learning Rate: 0.00225464
	LOSS [training: 2.4427649898151 | validation: 2.604159391976544]
	TIME [epoch: 8.34 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4368241135018835		[learning rate: 0.0022493]
	Learning Rate: 0.00224932
	LOSS [training: 2.4368241135018835 | validation: 2.6026885316819635]
	TIME [epoch: 8.34 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.459736606587437		[learning rate: 0.002244]
	Learning Rate: 0.00224401
	LOSS [training: 2.459736606587437 | validation: 2.6568247063485666]
	TIME [epoch: 8.34 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4611711067347932		[learning rate: 0.0022387]
	Learning Rate: 0.00223872
	LOSS [training: 2.4611711067347932 | validation: 2.6223270093684077]
	TIME [epoch: 8.32 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4598125796564583		[learning rate: 0.0022334]
	Learning Rate: 0.00223344
	LOSS [training: 2.4598125796564583 | validation: 2.639099418158588]
	TIME [epoch: 8.35 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5324752861632307		[learning rate: 0.0022282]
	Learning Rate: 0.00222817
	LOSS [training: 2.5324752861632307 | validation: 2.632860580600627]
	TIME [epoch: 8.36 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4457934451120042		[learning rate: 0.0022229]
	Learning Rate: 0.00222292
	LOSS [training: 2.4457934451120042 | validation: 2.5930838233127176]
	TIME [epoch: 8.34 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4367710609156013		[learning rate: 0.0022177]
	Learning Rate: 0.00221767
	LOSS [training: 2.4367710609156013 | validation: 2.5951682948829675]
	TIME [epoch: 8.32 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437245170715676		[learning rate: 0.0022124]
	Learning Rate: 0.00221244
	LOSS [training: 2.437245170715676 | validation: 2.598448316581492]
	TIME [epoch: 8.32 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.440595152194499		[learning rate: 0.0022072]
	Learning Rate: 0.00220722
	LOSS [training: 2.440595152194499 | validation: 2.599752437532551]
	TIME [epoch: 8.33 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.439088560217437		[learning rate: 0.002202]
	Learning Rate: 0.00220202
	LOSS [training: 2.439088560217437 | validation: 2.597030794764426]
	TIME [epoch: 8.38 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.439787868819895		[learning rate: 0.0021968]
	Learning Rate: 0.00219682
	LOSS [training: 2.439787868819895 | validation: 2.5984026791369033]
	TIME [epoch: 8.34 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4390370224439915		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 2.4390370224439915 | validation: 2.593523390945431]
	TIME [epoch: 8.33 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.436834836828484		[learning rate: 0.0021865]
	Learning Rate: 0.00218647
	LOSS [training: 2.436834836828484 | validation: 2.5984524007222287]
	TIME [epoch: 8.35 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4364461775928614		[learning rate: 0.0021813]
	Learning Rate: 0.00218131
	LOSS [training: 2.4364461775928614 | validation: 2.606713886068869]
	TIME [epoch: 8.33 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.485886359718479		[learning rate: 0.0021762]
	Learning Rate: 0.00217617
	LOSS [training: 2.485886359718479 | validation: 2.7839768896321333]
	TIME [epoch: 8.35 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5684441164887217		[learning rate: 0.002171]
	Learning Rate: 0.00217103
	LOSS [training: 2.5684441164887217 | validation: 2.6309708284769133]
	TIME [epoch: 8.37 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.487718415482836		[learning rate: 0.0021659]
	Learning Rate: 0.00216591
	LOSS [training: 2.487718415482836 | validation: 2.6523535602511066]
	TIME [epoch: 8.33 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4628707505667107		[learning rate: 0.0021608]
	Learning Rate: 0.0021608
	LOSS [training: 2.4628707505667107 | validation: 2.6072151716428924]
	TIME [epoch: 8.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4530730996353176		[learning rate: 0.0021557]
	Learning Rate: 0.00215571
	LOSS [training: 2.4530730996353176 | validation: 2.6152979827732112]
	TIME [epoch: 8.34 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.448265883157662		[learning rate: 0.0021506]
	Learning Rate: 0.00215062
	LOSS [training: 2.448265883157662 | validation: 2.6056269453383845]
	TIME [epoch: 8.32 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4363931519770206		[learning rate: 0.0021455]
	Learning Rate: 0.00214555
	LOSS [training: 2.4363931519770206 | validation: 2.5988503942618406]
	TIME [epoch: 8.36 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.436771677095687		[learning rate: 0.0021405]
	Learning Rate: 0.00214049
	LOSS [training: 2.436771677095687 | validation: 2.6078453396768895]
	TIME [epoch: 8.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4461059417710205		[learning rate: 0.0021354]
	Learning Rate: 0.00213544
	LOSS [training: 2.4461059417710205 | validation: 2.6210736601087974]
	TIME [epoch: 8.32 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4417012441247987		[learning rate: 0.0021304]
	Learning Rate: 0.0021304
	LOSS [training: 2.4417012441247987 | validation: 2.6042187055354074]
	TIME [epoch: 8.32 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437269746700296		[learning rate: 0.0021254]
	Learning Rate: 0.00212538
	LOSS [training: 2.437269746700296 | validation: 2.6071475209751025]
	TIME [epoch: 8.32 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4381654616577624		[learning rate: 0.0021204]
	Learning Rate: 0.00212036
	LOSS [training: 2.4381654616577624 | validation: 2.6028258187592463]
	TIME [epoch: 8.32 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4425289575081592		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 2.4425289575081592 | validation: 2.6028859216938183]
	TIME [epoch: 8.36 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4452958894320242		[learning rate: 0.0021104]
	Learning Rate: 0.00211037
	LOSS [training: 2.4452958894320242 | validation: 2.609790746385447]
	TIME [epoch: 8.32 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4599357331484852		[learning rate: 0.0021054]
	Learning Rate: 0.00210539
	LOSS [training: 2.4599357331484852 | validation: 2.6207572636766474]
	TIME [epoch: 8.32 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4536773951870776		[learning rate: 0.0021004]
	Learning Rate: 0.00210043
	LOSS [training: 2.4536773951870776 | validation: 2.5943375911512065]
	TIME [epoch: 8.32 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4469610145072966		[learning rate: 0.0020955]
	Learning Rate: 0.00209547
	LOSS [training: 2.4469610145072966 | validation: 2.588933352779136]
	TIME [epoch: 8.31 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.436862549191001		[learning rate: 0.0020905]
	Learning Rate: 0.00209053
	LOSS [training: 2.436862549191001 | validation: 2.6671604165487115]
	TIME [epoch: 8.36 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6835325710982536		[learning rate: 0.0020856]
	Learning Rate: 0.0020856
	LOSS [training: 2.6835325710982536 | validation: 3.0000848233228083]
	TIME [epoch: 8.33 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7987391981338954		[learning rate: 0.0020807]
	Learning Rate: 0.00208068
	LOSS [training: 2.7987391981338954 | validation: 3.086600403388994]
	TIME [epoch: 8.31 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846692376220359		[learning rate: 0.0020758]
	Learning Rate: 0.00207577
	LOSS [training: 2.846692376220359 | validation: 3.089868442017054]
	TIME [epoch: 8.32 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837480681193744		[learning rate: 0.0020709]
	Learning Rate: 0.00207087
	LOSS [training: 2.837480681193744 | validation: 3.0443523104107006]
	TIME [epoch: 8.31 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825867577933601		[learning rate: 0.002066]
	Learning Rate: 0.00206599
	LOSS [training: 2.825867577933601 | validation: 3.1196193024939722]
	TIME [epoch: 8.33 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8892145364849937		[learning rate: 0.0020611]
	Learning Rate: 0.00206112
	LOSS [training: 2.8892145364849937 | validation: 3.077619754035133]
	TIME [epoch: 8.37 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8206003118802006		[learning rate: 0.0020563]
	Learning Rate: 0.00205626
	LOSS [training: 2.8206003118802006 | validation: 3.002891427339205]
	TIME [epoch: 8.33 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7761849385321478		[learning rate: 0.0020514]
	Learning Rate: 0.0020514
	LOSS [training: 2.7761849385321478 | validation: 3.0028708751955264]
	TIME [epoch: 8.33 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8154224517058903		[learning rate: 0.0020466]
	Learning Rate: 0.00204657
	LOSS [training: 2.8154224517058903 | validation: 3.1377994253101487]
	TIME [epoch: 8.31 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.979431630535042		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 2.979431630535042 | validation: 3.294599524033317]
	TIME [epoch: 8.32 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.045447831944462		[learning rate: 0.0020369]
	Learning Rate: 0.00203692
	LOSS [training: 3.045447831944462 | validation: 3.2405055775964713]
	TIME [epoch: 8.34 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0215554355736076		[learning rate: 0.0020321]
	Learning Rate: 0.00203212
	LOSS [training: 3.0215554355736076 | validation: 3.305701821845342]
	TIME [epoch: 8.36 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166678879816524		[learning rate: 0.0020273]
	Learning Rate: 0.00202732
	LOSS [training: 3.166678879816524 | validation: 3.602754043193805]
	TIME [epoch: 8.33 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4170966319099803		[learning rate: 0.0020225]
	Learning Rate: 0.00202254
	LOSS [training: 3.4170966319099803 | validation: 3.6899711600219534]
	TIME [epoch: 8.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.459390628687826		[learning rate: 0.0020178]
	Learning Rate: 0.00201777
	LOSS [training: 3.459390628687826 | validation: 3.633409223229915]
	TIME [epoch: 8.32 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.477196950602292		[learning rate: 0.002013]
	Learning Rate: 0.00201301
	LOSS [training: 3.477196950602292 | validation: 3.7903114399838476]
	TIME [epoch: 8.32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.617406802161334		[learning rate: 0.0020083]
	Learning Rate: 0.00200826
	LOSS [training: 3.617406802161334 | validation: 3.9183774027121334]
	TIME [epoch: 8.37 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7035480699143664		[learning rate: 0.0020035]
	Learning Rate: 0.00200353
	LOSS [training: 3.7035480699143664 | validation: 3.9453371008487284]
	TIME [epoch: 8.32 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.675249469427003		[learning rate: 0.0019988]
	Learning Rate: 0.0019988
	LOSS [training: 3.675249469427003 | validation: 3.862773977591394]
	TIME [epoch: 8.32 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.589982064352965		[learning rate: 0.0019941]
	Learning Rate: 0.00199408
	LOSS [training: 3.589982064352965 | validation: 3.6466925401505437]
	TIME [epoch: 8.31 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.442097138583427		[learning rate: 0.0019894]
	Learning Rate: 0.00198938
	LOSS [training: 3.442097138583427 | validation: 3.764855064139449]
	TIME [epoch: 8.32 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5485441869344863		[learning rate: 0.0019847]
	Learning Rate: 0.00198469
	LOSS [training: 3.5485441869344863 | validation: 3.7402921621134557]
	TIME [epoch: 8.34 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4723424638157065		[learning rate: 0.00198]
	Learning Rate: 0.00198001
	LOSS [training: 3.4723424638157065 | validation: 3.601408580613273]
	TIME [epoch: 8.35 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.367764753291111		[learning rate: 0.0019753]
	Learning Rate: 0.00197534
	LOSS [training: 3.367764753291111 | validation: 3.5339364169580287]
	TIME [epoch: 8.32 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3165275851097915		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 3.3165275851097915 | validation: 3.486204918003148]
	TIME [epoch: 8.32 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261220851102314		[learning rate: 0.001966]
	Learning Rate: 0.00196603
	LOSS [training: 3.261220851102314 | validation: 3.379593823946298]
	TIME [epoch: 8.32 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.176180844527712		[learning rate: 0.0019614]
	Learning Rate: 0.00196139
	LOSS [training: 3.176180844527712 | validation: 3.3630889220101317]
	TIME [epoch: 8.32 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1429330585939987		[learning rate: 0.0019568]
	Learning Rate: 0.00195676
	LOSS [training: 3.1429330585939987 | validation: 3.4084727206008214]
	TIME [epoch: 8.37 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207996695794925		[learning rate: 0.0019521]
	Learning Rate: 0.00195215
	LOSS [training: 3.207996695794925 | validation: 3.4889839286002977]
	TIME [epoch: 8.33 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292480705474963		[learning rate: 0.0019475]
	Learning Rate: 0.00194754
	LOSS [training: 3.292480705474963 | validation: 3.4829429354197297]
	TIME [epoch: 8.33 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.293834083289572		[learning rate: 0.0019429]
	Learning Rate: 0.00194295
	LOSS [training: 3.293834083289572 | validation: 3.4822069373314886]
	TIME [epoch: 8.33 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3072852493252625		[learning rate: 0.0019384]
	Learning Rate: 0.00193837
	LOSS [training: 3.3072852493252625 | validation: 3.590013434817611]
	TIME [epoch: 8.31 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3694377890401306		[learning rate: 0.0019338]
	Learning Rate: 0.00193379
	LOSS [training: 3.3694377890401306 | validation: 3.5815256816338]
	TIME [epoch: 8.32 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.356202268501352		[learning rate: 0.0019292]
	Learning Rate: 0.00192923
	LOSS [training: 3.356202268501352 | validation: 3.605158932604044]
	TIME [epoch: 8.37 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3879316568175017		[learning rate: 0.0019247]
	Learning Rate: 0.00192468
	LOSS [training: 3.3879316568175017 | validation: 3.617835548973367]
	TIME [epoch: 8.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3857058737387007		[learning rate: 0.0019201]
	Learning Rate: 0.00192014
	LOSS [training: 3.3857058737387007 | validation: 3.642233432660644]
	TIME [epoch: 8.32 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3511074030347165		[learning rate: 0.0019156]
	Learning Rate: 0.00191561
	LOSS [training: 3.3511074030347165 | validation: 3.497311417079743]
	TIME [epoch: 8.32 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.267996349824176		[learning rate: 0.0019111]
	Learning Rate: 0.00191109
	LOSS [training: 3.267996349824176 | validation: 3.491512174279417]
	TIME [epoch: 8.33 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2427720301245557		[learning rate: 0.0019066]
	Learning Rate: 0.00190659
	LOSS [training: 3.2427720301245557 | validation: 3.4545726335308204]
	TIME [epoch: 8.35 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2453194327957613		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 3.2453194327957613 | validation: 3.468284187317416]
	TIME [epoch: 8.34 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224130051575315		[learning rate: 0.0018976]
	Learning Rate: 0.0018976
	LOSS [training: 3.224130051575315 | validation: 3.379223234994938]
	TIME [epoch: 8.32 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106927159705507		[learning rate: 0.0018931]
	Learning Rate: 0.00189313
	LOSS [training: 3.2106927159705507 | validation: 3.4907907966711607]
	TIME [epoch: 8.32 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3201068733497108		[learning rate: 0.0018887]
	Learning Rate: 0.00188866
	LOSS [training: 3.3201068733497108 | validation: 3.5477791392923903]
	TIME [epoch: 8.32 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3323443331270317		[learning rate: 0.0018842]
	Learning Rate: 0.00188421
	LOSS [training: 3.3323443331270317 | validation: 3.497758790131714]
	TIME [epoch: 8.32 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2670515045092037		[learning rate: 0.0018798]
	Learning Rate: 0.00187976
	LOSS [training: 3.2670515045092037 | validation: 3.4292617661176785]
	TIME [epoch: 8.36 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.168442372575238		[learning rate: 0.0018753]
	Learning Rate: 0.00187533
	LOSS [training: 3.168442372575238 | validation: 3.3472194141501923]
	TIME [epoch: 8.32 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1069053084960854		[learning rate: 0.0018709]
	Learning Rate: 0.0018709
	LOSS [training: 3.1069053084960854 | validation: 3.3139217898653754]
	TIME [epoch: 8.32 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.055768610117905		[learning rate: 0.0018665]
	Learning Rate: 0.00186649
	LOSS [training: 3.055768610117905 | validation: 3.2680045710058376]
	TIME [epoch: 8.32 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.012304306880693		[learning rate: 0.0018621]
	Learning Rate: 0.00186209
	LOSS [training: 3.012304306880693 | validation: 3.2155392783209678]
	TIME [epoch: 8.31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.958309826617331		[learning rate: 0.0018577]
	Learning Rate: 0.00185769
	LOSS [training: 2.958309826617331 | validation: 3.155255676796436]
	TIME [epoch: 8.35 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9217692887030164		[learning rate: 0.0018533]
	Learning Rate: 0.00185331
	LOSS [training: 2.9217692887030164 | validation: 3.1275256921358654]
	TIME [epoch: 8.33 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8798563697432953		[learning rate: 0.0018489]
	Learning Rate: 0.00184894
	LOSS [training: 2.8798563697432953 | validation: 3.0717764216712347]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8277754872562793		[learning rate: 0.0018446]
	Learning Rate: 0.00184458
	LOSS [training: 2.8277754872562793 | validation: 3.0105385099430046]
	TIME [epoch: 8.32 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7983747287410887		[learning rate: 0.0018402]
	Learning Rate: 0.00184023
	LOSS [training: 2.7983747287410887 | validation: 3.0230026454881562]
	TIME [epoch: 8.31 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.804549348328047		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 2.804549348328047 | validation: 3.042515584079564]
	TIME [epoch: 8.32 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838116080358101		[learning rate: 0.0018316]
	Learning Rate: 0.00183156
	LOSS [training: 2.838116080358101 | validation: 3.062857048878887]
	TIME [epoch: 8.36 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854460279988988		[learning rate: 0.0018272]
	Learning Rate: 0.00182724
	LOSS [training: 2.854460279988988 | validation: 3.0667003725847968]
	TIME [epoch: 8.32 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837963185790482		[learning rate: 0.0018229]
	Learning Rate: 0.00182293
	LOSS [training: 2.837963185790482 | validation: 3.025983406811985]
	TIME [epoch: 8.33 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83271844764434		[learning rate: 0.0018186]
	Learning Rate: 0.00181863
	LOSS [training: 2.83271844764434 | validation: 3.062393677574021]
	TIME [epoch: 8.31 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.872022562247949		[learning rate: 0.0018143]
	Learning Rate: 0.00181434
	LOSS [training: 2.872022562247949 | validation: 3.1011731923984103]
	TIME [epoch: 8.32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9062735301061626		[learning rate: 0.0018101]
	Learning Rate: 0.00181006
	LOSS [training: 2.9062735301061626 | validation: 3.089462874902319]
	TIME [epoch: 8.33 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8646992289448203		[learning rate: 0.0018058]
	Learning Rate: 0.00180579
	LOSS [training: 2.8646992289448203 | validation: 3.025618887921647]
	TIME [epoch: 8.35 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.802749970815129		[learning rate: 0.0018015]
	Learning Rate: 0.00180153
	LOSS [training: 2.802749970815129 | validation: 2.9901466140181974]
	TIME [epoch: 8.32 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.798990160880805		[learning rate: 0.0017973]
	Learning Rate: 0.00179728
	LOSS [training: 2.798990160880805 | validation: 2.9992769625454283]
	TIME [epoch: 8.32 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7789874609592875		[learning rate: 0.001793]
	Learning Rate: 0.00179304
	LOSS [training: 2.7789874609592875 | validation: 2.9458496837074506]
	TIME [epoch: 8.31 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7464904868939177		[learning rate: 0.0017888]
	Learning Rate: 0.00178881
	LOSS [training: 2.7464904868939177 | validation: 2.9564813401027568]
	TIME [epoch: 8.31 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7220505044149492		[learning rate: 0.0017846]
	Learning Rate: 0.00178459
	LOSS [training: 2.7220505044149492 | validation: 2.9064288809222028]
	TIME [epoch: 8.37 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.689482940269137		[learning rate: 0.0017804]
	Learning Rate: 0.00178038
	LOSS [training: 2.689482940269137 | validation: 2.8743792768915863]
	TIME [epoch: 8.32 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.67952473379018		[learning rate: 0.0017762]
	Learning Rate: 0.00177618
	LOSS [training: 2.67952473379018 | validation: 2.9038701181805173]
	TIME [epoch: 8.31 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.716087863246324		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 2.716087863246324 | validation: 2.937826444807719]
	TIME [epoch: 8.31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7489199152048363		[learning rate: 0.0017678]
	Learning Rate: 0.00176781
	LOSS [training: 2.7489199152048363 | validation: 2.996705139108746]
	TIME [epoch: 8.32 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.802335559310513		[learning rate: 0.0017636]
	Learning Rate: 0.00176364
	LOSS [training: 2.802335559310513 | validation: 3.03375929416435]
	TIME [epoch: 8.33 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823985402099601		[learning rate: 0.0017595]
	Learning Rate: 0.00175948
	LOSS [training: 2.823985402099601 | validation: 3.046668949785052]
	TIME [epoch: 8.36 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8556169294333724		[learning rate: 0.0017553]
	Learning Rate: 0.00175533
	LOSS [training: 2.8556169294333724 | validation: 3.0809819145811765]
	TIME [epoch: 8.32 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8973925879739144		[learning rate: 0.0017512]
	Learning Rate: 0.00175119
	LOSS [training: 2.8973925879739144 | validation: 3.0999542067233703]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8872365561730646		[learning rate: 0.0017471]
	Learning Rate: 0.00174706
	LOSS [training: 2.8872365561730646 | validation: 3.115454038390763]
	TIME [epoch: 8.31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.910494326615551		[learning rate: 0.0017429]
	Learning Rate: 0.00174294
	LOSS [training: 2.910494326615551 | validation: 3.1302197314522324]
	TIME [epoch: 8.31 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9128246715643407		[learning rate: 0.0017388]
	Learning Rate: 0.00173883
	LOSS [training: 2.9128246715643407 | validation: 3.1278411486359956]
	TIME [epoch: 8.35 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.926867161429995		[learning rate: 0.0017347]
	Learning Rate: 0.00173473
	LOSS [training: 2.926867161429995 | validation: 3.145189016092389]
	TIME [epoch: 8.34 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.934557910986117		[learning rate: 0.0017306]
	Learning Rate: 0.00173063
	LOSS [training: 2.934557910986117 | validation: 3.1562635820102134]
	TIME [epoch: 8.32 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.958576588013738		[learning rate: 0.0017266]
	Learning Rate: 0.00172655
	LOSS [training: 2.958576588013738 | validation: 3.189127801340585]
	TIME [epoch: 8.32 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9752657620590472		[learning rate: 0.0017225]
	Learning Rate: 0.00172248
	LOSS [training: 2.9752657620590472 | validation: 3.1835900898997154]
	TIME [epoch: 8.31 sec]
EPOCH 796/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 2.970839438393788		[learning rate: 0.0017184]
	Learning Rate: 0.00171842
	LOSS [training: 2.970839438393788 | validation: 3.1486413634435273]
	TIME [epoch: 105 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9829006661749338		[learning rate: 0.0017144]
	Learning Rate: 0.00171436
	LOSS [training: 2.9829006661749338 | validation: 3.1529626712079883]
	TIME [epoch: 8.36 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9952472223604127		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 2.9952472223604127 | validation: 3.1576019077915487]
	TIME [epoch: 8.34 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.989547255384797		[learning rate: 0.0017063]
	Learning Rate: 0.00170628
	LOSS [training: 2.989547255384797 | validation: 3.148962887797464]
	TIME [epoch: 8.33 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9797380103384032		[learning rate: 0.0017023]
	Learning Rate: 0.00170226
	LOSS [training: 2.9797380103384032 | validation: 3.115381461972948]
	TIME [epoch: 8.36 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9585742575577667		[learning rate: 0.0016982]
	Learning Rate: 0.00169824
	LOSS [training: 2.9585742575577667 | validation: 3.1001905520443507]
	TIME [epoch: 8.36 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.944630454838463		[learning rate: 0.0016942]
	Learning Rate: 0.00169424
	LOSS [training: 2.944630454838463 | validation: 3.1001476207272742]
	TIME [epoch: 8.34 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9433250263716744		[learning rate: 0.0016902]
	Learning Rate: 0.00169024
	LOSS [training: 2.9433250263716744 | validation: 3.08985636655715]
	TIME [epoch: 8.33 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.934575138944118		[learning rate: 0.0016863]
	Learning Rate: 0.00168625
	LOSS [training: 2.934575138944118 | validation: 3.0804388182327918]
	TIME [epoch: 8.34 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.931684352097367		[learning rate: 0.0016823]
	Learning Rate: 0.00168228
	LOSS [training: 2.931684352097367 | validation: 3.0968926856490797]
	TIME [epoch: 8.34 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9413600182970208		[learning rate: 0.0016783]
	Learning Rate: 0.00167831
	LOSS [training: 2.9413600182970208 | validation: 3.0871223815522653]
	TIME [epoch: 8.38 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9290483333085966		[learning rate: 0.0016743]
	Learning Rate: 0.00167435
	LOSS [training: 2.9290483333085966 | validation: 3.074484075081653]
	TIME [epoch: 8.34 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9096781412644885		[learning rate: 0.0016704]
	Learning Rate: 0.0016704
	LOSS [training: 2.9096781412644885 | validation: 3.070035514456824]
	TIME [epoch: 8.34 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9203110987674834		[learning rate: 0.0016665]
	Learning Rate: 0.00166646
	LOSS [training: 2.9203110987674834 | validation: 3.0917285547767097]
	TIME [epoch: 8.34 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9371782152551402		[learning rate: 0.0016625]
	Learning Rate: 0.00166253
	LOSS [training: 2.9371782152551402 | validation: 3.0950835072436194]
	TIME [epoch: 8.33 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9326706961344557		[learning rate: 0.0016586]
	Learning Rate: 0.00165861
	LOSS [training: 2.9326706961344557 | validation: 3.0944932533236944]
	TIME [epoch: 8.34 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9244961058630796		[learning rate: 0.0016547]
	Learning Rate: 0.0016547
	LOSS [training: 2.9244961058630796 | validation: 3.0786589558899897]
	TIME [epoch: 8.37 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9165255691656142		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 2.9165255691656142 | validation: 3.071875101868139]
	TIME [epoch: 8.33 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.913181471026336		[learning rate: 0.0016469]
	Learning Rate: 0.0016469
	LOSS [training: 2.913181471026336 | validation: 3.0590865130705347]
	TIME [epoch: 8.33 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8981255396184746		[learning rate: 0.001643]
	Learning Rate: 0.00164301
	LOSS [training: 2.8981255396184746 | validation: 3.051690911719992]
	TIME [epoch: 8.34 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.893183660473808		[learning rate: 0.0016391]
	Learning Rate: 0.00163914
	LOSS [training: 2.893183660473808 | validation: 3.0536601292911727]
	TIME [epoch: 8.33 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8957609270241047		[learning rate: 0.0016353]
	Learning Rate: 0.00163527
	LOSS [training: 2.8957609270241047 | validation: 3.0500763549960928]
	TIME [epoch: 8.36 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8878377893364973		[learning rate: 0.0016314]
	Learning Rate: 0.00163141
	LOSS [training: 2.8878377893364973 | validation: 3.0472894794162806]
	TIME [epoch: 8.36 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8958271327169482		[learning rate: 0.0016276]
	Learning Rate: 0.00162757
	LOSS [training: 2.8958271327169482 | validation: 3.0617691839433494]
	TIME [epoch: 8.33 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.910839769162987		[learning rate: 0.0016237]
	Learning Rate: 0.00162373
	LOSS [training: 2.910839769162987 | validation: 3.0739837521161073]
	TIME [epoch: 8.33 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9184901340924982		[learning rate: 0.0016199]
	Learning Rate: 0.0016199
	LOSS [training: 2.9184901340924982 | validation: 3.0772121485731456]
	TIME [epoch: 8.33 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.920974113415947		[learning rate: 0.0016161]
	Learning Rate: 0.00161608
	LOSS [training: 2.920974113415947 | validation: 3.081396270640239]
	TIME [epoch: 8.34 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9250326757267424		[learning rate: 0.0016123]
	Learning Rate: 0.00161226
	LOSS [training: 2.9250326757267424 | validation: 3.08134741092522]
	TIME [epoch: 8.36 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9198153494680366		[learning rate: 0.0016085]
	Learning Rate: 0.00160846
	LOSS [training: 2.9198153494680366 | validation: 3.0786880968011205]
	TIME [epoch: 8.35 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.921819795765062		[learning rate: 0.0016047]
	Learning Rate: 0.00160467
	LOSS [training: 2.921819795765062 | validation: 3.0853230474658355]
	TIME [epoch: 8.33 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9322218471574253		[learning rate: 0.0016009]
	Learning Rate: 0.00160088
	LOSS [training: 2.9322218471574253 | validation: 3.0868628673388354]
	TIME [epoch: 8.34 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.928629503550626		[learning rate: 0.0015971]
	Learning Rate: 0.0015971
	LOSS [training: 2.928629503550626 | validation: 3.0824322815639693]
	TIME [epoch: 8.34 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9227300011629507		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 2.9227300011629507 | validation: 3.0783286867582578]
	TIME [epoch: 8.34 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9206576964137954		[learning rate: 0.0015896]
	Learning Rate: 0.00158958
	LOSS [training: 2.9206576964137954 | validation: 3.076533254775016]
	TIME [epoch: 8.38 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.919730976063599		[learning rate: 0.0015858]
	Learning Rate: 0.00158583
	LOSS [training: 2.919730976063599 | validation: 3.0762516497268146]
	TIME [epoch: 8.34 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9207566990630682		[learning rate: 0.0015821]
	Learning Rate: 0.00158209
	LOSS [training: 2.9207566990630682 | validation: 3.077159318958364]
	TIME [epoch: 8.34 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.91786422427263		[learning rate: 0.0015784]
	Learning Rate: 0.00157836
	LOSS [training: 2.91786422427263 | validation: 3.0764515397702055]
	TIME [epoch: 8.33 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9139725651366737		[learning rate: 0.0015746]
	Learning Rate: 0.00157463
	LOSS [training: 2.9139725651366737 | validation: 3.070771067749961]
	TIME [epoch: 8.33 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.918478883732468		[learning rate: 0.0015709]
	Learning Rate: 0.00157092
	LOSS [training: 2.918478883732468 | validation: 3.078865642974212]
	TIME [epoch: 8.35 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9211652733544544		[learning rate: 0.0015672]
	Learning Rate: 0.00156721
	LOSS [training: 2.9211652733544544 | validation: 3.0787696929815302]
	TIME [epoch: 8.36 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.921207068668015		[learning rate: 0.0015635]
	Learning Rate: 0.00156352
	LOSS [training: 2.921207068668015 | validation: 3.083670078196705]
	TIME [epoch: 8.34 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9283976023107847		[learning rate: 0.0015598]
	Learning Rate: 0.00155983
	LOSS [training: 2.9283976023107847 | validation: 3.090452650730935]
	TIME [epoch: 8.34 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9358484990424256		[learning rate: 0.0015561]
	Learning Rate: 0.00155615
	LOSS [training: 2.9358484990424256 | validation: 3.094998038709119]
	TIME [epoch: 8.33 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9312206828277096		[learning rate: 0.0015525]
	Learning Rate: 0.00155248
	LOSS [training: 2.9312206828277096 | validation: 3.084724726488255]
	TIME [epoch: 8.33 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.924785088010265		[learning rate: 0.0015488]
	Learning Rate: 0.00154882
	LOSS [training: 2.924785088010265 | validation: 3.0809485342496172]
	TIME [epoch: 8.36 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9250869910162507		[learning rate: 0.0015452]
	Learning Rate: 0.00154516
	LOSS [training: 2.9250869910162507 | validation: 3.0865502934524076]
	TIME [epoch: 8.36 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.929817115931585		[learning rate: 0.0015415]
	Learning Rate: 0.00154152
	LOSS [training: 2.929817115931585 | validation: 3.088797534113906]
	TIME [epoch: 8.34 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9247087106314886		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 2.9247087106314886 | validation: 3.083554133690304]
	TIME [epoch: 8.33 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.920564975753476		[learning rate: 0.0015343]
	Learning Rate: 0.00153425
	LOSS [training: 2.920564975753476 | validation: 3.075837390146605]
	TIME [epoch: 8.34 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9122571579502554		[learning rate: 0.0015306]
	Learning Rate: 0.00153064
	LOSS [training: 2.9122571579502554 | validation: 3.076548679133123]
	TIME [epoch: 8.33 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9100962877090004		[learning rate: 0.001527]
	Learning Rate: 0.00152703
	LOSS [training: 2.9100962877090004 | validation: 3.053755417811745]
	TIME [epoch: 8.38 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8957582986583406		[learning rate: 0.0015234]
	Learning Rate: 0.00152342
	LOSS [training: 2.8957582986583406 | validation: 3.0449146755062935]
	TIME [epoch: 8.34 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.887398526186759		[learning rate: 0.0015198]
	Learning Rate: 0.00151983
	LOSS [training: 2.887398526186759 | validation: 3.0507270661280694]
	TIME [epoch: 8.33 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8957414241296346		[learning rate: 0.0015162]
	Learning Rate: 0.00151624
	LOSS [training: 2.8957414241296346 | validation: 3.062654566726157]
	TIME [epoch: 8.33 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9046235909710103		[learning rate: 0.0015127]
	Learning Rate: 0.00151267
	LOSS [training: 2.9046235909710103 | validation: 3.061420215611898]
	TIME [epoch: 8.33 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9032088516767582		[learning rate: 0.0015091]
	Learning Rate: 0.0015091
	LOSS [training: 2.9032088516767582 | validation: 3.0656894978797093]
	TIME [epoch: 8.35 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.907991243556767		[learning rate: 0.0015055]
	Learning Rate: 0.00150554
	LOSS [training: 2.907991243556767 | validation: 3.06750203092153]
	TIME [epoch: 8.37 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9151426316356153		[learning rate: 0.001502]
	Learning Rate: 0.00150199
	LOSS [training: 2.9151426316356153 | validation: 3.0871368704960567]
	TIME [epoch: 8.34 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9289580054212077		[learning rate: 0.0014984]
	Learning Rate: 0.00149845
	LOSS [training: 2.9289580054212077 | validation: 3.0942708304852062]
	TIME [epoch: 8.33 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.937952568301368		[learning rate: 0.0014949]
	Learning Rate: 0.00149491
	LOSS [training: 2.937952568301368 | validation: 3.101402157759946]
	TIME [epoch: 8.34 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9442112460147145		[learning rate: 0.0014914]
	Learning Rate: 0.00149139
	LOSS [training: 2.9442112460147145 | validation: 3.1111133496102354]
	TIME [epoch: 8.33 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.942859086163363		[learning rate: 0.0014879]
	Learning Rate: 0.00148787
	LOSS [training: 2.942859086163363 | validation: 3.094429346102886]
	TIME [epoch: 8.35 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.931960660363708		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 2.931960660363708 | validation: 3.0903591673019117]
	TIME [epoch: 8.37 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9261231039344326		[learning rate: 0.0014809]
	Learning Rate: 0.00148086
	LOSS [training: 2.9261231039344326 | validation: 3.085455810383728]
	TIME [epoch: 8.34 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.923950481253481		[learning rate: 0.0014774]
	Learning Rate: 0.00147736
	LOSS [training: 2.923950481253481 | validation: 3.0821121513704486]
	TIME [epoch: 8.33 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.925836071438615		[learning rate: 0.0014739]
	Learning Rate: 0.00147388
	LOSS [training: 2.925836071438615 | validation: 3.0891802804231574]
	TIME [epoch: 8.33 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.926701736283375		[learning rate: 0.0014704]
	Learning Rate: 0.0014704
	LOSS [training: 2.926701736283375 | validation: 3.0825791761604493]
	TIME [epoch: 8.34 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9206110726875796		[learning rate: 0.0014669]
	Learning Rate: 0.00146693
	LOSS [training: 2.9206110726875796 | validation: 3.0804660363940606]
	TIME [epoch: 8.36 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9238305250799574		[learning rate: 0.0014635]
	Learning Rate: 0.00146347
	LOSS [training: 2.9238305250799574 | validation: 3.0844660707290856]
	TIME [epoch: 8.36 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.924427903778982		[learning rate: 0.00146]
	Learning Rate: 0.00146002
	LOSS [training: 2.924427903778982 | validation: 3.08838032535098]
	TIME [epoch: 8.34 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.927290748954188		[learning rate: 0.0014566]
	Learning Rate: 0.00145658
	LOSS [training: 2.927290748954188 | validation: 3.091141280776011]
	TIME [epoch: 8.34 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.931971473049533		[learning rate: 0.0014531]
	Learning Rate: 0.00145314
	LOSS [training: 2.931971473049533 | validation: 3.1006166045825605]
	TIME [epoch: 8.34 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9439129818737286		[learning rate: 0.0014497]
	Learning Rate: 0.00144971
	LOSS [training: 2.9439129818737286 | validation: 3.103188960307567]
	TIME [epoch: 8.33 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9411749989228215		[learning rate: 0.0014463]
	Learning Rate: 0.00144629
	LOSS [training: 2.9411749989228215 | validation: 3.101275422397786]
	TIME [epoch: 8.38 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9480983536595216		[learning rate: 0.0014429]
	Learning Rate: 0.00144288
	LOSS [training: 2.9480983536595216 | validation: 3.112466820103993]
	TIME [epoch: 8.34 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9451766117159845		[learning rate: 0.0014395]
	Learning Rate: 0.00143948
	LOSS [training: 2.9451766117159845 | validation: 3.094874567334301]
	TIME [epoch: 8.33 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.936353737202523		[learning rate: 0.0014361]
	Learning Rate: 0.00143608
	LOSS [training: 2.936353737202523 | validation: 3.101011082946915]
	TIME [epoch: 8.34 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9377574020958703		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 2.9377574020958703 | validation: 3.1014316558612554]
	TIME [epoch: 8.33 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9444160903540353		[learning rate: 0.0014293]
	Learning Rate: 0.00142932
	LOSS [training: 2.9444160903540353 | validation: 3.11173730323324]
	TIME [epoch: 8.34 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.958476345936131		[learning rate: 0.0014259]
	Learning Rate: 0.00142594
	LOSS [training: 2.958476345936131 | validation: 3.1181038410771933]
	TIME [epoch: 8.38 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9674095822989655		[learning rate: 0.0014226]
	Learning Rate: 0.00142258
	LOSS [training: 2.9674095822989655 | validation: 3.130692718781634]
	TIME [epoch: 8.34 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9578454536972414		[learning rate: 0.0014192]
	Learning Rate: 0.00141923
	LOSS [training: 2.9578454536972414 | validation: 3.1092464699121125]
	TIME [epoch: 8.34 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.946899339428416		[learning rate: 0.0014159]
	Learning Rate: 0.00141588
	LOSS [training: 2.946899339428416 | validation: 3.1036910347657596]
	TIME [epoch: 8.34 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9432959887143433		[learning rate: 0.0014125]
	Learning Rate: 0.00141254
	LOSS [training: 2.9432959887143433 | validation: 3.1046003997935374]
	TIME [epoch: 8.34 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.941922644818958		[learning rate: 0.0014092]
	Learning Rate: 0.00140921
	LOSS [training: 2.941922644818958 | validation: 3.1062936305184277]
	TIME [epoch: 8.36 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9479055203746007		[learning rate: 0.0014059]
	Learning Rate: 0.00140588
	LOSS [training: 2.9479055203746007 | validation: 3.1091403735525214]
	TIME [epoch: 8.37 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.95274990724252		[learning rate: 0.0014026]
	Learning Rate: 0.00140257
	LOSS [training: 2.95274990724252 | validation: 3.1107504618397943]
	TIME [epoch: 8.34 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9487483559067234		[learning rate: 0.0013993]
	Learning Rate: 0.00139926
	LOSS [training: 2.9487483559067234 | validation: 3.1095996016322545]
	TIME [epoch: 8.34 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.95336522186183		[learning rate: 0.001396]
	Learning Rate: 0.00139596
	LOSS [training: 2.95336522186183 | validation: 3.1182793036861582]
	TIME [epoch: 8.34 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.96228044826001		[learning rate: 0.0013927]
	Learning Rate: 0.00139266
	LOSS [training: 2.96228044826001 | validation: 3.1332310387696367]
	TIME [epoch: 8.34 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.973522962568756		[learning rate: 0.0013894]
	Learning Rate: 0.00138938
	LOSS [training: 2.973522962568756 | validation: 3.1325084993930474]
	TIME [epoch: 8.36 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9735875197819683		[learning rate: 0.0013861]
	Learning Rate: 0.0013861
	LOSS [training: 2.9735875197819683 | validation: 3.1328307183354385]
	TIME [epoch: 8.34 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9747532638884477		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 2.9747532638884477 | validation: 3.1334555399809565]
	TIME [epoch: 8.33 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.97458439147591		[learning rate: 0.0013796]
	Learning Rate: 0.00137957
	LOSS [training: 2.97458439147591 | validation: 3.1416146721936036]
	TIME [epoch: 8.34 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.974663950843609		[learning rate: 0.0013763]
	Learning Rate: 0.00137632
	LOSS [training: 2.974663950843609 | validation: 3.1320446705193943]
	TIME [epoch: 8.32 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9760654381495084		[learning rate: 0.0013731]
	Learning Rate: 0.00137307
	LOSS [training: 2.9760654381495084 | validation: 3.1444396643189743]
	TIME [epoch: 8.34 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9824760483357204		[learning rate: 0.0013698]
	Learning Rate: 0.00136983
	LOSS [training: 2.9824760483357204 | validation: 3.12773014767229]
	TIME [epoch: 8.37 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.966231890068449		[learning rate: 0.0013666]
	Learning Rate: 0.0013666
	LOSS [training: 2.966231890068449 | validation: 3.1207971496371902]
	TIME [epoch: 8.35 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.956617609794687		[learning rate: 0.0013634]
	Learning Rate: 0.00136338
	LOSS [training: 2.956617609794687 | validation: 3.1073764046690067]
	TIME [epoch: 8.34 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9472250383426672		[learning rate: 0.0013602]
	Learning Rate: 0.00136016
	LOSS [training: 2.9472250383426672 | validation: 3.1134179346004522]
	TIME [epoch: 8.34 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.962457332209776		[learning rate: 0.001357]
	Learning Rate: 0.00135695
	LOSS [training: 2.962457332209776 | validation: 3.130102085044422]
	TIME [epoch: 8.33 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9666206710729757		[learning rate: 0.0013538]
	Learning Rate: 0.00135375
	LOSS [training: 2.9666206710729757 | validation: 3.121695009156674]
	TIME [epoch: 8.35 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9656156470237396		[learning rate: 0.0013506]
	Learning Rate: 0.00135056
	LOSS [training: 2.9656156470237396 | validation: 3.12870496421791]
	TIME [epoch: 8.38 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9731830207023036		[learning rate: 0.0013474]
	Learning Rate: 0.00134737
	LOSS [training: 2.9731830207023036 | validation: 3.1497658413442773]
	TIME [epoch: 8.34 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.997071888643812		[learning rate: 0.0013442]
	Learning Rate: 0.00134419
	LOSS [training: 2.997071888643812 | validation: 3.1662000174784843]
	TIME [epoch: 8.34 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.996044716175882		[learning rate: 0.001341]
	Learning Rate: 0.00134102
	LOSS [training: 2.996044716175882 | validation: 3.131483484547838]
	TIME [epoch: 8.34 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.961956273029613		[learning rate: 0.0013379]
	Learning Rate: 0.00133786
	LOSS [training: 2.961956273029613 | validation: 3.1118250581145093]
	TIME [epoch: 8.34 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9513343803808665		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 2.9513343803808665 | validation: 3.1184049867490415]
	TIME [epoch: 8.36 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.962899705470716		[learning rate: 0.0013316]
	Learning Rate: 0.00133155
	LOSS [training: 2.962899705470716 | validation: 3.116653585293851]
	TIME [epoch: 8.35 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.965553748481262		[learning rate: 0.0013284]
	Learning Rate: 0.00132841
	LOSS [training: 2.965553748481262 | validation: 3.13488977676446]
	TIME [epoch: 8.34 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.983008850581507		[learning rate: 0.0013253]
	Learning Rate: 0.00132528
	LOSS [training: 2.983008850581507 | validation: 3.1506530182533146]
	TIME [epoch: 8.34 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9947307456798655		[learning rate: 0.0013222]
	Learning Rate: 0.00132215
	LOSS [training: 2.9947307456798655 | validation: 3.1447859792681934]
	TIME [epoch: 8.34 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9810515566419085		[learning rate: 0.001319]
	Learning Rate: 0.00131904
	LOSS [training: 2.9810515566419085 | validation: 3.1558117961631753]
	TIME [epoch: 8.34 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0013005474439702		[learning rate: 0.0013159]
	Learning Rate: 0.00131592
	LOSS [training: 3.0013005474439702 | validation: 3.169720580303509]
	TIME [epoch: 8.37 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.002846792021244		[learning rate: 0.0013128]
	Learning Rate: 0.00131282
	LOSS [training: 3.002846792021244 | validation: 3.163558654917878]
	TIME [epoch: 8.36 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9945691576996216		[learning rate: 0.0013097]
	Learning Rate: 0.00130972
	LOSS [training: 2.9945691576996216 | validation: 3.1291642632451486]
	TIME [epoch: 8.33 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.969670386970222		[learning rate: 0.0013066]
	Learning Rate: 0.00130663
	LOSS [training: 2.969670386970222 | validation: 3.1411647234153586]
	TIME [epoch: 8.34 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9789131460687903		[learning rate: 0.0013036]
	Learning Rate: 0.00130355
	LOSS [training: 2.9789131460687903 | validation: 3.127226288439769]
	TIME [epoch: 8.34 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.958301998753192		[learning rate: 0.0013005]
	Learning Rate: 0.00130048
	LOSS [training: 2.958301998753192 | validation: 3.1163921519806874]
	TIME [epoch: 8.34 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9565855938679886		[learning rate: 0.0012974]
	Learning Rate: 0.00129741
	LOSS [training: 2.9565855938679886 | validation: 3.1303947480948233]
	TIME [epoch: 8.37 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9645981389619545		[learning rate: 0.0012943]
	Learning Rate: 0.00129435
	LOSS [training: 2.9645981389619545 | validation: 3.1142347320826063]
	TIME [epoch: 8.35 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.957471311937308		[learning rate: 0.0012913]
	Learning Rate: 0.0012913
	LOSS [training: 2.957471311937308 | validation: 3.1262481170355088]
	TIME [epoch: 8.34 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.965545124066324		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 2.965545124066324 | validation: 3.1292184985210696]
	TIME [epoch: 8.33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9737547833436393		[learning rate: 0.0012852]
	Learning Rate: 0.00128521
	LOSS [training: 2.9737547833436393 | validation: 3.1353822046525606]
	TIME [epoch: 8.34 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9863459499458873		[learning rate: 0.0012822]
	Learning Rate: 0.00128218
	LOSS [training: 2.9863459499458873 | validation: 3.1624312326933346]
	TIME [epoch: 8.35 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9885587294403653		[learning rate: 0.0012792]
	Learning Rate: 0.00127915
	LOSS [training: 2.9885587294403653 | validation: 3.1388611063824303]
	TIME [epoch: 8.37 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9742069602881194		[learning rate: 0.0012761]
	Learning Rate: 0.00127614
	LOSS [training: 2.9742069602881194 | validation: 3.134203835100391]
	TIME [epoch: 8.34 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.973167692715445		[learning rate: 0.0012731]
	Learning Rate: 0.00127313
	LOSS [training: 2.973167692715445 | validation: 3.1256071489059996]
	TIME [epoch: 8.34 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.971251509023298		[learning rate: 0.0012701]
	Learning Rate: 0.00127012
	LOSS [training: 2.971251509023298 | validation: 3.1328923265752047]
	TIME [epoch: 8.34 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9720653234155194		[learning rate: 0.0012671]
	Learning Rate: 0.00126713
	LOSS [training: 2.9720653234155194 | validation: 3.1249761643477654]
	TIME [epoch: 8.34 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9670424214010884		[learning rate: 0.0012641]
	Learning Rate: 0.00126414
	LOSS [training: 2.9670424214010884 | validation: 3.1357279479159272]
	TIME [epoch: 8.36 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.976485197972324		[learning rate: 0.0012612]
	Learning Rate: 0.00126116
	LOSS [training: 2.976485197972324 | validation: 3.1314477023493286]
	TIME [epoch: 8.35 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.977798976769361		[learning rate: 0.0012582]
	Learning Rate: 0.00125818
	LOSS [training: 2.977798976769361 | validation: 3.147430151372464]
	TIME [epoch: 8.34 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.995390403687878		[learning rate: 0.0012552]
	Learning Rate: 0.00125521
	LOSS [training: 2.995390403687878 | validation: 3.1434251884549864]
	TIME [epoch: 8.32 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9780572052266416		[learning rate: 0.0012523]
	Learning Rate: 0.00125225
	LOSS [training: 2.9780572052266416 | validation: 3.1420858669697314]
	TIME [epoch: 8.33 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0012025798826807		[learning rate: 0.0012493]
	Learning Rate: 0.0012493
	LOSS [training: 3.0012025798826807 | validation: 3.1763446425030417]
	TIME [epoch: 8.34 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.018668478237439		[learning rate: 0.0012464]
	Learning Rate: 0.00124635
	LOSS [training: 3.018668478237439 | validation: 3.173120783501302]
	TIME [epoch: 8.37 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.008986211426202		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 3.008986211426202 | validation: 3.1362157723164867]
	TIME [epoch: 8.35 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.951366683485774		[learning rate: 0.0012405]
	Learning Rate: 0.00124048
	LOSS [training: 2.951366683485774 | validation: 3.0852945945193193]
	TIME [epoch: 8.34 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.924721353354297		[learning rate: 0.0012376]
	Learning Rate: 0.00123755
	LOSS [training: 2.924721353354297 | validation: 3.0796355115403924]
	TIME [epoch: 8.33 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.924323988647573		[learning rate: 0.0012346]
	Learning Rate: 0.00123463
	LOSS [training: 2.924323988647573 | validation: 3.092924445284556]
	TIME [epoch: 8.33 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9464825985730045		[learning rate: 0.0012317]
	Learning Rate: 0.00123172
	LOSS [training: 2.9464825985730045 | validation: 3.1081782841966095]
	TIME [epoch: 8.35 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9539406387089837		[learning rate: 0.0012288]
	Learning Rate: 0.00122882
	LOSS [training: 2.9539406387089837 | validation: 3.1093218065354993]
	TIME [epoch: 8.37 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9512033678130836		[learning rate: 0.0012259]
	Learning Rate: 0.00122592
	LOSS [training: 2.9512033678130836 | validation: 3.099677549862345]
	TIME [epoch: 8.34 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9409798708867303		[learning rate: 0.001223]
	Learning Rate: 0.00122303
	LOSS [training: 2.9409798708867303 | validation: 3.108086280734901]
	TIME [epoch: 8.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9530632929554597		[learning rate: 0.0012201]
	Learning Rate: 0.00122014
	LOSS [training: 2.9530632929554597 | validation: 3.1161606650610327]
	TIME [epoch: 8.33 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9570438665832626		[learning rate: 0.0012173]
	Learning Rate: 0.00121726
	LOSS [training: 2.9570438665832626 | validation: 3.117913262407865]
	TIME [epoch: 8.34 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9566052181310063		[learning rate: 0.0012144]
	Learning Rate: 0.00121439
	LOSS [training: 2.9566052181310063 | validation: 3.1178151030456305]
	TIME [epoch: 8.35 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9542196277105863		[learning rate: 0.0012115]
	Learning Rate: 0.00121153
	LOSS [training: 2.9542196277105863 | validation: 3.108130893064721]
	TIME [epoch: 8.38 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9472801743991357		[learning rate: 0.0012087]
	Learning Rate: 0.00120867
	LOSS [training: 2.9472801743991357 | validation: 3.1090902244637433]
	TIME [epoch: 8.34 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.950237294155373		[learning rate: 0.0012058]
	Learning Rate: 0.00120582
	LOSS [training: 2.950237294155373 | validation: 3.108584829266232]
	TIME [epoch: 8.34 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9520198381381064		[learning rate: 0.001203]
	Learning Rate: 0.00120297
	LOSS [training: 2.9520198381381064 | validation: 3.121693857182976]
	TIME [epoch: 8.33 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9623797080951952		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 2.9623797080951952 | validation: 3.117464119404646]
	TIME [epoch: 8.34 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.961081384297085		[learning rate: 0.0011973]
	Learning Rate: 0.00119731
	LOSS [training: 2.961081384297085 | validation: 3.127011634321299]
	TIME [epoch: 8.36 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.961556363384044		[learning rate: 0.0011945]
	Learning Rate: 0.00119448
	LOSS [training: 2.961556363384044 | validation: 3.106219608025828]
	TIME [epoch: 8.36 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9438886203767867		[learning rate: 0.0011917]
	Learning Rate: 0.00119166
	LOSS [training: 2.9438886203767867 | validation: 3.1059032451404023]
	TIME [epoch: 8.33 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9425009498787813		[learning rate: 0.0011889]
	Learning Rate: 0.00118885
	LOSS [training: 2.9425009498787813 | validation: 3.1020934183435918]
	TIME [epoch: 8.34 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.941984966993912		[learning rate: 0.001186]
	Learning Rate: 0.00118605
	LOSS [training: 2.941984966993912 | validation: 3.098912027552933]
	TIME [epoch: 8.33 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9448032422824904		[learning rate: 0.0011833]
	Learning Rate: 0.00118325
	LOSS [training: 2.9448032422824904 | validation: 3.103266054632325]
	TIME [epoch: 8.34 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.942857096651529		[learning rate: 0.0011805]
	Learning Rate: 0.00118046
	LOSS [training: 2.942857096651529 | validation: 3.097713351553991]
	TIME [epoch: 8.38 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.938904936593488		[learning rate: 0.0011777]
	Learning Rate: 0.00117768
	LOSS [training: 2.938904936593488 | validation: 3.1032386274374124]
	TIME [epoch: 8.35 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9457952543952013		[learning rate: 0.0011749]
	Learning Rate: 0.0011749
	LOSS [training: 2.9457952543952013 | validation: 3.1050906903526543]
	TIME [epoch: 8.33 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9428019860069843		[learning rate: 0.0011721]
	Learning Rate: 0.00117213
	LOSS [training: 2.9428019860069843 | validation: 3.1074270380376525]
	TIME [epoch: 8.34 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9523958053963244		[learning rate: 0.0011694]
	Learning Rate: 0.00116936
	LOSS [training: 2.9523958053963244 | validation: 3.110514633768732]
	TIME [epoch: 8.33 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.957137305525882		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 2.957137305525882 | validation: 3.1179297222031037]
	TIME [epoch: 8.35 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9559814966936866		[learning rate: 0.0011639]
	Learning Rate: 0.00116385
	LOSS [training: 2.9559814966936866 | validation: 3.115781743913157]
	TIME [epoch: 8.37 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.952769557290126		[learning rate: 0.0011611]
	Learning Rate: 0.00116111
	LOSS [training: 2.952769557290126 | validation: 3.1095345461631707]
	TIME [epoch: 8.33 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.947142742986272		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 2.947142742986272 | validation: 3.1078706544770407]
	TIME [epoch: 8.34 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9518585571456426		[learning rate: 0.0011556]
	Learning Rate: 0.00115563
	LOSS [training: 2.9518585571456426 | validation: 3.111058305959742]
	TIME [epoch: 8.34 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9552506444414197		[learning rate: 0.0011529]
	Learning Rate: 0.00115291
	LOSS [training: 2.9552506444414197 | validation: 3.1184277600009045]
	TIME [epoch: 8.32 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.957540568233349		[learning rate: 0.0011502]
	Learning Rate: 0.00115019
	LOSS [training: 2.957540568233349 | validation: 3.11698345303345]
	TIME [epoch: 8.36 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9550390401727085		[learning rate: 0.0011475]
	Learning Rate: 0.00114748
	LOSS [training: 2.9550390401727085 | validation: 3.1172692322561963]
	TIME [epoch: 8.36 sec]
EPOCH 968/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.0010000000000000002
		[batch 4/4] avg loss: 2.939416236368266		[learning rate: 0.0011448]
	Learning Rate: 0.00114477
	LOSS [training: 2.939416236368266 | validation: 3.0802213374271825]
	TIME [epoch: 105 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9989350898625386		[learning rate: 0.0011421]
	Learning Rate: 0.00114207
	LOSS [training: 2.9989350898625386 | validation: 3.09910542069361]
	TIME [epoch: 8.36 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8976322153816696		[learning rate: 0.0011394]
	Learning Rate: 0.00113937
	LOSS [training: 2.8976322153816696 | validation: 3.031575869984161]
	TIME [epoch: 8.36 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.874840739674199		[learning rate: 0.0011367]
	Learning Rate: 0.00113669
	LOSS [training: 2.874840739674199 | validation: 3.029667324262176]
	TIME [epoch: 8.36 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.876551316876014		[learning rate: 0.001134]
	Learning Rate: 0.00113401
	LOSS [training: 2.876551316876014 | validation: 3.055076439298232]
	TIME [epoch: 8.34 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.886915109297459		[learning rate: 0.0011313]
	Learning Rate: 0.00113133
	LOSS [training: 2.886915109297459 | validation: 3.0275419949329674]
	TIME [epoch: 8.33 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.873023667083376		[learning rate: 0.0011287]
	Learning Rate: 0.00112866
	LOSS [training: 2.873023667083376 | validation: 3.0331683501784767]
	TIME [epoch: 8.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.880284053389531		[learning rate: 0.001126]
	Learning Rate: 0.001126
	LOSS [training: 2.880284053389531 | validation: 3.046330510263056]
	TIME [epoch: 8.34 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8971968051751826		[learning rate: 0.0011233]
	Learning Rate: 0.00112334
	LOSS [training: 2.8971968051751826 | validation: 3.064164649988244]
	TIME [epoch: 8.37 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.907378523148401		[learning rate: 0.0011207]
	Learning Rate: 0.00112069
	LOSS [training: 2.907378523148401 | validation: 3.062544753391318]
	TIME [epoch: 8.35 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.906287092394058		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 2.906287092394058 | validation: 3.0633511011677808]
	TIME [epoch: 8.34 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.899184399474251		[learning rate: 0.0011154]
	Learning Rate: 0.00111541
	LOSS [training: 2.899184399474251 | validation: 3.0432419419626706]
	TIME [epoch: 8.33 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.897051713797474		[learning rate: 0.0011128]
	Learning Rate: 0.00111278
	LOSS [training: 2.897051713797474 | validation: 3.068669576288655]
	TIME [epoch: 8.31 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.915006899014542		[learning rate: 0.0011102]
	Learning Rate: 0.00111016
	LOSS [training: 2.915006899014542 | validation: 3.070122252551605]
	TIME [epoch: 8.32 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9110346444648383		[learning rate: 0.0011075]
	Learning Rate: 0.00110754
	LOSS [training: 2.9110346444648383 | validation: 3.0594675645175133]
	TIME [epoch: 8.34 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9031103499660493		[learning rate: 0.0011049]
	Learning Rate: 0.00110493
	LOSS [training: 2.9031103499660493 | validation: 3.0639057399090097]
	TIME [epoch: 8.34 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9082102406099914		[learning rate: 0.0011023]
	Learning Rate: 0.00110232
	LOSS [training: 2.9082102406099914 | validation: 3.0657536609122804]
	TIME [epoch: 8.31 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9109687970491387		[learning rate: 0.0010997]
	Learning Rate: 0.00109972
	LOSS [training: 2.9109687970491387 | validation: 3.058577745704264]
	TIME [epoch: 8.26 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8954525023724553		[learning rate: 0.0010971]
	Learning Rate: 0.00109713
	LOSS [training: 2.8954525023724553 | validation: 3.048473695780064]
	TIME [epoch: 8.27 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.888137083541518		[learning rate: 0.0010945]
	Learning Rate: 0.00109454
	LOSS [training: 2.888137083541518 | validation: 3.0380996026783045]
	TIME [epoch: 8.27 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.883364300561647		[learning rate: 0.001092]
	Learning Rate: 0.00109196
	LOSS [training: 2.883364300561647 | validation: 3.0434608692133622]
	TIME [epoch: 8.31 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8875263825677115		[learning rate: 0.0010894]
	Learning Rate: 0.00108938
	LOSS [training: 2.8875263825677115 | validation: 3.0456972324168126]
	TIME [epoch: 8.31 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.888589570593416		[learning rate: 0.0010868]
	Learning Rate: 0.00108681
	LOSS [training: 2.888589570593416 | validation: 3.0466466266813708]
	TIME [epoch: 8.25 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8895578622098173		[learning rate: 0.0010842]
	Learning Rate: 0.00108425
	LOSS [training: 2.8895578622098173 | validation: 3.050259660318101]
	TIME [epoch: 8.32 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9150400353119372		[learning rate: 0.0010817]
	Learning Rate: 0.00108169
	LOSS [training: 2.9150400353119372 | validation: 3.136122812725911]
	TIME [epoch: 8.28 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.983421932008149		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 2.983421932008149 | validation: 3.138547350891102]
	TIME [epoch: 8.34 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9515984995348363		[learning rate: 0.0010766]
	Learning Rate: 0.00107659
	LOSS [training: 2.9515984995348363 | validation: 3.06404095309259]
	TIME [epoch: 8.34 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9038919830005185		[learning rate: 0.0010741]
	Learning Rate: 0.00107405
	LOSS [training: 2.9038919830005185 | validation: 3.095293210751817]
	TIME [epoch: 8.37 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9467400439287577		[learning rate: 0.0010715]
	Learning Rate: 0.00107152
	LOSS [training: 2.9467400439287577 | validation: 3.12563422108916]
	TIME [epoch: 8.32 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.00583477483515		[learning rate: 0.001069]
	Learning Rate: 0.00106899
	LOSS [training: 3.00583477483515 | validation: 3.150236446839501]
	TIME [epoch: 8.32 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.015143990858359		[learning rate: 0.0010665]
	Learning Rate: 0.00106647
	LOSS [training: 3.015143990858359 | validation: 3.2896848037648416]
	TIME [epoch: 8.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210800030232466		[learning rate: 0.001064]
	Learning Rate: 0.00106395
	LOSS [training: 3.210800030232466 | validation: 3.411433139780761]
	TIME [epoch: 8.34 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.25646738983255		[learning rate: 0.0010614]
	Learning Rate: 0.00106144
	LOSS [training: 3.25646738983255 | validation: 3.360410102760094]
	TIME [epoch: 8.33 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1534511537976826		[learning rate: 0.0010589]
	Learning Rate: 0.00105894
	LOSS [training: 3.1534511537976826 | validation: 3.288578497687411]
	TIME [epoch: 8.38 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.133322625270382		[learning rate: 0.0010564]
	Learning Rate: 0.00105644
	LOSS [training: 3.133322625270382 | validation: 3.289191607828144]
	TIME [epoch: 8.36 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202885928236694		[learning rate: 0.001054]
	Learning Rate: 0.00105395
	LOSS [training: 3.202885928236694 | validation: 3.429275438186557]
	TIME [epoch: 8.33 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.249646042227496		[learning rate: 0.0010515]
	Learning Rate: 0.00105147
	LOSS [training: 3.249646042227496 | validation: 3.3314811677100646]
	TIME [epoch: 8.33 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1851661583569695		[learning rate: 0.001049]
	Learning Rate: 0.00104898
	LOSS [training: 3.1851661583569695 | validation: 3.4472557348290067]
	TIME [epoch: 8.35 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2657655293822825		[learning rate: 0.0010465]
	Learning Rate: 0.00104651
	LOSS [training: 3.2657655293822825 | validation: 3.3713075997899815]
	TIME [epoch: 8.33 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218124741698322		[learning rate: 0.001044]
	Learning Rate: 0.00104404
	LOSS [training: 3.218124741698322 | validation: 3.3702634022499147]
	TIME [epoch: 8.39 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218272334193213		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 3.218272334193213 | validation: 3.360302005608422]
	TIME [epoch: 8.37 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1948622165698373		[learning rate: 0.0010391]
	Learning Rate: 0.00103912
	LOSS [training: 3.1948622165698373 | validation: 3.348901372353504]
	TIME [epoch: 8.34 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1887531748690003		[learning rate: 0.0010367]
	Learning Rate: 0.00103667
	LOSS [training: 3.1887531748690003 | validation: 3.3233354271301714]
	TIME [epoch: 8.35 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1648282269630665		[learning rate: 0.0010342]
	Learning Rate: 0.00103423
	LOSS [training: 3.1648282269630665 | validation: 3.3230387920834934]
	TIME [epoch: 8.35 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.159209122214885		[learning rate: 0.0010318]
	Learning Rate: 0.00103179
	LOSS [training: 3.159209122214885 | validation: 3.32299613653111]
	TIME [epoch: 8.33 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.152524075100518		[learning rate: 0.0010294]
	Learning Rate: 0.00102935
	LOSS [training: 3.152524075100518 | validation: 3.2877796464055185]
	TIME [epoch: 8.38 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1232339532672135		[learning rate: 0.0010269]
	Learning Rate: 0.00102692
	LOSS [training: 3.1232339532672135 | validation: 3.277877756937008]
	TIME [epoch: 8.37 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.113359142134146		[learning rate: 0.0010245]
	Learning Rate: 0.0010245
	LOSS [training: 3.113359142134146 | validation: 3.248432745918336]
	TIME [epoch: 8.33 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1070301064299346		[learning rate: 0.0010221]
	Learning Rate: 0.00102209
	LOSS [training: 3.1070301064299346 | validation: 3.262926695432789]
	TIME [epoch: 8.36 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.106073425233462		[learning rate: 0.0010197]
	Learning Rate: 0.00101967
	LOSS [training: 3.106073425233462 | validation: 3.2679417433575897]
	TIME [epoch: 8.34 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.108054363432276		[learning rate: 0.0010173]
	Learning Rate: 0.00101727
	LOSS [training: 3.108054363432276 | validation: 3.2625131881926457]
	TIME [epoch: 8.35 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1043317614014017		[learning rate: 0.0010149]
	Learning Rate: 0.00101487
	LOSS [training: 3.1043317614014017 | validation: 3.2545371371324476]
	TIME [epoch: 8.37 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0777197185216174		[learning rate: 0.0010125]
	Learning Rate: 0.00101248
	LOSS [training: 3.0777197185216174 | validation: 3.2290505205555977]
	TIME [epoch: 8.39 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0493750974493077		[learning rate: 0.0010101]
	Learning Rate: 0.00101009
	LOSS [training: 3.0493750974493077 | validation: 3.1552341494315934]
	TIME [epoch: 8.42 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.020856348185552		[learning rate: 0.0010077]
	Learning Rate: 0.0010077
	LOSS [training: 3.020856348185552 | validation: 3.2173982220025357]
	TIME [epoch: 8.35 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.053497044569557		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 3.053497044569557 | validation: 3.272511811764141]
	TIME [epoch: 8.34 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.101439327157867		[learning rate: 0.001003]
	Learning Rate: 0.00100296
	LOSS [training: 3.101439327157867 | validation: 3.266529124401746]
	TIME [epoch: 8.35 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1170749645232876		[learning rate: 0.0010006]
	Learning Rate: 0.00100059
	LOSS [training: 3.1170749645232876 | validation: 3.293730732867804]
	TIME [epoch: 8.35 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1366014252213414		[learning rate: 0.00099823]
	Learning Rate: 0.000998231
	LOSS [training: 3.1366014252213414 | validation: 3.2987978453397746]
	TIME [epoch: 8.38 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1134670511461646		[learning rate: 0.00099588]
	Learning Rate: 0.000995876
	LOSS [training: 3.1134670511461646 | validation: 3.2313336842080416]
	TIME [epoch: 8.35 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0912317333152317		[learning rate: 0.00099353]
	Learning Rate: 0.000993527
	LOSS [training: 3.0912317333152317 | validation: 3.263677436243029]
	TIME [epoch: 8.34 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1172445073067596		[learning rate: 0.00099118]
	Learning Rate: 0.000991183
	LOSS [training: 3.1172445073067596 | validation: 3.282396713433111]
	TIME [epoch: 8.34 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1353026926598258		[learning rate: 0.00098885]
	Learning Rate: 0.000988845
	LOSS [training: 3.1353026926598258 | validation: 3.2995912699338024]
	TIME [epoch: 8.33 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1267841981690854		[learning rate: 0.00098651]
	Learning Rate: 0.000986513
	LOSS [training: 3.1267841981690854 | validation: 3.2733915085990297]
	TIME [epoch: 8.35 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1243293315750225		[learning rate: 0.00098419]
	Learning Rate: 0.000984185
	LOSS [training: 3.1243293315750225 | validation: 3.314385702602194]
	TIME [epoch: 8.37 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1398452175210902		[learning rate: 0.00098186]
	Learning Rate: 0.000981864
	LOSS [training: 3.1398452175210902 | validation: 3.2741504746193186]
	TIME [epoch: 8.35 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.116089147901856		[learning rate: 0.00097955]
	Learning Rate: 0.000979548
	LOSS [training: 3.116089147901856 | validation: 3.268405754431394]
	TIME [epoch: 8.34 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1333933974980392		[learning rate: 0.00097724]
	Learning Rate: 0.000977237
	LOSS [training: 3.1333933974980392 | validation: 3.321898269688516]
	TIME [epoch: 8.34 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.142139737785026		[learning rate: 0.00097493]
	Learning Rate: 0.000974932
	LOSS [training: 3.142139737785026 | validation: 3.31474615080955]
	TIME [epoch: 8.36 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1872503492058986		[learning rate: 0.00097263]
	Learning Rate: 0.000972632
	LOSS [training: 3.1872503492058986 | validation: 3.349605228480652]
	TIME [epoch: 8.34 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.089727586181166		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 3.089727586181166 | validation: 3.1068714009957636]
	TIME [epoch: 8.37 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.92702233941138		[learning rate: 0.00096805]
	Learning Rate: 0.000968049
	LOSS [training: 2.92702233941138 | validation: 3.081921040998992]
	TIME [epoch: 8.35 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.919648091325672		[learning rate: 0.00096577]
	Learning Rate: 0.000965766
	LOSS [training: 2.919648091325672 | validation: 3.1536726811243336]
	TIME [epoch: 8.35 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1305232142838433		[learning rate: 0.00096349]
	Learning Rate: 0.000963488
	LOSS [training: 3.1305232142838433 | validation: 3.3626525094362627]
	TIME [epoch: 8.34 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054277135508875		[learning rate: 0.00096121]
	Learning Rate: 0.000961215
	LOSS [training: 3.2054277135508875 | validation: 3.3327186757088185]
	TIME [epoch: 8.34 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.154007496333967		[learning rate: 0.00095895]
	Learning Rate: 0.000958948
	LOSS [training: 3.154007496333967 | validation: 3.2881925757206485]
	TIME [epoch: 8.34 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.162451235509178		[learning rate: 0.00095669]
	Learning Rate: 0.000956686
	LOSS [training: 3.162451235509178 | validation: 3.385566268135486]
	TIME [epoch: 8.36 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2217785081561754		[learning rate: 0.00095443]
	Learning Rate: 0.000954429
	LOSS [training: 3.2217785081561754 | validation: 3.2547390423200033]
	TIME [epoch: 8.36 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.036725552477458		[learning rate: 0.00095218]
	Learning Rate: 0.000952178
	LOSS [training: 3.036725552477458 | validation: 3.0719362781739963]
	TIME [epoch: 8.35 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.901260761845939		[learning rate: 0.00094993]
	Learning Rate: 0.000949932
	LOSS [training: 2.901260761845939 | validation: 3.0366959363514887]
	TIME [epoch: 8.35 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.87496896678227		[learning rate: 0.00094769]
	Learning Rate: 0.000947691
	LOSS [training: 2.87496896678227 | validation: 3.042108579717546]
	TIME [epoch: 8.34 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.888505130397654		[learning rate: 0.00094546]
	Learning Rate: 0.000945455
	LOSS [training: 2.888505130397654 | validation: 3.0280081772856517]
	TIME [epoch: 8.34 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8740866027627967		[learning rate: 0.00094323]
	Learning Rate: 0.000943225
	LOSS [training: 2.8740866027627967 | validation: 3.0291984852318428]
	TIME [epoch: 8.36 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.870398844451471		[learning rate: 0.000941]
	Learning Rate: 0.000941
	LOSS [training: 2.870398844451471 | validation: 3.0233131598753236]
	TIME [epoch: 8.38 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8667211015569984		[learning rate: 0.00093878]
	Learning Rate: 0.000938781
	LOSS [training: 2.8667211015569984 | validation: 3.0178597297870957]
	TIME [epoch: 8.35 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8654232049896713		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 2.8654232049896713 | validation: 3.0243213309945123]
	TIME [epoch: 8.35 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8769115345232814		[learning rate: 0.00093436]
	Learning Rate: 0.000934357
	LOSS [training: 2.8769115345232814 | validation: 3.040117477805427]
	TIME [epoch: 8.36 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.883307736958461		[learning rate: 0.00093215]
	Learning Rate: 0.000932153
	LOSS [training: 2.883307736958461 | validation: 3.0265278585663102]
	TIME [epoch: 8.35 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8696541299599727		[learning rate: 0.00092995]
	Learning Rate: 0.000929954
	LOSS [training: 2.8696541299599727 | validation: 3.031415607391562]
	TIME [epoch: 8.34 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.870774438497826		[learning rate: 0.00092776]
	Learning Rate: 0.000927761
	LOSS [training: 2.870774438497826 | validation: 3.0209740971778034]
	TIME [epoch: 8.39 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.866043802470921		[learning rate: 0.00092557]
	Learning Rate: 0.000925572
	LOSS [training: 2.866043802470921 | validation: 3.0346921807101888]
	TIME [epoch: 8.35 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8870489151988785		[learning rate: 0.00092339]
	Learning Rate: 0.000923389
	LOSS [training: 2.8870489151988785 | validation: 3.0414898968852233]
	TIME [epoch: 8.36 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8973123831660725		[learning rate: 0.00092121]
	Learning Rate: 0.000921211
	LOSS [training: 2.8973123831660725 | validation: 3.0612067410460515]
	TIME [epoch: 8.35 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.923333123881324		[learning rate: 0.00091904]
	Learning Rate: 0.000919038
	LOSS [training: 2.923333123881324 | validation: 3.096091435460349]
	TIME [epoch: 8.35 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9244324672942263		[learning rate: 0.00091687]
	Learning Rate: 0.00091687
	LOSS [training: 2.9244324672942263 | validation: 3.0664282993060423]
	TIME [epoch: 8.36 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.898705164034916		[learning rate: 0.00091471]
	Learning Rate: 0.000914707
	LOSS [training: 2.898705164034916 | validation: 3.029125380723606]
	TIME [epoch: 8.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8707170681893017		[learning rate: 0.00091255]
	Learning Rate: 0.00091255
	LOSS [training: 2.8707170681893017 | validation: 3.0231780495432377]
	TIME [epoch: 8.34 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8633048639637364		[learning rate: 0.0009104]
	Learning Rate: 0.000910397
	LOSS [training: 2.8633048639637364 | validation: 3.016353741437734]
	TIME [epoch: 8.35 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.861335978829782		[learning rate: 0.00090825]
	Learning Rate: 0.00090825
	LOSS [training: 2.861335978829782 | validation: 3.019310640096152]
	TIME [epoch: 8.34 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8618079943768775		[learning rate: 0.00090611]
	Learning Rate: 0.000906107
	LOSS [training: 2.8618079943768775 | validation: 3.0094757819341558]
	TIME [epoch: 8.34 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8567939084535032		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 2.8567939084535032 | validation: 3.015791009795138]
	TIME [epoch: 8.35 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8549009658729325		[learning rate: 0.00090184]
	Learning Rate: 0.000901837
	LOSS [training: 2.8549009658729325 | validation: 3.010157029487755]
	TIME [epoch: 8.39 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8555817220556463		[learning rate: 0.00089971]
	Learning Rate: 0.00089971
	LOSS [training: 2.8555817220556463 | validation: 3.011985041818254]
	TIME [epoch: 8.35 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.856613255897398		[learning rate: 0.00089759]
	Learning Rate: 0.000897588
	LOSS [training: 2.856613255897398 | validation: 3.013273741186859]
	TIME [epoch: 8.35 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8599146597011034		[learning rate: 0.00089547]
	Learning Rate: 0.00089547
	LOSS [training: 2.8599146597011034 | validation: 3.0108685671740645]
	TIME [epoch: 8.35 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853336911209624		[learning rate: 0.00089336]
	Learning Rate: 0.000893358
	LOSS [training: 2.853336911209624 | validation: 2.9950399744125296]
	TIME [epoch: 8.35 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8440665105455394		[learning rate: 0.00089125]
	Learning Rate: 0.000891251
	LOSS [training: 2.8440665105455394 | validation: 3.0049072588762895]
	TIME [epoch: 8.35 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850894860928547		[learning rate: 0.00088915]
	Learning Rate: 0.000889149
	LOSS [training: 2.850894860928547 | validation: 3.003861867315902]
	TIME [epoch: 8.38 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.834855109442288		[learning rate: 0.00088705]
	Learning Rate: 0.000887051
	LOSS [training: 2.834855109442288 | validation: 2.9691878865564414]
	TIME [epoch: 8.36 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828460444369855		[learning rate: 0.00088496]
	Learning Rate: 0.000884959
	LOSS [training: 2.828460444369855 | validation: 3.007920643917781]
	TIME [epoch: 8.34 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.858699459660066		[learning rate: 0.00088287]
	Learning Rate: 0.000882871
	LOSS [training: 2.858699459660066 | validation: 3.0227174280865907]
	TIME [epoch: 8.33 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8647624346803626		[learning rate: 0.00088079]
	Learning Rate: 0.000880789
	LOSS [training: 2.8647624346803626 | validation: 3.013743440345486]
	TIME [epoch: 8.34 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850846695743358		[learning rate: 0.00087871]
	Learning Rate: 0.000878711
	LOSS [training: 2.850846695743358 | validation: 2.9859591030021755]
	TIME [epoch: 8.34 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7987783934186306		[learning rate: 0.00087664]
	Learning Rate: 0.000876639
	LOSS [training: 2.7987783934186306 | validation: 2.9193295545430957]
	TIME [epoch: 8.37 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7598892631874934		[learning rate: 0.00087457]
	Learning Rate: 0.000874571
	LOSS [training: 2.7598892631874934 | validation: 2.9165383380085608]
	TIME [epoch: 8.37 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.761827594245159		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 2.761827594245159 | validation: 2.930442849342189]
	TIME [epoch: 8.33 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.793238727300209		[learning rate: 0.00087045]
	Learning Rate: 0.00087045
	LOSS [training: 2.793238727300209 | validation: 2.9527001648030256]
	TIME [epoch: 8.34 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.78229914495357		[learning rate: 0.0008684]
	Learning Rate: 0.000868396
	LOSS [training: 2.78229914495357 | validation: 2.9156924010914747]
	TIME [epoch: 8.33 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7616372915989196		[learning rate: 0.00086635]
	Learning Rate: 0.000866348
	LOSS [training: 2.7616372915989196 | validation: 2.9246304163697494]
	TIME [epoch: 8.33 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7752179103424814		[learning rate: 0.0008643]
	Learning Rate: 0.000864304
	LOSS [training: 2.7752179103424814 | validation: 2.9384176284527843]
	TIME [epoch: 8.36 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7873830815448617		[learning rate: 0.00086227]
	Learning Rate: 0.000862266
	LOSS [training: 2.7873830815448617 | validation: 2.947926946637731]
	TIME [epoch: 8.37 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.791675640315163		[learning rate: 0.00086023]
	Learning Rate: 0.000860232
	LOSS [training: 2.791675640315163 | validation: 2.949216206829072]
	TIME [epoch: 8.34 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8010051452291815		[learning rate: 0.0008582]
	Learning Rate: 0.000858202
	LOSS [training: 2.8010051452291815 | validation: 2.9607396289931094]
	TIME [epoch: 8.32 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8069415817406043		[learning rate: 0.00085618]
	Learning Rate: 0.000856178
	LOSS [training: 2.8069415817406043 | validation: 2.9581948694089504]
	TIME [epoch: 8.34 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8018895461912856		[learning rate: 0.00085416]
	Learning Rate: 0.000854159
	LOSS [training: 2.8018895461912856 | validation: 2.955466268498378]
	TIME [epoch: 8.34 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7985112621941344		[learning rate: 0.00085214]
	Learning Rate: 0.000852144
	LOSS [training: 2.7985112621941344 | validation: 2.9538627258789676]
	TIME [epoch: 8.35 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.804442596436828		[learning rate: 0.00085013]
	Learning Rate: 0.000850134
	LOSS [training: 2.804442596436828 | validation: 2.972038967326391]
	TIME [epoch: 8.39 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8217728492409604		[learning rate: 0.00084813]
	Learning Rate: 0.000848128
	LOSS [training: 2.8217728492409604 | validation: 2.9818650639848467]
	TIME [epoch: 8.34 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82953736799418		[learning rate: 0.00084613]
	Learning Rate: 0.000846128
	LOSS [training: 2.82953736799418 | validation: 2.9858033471548957]
	TIME [epoch: 8.34 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8325761209768983		[learning rate: 0.00084413]
	Learning Rate: 0.000844132
	LOSS [training: 2.8325761209768983 | validation: 2.986591594605776]
	TIME [epoch: 8.34 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8316413533022087		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 2.8316413533022087 | validation: 2.986494539619355]
	TIME [epoch: 8.34 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8341752573656294		[learning rate: 0.00084015]
	Learning Rate: 0.000840154
	LOSS [training: 2.8341752573656294 | validation: 3.0010958611362932]
	TIME [epoch: 8.34 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8457885308265576		[learning rate: 0.00083817]
	Learning Rate: 0.000838173
	LOSS [training: 2.8457885308265576 | validation: 2.9996573252841516]
	TIME [epoch: 8.37 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847006552294623		[learning rate: 0.0008362]
	Learning Rate: 0.000836195
	LOSS [training: 2.847006552294623 | validation: 2.997991750774034]
	TIME [epoch: 8.34 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842354212064398		[learning rate: 0.00083422]
	Learning Rate: 0.000834223
	LOSS [training: 2.842354212064398 | validation: 3.0003132514392314]
	TIME [epoch: 8.33 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850890139072251		[learning rate: 0.00083226]
	Learning Rate: 0.000832255
	LOSS [training: 2.850890139072251 | validation: 3.003012137628879]
	TIME [epoch: 8.33 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8449198263323074		[learning rate: 0.00083029]
	Learning Rate: 0.000830292
	LOSS [training: 2.8449198263323074 | validation: 2.993698457815384]
	TIME [epoch: 8.33 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.836554787277852		[learning rate: 0.00082833]
	Learning Rate: 0.000828333
	LOSS [training: 2.836554787277852 | validation: 2.985633044014058]
	TIME [epoch: 8.34 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831331577168672		[learning rate: 0.00082638]
	Learning Rate: 0.00082638
	LOSS [training: 2.831331577168672 | validation: 2.9873884093958383]
	TIME [epoch: 8.37 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832827040331397		[learning rate: 0.00082443]
	Learning Rate: 0.00082443
	LOSS [training: 2.832827040331397 | validation: 2.989124153182409]
	TIME [epoch: 8.32 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83509285875222		[learning rate: 0.00082249]
	Learning Rate: 0.000822485
	LOSS [training: 2.83509285875222 | validation: 2.9946515963906473]
	TIME [epoch: 8.32 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8450386720954945		[learning rate: 0.00082055]
	Learning Rate: 0.000820545
	LOSS [training: 2.8450386720954945 | validation: 3.000800131876294]
	TIME [epoch: 8.33 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.845907436393791		[learning rate: 0.00081861]
	Learning Rate: 0.00081861
	LOSS [training: 2.845907436393791 | validation: 3.006060696380953]
	TIME [epoch: 8.33 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503713738480165		[learning rate: 0.00081668]
	Learning Rate: 0.000816679
	LOSS [training: 2.8503713738480165 | validation: 3.0052352926893864]
	TIME [epoch: 8.33 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8467709021167846		[learning rate: 0.00081475]
	Learning Rate: 0.000814752
	LOSS [training: 2.8467709021167846 | validation: 3.0011536801870893]
	TIME [epoch: 8.37 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8445242996249327		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 2.8445242996249327 | validation: 3.0033726224624835]
	TIME [epoch: 8.35 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854937838330344		[learning rate: 0.00081091]
	Learning Rate: 0.000810913
	LOSS [training: 2.854937838330344 | validation: 3.017902339432256]
	TIME [epoch: 8.33 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8635970317467834		[learning rate: 0.000809]
	Learning Rate: 0.000809
	LOSS [training: 2.8635970317467834 | validation: 3.0197658088773487]
	TIME [epoch: 8.34 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8626775044524893		[learning rate: 0.00080709]
	Learning Rate: 0.000807092
	LOSS [training: 2.8626775044524893 | validation: 3.017835022468817]
	TIME [epoch: 8.34 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8629436165667332		[learning rate: 0.00080519]
	Learning Rate: 0.000805188
	LOSS [training: 2.8629436165667332 | validation: 3.0175528874651363]
	TIME [epoch: 8.34 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8609330446894186		[learning rate: 0.00080329]
	Learning Rate: 0.000803289
	LOSS [training: 2.8609330446894186 | validation: 3.014681791691258]
	TIME [epoch: 8.36 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8632359391266595		[learning rate: 0.00080139]
	Learning Rate: 0.000801394
	LOSS [training: 2.8632359391266595 | validation: 3.018079846759355]
	TIME [epoch: 8.38 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8633617126761113		[learning rate: 0.0007995]
	Learning Rate: 0.000799504
	LOSS [training: 2.8633617126761113 | validation: 3.0193021125585338]
	TIME [epoch: 8.33 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.864761620887348		[learning rate: 0.00079762]
	Learning Rate: 0.000797618
	LOSS [training: 2.864761620887348 | validation: 3.0185986818618122]
	TIME [epoch: 8.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.86086570158644		[learning rate: 0.00079574]
	Learning Rate: 0.000795736
	LOSS [training: 2.86086570158644 | validation: 3.012763537637235]
	TIME [epoch: 8.33 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.858559091890527		[learning rate: 0.00079386]
	Learning Rate: 0.000793859
	LOSS [training: 2.858559091890527 | validation: 3.0176835142942156]
	TIME [epoch: 8.33 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.857487175811149		[learning rate: 0.00079199]
	Learning Rate: 0.000791987
	LOSS [training: 2.857487175811149 | validation: 3.0101637577303544]
	TIME [epoch: 8.34 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854436596617489		[learning rate: 0.00079012]
	Learning Rate: 0.000790119
	LOSS [training: 2.854436596617489 | validation: 3.009507274632015]
	TIME [epoch: 8.37 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8555006650394805		[learning rate: 0.00078826]
	Learning Rate: 0.000788255
	LOSS [training: 2.8555006650394805 | validation: 3.007524397885549]
	TIME [epoch: 8.33 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8542610339549688		[learning rate: 0.0007864]
	Learning Rate: 0.000786396
	LOSS [training: 2.8542610339549688 | validation: 3.0118627694372098]
	TIME [epoch: 8.33 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8574888753740266		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 2.8574888753740266 | validation: 3.0145503677094205]
	TIME [epoch: 8.33 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8551717727169708		[learning rate: 0.00078269]
	Learning Rate: 0.00078269
	LOSS [training: 2.8551717727169708 | validation: 3.00470876569594]
	TIME [epoch: 8.32 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8457273604290423		[learning rate: 0.00078084]
	Learning Rate: 0.000780844
	LOSS [training: 2.8457273604290423 | validation: 2.9944835848461784]
	TIME [epoch: 8.34 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8405773512683394		[learning rate: 0.000779]
	Learning Rate: 0.000779002
	LOSS [training: 2.8405773512683394 | validation: 2.9888433670494403]
	TIME [epoch: 8.36 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8346755490028057		[learning rate: 0.00077716]
	Learning Rate: 0.000777164
	LOSS [training: 2.8346755490028057 | validation: 2.9914949482490867]
	TIME [epoch: 8.33 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8343698991261377		[learning rate: 0.00077533]
	Learning Rate: 0.000775331
	LOSS [training: 2.8343698991261377 | validation: 2.9847266941439576]
	TIME [epoch: 8.33 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8255441731948228		[learning rate: 0.0007735]
	Learning Rate: 0.000773502
	LOSS [training: 2.8255441731948228 | validation: 2.978118885031532]
	TIME [epoch: 8.33 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8232108611901703		[learning rate: 0.00077168]
	Learning Rate: 0.000771678
	LOSS [training: 2.8232108611901703 | validation: 2.9754056983350603]
	TIME [epoch: 8.32 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.819403874511634		[learning rate: 0.00076986]
	Learning Rate: 0.000769857
	LOSS [training: 2.819403874511634 | validation: 2.96608270109382]
	TIME [epoch: 8.33 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.811087220587762		[learning rate: 0.00076804]
	Learning Rate: 0.000768041
	LOSS [training: 2.811087220587762 | validation: 2.962114899195421]
	TIME [epoch: 8.37 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8096253067087096		[learning rate: 0.00076623]
	Learning Rate: 0.00076623
	LOSS [training: 2.8096253067087096 | validation: 2.9687492488869056]
	TIME [epoch: 8.34 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.820000915605402		[learning rate: 0.00076442]
	Learning Rate: 0.000764422
	LOSS [training: 2.820000915605402 | validation: 2.984213860362413]
	TIME [epoch: 8.33 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8344359066414215		[learning rate: 0.00076262]
	Learning Rate: 0.000762619
	LOSS [training: 2.8344359066414215 | validation: 2.989554716693297]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83374123514545		[learning rate: 0.00076082]
	Learning Rate: 0.00076082
	LOSS [training: 2.83374123514545 | validation: 2.9900538245734865]
	TIME [epoch: 8.32 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8349637881233503		[learning rate: 0.00075903]
	Learning Rate: 0.000759026
	LOSS [training: 2.8349637881233503 | validation: 2.9965649709571647]
	TIME [epoch: 8.32 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841408534098824		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 2.841408534098824 | validation: 2.9984071901804974]
	TIME [epoch: 8.36 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844370518231414		[learning rate: 0.00075545]
	Learning Rate: 0.000755449
	LOSS [training: 2.844370518231414 | validation: 2.99976058602497]
	TIME [epoch: 8.34 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847985625334957		[learning rate: 0.00075367]
	Learning Rate: 0.000753667
	LOSS [training: 2.847985625334957 | validation: 2.998229480153129]
	TIME [epoch: 8.33 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8477874236445575		[learning rate: 0.00075189]
	Learning Rate: 0.000751889
	LOSS [training: 2.8477874236445575 | validation: 3.0068216156687124]
	TIME [epoch: 8.33 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850209230937569		[learning rate: 0.00075012]
	Learning Rate: 0.000750116
	LOSS [training: 2.850209230937569 | validation: 3.001363959437777]
	TIME [epoch: 8.33 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846483602543034		[learning rate: 0.00074835]
	Learning Rate: 0.000748346
	LOSS [training: 2.846483602543034 | validation: 2.996502543035622]
	TIME [epoch: 8.32 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841072439375779		[learning rate: 0.00074658]
	Learning Rate: 0.000746581
	LOSS [training: 2.841072439375779 | validation: 2.99757414265269]
	TIME [epoch: 8.36 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8410015066708834		[learning rate: 0.00074482]
	Learning Rate: 0.00074482
	LOSS [training: 2.8410015066708834 | validation: 2.9970007669176058]
	TIME [epoch: 8.35 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837514194699308		[learning rate: 0.00074306]
	Learning Rate: 0.000743063
	LOSS [training: 2.837514194699308 | validation: 2.988180256265818]
	TIME [epoch: 8.34 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8356475840272193		[learning rate: 0.00074131]
	Learning Rate: 0.00074131
	LOSS [training: 2.8356475840272193 | validation: 2.9926712398646904]
	TIME [epoch: 8.32 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840836006470468		[learning rate: 0.00073956]
	Learning Rate: 0.000739562
	LOSS [training: 2.840836006470468 | validation: 2.996912440559238]
	TIME [epoch: 8.33 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8465165234139014		[learning rate: 0.00073782]
	Learning Rate: 0.000737817
	LOSS [training: 2.8465165234139014 | validation: 3.004447424272481]
	TIME [epoch: 8.32 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8478986369509136		[learning rate: 0.00073608]
	Learning Rate: 0.000736077
	LOSS [training: 2.8478986369509136 | validation: 2.999230166834821]
	TIME [epoch: 8.35 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842789087058174		[learning rate: 0.00073434]
	Learning Rate: 0.000734341
	LOSS [training: 2.842789087058174 | validation: 3.0034889422491093]
	TIME [epoch: 8.37 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.845873833871717		[learning rate: 0.00073261]
	Learning Rate: 0.000732608
	LOSS [training: 2.845873833871717 | validation: 3.0029168500151977]
	TIME [epoch: 8.32 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8486845767059057		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 2.8486845767059057 | validation: 3.004205571526365]
	TIME [epoch: 8.33 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501133129048313		[learning rate: 0.00072916]
	Learning Rate: 0.000729156
	LOSS [training: 2.8501133129048313 | validation: 3.0033027766220766]
	TIME [epoch: 8.32 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849144076557647		[learning rate: 0.00072744]
	Learning Rate: 0.000727436
	LOSS [training: 2.849144076557647 | validation: 3.0044473027042082]
	TIME [epoch: 8.33 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850808900925007		[learning rate: 0.00072572]
	Learning Rate: 0.00072572
	LOSS [training: 2.850808900925007 | validation: 3.00785952380462]
	TIME [epoch: 8.34 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503964753436972		[learning rate: 0.00072401]
	Learning Rate: 0.000724008
	LOSS [training: 2.8503964753436972 | validation: 3.0091932524131986]
	TIME [epoch: 8.38 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8518479815699256		[learning rate: 0.0007223]
	Learning Rate: 0.000722301
	LOSS [training: 2.8518479815699256 | validation: 3.0054781558509216]
	TIME [epoch: 8.33 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8489452395036947		[learning rate: 0.0007206]
	Learning Rate: 0.000720597
	LOSS [training: 2.8489452395036947 | validation: 3.00685835222733]
	TIME [epoch: 8.32 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849655063658109		[learning rate: 0.0007189]
	Learning Rate: 0.000718897
	LOSS [training: 2.849655063658109 | validation: 3.0037506991159333]
	TIME [epoch: 8.33 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8493577659997005		[learning rate: 0.0007172]
	Learning Rate: 0.000717201
	LOSS [training: 2.8493577659997005 | validation: 3.007746832192339]
	TIME [epoch: 8.33 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85308893386068		[learning rate: 0.00071551]
	Learning Rate: 0.00071551
	LOSS [training: 2.85308893386068 | validation: 3.0100336420800478]
	TIME [epoch: 8.34 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852824035550759		[learning rate: 0.00071382]
	Learning Rate: 0.000713822
	LOSS [training: 2.852824035550759 | validation: 3.007984429084109]
	TIME [epoch: 8.38 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8479855917198993		[learning rate: 0.00071214]
	Learning Rate: 0.000712138
	LOSS [training: 2.8479855917198993 | validation: 2.9995350998944286]
	TIME [epoch: 8.31 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8404409790748573		[learning rate: 0.00071046]
	Learning Rate: 0.000710458
	LOSS [training: 2.8404409790748573 | validation: 2.9931384423582355]
	TIME [epoch: 8.33 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840162446527722		[learning rate: 0.00070878]
	Learning Rate: 0.000708782
	LOSS [training: 2.840162446527722 | validation: 2.9946110437267732]
	TIME [epoch: 8.33 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837134490919149		[learning rate: 0.00070711]
	Learning Rate: 0.00070711
	LOSS [training: 2.837134490919149 | validation: 2.9878000036897987]
	TIME [epoch: 8.32 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8341893909086857		[learning rate: 0.00070544]
	Learning Rate: 0.000705443
	LOSS [training: 2.8341893909086857 | validation: 2.9864068595159]
	TIME [epoch: 8.35 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831868543493187		[learning rate: 0.00070378]
	Learning Rate: 0.000703778
	LOSS [training: 2.831868543493187 | validation: 2.9859553878514986]
	TIME [epoch: 8.37 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8273289253789198		[learning rate: 0.00070212]
	Learning Rate: 0.000702118
	LOSS [training: 2.8273289253789198 | validation: 2.9812819508280723]
	TIME [epoch: 8.34 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826218575200404		[learning rate: 0.00070046]
	Learning Rate: 0.000700462
	LOSS [training: 2.826218575200404 | validation: 2.980440097243555]
	TIME [epoch: 8.32 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8266740028510586		[learning rate: 0.00069881]
	Learning Rate: 0.00069881
	LOSS [training: 2.8266740028510586 | validation: 2.9822662740811885]
	TIME [epoch: 8.34 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8272193264948013		[learning rate: 0.00069716]
	Learning Rate: 0.000697161
	LOSS [training: 2.8272193264948013 | validation: 2.984358686732886]
	TIME [epoch: 8.33 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831707698768171		[learning rate: 0.00069552]
	Learning Rate: 0.000695517
	LOSS [training: 2.831707698768171 | validation: 2.989936419615907]
	TIME [epoch: 8.34 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83245528039285		[learning rate: 0.00069388]
	Learning Rate: 0.000693876
	LOSS [training: 2.83245528039285 | validation: 2.988778777024068]
	TIME [epoch: 8.36 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837176597414228		[learning rate: 0.00069224]
	Learning Rate: 0.00069224
	LOSS [training: 2.837176597414228 | validation: 2.993072766133129]
	TIME [epoch: 8.35 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.835849236740188		[learning rate: 0.00069061]
	Learning Rate: 0.000690607
	LOSS [training: 2.835849236740188 | validation: 2.9906111669118403]
	TIME [epoch: 8.33 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8326085300563024		[learning rate: 0.00068898]
	Learning Rate: 0.000688978
	LOSS [training: 2.8326085300563024 | validation: 2.9847202502543615]
	TIME [epoch: 8.34 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8302273368120625		[learning rate: 0.00068735]
	Learning Rate: 0.000687352
	LOSS [training: 2.8302273368120625 | validation: 2.98480993576613]
	TIME [epoch: 8.33 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8299394192938117		[learning rate: 0.00068573]
	Learning Rate: 0.000685731
	LOSS [training: 2.8299394192938117 | validation: 2.984260612373581]
	TIME [epoch: 8.35 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8291734145356733		[learning rate: 0.00068411]
	Learning Rate: 0.000684114
	LOSS [training: 2.8291734145356733 | validation: 2.9805306506371876]
	TIME [epoch: 8.35 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827064102845303		[learning rate: 0.0006825]
	Learning Rate: 0.0006825
	LOSS [training: 2.827064102845303 | validation: 2.982726478738238]
	TIME [epoch: 8.35 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832058706544664		[learning rate: 0.00068089]
	Learning Rate: 0.00068089
	LOSS [training: 2.832058706544664 | validation: 2.9935858174564247]
	TIME [epoch: 8.33 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8355892561207146		[learning rate: 0.00067928]
	Learning Rate: 0.000679284
	LOSS [training: 2.8355892561207146 | validation: 2.9888173740720996]
	TIME [epoch: 8.34 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8334211835350076		[learning rate: 0.00067768]
	Learning Rate: 0.000677682
	LOSS [training: 2.8334211835350076 | validation: 2.9854477440568736]
	TIME [epoch: 8.33 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8322900323285145		[learning rate: 0.00067608]
	Learning Rate: 0.000676083
	LOSS [training: 2.8322900323285145 | validation: 2.9897090881351303]
	TIME [epoch: 8.33 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8361786182804805		[learning rate: 0.00067449]
	Learning Rate: 0.000674488
	LOSS [training: 2.8361786182804805 | validation: 2.9893172426978287]
	TIME [epoch: 8.34 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83646869092333		[learning rate: 0.0006729]
	Learning Rate: 0.000672897
	LOSS [training: 2.83646869092333 | validation: 2.9944398428453924]
	TIME [epoch: 8.37 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8403191681563946		[learning rate: 0.00067131]
	Learning Rate: 0.00067131
	LOSS [training: 2.8403191681563946 | validation: 2.995656000644697]
	TIME [epoch: 8.33 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8423365390242195		[learning rate: 0.00066973]
	Learning Rate: 0.000669726
	LOSS [training: 2.8423365390242195 | validation: 2.9963048459280013]
	TIME [epoch: 8.32 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8348246793688867		[learning rate: 0.00066815]
	Learning Rate: 0.000668147
	LOSS [training: 2.8348246793688867 | validation: 2.9858302314080607]
	TIME [epoch: 8.33 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.83221580644122		[learning rate: 0.00066657]
	Learning Rate: 0.000666571
	LOSS [training: 2.83221580644122 | validation: 2.9885922340758087]
	TIME [epoch: 8.34 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837017977867309		[learning rate: 0.000665]
	Learning Rate: 0.000664998
	LOSS [training: 2.837017977867309 | validation: 2.995209043900991]
	TIME [epoch: 8.34 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8386424100542453		[learning rate: 0.00066343]
	Learning Rate: 0.00066343
	LOSS [training: 2.8386424100542453 | validation: 2.9884428534230905]
	TIME [epoch: 8.37 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8336134443877716		[learning rate: 0.00066186]
	Learning Rate: 0.000661865
	LOSS [training: 2.8336134443877716 | validation: 2.9933495876664065]
	TIME [epoch: 8.33 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8390619333954983		[learning rate: 0.0006603]
	Learning Rate: 0.000660304
	LOSS [training: 2.8390619333954983 | validation: 2.9945596488848114]
	TIME [epoch: 8.33 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8369072707964804		[learning rate: 0.00065875]
	Learning Rate: 0.000658746
	LOSS [training: 2.8369072707964804 | validation: 2.9914840413309016]
	TIME [epoch: 8.33 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8354004710465377		[learning rate: 0.00065719]
	Learning Rate: 0.000657192
	LOSS [training: 2.8354004710465377 | validation: 2.988816005359305]
	TIME [epoch: 8.29 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833903925322333		[learning rate: 0.00065564]
	Learning Rate: 0.000655642
	LOSS [training: 2.833903925322333 | validation: 2.990258549540065]
	TIME [epoch: 8.35 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8373174436991873		[learning rate: 0.0006541]
	Learning Rate: 0.000654095
	LOSS [training: 2.8373174436991873 | validation: 2.9977044034181795]
	TIME [epoch: 8.33 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8396213375193056		[learning rate: 0.00065255]
	Learning Rate: 0.000652552
	LOSS [training: 2.8396213375193056 | validation: 2.9949421774795963]
	TIME [epoch: 8.34 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8388348994562573		[learning rate: 0.00065101]
	Learning Rate: 0.000651013
	LOSS [training: 2.8388348994562573 | validation: 2.9909448656104827]
	TIME [epoch: 8.29 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838032410180641		[learning rate: 0.00064948]
	Learning Rate: 0.000649478
	LOSS [training: 2.838032410180641 | validation: 2.99559985823547]
	TIME [epoch: 8.33 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8370637957544753		[learning rate: 0.00064795]
	Learning Rate: 0.000647945
	LOSS [training: 2.8370637957544753 | validation: 2.994114214648162]
	TIME [epoch: 8.29 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8376935553687987		[learning rate: 0.00064642]
	Learning Rate: 0.000646417
	LOSS [training: 2.8376935553687987 | validation: 2.988268317279383]
	TIME [epoch: 8.34 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8312953708136983		[learning rate: 0.00064489]
	Learning Rate: 0.000644892
	LOSS [training: 2.8312953708136983 | validation: 2.9819651946646326]
	TIME [epoch: 8.37 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827902824917013		[learning rate: 0.00064337]
	Learning Rate: 0.000643371
	LOSS [training: 2.827902824917013 | validation: 2.9826161948719054]
	TIME [epoch: 8.35 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827108002487419		[learning rate: 0.00064185]
	Learning Rate: 0.000641854
	LOSS [training: 2.827108002487419 | validation: 2.978118027209727]
	TIME [epoch: 8.33 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.821149314057524		[learning rate: 0.00064034]
	Learning Rate: 0.00064034
	LOSS [training: 2.821149314057524 | validation: 2.9727385653078655]
	TIME [epoch: 8.34 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8184155846884202		[learning rate: 0.00063883]
	Learning Rate: 0.000638829
	LOSS [training: 2.8184155846884202 | validation: 2.9758192440366464]
	TIME [epoch: 8.35 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824076202535477		[learning rate: 0.00063732]
	Learning Rate: 0.000637322
	LOSS [training: 2.824076202535477 | validation: 2.9803174040019362]
	TIME [epoch: 8.33 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826198730377948		[learning rate: 0.00063582]
	Learning Rate: 0.000635819
	LOSS [training: 2.826198730377948 | validation: 2.982452818320806]
	TIME [epoch: 8.37 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8289314160307404		[learning rate: 0.00063432]
	Learning Rate: 0.000634319
	LOSS [training: 2.8289314160307404 | validation: 2.985812098620513]
	TIME [epoch: 8.33 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8304040923627114		[learning rate: 0.00063282]
	Learning Rate: 0.000632823
	LOSS [training: 2.8304040923627114 | validation: 2.9849323047939755]
	TIME [epoch: 8.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831825031357769		[learning rate: 0.00063133]
	Learning Rate: 0.00063133
	LOSS [training: 2.831825031357769 | validation: 2.989368678059553]
	TIME [epoch: 8.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8331150691345703		[learning rate: 0.00062984]
	Learning Rate: 0.000629841
	LOSS [training: 2.8331150691345703 | validation: 2.988212972034736]
	TIME [epoch: 8.28 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8332489910207554		[learning rate: 0.00062836]
	Learning Rate: 0.000628355
	LOSS [training: 2.8332489910207554 | validation: 2.98499018507684]
	TIME [epoch: 8.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8315809236711114		[learning rate: 0.00062687]
	Learning Rate: 0.000626873
	LOSS [training: 2.8315809236711114 | validation: 2.983558106051481]
	TIME [epoch: 8.31 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.829659849561194		[learning rate: 0.00062539]
	Learning Rate: 0.000625394
	LOSS [training: 2.829659849561194 | validation: 2.9847120483578196]
	TIME [epoch: 8.34 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8290295306211304		[learning rate: 0.00062392]
	Learning Rate: 0.000623919
	LOSS [training: 2.8290295306211304 | validation: 2.9832308384491713]
	TIME [epoch: 8.29 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8294485027654153		[learning rate: 0.00062245]
	Learning Rate: 0.000622447
	LOSS [training: 2.8294485027654153 | validation: 2.984309970094439]
	TIME [epoch: 8.33 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8283121841527854		[learning rate: 0.00062098]
	Learning Rate: 0.000620979
	LOSS [training: 2.8283121841527854 | validation: 2.9809088597162967]
	TIME [epoch: 8.26 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8261942501768913		[learning rate: 0.00061951]
	Learning Rate: 0.000619514
	LOSS [training: 2.8261942501768913 | validation: 2.9789730425434113]
	TIME [epoch: 8.27 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825799153553864		[learning rate: 0.00061805]
	Learning Rate: 0.000618053
	LOSS [training: 2.825799153553864 | validation: 2.97848276116775]
	TIME [epoch: 8.28 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8242829433244374		[learning rate: 0.0006166]
	Learning Rate: 0.000616595
	LOSS [training: 2.8242829433244374 | validation: 2.976578332505693]
	TIME [epoch: 8.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8213563197395426		[learning rate: 0.00061514]
	Learning Rate: 0.000615141
	LOSS [training: 2.8213563197395426 | validation: 2.973080574971516]
	TIME [epoch: 8.26 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.817757537633611		[learning rate: 0.00061369]
	Learning Rate: 0.00061369
	LOSS [training: 2.817757537633611 | validation: 2.967142289798109]
	TIME [epoch: 8.27 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814289249550533		[learning rate: 0.00061224]
	Learning Rate: 0.000612242
	LOSS [training: 2.814289249550533 | validation: 2.9714569438749043]
	TIME [epoch: 8.26 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8190942642696415		[learning rate: 0.0006108]
	Learning Rate: 0.000610798
	LOSS [training: 2.8190942642696415 | validation: 2.977016153299551]
	TIME [epoch: 8.26 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8205379183788626		[learning rate: 0.00060936]
	Learning Rate: 0.000609357
	LOSS [training: 2.8205379183788626 | validation: 2.9736053110585927]
	TIME [epoch: 8.27 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8197040977506322		[learning rate: 0.00060792]
	Learning Rate: 0.00060792
	LOSS [training: 2.8197040977506322 | validation: 2.9771080141865274]
	TIME [epoch: 8.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8233684697634702		[learning rate: 0.00060649]
	Learning Rate: 0.000606486
	LOSS [training: 2.8233684697634702 | validation: 2.9761428215686383]
	TIME [epoch: 8.28 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82239544171299		[learning rate: 0.00060506]
	Learning Rate: 0.000605055
	LOSS [training: 2.82239544171299 | validation: 2.977782033186746]
	TIME [epoch: 8.28 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823820206822118		[learning rate: 0.00060363]
	Learning Rate: 0.000603628
	LOSS [training: 2.823820206822118 | validation: 2.979057879284925]
	TIME [epoch: 8.27 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825491668738563		[learning rate: 0.0006022]
	Learning Rate: 0.000602204
	LOSS [training: 2.825491668738563 | validation: 2.9796384811421337]
	TIME [epoch: 8.26 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258825095447384		[learning rate: 0.00060078]
	Learning Rate: 0.000600784
	LOSS [training: 2.8258825095447384 | validation: 2.9803588466623108]
	TIME [epoch: 8.27 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8260018755472234		[learning rate: 0.00059937]
	Learning Rate: 0.000599366
	LOSS [training: 2.8260018755472234 | validation: 2.9783067971426425]
	TIME [epoch: 8.29 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258464242121315		[learning rate: 0.00059795]
	Learning Rate: 0.000597953
	LOSS [training: 2.8258464242121315 | validation: 2.9829983269843874]
	TIME [epoch: 8.28 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8288106812310367		[learning rate: 0.00059654]
	Learning Rate: 0.000596542
	LOSS [training: 2.8288106812310367 | validation: 2.983352468751278]
	TIME [epoch: 8.26 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8282353614092797		[learning rate: 0.00059513]
	Learning Rate: 0.000595135
	LOSS [training: 2.8282353614092797 | validation: 2.9816542550253082]
	TIME [epoch: 8.26 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8271542832090106		[learning rate: 0.00059373]
	Learning Rate: 0.000593731
	LOSS [training: 2.8271542832090106 | validation: 2.9791962503989082]
	TIME [epoch: 8.25 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8253946166482935		[learning rate: 0.00059233]
	Learning Rate: 0.000592331
	LOSS [training: 2.8253946166482935 | validation: 2.978955855064281]
	TIME [epoch: 8.27 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8250595812135906		[learning rate: 0.00059093]
	Learning Rate: 0.000590933
	LOSS [training: 2.8250595812135906 | validation: 2.980892847652834]
	TIME [epoch: 8.29 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8290205658017453		[learning rate: 0.00058954]
	Learning Rate: 0.000589539
	LOSS [training: 2.8290205658017453 | validation: 2.983915799074748]
	TIME [epoch: 8.28 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8281805275727345		[learning rate: 0.00058815]
	Learning Rate: 0.000588149
	LOSS [training: 2.8281805275727345 | validation: 2.9787710710310167]
	TIME [epoch: 8.25 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8222160807117853		[learning rate: 0.00058676]
	Learning Rate: 0.000586761
	LOSS [training: 2.8222160807117853 | validation: 2.9730734127167615]
	TIME [epoch: 8.29 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8167573692070245		[learning rate: 0.00058538]
	Learning Rate: 0.000585377
	LOSS [training: 2.8167573692070245 | validation: 2.9707373012827416]
	TIME [epoch: 8.25 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8183725208239987		[learning rate: 0.000584]
	Learning Rate: 0.000583997
	LOSS [training: 2.8183725208239987 | validation: 2.9773130103730034]
	TIME [epoch: 8.28 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824226269800844		[learning rate: 0.00058262]
	Learning Rate: 0.000582619
	LOSS [training: 2.824226269800844 | validation: 2.975744801721486]
	TIME [epoch: 8.28 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823649873883135		[learning rate: 0.00058124]
	Learning Rate: 0.000581245
	LOSS [training: 2.823649873883135 | validation: 2.9778811424669724]
	TIME [epoch: 8.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824093902374394		[learning rate: 0.00057987]
	Learning Rate: 0.000579874
	LOSS [training: 2.824093902374394 | validation: 2.9757054109658356]
	TIME [epoch: 8.28 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8223564020807697		[learning rate: 0.00057851]
	Learning Rate: 0.000578506
	LOSS [training: 2.8223564020807697 | validation: 2.9782863266705952]
	TIME [epoch: 8.26 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8217779032906196		[learning rate: 0.00057714]
	Learning Rate: 0.000577141
	LOSS [training: 2.8217779032906196 | validation: 2.9749056384407986]
	TIME [epoch: 8.28 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8200145745217284		[learning rate: 0.00057578]
	Learning Rate: 0.00057578
	LOSS [training: 2.8200145745217284 | validation: 2.9691758241271713]
	TIME [epoch: 8.27 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813042926240377		[learning rate: 0.00057442]
	Learning Rate: 0.000574422
	LOSS [training: 2.813042926240377 | validation: 2.963631076030034]
	TIME [epoch: 8.28 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8079024346131045		[learning rate: 0.00057307]
	Learning Rate: 0.000573067
	LOSS [training: 2.8079024346131045 | validation: 2.960927828403223]
	TIME [epoch: 8.31 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8050334836678377		[learning rate: 0.00057171]
	Learning Rate: 0.000571715
	LOSS [training: 2.8050334836678377 | validation: 2.9514770515998228]
	TIME [epoch: 8.25 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7975437887985155		[learning rate: 0.00057037]
	Learning Rate: 0.000570366
	LOSS [training: 2.7975437887985155 | validation: 2.9533564229552915]
	TIME [epoch: 8.27 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8013222023078233		[learning rate: 0.00056902]
	Learning Rate: 0.000569021
	LOSS [training: 2.8013222023078233 | validation: 2.959481611836286]
	TIME [epoch: 8.26 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8064166454822534		[learning rate: 0.00056768]
	Learning Rate: 0.000567679
	LOSS [training: 2.8064166454822534 | validation: 2.961708217368956]
	TIME [epoch: 8.27 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.80324271702269		[learning rate: 0.00056634]
	Learning Rate: 0.00056634
	LOSS [training: 2.80324271702269 | validation: 2.9572482829162476]
	TIME [epoch: 8.26 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.805217862030203		[learning rate: 0.000565]
	Learning Rate: 0.000565004
	LOSS [training: 2.805217862030203 | validation: 2.9622156824038557]
	TIME [epoch: 8.29 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8066133249614365		[learning rate: 0.00056367]
	Learning Rate: 0.000563671
	LOSS [training: 2.8066133249614365 | validation: 2.9573718391748613]
	TIME [epoch: 8.28 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8039297206164493		[learning rate: 0.00056234]
	Learning Rate: 0.000562341
	LOSS [training: 2.8039297206164493 | validation: 2.9569847136621794]
	TIME [epoch: 8.27 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.805081197876616		[learning rate: 0.00056101]
	Learning Rate: 0.000561015
	LOSS [training: 2.805081197876616 | validation: 2.964953695022631]
	TIME [epoch: 8.27 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8085647112075045		[learning rate: 0.00055969]
	Learning Rate: 0.000559692
	LOSS [training: 2.8085647112075045 | validation: 2.9610214012065006]
	TIME [epoch: 8.26 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.807250147299352		[learning rate: 0.00055837]
	Learning Rate: 0.000558371
	LOSS [training: 2.807250147299352 | validation: 2.9627524751025387]
	TIME [epoch: 8.27 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.812303346041034		[learning rate: 0.00055705]
	Learning Rate: 0.000557054
	LOSS [training: 2.812303346041034 | validation: 2.9665539624818766]
	TIME [epoch: 8.28 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8081620824490723		[learning rate: 0.00055574]
	Learning Rate: 0.00055574
	LOSS [training: 2.8081620824490723 | validation: 2.958497388294311]
	TIME [epoch: 8.28 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8034437727163297		[learning rate: 0.00055443]
	Learning Rate: 0.000554429
	LOSS [training: 2.8034437727163297 | validation: 2.9563752697229746]
	TIME [epoch: 8.24 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8007542277080772		[learning rate: 0.00055312]
	Learning Rate: 0.000553121
	LOSS [training: 2.8007542277080772 | validation: 2.957707556225613]
	TIME [epoch: 8.25 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8071161377413314		[learning rate: 0.00055182]
	Learning Rate: 0.000551817
	LOSS [training: 2.8071161377413314 | validation: 2.9670068488974097]
	TIME [epoch: 8.27 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8138739066599157		[learning rate: 0.00055052]
	Learning Rate: 0.000550515
	LOSS [training: 2.8138739066599157 | validation: 2.9723618162180214]
	TIME [epoch: 8.26 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8201035884159156		[learning rate: 0.00054922]
	Learning Rate: 0.000549216
	LOSS [training: 2.8201035884159156 | validation: 2.9813343464924307]
	TIME [epoch: 8.28 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258109428186855		[learning rate: 0.00054792]
	Learning Rate: 0.000547921
	LOSS [training: 2.8258109428186855 | validation: 2.9807303126234608]
	TIME [epoch: 8.29 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8277536462201915		[learning rate: 0.00054663]
	Learning Rate: 0.000546629
	LOSS [training: 2.8277536462201915 | validation: 2.9862054209275906]
	TIME [epoch: 8.25 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8311287360664963		[learning rate: 0.00054534]
	Learning Rate: 0.000545339
	LOSS [training: 2.8311287360664963 | validation: 2.9856041825354356]
	TIME [epoch: 8.27 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8301611716224255		[learning rate: 0.00054405]
	Learning Rate: 0.000544053
	LOSS [training: 2.8301611716224255 | validation: 2.9823549999999885]
	TIME [epoch: 8.26 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8308834919698205		[learning rate: 0.00054277]
	Learning Rate: 0.000542769
	LOSS [training: 2.8308834919698205 | validation: 2.9839585069563874]
	TIME [epoch: 8.26 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8312152244813484		[learning rate: 0.00054149]
	Learning Rate: 0.000541489
	LOSS [training: 2.8312152244813484 | validation: 2.98716373546051]
	TIME [epoch: 8.29 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833475375230349		[learning rate: 0.00054021]
	Learning Rate: 0.000540212
	LOSS [training: 2.833475375230349 | validation: 2.9885274602543546]
	TIME [epoch: 8.32 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.835890578912676		[learning rate: 0.00053894]
	Learning Rate: 0.000538938
	LOSS [training: 2.835890578912676 | validation: 2.992056863474409]
	TIME [epoch: 8.26 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8374493899237994		[learning rate: 0.00053767]
	Learning Rate: 0.000537666
	LOSS [training: 2.8374493899237994 | validation: 2.988718465380221]
	TIME [epoch: 8.27 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8340087126962548		[learning rate: 0.0005364]
	Learning Rate: 0.000536398
	LOSS [training: 2.8340087126962548 | validation: 2.9892451886894245]
	TIME [epoch: 8.29 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8367362024111333		[learning rate: 0.00053513]
	Learning Rate: 0.000535133
	LOSS [training: 2.8367362024111333 | validation: 2.9973093748619837]
	TIME [epoch: 8.28 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842483696525552		[learning rate: 0.00053387]
	Learning Rate: 0.00053387
	LOSS [training: 2.842483696525552 | validation: 2.999731946641389]
	TIME [epoch: 8.28 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8459097772789352		[learning rate: 0.00053261]
	Learning Rate: 0.000532611
	LOSS [training: 2.8459097772789352 | validation: 3.00326988029771]
	TIME [epoch: 8.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846604542297937		[learning rate: 0.00053135]
	Learning Rate: 0.000531355
	LOSS [training: 2.846604542297937 | validation: 2.9987633578749104]
	TIME [epoch: 8.29 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8440689218069055		[learning rate: 0.0005301]
	Learning Rate: 0.000530101
	LOSS [training: 2.8440689218069055 | validation: 3.003415383311365]
	TIME [epoch: 8.27 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8438962097394174		[learning rate: 0.00052885]
	Learning Rate: 0.000528851
	LOSS [training: 2.8438962097394174 | validation: 3.001989059695199]
	TIME [epoch: 8.27 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8445826326302415		[learning rate: 0.0005276]
	Learning Rate: 0.000527603
	LOSS [training: 2.8445826326302415 | validation: 3.0003087050532686]
	TIME [epoch: 8.29 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.845179291715012		[learning rate: 0.00052636]
	Learning Rate: 0.000526359
	LOSS [training: 2.845179291715012 | validation: 3.0046713517768984]
	TIME [epoch: 8.27 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847022459530779		[learning rate: 0.00052512]
	Learning Rate: 0.000525117
	LOSS [training: 2.847022459530779 | validation: 3.005235143118404]
	TIME [epoch: 8.32 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849194694144915		[learning rate: 0.00052388]
	Learning Rate: 0.000523879
	LOSS [training: 2.849194694144915 | validation: 3.0046743708316326]
	TIME [epoch: 8.27 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846909921141597		[learning rate: 0.00052264]
	Learning Rate: 0.000522643
	LOSS [training: 2.846909921141597 | validation: 3.000440050806959]
	TIME [epoch: 8.27 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8441328687277485		[learning rate: 0.00052141]
	Learning Rate: 0.00052141
	LOSS [training: 2.8441328687277485 | validation: 2.997564467876514]
	TIME [epoch: 8.27 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8431381715849		[learning rate: 0.00052018]
	Learning Rate: 0.00052018
	LOSS [training: 2.8431381715849 | validation: 2.9974019500804867]
	TIME [epoch: 8.29 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8401330393031006		[learning rate: 0.00051895]
	Learning Rate: 0.000518953
	LOSS [training: 2.8401330393031006 | validation: 2.9947143601717565]
	TIME [epoch: 8.26 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84095092640701		[learning rate: 0.00051773]
	Learning Rate: 0.000517729
	LOSS [training: 2.84095092640701 | validation: 2.9999187534356646]
	TIME [epoch: 8.29 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8423527478869803		[learning rate: 0.00051651]
	Learning Rate: 0.000516508
	LOSS [training: 2.8423527478869803 | validation: 2.9966318260820684]
	TIME [epoch: 8.33 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841862783347105		[learning rate: 0.00051529]
	Learning Rate: 0.000515289
	LOSS [training: 2.841862783347105 | validation: 2.997591649775055]
	TIME [epoch: 8.28 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843492490920335		[learning rate: 0.00051407]
	Learning Rate: 0.000514074
	LOSS [training: 2.843492490920335 | validation: 2.9999593707536363]
	TIME [epoch: 8.29 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843571979439517		[learning rate: 0.00051286]
	Learning Rate: 0.000512861
	LOSS [training: 2.843571979439517 | validation: 2.9987670875979746]
	TIME [epoch: 8.27 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8409736240790058		[learning rate: 0.00051165]
	Learning Rate: 0.000511652
	LOSS [training: 2.8409736240790058 | validation: 2.996026489778094]
	TIME [epoch: 8.29 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840839738021599		[learning rate: 0.00051044]
	Learning Rate: 0.000510445
	LOSS [training: 2.840839738021599 | validation: 2.9958832138809237]
	TIME [epoch: 8.27 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840073713022		[learning rate: 0.00050924]
	Learning Rate: 0.000509241
	LOSS [training: 2.840073713022 | validation: 2.9951345939923533]
	TIME [epoch: 8.32 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.836528810005405		[learning rate: 0.00050804]
	Learning Rate: 0.000508039
	LOSS [training: 2.836528810005405 | validation: 2.994575556519916]
	TIME [epoch: 8.26 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8378572306458323		[learning rate: 0.00050684]
	Learning Rate: 0.000506841
	LOSS [training: 2.8378572306458323 | validation: 2.9934550907909947]
	TIME [epoch: 8.27 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8364852777586127		[learning rate: 0.00050565]
	Learning Rate: 0.000505646
	LOSS [training: 2.8364852777586127 | validation: 2.990969690934164]
	TIME [epoch: 8.26 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.836227077384158		[learning rate: 0.00050445]
	Learning Rate: 0.000504453
	LOSS [training: 2.836227077384158 | validation: 2.98968914144501]
	TIME [epoch: 8.28 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8341745961417346		[learning rate: 0.00050326]
	Learning Rate: 0.000503263
	LOSS [training: 2.8341745961417346 | validation: 2.988244910059832]
	TIME [epoch: 8.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831028572028109		[learning rate: 0.00050208]
	Learning Rate: 0.000502076
	LOSS [training: 2.831028572028109 | validation: 2.9869051488494613]
	TIME [epoch: 8.31 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8312231056538018		[learning rate: 0.00050089]
	Learning Rate: 0.000500891
	LOSS [training: 2.8312231056538018 | validation: 2.984281368887711]
	TIME [epoch: 8.28 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8289330959401897		[learning rate: 0.00049971]
	Learning Rate: 0.00049971
	LOSS [training: 2.8289330959401897 | validation: 2.982487436718287]
	TIME [epoch: 8.27 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8281722713974813		[learning rate: 0.00049853]
	Learning Rate: 0.000498531
	LOSS [training: 2.8281722713974813 | validation: 2.987266961965701]
	TIME [epoch: 8.29 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8280549605301863		[learning rate: 0.00049736]
	Learning Rate: 0.000497355
	LOSS [training: 2.8280549605301863 | validation: 2.984570727885223]
	TIME [epoch: 8.29 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828845556651739		[learning rate: 0.00049618]
	Learning Rate: 0.000496182
	LOSS [training: 2.828845556651739 | validation: 2.9876737286721737]
	TIME [epoch: 8.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8304269730538834		[learning rate: 0.00049501]
	Learning Rate: 0.000495012
	LOSS [training: 2.8304269730538834 | validation: 2.9843430481976974]
	TIME [epoch: 8.31 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8277016089372244		[learning rate: 0.00049384]
	Learning Rate: 0.000493844
	LOSS [training: 2.8277016089372244 | validation: 2.983148270268673]
	TIME [epoch: 8.31 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826387962623812		[learning rate: 0.00049268]
	Learning Rate: 0.000492679
	LOSS [training: 2.826387962623812 | validation: 2.980396549645544]
	TIME [epoch: 8.27 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241636864950204		[learning rate: 0.00049152]
	Learning Rate: 0.000491517
	LOSS [training: 2.8241636864950204 | validation: 2.979400993742896]
	TIME [epoch: 8.27 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824566054750834		[learning rate: 0.00049036]
	Learning Rate: 0.000490358
	LOSS [training: 2.824566054750834 | validation: 2.9816487062371726]
	TIME [epoch: 8.26 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823405100307725		[learning rate: 0.0004892]
	Learning Rate: 0.000489201
	LOSS [training: 2.823405100307725 | validation: 2.979154111541363]
	TIME [epoch: 8.27 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8233923712111104		[learning rate: 0.00048805]
	Learning Rate: 0.000488047
	LOSS [training: 2.8233923712111104 | validation: 2.9806386817635646]
	TIME [epoch: 8.29 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825435393652713		[learning rate: 0.0004869]
	Learning Rate: 0.000486896
	LOSS [training: 2.825435393652713 | validation: 2.9792672769124264]
	TIME [epoch: 8.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8238484360295883		[learning rate: 0.00048575]
	Learning Rate: 0.000485747
	LOSS [training: 2.8238484360295883 | validation: 2.9785394134360983]
	TIME [epoch: 8.28 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8225193827606785		[learning rate: 0.0004846]
	Learning Rate: 0.000484601
	LOSS [training: 2.8225193827606785 | validation: 2.9801870171492215]
	TIME [epoch: 8.28 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241931741074513		[learning rate: 0.00048346]
	Learning Rate: 0.000483458
	LOSS [training: 2.8241931741074513 | validation: 2.9811186236682605]
	TIME [epoch: 8.26 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8256107679216607		[learning rate: 0.00048232]
	Learning Rate: 0.000482318
	LOSS [training: 2.8256107679216607 | validation: 2.9825801544787423]
	TIME [epoch: 8.28 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827003393426059		[learning rate: 0.00048118]
	Learning Rate: 0.00048118
	LOSS [training: 2.827003393426059 | validation: 2.984854105393939]
	TIME [epoch: 8.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8273207566453555		[learning rate: 0.00048005]
	Learning Rate: 0.000480045
	LOSS [training: 2.8273207566453555 | validation: 2.983223607122878]
	TIME [epoch: 8.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8255588473147415		[learning rate: 0.00047891]
	Learning Rate: 0.000478913
	LOSS [training: 2.8255588473147415 | validation: 2.9808596558787483]
	TIME [epoch: 8.29 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823943405425908		[learning rate: 0.00047778]
	Learning Rate: 0.000477783
	LOSS [training: 2.823943405425908 | validation: 2.9784860605935926]
	TIME [epoch: 8.28 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8247397047697236		[learning rate: 0.00047666]
	Learning Rate: 0.000476656
	LOSS [training: 2.8247397047697236 | validation: 2.9794436559612523]
	TIME [epoch: 8.29 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8242196308672884		[learning rate: 0.00047553]
	Learning Rate: 0.000475532
	LOSS [training: 2.8242196308672884 | validation: 2.978654929101129]
	TIME [epoch: 8.28 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823471800149723		[learning rate: 0.00047441]
	Learning Rate: 0.00047441
	LOSS [training: 2.823471800149723 | validation: 2.9813682757180953]
	TIME [epoch: 8.29 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822841242960619		[learning rate: 0.00047329]
	Learning Rate: 0.000473291
	LOSS [training: 2.822841242960619 | validation: 2.978749617621224]
	TIME [epoch: 8.32 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822455577878422		[learning rate: 0.00047217]
	Learning Rate: 0.000472175
	LOSS [training: 2.822455577878422 | validation: 2.9775373750939877]
	TIME [epoch: 8.29 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8227547849071875		[learning rate: 0.00047106]
	Learning Rate: 0.000471061
	LOSS [training: 2.8227547849071875 | validation: 2.978264218873708]
	TIME [epoch: 8.28 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.821450594046465		[learning rate: 0.00046995]
	Learning Rate: 0.00046995
	LOSS [training: 2.821450594046465 | validation: 2.9780086437120694]
	TIME [epoch: 8.29 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8217427482720003		[learning rate: 0.00046884]
	Learning Rate: 0.000468841
	LOSS [training: 2.8217427482720003 | validation: 2.9791753410557575]
	TIME [epoch: 8.28 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822399491192228		[learning rate: 0.00046774]
	Learning Rate: 0.000467735
	LOSS [training: 2.822399491192228 | validation: 2.9780245610646805]
	TIME [epoch: 8.29 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8233364921034894		[learning rate: 0.00046663]
	Learning Rate: 0.000466632
	LOSS [training: 2.8233364921034894 | validation: 2.978538364197433]
	TIME [epoch: 8.32 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824450752162722		[learning rate: 0.00046553]
	Learning Rate: 0.000465531
	LOSS [training: 2.824450752162722 | validation: 2.98120384959569]
	TIME [epoch: 8.28 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8249633213785192		[learning rate: 0.00046443]
	Learning Rate: 0.000464433
	LOSS [training: 2.8249633213785192 | validation: 2.9806515981514075]
	TIME [epoch: 8.28 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823875525538676		[learning rate: 0.00046334]
	Learning Rate: 0.000463338
	LOSS [training: 2.823875525538676 | validation: 2.9810158042590684]
	TIME [epoch: 8.27 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8231755660407534		[learning rate: 0.00046224]
	Learning Rate: 0.000462245
	LOSS [training: 2.8231755660407534 | validation: 2.980427010047885]
	TIME [epoch: 8.29 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823260288867477		[learning rate: 0.00046115]
	Learning Rate: 0.000461154
	LOSS [training: 2.823260288867477 | validation: 2.9791354495936746]
	TIME [epoch: 8.29 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8239031475476826		[learning rate: 0.00046007]
	Learning Rate: 0.000460066
	LOSS [training: 2.8239031475476826 | validation: 2.9774343025576138]
	TIME [epoch: 8.32 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8227900373003316		[learning rate: 0.00045898]
	Learning Rate: 0.000458981
	LOSS [training: 2.8227900373003316 | validation: 2.977310273170687]
	TIME [epoch: 8.31 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822388336709442		[learning rate: 0.0004579]
	Learning Rate: 0.000457898
	LOSS [training: 2.822388336709442 | validation: 2.9787374006804654]
	TIME [epoch: 8.29 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8233262607943748		[learning rate: 0.00045682]
	Learning Rate: 0.000456818
	LOSS [training: 2.8233262607943748 | validation: 2.981245468458282]
	TIME [epoch: 8.28 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8229272087520823		[learning rate: 0.00045574]
	Learning Rate: 0.000455741
	LOSS [training: 2.8229272087520823 | validation: 2.977415175119216]
	TIME [epoch: 8.29 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82214494947952		[learning rate: 0.00045467]
	Learning Rate: 0.000454666
	LOSS [training: 2.82214494947952 | validation: 2.977233486525326]
	TIME [epoch: 8.28 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8231313850697326		[learning rate: 0.00045359]
	Learning Rate: 0.000453593
	LOSS [training: 2.8231313850697326 | validation: 2.9778978542978205]
	TIME [epoch: 8.31 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.821968039420658		[learning rate: 0.00045252]
	Learning Rate: 0.000452523
	LOSS [training: 2.821968039420658 | validation: 2.9778207811822446]
	TIME [epoch: 8.34 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822114351264333		[learning rate: 0.00045146]
	Learning Rate: 0.000451456
	LOSS [training: 2.822114351264333 | validation: 2.9784804285858955]
	TIME [epoch: 8.31 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8230209820767573		[learning rate: 0.00045039]
	Learning Rate: 0.000450391
	LOSS [training: 2.8230209820767573 | validation: 2.978860757227899]
	TIME [epoch: 8.32 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8232757626071705		[learning rate: 0.00044933]
	Learning Rate: 0.000449329
	LOSS [training: 2.8232757626071705 | validation: 2.979495157213548]
	TIME [epoch: 8.33 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8232590854516646		[learning rate: 0.00044827]
	Learning Rate: 0.000448269
	LOSS [training: 2.8232590854516646 | validation: 2.9778106860962312]
	TIME [epoch: 8.33 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8224812769360064		[learning rate: 0.00044721]
	Learning Rate: 0.000447211
	LOSS [training: 2.8224812769360064 | validation: 2.9791901428808627]
	TIME [epoch: 8.33 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824178834507552		[learning rate: 0.00044616]
	Learning Rate: 0.000446157
	LOSS [training: 2.824178834507552 | validation: 2.9783876380907404]
	TIME [epoch: 8.37 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8229592387995286		[learning rate: 0.0004451]
	Learning Rate: 0.000445104
	LOSS [training: 2.8229592387995286 | validation: 2.976713649898156]
	TIME [epoch: 8.33 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8216169449868875		[learning rate: 0.00044405]
	Learning Rate: 0.000444054
	LOSS [training: 2.8216169449868875 | validation: 2.977790479300287]
	TIME [epoch: 8.32 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8213859110376376		[learning rate: 0.00044301]
	Learning Rate: 0.000443007
	LOSS [training: 2.8213859110376376 | validation: 2.9758210473289592]
	TIME [epoch: 8.33 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.819488919207302		[learning rate: 0.00044196]
	Learning Rate: 0.000441962
	LOSS [training: 2.819488919207302 | validation: 2.974735625648038]
	TIME [epoch: 8.33 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.817945335527879		[learning rate: 0.00044092]
	Learning Rate: 0.000440919
	LOSS [training: 2.817945335527879 | validation: 2.9702751311802444]
	TIME [epoch: 8.34 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814560640618396		[learning rate: 0.00043988]
	Learning Rate: 0.000439879
	LOSS [training: 2.814560640618396 | validation: 2.969907637860006]
	TIME [epoch: 8.37 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8131568919131005		[learning rate: 0.00043884]
	Learning Rate: 0.000438842
	LOSS [training: 2.8131568919131005 | validation: 2.9672792231461735]
	TIME [epoch: 8.34 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.811774050727781		[learning rate: 0.00043781]
	Learning Rate: 0.000437806
	LOSS [training: 2.811774050727781 | validation: 2.9690511387102996]
	TIME [epoch: 8.33 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813416057087313		[learning rate: 0.00043677]
	Learning Rate: 0.000436774
	LOSS [training: 2.813416057087313 | validation: 2.971881951917106]
	TIME [epoch: 8.33 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8157329065619106		[learning rate: 0.00043574]
	Learning Rate: 0.000435743
	LOSS [training: 2.8157329065619106 | validation: 2.9714397422427483]
	TIME [epoch: 8.33 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8163822783299715		[learning rate: 0.00043472]
	Learning Rate: 0.000434715
	LOSS [training: 2.8163822783299715 | validation: 2.9699123392987303]
	TIME [epoch: 8.34 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814546406819775		[learning rate: 0.00043369]
	Learning Rate: 0.00043369
	LOSS [training: 2.814546406819775 | validation: 2.967634445405521]
	TIME [epoch: 8.37 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814776667989047		[learning rate: 0.00043267]
	Learning Rate: 0.000432667
	LOSS [training: 2.814776667989047 | validation: 2.972901984186274]
	TIME [epoch: 8.32 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.81805391016832		[learning rate: 0.00043165]
	Learning Rate: 0.000431647
	LOSS [training: 2.81805391016832 | validation: 2.9717750800751075]
	TIME [epoch: 8.33 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.816941932661155		[learning rate: 0.00043063]
	Learning Rate: 0.000430628
	LOSS [training: 2.816941932661155 | validation: 2.9744030619292783]
	TIME [epoch: 8.33 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.81887559458341		[learning rate: 0.00042961]
	Learning Rate: 0.000429613
	LOSS [training: 2.81887559458341 | validation: 2.9740729374336707]
	TIME [epoch: 8.34 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8192893979534657		[learning rate: 0.0004286]
	Learning Rate: 0.000428599
	LOSS [training: 2.8192893979534657 | validation: 2.9722566815892484]
	TIME [epoch: 8.33 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8174179151310175		[learning rate: 0.00042759]
	Learning Rate: 0.000427588
	LOSS [training: 2.8174179151310175 | validation: 2.9719875407219223]
	TIME [epoch: 8.36 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8149634104322363		[learning rate: 0.00042658]
	Learning Rate: 0.000426579
	LOSS [training: 2.8149634104322363 | validation: 2.9675943872643096]
	TIME [epoch: 8.35 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8112828171666653		[learning rate: 0.00042557]
	Learning Rate: 0.000425573
	LOSS [training: 2.8112828171666653 | validation: 2.962027127686643]
	TIME [epoch: 8.33 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8052812625489163		[learning rate: 0.00042457]
	Learning Rate: 0.000424569
	LOSS [training: 2.8052812625489163 | validation: 2.960485835379856]
	TIME [epoch: 8.34 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8058040737071597		[learning rate: 0.00042357]
	Learning Rate: 0.000423568
	LOSS [training: 2.8058040737071597 | validation: 2.958403147397884]
	TIME [epoch: 8.33 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.804176439856332		[learning rate: 0.00042257]
	Learning Rate: 0.000422569
	LOSS [training: 2.804176439856332 | validation: 2.9564909638293186]
	TIME [epoch: 8.33 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.801406653853694		[learning rate: 0.00042157]
	Learning Rate: 0.000421572
	LOSS [training: 2.801406653853694 | validation: 2.9572287271042703]
	TIME [epoch: 8.37 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8009730291269226		[learning rate: 0.00042058]
	Learning Rate: 0.000420578
	LOSS [training: 2.8009730291269226 | validation: 2.959253108758159]
	TIME [epoch: 8.35 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8063672619707605		[learning rate: 0.00041959]
	Learning Rate: 0.000419586
	LOSS [training: 2.8063672619707605 | validation: 2.9614609361197717]
	TIME [epoch: 8.34 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8061725589867863		[learning rate: 0.0004186]
	Learning Rate: 0.000418596
	LOSS [training: 2.8061725589867863 | validation: 2.9590587306186977]
	TIME [epoch: 8.33 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8030760119887215		[learning rate: 0.00041761]
	Learning Rate: 0.000417608
	LOSS [training: 2.8030760119887215 | validation: 2.9562325074045783]
	TIME [epoch: 8.34 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8014994760396257		[learning rate: 0.00041662]
	Learning Rate: 0.000416623
	LOSS [training: 2.8014994760396257 | validation: 2.955072087381711]
	TIME [epoch: 8.33 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.799745547779606		[learning rate: 0.00041564]
	Learning Rate: 0.000415641
	LOSS [training: 2.799745547779606 | validation: 2.9561123101269775]
	TIME [epoch: 8.35 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8011924908284174		[learning rate: 0.00041466]
	Learning Rate: 0.00041466
	LOSS [training: 2.8011924908284174 | validation: 2.9555148836574983]
	TIME [epoch: 8.37 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.802610209157176		[learning rate: 0.00041368]
	Learning Rate: 0.000413682
	LOSS [training: 2.802610209157176 | validation: 2.9595398892820963]
	TIME [epoch: 8.33 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.803710548328178		[learning rate: 0.00041271]
	Learning Rate: 0.000412706
	LOSS [training: 2.803710548328178 | validation: 2.9590285638292975]
	TIME [epoch: 8.33 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8069292361760247		[learning rate: 0.00041173]
	Learning Rate: 0.000411733
	LOSS [training: 2.8069292361760247 | validation: 2.9656139806441884]
	TIME [epoch: 8.33 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.810614091642544		[learning rate: 0.00041076]
	Learning Rate: 0.000410761
	LOSS [training: 2.810614091642544 | validation: 2.966829305196855]
	TIME [epoch: 8.32 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8136981835749086		[learning rate: 0.00040979]
	Learning Rate: 0.000409793
	LOSS [training: 2.8136981835749086 | validation: 2.972971292151267]
	TIME [epoch: 8.34 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8190876610473037		[learning rate: 0.00040883]
	Learning Rate: 0.000408826
	LOSS [training: 2.8190876610473037 | validation: 2.9744127546465307]
	TIME [epoch: 8.37 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8183002662316055		[learning rate: 0.00040786]
	Learning Rate: 0.000407862
	LOSS [training: 2.8183002662316055 | validation: 2.9744120610548967]
	TIME [epoch: 8.33 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.818480967787674		[learning rate: 0.0004069]
	Learning Rate: 0.0004069
	LOSS [training: 2.818480967787674 | validation: 2.9716193663144406]
	TIME [epoch: 8.32 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8184559799989435		[learning rate: 0.00040594]
	Learning Rate: 0.00040594
	LOSS [training: 2.8184559799989435 | validation: 2.9724106448433725]
	TIME [epoch: 8.32 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8180820858979923		[learning rate: 0.00040498]
	Learning Rate: 0.000404982
	LOSS [training: 2.8180820858979923 | validation: 2.971614311451912]
	TIME [epoch: 8.33 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.816572912699715		[learning rate: 0.00040403]
	Learning Rate: 0.000404027
	LOSS [training: 2.816572912699715 | validation: 2.9709015931171865]
	TIME [epoch: 8.33 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8143274033714105		[learning rate: 0.00040307]
	Learning Rate: 0.000403074
	LOSS [training: 2.8143274033714105 | validation: 2.9703701895161316]
	TIME [epoch: 8.38 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814040445600547		[learning rate: 0.00040212]
	Learning Rate: 0.000402123
	LOSS [training: 2.814040445600547 | validation: 2.9682407071521966]
	TIME [epoch: 8.34 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8145965969800653		[learning rate: 0.00040117]
	Learning Rate: 0.000401175
	LOSS [training: 2.8145965969800653 | validation: 2.969682731629802]
	TIME [epoch: 8.33 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8136373105081605		[learning rate: 0.00040023]
	Learning Rate: 0.000400228
	LOSS [training: 2.8136373105081605 | validation: 2.9658767861395834]
	TIME [epoch: 8.32 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.811293880949174		[learning rate: 0.00039928]
	Learning Rate: 0.000399284
	LOSS [training: 2.811293880949174 | validation: 2.9674506508448477]
	TIME [epoch: 8.33 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8119368640603177		[learning rate: 0.00039834]
	Learning Rate: 0.000398342
	LOSS [training: 2.8119368640603177 | validation: 2.9683357504606356]
	TIME [epoch: 8.33 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8122350106526035		[learning rate: 0.0003974]
	Learning Rate: 0.000397403
	LOSS [training: 2.8122350106526035 | validation: 2.967845428264833]
	TIME [epoch: 8.37 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8127013355193906		[learning rate: 0.00039647]
	Learning Rate: 0.000396465
	LOSS [training: 2.8127013355193906 | validation: 2.9679202889335]
	TIME [epoch: 8.35 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8128252776263203		[learning rate: 0.00039553]
	Learning Rate: 0.00039553
	LOSS [training: 2.8128252776263203 | validation: 2.967546002525299]
	TIME [epoch: 8.33 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814452236509211		[learning rate: 0.0003946]
	Learning Rate: 0.000394597
	LOSS [training: 2.814452236509211 | validation: 2.971419324243233]
	TIME [epoch: 8.33 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8142197706749887		[learning rate: 0.00039367]
	Learning Rate: 0.000393666
	LOSS [training: 2.8142197706749887 | validation: 2.971006151995547]
	TIME [epoch: 8.32 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.812972589223354		[learning rate: 0.00039274]
	Learning Rate: 0.000392738
	LOSS [training: 2.812972589223354 | validation: 2.9701814832115403]
	TIME [epoch: 8.33 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814607125018293		[learning rate: 0.00039181]
	Learning Rate: 0.000391811
	LOSS [training: 2.814607125018293 | validation: 2.97079108091396]
	TIME [epoch: 8.36 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813742793172435		[learning rate: 0.00039089]
	Learning Rate: 0.000390887
	LOSS [training: 2.813742793172435 | validation: 2.9687303121795052]
	TIME [epoch: 8.34 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813686135412511		[learning rate: 0.00038997]
	Learning Rate: 0.000389965
	LOSS [training: 2.813686135412511 | validation: 2.971691936965172]
	TIME [epoch: 8.33 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814448951825549		[learning rate: 0.00038905]
	Learning Rate: 0.000389045
	LOSS [training: 2.814448951825549 | validation: 2.9699226882360694]
	TIME [epoch: 8.32 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8123838922554345		[learning rate: 0.00038813]
	Learning Rate: 0.000388127
	LOSS [training: 2.8123838922554345 | validation: 2.9633941138407747]
	TIME [epoch: 8.33 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.806664443180321		[learning rate: 0.00038721]
	Learning Rate: 0.000387212
	LOSS [training: 2.806664443180321 | validation: 2.964424942510118]
	TIME [epoch: 8.32 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8108329020552505		[learning rate: 0.0003863]
	Learning Rate: 0.000386299
	LOSS [training: 2.8108329020552505 | validation: 2.967649109272174]
	TIME [epoch: 8.34 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813014999986189		[learning rate: 0.00038539]
	Learning Rate: 0.000385387
	LOSS [training: 2.813014999986189 | validation: 2.9715862558030928]
	TIME [epoch: 8.34 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813776578027981		[learning rate: 0.00038448]
	Learning Rate: 0.000384478
	LOSS [training: 2.813776578027981 | validation: 2.9698805537238178]
	TIME [epoch: 8.32 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8141026218373093		[learning rate: 0.00038357]
	Learning Rate: 0.000383571
	LOSS [training: 2.8141026218373093 | validation: 2.968063822387491]
	TIME [epoch: 8.32 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813617171779935		[learning rate: 0.00038267]
	Learning Rate: 0.000382667
	LOSS [training: 2.813617171779935 | validation: 2.9684526366642356]
	TIME [epoch: 8.32 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813415444340852		[learning rate: 0.00038176]
	Learning Rate: 0.000381764
	LOSS [training: 2.813415444340852 | validation: 2.9678876153283733]
	TIME [epoch: 8.33 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8129597498257506		[learning rate: 0.00038086]
	Learning Rate: 0.000380863
	LOSS [training: 2.8129597498257506 | validation: 2.970278904194486]
	TIME [epoch: 8.34 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8152700327964357		[learning rate: 0.00037997]
	Learning Rate: 0.000379965
	LOSS [training: 2.8152700327964357 | validation: 2.9728036408658567]
	TIME [epoch: 8.35 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814278894722139		[learning rate: 0.00037907]
	Learning Rate: 0.000379069
	LOSS [training: 2.814278894722139 | validation: 2.9701788519510712]
	TIME [epoch: 8.33 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.815473405425443		[learning rate: 0.00037817]
	Learning Rate: 0.000378175
	LOSS [training: 2.815473405425443 | validation: 2.9699930699361072]
	TIME [epoch: 8.34 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814804901010504		[learning rate: 0.00037728]
	Learning Rate: 0.000377283
	LOSS [training: 2.814804901010504 | validation: 2.970266992463837]
	TIME [epoch: 8.32 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8138235694868863		[learning rate: 0.00037639]
	Learning Rate: 0.000376393
	LOSS [training: 2.8138235694868863 | validation: 2.9691221002152917]
	TIME [epoch: 8.33 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8126538710044597		[learning rate: 0.0003755]
	Learning Rate: 0.000375505
	LOSS [training: 2.8126538710044597 | validation: 2.9681741054057227]
	TIME [epoch: 8.32 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8105157251688286		[learning rate: 0.00037462]
	Learning Rate: 0.000374619
	LOSS [training: 2.8105157251688286 | validation: 2.966009442813435]
	TIME [epoch: 8.37 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8087590783312693		[learning rate: 0.00037374]
	Learning Rate: 0.000373735
	LOSS [training: 2.8087590783312693 | validation: 2.9650818088356212]
	TIME [epoch: 8.32 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.809219924742825		[learning rate: 0.00037285]
	Learning Rate: 0.000372854
	LOSS [training: 2.809219924742825 | validation: 2.9655981998208993]
	TIME [epoch: 8.32 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8098264322811346		[learning rate: 0.00037197]
	Learning Rate: 0.000371974
	LOSS [training: 2.8098264322811346 | validation: 2.9624847390448634]
	TIME [epoch: 8.31 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8093652361864407		[learning rate: 0.0003711]
	Learning Rate: 0.000371097
	LOSS [training: 2.8093652361864407 | validation: 2.9655252082191805]
	TIME [epoch: 8.33 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.808307980550732		[learning rate: 0.00037022]
	Learning Rate: 0.000370221
	LOSS [training: 2.808307980550732 | validation: 2.9625165745073616]
	TIME [epoch: 8.33 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8072575551529626		[learning rate: 0.00036935]
	Learning Rate: 0.000369348
	LOSS [training: 2.8072575551529626 | validation: 2.9628737018686637]
	TIME [epoch: 8.36 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8088756587812846		[learning rate: 0.00036848]
	Learning Rate: 0.000368477
	LOSS [training: 2.8088756587812846 | validation: 2.963974429759131]
	TIME [epoch: 8.32 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8089314455234615		[learning rate: 0.00036761]
	Learning Rate: 0.000367608
	LOSS [training: 2.8089314455234615 | validation: 2.9625787741302703]
	TIME [epoch: 8.33 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8065432337716185		[learning rate: 0.00036674]
	Learning Rate: 0.000366741
	LOSS [training: 2.8065432337716185 | validation: 2.9617268110518555]
	TIME [epoch: 8.32 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8060726698798035		[learning rate: 0.00036588]
	Learning Rate: 0.000365876
	LOSS [training: 2.8060726698798035 | validation: 2.9586125175970226]
	TIME [epoch: 8.33 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8069902079107365		[learning rate: 0.00036501]
	Learning Rate: 0.000365012
	LOSS [training: 2.8069902079107365 | validation: 2.9612859331019266]
	TIME [epoch: 8.34 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8089904858180463		[learning rate: 0.00036415]
	Learning Rate: 0.000364152
	LOSS [training: 2.8089904858180463 | validation: 2.9656484868110704]
	TIME [epoch: 8.35 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8113398723195218		[learning rate: 0.00036329]
	Learning Rate: 0.000363293
	LOSS [training: 2.8113398723195218 | validation: 2.9662011103926185]
	TIME [epoch: 8.34 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8113199078828353		[learning rate: 0.00036244]
	Learning Rate: 0.000362436
	LOSS [training: 2.8113199078828353 | validation: 2.9679725751300303]
	TIME [epoch: 8.33 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8118196762715524		[learning rate: 0.00036158]
	Learning Rate: 0.000361581
	LOSS [training: 2.8118196762715524 | validation: 2.9689443158532516]
	TIME [epoch: 8.32 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813083087687052		[learning rate: 0.00036073]
	Learning Rate: 0.000360728
	LOSS [training: 2.813083087687052 | validation: 2.9668155623946335]
	TIME [epoch: 8.32 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.81183560648444		[learning rate: 0.00035988]
	Learning Rate: 0.000359877
	LOSS [training: 2.81183560648444 | validation: 2.9672549342866086]
	TIME [epoch: 8.32 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8120332021343835		[learning rate: 0.00035903]
	Learning Rate: 0.000359028
	LOSS [training: 2.8120332021343835 | validation: 2.9663678879261135]
	TIME [epoch: 8.36 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.812364476377555		[learning rate: 0.00035818]
	Learning Rate: 0.000358181
	LOSS [training: 2.812364476377555 | validation: 2.967144795367424]
	TIME [epoch: 8.34 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8136925234654866		[learning rate: 0.00035734]
	Learning Rate: 0.000357336
	LOSS [training: 2.8136925234654866 | validation: 2.968247769332913]
	TIME [epoch: 8.31 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.81353444001136		[learning rate: 0.00035649]
	Learning Rate: 0.000356493
	LOSS [training: 2.81353444001136 | validation: 2.9676017392450564]
	TIME [epoch: 8.33 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8121635636975166		[learning rate: 0.00035565]
	Learning Rate: 0.000355652
	LOSS [training: 2.8121635636975166 | validation: 2.9661895811865646]
	TIME [epoch: 8.32 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8109905877855583		[learning rate: 0.00035481]
	Learning Rate: 0.000354813
	LOSS [training: 2.8109905877855583 | validation: 2.9657618031158797]
	TIME [epoch: 8.31 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.811977336376551		[learning rate: 0.00035398]
	Learning Rate: 0.000353976
	LOSS [training: 2.811977336376551 | validation: 2.968797361487823]
	TIME [epoch: 8.34 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.810027183383985		[learning rate: 0.00035314]
	Learning Rate: 0.000353141
	LOSS [training: 2.810027183383985 | validation: 2.962636909120768]
	TIME [epoch: 8.36 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.808428935523124		[learning rate: 0.00035231]
	Learning Rate: 0.000352309
	LOSS [training: 2.808428935523124 | validation: 2.965250918938522]
	TIME [epoch: 8.32 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8094676067065336		[learning rate: 0.00035148]
	Learning Rate: 0.000351477
	LOSS [training: 2.8094676067065336 | validation: 2.964286898571742]
	TIME [epoch: 8.34 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.807599691787739		[learning rate: 0.00035065]
	Learning Rate: 0.000350648
	LOSS [training: 2.807599691787739 | validation: 2.9625433475943908]
	TIME [epoch: 8.33 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.806431841977212		[learning rate: 0.00034982]
	Learning Rate: 0.000349821
	LOSS [training: 2.806431841977212 | validation: 2.961670653166353]
	TIME [epoch: 8.32 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8059504124461743		[learning rate: 0.000349]
	Learning Rate: 0.000348996
	LOSS [training: 2.8059504124461743 | validation: 2.959843357780568]
	TIME [epoch: 8.35 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8051996240359345		[learning rate: 0.00034817]
	Learning Rate: 0.000348173
	LOSS [training: 2.8051996240359345 | validation: 2.9611544597561315]
	TIME [epoch: 8.38 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8048813048476466		[learning rate: 0.00034735]
	Learning Rate: 0.000347352
	LOSS [training: 2.8048813048476466 | validation: 2.959418447165637]
	TIME [epoch: 8.34 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.804122059751826		[learning rate: 0.00034653]
	Learning Rate: 0.000346532
	LOSS [training: 2.804122059751826 | validation: 2.958993395904235]
	TIME [epoch: 8.34 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8046395030587625		[learning rate: 0.00034571]
	Learning Rate: 0.000345715
	LOSS [training: 2.8046395030587625 | validation: 2.9614447735133336]
	TIME [epoch: 8.32 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.807139831524237		[learning rate: 0.0003449]
	Learning Rate: 0.000344899
	LOSS [training: 2.807139831524237 | validation: 2.965825895115796]
	TIME [epoch: 8.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.810357526796594		[learning rate: 0.00034409]
	Learning Rate: 0.000344086
	LOSS [training: 2.810357526796594 | validation: 2.9656344091567592]
	TIME [epoch: 8.33 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8112793258883197		[learning rate: 0.00034327]
	Learning Rate: 0.000343274
	LOSS [training: 2.8112793258883197 | validation: 2.965804020129638]
	TIME [epoch: 8.38 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.80937595134367		[learning rate: 0.00034246]
	Learning Rate: 0.000342464
	LOSS [training: 2.80937595134367 | validation: 2.962969058084263]
	TIME [epoch: 8.33 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8098097166725995		[learning rate: 0.00034166]
	Learning Rate: 0.000341657
	LOSS [training: 2.8098097166725995 | validation: 2.9662172174085053]
	TIME [epoch: 8.32 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8104217653060806		[learning rate: 0.00034085]
	Learning Rate: 0.000340851
	LOSS [training: 2.8104217653060806 | validation: 2.966974045073015]
	TIME [epoch: 8.33 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8095057416515257		[learning rate: 0.00034005]
	Learning Rate: 0.000340047
	LOSS [training: 2.8095057416515257 | validation: 2.9640293233314674]
	TIME [epoch: 8.32 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8095915943509473		[learning rate: 0.00033924]
	Learning Rate: 0.000339245
	LOSS [training: 2.8095915943509473 | validation: 2.9642904364252405]
	TIME [epoch: 8.34 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8078479132314555		[learning rate: 0.00033844]
	Learning Rate: 0.000338444
	LOSS [training: 2.8078479132314555 | validation: 2.9625961479066927]
	TIME [epoch: 8.36 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8074287625779113		[learning rate: 0.00033765]
	Learning Rate: 0.000337646
	LOSS [training: 2.8074287625779113 | validation: 2.963090471521184]
	TIME [epoch: 8.35 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.810727765267047		[learning rate: 0.00033685]
	Learning Rate: 0.00033685
	LOSS [training: 2.810727765267047 | validation: 2.9688876352438562]
	TIME [epoch: 8.33 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813679329335584		[learning rate: 0.00033605]
	Learning Rate: 0.000336055
	LOSS [training: 2.813679329335584 | validation: 2.9681725899542]
	TIME [epoch: 8.32 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814468170283794		[learning rate: 0.00033526]
	Learning Rate: 0.000335262
	LOSS [training: 2.814468170283794 | validation: 2.96774752389054]
	TIME [epoch: 8.32 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8119298435395295		[learning rate: 0.00033447]
	Learning Rate: 0.000334471
	LOSS [training: 2.8119298435395295 | validation: 2.9629542802080655]
	TIME [epoch: 8.34 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8105820571032702		[learning rate: 0.00033368]
	Learning Rate: 0.000333682
	LOSS [training: 2.8105820571032702 | validation: 2.9674105908586474]
	TIME [epoch: 8.36 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8137763043286794		[learning rate: 0.0003329]
	Learning Rate: 0.000332895
	LOSS [training: 2.8137763043286794 | validation: 2.9701516117887667]
	TIME [epoch: 8.35 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8158968547031176		[learning rate: 0.00033211]
	Learning Rate: 0.00033211
	LOSS [training: 2.8158968547031176 | validation: 2.968815630402294]
	TIME [epoch: 8.33 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814149475273772		[learning rate: 0.00033133]
	Learning Rate: 0.000331327
	LOSS [training: 2.814149475273772 | validation: 2.967902383177773]
	TIME [epoch: 8.32 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.812034116083229		[learning rate: 0.00033055]
	Learning Rate: 0.000330545
	LOSS [training: 2.812034116083229 | validation: 2.9659775136677795]
	TIME [epoch: 8.32 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.811750752957734		[learning rate: 0.00032977]
	Learning Rate: 0.000329765
	LOSS [training: 2.811750752957734 | validation: 2.9700875838657748]
	TIME [epoch: 8.31 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8152238888804417		[learning rate: 0.00032899]
	Learning Rate: 0.000328988
	LOSS [training: 2.8152238888804417 | validation: 2.971782931215377]
	TIME [epoch: 8.34 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.817143790454665		[learning rate: 0.00032821]
	Learning Rate: 0.000328212
	LOSS [training: 2.817143790454665 | validation: 2.972641623140417]
	TIME [epoch: 8.36 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.819315403869954		[learning rate: 0.00032744]
	Learning Rate: 0.000327437
	LOSS [training: 2.819315403869954 | validation: 2.973777454043053]
	TIME [epoch: 8.32 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8189984111398942		[learning rate: 0.00032666]
	Learning Rate: 0.000326665
	LOSS [training: 2.8189984111398942 | validation: 2.97124146399321]
	TIME [epoch: 8.32 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8175735305180334		[learning rate: 0.00032589]
	Learning Rate: 0.000325894
	LOSS [training: 2.8175735305180334 | validation: 2.971418537767704]
	TIME [epoch: 8.34 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8177136852169866		[learning rate: 0.00032513]
	Learning Rate: 0.000325126
	LOSS [training: 2.8177136852169866 | validation: 2.9707462182467292]
	TIME [epoch: 8.33 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.815424408521246		[learning rate: 0.00032436]
	Learning Rate: 0.000324359
	LOSS [training: 2.815424408521246 | validation: 2.9697051926430653]
	TIME [epoch: 8.35 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.815745881164053		[learning rate: 0.00032359]
	Learning Rate: 0.000323594
	LOSS [training: 2.815745881164053 | validation: 2.971961224358196]
	TIME [epoch: 8.37 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.817670997170273		[learning rate: 0.00032283]
	Learning Rate: 0.00032283
	LOSS [training: 2.817670997170273 | validation: 2.9725604222527204]
	TIME [epoch: 8.34 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.820123330068788		[learning rate: 0.00032207]
	Learning Rate: 0.000322069
	LOSS [training: 2.820123330068788 | validation: 2.9726152018101946]
	TIME [epoch: 8.33 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.820174811637909		[learning rate: 0.00032131]
	Learning Rate: 0.000321309
	LOSS [training: 2.820174811637909 | validation: 2.9768551830570327]
	TIME [epoch: 8.32 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8220608154168336		[learning rate: 0.00032055]
	Learning Rate: 0.000320551
	LOSS [training: 2.8220608154168336 | validation: 2.9777152091313965]
	TIME [epoch: 8.35 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.822965760591186		[learning rate: 0.0003198]
	Learning Rate: 0.000319795
	LOSS [training: 2.822965760591186 | validation: 2.9781927658437675]
	TIME [epoch: 8.35 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82236402941375		[learning rate: 0.00031904]
	Learning Rate: 0.000319041
	LOSS [training: 2.82236402941375 | validation: 2.978350677822524]
	TIME [epoch: 8.38 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.821979986344471		[learning rate: 0.00031829]
	Learning Rate: 0.000318288
	LOSS [training: 2.821979986344471 | validation: 2.979804194291012]
	TIME [epoch: 8.33 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8233035315325115		[learning rate: 0.00031754]
	Learning Rate: 0.000317537
	LOSS [training: 2.8233035315325115 | validation: 2.9813054411873585]
	TIME [epoch: 8.34 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8230234189514496		[learning rate: 0.00031679]
	Learning Rate: 0.000316788
	LOSS [training: 2.8230234189514496 | validation: 2.9779362894177233]
	TIME [epoch: 8.34 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8238110903850453		[learning rate: 0.00031604]
	Learning Rate: 0.000316041
	LOSS [training: 2.8238110903850453 | validation: 2.9775500011043357]
	TIME [epoch: 8.32 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241610957311227		[learning rate: 0.0003153]
	Learning Rate: 0.000315296
	LOSS [training: 2.8241610957311227 | validation: 2.981821055588792]
	TIME [epoch: 8.34 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8240853566527395		[learning rate: 0.00031455]
	Learning Rate: 0.000314552
	LOSS [training: 2.8240853566527395 | validation: 2.978369331008288]
	TIME [epoch: 8.38 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8227742535029128		[learning rate: 0.00031381]
	Learning Rate: 0.00031381
	LOSS [training: 2.8227742535029128 | validation: 2.978028816172235]
	TIME [epoch: 8.33 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823479914736792		[learning rate: 0.00031307]
	Learning Rate: 0.00031307
	LOSS [training: 2.823479914736792 | validation: 2.9808869541308534]
	TIME [epoch: 8.34 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8246964262187366		[learning rate: 0.00031233]
	Learning Rate: 0.000312331
	LOSS [training: 2.8246964262187366 | validation: 2.9792094513438614]
	TIME [epoch: 8.42 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8238159487687113		[learning rate: 0.00031159]
	Learning Rate: 0.000311594
	LOSS [training: 2.8238159487687113 | validation: 2.9791883205043104]
	TIME [epoch: 8.34 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8234578047629117		[learning rate: 0.00031086]
	Learning Rate: 0.00031086
	LOSS [training: 2.8234578047629117 | validation: 2.979153073359492]
	TIME [epoch: 8.34 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824116947403848		[learning rate: 0.00031013]
	Learning Rate: 0.000310126
	LOSS [training: 2.824116947403848 | validation: 2.978521434670372]
	TIME [epoch: 8.38 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241804876884142		[learning rate: 0.00030939]
	Learning Rate: 0.000309395
	LOSS [training: 2.8241804876884142 | validation: 2.9795765512208474]
	TIME [epoch: 8.33 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823313303168176		[learning rate: 0.00030866]
	Learning Rate: 0.000308665
	LOSS [training: 2.823313303168176 | validation: 2.9764764005666082]
	TIME [epoch: 8.33 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824189803833571		[learning rate: 0.00030794]
	Learning Rate: 0.000307937
	LOSS [training: 2.824189803833571 | validation: 2.9806089369560915]
	TIME [epoch: 8.33 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241004738579853		[learning rate: 0.00030721]
	Learning Rate: 0.00030721
	LOSS [training: 2.8241004738579853 | validation: 2.979668083254687]
	TIME [epoch: 8.32 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82474027276103		[learning rate: 0.00030649]
	Learning Rate: 0.000306486
	LOSS [training: 2.82474027276103 | validation: 2.9802414904296626]
	TIME [epoch: 8.34 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8255193388684856		[learning rate: 0.00030576]
	Learning Rate: 0.000305763
	LOSS [training: 2.8255193388684856 | validation: 2.980301069065194]
	TIME [epoch: 8.36 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825436562821713		[learning rate: 0.00030504]
	Learning Rate: 0.000305042
	LOSS [training: 2.825436562821713 | validation: 2.982168073202648]
	TIME [epoch: 8.34 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8261329550075796		[learning rate: 0.00030432]
	Learning Rate: 0.000304322
	LOSS [training: 2.8261329550075796 | validation: 2.9823105417549343]
	TIME [epoch: 8.32 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8260070360694813		[learning rate: 0.0003036]
	Learning Rate: 0.000303604
	LOSS [training: 2.8260070360694813 | validation: 2.9816817581600414]
	TIME [epoch: 8.33 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8260428848959176		[learning rate: 0.00030289]
	Learning Rate: 0.000302888
	LOSS [training: 2.8260428848959176 | validation: 2.980944615480515]
	TIME [epoch: 8.32 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258582596418624		[learning rate: 0.00030217]
	Learning Rate: 0.000302174
	LOSS [training: 2.8258582596418624 | validation: 2.9826000181918992]
	TIME [epoch: 8.33 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827713632207205		[learning rate: 0.00030146]
	Learning Rate: 0.000301461
	LOSS [training: 2.827713632207205 | validation: 2.984608745386084]
	TIME [epoch: 8.35 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827314114706847		[learning rate: 0.00030075]
	Learning Rate: 0.00030075
	LOSS [training: 2.827314114706847 | validation: 2.9826866230851046]
	TIME [epoch: 8.35 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82613967608809		[learning rate: 0.00030004]
	Learning Rate: 0.00030004
	LOSS [training: 2.82613967608809 | validation: 2.9810793249142886]
	TIME [epoch: 8.32 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827262606817563		[learning rate: 0.00029933]
	Learning Rate: 0.000299332
	LOSS [training: 2.827262606817563 | validation: 2.982264191754227]
	TIME [epoch: 8.32 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8281157135663917		[learning rate: 0.00029863]
	Learning Rate: 0.000298626
	LOSS [training: 2.8281157135663917 | validation: 2.9835736377270576]
	TIME [epoch: 8.33 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8279609639054835		[learning rate: 0.00029792]
	Learning Rate: 0.000297922
	LOSS [training: 2.8279609639054835 | validation: 2.9829239738844695]
	TIME [epoch: 8.34 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8278189077566944		[learning rate: 0.00029722]
	Learning Rate: 0.000297219
	LOSS [training: 2.8278189077566944 | validation: 2.9826187120454986]
	TIME [epoch: 8.34 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826556277887108		[learning rate: 0.00029652]
	Learning Rate: 0.000296518
	LOSS [training: 2.826556277887108 | validation: 2.98493916229425]
	TIME [epoch: 8.36 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8269925256845347		[learning rate: 0.00029582]
	Learning Rate: 0.000295819
	LOSS [training: 2.8269925256845347 | validation: 2.982494765368454]
	TIME [epoch: 8.32 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826796995339032		[learning rate: 0.00029512]
	Learning Rate: 0.000295121
	LOSS [training: 2.826796995339032 | validation: 2.985187044151339]
	TIME [epoch: 8.33 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8280227182313276		[learning rate: 0.00029442]
	Learning Rate: 0.000294425
	LOSS [training: 2.8280227182313276 | validation: 2.983121671884203]
	TIME [epoch: 8.33 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8272644141372267		[learning rate: 0.00029373]
	Learning Rate: 0.00029373
	LOSS [training: 2.8272644141372267 | validation: 2.982929399978385]
	TIME [epoch: 8.33 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826167003569636		[learning rate: 0.00029304]
	Learning Rate: 0.000293037
	LOSS [training: 2.826167003569636 | validation: 2.9835838361192293]
	TIME [epoch: 8.34 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8274385528026595		[learning rate: 0.00029235]
	Learning Rate: 0.000292346
	LOSS [training: 2.8274385528026595 | validation: 2.98242596857892]
	TIME [epoch: 8.37 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8266449711238826		[learning rate: 0.00029166]
	Learning Rate: 0.000291657
	LOSS [training: 2.8266449711238826 | validation: 2.9843754843521397]
	TIME [epoch: 8.34 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8255173164842335		[learning rate: 0.00029097]
	Learning Rate: 0.000290969
	LOSS [training: 2.8255173164842335 | validation: 2.9813768678733075]
	TIME [epoch: 8.33 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82405259063931		[learning rate: 0.00029028]
	Learning Rate: 0.000290282
	LOSS [training: 2.82405259063931 | validation: 2.981045983899304]
	TIME [epoch: 8.34 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8245023907227704		[learning rate: 0.0002896]
	Learning Rate: 0.000289598
	LOSS [training: 2.8245023907227704 | validation: 2.982451473132184]
	TIME [epoch: 8.31 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8261844483760283		[learning rate: 0.00028891]
	Learning Rate: 0.000288914
	LOSS [training: 2.8261844483760283 | validation: 2.9814433724687683]
	TIME [epoch: 8.34 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825872203938563		[learning rate: 0.00028823]
	Learning Rate: 0.000288233
	LOSS [training: 2.825872203938563 | validation: 2.9833982242668133]
	TIME [epoch: 8.37 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826032257679641		[learning rate: 0.00028755]
	Learning Rate: 0.000287553
	LOSS [training: 2.826032257679641 | validation: 2.9809368675068195]
	TIME [epoch: 8.32 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258923380753664		[learning rate: 0.00028687]
	Learning Rate: 0.000286875
	LOSS [training: 2.8258923380753664 | validation: 2.9821983815803916]
	TIME [epoch: 8.33 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8262266251832004		[learning rate: 0.0002862]
	Learning Rate: 0.000286198
	LOSS [training: 2.8262266251832004 | validation: 2.982610886184568]
	TIME [epoch: 8.33 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8255869456981904		[learning rate: 0.00028552]
	Learning Rate: 0.000285523
	LOSS [training: 2.8255869456981904 | validation: 2.9804956862598004]
	TIME [epoch: 8.32 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8264833890417878		[learning rate: 0.00028485]
	Learning Rate: 0.000284849
	LOSS [training: 2.8264833890417878 | validation: 2.9848445314206478]
	TIME [epoch: 8.33 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8257504596191168		[learning rate: 0.00028418]
	Learning Rate: 0.000284178
	LOSS [training: 2.8257504596191168 | validation: 2.9825906877731843]
	TIME [epoch: 8.36 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8260953662274533		[learning rate: 0.00028351]
	Learning Rate: 0.000283507
	LOSS [training: 2.8260953662274533 | validation: 2.9832709397668733]
	TIME [epoch: 8.33 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8263496543054725		[learning rate: 0.00028284]
	Learning Rate: 0.000282839
	LOSS [training: 2.8263496543054725 | validation: 2.98312884458184]
	TIME [epoch: 8.32 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8268842912848293		[learning rate: 0.00028217]
	Learning Rate: 0.000282171
	LOSS [training: 2.8268842912848293 | validation: 2.983471684426772]
	TIME [epoch: 8.32 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8267238891952218		[learning rate: 0.00028151]
	Learning Rate: 0.000281506
	LOSS [training: 2.8267238891952218 | validation: 2.984466152175565]
	TIME [epoch: 8.32 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826963796943852		[learning rate: 0.00028084]
	Learning Rate: 0.000280842
	LOSS [training: 2.826963796943852 | validation: 2.982751068406996]
	TIME [epoch: 8.34 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.82715415190962		[learning rate: 0.00028018]
	Learning Rate: 0.000280179
	LOSS [training: 2.82715415190962 | validation: 2.9866311977632956]
	TIME [epoch: 8.36 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826661293292023		[learning rate: 0.00027952]
	Learning Rate: 0.000279518
	LOSS [training: 2.826661293292023 | validation: 2.983539725401113]
	TIME [epoch: 8.35 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8274947812945204		[learning rate: 0.00027886]
	Learning Rate: 0.000278859
	LOSS [training: 2.8274947812945204 | validation: 2.9858680651029132]
	TIME [epoch: 8.32 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827130926129507		[learning rate: 0.0002782]
	Learning Rate: 0.000278201
	LOSS [training: 2.827130926129507 | validation: 2.982670908962777]
	TIME [epoch: 8.32 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.827172153389721		[learning rate: 0.00027755]
	Learning Rate: 0.000277545
	LOSS [training: 2.827172153389721 | validation: 2.983344708876653]
	TIME [epoch: 8.33 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8264999754146274		[learning rate: 0.00027689]
	Learning Rate: 0.00027689
	LOSS [training: 2.8264999754146274 | validation: 2.98327949325407]
	TIME [epoch: 8.34 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826134605503124		[learning rate: 0.00027624]
	Learning Rate: 0.000276237
	LOSS [training: 2.826134605503124 | validation: 2.980467575534273]
	TIME [epoch: 8.34 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824762881715708		[learning rate: 0.00027559]
	Learning Rate: 0.000275586
	LOSS [training: 2.824762881715708 | validation: 2.983830278914608]
	TIME [epoch: 8.36 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.824135940247146		[learning rate: 0.00027494]
	Learning Rate: 0.000274935
	LOSS [training: 2.824135940247146 | validation: 2.9800246753174546]
	TIME [epoch: 8.33 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823837031387594		[learning rate: 0.00027429]
	Learning Rate: 0.000274287
	LOSS [training: 2.823837031387594 | validation: 2.978910289468968]
	TIME [epoch: 8.33 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8235462680737102		[learning rate: 0.00027364]
	Learning Rate: 0.00027364
	LOSS [training: 2.8235462680737102 | validation: 2.9809144889550936]
	TIME [epoch: 8.33 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.823087619744836		[learning rate: 0.00027299]
	Learning Rate: 0.000272994
	LOSS [training: 2.823087619744836 | validation: 2.9838680199599406]
	TIME [epoch: 8.34 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8236938533116827		[learning rate: 0.00027235]
	Learning Rate: 0.000272351
	LOSS [training: 2.8236938533116827 | validation: 2.9826182306932245]
	TIME [epoch: 8.36 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825860421949613		[learning rate: 0.00027171]
	Learning Rate: 0.000271708
	LOSS [training: 2.825860421949613 | validation: 2.9819962757572522]
	TIME [epoch: 8.37 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.826660765829984		[learning rate: 0.00027107]
	Learning Rate: 0.000271067
	LOSS [training: 2.826660765829984 | validation: 2.9832721121200834]
	TIME [epoch: 8.34 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258106246753325		[learning rate: 0.00027043]
	Learning Rate: 0.000270428
	LOSS [training: 2.8258106246753325 | validation: 2.985367833878727]
	TIME [epoch: 8.33 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8268966362916403		[learning rate: 0.00026979]
	Learning Rate: 0.00026979
	LOSS [training: 2.8268966362916403 | validation: 2.983845998013212]
	TIME [epoch: 8.33 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8278125805637706		[learning rate: 0.00026915]
	Learning Rate: 0.000269154
	LOSS [training: 2.8278125805637706 | validation: 2.9833433830773224]
	TIME [epoch: 8.33 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8278402486703014		[learning rate: 0.00026852]
	Learning Rate: 0.000268519
	LOSS [training: 2.8278402486703014 | validation: 2.9860050926797586]
	TIME [epoch: 8.34 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828426517002691		[learning rate: 0.00026789]
	Learning Rate: 0.000267885
	LOSS [training: 2.828426517002691 | validation: 2.98631175865411]
	TIME [epoch: 8.38 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8284936065536863		[learning rate: 0.00026725]
	Learning Rate: 0.000267253
	LOSS [training: 2.8284936065536863 | validation: 2.985645868263305]
	TIME [epoch: 8.34 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828221807442964		[learning rate: 0.00026662]
	Learning Rate: 0.000266623
	LOSS [training: 2.828221807442964 | validation: 2.986930638142197]
	TIME [epoch: 8.34 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8288295094593825		[learning rate: 0.00026599]
	Learning Rate: 0.000265994
	LOSS [training: 2.8288295094593825 | validation: 2.9841023993820173]
	TIME [epoch: 8.34 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8285363717327976		[learning rate: 0.00026537]
	Learning Rate: 0.000265367
	LOSS [training: 2.8285363717327976 | validation: 2.9833623006321406]
	TIME [epoch: 8.33 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8288026513005584		[learning rate: 0.00026474]
	Learning Rate: 0.000264741
	LOSS [training: 2.8288026513005584 | validation: 2.984261270579903]
	TIME [epoch: 8.35 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828922693249501		[learning rate: 0.00026412]
	Learning Rate: 0.000264116
	LOSS [training: 2.828922693249501 | validation: 2.9886978817573016]
	TIME [epoch: 8.37 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828893635319153		[learning rate: 0.00026349]
	Learning Rate: 0.000263493
	LOSS [training: 2.828893635319153 | validation: 2.986747030884759]
	TIME [epoch: 8.34 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8286344092585494		[learning rate: 0.00026287]
	Learning Rate: 0.000262872
	LOSS [training: 2.8286344092585494 | validation: 2.989665352207695]
	TIME [epoch: 8.33 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.830260929488426		[learning rate: 0.00026225]
	Learning Rate: 0.000262252
	LOSS [training: 2.830260929488426 | validation: 2.9883583833269154]
	TIME [epoch: 8.33 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8305138475888194		[learning rate: 0.00026163]
	Learning Rate: 0.000261633
	LOSS [training: 2.8305138475888194 | validation: 2.986575416313189]
	TIME [epoch: 8.32 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8292103906317942		[learning rate: 0.00026102]
	Learning Rate: 0.000261016
	LOSS [training: 2.8292103906317942 | validation: 2.9859064849423826]
	TIME [epoch: 8.34 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8292953294365075		[learning rate: 0.0002604]
	Learning Rate: 0.0002604
	LOSS [training: 2.8292953294365075 | validation: 2.9882252757062453]
	TIME [epoch: 8.37 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8288168112246317		[learning rate: 0.00025979]
	Learning Rate: 0.000259786
	LOSS [training: 2.8288168112246317 | validation: 2.9879942786270135]
	TIME [epoch: 8.35 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.830264974842585		[learning rate: 0.00025917]
	Learning Rate: 0.000259173
	LOSS [training: 2.830264974842585 | validation: 2.9872871943582853]
	TIME [epoch: 8.32 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8304963665223095		[learning rate: 0.00025856]
	Learning Rate: 0.000258562
	LOSS [training: 2.8304963665223095 | validation: 2.987212064584913]
	TIME [epoch: 8.33 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8308126198815846		[learning rate: 0.00025795]
	Learning Rate: 0.000257952
	LOSS [training: 2.8308126198815846 | validation: 2.989503507335261]
	TIME [epoch: 8.32 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.829864653603723		[learning rate: 0.00025734]
	Learning Rate: 0.000257343
	LOSS [training: 2.829864653603723 | validation: 2.986707785001565]
	TIME [epoch: 8.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.830443629030365		[learning rate: 0.00025674]
	Learning Rate: 0.000256736
	LOSS [training: 2.830443629030365 | validation: 2.986442227970117]
	TIME [epoch: 8.36 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8298163479956253		[learning rate: 0.00025613]
	Learning Rate: 0.000256131
	LOSS [training: 2.8298163479956253 | validation: 2.988046407201717]
	TIME [epoch: 8.34 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.830592260828819		[learning rate: 0.00025553]
	Learning Rate: 0.000255527
	LOSS [training: 2.830592260828819 | validation: 2.985889315843804]
	TIME [epoch: 8.31 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.831962631380881		[learning rate: 0.00025492]
	Learning Rate: 0.000254924
	LOSS [training: 2.831962631380881 | validation: 2.9899548005797496]
	TIME [epoch: 8.33 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832080522301587		[learning rate: 0.00025432]
	Learning Rate: 0.000254322
	LOSS [training: 2.832080522301587 | validation: 2.9914438262185064]
	TIME [epoch: 8.34 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833080240803285		[learning rate: 0.00025372]
	Learning Rate: 0.000253723
	LOSS [training: 2.833080240803285 | validation: 2.9884644915752077]
	TIME [epoch: 8.34 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832879887239468		[learning rate: 0.00025312]
	Learning Rate: 0.000253124
	LOSS [training: 2.832879887239468 | validation: 2.9904869069999718]
	TIME [epoch: 8.34 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833970663576359		[learning rate: 0.00025253]
	Learning Rate: 0.000252527
	LOSS [training: 2.833970663576359 | validation: 2.991739598018509]
	TIME [epoch: 8.35 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832549684373647		[learning rate: 0.00025193]
	Learning Rate: 0.000251931
	LOSS [training: 2.832549684373647 | validation: 2.992431752533494]
	TIME [epoch: 8.32 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833421782821538		[learning rate: 0.00025134]
	Learning Rate: 0.000251337
	LOSS [training: 2.833421782821538 | validation: 2.9925438796485144]
	TIME [epoch: 8.33 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8334711230425316		[learning rate: 0.00025074]
	Learning Rate: 0.000250744
	LOSS [training: 2.8334711230425316 | validation: 2.992884404999148]
	TIME [epoch: 8.34 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8349753813049636		[learning rate: 0.00025015]
	Learning Rate: 0.000250153
	LOSS [training: 2.8349753813049636 | validation: 2.9943251323549473]
	TIME [epoch: 8.32 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8355291207614925		[learning rate: 0.00024956]
	Learning Rate: 0.000249563
	LOSS [training: 2.8355291207614925 | validation: 2.9903797682173052]
	TIME [epoch: 8.32 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8344569099159154		[learning rate: 0.00024897]
	Learning Rate: 0.000248974
	LOSS [training: 2.8344569099159154 | validation: 2.9923553171115054]
	TIME [epoch: 8.38 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833574218846292		[learning rate: 0.00024839]
	Learning Rate: 0.000248387
	LOSS [training: 2.833574218846292 | validation: 2.9929638971944175]
	TIME [epoch: 8.33 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8344062280173343		[learning rate: 0.0002478]
	Learning Rate: 0.000247801
	LOSS [training: 2.8344062280173343 | validation: 2.995002312370417]
	TIME [epoch: 8.32 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8358295051022036		[learning rate: 0.00024722]
	Learning Rate: 0.000247216
	LOSS [training: 2.8358295051022036 | validation: 2.991855797460362]
	TIME [epoch: 8.32 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8368503348763303		[learning rate: 0.00024663]
	Learning Rate: 0.000246633
	LOSS [training: 2.8368503348763303 | validation: 2.9965267279507963]
	TIME [epoch: 8.34 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837203139780715		[learning rate: 0.00024605]
	Learning Rate: 0.000246051
	LOSS [training: 2.837203139780715 | validation: 2.9949636015722456]
	TIME [epoch: 8.35 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8365158272461377		[learning rate: 0.00024547]
	Learning Rate: 0.000245471
	LOSS [training: 2.8365158272461377 | validation: 2.997039384805933]
	TIME [epoch: 8.36 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8380765261663203		[learning rate: 0.00024489]
	Learning Rate: 0.000244892
	LOSS [training: 2.8380765261663203 | validation: 2.9962847432419855]
	TIME [epoch: 8.34 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8394461443916237		[learning rate: 0.00024431]
	Learning Rate: 0.000244314
	LOSS [training: 2.8394461443916237 | validation: 2.9988919239242473]
	TIME [epoch: 8.33 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8388382359771396		[learning rate: 0.00024374]
	Learning Rate: 0.000243738
	LOSS [training: 2.8388382359771396 | validation: 2.995868237077339]
	TIME [epoch: 8.33 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8393183040083803		[learning rate: 0.00024316]
	Learning Rate: 0.000243163
	LOSS [training: 2.8393183040083803 | validation: 2.996733780873529]
	TIME [epoch: 8.34 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838176774510823		[learning rate: 0.00024259]
	Learning Rate: 0.000242589
	LOSS [training: 2.838176774510823 | validation: 2.995422586533718]
	TIME [epoch: 8.33 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838171872863452		[learning rate: 0.00024202]
	Learning Rate: 0.000242017
	LOSS [training: 2.838171872863452 | validation: 2.997307558628308]
	TIME [epoch: 8.37 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.837875576020375		[learning rate: 0.00024145]
	Learning Rate: 0.000241446
	LOSS [training: 2.837875576020375 | validation: 2.995327406345817]
	TIME [epoch: 8.36 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838978481107716		[learning rate: 0.00024088]
	Learning Rate: 0.000240877
	LOSS [training: 2.838978481107716 | validation: 2.9974318172728243]
	TIME [epoch: 8.34 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8399854538840037		[learning rate: 0.00024031]
	Learning Rate: 0.000240309
	LOSS [training: 2.8399854538840037 | validation: 2.99908149300959]
	TIME [epoch: 8.33 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840489940337112		[learning rate: 0.00023974]
	Learning Rate: 0.000239742
	LOSS [training: 2.840489940337112 | validation: 3.000312212316702]
	TIME [epoch: 8.33 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8413143221513057		[learning rate: 0.00023918]
	Learning Rate: 0.000239176
	LOSS [training: 2.8413143221513057 | validation: 2.9988640711010977]
	TIME [epoch: 8.32 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84167291901542		[learning rate: 0.00023861]
	Learning Rate: 0.000238612
	LOSS [training: 2.84167291901542 | validation: 3.00163852008569]
	TIME [epoch: 8.38 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8417753527793175		[learning rate: 0.00023805]
	Learning Rate: 0.000238049
	LOSS [training: 2.8417753527793175 | validation: 3.0016033392681356]
	TIME [epoch: 8.36 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8424798879083624		[learning rate: 0.00023749]
	Learning Rate: 0.000237488
	LOSS [training: 2.8424798879083624 | validation: 3.001352310006835]
	TIME [epoch: 8.34 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84249368852714		[learning rate: 0.00023693]
	Learning Rate: 0.000236927
	LOSS [training: 2.84249368852714 | validation: 3.000127604393699]
	TIME [epoch: 8.34 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8427727067896154		[learning rate: 0.00023637]
	Learning Rate: 0.000236369
	LOSS [training: 2.8427727067896154 | validation: 2.9985356085692736]
	TIME [epoch: 8.34 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84167118474743		[learning rate: 0.00023581]
	Learning Rate: 0.000235811
	LOSS [training: 2.84167118474743 | validation: 2.998744872261082]
	TIME [epoch: 8.35 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8419077605460674		[learning rate: 0.00023525]
	Learning Rate: 0.000235255
	LOSS [training: 2.8419077605460674 | validation: 2.998163395297257]
	TIME [epoch: 8.35 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842081806798644		[learning rate: 0.0002347]
	Learning Rate: 0.0002347
	LOSS [training: 2.842081806798644 | validation: 3.0000531255599454]
	TIME [epoch: 8.37 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8425195811912474		[learning rate: 0.00023415]
	Learning Rate: 0.000234146
	LOSS [training: 2.8425195811912474 | validation: 3.0016491922199053]
	TIME [epoch: 8.34 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8436261954184774		[learning rate: 0.00023359]
	Learning Rate: 0.000233594
	LOSS [training: 2.8436261954184774 | validation: 3.002217804427043]
	TIME [epoch: 8.32 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842117670214739		[learning rate: 0.00023304]
	Learning Rate: 0.000233043
	LOSS [training: 2.842117670214739 | validation: 2.999683361999928]
	TIME [epoch: 8.34 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842108846223912		[learning rate: 0.00023249]
	Learning Rate: 0.000232493
	LOSS [training: 2.842108846223912 | validation: 3.000610530705113]
	TIME [epoch: 8.34 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842751974478744		[learning rate: 0.00023194]
	Learning Rate: 0.000231945
	LOSS [training: 2.842751974478744 | validation: 3.0020387751659037]
	TIME [epoch: 8.35 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844279352713229		[learning rate: 0.0002314]
	Learning Rate: 0.000231398
	LOSS [training: 2.844279352713229 | validation: 3.001865358322455]
	TIME [epoch: 8.38 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8435486034902877		[learning rate: 0.00023085]
	Learning Rate: 0.000230852
	LOSS [training: 2.8435486034902877 | validation: 3.0014362420662084]
	TIME [epoch: 8.33 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8430219885698373		[learning rate: 0.00023031]
	Learning Rate: 0.000230307
	LOSS [training: 2.8430219885698373 | validation: 3.000849056532771]
	TIME [epoch: 8.33 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8438549230634855		[learning rate: 0.00022976]
	Learning Rate: 0.000229764
	LOSS [training: 2.8438549230634855 | validation: 3.0021725164536077]
	TIME [epoch: 8.33 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843510380430773		[learning rate: 0.00022922]
	Learning Rate: 0.000229222
	LOSS [training: 2.843510380430773 | validation: 3.000463281130696]
	TIME [epoch: 8.33 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842742502436213		[learning rate: 0.00022868]
	Learning Rate: 0.000228681
	LOSS [training: 2.842742502436213 | validation: 2.99820135985998]
	TIME [epoch: 8.34 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8425022192496336		[learning rate: 0.00022814]
	Learning Rate: 0.000228142
	LOSS [training: 2.8425022192496336 | validation: 3.0007080143103013]
	TIME [epoch: 8.37 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841905090153419		[learning rate: 0.0002276]
	Learning Rate: 0.000227604
	LOSS [training: 2.841905090153419 | validation: 3.000633831777122]
	TIME [epoch: 8.34 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843079244464996		[learning rate: 0.00022707]
	Learning Rate: 0.000227067
	LOSS [training: 2.843079244464996 | validation: 3.0012424340845767]
	TIME [epoch: 8.33 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8422830547531697		[learning rate: 0.00022653]
	Learning Rate: 0.000226531
	LOSS [training: 2.8422830547531697 | validation: 3.0003881431088475]
	TIME [epoch: 8.32 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8422717509617197		[learning rate: 0.000226]
	Learning Rate: 0.000225997
	LOSS [training: 2.8422717509617197 | validation: 2.999773223838356]
	TIME [epoch: 8.33 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8423277005306877		[learning rate: 0.00022546]
	Learning Rate: 0.000225464
	LOSS [training: 2.8423277005306877 | validation: 2.999555460676194]
	TIME [epoch: 8.34 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8430607918992896		[learning rate: 0.00022493]
	Learning Rate: 0.000224932
	LOSS [training: 2.8430607918992896 | validation: 3.0029858895112334]
	TIME [epoch: 8.38 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8448834216896244		[learning rate: 0.0002244]
	Learning Rate: 0.000224401
	LOSS [training: 2.8448834216896244 | validation: 3.002331851429606]
	TIME [epoch: 8.33 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844767185807085		[learning rate: 0.00022387]
	Learning Rate: 0.000223872
	LOSS [training: 2.844767185807085 | validation: 3.0037942701874183]
	TIME [epoch: 8.33 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8449119345945517		[learning rate: 0.00022334]
	Learning Rate: 0.000223344
	LOSS [training: 2.8449119345945517 | validation: 3.0044981960174884]
	TIME [epoch: 8.33 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84414422000593		[learning rate: 0.00022282]
	Learning Rate: 0.000222817
	LOSS [training: 2.84414422000593 | validation: 3.0009883692458708]
	TIME [epoch: 8.32 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8446577139629516		[learning rate: 0.00022229]
	Learning Rate: 0.000222292
	LOSS [training: 2.8446577139629516 | validation: 3.0043742288094517]
	TIME [epoch: 8.33 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8437548116748967		[learning rate: 0.00022177]
	Learning Rate: 0.000221767
	LOSS [training: 2.8437548116748967 | validation: 3.0003549350237835]
	TIME [epoch: 8.38 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8451542560970853		[learning rate: 0.00022124]
	Learning Rate: 0.000221244
	LOSS [training: 2.8451542560970853 | validation: 3.0021928300573153]
	TIME [epoch: 8.32 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8439019757959567		[learning rate: 0.00022072]
	Learning Rate: 0.000220722
	LOSS [training: 2.8439019757959567 | validation: 3.0025678862795724]
	TIME [epoch: 8.31 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844724449978945		[learning rate: 0.0002202]
	Learning Rate: 0.000220202
	LOSS [training: 2.844724449978945 | validation: 3.0018789261428838]
	TIME [epoch: 8.32 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8431822200652315		[learning rate: 0.00021968]
	Learning Rate: 0.000219682
	LOSS [training: 2.8431822200652315 | validation: 3.0001209695158133]
	TIME [epoch: 8.32 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8430374117710366		[learning rate: 0.00021916]
	Learning Rate: 0.000219164
	LOSS [training: 2.8430374117710366 | validation: 2.999526828200641]
	TIME [epoch: 8.33 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8424295712244936		[learning rate: 0.00021865]
	Learning Rate: 0.000218647
	LOSS [training: 2.8424295712244936 | validation: 3.0008026288381497]
	TIME [epoch: 8.37 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8429763753412476		[learning rate: 0.00021813]
	Learning Rate: 0.000218131
	LOSS [training: 2.8429763753412476 | validation: 2.9995117518723857]
	TIME [epoch: 8.35 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843197675497322		[learning rate: 0.00021762]
	Learning Rate: 0.000217617
	LOSS [training: 2.843197675497322 | validation: 2.999948796317235]
	TIME [epoch: 8.34 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8424632582473386		[learning rate: 0.0002171]
	Learning Rate: 0.000217103
	LOSS [training: 2.8424632582473386 | validation: 3.000825057857297]
	TIME [epoch: 8.33 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8416300016267018		[learning rate: 0.00021659]
	Learning Rate: 0.000216591
	LOSS [training: 2.8416300016267018 | validation: 3.003216057636762]
	TIME [epoch: 8.34 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8414707704824287		[learning rate: 0.00021608]
	Learning Rate: 0.00021608
	LOSS [training: 2.8414707704824287 | validation: 2.9986424173408657]
	TIME [epoch: 8.32 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8418014440547665		[learning rate: 0.00021557]
	Learning Rate: 0.000215571
	LOSS [training: 2.8418014440547665 | validation: 2.997853162556816]
	TIME [epoch: 8.36 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8412111467935564		[learning rate: 0.00021506]
	Learning Rate: 0.000215062
	LOSS [training: 2.8412111467935564 | validation: 2.99792508770658]
	TIME [epoch: 8.34 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841042892022684		[learning rate: 0.00021455]
	Learning Rate: 0.000214555
	LOSS [training: 2.841042892022684 | validation: 2.998429778534686]
	TIME [epoch: 8.33 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8415582208494192		[learning rate: 0.00021405]
	Learning Rate: 0.000214049
	LOSS [training: 2.8415582208494192 | validation: 3.0000357341315746]
	TIME [epoch: 8.33 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8395898715804213		[learning rate: 0.00021354]
	Learning Rate: 0.000213544
	LOSS [training: 2.8395898715804213 | validation: 2.998450618320435]
	TIME [epoch: 8.33 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.839479807711431		[learning rate: 0.00021304]
	Learning Rate: 0.00021304
	LOSS [training: 2.839479807711431 | validation: 2.9982807108212834]
	TIME [epoch: 8.32 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8405528791240497		[learning rate: 0.00021254]
	Learning Rate: 0.000212538
	LOSS [training: 2.8405528791240497 | validation: 2.996961840770454]
	TIME [epoch: 8.34 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842249884824807		[learning rate: 0.00021204]
	Learning Rate: 0.000212036
	LOSS [training: 2.842249884824807 | validation: 2.9995185280636196]
	TIME [epoch: 8.37 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8428471636479		[learning rate: 0.00021154]
	Learning Rate: 0.000211536
	LOSS [training: 2.8428471636479 | validation: 2.9979757269362945]
	TIME [epoch: 8.32 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84159690142575		[learning rate: 0.00021104]
	Learning Rate: 0.000211037
	LOSS [training: 2.84159690142575 | validation: 2.999528007041958]
	TIME [epoch: 8.32 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8413181973964075		[learning rate: 0.00021054]
	Learning Rate: 0.000210539
	LOSS [training: 2.8413181973964075 | validation: 2.997837319465695]
	TIME [epoch: 8.33 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840903613643469		[learning rate: 0.00021004]
	Learning Rate: 0.000210043
	LOSS [training: 2.840903613643469 | validation: 2.9990215648045035]
	TIME [epoch: 8.33 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8404312256829973		[learning rate: 0.00020955]
	Learning Rate: 0.000209547
	LOSS [training: 2.8404312256829973 | validation: 3.001899044746514]
	TIME [epoch: 8.33 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84184320248196		[learning rate: 0.00020905]
	Learning Rate: 0.000209053
	LOSS [training: 2.84184320248196 | validation: 2.99704787027611]
	TIME [epoch: 8.37 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840337777455485		[learning rate: 0.00020856]
	Learning Rate: 0.00020856
	LOSS [training: 2.840337777455485 | validation: 2.998514949920451]
	TIME [epoch: 8.32 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8402502843387216		[learning rate: 0.00020807]
	Learning Rate: 0.000208068
	LOSS [training: 2.8402502843387216 | validation: 2.9981995600208844]
	TIME [epoch: 8.33 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840655128262726		[learning rate: 0.00020758]
	Learning Rate: 0.000207577
	LOSS [training: 2.840655128262726 | validation: 2.999000943488078]
	TIME [epoch: 8.33 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8407490008546024		[learning rate: 0.00020709]
	Learning Rate: 0.000207087
	LOSS [training: 2.8407490008546024 | validation: 2.995385579108831]
	TIME [epoch: 8.33 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8414963930055066		[learning rate: 0.0002066]
	Learning Rate: 0.000206599
	LOSS [training: 2.8414963930055066 | validation: 2.999597574059253]
	TIME [epoch: 8.33 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8399483985683815		[learning rate: 0.00020611]
	Learning Rate: 0.000206112
	LOSS [training: 2.8399483985683815 | validation: 3.000814789734168]
	TIME [epoch: 8.38 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8420264938484854		[learning rate: 0.00020563]
	Learning Rate: 0.000205625
	LOSS [training: 2.8420264938484854 | validation: 2.999797312878541]
	TIME [epoch: 8.33 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8405802920319885		[learning rate: 0.00020514]
	Learning Rate: 0.00020514
	LOSS [training: 2.8405802920319885 | validation: 3.001462588797227]
	TIME [epoch: 8.34 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8418960039440315		[learning rate: 0.00020466]
	Learning Rate: 0.000204657
	LOSS [training: 2.8418960039440315 | validation: 2.999016934999856]
	TIME [epoch: 8.33 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840765121330573		[learning rate: 0.00020417]
	Learning Rate: 0.000204174
	LOSS [training: 2.840765121330573 | validation: 3.0001846089786044]
	TIME [epoch: 8.33 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8424205913445526		[learning rate: 0.00020369]
	Learning Rate: 0.000203692
	LOSS [training: 2.8424205913445526 | validation: 3.0024221126505353]
	TIME [epoch: 8.34 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842791819806676		[learning rate: 0.00020321]
	Learning Rate: 0.000203212
	LOSS [training: 2.842791819806676 | validation: 3.0002422019438093]
	TIME [epoch: 8.36 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841071535536478		[learning rate: 0.00020273]
	Learning Rate: 0.000202732
	LOSS [training: 2.841071535536478 | validation: 2.9984262923526197]
	TIME [epoch: 8.34 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8410559094737273		[learning rate: 0.00020225]
	Learning Rate: 0.000202254
	LOSS [training: 2.8410559094737273 | validation: 2.998661795630352]
	TIME [epoch: 8.32 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8416377097110095		[learning rate: 0.00020178]
	Learning Rate: 0.000201777
	LOSS [training: 2.8416377097110095 | validation: 3.0012131249602714]
	TIME [epoch: 8.33 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8427441814083876		[learning rate: 0.0002013]
	Learning Rate: 0.000201301
	LOSS [training: 2.8427441814083876 | validation: 3.0021235619247437]
	TIME [epoch: 8.33 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843060453456262		[learning rate: 0.00020083]
	Learning Rate: 0.000200826
	LOSS [training: 2.843060453456262 | validation: 2.9979041174618315]
	TIME [epoch: 8.34 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8415776510295743		[learning rate: 0.00020035]
	Learning Rate: 0.000200353
	LOSS [training: 2.8415776510295743 | validation: 3.0012394659037476]
	TIME [epoch: 8.35 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8421886944603028		[learning rate: 0.00019988]
	Learning Rate: 0.00019988
	LOSS [training: 2.8421886944603028 | validation: 2.999649922835165]
	TIME [epoch: 8.33 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842524871207898		[learning rate: 0.00019941]
	Learning Rate: 0.000199408
	LOSS [training: 2.842524871207898 | validation: 3.000568934410256]
	TIME [epoch: 8.33 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842670579534003		[learning rate: 0.00019894]
	Learning Rate: 0.000198938
	LOSS [training: 2.842670579534003 | validation: 3.0007065998156817]
	TIME [epoch: 8.33 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843276569451242		[learning rate: 0.00019847]
	Learning Rate: 0.000198469
	LOSS [training: 2.843276569451242 | validation: 3.0007026366625897]
	TIME [epoch: 8.33 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8430929007526897		[learning rate: 0.000198]
	Learning Rate: 0.000198001
	LOSS [training: 2.8430929007526897 | validation: 2.9971437870731434]
	TIME [epoch: 8.33 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842175281725155		[learning rate: 0.00019753]
	Learning Rate: 0.000197534
	LOSS [training: 2.842175281725155 | validation: 3.0004420207105476]
	TIME [epoch: 8.35 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841599869728949		[learning rate: 0.00019707]
	Learning Rate: 0.000197068
	LOSS [training: 2.841599869728949 | validation: 3.000921194870056]
	TIME [epoch: 8.36 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8408408306411546		[learning rate: 0.0001966]
	Learning Rate: 0.000196603
	LOSS [training: 2.8408408306411546 | validation: 3.0007776127377466]
	TIME [epoch: 8.32 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8410392562533255		[learning rate: 0.00019614]
	Learning Rate: 0.000196139
	LOSS [training: 2.8410392562533255 | validation: 2.9963260137205507]
	TIME [epoch: 8.33 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8406697894276443		[learning rate: 0.00019568]
	Learning Rate: 0.000195676
	LOSS [training: 2.8406697894276443 | validation: 3.0010562297114984]
	TIME [epoch: 8.33 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840868837530566		[learning rate: 0.00019521]
	Learning Rate: 0.000195215
	LOSS [training: 2.840868837530566 | validation: 3.000362947824862]
	TIME [epoch: 8.33 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.841826805183087		[learning rate: 0.00019475]
	Learning Rate: 0.000194754
	LOSS [training: 2.841826805183087 | validation: 3.0014517883498018]
	TIME [epoch: 8.35 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8422626558837383		[learning rate: 0.00019429]
	Learning Rate: 0.000194295
	LOSS [training: 2.8422626558837383 | validation: 3.0011920155748815]
	TIME [epoch: 8.37 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.842834863030136		[learning rate: 0.00019384]
	Learning Rate: 0.000193837
	LOSS [training: 2.842834863030136 | validation: 3.0022901986112505]
	TIME [epoch: 8.33 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844399611964916		[learning rate: 0.00019338]
	Learning Rate: 0.000193379
	LOSS [training: 2.844399611964916 | validation: 3.004819611776834]
	TIME [epoch: 8.34 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843920922718544		[learning rate: 0.00019292]
	Learning Rate: 0.000192923
	LOSS [training: 2.843920922718544 | validation: 3.0053868032900812]
	TIME [epoch: 8.33 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844723046550242		[learning rate: 0.00019247]
	Learning Rate: 0.000192468
	LOSS [training: 2.844723046550242 | validation: 2.999676396798882]
	TIME [epoch: 8.33 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.843393826257577		[learning rate: 0.00019201]
	Learning Rate: 0.000192014
	LOSS [training: 2.843393826257577 | validation: 3.0031960076824276]
	TIME [epoch: 8.34 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844310486018295		[learning rate: 0.00019156]
	Learning Rate: 0.000191561
	LOSS [training: 2.844310486018295 | validation: 3.004634385875664]
	TIME [epoch: 8.38 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8454159003779798		[learning rate: 0.00019111]
	Learning Rate: 0.000191109
	LOSS [training: 2.8454159003779798 | validation: 3.003193028086789]
	TIME [epoch: 8.34 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84810198056927		[learning rate: 0.00019066]
	Learning Rate: 0.000190659
	LOSS [training: 2.84810198056927 | validation: 3.0048334177068354]
	TIME [epoch: 8.33 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8470170951704024		[learning rate: 0.00019021]
	Learning Rate: 0.000190209
	LOSS [training: 2.8470170951704024 | validation: 3.0089347514913083]
	TIME [epoch: 8.34 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846917828381857		[learning rate: 0.00018976]
	Learning Rate: 0.00018976
	LOSS [training: 2.846917828381857 | validation: 3.00550548144789]
	TIME [epoch: 8.33 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8465584377795987		[learning rate: 0.00018931]
	Learning Rate: 0.000189313
	LOSS [training: 2.8465584377795987 | validation: 3.0038397369129117]
	TIME [epoch: 8.33 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8466824477837154		[learning rate: 0.00018887]
	Learning Rate: 0.000188866
	LOSS [training: 2.8466824477837154 | validation: 3.0068434817721865]
	TIME [epoch: 8.37 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8481029635786985		[learning rate: 0.00018842]
	Learning Rate: 0.00018842
	LOSS [training: 2.8481029635786985 | validation: 3.008339205327778]
	TIME [epoch: 8.34 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848366880434792		[learning rate: 0.00018798]
	Learning Rate: 0.000187976
	LOSS [training: 2.848366880434792 | validation: 3.0049630694801928]
	TIME [epoch: 8.33 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492076034577765		[learning rate: 0.00018753]
	Learning Rate: 0.000187533
	LOSS [training: 2.8492076034577765 | validation: 3.004305820669605]
	TIME [epoch: 8.33 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8474618442940876		[learning rate: 0.00018709]
	Learning Rate: 0.00018709
	LOSS [training: 2.8474618442940876 | validation: 3.0081528406957396]
	TIME [epoch: 8.33 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8470057912769935		[learning rate: 0.00018665]
	Learning Rate: 0.000186649
	LOSS [training: 2.8470057912769935 | validation: 3.006253033768175]
	TIME [epoch: 8.33 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84814146888169		[learning rate: 0.00018621]
	Learning Rate: 0.000186209
	LOSS [training: 2.84814146888169 | validation: 3.0104822271950535]
	TIME [epoch: 8.36 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847897794992701		[learning rate: 0.00018577]
	Learning Rate: 0.000185769
	LOSS [training: 2.847897794992701 | validation: 3.0102273110434496]
	TIME [epoch: 8.34 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8487165229460185		[learning rate: 0.00018533]
	Learning Rate: 0.000185331
	LOSS [training: 2.8487165229460185 | validation: 3.0070985275964772]
	TIME [epoch: 8.33 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8485067327500757		[learning rate: 0.00018489]
	Learning Rate: 0.000184894
	LOSS [training: 2.8485067327500757 | validation: 3.0079980549905043]
	TIME [epoch: 8.32 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847641378006404		[learning rate: 0.00018446]
	Learning Rate: 0.000184458
	LOSS [training: 2.847641378006404 | validation: 3.0082583681246433]
	TIME [epoch: 8.33 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8486963454189342		[learning rate: 0.00018402]
	Learning Rate: 0.000184023
	LOSS [training: 2.8486963454189342 | validation: 3.007338838903112]
	TIME [epoch: 8.34 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848669955026307		[learning rate: 0.00018359]
	Learning Rate: 0.000183589
	LOSS [training: 2.848669955026307 | validation: 3.0096281565135725]
	TIME [epoch: 8.37 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848303324159541		[learning rate: 0.00018316]
	Learning Rate: 0.000183156
	LOSS [training: 2.848303324159541 | validation: 3.007559556614536]
	TIME [epoch: 8.34 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84954528055592		[learning rate: 0.00018272]
	Learning Rate: 0.000182724
	LOSS [training: 2.84954528055592 | validation: 3.0075529034451494]
	TIME [epoch: 8.33 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8482594828593553		[learning rate: 0.00018229]
	Learning Rate: 0.000182293
	LOSS [training: 2.8482594828593553 | validation: 3.0094524496085224]
	TIME [epoch: 8.33 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8485155160630535		[learning rate: 0.00018186]
	Learning Rate: 0.000181863
	LOSS [training: 2.8485155160630535 | validation: 3.0079573785901887]
	TIME [epoch: 8.33 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496958580827996		[learning rate: 0.00018143]
	Learning Rate: 0.000181434
	LOSS [training: 2.8496958580827996 | validation: 3.007829441944321]
	TIME [epoch: 8.34 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848913851964544		[learning rate: 0.00018101]
	Learning Rate: 0.000181006
	LOSS [training: 2.848913851964544 | validation: 3.0056535300486624]
	TIME [epoch: 8.34 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850127583110586		[learning rate: 0.00018058]
	Learning Rate: 0.000180579
	LOSS [training: 2.850127583110586 | validation: 3.010651732973737]
	TIME [epoch: 8.36 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850488557341225		[learning rate: 0.00018015]
	Learning Rate: 0.000180153
	LOSS [training: 2.850488557341225 | validation: 3.0084497884769137]
	TIME [epoch: 8.34 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850507975737724		[learning rate: 0.00017973]
	Learning Rate: 0.000179728
	LOSS [training: 2.850507975737724 | validation: 3.009665291799198]
	TIME [epoch: 8.33 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8510801128359082		[learning rate: 0.0001793]
	Learning Rate: 0.000179304
	LOSS [training: 2.8510801128359082 | validation: 3.006580689387239]
	TIME [epoch: 8.33 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849412189119169		[learning rate: 0.00017888]
	Learning Rate: 0.000178881
	LOSS [training: 2.849412189119169 | validation: 3.006087479107104]
	TIME [epoch: 8.32 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492804798721343		[learning rate: 0.00017846]
	Learning Rate: 0.000178459
	LOSS [training: 2.8492804798721343 | validation: 3.0064846980481557]
	TIME [epoch: 8.35 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85026213964501		[learning rate: 0.00017804]
	Learning Rate: 0.000178038
	LOSS [training: 2.85026213964501 | validation: 3.0075523342021198]
	TIME [epoch: 8.38 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492277155682		[learning rate: 0.00017762]
	Learning Rate: 0.000177618
	LOSS [training: 2.8492277155682 | validation: 3.0074692184436254]
	TIME [epoch: 8.32 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849135963981472		[learning rate: 0.0001772]
	Learning Rate: 0.000177199
	LOSS [training: 2.849135963981472 | validation: 3.006637671660495]
	TIME [epoch: 8.32 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849208826160935		[learning rate: 0.00017678]
	Learning Rate: 0.000176781
	LOSS [training: 2.849208826160935 | validation: 3.0079123567919828]
	TIME [epoch: 8.33 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8487108059359842		[learning rate: 0.00017636]
	Learning Rate: 0.000176364
	LOSS [training: 2.8487108059359842 | validation: 3.007351555780136]
	TIME [epoch: 8.33 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850501764220755		[learning rate: 0.00017595]
	Learning Rate: 0.000175948
	LOSS [training: 2.850501764220755 | validation: 3.005573036688048]
	TIME [epoch: 8.35 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8500906882678847		[learning rate: 0.00017553]
	Learning Rate: 0.000175533
	LOSS [training: 2.8500906882678847 | validation: 3.0086255120033156]
	TIME [epoch: 8.38 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849556348170663		[learning rate: 0.00017512]
	Learning Rate: 0.000175119
	LOSS [training: 2.849556348170663 | validation: 3.0091835029691065]
	TIME [epoch: 8.33 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8491270801042337		[learning rate: 0.00017471]
	Learning Rate: 0.000174706
	LOSS [training: 2.8491270801042337 | validation: 3.0062162245950756]
	TIME [epoch: 8.33 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849730216355531		[learning rate: 0.00017429]
	Learning Rate: 0.000174294
	LOSS [training: 2.849730216355531 | validation: 3.010300941098081]
	TIME [epoch: 8.33 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8495915201415642		[learning rate: 0.00017388]
	Learning Rate: 0.000173883
	LOSS [training: 2.8495915201415642 | validation: 3.0085286506529396]
	TIME [epoch: 8.33 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850365275032567		[learning rate: 0.00017347]
	Learning Rate: 0.000173473
	LOSS [training: 2.850365275032567 | validation: 3.0104713958313]
	TIME [epoch: 8.34 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501181240097164		[learning rate: 0.00017306]
	Learning Rate: 0.000173063
	LOSS [training: 2.8501181240097164 | validation: 3.0083854311509]
	TIME [epoch: 8.38 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8493431846803308		[learning rate: 0.00017266]
	Learning Rate: 0.000172655
	LOSS [training: 2.8493431846803308 | validation: 3.0075724337580327]
	TIME [epoch: 8.34 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848653287827258		[learning rate: 0.00017225]
	Learning Rate: 0.000172248
	LOSS [training: 2.848653287827258 | validation: 3.008673705825715]
	TIME [epoch: 8.34 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8493718737286984		[learning rate: 0.00017184]
	Learning Rate: 0.000171842
	LOSS [training: 2.8493718737286984 | validation: 3.0051489589718248]
	TIME [epoch: 8.34 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8493551810629403		[learning rate: 0.00017144]
	Learning Rate: 0.000171436
	LOSS [training: 2.8493551810629403 | validation: 3.0073411204259637]
	TIME [epoch: 8.33 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8495961168979083		[learning rate: 0.00017103]
	Learning Rate: 0.000171032
	LOSS [training: 2.8495961168979083 | validation: 3.0065689808498783]
	TIME [epoch: 8.34 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849207525616519		[learning rate: 0.00017063]
	Learning Rate: 0.000170628
	LOSS [training: 2.849207525616519 | validation: 3.010283145857473]
	TIME [epoch: 8.37 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8499955812252855		[learning rate: 0.00017023]
	Learning Rate: 0.000170226
	LOSS [training: 2.8499955812252855 | validation: 3.0069117410307302]
	TIME [epoch: 8.36 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849573228146406		[learning rate: 0.00016982]
	Learning Rate: 0.000169824
	LOSS [training: 2.849573228146406 | validation: 3.0061004879458113]
	TIME [epoch: 8.34 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849435812945412		[learning rate: 0.00016942]
	Learning Rate: 0.000169424
	LOSS [training: 2.849435812945412 | validation: 3.007621427427856]
	TIME [epoch: 8.33 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492957327354596		[learning rate: 0.00016902]
	Learning Rate: 0.000169024
	LOSS [training: 2.8492957327354596 | validation: 3.0064933401898335]
	TIME [epoch: 8.34 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8498333138540315		[learning rate: 0.00016863]
	Learning Rate: 0.000168625
	LOSS [training: 2.8498333138540315 | validation: 3.008749170197774]
	TIME [epoch: 8.34 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849615220025414		[learning rate: 0.00016823]
	Learning Rate: 0.000168228
	LOSS [training: 2.849615220025414 | validation: 3.0081381475196456]
	TIME [epoch: 8.37 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850353028050567		[learning rate: 0.00016783]
	Learning Rate: 0.000167831
	LOSS [training: 2.850353028050567 | validation: 3.00803227763958]
	TIME [epoch: 8.35 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850269951445158		[learning rate: 0.00016743]
	Learning Rate: 0.000167435
	LOSS [training: 2.850269951445158 | validation: 3.007398158032502]
	TIME [epoch: 8.34 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848903712743203		[learning rate: 0.00016704]
	Learning Rate: 0.00016704
	LOSS [training: 2.848903712743203 | validation: 3.0092969637487235]
	TIME [epoch: 8.34 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849348837825492		[learning rate: 0.00016665]
	Learning Rate: 0.000166646
	LOSS [training: 2.849348837825492 | validation: 3.0065662224089893]
	TIME [epoch: 8.34 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8491429079356325		[learning rate: 0.00016625]
	Learning Rate: 0.000166253
	LOSS [training: 2.8491429079356325 | validation: 3.0078890654769985]
	TIME [epoch: 8.34 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8484642791840953		[learning rate: 0.00016586]
	Learning Rate: 0.000165861
	LOSS [training: 2.8484642791840953 | validation: 3.0088103666839734]
	TIME [epoch: 8.34 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849996055044771		[learning rate: 0.00016547]
	Learning Rate: 0.00016547
	LOSS [training: 2.849996055044771 | validation: 3.006389910960089]
	TIME [epoch: 8.38 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507804551697484		[learning rate: 0.00016508]
	Learning Rate: 0.000165079
	LOSS [training: 2.8507804551697484 | validation: 3.0052800861383027]
	TIME [epoch: 8.33 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8500457788201987		[learning rate: 0.00016469]
	Learning Rate: 0.00016469
	LOSS [training: 2.8500457788201987 | validation: 3.0076923118138064]
	TIME [epoch: 8.34 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850203263886705		[learning rate: 0.0001643]
	Learning Rate: 0.000164301
	LOSS [training: 2.850203263886705 | validation: 3.0057326669718343]
	TIME [epoch: 8.34 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849166890016748		[learning rate: 0.00016391]
	Learning Rate: 0.000163914
	LOSS [training: 2.849166890016748 | validation: 3.005857750546353]
	TIME [epoch: 8.33 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8484703379589136		[learning rate: 0.00016353]
	Learning Rate: 0.000163527
	LOSS [training: 2.8484703379589136 | validation: 3.0081632673013283]
	TIME [epoch: 8.36 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8495456604064557		[learning rate: 0.00016314]
	Learning Rate: 0.000163141
	LOSS [training: 2.8495456604064557 | validation: 3.004956957980444]
	TIME [epoch: 8.37 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8490944825983293		[learning rate: 0.00016276]
	Learning Rate: 0.000162757
	LOSS [training: 2.8490944825983293 | validation: 3.008013584065649]
	TIME [epoch: 8.34 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849168015150416		[learning rate: 0.00016237]
	Learning Rate: 0.000162373
	LOSS [training: 2.849168015150416 | validation: 3.0084240275250904]
	TIME [epoch: 8.33 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8493635316783985		[learning rate: 0.00016199]
	Learning Rate: 0.00016199
	LOSS [training: 2.8493635316783985 | validation: 3.0066243953864538]
	TIME [epoch: 8.34 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8508228731619756		[learning rate: 0.00016161]
	Learning Rate: 0.000161608
	LOSS [training: 2.8508228731619756 | validation: 3.0077367459173114]
	TIME [epoch: 8.32 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850037450952052		[learning rate: 0.00016123]
	Learning Rate: 0.000161226
	LOSS [training: 2.850037450952052 | validation: 3.0067716796074206]
	TIME [epoch: 8.35 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850951009317355		[learning rate: 0.00016085]
	Learning Rate: 0.000160846
	LOSS [training: 2.850951009317355 | validation: 3.008597488371513]
	TIME [epoch: 8.38 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8517952850239383		[learning rate: 0.00016047]
	Learning Rate: 0.000160467
	LOSS [training: 2.8517952850239383 | validation: 3.0087198292774957]
	TIME [epoch: 8.33 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8502584766799433		[learning rate: 0.00016009]
	Learning Rate: 0.000160088
	LOSS [training: 2.8502584766799433 | validation: 3.007344594533735]
	TIME [epoch: 8.33 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8499903577505776		[learning rate: 0.00015971]
	Learning Rate: 0.00015971
	LOSS [training: 2.8499903577505776 | validation: 3.01031521132353]
	TIME [epoch: 8.33 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8497000150153178		[learning rate: 0.00015933]
	Learning Rate: 0.000159334
	LOSS [training: 2.8497000150153178 | validation: 3.005971476775369]
	TIME [epoch: 8.33 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849258660062518		[learning rate: 0.00015896]
	Learning Rate: 0.000158958
	LOSS [training: 2.849258660062518 | validation: 3.0047740448706275]
	TIME [epoch: 8.34 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848272156752012		[learning rate: 0.00015858]
	Learning Rate: 0.000158583
	LOSS [training: 2.848272156752012 | validation: 3.005521525542453]
	TIME [epoch: 8.38 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8480980792861406		[learning rate: 0.00015821]
	Learning Rate: 0.000158209
	LOSS [training: 2.8480980792861406 | validation: 3.0092694137289517]
	TIME [epoch: 8.34 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8498732400606674		[learning rate: 0.00015784]
	Learning Rate: 0.000157836
	LOSS [training: 2.8498732400606674 | validation: 3.0078379007333327]
	TIME [epoch: 8.33 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850998162124439		[learning rate: 0.00015746]
	Learning Rate: 0.000157463
	LOSS [training: 2.850998162124439 | validation: 3.007061760425139]
	TIME [epoch: 8.34 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8505821165636283		[learning rate: 0.00015709]
	Learning Rate: 0.000157092
	LOSS [training: 2.8505821165636283 | validation: 3.0082364497876486]
	TIME [epoch: 8.33 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850890527434583		[learning rate: 0.00015672]
	Learning Rate: 0.000156721
	LOSS [training: 2.850890527434583 | validation: 3.008583982579947]
	TIME [epoch: 8.34 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851282590487787		[learning rate: 0.00015635]
	Learning Rate: 0.000156352
	LOSS [training: 2.851282590487787 | validation: 3.009294132975043]
	TIME [epoch: 8.37 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8511244165178375		[learning rate: 0.00015598]
	Learning Rate: 0.000155983
	LOSS [training: 2.8511244165178375 | validation: 3.0069990049041415]
	TIME [epoch: 8.35 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850847400849718		[learning rate: 0.00015561]
	Learning Rate: 0.000155615
	LOSS [training: 2.850847400849718 | validation: 3.0089133140769824]
	TIME [epoch: 8.33 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501778505784467		[learning rate: 0.00015525]
	Learning Rate: 0.000155248
	LOSS [training: 2.8501778505784467 | validation: 3.009531686062237]
	TIME [epoch: 8.34 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507625087246042		[learning rate: 0.00015488]
	Learning Rate: 0.000154882
	LOSS [training: 2.8507625087246042 | validation: 3.0075667042371848]
	TIME [epoch: 8.33 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8511914419994873		[learning rate: 0.00015452]
	Learning Rate: 0.000154516
	LOSS [training: 2.8511914419994873 | validation: 3.0092871470363574]
	TIME [epoch: 8.33 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503795600785136		[learning rate: 0.00015415]
	Learning Rate: 0.000154152
	LOSS [training: 2.8503795600785136 | validation: 3.00670746674374]
	TIME [epoch: 8.36 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8488302957233254		[learning rate: 0.00015379]
	Learning Rate: 0.000153788
	LOSS [training: 2.8488302957233254 | validation: 3.0086076467504244]
	TIME [epoch: 8.36 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8494682996211527		[learning rate: 0.00015343]
	Learning Rate: 0.000153425
	LOSS [training: 2.8494682996211527 | validation: 3.007366440954329]
	TIME [epoch: 8.33 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496850298645473		[learning rate: 0.00015306]
	Learning Rate: 0.000153064
	LOSS [training: 2.8496850298645473 | validation: 3.007175674236521]
	TIME [epoch: 8.34 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8500728105936544		[learning rate: 0.0001527]
	Learning Rate: 0.000152703
	LOSS [training: 2.8500728105936544 | validation: 3.007892675265393]
	TIME [epoch: 8.34 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849550969583336		[learning rate: 0.00015234]
	Learning Rate: 0.000152342
	LOSS [training: 2.849550969583336 | validation: 3.0082257545825293]
	TIME [epoch: 8.34 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8490499894216033		[learning rate: 0.00015198]
	Learning Rate: 0.000151983
	LOSS [training: 2.8490499894216033 | validation: 3.0100396663136157]
	TIME [epoch: 8.35 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850294541563234		[learning rate: 0.00015162]
	Learning Rate: 0.000151624
	LOSS [training: 2.850294541563234 | validation: 3.0100395543987366]
	TIME [epoch: 8.38 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8522265417054267		[learning rate: 0.00015127]
	Learning Rate: 0.000151267
	LOSS [training: 2.8522265417054267 | validation: 3.008044861827964]
	TIME [epoch: 8.33 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85011295421042		[learning rate: 0.00015091]
	Learning Rate: 0.00015091
	LOSS [training: 2.85011295421042 | validation: 3.0081491717862177]
	TIME [epoch: 8.34 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850708968720271		[learning rate: 0.00015055]
	Learning Rate: 0.000150554
	LOSS [training: 2.850708968720271 | validation: 3.0117194253059205]
	TIME [epoch: 8.33 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8498516126685174		[learning rate: 0.0001502]
	Learning Rate: 0.000150199
	LOSS [training: 2.8498516126685174 | validation: 3.0078643651193033]
	TIME [epoch: 8.33 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850789908733161		[learning rate: 0.00014984]
	Learning Rate: 0.000149845
	LOSS [training: 2.850789908733161 | validation: 3.008750809912324]
	TIME [epoch: 8.35 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85053884582454		[learning rate: 0.00014949]
	Learning Rate: 0.000149491
	LOSS [training: 2.85053884582454 | validation: 3.0083875234270385]
	TIME [epoch: 8.37 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8512319019808463		[learning rate: 0.00014914]
	Learning Rate: 0.000149139
	LOSS [training: 2.8512319019808463 | validation: 3.0076895823751784]
	TIME [epoch: 8.33 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850840275106819		[learning rate: 0.00014879]
	Learning Rate: 0.000148787
	LOSS [training: 2.850840275106819 | validation: 3.0099501865966047]
	TIME [epoch: 8.34 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8505563219406227		[learning rate: 0.00014844]
	Learning Rate: 0.000148436
	LOSS [training: 2.8505563219406227 | validation: 3.01166815630575]
	TIME [epoch: 8.33 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8521183274545114		[learning rate: 0.00014809]
	Learning Rate: 0.000148086
	LOSS [training: 2.8521183274545114 | validation: 3.0078860622355696]
	TIME [epoch: 8.33 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851626300767533		[learning rate: 0.00014774]
	Learning Rate: 0.000147736
	LOSS [training: 2.851626300767533 | validation: 3.008396619624116]
	TIME [epoch: 8.34 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8518144842386226		[learning rate: 0.00014739]
	Learning Rate: 0.000147388
	LOSS [training: 2.8518144842386226 | validation: 3.011312557459268]
	TIME [epoch: 8.36 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8517733944105523		[learning rate: 0.00014704]
	Learning Rate: 0.00014704
	LOSS [training: 2.8517733944105523 | validation: 3.009371457781942]
	TIME [epoch: 8.34 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852189387965795		[learning rate: 0.00014669]
	Learning Rate: 0.000146693
	LOSS [training: 2.852189387965795 | validation: 3.010122523016512]
	TIME [epoch: 8.32 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852709262971763		[learning rate: 0.00014635]
	Learning Rate: 0.000146347
	LOSS [training: 2.852709262971763 | validation: 3.0100046743394646]
	TIME [epoch: 8.32 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8520538194860836		[learning rate: 0.000146]
	Learning Rate: 0.000146002
	LOSS [training: 2.8520538194860836 | validation: 3.010187784145129]
	TIME [epoch: 8.33 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854202255583746		[learning rate: 0.00014566]
	Learning Rate: 0.000145658
	LOSS [training: 2.854202255583746 | validation: 3.01050321309042]
	TIME [epoch: 8.35 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8522963142128		[learning rate: 0.00014531]
	Learning Rate: 0.000145314
	LOSS [training: 2.8522963142128 | validation: 3.010543794203421]
	TIME [epoch: 8.37 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852632272335483		[learning rate: 0.00014497]
	Learning Rate: 0.000144971
	LOSS [training: 2.852632272335483 | validation: 3.008868104762933]
	TIME [epoch: 8.34 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85073795285272		[learning rate: 0.00014463]
	Learning Rate: 0.000144629
	LOSS [training: 2.85073795285272 | validation: 3.010877132016291]
	TIME [epoch: 8.33 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851788031750254		[learning rate: 0.00014429]
	Learning Rate: 0.000144288
	LOSS [training: 2.851788031750254 | validation: 3.009876281415328]
	TIME [epoch: 8.33 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8526927540621423		[learning rate: 0.00014395]
	Learning Rate: 0.000143948
	LOSS [training: 2.8526927540621423 | validation: 3.013016067585781]
	TIME [epoch: 8.34 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852893870336121		[learning rate: 0.00014361]
	Learning Rate: 0.000143608
	LOSS [training: 2.852893870336121 | validation: 3.0118445403775453]
	TIME [epoch: 8.33 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8530415787527192		[learning rate: 0.00014327]
	Learning Rate: 0.00014327
	LOSS [training: 2.8530415787527192 | validation: 3.0107366858838542]
	TIME [epoch: 8.37 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8517415514421804		[learning rate: 0.00014293]
	Learning Rate: 0.000142932
	LOSS [training: 2.8517415514421804 | validation: 3.0087452517797066]
	TIME [epoch: 8.35 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853365524110659		[learning rate: 0.00014259]
	Learning Rate: 0.000142594
	LOSS [training: 2.853365524110659 | validation: 3.0125956372871765]
	TIME [epoch: 8.32 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85485987396107		[learning rate: 0.00014226]
	Learning Rate: 0.000142258
	LOSS [training: 2.85485987396107 | validation: 3.0130155119686366]
	TIME [epoch: 8.33 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8539626316421236		[learning rate: 0.00014192]
	Learning Rate: 0.000141923
	LOSS [training: 2.8539626316421236 | validation: 3.0131095990616927]
	TIME [epoch: 8.33 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8528452774352955		[learning rate: 0.00014159]
	Learning Rate: 0.000141588
	LOSS [training: 2.8528452774352955 | validation: 3.011962625664185]
	TIME [epoch: 8.34 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8545585765291186		[learning rate: 0.00014125]
	Learning Rate: 0.000141254
	LOSS [training: 2.8545585765291186 | validation: 3.013232297503597]
	TIME [epoch: 8.35 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8531168027795353		[learning rate: 0.00014092]
	Learning Rate: 0.000140921
	LOSS [training: 2.8531168027795353 | validation: 3.0125029296282166]
	TIME [epoch: 8.37 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8541963452437056		[learning rate: 0.00014059]
	Learning Rate: 0.000140588
	LOSS [training: 2.8541963452437056 | validation: 3.0108109281173174]
	TIME [epoch: 8.33 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8539372960052853		[learning rate: 0.00014026]
	Learning Rate: 0.000140257
	LOSS [training: 2.8539372960052853 | validation: 3.012256127490959]
	TIME [epoch: 8.34 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854397619556491		[learning rate: 0.00013993]
	Learning Rate: 0.000139926
	LOSS [training: 2.854397619556491 | validation: 3.0121286621966945]
	TIME [epoch: 8.34 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8534680218722297		[learning rate: 0.0001396]
	Learning Rate: 0.000139596
	LOSS [training: 2.8534680218722297 | validation: 3.012684354540892]
	TIME [epoch: 8.34 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85366578059446		[learning rate: 0.00013927]
	Learning Rate: 0.000139266
	LOSS [training: 2.85366578059446 | validation: 3.0135393100981]
	TIME [epoch: 8.34 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8553758310414885		[learning rate: 0.00013894]
	Learning Rate: 0.000138938
	LOSS [training: 2.8553758310414885 | validation: 3.0094442689012806]
	TIME [epoch: 8.38 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852886088843361		[learning rate: 0.00013861]
	Learning Rate: 0.00013861
	LOSS [training: 2.852886088843361 | validation: 3.0122916199592717]
	TIME [epoch: 8.33 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852101413040911		[learning rate: 0.00013828]
	Learning Rate: 0.000138283
	LOSS [training: 2.852101413040911 | validation: 3.0086707572770663]
	TIME [epoch: 8.33 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8517341732628756		[learning rate: 0.00013796]
	Learning Rate: 0.000137957
	LOSS [training: 2.8517341732628756 | validation: 3.0095647063878666]
	TIME [epoch: 8.33 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8518116993459466		[learning rate: 0.00013763]
	Learning Rate: 0.000137632
	LOSS [training: 2.8518116993459466 | validation: 3.009644869964002]
	TIME [epoch: 8.34 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852155161124061		[learning rate: 0.00013731]
	Learning Rate: 0.000137307
	LOSS [training: 2.852155161124061 | validation: 3.009217792616141]
	TIME [epoch: 8.35 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851168376753184		[learning rate: 0.00013698]
	Learning Rate: 0.000136983
	LOSS [training: 2.851168376753184 | validation: 3.0099096536511025]
	TIME [epoch: 8.38 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851085939233157		[learning rate: 0.00013666]
	Learning Rate: 0.00013666
	LOSS [training: 2.851085939233157 | validation: 3.0089945174306605]
	TIME [epoch: 8.34 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8528160882384275		[learning rate: 0.00013634]
	Learning Rate: 0.000136338
	LOSS [training: 2.8528160882384275 | validation: 3.007555291380803]
	TIME [epoch: 8.34 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8519277053334116		[learning rate: 0.00013602]
	Learning Rate: 0.000136016
	LOSS [training: 2.8519277053334116 | validation: 3.0107605966845044]
	TIME [epoch: 8.33 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852623749895373		[learning rate: 0.0001357]
	Learning Rate: 0.000135695
	LOSS [training: 2.852623749895373 | validation: 3.0110959141545015]
	TIME [epoch: 8.33 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852610812897911		[learning rate: 0.00013538]
	Learning Rate: 0.000135375
	LOSS [training: 2.852610812897911 | validation: 3.013981903793172]
	TIME [epoch: 8.34 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8533913038150205		[learning rate: 0.00013506]
	Learning Rate: 0.000135056
	LOSS [training: 2.8533913038150205 | validation: 3.012869249632744]
	TIME [epoch: 8.37 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8521909825693896		[learning rate: 0.00013474]
	Learning Rate: 0.000134737
	LOSS [training: 2.8521909825693896 | validation: 3.0114322706939856]
	TIME [epoch: 8.33 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8527918262696623		[learning rate: 0.00013442]
	Learning Rate: 0.000134419
	LOSS [training: 2.8527918262696623 | validation: 3.0116946180937356]
	TIME [epoch: 8.33 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853890025858064		[learning rate: 0.0001341]
	Learning Rate: 0.000134102
	LOSS [training: 2.853890025858064 | validation: 3.0111933757009037]
	TIME [epoch: 8.33 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8521082323199476		[learning rate: 0.00013379]
	Learning Rate: 0.000133786
	LOSS [training: 2.8521082323199476 | validation: 3.01070723244659]
	TIME [epoch: 8.33 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8526244915803396		[learning rate: 0.00013347]
	Learning Rate: 0.00013347
	LOSS [training: 2.8526244915803396 | validation: 3.008595958395997]
	TIME [epoch: 8.34 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85138697420615		[learning rate: 0.00013316]
	Learning Rate: 0.000133155
	LOSS [training: 2.85138697420615 | validation: 3.011704309703515]
	TIME [epoch: 8.38 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507981362400208		[learning rate: 0.00013284]
	Learning Rate: 0.000132841
	LOSS [training: 2.8507981362400208 | validation: 3.0116547463304593]
	TIME [epoch: 8.34 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85215534816951		[learning rate: 0.00013253]
	Learning Rate: 0.000132528
	LOSS [training: 2.85215534816951 | validation: 3.0089078103658435]
	TIME [epoch: 8.33 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8510218284465862		[learning rate: 0.00013222]
	Learning Rate: 0.000132215
	LOSS [training: 2.8510218284465862 | validation: 3.0079077539288916]
	TIME [epoch: 8.33 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8511303493290048		[learning rate: 0.0001319]
	Learning Rate: 0.000131904
	LOSS [training: 2.8511303493290048 | validation: 3.007459883557951]
	TIME [epoch: 8.33 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8499414259384106		[learning rate: 0.00013159]
	Learning Rate: 0.000131592
	LOSS [training: 2.8499414259384106 | validation: 3.008753702449686]
	TIME [epoch: 8.34 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8506663219268846		[learning rate: 0.00013128]
	Learning Rate: 0.000131282
	LOSS [training: 2.8506663219268846 | validation: 3.0080252920114283]
	TIME [epoch: 8.37 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507506555516677		[learning rate: 0.00013097]
	Learning Rate: 0.000130972
	LOSS [training: 2.8507506555516677 | validation: 3.007851920745976]
	TIME [epoch: 8.35 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850763302705361		[learning rate: 0.00013066]
	Learning Rate: 0.000130663
	LOSS [training: 2.850763302705361 | validation: 3.008523189769976]
	TIME [epoch: 8.33 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849521623923506		[learning rate: 0.00013036]
	Learning Rate: 0.000130355
	LOSS [training: 2.849521623923506 | validation: 3.0079500820671594]
	TIME [epoch: 8.33 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8520258317399705		[learning rate: 0.00013005]
	Learning Rate: 0.000130048
	LOSS [training: 2.8520258317399705 | validation: 3.009000556779279]
	TIME [epoch: 8.33 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8504217273510974		[learning rate: 0.00012974]
	Learning Rate: 0.000129741
	LOSS [training: 2.8504217273510974 | validation: 3.0089457319276747]
	TIME [epoch: 8.34 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851255282727638		[learning rate: 0.00012943]
	Learning Rate: 0.000129435
	LOSS [training: 2.851255282727638 | validation: 3.0084604914203243]
	TIME [epoch: 8.36 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507805699666933		[learning rate: 0.00012913]
	Learning Rate: 0.00012913
	LOSS [training: 2.8507805699666933 | validation: 3.0112329928498944]
	TIME [epoch: 8.36 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850878742373595		[learning rate: 0.00012882]
	Learning Rate: 0.000128825
	LOSS [training: 2.850878742373595 | validation: 3.008153484677779]
	TIME [epoch: 8.33 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849958303092418		[learning rate: 0.00012852]
	Learning Rate: 0.000128521
	LOSS [training: 2.849958303092418 | validation: 3.006783241645034]
	TIME [epoch: 8.33 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8505801116001046		[learning rate: 0.00012822]
	Learning Rate: 0.000128218
	LOSS [training: 2.8505801116001046 | validation: 3.008475945335535]
	TIME [epoch: 8.33 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850563663652808		[learning rate: 0.00012792]
	Learning Rate: 0.000127915
	LOSS [training: 2.850563663652808 | validation: 3.009489229286408]
	TIME [epoch: 8.33 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850072993441355		[learning rate: 0.00012761]
	Learning Rate: 0.000127614
	LOSS [training: 2.850072993441355 | validation: 3.0094471781373575]
	TIME [epoch: 8.34 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496726204974774		[learning rate: 0.00012731]
	Learning Rate: 0.000127313
	LOSS [training: 2.8496726204974774 | validation: 3.007705406365309]
	TIME [epoch: 8.36 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8494768872953795		[learning rate: 0.00012701]
	Learning Rate: 0.000127012
	LOSS [training: 2.8494768872953795 | validation: 3.005353607519782]
	TIME [epoch: 8.33 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849107838050301		[learning rate: 0.00012671]
	Learning Rate: 0.000126713
	LOSS [training: 2.849107838050301 | validation: 3.0080221769298356]
	TIME [epoch: 8.33 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503292482574474		[learning rate: 0.00012641]
	Learning Rate: 0.000126414
	LOSS [training: 2.8503292482574474 | validation: 3.0075719806591024]
	TIME [epoch: 8.32 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848704280763941		[learning rate: 0.00012612]
	Learning Rate: 0.000126116
	LOSS [training: 2.848704280763941 | validation: 3.0070558901050513]
	TIME [epoch: 8.33 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848290182234302		[learning rate: 0.00012582]
	Learning Rate: 0.000125818
	LOSS [training: 2.848290182234302 | validation: 3.008643953394661]
	TIME [epoch: 8.35 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8489444510742548		[learning rate: 0.00012552]
	Learning Rate: 0.000125521
	LOSS [training: 2.8489444510742548 | validation: 3.007718878795642]
	TIME [epoch: 8.37 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492436796776484		[learning rate: 0.00012523]
	Learning Rate: 0.000125225
	LOSS [training: 2.8492436796776484 | validation: 3.007326120526707]
	TIME [epoch: 8.33 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849876133324068		[learning rate: 0.00012493]
	Learning Rate: 0.00012493
	LOSS [training: 2.849876133324068 | validation: 3.007411460424528]
	TIME [epoch: 8.32 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8483513550248203		[learning rate: 0.00012464]
	Learning Rate: 0.000124635
	LOSS [training: 2.8483513550248203 | validation: 3.0068235498322142]
	TIME [epoch: 8.33 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849846184462589		[learning rate: 0.00012434]
	Learning Rate: 0.000124341
	LOSS [training: 2.849846184462589 | validation: 3.005990004574912]
	TIME [epoch: 8.32 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847575479479138		[learning rate: 0.00012405]
	Learning Rate: 0.000124048
	LOSS [training: 2.847575479479138 | validation: 3.0099683857265056]
	TIME [epoch: 8.34 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848021437112111		[learning rate: 0.00012376]
	Learning Rate: 0.000123755
	LOSS [training: 2.848021437112111 | validation: 3.0057505659248234]
	TIME [epoch: 8.37 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8492313279828667		[learning rate: 0.00012346]
	Learning Rate: 0.000123463
	LOSS [training: 2.8492313279828667 | validation: 3.0059251481871314]
	TIME [epoch: 8.34 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8500217227966878		[learning rate: 0.00012317]
	Learning Rate: 0.000123172
	LOSS [training: 2.8500217227966878 | validation: 3.0099637230646463]
	TIME [epoch: 8.34 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8491584808591544		[learning rate: 0.00012288]
	Learning Rate: 0.000122882
	LOSS [training: 2.8491584808591544 | validation: 3.007404780318872]
	TIME [epoch: 8.33 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8479532509472905		[learning rate: 0.00012259]
	Learning Rate: 0.000122592
	LOSS [training: 2.8479532509472905 | validation: 3.0078615571812652]
	TIME [epoch: 8.33 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8484231985291806		[learning rate: 0.0001223]
	Learning Rate: 0.000122303
	LOSS [training: 2.8484231985291806 | validation: 3.006226480522238]
	TIME [epoch: 8.34 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8484706285530987		[learning rate: 0.00012201]
	Learning Rate: 0.000122014
	LOSS [training: 2.8484706285530987 | validation: 3.0064998028954104]
	TIME [epoch: 8.37 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848551790050942		[learning rate: 0.00012173]
	Learning Rate: 0.000121726
	LOSS [training: 2.848551790050942 | validation: 3.0044215347062373]
	TIME [epoch: 8.34 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496895325459497		[learning rate: 0.00012144]
	Learning Rate: 0.000121439
	LOSS [training: 2.8496895325459497 | validation: 3.006349000399164]
	TIME [epoch: 8.33 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8481661208153772		[learning rate: 0.00012115]
	Learning Rate: 0.000121153
	LOSS [training: 2.8481661208153772 | validation: 3.0053753251414856]
	TIME [epoch: 8.33 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8481746723429078		[learning rate: 0.00012087]
	Learning Rate: 0.000120867
	LOSS [training: 2.8481746723429078 | validation: 3.0069562276972563]
	TIME [epoch: 8.32 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8477054352172773		[learning rate: 0.00012058]
	Learning Rate: 0.000120582
	LOSS [training: 2.8477054352172773 | validation: 3.004376188623919]
	TIME [epoch: 8.33 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8483294470795357		[learning rate: 0.0001203]
	Learning Rate: 0.000120297
	LOSS [training: 2.8483294470795357 | validation: 3.0053901560917136]
	TIME [epoch: 8.36 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8482621468410887		[learning rate: 0.00012001]
	Learning Rate: 0.000120014
	LOSS [training: 2.8482621468410887 | validation: 3.004546354605799]
	TIME [epoch: 8.35 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496722759610797		[learning rate: 0.00011973]
	Learning Rate: 0.000119731
	LOSS [training: 2.8496722759610797 | validation: 3.0046204294842465]
	TIME [epoch: 8.33 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8485263899781645		[learning rate: 0.00011945]
	Learning Rate: 0.000119448
	LOSS [training: 2.8485263899781645 | validation: 3.007022779501428]
	TIME [epoch: 8.33 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8483847594086042		[learning rate: 0.00011917]
	Learning Rate: 0.000119166
	LOSS [training: 2.8483847594086042 | validation: 3.0076718483844784]
	TIME [epoch: 8.33 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8490190668112287		[learning rate: 0.00011889]
	Learning Rate: 0.000118885
	LOSS [training: 2.8490190668112287 | validation: 3.006592044395581]
	TIME [epoch: 8.34 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8486084023612115		[learning rate: 0.0001186]
	Learning Rate: 0.000118605
	LOSS [training: 2.8486084023612115 | validation: 3.0076081443703577]
	TIME [epoch: 8.35 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848433788412442		[learning rate: 0.00011833]
	Learning Rate: 0.000118325
	LOSS [training: 2.848433788412442 | validation: 3.0063491786838004]
	TIME [epoch: 8.36 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8487505452882513		[learning rate: 0.00011805]
	Learning Rate: 0.000118046
	LOSS [training: 2.8487505452882513 | validation: 3.0063694734017163]
	TIME [epoch: 8.34 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848786759771216		[learning rate: 0.00011777]
	Learning Rate: 0.000117768
	LOSS [training: 2.848786759771216 | validation: 3.005366418920291]
	TIME [epoch: 8.33 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8482878494077903		[learning rate: 0.00011749]
	Learning Rate: 0.00011749
	LOSS [training: 2.8482878494077903 | validation: 3.005658516202468]
	TIME [epoch: 8.34 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847687005634276		[learning rate: 0.00011721]
	Learning Rate: 0.000117213
	LOSS [training: 2.847687005634276 | validation: 3.0068745292356756]
	TIME [epoch: 8.34 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8486456635271376		[learning rate: 0.00011694]
	Learning Rate: 0.000116936
	LOSS [training: 2.8486456635271376 | validation: 3.005847231159384]
	TIME [epoch: 8.35 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8485731680971327		[learning rate: 0.00011666]
	Learning Rate: 0.00011666
	LOSS [training: 2.8485731680971327 | validation: 3.009211882256841]
	TIME [epoch: 8.37 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8494259302452503		[learning rate: 0.00011639]
	Learning Rate: 0.000116385
	LOSS [training: 2.8494259302452503 | validation: 3.0066988542411917]
	TIME [epoch: 8.34 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501958491531356		[learning rate: 0.00011611]
	Learning Rate: 0.000116111
	LOSS [training: 2.8501958491531356 | validation: 3.0088268268404725]
	TIME [epoch: 8.33 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850086859449498		[learning rate: 0.00011584]
	Learning Rate: 0.000115837
	LOSS [training: 2.850086859449498 | validation: 3.0086254261354575]
	TIME [epoch: 8.33 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503461842674858		[learning rate: 0.00011556]
	Learning Rate: 0.000115563
	LOSS [training: 2.8503461842674858 | validation: 3.0099563401813363]
	TIME [epoch: 8.33 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.848643665529307		[learning rate: 0.00011529]
	Learning Rate: 0.000115291
	LOSS [training: 2.848643665529307 | validation: 3.008090645574118]
	TIME [epoch: 8.35 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84941458741856		[learning rate: 0.00011502]
	Learning Rate: 0.000115019
	LOSS [training: 2.84941458741856 | validation: 3.0080609740186866]
	TIME [epoch: 8.38 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8500696616577863		[learning rate: 0.00011475]
	Learning Rate: 0.000114748
	LOSS [training: 2.8500696616577863 | validation: 3.006408596543282]
	TIME [epoch: 8.34 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503707416795905		[learning rate: 0.00011448]
	Learning Rate: 0.000114477
	LOSS [training: 2.8503707416795905 | validation: 3.0056764266191847]
	TIME [epoch: 8.34 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.849548793231468		[learning rate: 0.00011421]
	Learning Rate: 0.000114207
	LOSS [training: 2.849548793231468 | validation: 3.007860426042564]
	TIME [epoch: 8.34 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8506915521480223		[learning rate: 0.00011394]
	Learning Rate: 0.000113938
	LOSS [training: 2.8506915521480223 | validation: 3.0079528291974715]
	TIME [epoch: 8.34 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503318150095325		[learning rate: 0.00011367]
	Learning Rate: 0.000113669
	LOSS [training: 2.8503318150095325 | validation: 3.0102697259219844]
	TIME [epoch: 8.34 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850315287771069		[learning rate: 0.0001134]
	Learning Rate: 0.000113401
	LOSS [training: 2.850315287771069 | validation: 3.00753688795439]
	TIME [epoch: 8.38 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85146604783958		[learning rate: 0.00011313]
	Learning Rate: 0.000113133
	LOSS [training: 2.85146604783958 | validation: 3.0112320476114247]
	TIME [epoch: 8.34 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8513553050622735		[learning rate: 0.00011287]
	Learning Rate: 0.000112866
	LOSS [training: 2.8513553050622735 | validation: 3.009792369434356]
	TIME [epoch: 8.34 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8509495743675997		[learning rate: 0.0001126]
	Learning Rate: 0.0001126
	LOSS [training: 2.8509495743675997 | validation: 3.0089364933192577]
	TIME [epoch: 8.34 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852204858504715		[learning rate: 0.00011233]
	Learning Rate: 0.000112334
	LOSS [training: 2.852204858504715 | validation: 3.0108891353030236]
	TIME [epoch: 8.34 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8519335502601555		[learning rate: 0.00011207]
	Learning Rate: 0.000112069
	LOSS [training: 2.8519335502601555 | validation: 3.008782007253621]
	TIME [epoch: 8.34 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851955744151089		[learning rate: 0.00011181]
	Learning Rate: 0.000111805
	LOSS [training: 2.851955744151089 | validation: 3.010883760709529]
	TIME [epoch: 8.38 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851884771544367		[learning rate: 0.00011154]
	Learning Rate: 0.000111541
	LOSS [training: 2.851884771544367 | validation: 3.009743979327248]
	TIME [epoch: 8.34 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851584695495874		[learning rate: 0.00011128]
	Learning Rate: 0.000111278
	LOSS [training: 2.851584695495874 | validation: 3.012920075611256]
	TIME [epoch: 8.34 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851626996291524		[learning rate: 0.00011102]
	Learning Rate: 0.000111016
	LOSS [training: 2.851626996291524 | validation: 3.0106503236154563]
	TIME [epoch: 8.34 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8514579404411706		[learning rate: 0.00011075]
	Learning Rate: 0.000110754
	LOSS [training: 2.8514579404411706 | validation: 3.0088048308431303]
	TIME [epoch: 8.34 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507626345373818		[learning rate: 0.00011049]
	Learning Rate: 0.000110493
	LOSS [training: 2.8507626345373818 | validation: 3.0090759876184974]
	TIME [epoch: 8.34 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8523733062532393		[learning rate: 0.00011023]
	Learning Rate: 0.000110232
	LOSS [training: 2.8523733062532393 | validation: 3.012544392948658]
	TIME [epoch: 8.37 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8512798270516093		[learning rate: 0.00010997]
	Learning Rate: 0.000109972
	LOSS [training: 2.8512798270516093 | validation: 3.0103827990514755]
	TIME [epoch: 8.34 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501987307641015		[learning rate: 0.00010971]
	Learning Rate: 0.000109713
	LOSS [training: 2.8501987307641015 | validation: 3.0092933004045483]
	TIME [epoch: 8.34 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8506494847410035		[learning rate: 0.00010945]
	Learning Rate: 0.000109454
	LOSS [training: 2.8506494847410035 | validation: 3.0099003958846047]
	TIME [epoch: 8.33 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8503992937218348		[learning rate: 0.0001092]
	Learning Rate: 0.000109196
	LOSS [training: 2.8503992937218348 | validation: 3.0072697240793302]
	TIME [epoch: 8.33 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8501244782536586		[learning rate: 0.00010894]
	Learning Rate: 0.000108938
	LOSS [training: 2.8501244782536586 | validation: 3.009949982650819]
	TIME [epoch: 8.34 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851167536274508		[learning rate: 0.00010868]
	Learning Rate: 0.000108681
	LOSS [training: 2.851167536274508 | validation: 3.0087286005786336]
	TIME [epoch: 8.37 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850592417067819		[learning rate: 0.00010842]
	Learning Rate: 0.000108425
	LOSS [training: 2.850592417067819 | validation: 3.0082032880731315]
	TIME [epoch: 8.35 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8496496368464346		[learning rate: 0.00010817]
	Learning Rate: 0.000108169
	LOSS [training: 2.8496496368464346 | validation: 3.009163847336495]
	TIME [epoch: 8.33 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850530964117691		[learning rate: 0.00010791]
	Learning Rate: 0.000107914
	LOSS [training: 2.850530964117691 | validation: 3.0090506072694385]
	TIME [epoch: 8.34 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8508503338644604		[learning rate: 0.00010766]
	Learning Rate: 0.000107659
	LOSS [training: 2.8508503338644604 | validation: 3.009915070780879]
	TIME [epoch: 8.33 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8511205701424007		[learning rate: 0.00010741]
	Learning Rate: 0.000107405
	LOSS [training: 2.8511205701424007 | validation: 3.009785213428714]
	TIME [epoch: 8.33 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8512702006900392		[learning rate: 0.00010715]
	Learning Rate: 0.000107152
	LOSS [training: 2.8512702006900392 | validation: 3.011613718205445]
	TIME [epoch: 8.35 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8526701347478225		[learning rate: 0.0001069]
	Learning Rate: 0.000106899
	LOSS [training: 2.8526701347478225 | validation: 3.0090882033055326]
	TIME [epoch: 8.37 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8507855566951834		[learning rate: 0.00010665]
	Learning Rate: 0.000106647
	LOSS [training: 2.8507855566951834 | validation: 3.009256194377275]
	TIME [epoch: 8.33 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8528936145098265		[learning rate: 0.0001064]
	Learning Rate: 0.000106395
	LOSS [training: 2.8528936145098265 | validation: 3.008052466175238]
	TIME [epoch: 8.33 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8509448036290026		[learning rate: 0.00010614]
	Learning Rate: 0.000106145
	LOSS [training: 2.8509448036290026 | validation: 3.0098979083067823]
	TIME [epoch: 8.32 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851509976788753		[learning rate: 0.00010589]
	Learning Rate: 0.000105894
	LOSS [training: 2.851509976788753 | validation: 3.0082038428404427]
	TIME [epoch: 8.33 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8510335167543164		[learning rate: 0.00010564]
	Learning Rate: 0.000105644
	LOSS [training: 2.8510335167543164 | validation: 3.010322418535109]
	TIME [epoch: 8.34 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.850866659269004		[learning rate: 0.0001054]
	Learning Rate: 0.000105395
	LOSS [training: 2.850866659269004 | validation: 3.0133305375992645]
	TIME [epoch: 8.36 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852435739775183		[learning rate: 0.00010515]
	Learning Rate: 0.000105147
	LOSS [training: 2.852435739775183 | validation: 3.011050521959471]
	TIME [epoch: 8.33 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8522741874886326		[learning rate: 0.0001049]
	Learning Rate: 0.000104899
	LOSS [training: 2.8522741874886326 | validation: 3.011228091873914]
	TIME [epoch: 8.33 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8520192724001343		[learning rate: 0.00010465]
	Learning Rate: 0.000104651
	LOSS [training: 2.8520192724001343 | validation: 3.011554760356754]
	TIME [epoch: 8.33 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8532267775365536		[learning rate: 0.0001044]
	Learning Rate: 0.000104404
	LOSS [training: 2.8532267775365536 | validation: 3.0125757120110697]
	TIME [epoch: 8.33 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852963393043394		[learning rate: 0.00010416]
	Learning Rate: 0.000104158
	LOSS [training: 2.852963393043394 | validation: 3.010440088593799]
	TIME [epoch: 8.34 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8533139593025254		[learning rate: 0.00010391]
	Learning Rate: 0.000103912
	LOSS [training: 2.8533139593025254 | validation: 3.0118770627501394]
	TIME [epoch: 8.38 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.851849643502115		[learning rate: 0.00010367]
	Learning Rate: 0.000103667
	LOSS [training: 2.851849643502115 | validation: 3.0117189515128775]
	TIME [epoch: 8.34 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.852808153093605		[learning rate: 0.00010342]
	Learning Rate: 0.000103423
	LOSS [training: 2.852808153093605 | validation: 3.0117452793087462]
	TIME [epoch: 8.34 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8521290409532227		[learning rate: 0.00010318]
	Learning Rate: 0.000103179
	LOSS [training: 2.8521290409532227 | validation: 3.013251678424263]
	TIME [epoch: 8.33 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85298078784799		[learning rate: 0.00010294]
	Learning Rate: 0.000102935
	LOSS [training: 2.85298078784799 | validation: 3.0105184502250437]
	TIME [epoch: 8.34 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8535746906522905		[learning rate: 0.00010269]
	Learning Rate: 0.000102692
	LOSS [training: 2.8535746906522905 | validation: 3.013105669197282]
	TIME [epoch: 8.34 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8533461641722715		[learning rate: 0.00010245]
	Learning Rate: 0.00010245
	LOSS [training: 2.8533461641722715 | validation: 3.0131022402645016]
	TIME [epoch: 8.38 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8533506666237143		[learning rate: 0.00010221]
	Learning Rate: 0.000102209
	LOSS [training: 2.8533506666237143 | validation: 3.011803772476557]
	TIME [epoch: 8.35 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8523978211654253		[learning rate: 0.00010197]
	Learning Rate: 0.000101967
	LOSS [training: 2.8523978211654253 | validation: 3.0110900333107047]
	TIME [epoch: 8.34 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8530617935891303		[learning rate: 0.00010173]
	Learning Rate: 0.000101727
	LOSS [training: 2.8530617935891303 | validation: 3.0122993209473545]
	TIME [epoch: 8.34 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853093106826045		[learning rate: 0.00010149]
	Learning Rate: 0.000101487
	LOSS [training: 2.853093106826045 | validation: 3.012247206629428]
	TIME [epoch: 8.33 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853300137402961		[learning rate: 0.00010125]
	Learning Rate: 0.000101248
	LOSS [training: 2.853300137402961 | validation: 3.0100708901048563]
	TIME [epoch: 8.35 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8531480376014913		[learning rate: 0.00010101]
	Learning Rate: 0.000101009
	LOSS [training: 2.8531480376014913 | validation: 3.011081014239564]
	TIME [epoch: 8.37 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853460299946791		[learning rate: 0.00010077]
	Learning Rate: 0.000100771
	LOSS [training: 2.853460299946791 | validation: 3.0142346974244845]
	TIME [epoch: 8.36 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8523626601549044		[learning rate: 0.00010053]
	Learning Rate: 0.000100533
	LOSS [training: 2.8523626601549044 | validation: 3.0106489880032012]
	TIME [epoch: 8.34 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8528329240231174		[learning rate: 0.0001003]
	Learning Rate: 0.000100296
	LOSS [training: 2.8528329240231174 | validation: 3.011499662095934]
	TIME [epoch: 8.34 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8515042127231105		[learning rate: 0.00010006]
	Learning Rate: 0.000100059
	LOSS [training: 2.8515042127231105 | validation: 3.0094841573995756]
	TIME [epoch: 8.34 sec]
Finished training in 17095.540 seconds.
