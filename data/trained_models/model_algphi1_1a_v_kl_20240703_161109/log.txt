Args:
Namespace(name='model_algphi1_1a_v_kl', outdir='out/model_training/model_algphi1_1a_v_kl', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='binary_choice', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3131528438

Training model...

Saving initial model state to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 10.744097859775884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.744097859775884 | validation: 10.671930597425694]
	TIME [epoch: 94.6 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 10.763347627224555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.763347627224555 | validation: 10.702414612057666]
	TIME [epoch: 4.38 sec]
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 10.770820543702385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.770820543702385 | validation: 10.688008867224553]
	TIME [epoch: 4.32 sec]
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 10.762711550243178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.762711550243178 | validation: 10.673294589700893]
	TIME [epoch: 4.33 sec]
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 10.745717147235855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.745717147235855 | validation: 10.630700150148707]
	TIME [epoch: 4.36 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 10.698847672361158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.698847672361158 | validation: 10.585336953763376]
	TIME [epoch: 4.34 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 10.676301176373578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.676301176373578 | validation: 10.573690581135313]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 10.6747346400238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.6747346400238 | validation: 10.587283968726648]
	TIME [epoch: 4.31 sec]
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 10.67969118251196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.67969118251196 | validation: 10.59155933772891]
	TIME [epoch: 4.3 sec]
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 10.68081851776643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.68081851776643 | validation: 10.57831562512113]
	TIME [epoch: 4.3 sec]
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 10.664976630528667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.664976630528667 | validation: 10.558505650853094]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 10.642912774908213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.642912774908213 | validation: 10.54205365099521]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 10.624970584080918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.624970584080918 | validation: 10.528011792350519]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 10.63071688279506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.63071688279506 | validation: 10.538941027302371]
	TIME [epoch: 4.3 sec]
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 10.637938263014073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.637938263014073 | validation: 10.55568771429532]
	TIME [epoch: 4.34 sec]
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 10.663664168465498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.663664168465498 | validation: 10.55699307143253]
	TIME [epoch: 4.32 sec]
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 10.648560542095165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.648560542095165 | validation: 10.545874736430239]
	TIME [epoch: 4.3 sec]
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 10.636541623362659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.636541623362659 | validation: 10.542177230883393]
	TIME [epoch: 4.3 sec]
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 10.6187134618649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.6187134618649 | validation: 10.561522180021655]
	TIME [epoch: 4.31 sec]
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 10.633575083875119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.633575083875119 | validation: 10.527728723938221]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 10.63139032675424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.63139032675424 | validation: 10.530996066701416]
	TIME [epoch: 4.31 sec]
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 10.613024733457475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.613024733457475 | validation: 10.511214489641855]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 10.597038758199716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.597038758199716 | validation: 10.482340290022801]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 10.580390710108725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.580390710108725 | validation: 10.501040915707327]
	TIME [epoch: 4.32 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 10.54876981849387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.54876981849387 | validation: 10.466075065810319]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 10.551779208634839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.551779208634839 | validation: 10.457822202551597]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_26.pth
	Model improved!!!
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 10.537771124452796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.537771124452796 | validation: 10.456697481389835]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 10.51612469898418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.51612469898418 | validation: 10.41718391282982]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 10.467366159400811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.467366159400811 | validation: 10.383887079270046]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 10.454674477044666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.454674477044666 | validation: 10.380822926085607]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_30.pth
	Model improved!!!
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 10.439259784826735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.439259784826735 | validation: 10.352711300259081]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 10.406951204348623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.406951204348623 | validation: 10.316346083732228]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_32.pth
	Model improved!!!
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 10.360317064793335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.360317064793335 | validation: 10.27008269052715]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 10.320972532591531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.320972532591531 | validation: 10.227510079955058]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 10.301329389320667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.301329389320667 | validation: 10.259018941918745]
	TIME [epoch: 4.31 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 10.32412060504427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.32412060504427 | validation: 10.231877797072055]
	TIME [epoch: 4.3 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 10.280389744585651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.280389744585651 | validation: 10.155392179974111]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_37.pth
	Model improved!!!
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 10.198537768335449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.198537768335449 | validation: 10.034468042503136]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_38.pth
	Model improved!!!
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 10.121027840610136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.121027840610136 | validation: 9.99164203405953]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_39.pth
	Model improved!!!
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 10.026984574227198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.026984574227198 | validation: 9.857635657485307]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_40.pth
	Model improved!!!
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 9.958153770385735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.958153770385735 | validation: 9.951833109531332]
	TIME [epoch: 4.3 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 9.96234670757354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.96234670757354 | validation: 9.794782897511185]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_42.pth
	Model improved!!!
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 9.915029599651547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.915029599651547 | validation: 9.876203546081669]
	TIME [epoch: 4.34 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 9.861577152862088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.861577152862088 | validation: 9.743247316502874]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_44.pth
	Model improved!!!
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 9.716719745793313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.716719745793313 | validation: 9.614035183078322]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_45.pth
	Model improved!!!
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 9.670680077437652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.670680077437652 | validation: 9.579716354308346]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_46.pth
	Model improved!!!
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 9.629693054256819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.629693054256819 | validation: 9.572714097615227]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_47.pth
	Model improved!!!
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 9.466307704481276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.466307704481276 | validation: 9.316633866527237]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_48.pth
	Model improved!!!
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 9.318018458977049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.318018458977049 | validation: 9.242902434439433]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_49.pth
	Model improved!!!
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 9.251714155946289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.251714155946289 | validation: 9.154416423580162]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_50.pth
	Model improved!!!
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 9.110959115600577		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 9.110959115600577 | validation: 8.917650443700321]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_51.pth
	Model improved!!!
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 9.017794100761524		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 9.017794100761524 | validation: 9.012239652013847]
	TIME [epoch: 4.34 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 9.099271994983397		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 9.099271994983397 | validation: 9.083417499053898]
	TIME [epoch: 4.31 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 9.167834897336036		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 9.167834897336036 | validation: 9.26761990845334]
	TIME [epoch: 4.31 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 9.2800717801655		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 9.2800717801655 | validation: 9.224809818945817]
	TIME [epoch: 4.31 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 9.324292850432712		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 9.324292850432712 | validation: 9.235341093940551]
	TIME [epoch: 4.3 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 9.27501510936277		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 9.27501510936277 | validation: 9.127684474975048]
	TIME [epoch: 4.31 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 9.139653966490453		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 9.139653966490453 | validation: 9.045343046855974]
	TIME [epoch: 4.3 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 9.015265071497922		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 9.015265071497922 | validation: 8.985940377359347]
	TIME [epoch: 4.3 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 8.996619337859663		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 8.996619337859663 | validation: 8.899142672354833]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 8.964853654440418		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 8.964853654440418 | validation: 8.868691614176011]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_61.pth
	Model improved!!!
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 8.939901388109979		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 8.939901388109979 | validation: 8.891434966311964]
	TIME [epoch: 4.33 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 8.936238750013283		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 8.936238750013283 | validation: 8.840380369016438]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_63.pth
	Model improved!!!
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 8.966270609671538		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 8.966270609671538 | validation: 8.892744443996126]
	TIME [epoch: 4.3 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 8.882973209488835		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 8.882973209488835 | validation: 8.806184352801878]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_65.pth
	Model improved!!!
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 8.798275540743504		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 8.798275540743504 | validation: 8.681445139771004]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_66.pth
	Model improved!!!
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 8.747327936923066		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 8.747327936923066 | validation: 8.659156779396497]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_67.pth
	Model improved!!!
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 8.685767802624602		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 8.685767802624602 | validation: 8.669174276707514]
	TIME [epoch: 4.31 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 8.672794168327801		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 8.672794168327801 | validation: 8.520372242867722]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 8.548410096261156		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 8.548410096261156 | validation: 8.417825702492973]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_70.pth
	Model improved!!!
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 8.536221466411341		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 8.536221466411341 | validation: 8.376987634931922]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 8.388680340714325		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 8.388680340714325 | validation: 8.22116428572891]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 8.251602002507697		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 8.251602002507697 | validation: 8.181263124108774]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_73.pth
	Model improved!!!
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 8.250484476668174		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 8.250484476668174 | validation: 8.221949602156595]
	TIME [epoch: 4.3 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 8.259878715235029		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 8.259878715235029 | validation: 8.361936043025086]
	TIME [epoch: 4.29 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 8.389861117822546		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 8.389861117822546 | validation: 8.438652430451832]
	TIME [epoch: 4.3 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 8.3726245783235		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 8.3726245783235 | validation: 8.41207360550316]
	TIME [epoch: 4.3 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 8.4455609637733		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 8.4455609637733 | validation: 8.45796662749011]
	TIME [epoch: 4.29 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 8.50363046298795		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 8.50363046298795 | validation: 8.460526251886623]
	TIME [epoch: 4.3 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 8.442025771742212		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 8.442025771742212 | validation: 8.473351391007085]
	TIME [epoch: 4.34 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 8.519422787244482		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 8.519422787244482 | validation: 8.55170129753001]
	TIME [epoch: 4.3 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 8.520516027828261		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 8.520516027828261 | validation: 8.433034024175521]
	TIME [epoch: 4.3 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 8.519454543173689		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 8.519454543173689 | validation: 8.4689186858148]
	TIME [epoch: 4.3 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 8.521102772364241		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 8.521102772364241 | validation: 8.376847853528247]
	TIME [epoch: 4.3 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 8.493009206817904		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 8.493009206817904 | validation: 8.432877557927357]
	TIME [epoch: 4.29 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 8.400140004703104		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 8.400140004703104 | validation: 8.358088680404508]
	TIME [epoch: 4.3 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 8.431807417698742		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 8.431807417698742 | validation: 8.347577250878725]
	TIME [epoch: 4.29 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 8.510693489826574		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 8.510693489826574 | validation: 8.303319614754745]
	TIME [epoch: 4.3 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 8.519204599968095		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 8.519204599968095 | validation: 8.482761805691217]
	TIME [epoch: 4.3 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 8.46221199360582		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 8.46221199360582 | validation: 8.455631702928954]
	TIME [epoch: 4.33 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 8.404606159357126		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 8.404606159357126 | validation: 8.226003508652724]
	TIME [epoch: 4.3 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 8.352480356209014		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 8.352480356209014 | validation: 8.193215593753111]
	TIME [epoch: 4.3 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 8.266837912927427		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 8.266837912927427 | validation: 8.230995633083555]
	TIME [epoch: 4.3 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 8.292428933378538		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 8.292428933378538 | validation: 8.333150288593139]
	TIME [epoch: 4.3 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 8.380523419752059		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 8.380523419752059 | validation: 8.207742805597498]
	TIME [epoch: 4.29 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 8.366035156082189		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 8.366035156082189 | validation: 8.278919329572062]
	TIME [epoch: 4.29 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 8.365653778773208		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 8.365653778773208 | validation: 8.25563604478495]
	TIME [epoch: 4.3 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 8.364827189180286		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 8.364827189180286 | validation: 8.248119159107226]
	TIME [epoch: 4.3 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 8.313322467903948		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 8.313322467903948 | validation: 8.213652097010982]
	TIME [epoch: 4.3 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 8.34568739447252		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 8.34568739447252 | validation: 8.28371348937387]
	TIME [epoch: 4.33 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 8.369269384410195		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 8.369269384410195 | validation: 8.363534782943937]
	TIME [epoch: 4.3 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 8.381739368355367		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 8.381739368355367 | validation: 8.292646261762805]
	TIME [epoch: 4.3 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 8.446961805858098		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 8.446961805858098 | validation: 8.336063858254839]
	TIME [epoch: 4.3 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 8.449209279268064		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 8.449209279268064 | validation: 8.252509987108395]
	TIME [epoch: 4.29 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 8.348130942760355		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 8.348130942760355 | validation: 8.307000608224506]
	TIME [epoch: 4.29 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 8.369807655769392		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 8.369807655769392 | validation: 8.274230966933967]
	TIME [epoch: 4.29 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 8.36512609972719		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 8.36512609972719 | validation: 8.271041654085003]
	TIME [epoch: 4.3 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 8.367690078402521		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 8.367690078402521 | validation: 8.242729511504368]
	TIME [epoch: 4.29 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 8.376640551934845		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 8.376640551934845 | validation: 8.356567759063356]
	TIME [epoch: 4.31 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 8.361790257395848		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 8.361790257395848 | validation: 8.19141822106656]
	TIME [epoch: 4.32 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 8.381309565518809		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 8.381309565518809 | validation: 8.17425513171219]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_111.pth
	Model improved!!!
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 8.328593811620074		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 8.328593811620074 | validation: 8.178295112329081]
	TIME [epoch: 4.31 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 8.325531203118821		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 8.325531203118821 | validation: 8.177878548618114]
	TIME [epoch: 4.31 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 8.303627873652953		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 8.303627873652953 | validation: 8.177830192571177]
	TIME [epoch: 4.3 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 8.235809858086824		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 8.235809858086824 | validation: 8.056990082109344]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_115.pth
	Model improved!!!
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 8.175472887454571		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 8.175472887454571 | validation: 8.02146069262721]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_116.pth
	Model improved!!!
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 8.220876053817312		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 8.220876053817312 | validation: 8.166287978571122]
	TIME [epoch: 4.3 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 8.26632624541362		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 8.26632624541362 | validation: 8.212179138723844]
	TIME [epoch: 4.3 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 8.250473867278195		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 8.250473867278195 | validation: 8.09609522834234]
	TIME [epoch: 4.32 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 8.250799001581946		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 8.250799001581946 | validation: 7.965588582563134]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_120.pth
	Model improved!!!
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 8.188139800625946		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 8.188139800625946 | validation: 8.136330083152629]
	TIME [epoch: 4.32 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 8.214185652007721		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 8.214185652007721 | validation: 8.111431075921875]
	TIME [epoch: 4.3 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 8.191539722760913		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 8.191539722760913 | validation: 8.170073526286169]
	TIME [epoch: 4.3 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 8.174950958764718		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 8.174950958764718 | validation: 8.066036258208616]
	TIME [epoch: 4.29 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 8.220719951988768		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 8.220719951988768 | validation: 8.150574168801803]
	TIME [epoch: 4.3 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 8.234096350902611		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 8.234096350902611 | validation: 8.12008027128445]
	TIME [epoch: 4.3 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 8.262823989864705		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 8.262823989864705 | validation: 8.065508325799305]
	TIME [epoch: 4.29 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 8.224904099016623		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 8.224904099016623 | validation: 8.170033327091717]
	TIME [epoch: 4.3 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 8.177626565196942		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 8.177626565196942 | validation: 8.232324495248799]
	TIME [epoch: 4.33 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 8.27001349585631		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 8.27001349585631 | validation: 8.194121038579574]
	TIME [epoch: 4.31 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 8.190453646789495		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 8.190453646789495 | validation: 8.06777076671019]
	TIME [epoch: 4.3 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 8.189835174395107		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 8.189835174395107 | validation: 8.177153017561018]
	TIME [epoch: 4.29 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 8.149831812320004		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 8.149831812320004 | validation: 8.00997194090014]
	TIME [epoch: 4.3 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 8.164688549857612		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 8.164688549857612 | validation: 7.998629512726197]
	TIME [epoch: 4.31 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 8.202716665601965		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 8.202716665601965 | validation: 8.049338384136714]
	TIME [epoch: 4.31 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 8.137984375016487		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 8.137984375016487 | validation: 8.023390348280063]
	TIME [epoch: 4.31 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 8.074917322123344		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 8.074917322123344 | validation: 8.109985836816339]
	TIME [epoch: 4.31 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 8.087804448956721		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 8.087804448956721 | validation: 8.151625679000801]
	TIME [epoch: 4.3 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 8.146790479827434		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 8.146790479827434 | validation: 8.062602804202106]
	TIME [epoch: 4.34 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 8.184693875506728		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 8.184693875506728 | validation: 8.076480253172406]
	TIME [epoch: 4.32 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 8.196546193676985		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 8.196546193676985 | validation: 8.183565373162086]
	TIME [epoch: 4.31 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 8.239321683116454		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 8.239321683116454 | validation: 8.098164672082559]
	TIME [epoch: 4.3 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 8.26572484380494		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 8.26572484380494 | validation: 8.217776236884681]
	TIME [epoch: 4.31 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 8.276012657011883		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 8.276012657011883 | validation: 8.162748805359547]
	TIME [epoch: 4.3 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 8.301385277914752		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 8.301385277914752 | validation: 8.160710257330226]
	TIME [epoch: 4.3 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 8.288659442707118		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 8.288659442707118 | validation: 8.153125534575196]
	TIME [epoch: 4.3 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 8.236767978799305		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 8.236767978799305 | validation: 8.10913634682579]
	TIME [epoch: 4.3 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 8.20234161301078		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 8.20234161301078 | validation: 8.122531206281735]
	TIME [epoch: 4.31 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 8.202621769866555		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 8.202621769866555 | validation: 8.152974373348155]
	TIME [epoch: 4.34 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 8.232187894434096		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 8.232187894434096 | validation: 8.06438924084313]
	TIME [epoch: 4.31 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 8.175837150671521		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 8.175837150671521 | validation: 8.095199817547295]
	TIME [epoch: 4.3 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 8.173621078084233		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 8.173621078084233 | validation: 8.00054396742828]
	TIME [epoch: 4.3 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 8.15467484685663		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 8.15467484685663 | validation: 8.035694246398426]
	TIME [epoch: 4.31 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 8.13462296781006		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 8.13462296781006 | validation: 7.998693379761493]
	TIME [epoch: 4.3 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 8.096160316834025		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 8.096160316834025 | validation: 8.038580644672745]
	TIME [epoch: 4.3 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 8.084124950055028		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 8.084124950055028 | validation: 8.019399414848145]
	TIME [epoch: 4.3 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 8.050811888101036		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 8.050811888101036 | validation: 7.960986383532982]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_157.pth
	Model improved!!!
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 8.065277834188121		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 8.065277834188121 | validation: 7.926218038943418]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_158.pth
	Model improved!!!
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 8.077731028811044		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 8.077731028811044 | validation: 7.9289001416154425]
	TIME [epoch: 4.34 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 8.040521977239749		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 8.040521977239749 | validation: 8.02817087631407]
	TIME [epoch: 4.3 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 8.113755345815033		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 8.113755345815033 | validation: 8.059596076794453]
	TIME [epoch: 4.3 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 8.117721438961595		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 8.117721438961595 | validation: 7.980256250759622]
	TIME [epoch: 4.3 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 8.089816774596851		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 8.089816774596851 | validation: 7.979518943548471]
	TIME [epoch: 4.29 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 8.060832223823047		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 8.060832223823047 | validation: 8.001644506386082]
	TIME [epoch: 4.3 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 8.040432018504564		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 8.040432018504564 | validation: 7.910466609195494]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_165.pth
	Model improved!!!
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 8.00198619098893		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 8.00198619098893 | validation: 7.942887884207169]
	TIME [epoch: 4.32 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 8.039964451379321		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 8.039964451379321 | validation: 7.963123872146196]
	TIME [epoch: 4.31 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 8.03635126584146		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 8.03635126584146 | validation: 7.9034943099159936]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_168.pth
	Model improved!!!
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 7.996015165787185		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 7.996015165787185 | validation: 8.027190210082487]
	TIME [epoch: 4.34 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 8.029656171801829		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 8.029656171801829 | validation: 7.970766941263186]
	TIME [epoch: 4.31 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 7.998591500890895		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 7.998591500890895 | validation: 7.906091320806652]
	TIME [epoch: 4.31 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 8.023598028786814		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 8.023598028786814 | validation: 8.020576401921002]
	TIME [epoch: 4.31 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 8.030316934193495		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 8.030316934193495 | validation: 7.855519601368852]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_173.pth
	Model improved!!!
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 7.995819530732307		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 7.995819530732307 | validation: 7.953082357712941]
	TIME [epoch: 4.31 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 8.047991203626834		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 8.047991203626834 | validation: 7.891011546925782]
	TIME [epoch: 4.31 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 8.061685323552513		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 8.061685323552513 | validation: 7.944658430331894]
	TIME [epoch: 4.31 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 8.029118851787347		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 8.029118851787347 | validation: 7.921372809880112]
	TIME [epoch: 4.31 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 8.027172269760733		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 8.027172269760733 | validation: 7.9183421488017505]
	TIME [epoch: 4.34 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 7.959300703863505		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 7.959300703863505 | validation: 7.890055416268034]
	TIME [epoch: 4.35 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 7.957801559834537		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 7.957801559834537 | validation: 7.848083043905454]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_180.pth
	Model improved!!!
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 8.00336062211274		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 8.00336062211274 | validation: 7.928562464094449]
	TIME [epoch: 4.3 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 8.040139466751612		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 8.040139466751612 | validation: 8.018421096247703]
	TIME [epoch: 4.31 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 8.004794682812593		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 8.004794682812593 | validation: 8.075231562996994]
	TIME [epoch: 4.3 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 8.047588515076846		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 8.047588515076846 | validation: 7.91193353338787]
	TIME [epoch: 4.3 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 7.989911010905428		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 7.989911010905428 | validation: 7.916799361046829]
	TIME [epoch: 4.3 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 8.030197168724953		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 8.030197168724953 | validation: 8.058854875975857]
	TIME [epoch: 4.3 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 7.992314407673651		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 7.992314407673651 | validation: 7.941886727442847]
	TIME [epoch: 4.3 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 7.980765129788271		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 7.980765129788271 | validation: 7.862438094694554]
	TIME [epoch: 4.34 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 7.9865148439570195		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 7.9865148439570195 | validation: 7.984992370431225]
	TIME [epoch: 4.32 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 7.937741541749491		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 7.937741541749491 | validation: 7.9159374933331215]
	TIME [epoch: 4.3 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 7.971625092688123		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 7.971625092688123 | validation: 7.972185779697819]
	TIME [epoch: 4.3 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 7.980919699716277		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 7.980919699716277 | validation: 7.8331859029721285]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_192.pth
	Model improved!!!
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 7.941644547509921		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 7.941644547509921 | validation: 7.959622502622974]
	TIME [epoch: 4.3 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 7.983348111546989		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 7.983348111546989 | validation: 7.862353433630329]
	TIME [epoch: 4.3 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 7.975943321134233		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 7.975943321134233 | validation: 7.879515487669098]
	TIME [epoch: 4.29 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 7.981106619601604		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 7.981106619601604 | validation: 7.896504654551198]
	TIME [epoch: 4.3 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 7.963274240948184		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 7.963274240948184 | validation: 7.881400954874623]
	TIME [epoch: 4.3 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 7.973061256248642		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 7.973061256248642 | validation: 7.838346270941461]
	TIME [epoch: 4.34 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 8.023880960364014		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 8.023880960364014 | validation: 7.906310131449378]
	TIME [epoch: 4.31 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 7.980922426208595		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 7.980922426208595 | validation: 7.9492175597608625]
	TIME [epoch: 4.3 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 7.966094059336154		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 7.966094059336154 | validation: 7.913467587091319]
	TIME [epoch: 4.3 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 7.974849704850463		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 7.974849704850463 | validation: 7.9141138830080235]
	TIME [epoch: 4.3 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 7.986815793679986		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 7.986815793679986 | validation: 7.939546401002791]
	TIME [epoch: 4.3 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 7.980188491650873		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 7.980188491650873 | validation: 7.930799724848807]
	TIME [epoch: 4.3 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 8.040035011866182		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 8.040035011866182 | validation: 7.887284216018012]
	TIME [epoch: 4.3 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 7.956324693897315		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 7.956324693897315 | validation: 7.936379173678464]
	TIME [epoch: 4.29 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 8.00705516994726		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 8.00705516994726 | validation: 7.866396898219445]
	TIME [epoch: 4.31 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 8.003139754142877		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 8.003139754142877 | validation: 7.959473052879247]
	TIME [epoch: 4.33 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 7.995854727110585		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 7.995854727110585 | validation: 7.825504725141819]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_209.pth
	Model improved!!!
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 7.979096938586294		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 7.979096938586294 | validation: 7.853934134745815]
	TIME [epoch: 4.3 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 7.975135483026543		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 7.975135483026543 | validation: 7.91866692833338]
	TIME [epoch: 4.29 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 7.945561685577756		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 7.945561685577756 | validation: 7.9363618296110126]
	TIME [epoch: 4.29 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 7.946957184452007		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 7.946957184452007 | validation: 7.728619187873424]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_kl_20240703_161109/states/model_algphi1_1a_v_kl_213.pth
	Model improved!!!
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 7.936399064478811		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 7.936399064478811 | validation: 7.849214731918867]
	TIME [epoch: 4.31 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 7.950059125706079		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 7.950059125706079 | validation: 7.892679955309032]
	TIME [epoch: 4.3 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 7.888554161698362		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 7.888554161698362 | validation: 7.795384281309738]
	TIME [epoch: 4.3 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 7.99492243275424		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 7.99492243275424 | validation: 7.9626039476875805]
	TIME [epoch: 4.32 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 7.91816926581037		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 7.91816926581037 | validation: 7.854159059779534]
	TIME [epoch: 4.34 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 7.963946227285322		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 7.963946227285322 | validation: 7.927778982998097]
	TIME [epoch: 4.31 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 7.93134776861009		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 7.93134776861009 | validation: 7.86741348028607]
	TIME [epoch: 4.31 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 7.9292094293442545		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 7.9292094293442545 | validation: 7.8949102424583675]
	TIME [epoch: 4.31 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 7.91633243882276		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 7.91633243882276 | validation: 7.9068483535184075]
	TIME [epoch: 4.31 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 7.94079242789675		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 7.94079242789675 | validation: 7.878843231477416]
	TIME [epoch: 4.31 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 7.936510943457306		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 7.936510943457306 | validation: 7.870432429856153]
	TIME [epoch: 4.31 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 7.977892815460981		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 7.977892815460981 | validation: 7.961457090015782]
	TIME [epoch: 4.31 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 7.946549577493876		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 7.946549577493876 | validation: 7.799135652354989]
	TIME [epoch: 4.31 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 8.011259820040731		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 8.011259820040731 | validation: 7.908392055671762]
	TIME [epoch: 4.33 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 7.91642059232389		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 7.91642059232389 | validation: 7.958216265275174]
	TIME [epoch: 4.34 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 7.918819655094849		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 7.918819655094849 | validation: 7.92424926826156]
	TIME [epoch: 4.32 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 7.961923142358065		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 7.961923142358065 | validation: 7.882517329906854]
	TIME [epoch: 4.31 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 7.94285748859826		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 7.94285748859826 | validation: 7.7710825232980385]
	TIME [epoch: 4.31 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 7.9343623781348835		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 7.9343623781348835 | validation: 7.9392596656988115]
	TIME [epoch: 4.31 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 7.940627362480097		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 7.940627362480097 | validation: 7.874176817299437]
	TIME [epoch: 4.31 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 7.96077599050643		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 7.96077599050643 | validation: 7.874926037466778]
	TIME [epoch: 4.31 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 7.9543893862251895		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 7.9543893862251895 | validation: 7.906859180163794]
	TIME [epoch: 4.3 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 7.945843014589856		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 7.945843014589856 | validation: 7.970519283483229]
	TIME [epoch: 4.31 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 7.964683711650064		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 7.964683711650064 | validation: 7.803433932355862]
	TIME [epoch: 4.32 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 7.972660213188077		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 7.972660213188077 | validation: 7.838117158164643]
	TIME [epoch: 4.35 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 7.963943699945707		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 7.963943699945707 | validation: 7.807940789419369]
	TIME [epoch: 4.32 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 7.954879229237852		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 7.954879229237852 | validation: 7.836848301379938]
	TIME [epoch: 4.31 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 7.9169793188267485		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 7.9169793188267485 | validation: 7.935608298629012]
	TIME [epoch: 4.31 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 7.95477546512709		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 7.95477546512709 | validation: 7.92911022961605]
	TIME [epoch: 4.31 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 7.986837163492849		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 7.986837163492849 | validation: 7.91484451191663]
	TIME [epoch: 4.31 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 7.938182142798682		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 7.938182142798682 | validation: 7.9304080657645475]
	TIME [epoch: 4.31 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 7.957199566854922		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 7.957199566854922 | validation: 7.803392007660113]
	TIME [epoch: 4.31 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 7.958887665479539		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 7.958887665479539 | validation: 7.9600969094995655]
	TIME [epoch: 4.31 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 7.989144981714146		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 7.989144981714146 | validation: 7.964188768503406]
	TIME [epoch: 4.33 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 7.950759234129125		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 7.950759234129125 | validation: 7.946854844338294]
	TIME [epoch: 4.34 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 8.02566827524683		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 8.02566827524683 | validation: 7.787436298182174]
	TIME [epoch: 4.31 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 7.967970073107597		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 7.967970073107597 | validation: 7.788088270287712]
	TIME [epoch: 4.31 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 7.972366223220991		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 7.972366223220991 | validation: 7.90273893571819]
	TIME [epoch: 4.31 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 7.994081735814344		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 7.994081735814344 | validation: 8.00993234813391]
	TIME [epoch: 4.31 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 8.019457995581828		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 8.019457995581828 | validation: 7.900644987901158]
	TIME [epoch: 4.3 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 7.98523455422968		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 7.98523455422968 | validation: 7.919594984373379]
	TIME [epoch: 4.3 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 7.99364074238346		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 7.99364074238346 | validation: 7.982633964627373]
	TIME [epoch: 4.3 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 7.968242080221122		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 7.968242080221122 | validation: 8.013178603962167]
	TIME [epoch: 4.3 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 7.99726324937793		[learning rate: 0.00041847]
nan encountered in epoch 257 (validation loss).
	Learning Rate: 0.000418472
	LOSS [training: 7.99726324937793 | validation: nan]
	TIME [epoch: 4.32 sec]
EPOCH 258/500:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
