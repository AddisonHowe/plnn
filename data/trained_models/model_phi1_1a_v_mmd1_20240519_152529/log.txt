Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 118648596

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.78253711826015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.78253711826015 | validation: 6.7436632554165]
	TIME [epoch: 101 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.408753627848591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.408753627848591 | validation: 6.606273129590312]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.203628825968556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.203628825968556 | validation: 6.251357649533247]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.013514508385292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.013514508385292 | validation: 6.141466896253424]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.6625741248510915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6625741248510915 | validation: 5.701679135930815]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.227263852457588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.227263852457588 | validation: 5.719952370959522]
	TIME [epoch: 8.26 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.337658367447672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.337658367447672 | validation: 5.7831280793906945]
	TIME [epoch: 8.24 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0711416565057155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0711416565057155 | validation: 5.191269331070719]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815821351274451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.815821351274451 | validation: 5.094193732707083]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.702986638309595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.702986638309595 | validation: 5.05808673476454]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.680014747054462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.680014747054462 | validation: 4.870041872263357]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.565380875231848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.565380875231848 | validation: 4.833142305559363]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.545614130869355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.545614130869355 | validation: 4.862086580313585]
	TIME [epoch: 8.22 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.489576897950508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.489576897950508 | validation: 4.606604362769847]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.344725813133044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.344725813133044 | validation: 4.652079090457436]
	TIME [epoch: 8.31 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.345312876735393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.345312876735393 | validation: 4.501442635982396]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.216486827122393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.216486827122393 | validation: 4.524551412061658]
	TIME [epoch: 8.32 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.169293075222876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.169293075222876 | validation: 4.522210303041852]
	TIME [epoch: 8.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156124196625379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.156124196625379 | validation: 4.253357412021238]
	TIME [epoch: 8.28 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.057825385383435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.057825385383435 | validation: 4.211503054502108]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9449921615798376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9449921615798376 | validation: 4.160653089941773]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9906161616040885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9906161616040885 | validation: 4.1223239912568985]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9200173217451635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9200173217451635 | validation: 4.095620428017241]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.87032994125234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.87032994125234 | validation: 4.130713442486013]
	TIME [epoch: 8.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8396820792759945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8396820792759945 | validation: 4.055493254411982]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7848566641770676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7848566641770676 | validation: 4.0277796111860615]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857390022001449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.857390022001449 | validation: 4.006264260153355]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7991727502900052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7991727502900052 | validation: 3.979908245164075]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7311121970561225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7311121970561225 | validation: 3.9317138344594547]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.73667901957705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.73667901957705 | validation: 3.9421393267020353]
	TIME [epoch: 8.21 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.720724373922052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.720724373922052 | validation: 3.898465555528168]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6843377193012192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6843377193012192 | validation: 3.9235911205144207]
	TIME [epoch: 8.21 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.698334057962338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.698334057962338 | validation: 3.896234897544093]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.673087664304885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.673087664304885 | validation: 3.87377074404572]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.667337149677435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.667337149677435 | validation: 3.854090863549281]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6615572830093654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6615572830093654 | validation: 3.834959582332071]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.63417711999617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.63417711999617 | validation: 3.807559347833]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.665616048228268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.665616048228268 | validation: 3.8600056335617445]
	TIME [epoch: 8.26 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.633290240129273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.633290240129273 | validation: 3.7824622437888]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6049931114289286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6049931114289286 | validation: 3.7560267056911165]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6046639617702456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6046639617702456 | validation: 3.7337029289384955]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6331849796289184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6331849796289184 | validation: 3.946889981326679]
	TIME [epoch: 8.33 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6964978232094725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6964978232094725 | validation: 3.7278585420834487]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.58132738757719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.58132738757719 | validation: 3.7012685727187344]
	TIME [epoch: 8.38 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5676350586634706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5676350586634706 | validation: 3.6867432180762356]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.558953865685975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.558953865685975 | validation: 3.668893085149558]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5534587219500846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5534587219500846 | validation: 3.653580751419102]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.539045307254925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539045307254925 | validation: 3.6412402077131225]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5264766323738663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5264766323738663 | validation: 3.635973280558276]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.526808075355805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.526808075355805 | validation: 3.6193454607897406]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5384370480425846		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 3.5384370480425846 | validation: 3.593078179714758]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5082293037205075		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 3.5082293037205075 | validation: 3.5834853167484906]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4996817456332963		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 3.4996817456332963 | validation: 3.5523918993195487]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4937557804335824		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 3.4937557804335824 | validation: 3.5547053056958777]
	TIME [epoch: 8.25 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5016936211583993		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 3.5016936211583993 | validation: 3.5591587223917545]
	TIME [epoch: 8.21 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4910116848712227		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 3.4910116848712227 | validation: 3.5550475923616114]
	TIME [epoch: 8.21 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.469090353441783		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.469090353441783 | validation: 3.5081998527333598]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4992092067112774		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.4992092067112774 | validation: 3.566606925347738]
	TIME [epoch: 8.23 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.506335245720811		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.506335245720811 | validation: 3.5624541536653744]
	TIME [epoch: 8.22 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4784646329213427		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.4784646329213427 | validation: 3.5227003863974016]
	TIME [epoch: 8.26 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4631035585567327		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.4631035585567327 | validation: 3.518462481724633]
	TIME [epoch: 8.22 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4567952473549477		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.4567952473549477 | validation: 3.5194932517015087]
	TIME [epoch: 8.21 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.469460562623497		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.469460562623497 | validation: 3.5017405221108175]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4779584813609805		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 3.4779584813609805 | validation: 3.507431982191484]
	TIME [epoch: 8.22 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.429958911452017		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 3.429958911452017 | validation: 3.72614213533449]
	TIME [epoch: 8.25 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.521477108934345		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 3.521477108934345 | validation: 3.5412307099029965]
	TIME [epoch: 8.22 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.435331616900093		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.435331616900093 | validation: 3.5514060878315776]
	TIME [epoch: 8.22 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4368267032036117		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.4368267032036117 | validation: 3.5031996475096445]
	TIME [epoch: 8.21 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4146838672844604		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.4146838672844604 | validation: 3.439595952881704]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.432532972321473		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.432532972321473 | validation: 3.4201256093224734]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.407846116737245		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 3.407846116737245 | validation: 3.406803672645059]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.389617362257836		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.389617362257836 | validation: 4.053022229522833]
	TIME [epoch: 8.2 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.60089960946276		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.60089960946276 | validation: 3.461006362905872]
	TIME [epoch: 8.21 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.414824525141168		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.414824525141168 | validation: 3.4391660619953086]
	TIME [epoch: 8.21 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3983027956795278		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.3983027956795278 | validation: 3.428823494104208]
	TIME [epoch: 8.2 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.384097813663566		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 3.384097813663566 | validation: 3.425596832523707]
	TIME [epoch: 8.24 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.400561743116408		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 3.400561743116408 | validation: 3.405011929670198]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3702889971510075		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 3.3702889971510075 | validation: 3.4265616144020488]
	TIME [epoch: 8.21 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3993890825052877		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.3993890825052877 | validation: 3.508306822870373]
	TIME [epoch: 8.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3878777416637034		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 3.3878777416637034 | validation: 3.3819617714459973]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353476547682386		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 3.353476547682386 | validation: 3.3865227465583367]
	TIME [epoch: 8.23 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.356363461841714		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.356363461841714 | validation: 3.3710946334162744]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3597565785759134		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 3.3597565785759134 | validation: 3.3684247253498865]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3525052308197085		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.3525052308197085 | validation: 3.476194886577204]
	TIME [epoch: 8.21 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4182395195615256		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.4182395195615256 | validation: 3.4981543952116527]
	TIME [epoch: 8.21 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.386949897560353		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.386949897560353 | validation: 3.378825000885711]
	TIME [epoch: 8.21 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3448152854654847		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.3448152854654847 | validation: 3.4955605137722]
	TIME [epoch: 8.24 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3840612513646904		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 3.3840612513646904 | validation: 3.371399565371739]
	TIME [epoch: 8.22 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3433264772497036		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.3433264772497036 | validation: 3.361258226981283]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3448570910546946		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 3.3448570910546946 | validation: 3.3568071077503903]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347860089859956		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 3.347860089859956 | validation: 3.3514040099193467]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33822473361537		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 3.33822473361537 | validation: 3.354446472077034]
	TIME [epoch: 8.22 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32988671038769		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 3.32988671038769 | validation: 3.344522601201665]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3770284144577394		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 3.3770284144577394 | validation: 3.4519285132573563]
	TIME [epoch: 8.21 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3500237559381856		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 3.3500237559381856 | validation: 3.3595610184723017]
	TIME [epoch: 8.21 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3240008605613496		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 3.3240008605613496 | validation: 3.340377640417272]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318498459077955		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 3.318498459077955 | validation: 3.33444622396117]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343106429184119		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 3.343106429184119 | validation: 3.352750808880079]
	TIME [epoch: 8.26 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3638485753400333		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.3638485753400333 | validation: 3.376208562627824]
	TIME [epoch: 8.21 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332240374579982		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 3.332240374579982 | validation: 3.3323579951248576]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3208549803961045		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 3.3208549803961045 | validation: 3.3318948589845663]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320507195575124		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 3.320507195575124 | validation: 3.3408816605071765]
	TIME [epoch: 8.21 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3638172008522944		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 3.3638172008522944 | validation: 3.3945617408965667]
	TIME [epoch: 8.21 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3493206010886083		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 3.3493206010886083 | validation: 3.361844038624306]
	TIME [epoch: 8.24 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325293636662831		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.325293636662831 | validation: 3.3428750545514205]
	TIME [epoch: 8.21 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3187790576170544		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 3.3187790576170544 | validation: 3.337308732617174]
	TIME [epoch: 8.2 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330710133780588		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 3.330710133780588 | validation: 3.354543245204039]
	TIME [epoch: 8.2 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.32018110532488		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.32018110532488 | validation: 3.339235421018955]
	TIME [epoch: 8.2 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326189548220194		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 3.326189548220194 | validation: 3.3628587138223534]
	TIME [epoch: 8.23 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3177213208454726		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 3.3177213208454726 | validation: 3.316480690211966]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.321649194249973		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.321649194249973 | validation: 3.365623471020033]
	TIME [epoch: 8.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3244098415519456		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 3.3244098415519456 | validation: 3.33565094372479]
	TIME [epoch: 8.21 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3203701323329518		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 3.3203701323329518 | validation: 3.3446711448763877]
	TIME [epoch: 8.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3140368284701545		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 3.3140368284701545 | validation: 3.3159002830608895]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322616217202002		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 3.322616217202002 | validation: 3.377947708347387]
	TIME [epoch: 8.25 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3233441684938305		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 3.3233441684938305 | validation: 3.3216548714761727]
	TIME [epoch: 8.19 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3041845818390216		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.3041845818390216 | validation: 3.325048807554362]
	TIME [epoch: 8.2 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3181689289423786		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 3.3181689289423786 | validation: 3.3553324933825746]
	TIME [epoch: 8.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325591789488485		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.325591789488485 | validation: 3.309612607331663]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3031979799390325		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 3.3031979799390325 | validation: 3.320590207854704]
	TIME [epoch: 8.24 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.317541872623399		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 3.317541872623399 | validation: 3.3263325041663796]
	TIME [epoch: 8.22 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308022935558359		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 3.308022935558359 | validation: 3.3113362088646365]
	TIME [epoch: 8.21 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3084529488518832		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 3.3084529488518832 | validation: 3.3559034221655146]
	TIME [epoch: 8.2 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3194392360796696		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 3.3194392360796696 | validation: 3.30938585026128]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2936270351224977		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 3.2936270351224977 | validation: 3.3497324252506955]
	TIME [epoch: 8.23 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3449384424586492		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 3.3449384424586492 | validation: 3.3157735475794254]
	TIME [epoch: 8.27 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2978945255544128		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 3.2978945255544128 | validation: 3.30602771210134]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294355786539747		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 3.294355786539747 | validation: 3.3017266081424568]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.316147332120212		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 3.316147332120212 | validation: 3.33375854109676]
	TIME [epoch: 8.21 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.307210907336032		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 3.307210907336032 | validation: 3.3184552757491064]
	TIME [epoch: 8.22 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297045708363397		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 3.297045708363397 | validation: 3.3028498260685146]
	TIME [epoch: 8.25 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3237440609281377		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 3.3237440609281377 | validation: 3.344203576008839]
	TIME [epoch: 8.23 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332052074949634		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 3.332052074949634 | validation: 3.3208611822990326]
	TIME [epoch: 8.22 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3009607470351536		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 3.3009607470351536 | validation: 3.3073085405979334]
	TIME [epoch: 8.23 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299496116115659		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 3.299496116115659 | validation: 3.303914802157048]
	TIME [epoch: 8.22 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3097627446112035		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 3.3097627446112035 | validation: 3.304654522735899]
	TIME [epoch: 8.22 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2940319692210016		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 3.2940319692210016 | validation: 3.331829375153666]
	TIME [epoch: 8.26 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3023273007895058		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 3.3023273007895058 | validation: 3.30275902386299]
	TIME [epoch: 8.22 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287647251217535		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 3.287647251217535 | validation: 3.3666278653319246]
	TIME [epoch: 8.21 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3674008821188663		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 3.3674008821188663 | validation: 3.3173452457245522]
	TIME [epoch: 8.21 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29567673246825		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 3.29567673246825 | validation: 3.297827978210158]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2873719399133434		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 3.2873719399133434 | validation: 3.2983116607654663]
	TIME [epoch: 8.24 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.290119851785013		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 3.290119851785013 | validation: 3.3160679364680883]
	TIME [epoch: 8.23 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2935850620692517		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 3.2935850620692517 | validation: 3.308708160693092]
	TIME [epoch: 8.21 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299283973767425		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 3.299283973767425 | validation: 3.329386955768766]
	TIME [epoch: 8.21 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3106560019624474		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 3.3106560019624474 | validation: 3.295521507097197]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28855556082841		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 3.28855556082841 | validation: 3.3165519560837744]
	TIME [epoch: 8.29 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2925628697087648		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 3.2925628697087648 | validation: 3.31102431006759]
	TIME [epoch: 8.25 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.300180852510901		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 3.300180852510901 | validation: 3.3275225125371763]
	TIME [epoch: 8.21 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2913673430183032		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 3.2913673430183032 | validation: 3.3028133520223473]
	TIME [epoch: 8.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287361373791509		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 3.287361373791509 | validation: 3.3039253248458804]
	TIME [epoch: 8.21 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297261689834029		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 3.297261689834029 | validation: 3.3054846854316526]
	TIME [epoch: 8.22 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294414380429305		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 3.294414380429305 | validation: 3.30273370086927]
	TIME [epoch: 8.23 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302969079863174		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 3.302969079863174 | validation: 3.3225361268427207]
	TIME [epoch: 8.29 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2976123391839125		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 3.2976123391839125 | validation: 3.3007487007120355]
	TIME [epoch: 8.26 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2924130240683		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 3.2924130240683 | validation: 3.2935645145503867]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2848457071925052		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 3.2848457071925052 | validation: 3.306781579754385]
	TIME [epoch: 8.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.313393550596651		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 3.313393550596651 | validation: 3.2990763218045096]
	TIME [epoch: 8.21 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.289275250943438		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 3.289275250943438 | validation: 3.3124322686167043]
	TIME [epoch: 8.25 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2928089392713975		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 3.2928089392713975 | validation: 3.300364524006758]
	TIME [epoch: 8.23 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2824036099205482		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 3.2824036099205482 | validation: 3.4081660559627736]
	TIME [epoch: 8.21 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.311477745357654		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 3.311477745357654 | validation: 3.3050468425846127]
	TIME [epoch: 8.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2930377254247407		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 3.2930377254247407 | validation: 3.292189049535656]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28508537214035		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 3.28508537214035 | validation: 3.3478770568352756]
	TIME [epoch: 8.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.302797115608514		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 3.302797115608514 | validation: 3.288503623499122]
	TIME [epoch: 8.28 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2914270298423305		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 3.2914270298423305 | validation: 3.3261575024494077]
	TIME [epoch: 8.29 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4696896454232955		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 3.4696896454232955 | validation: 3.3648523074572076]
	TIME [epoch: 8.22 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3391213144868233		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 3.3391213144868233 | validation: 3.3304879042048134]
	TIME [epoch: 8.22 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31052191609073		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 3.31052191609073 | validation: 3.3138539751500167]
	TIME [epoch: 8.27 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298053477199396		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 3.298053477199396 | validation: 3.3049440578095983]
	TIME [epoch: 8.33 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2944282954157513		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 3.2944282954157513 | validation: 3.3002100332952478]
	TIME [epoch: 8.29 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285984110977826		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 3.285984110977826 | validation: 3.2928665002707014]
	TIME [epoch: 8.29 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.293381787545412		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 3.293381787545412 | validation: 3.2947072896775698]
	TIME [epoch: 8.25 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2823019233619553		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 3.2823019233619553 | validation: 3.290186444813987]
	TIME [epoch: 8.22 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282208631078671		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 3.282208631078671 | validation: 3.2978901073691276]
	TIME [epoch: 8.24 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281673575259227		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 3.281673575259227 | validation: 3.290594286289071]
	TIME [epoch: 8.33 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2930065319458457		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 3.2930065319458457 | validation: 3.4401346422961954]
	TIME [epoch: 8.28 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354037999863867		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 3.354037999863867 | validation: 3.2978535028700313]
	TIME [epoch: 8.23 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280966710031812		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 3.280966710031812 | validation: 3.3097956972038647]
	TIME [epoch: 8.23 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3107308166659086		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 3.3107308166659086 | validation: 3.319299304287444]
	TIME [epoch: 8.22 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284607032748037		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 3.284607032748037 | validation: 3.284052376938]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2743044613175867		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 3.2743044613175867 | validation: 3.2836591808858744]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275065069247998		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 3.275065069247998 | validation: 3.2847150483523375]
	TIME [epoch: 8.29 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30119666553295		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 3.30119666553295 | validation: 3.2998615807029203]
	TIME [epoch: 8.22 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3280722160168055		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 3.3280722160168055 | validation: 3.290226317064203]
	TIME [epoch: 8.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2730156363842076		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 3.2730156363842076 | validation: 3.284175604221765]
	TIME [epoch: 8.21 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2831121853938194		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 3.2831121853938194 | validation: 3.3000036639407746]
	TIME [epoch: 8.26 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.376783423911368		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 3.376783423911368 | validation: 3.5328359620831584]
	TIME [epoch: 8.22 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4023811591008624		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 3.4023811591008624 | validation: 3.3228805222816034]
	TIME [epoch: 8.22 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2819558192961784		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 3.2819558192961784 | validation: 3.2865276723186616]
	TIME [epoch: 8.22 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3381029910766475		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 3.3381029910766475 | validation: 3.3780821054939834]
	TIME [epoch: 8.21 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3040692887131016		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 3.3040692887131016 | validation: 3.29119698631354]
	TIME [epoch: 8.22 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274811508562765		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 3.274811508562765 | validation: 3.2976175114480815]
	TIME [epoch: 8.25 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282909838676951		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 3.282909838676951 | validation: 3.2892257434175427]
	TIME [epoch: 8.21 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2731949859735536		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 3.2731949859735536 | validation: 3.2864660644093044]
	TIME [epoch: 8.22 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2717886367687243		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 3.2717886367687243 | validation: 3.2967178165265407]
	TIME [epoch: 8.22 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274157289879126		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 3.274157289879126 | validation: 3.3147328444195776]
	TIME [epoch: 8.21 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278081557635919		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 3.278081557635919 | validation: 3.27980213769463]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.266908104368127		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 3.266908104368127 | validation: 3.3121365606538387]
	TIME [epoch: 8.23 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2963588695763386		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 3.2963588695763386 | validation: 3.286408046778658]
	TIME [epoch: 8.21 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280827634949075		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 3.280827634949075 | validation: 3.309578455514336]
	TIME [epoch: 8.22 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3489242542601163		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 3.3489242542601163 | validation: 3.339198408894169]
	TIME [epoch: 8.21 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2932601769305054		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 3.2932601769305054 | validation: 3.2940499756143926]
	TIME [epoch: 8.21 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2842394895891966		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 3.2842394895891966 | validation: 3.282680075033726]
	TIME [epoch: 8.26 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294902154109887		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 3.294902154109887 | validation: 3.304768333202379]
	TIME [epoch: 8.21 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28877390156066		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 3.28877390156066 | validation: 3.3111932615147097]
	TIME [epoch: 8.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3035836483068803		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 3.3035836483068803 | validation: 3.3268871509465514]
	TIME [epoch: 8.21 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3046245014520395		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 3.3046245014520395 | validation: 3.274199730220003]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.262356584315069		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 3.262356584315069 | validation: 3.2752611615631957]
	TIME [epoch: 8.24 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.254556893823213		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 3.254556893823213 | validation: 3.2687556457394122]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3037109797570943		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 3.3037109797570943 | validation: 3.352808258431922]
	TIME [epoch: 8.23 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3007998479295306		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 3.3007998479295306 | validation: 3.2905505839018865]
	TIME [epoch: 8.22 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263446929126906		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 3.263446929126906 | validation: 3.26564656526465]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.256570086687317		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 3.256570086687317 | validation: 3.2645965379677957]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2557637381659768		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 3.2557637381659768 | validation: 3.2906026791025313]
	TIME [epoch: 8.26 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2779445334659627		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 3.2779445334659627 | validation: 3.2908754824747515]
	TIME [epoch: 8.22 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2938854186042708		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 3.2938854186042708 | validation: 3.2852211045591106]
	TIME [epoch: 8.22 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265984569863902		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 3.265984569863902 | validation: 3.318542561640597]
	TIME [epoch: 8.21 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3083697450716545		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 3.3083697450716545 | validation: 3.3928569443545133]
	TIME [epoch: 8.22 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.353435672407345		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 3.353435672407345 | validation: 3.3654390580055273]
	TIME [epoch: 8.24 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3057947080960695		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 3.3057947080960695 | validation: 3.2972470442622095]
	TIME [epoch: 8.23 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2569796989787414		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 3.2569796989787414 | validation: 3.291577992197592]
	TIME [epoch: 8.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.279071067593943		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 3.279071067593943 | validation: 3.3647904120727725]
	TIME [epoch: 8.21 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3245937660948726		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 3.3245937660948726 | validation: 3.298768238279015]
	TIME [epoch: 8.21 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2794092808641477		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 3.2794092808641477 | validation: 3.2677226643845723]
	TIME [epoch: 8.21 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.262769118114881		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 3.262769118114881 | validation: 3.2773618943131364]
	TIME [epoch: 8.26 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2834478702646694		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 3.2834478702646694 | validation: 3.263889637384802]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2476316889268393		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 3.2476316889268393 | validation: 3.3552072586190995]
	TIME [epoch: 8.21 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.334136294027195		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 3.334136294027195 | validation: 3.303422955112545]
	TIME [epoch: 8.19 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2995646663013654		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 3.2995646663013654 | validation: 3.327553987338246]
	TIME [epoch: 8.21 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320469745080989		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 3.320469745080989 | validation: 3.3593704625061815]
	TIME [epoch: 8.22 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3072502819766267		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 3.3072502819766267 | validation: 3.2897974541385784]
	TIME [epoch: 8.24 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2787668551465536		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 3.2787668551465536 | validation: 3.2978673546177255]
	TIME [epoch: 8.21 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2982712318328247		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 3.2982712318328247 | validation: 3.2852728926434227]
	TIME [epoch: 8.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2717292777048885		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 3.2717292777048885 | validation: 3.276831867753914]
	TIME [epoch: 8.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259879395478263		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 3.259879395478263 | validation: 3.2570115299910043]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247688024591171		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 3.247688024591171 | validation: 3.279588875008721]
	TIME [epoch: 8.26 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2671233843227903		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 3.2671233843227903 | validation: 3.2828060474233145]
	TIME [epoch: 8.22 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2607058755885525		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 3.2607058755885525 | validation: 3.3033030109348642]
	TIME [epoch: 8.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288964072009284		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 3.288964072009284 | validation: 3.306398248926773]
	TIME [epoch: 8.21 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260431810004793		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 3.260431810004793 | validation: 3.2519154356376547]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2518388685381905		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 3.2518388685381905 | validation: 3.329249408530369]
	TIME [epoch: 8.22 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.362555215267903		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 3.362555215267903 | validation: 3.3506341747923374]
	TIME [epoch: 8.24 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2945793771391094		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 3.2945793771391094 | validation: 3.292510139944908]
	TIME [epoch: 8.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274226744143707		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 3.274226744143707 | validation: 3.384802560955173]
	TIME [epoch: 8.21 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.360395726148136		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 3.360395726148136 | validation: 3.3345909632384814]
	TIME [epoch: 8.22 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3106762800375047		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 3.3106762800375047 | validation: 3.3099953467407097]
	TIME [epoch: 8.22 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3154539971065184		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 3.3154539971065184 | validation: 3.3701874648324317]
	TIME [epoch: 8.25 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315972917776424		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 3.315972917776424 | validation: 3.313523439437259]
	TIME [epoch: 8.23 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282347467435709		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 3.282347467435709 | validation: 3.317506192544108]
	TIME [epoch: 8.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3031615232154117		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 3.3031615232154117 | validation: 3.3019012295987817]
	TIME [epoch: 8.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.277904727679976		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 3.277904727679976 | validation: 3.288211046532842]
	TIME [epoch: 8.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2741370074829734		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 3.2741370074829734 | validation: 3.276604814472052]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2709124942928947		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 3.2709124942928947 | validation: 3.308009181570579]
	TIME [epoch: 8.26 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2800153566458867		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 3.2800153566458867 | validation: 3.2861546215913062]
	TIME [epoch: 8.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259729686368419		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 3.259729686368419 | validation: 3.393538126310933]
	TIME [epoch: 8.21 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.396932264872154		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 3.396932264872154 | validation: 3.4037644069797692]
	TIME [epoch: 8.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318706658205663		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 3.318706658205663 | validation: 3.279888165274518]
	TIME [epoch: 8.21 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275598446441892		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 3.275598446441892 | validation: 3.263625363302429]
	TIME [epoch: 8.22 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2605510072213106		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 3.2605510072213106 | validation: 3.31362063556913]
	TIME [epoch: 8.26 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2782797204016916		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 3.2782797204016916 | validation: 3.264563378275916]
	TIME [epoch: 8.21 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269738170494027		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 3.269738170494027 | validation: 3.297187494819638]
	TIME [epoch: 8.21 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2758159943138025		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 3.2758159943138025 | validation: 3.2849480150417376]
	TIME [epoch: 8.21 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2698078014839873		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 3.2698078014839873 | validation: 3.287918261692754]
	TIME [epoch: 8.21 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2704824975557845		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 3.2704824975557845 | validation: 3.2788257114769497]
	TIME [epoch: 8.25 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2809557747030933		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 3.2809557747030933 | validation: 3.273534167734771]
	TIME [epoch: 8.22 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2740173307524842		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 3.2740173307524842 | validation: 3.3734607511823897]
	TIME [epoch: 8.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3018031299487887		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 3.3018031299487887 | validation: 3.3362936423815572]
	TIME [epoch: 8.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284783331704101		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 3.284783331704101 | validation: 3.2562894481143094]
	TIME [epoch: 8.21 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243953364209344		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 3.243953364209344 | validation: 3.2535913302867563]
	TIME [epoch: 8.22 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24336486992876		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 3.24336486992876 | validation: 3.258803565705397]
	TIME [epoch: 8.25 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283527430704764		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 3.283527430704764 | validation: 3.256133256300008]
	TIME [epoch: 8.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2457107259907514		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 3.2457107259907514 | validation: 3.391303037788524]
	TIME [epoch: 8.21 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329260980219589		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 3.329260980219589 | validation: 3.3214662523690937]
	TIME [epoch: 8.22 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31504103828904		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 3.31504103828904 | validation: 3.300285974372387]
	TIME [epoch: 8.21 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.268111392792009		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 3.268111392792009 | validation: 3.278724236847373]
	TIME [epoch: 8.24 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2531027400508172		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 3.2531027400508172 | validation: 3.2434648373168065]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2453804331723903		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 3.2453804331723903 | validation: 3.2596038348166876]
	TIME [epoch: 8.22 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2591633342931123		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 3.2591633342931123 | validation: 3.2421066967281034]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2383843744596423		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 3.2383843744596423 | validation: 3.3758058284499155]
	TIME [epoch: 8.21 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3659225283950365		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 3.3659225283950365 | validation: 3.3584648314160876]
	TIME [epoch: 8.21 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325433819886824		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 3.325433819886824 | validation: 3.2819378856691683]
	TIME [epoch: 8.27 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2618687284606875		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 3.2618687284606875 | validation: 3.2534557870763336]
	TIME [epoch: 8.21 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2540003287050783		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 3.2540003287050783 | validation: 3.2773498082749963]
	TIME [epoch: 8.21 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3288780559001783		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 3.3288780559001783 | validation: 3.3072161475140334]
	TIME [epoch: 8.21 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2931296264877186		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 3.2931296264877186 | validation: 3.285223534850031]
	TIME [epoch: 8.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2838423254066216		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 3.2838423254066216 | validation: 3.2608533746400274]
	TIME [epoch: 8.22 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258716267128264		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 3.258716267128264 | validation: 3.260933377104409]
	TIME [epoch: 8.25 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2672472023041643		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 3.2672472023041643 | validation: 3.2835865181742503]
	TIME [epoch: 8.22 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.297106826664291		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 3.297106826664291 | validation: 3.283433186477569]
	TIME [epoch: 8.21 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2766366773956257		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 3.2766366773956257 | validation: 3.2780375070727388]
	TIME [epoch: 8.21 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275662962995122		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 3.275662962995122 | validation: 3.278642062518061]
	TIME [epoch: 8.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2667901995660484		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 3.2667901995660484 | validation: 3.280344773306179]
	TIME [epoch: 8.25 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2755784430266006		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 3.2755784430266006 | validation: 3.267073785029435]
	TIME [epoch: 8.22 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263792426729072		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 3.263792426729072 | validation: 3.2694244758548856]
	TIME [epoch: 8.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2608456777585455		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 3.2608456777585455 | validation: 3.2632413612789075]
	TIME [epoch: 8.21 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2581804113494117		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 3.2581804113494117 | validation: 3.273278117098847]
	TIME [epoch: 8.21 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298652029275994		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 3.298652029275994 | validation: 3.2696966404275067]
	TIME [epoch: 8.21 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2568965551873768		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 3.2568965551873768 | validation: 3.3046536294852364]
	TIME [epoch: 8.25 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.464600591166796		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 3.464600591166796 | validation: 3.285832672738663]
	TIME [epoch: 8.21 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2705623066545133		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 3.2705623066545133 | validation: 3.2666148009166003]
	TIME [epoch: 8.21 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2475526469548828		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 3.2475526469548828 | validation: 3.2342764891884146]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261537569710355		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 3.261537569710355 | validation: 3.265571815774097]
	TIME [epoch: 8.22 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2522534081783268		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 3.2522534081783268 | validation: 3.2541348104270074]
	TIME [epoch: 8.24 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2369573614516063		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 3.2369573614516063 | validation: 3.255416421868159]
	TIME [epoch: 8.23 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2412659644604536		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 3.2412659644604536 | validation: 3.2371094539677134]
	TIME [epoch: 8.21 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2270959747997843		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 3.2270959747997843 | validation: 3.2482886750534203]
	TIME [epoch: 8.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.455797690099061		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 3.455797690099061 | validation: 4.07196174950273]
	TIME [epoch: 8.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7177497055664217		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 3.7177497055664217 | validation: 3.6493484855566036]
	TIME [epoch: 8.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.358926892428928		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 3.358926892428928 | validation: 3.282070462474265]
	TIME [epoch: 8.26 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260574198004361		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 3.260574198004361 | validation: 3.258794531836009]
	TIME [epoch: 8.21 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2458042241477463		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 3.2458042241477463 | validation: 3.2458621066000286]
	TIME [epoch: 8.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.234546961643399		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 3.234546961643399 | validation: 3.2391794974090162]
	TIME [epoch: 8.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2348005685913743		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 3.2348005685913743 | validation: 3.2822034623603185]
	TIME [epoch: 8.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2573014886970615		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 3.2573014886970615 | validation: 3.241014117039059]
	TIME [epoch: 8.22 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2312514816179596		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 3.2312514816179596 | validation: 3.2329395944101034]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265866728846607		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 3.265866728846607 | validation: 3.3075602615799715]
	TIME [epoch: 8.22 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3716518681918797		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 3.3716518681918797 | validation: 4.031462426113649]
	TIME [epoch: 8.22 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.836341267676243		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 3.836341267676243 | validation: 3.454824587602561]
	TIME [epoch: 8.22 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3789506176440955		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 3.3789506176440955 | validation: 3.3414002453814815]
	TIME [epoch: 8.21 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278593488362648		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 3.278593488362648 | validation: 3.287942237711583]
	TIME [epoch: 8.24 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2557403024809406		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 3.2557403024809406 | validation: 3.2703234198704685]
	TIME [epoch: 8.22 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.242386744843715		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 3.242386744843715 | validation: 3.25463291647507]
	TIME [epoch: 8.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237939820305262		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 3.237939820305262 | validation: 3.268700464701692]
	TIME [epoch: 8.21 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2540372758474714		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 3.2540372758474714 | validation: 3.2685868975456005]
	TIME [epoch: 8.21 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2535289406208134		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 3.2535289406208134 | validation: 3.257269683777663]
	TIME [epoch: 8.21 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.244322498933792		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 3.244322498933792 | validation: 3.2487839548792996]
	TIME [epoch: 8.25 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2340533440148973		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 3.2340533440148973 | validation: 3.2393009217540767]
	TIME [epoch: 8.21 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2582641703641837		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 3.2582641703641837 | validation: 3.437209597096533]
	TIME [epoch: 8.21 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.305283059286008		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 3.305283059286008 | validation: 3.2402832671940707]
	TIME [epoch: 8.21 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236471155603958		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 3.236471155603958 | validation: 3.2275694699621673]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227020410395527		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 3.227020410395527 | validation: 3.2565792547871064]
	TIME [epoch: 8.25 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2389000386072535		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 3.2389000386072535 | validation: 3.2478293449849396]
	TIME [epoch: 8.23 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230720876696293		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 3.230720876696293 | validation: 3.2465207298555123]
	TIME [epoch: 8.23 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2345287373114116		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 3.2345287373114116 | validation: 3.2367749051223513]
	TIME [epoch: 8.22 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2318438221033627		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 3.2318438221033627 | validation: 3.2247716995324582]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226086812120354		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 3.226086812120354 | validation: 3.292351676760865]
	TIME [epoch: 8.22 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2954287825451445		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 3.2954287825451445 | validation: 3.2614691618950884]
	TIME [epoch: 8.25 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260146071165677		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 3.260146071165677 | validation: 3.2542537005997714]
	TIME [epoch: 8.21 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232926573441211		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 3.232926573441211 | validation: 3.2290990310904473]
	TIME [epoch: 8.21 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226295566924521		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 3.226295566924521 | validation: 3.2212660835849074]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236369920436773		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 3.236369920436773 | validation: 3.242875551415053]
	TIME [epoch: 8.21 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2396016954351587		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 3.2396016954351587 | validation: 3.2706608909368597]
	TIME [epoch: 8.23 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2658168495153532		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 3.2658168495153532 | validation: 3.2344414477586474]
	TIME [epoch: 8.23 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230723392849383		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 3.230723392849383 | validation: 3.2350498910972743]
	TIME [epoch: 8.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2307341824876876		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 3.2307341824876876 | validation: 3.2307948341194073]
	TIME [epoch: 8.21 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224539951417933		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 3.224539951417933 | validation: 3.2293746096733256]
	TIME [epoch: 8.21 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24266475837637		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 3.24266475837637 | validation: 3.2531670281309912]
	TIME [epoch: 8.21 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2453776438453628		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 3.2453776438453628 | validation: 3.245504637360516]
	TIME [epoch: 8.24 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.235219101129581		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 3.235219101129581 | validation: 3.2316193405291607]
	TIME [epoch: 8.22 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223815170539414		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 3.223815170539414 | validation: 3.252400806620955]
	TIME [epoch: 8.21 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2406447635558746		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 3.2406447635558746 | validation: 3.2216600026236293]
	TIME [epoch: 8.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215335117494276		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 3.215335117494276 | validation: 3.259132035904917]
	TIME [epoch: 8.21 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3665699451105286		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 3.3665699451105286 | validation: 3.248908563697129]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2419805314677363		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 3.2419805314677363 | validation: 3.2594215240312217]
	TIME [epoch: 8.25 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2315057954285207		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 3.2315057954285207 | validation: 3.2250942706081336]
	TIME [epoch: 8.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22502584727291		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 3.22502584727291 | validation: 3.222941034932061]
	TIME [epoch: 8.21 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220299732413727		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 3.220299732413727 | validation: 3.2195868664472034]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2194414613146103		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 3.2194414613146103 | validation: 3.2182408687119297]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214111554562634		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 3.214111554562634 | validation: 3.2179567102344615]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2172445628469712		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 3.2172445628469712 | validation: 3.2552889419928794]
	TIME [epoch: 8.23 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24131005095457		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 3.24131005095457 | validation: 3.239434233336086]
	TIME [epoch: 8.22 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2286900662821325		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 3.2286900662821325 | validation: 3.2320158011772557]
	TIME [epoch: 8.21 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23002647706092		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 3.23002647706092 | validation: 3.2170884579570433]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216773089837023		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 3.216773089837023 | validation: 3.2237633415381155]
	TIME [epoch: 8.21 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2295169995300523		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 3.2295169995300523 | validation: 3.2310947797384832]
	TIME [epoch: 8.26 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2331100020472463		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 3.2331100020472463 | validation: 3.2112147127658215]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2152795653446526		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 3.2152795653446526 | validation: 3.230485376523386]
	TIME [epoch: 8.22 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2256391793555137		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 3.2256391793555137 | validation: 3.216384505654897]
	TIME [epoch: 8.21 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2221107234313227		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 3.2221107234313227 | validation: 3.2237199713211546]
	TIME [epoch: 8.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228063847652784		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 3.228063847652784 | validation: 3.2125137245731343]
	TIME [epoch: 8.25 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2240894061266574		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 3.2240894061266574 | validation: 3.239719965846773]
	TIME [epoch: 8.23 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2767389907454425		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 3.2767389907454425 | validation: 3.3591826062194974]
	TIME [epoch: 8.21 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286940072188703		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 3.286940072188703 | validation: 3.2712437629338726]
	TIME [epoch: 8.21 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24919340795093		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 3.24919340795093 | validation: 3.2410008611516274]
	TIME [epoch: 8.21 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239963284780756		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 3.239963284780756 | validation: 3.2318076061427106]
	TIME [epoch: 8.22 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323274961966813		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 3.323274961966813 | validation: 3.370422840912929]
	TIME [epoch: 8.26 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.339818374093607		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 3.339818374093607 | validation: 3.297935845480752]
	TIME [epoch: 8.21 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2956878755322316		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 3.2956878755322316 | validation: 3.285983142979272]
	TIME [epoch: 8.22 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255070805142525		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 3.255070805142525 | validation: 3.230824991580573]
	TIME [epoch: 8.21 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2289426596435087		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 3.2289426596435087 | validation: 3.2448920962583534]
	TIME [epoch: 8.21 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.231628970060383		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 3.231628970060383 | validation: 3.236337704084098]
	TIME [epoch: 8.24 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226177356005679		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 3.226177356005679 | validation: 3.228947752167833]
	TIME [epoch: 8.22 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2207771310674396		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 3.2207771310674396 | validation: 3.2309841381126994]
	TIME [epoch: 8.21 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2160637168377564		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 3.2160637168377564 | validation: 3.226513237535893]
	TIME [epoch: 8.21 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2199955422320956		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 3.2199955422320956 | validation: 3.2098016204637654]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2135950440246632		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 3.2135950440246632 | validation: 3.209928387333764]
	TIME [epoch: 8.21 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2136279776854915		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 3.2136279776854915 | validation: 3.2115545871143407]
	TIME [epoch: 8.26 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214027961000065		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 3.214027961000065 | validation: 3.209018180055917]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2137619490618126		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 3.2137619490618126 | validation: 3.217478633197567]
	TIME [epoch: 8.21 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.293035607764441		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 3.293035607764441 | validation: 3.5622659155746788]
	TIME [epoch: 8.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3152991835559633		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 3.3152991835559633 | validation: 3.2312186558619356]
	TIME [epoch: 8.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22297864939432		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 3.22297864939432 | validation: 3.2513017281527965]
	TIME [epoch: 8.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2349615340343147		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 3.2349615340343147 | validation: 3.253234935398222]
	TIME [epoch: 8.23 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240885876036164		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 3.240885876036164 | validation: 3.236388385237925]
	TIME [epoch: 8.21 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219138779895448		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 3.219138779895448 | validation: 3.2177764403504527]
	TIME [epoch: 8.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2154020548269426		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 3.2154020548269426 | validation: 3.224671786126844]
	TIME [epoch: 8.21 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2129910765789997		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 3.2129910765789997 | validation: 3.2266416969917118]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2142309582800634		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 3.2142309582800634 | validation: 3.2127703861055]
	TIME [epoch: 8.22 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2098812498951963		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 3.2098812498951963 | validation: 3.218921741953766]
	TIME [epoch: 8.23 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228228606847383		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 3.228228606847383 | validation: 3.236608633697097]
	TIME [epoch: 8.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2148015716873193		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 3.2148015716873193 | validation: 3.2176656416789378]
	TIME [epoch: 8.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228413033063184		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 3.228413033063184 | validation: 3.26219184662234]
	TIME [epoch: 8.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2428947209382613		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 3.2428947209382613 | validation: 3.2212767920049368]
	TIME [epoch: 8.22 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2135709746114838		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 3.2135709746114838 | validation: 3.2367086781746437]
	TIME [epoch: 8.25 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22623819321261		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 3.22623819321261 | validation: 3.226824374870916]
	TIME [epoch: 8.21 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222308443813466		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 3.222308443813466 | validation: 3.2542414219401588]
	TIME [epoch: 8.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2394254543172103		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 3.2394254543172103 | validation: 3.2338052200223375]
	TIME [epoch: 8.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2209704826231085		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 3.2209704826231085 | validation: 3.218866216852451]
	TIME [epoch: 8.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227666602895484		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 3.227666602895484 | validation: 3.227119803963901]
	TIME [epoch: 8.22 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218961096311622		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 3.218961096311622 | validation: 3.218989535161631]
	TIME [epoch: 8.23 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2332997855463894		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 3.2332997855463894 | validation: 3.2674214242234534]
	TIME [epoch: 8.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247417673579307		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 3.247417673579307 | validation: 3.252327273458622]
	TIME [epoch: 8.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2247268899362367		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 3.2247268899362367 | validation: 3.242173658846876]
	TIME [epoch: 8.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2605622741572966		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 3.2605622741572966 | validation: 3.301711940060419]
	TIME [epoch: 8.21 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299569329015773		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 3.299569329015773 | validation: 3.2477383428921596]
	TIME [epoch: 8.24 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2296617930026974		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 3.2296617930026974 | validation: 3.220740947965541]
	TIME [epoch: 8.21 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081107103478175		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 3.2081107103478175 | validation: 3.2292144939839362]
	TIME [epoch: 8.21 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2164622766527384		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 3.2164622766527384 | validation: 3.219026942736008]
	TIME [epoch: 8.21 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2193428968958955		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 3.2193428968958955 | validation: 3.2793572909321984]
	TIME [epoch: 8.21 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287576884602285		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 3.287576884602285 | validation: 3.280361791509784]
	TIME [epoch: 8.21 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2728725595493424		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 3.2728725595493424 | validation: 3.240131920804017]
	TIME [epoch: 8.25 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23086380020649		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 3.23086380020649 | validation: 3.2548594152374966]
	TIME [epoch: 8.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3515028699733094		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 3.3515028699733094 | validation: 3.397449585543198]
	TIME [epoch: 8.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3725733269271245		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 3.3725733269271245 | validation: 3.3707257004149827]
	TIME [epoch: 8.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3445110370452213		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 3.3445110370452213 | validation: 3.3429336979167132]
	TIME [epoch: 8.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.313842476894534		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 3.313842476894534 | validation: 3.309045021665246]
	TIME [epoch: 8.23 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2801819105534515		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 3.2801819105534515 | validation: 3.2850532867072846]
	TIME [epoch: 8.21 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258656780493713		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 3.258656780493713 | validation: 3.2678125927886583]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2640185331468174		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 3.2640185331468174 | validation: 3.3317791330232143]
	TIME [epoch: 8.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2848743121996176		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 3.2848743121996176 | validation: 3.250032989718716]
	TIME [epoch: 8.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2377547339781243		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 3.2377547339781243 | validation: 3.2318182232073505]
	TIME [epoch: 8.21 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232200718696342		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 3.232200718696342 | validation: 3.2508597443445044]
	TIME [epoch: 8.26 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2410273927418864		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 3.2410273927418864 | validation: 3.231647325243806]
	TIME [epoch: 8.21 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223242008517155		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 3.223242008517155 | validation: 3.2246030661009275]
	TIME [epoch: 8.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2303671397989313		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 3.2303671397989313 | validation: 3.2386191176632266]
	TIME [epoch: 8.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2298198949470223		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 3.2298198949470223 | validation: 3.2182428948293795]
	TIME [epoch: 8.21 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2168967981858008		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 3.2168967981858008 | validation: 3.227976039502435]
	TIME [epoch: 8.22 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226581637561542		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 3.226581637561542 | validation: 3.306233522563808]
	TIME [epoch: 8.25 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4291699062461367		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 3.4291699062461367 | validation: 3.433490619350604]
	TIME [epoch: 8.22 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.364158822076851		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 3.364158822076851 | validation: 3.2882757789379653]
	TIME [epoch: 8.21 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2694699348069607		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 3.2694699348069607 | validation: 3.2454248838124924]
	TIME [epoch: 8.22 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2267328168219267		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 3.2267328168219267 | validation: 3.2148835643132374]
	TIME [epoch: 8.22 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2131871887467733		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 3.2131871887467733 | validation: 3.2320490798217056]
	TIME [epoch: 8.26 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217702313528316		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 3.217702313528316 | validation: 3.222709474836662]
	TIME [epoch: 8.23 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218750688641068		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 3.218750688641068 | validation: 3.247385654597858]
	TIME [epoch: 8.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.235996428710455		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 3.235996428710455 | validation: 3.295490810016315]
	TIME [epoch: 8.22 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2840974388785007		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 3.2840974388785007 | validation: 3.2692019830189905]
	TIME [epoch: 8.22 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2498906380867307		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 3.2498906380867307 | validation: 3.2351403409771073]
	TIME [epoch: 8.22 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230702167324748		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 3.230702167324748 | validation: 3.220961291032459]
	TIME [epoch: 8.25 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.262621098517569		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 3.262621098517569 | validation: 3.3270551904996895]
	TIME [epoch: 8.21 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2933331217381614		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 3.2933331217381614 | validation: 3.2869421527443947]
	TIME [epoch: 8.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2634509039956905		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 3.2634509039956905 | validation: 3.274471600469372]
	TIME [epoch: 8.21 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2506958123576206		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 3.2506958123576206 | validation: 3.2637930616499187]
	TIME [epoch: 8.21 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2461477234766525		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 3.2461477234766525 | validation: 3.2550808496582717]
	TIME [epoch: 8.22 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247322988491642		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 3.247322988491642 | validation: 3.2613788762975497]
	TIME [epoch: 8.25 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251209875944025		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 3.251209875944025 | validation: 3.2665707293940325]
	TIME [epoch: 8.21 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2459976959489154		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 3.2459976959489154 | validation: 3.253518834016741]
	TIME [epoch: 8.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236639138340462		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 3.236639138340462 | validation: 3.237946242732205]
	TIME [epoch: 8.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258520477435452		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 3.258520477435452 | validation: 3.388762409294995]
	TIME [epoch: 8.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3832569467689706		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 3.3832569467689706 | validation: 3.3924567591734673]
	TIME [epoch: 8.25 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3597450964800624		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 3.3597450964800624 | validation: 3.4141365788980713]
	TIME [epoch: 8.23 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3956227249528066		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 3.3956227249528066 | validation: 3.3908459490610308]
	TIME [epoch: 8.22 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.346141198914464		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 3.346141198914464 | validation: 3.338606528313781]
	TIME [epoch: 8.23 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.301135749422996		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 3.301135749422996 | validation: 3.2610999622250945]
	TIME [epoch: 8.22 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243671578390143		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 3.243671578390143 | validation: 3.238447913131167]
	TIME [epoch: 8.22 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.241565267488456		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 3.241565267488456 | validation: 3.2720378146936593]
	TIME [epoch: 8.27 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2509733868488357		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 3.2509733868488357 | validation: 3.2434891539725887]
	TIME [epoch: 8.23 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.248218035595527		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 3.248218035595527 | validation: 3.266345758802327]
	TIME [epoch: 8.22 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232352931997359		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 3.232352931997359 | validation: 3.2228971229119923]
	TIME [epoch: 8.22 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230033567702738		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 3.230033567702738 | validation: 3.241474519585411]
	TIME [epoch: 8.22 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2670495961154016		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 3.2670495961154016 | validation: 3.3569429860809477]
	TIME [epoch: 8.26 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3241508059599125		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 3.3241508059599125 | validation: 3.300100601243247]
	TIME [epoch: 8.24 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.272602630857132		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 3.272602630857132 | validation: 3.265638533232547]
	TIME [epoch: 8.22 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2503773983708872		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 3.2503773983708872 | validation: 3.2539913524681445]
	TIME [epoch: 8.22 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24199565260277		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 3.24199565260277 | validation: 3.246529285409296]
	TIME [epoch: 8.22 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2454982843918105		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 3.2454982843918105 | validation: 3.2651041142139556]
	TIME [epoch: 8.23 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240312077019903		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 3.240312077019903 | validation: 3.2429135548754324]
	TIME [epoch: 8.28 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2462761974838057		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 3.2462761974838057 | validation: 3.377459613777556]
	TIME [epoch: 8.23 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2745225911556157		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 3.2745225911556157 | validation: 3.2215230363752916]
	TIME [epoch: 8.22 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2153119059753688		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 3.2153119059753688 | validation: 3.2169774735743744]
	TIME [epoch: 8.22 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215003898597846		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 3.215003898597846 | validation: 3.2213154599087854]
	TIME [epoch: 8.21 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222666843427042		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 3.222666843427042 | validation: 3.2374478986556143]
	TIME [epoch: 8.24 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226685476447037		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 3.226685476447037 | validation: 3.2241373971620684]
	TIME [epoch: 8.25 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2207230606146884		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 3.2207230606146884 | validation: 3.230960472436264]
	TIME [epoch: 8.22 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2291314308195753		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 3.2291314308195753 | validation: 3.2261011456134723]
	TIME [epoch: 8.22 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220238792438262		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 3.220238792438262 | validation: 3.2443674151973916]
	TIME [epoch: 8.22 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232936208230135		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 3.232936208230135 | validation: 3.2192579227440037]
	TIME [epoch: 8.22 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2113815677566806		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 3.2113815677566806 | validation: 3.2110146988971184]
	TIME [epoch: 8.25 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213241728608229		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 3.213241728608229 | validation: 3.2225960136693566]
	TIME [epoch: 8.24 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2275222961990746		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 3.2275222961990746 | validation: 3.225809267419059]
	TIME [epoch: 8.21 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227446784455387		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 3.227446784455387 | validation: 3.253551880741087]
	TIME [epoch: 8.21 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226111475627142		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 3.226111475627142 | validation: 3.2279671350366343]
	TIME [epoch: 8.22 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226381325776929		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 3.226381325776929 | validation: 3.223184883425079]
	TIME [epoch: 8.21 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2114765324613597		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 3.2114765324613597 | validation: 3.2183369748785173]
	TIME [epoch: 8.26 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2103580651740233		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 3.2103580651740233 | validation: 3.2225644474049475]
	TIME [epoch: 8.22 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228822340849407		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 3.228822340849407 | validation: 3.2622119048716014]
	TIME [epoch: 8.21 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255156162830381		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 3.255156162830381 | validation: 3.2210788747250403]
	TIME [epoch: 8.22 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2231234881857764		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 3.2231234881857764 | validation: 3.2382711898792214]
	TIME [epoch: 8.21 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2366240835926954		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 3.2366240835926954 | validation: 3.222568676216157]
	TIME [epoch: 8.25 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2131039092542464		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 3.2131039092542464 | validation: 3.211828716134278]
	TIME [epoch: 8.23 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2074101934543147		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 3.2074101934543147 | validation: 3.2172001275568203]
	TIME [epoch: 8.21 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219198959801239		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 3.219198959801239 | validation: 3.229953696784324]
	TIME [epoch: 8.21 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2215657449611146		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 3.2215657449611146 | validation: 3.231719646370659]
	TIME [epoch: 8.21 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2310517299498644		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 3.2310517299498644 | validation: 3.246738262637474]
	TIME [epoch: 8.21 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2492522821260765		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 3.2492522821260765 | validation: 3.247242200665553]
	TIME [epoch: 8.27 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240309608734041		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 3.240309608734041 | validation: 3.2408305282386753]
	TIME [epoch: 8.22 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247421046462051		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 3.247421046462051 | validation: 3.3305788172887536]
	TIME [epoch: 8.22 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2780618159954598		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 3.2780618159954598 | validation: 3.2510389476940516]
	TIME [epoch: 8.21 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2398328465594393		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 3.2398328465594393 | validation: 3.2327781776247786]
	TIME [epoch: 8.22 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2204334192282644		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 3.2204334192282644 | validation: 3.2435753699267305]
	TIME [epoch: 8.23 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2460970759085335		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 3.2460970759085335 | validation: 3.2610698058490213]
	TIME [epoch: 8.25 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2350600084835124		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 3.2350600084835124 | validation: 3.2135146148776306]
	TIME [epoch: 8.21 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055479511408467		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 3.2055479511408467 | validation: 3.2073821602799883]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2199449192980536		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 3.2199449192980536 | validation: 3.2238109434140805]
	TIME [epoch: 8.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218008241213605		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 3.218008241213605 | validation: 3.2125445065113887]
	TIME [epoch: 8.21 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2140079005056075		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 3.2140079005056075 | validation: 3.2232407227463424]
	TIME [epoch: 8.24 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2165489538079175		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 3.2165489538079175 | validation: 3.2199650039315117]
	TIME [epoch: 8.22 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211932670761283		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 3.211932670761283 | validation: 3.2350229668122528]
	TIME [epoch: 8.19 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240551623426475		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 3.240551623426475 | validation: 3.244954270223942]
	TIME [epoch: 8.21 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2392285157224343		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 3.2392285157224343 | validation: 3.2295652151748815]
	TIME [epoch: 8.22 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2158649789227423		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 3.2158649789227423 | validation: 3.220997779937358]
	TIME [epoch: 8.21 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2082535814984503		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 3.2082535814984503 | validation: 3.2100146081461753]
	TIME [epoch: 8.25 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202763073761071		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 3.202763073761071 | validation: 3.2057279711210276]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208538290347733		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 3.208538290347733 | validation: 3.214219827941374]
	TIME [epoch: 8.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2134321038433056		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 3.2134321038433056 | validation: 3.21137239052515]
	TIME [epoch: 8.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2136476903108053		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 3.2136476903108053 | validation: 3.2695218787005977]
	TIME [epoch: 8.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2444903331305746		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 3.2444903331305746 | validation: 3.2488100727625007]
	TIME [epoch: 8.23 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2325590048248127		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 3.2325590048248127 | validation: 3.224659199849429]
	TIME [epoch: 8.22 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219970924792986		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 3.219970924792986 | validation: 3.214511060161661]
	TIME [epoch: 8.21 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090684191543293		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 3.2090684191543293 | validation: 3.2024743164364344]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057909769040314		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 3.2057909769040314 | validation: 3.204709867567292]
	TIME [epoch: 8.21 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2026213060506636		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 3.2026213060506636 | validation: 3.2124736352367402]
	TIME [epoch: 8.21 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051705620539823		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 3.2051705620539823 | validation: 3.19972610101214]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1984722452355756		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 3.1984722452355756 | validation: 3.206702416998321]
	TIME [epoch: 8.22 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202554084606265		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 3.202554084606265 | validation: 3.2057103573114585]
	TIME [epoch: 8.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2677463120108956		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 3.2677463120108956 | validation: 3.3484276865906724]
	TIME [epoch: 8.21 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31491645448793		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 3.31491645448793 | validation: 3.279635786555768]
	TIME [epoch: 8.49 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2416209609483384		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 3.2416209609483384 | validation: 3.258138211368764]
	TIME [epoch: 8.24 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269826558413217		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 3.269826558413217 | validation: 3.2806224973058473]
	TIME [epoch: 8.23 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251624475146264		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 3.251624475146264 | validation: 3.2342857711101263]
	TIME [epoch: 8.21 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227595534051692		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 3.227595534051692 | validation: 3.2302267782618417]
	TIME [epoch: 8.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224765567545871		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 3.224765567545871 | validation: 3.248509497600036]
	TIME [epoch: 8.21 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2413675469453973		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 3.2413675469453973 | validation: 3.231387241490461]
	TIME [epoch: 8.21 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2190454910530155		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 3.2190454910530155 | validation: 3.212059729504416]
	TIME [epoch: 8.26 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207690513613559		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 3.207690513613559 | validation: 3.20694208741219]
	TIME [epoch: 8.21 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2036438825453164		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 3.2036438825453164 | validation: 3.2052318972267857]
	TIME [epoch: 8.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089713783397165		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 3.2089713783397165 | validation: 3.21764213931259]
	TIME [epoch: 8.21 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224252987886591		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 3.224252987886591 | validation: 3.2281288476932763]
	TIME [epoch: 8.21 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2215318995094235		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 3.2215318995094235 | validation: 3.2174574481528895]
	TIME [epoch: 8.21 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2211698178293258		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 3.2211698178293258 | validation: 3.2250958250480988]
	TIME [epoch: 8.26 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22033895135283		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 3.22033895135283 | validation: 3.2283763371128344]
	TIME [epoch: 8.21 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226190137819841		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 3.226190137819841 | validation: 3.2061544587652824]
	TIME [epoch: 8.21 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205972122564847		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 3.205972122564847 | validation: 3.2123690442106465]
	TIME [epoch: 8.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205056010499861		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 3.205056010499861 | validation: 3.202649924382267]
	TIME [epoch: 8.21 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2212374113159843		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 3.2212374113159843 | validation: 3.2056839116374762]
	TIME [epoch: 8.24 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207604767423192		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 3.207604767423192 | validation: 3.1992034960738502]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2022136933329044		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 3.2022136933329044 | validation: 3.2152744357142495]
	TIME [epoch: 8.21 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212612112128401		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 3.212612112128401 | validation: 3.2145068451773833]
	TIME [epoch: 8.21 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2030306347439357		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 3.2030306347439357 | validation: 3.20221085371403]
	TIME [epoch: 8.21 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213185963376316		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 3.213185963376316 | validation: 3.211913406130706]
	TIME [epoch: 8.21 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059000313867623		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 3.2059000313867623 | validation: 3.2006259807807513]
	TIME [epoch: 8.25 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1998868868556327		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 3.1998868868556327 | validation: 3.2038036201229496]
	TIME [epoch: 8.21 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.200873133178884		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 3.200873133178884 | validation: 3.2031784508011034]
	TIME [epoch: 8.21 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2140066820689857		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 3.2140066820689857 | validation: 3.215825900143183]
	TIME [epoch: 8.21 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207249006730514		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 3.207249006730514 | validation: 3.2060413557240075]
	TIME [epoch: 8.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.199341490733464		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 3.199341490733464 | validation: 3.2002194129810015]
	TIME [epoch: 8.24 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2000450146191266		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 3.2000450146191266 | validation: 3.2059263328304164]
	TIME [epoch: 8.23 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073245125226766		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 3.2073245125226766 | validation: 3.2104408226521226]
	TIME [epoch: 8.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1995139324625796		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 3.1995139324625796 | validation: 3.2066907001954146]
	TIME [epoch: 8.21 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2019321056091394		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 3.2019321056091394 | validation: 3.201013712452497]
	TIME [epoch: 8.21 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205114859053269		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 3.205114859053269 | validation: 3.204830072638176]
	TIME [epoch: 8.21 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2103227112072275		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 3.2103227112072275 | validation: 3.2492095054695254]
	TIME [epoch: 8.34 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2474332205839596		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 3.2474332205839596 | validation: 3.241190211440157]
	TIME [epoch: 8.22 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228116458490881		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 3.228116458490881 | validation: 3.2038163878897206]
	TIME [epoch: 8.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207120851496326		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 3.207120851496326 | validation: 3.2186491451453985]
	TIME [epoch: 8.21 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2167037009107258		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 3.2167037009107258 | validation: 3.221772104874562]
	TIME [epoch: 8.21 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215955386948227		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 3.215955386948227 | validation: 3.213421911367889]
	TIME [epoch: 8.21 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207901113354961		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 3.207901113354961 | validation: 3.251103594492373]
	TIME [epoch: 8.26 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2837526570909654		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 3.2837526570909654 | validation: 3.272363949209357]
	TIME [epoch: 8.21 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2646925446598054		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 3.2646925446598054 | validation: 3.235747336221916]
	TIME [epoch: 8.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227102878799565		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 3.227102878799565 | validation: 3.22383573570553]
	TIME [epoch: 8.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2241814515434597		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 3.2241814515434597 | validation: 3.221733273349857]
	TIME [epoch: 8.21 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2174950508516393		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 3.2174950508516393 | validation: 3.2151298867847555]
	TIME [epoch: 8.23 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223582396322125		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 3.223582396322125 | validation: 3.2252542470051653]
	TIME [epoch: 8.22 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22083846655676		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 3.22083846655676 | validation: 3.2037563522919896]
	TIME [epoch: 8.21 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055549849624456		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 3.2055549849624456 | validation: 3.2067383761437256]
	TIME [epoch: 8.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214197898625338		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 3.214197898625338 | validation: 3.2659581288803956]
	TIME [epoch: 8.21 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251987350038406		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 3.251987350038406 | validation: 3.2679279037845044]
	TIME [epoch: 8.21 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238670613070446		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 3.238670613070446 | validation: 3.2316964106113226]
	TIME [epoch: 8.25 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2170526225923743		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 3.2170526225923743 | validation: 3.2275146546001454]
	TIME [epoch: 8.21 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2250125908977423		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 3.2250125908977423 | validation: 3.2330579835919853]
	TIME [epoch: 8.21 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221266611490016		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 3.221266611490016 | validation: 3.209831568852305]
	TIME [epoch: 8.21 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077749146466097		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 3.2077749146466097 | validation: 3.2179620194472567]
	TIME [epoch: 8.21 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2207978956658607		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 3.2207978956658607 | validation: 3.2784770793326503]
	TIME [epoch: 8.22 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2626220520139935		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 3.2626220520139935 | validation: 3.2584859395793293]
	TIME [epoch: 8.24 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2344095615949953		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 3.2344095615949953 | validation: 3.2470354725708903]
	TIME [epoch: 8.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2262452885031974		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 3.2262452885031974 | validation: 3.2306304048499714]
	TIME [epoch: 8.21 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2183254446913434		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 3.2183254446913434 | validation: 3.2165553649014056]
	TIME [epoch: 8.21 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2343690676357406		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 3.2343690676357406 | validation: 3.2403382585524088]
	TIME [epoch: 8.21 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2482251565140143		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 3.2482251565140143 | validation: 3.2239073767211415]
	TIME [epoch: 8.23 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2171330689747206		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 3.2171330689747206 | validation: 3.198943799712267]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045446015327554		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 3.2045446015327554 | validation: 3.2071876489715025]
	TIME [epoch: 8.21 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2001972544047774		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 3.2001972544047774 | validation: 3.198324823682895]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1959426576466403		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 3.1959426576466403 | validation: 3.195287160579668]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1938413466144375		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 3.1938413466144375 | validation: 3.197111530217005]
	TIME [epoch: 8.22 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196296804680301		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 3.196296804680301 | validation: 3.2025428781715246]
	TIME [epoch: 8.25 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202339262125195		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 3.202339262125195 | validation: 3.204296820552189]
	TIME [epoch: 8.21 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1982302628778907		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 3.1982302628778907 | validation: 3.1963387655835027]
	TIME [epoch: 8.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.195807901103916		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 3.195807901103916 | validation: 3.1932423651788686]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202709629489516		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 3.202709629489516 | validation: 3.196323152470158]
	TIME [epoch: 8.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20993351700343		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 3.20993351700343 | validation: 3.1989206113281208]
	TIME [epoch: 8.25 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196977168543836		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 3.196977168543836 | validation: 3.2016940681378823]
	TIME [epoch: 8.22 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207513949120629		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 3.207513949120629 | validation: 3.213253087622486]
	TIME [epoch: 8.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071249779276467		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 3.2071249779276467 | validation: 3.2253054010333955]
	TIME [epoch: 8.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215950708916381		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 3.215950708916381 | validation: 3.225149814824281]
	TIME [epoch: 8.21 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2137929979556157		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 3.2137929979556157 | validation: 3.216240406938831]
	TIME [epoch: 8.21 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054857290723127		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 3.2054857290723127 | validation: 3.192946183494283]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1939377923964996		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 3.1939377923964996 | validation: 3.1950874745061695]
	TIME [epoch: 8.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2023103929616963		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 3.2023103929616963 | validation: 3.2136003731786964]
	TIME [epoch: 8.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068852261582967		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 3.2068852261582967 | validation: 3.218882010189583]
	TIME [epoch: 8.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2251815176338647		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 3.2251815176338647 | validation: 3.2225867020526366]
	TIME [epoch: 8.21 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.235856242983369		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 3.235856242983369 | validation: 3.2401044001577772]
	TIME [epoch: 8.24 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2420574495299608		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 3.2420574495299608 | validation: 3.2459680032445917]
	TIME [epoch: 8.22 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2237741514601197		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 3.2237741514601197 | validation: 3.225963581906738]
	TIME [epoch: 8.21 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062392750893123		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 3.2062392750893123 | validation: 3.207462655801502]
	TIME [epoch: 8.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2391838935762474		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 3.2391838935762474 | validation: 3.352831600632776]
	TIME [epoch: 8.21 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28807868820018		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 3.28807868820018 | validation: 3.310960217402426]
	TIME [epoch: 8.21 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259643829724303		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 3.259643829724303 | validation: 3.2461182940729243]
	TIME [epoch: 8.25 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2368458797351174		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 3.2368458797351174 | validation: 3.2414797570808283]
	TIME [epoch: 8.21 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2321116563652614		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 3.2321116563652614 | validation: 3.2247196413808337]
	TIME [epoch: 8.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229378611869259		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 3.229378611869259 | validation: 3.252559314831892]
	TIME [epoch: 8.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2259508014735365		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 3.2259508014735365 | validation: 3.214500025668902]
	TIME [epoch: 8.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1954823213598353		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 3.1954823213598353 | validation: 3.1941068556452707]
	TIME [epoch: 8.22 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1947522588375863		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 3.1947522588375863 | validation: 3.2118160869052517]
	TIME [epoch: 8.25 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2012022615072238		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 3.2012022615072238 | validation: 3.1991366411072644]
	TIME [epoch: 8.21 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1992629961976164		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 3.1992629961976164 | validation: 3.1978814066611325]
	TIME [epoch: 8.21 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1986980677694263		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 3.1986980677694263 | validation: 3.223637439049643]
	TIME [epoch: 8.21 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222506371110708		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 3.222506371110708 | validation: 3.23388386969444]
	TIME [epoch: 8.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2233204445276984		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 3.2233204445276984 | validation: 3.224947835972819]
	TIME [epoch: 8.24 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216928515951113		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 3.216928515951113 | validation: 3.229077590148449]
	TIME [epoch: 8.21 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2155812414823473		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 3.2155812414823473 | validation: 3.217635926180293]
	TIME [epoch: 8.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20746858244733		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 3.20746858244733 | validation: 3.21120093729831]
	TIME [epoch: 8.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202197010190924		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 3.202197010190924 | validation: 3.2042139222724657]
	TIME [epoch: 8.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2006798745169935		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 3.2006798745169935 | validation: 3.1971294400374526]
	TIME [epoch: 8.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1998478560459214		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 3.1998478560459214 | validation: 3.2003079518027686]
	TIME [epoch: 8.25 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1989011699686474		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 3.1989011699686474 | validation: 3.1985357327169925]
	TIME [epoch: 8.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2049499308658356		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 3.2049499308658356 | validation: 3.2025354051047845]
	TIME [epoch: 8.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054437283054016		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 3.2054437283054016 | validation: 3.205642161605654]
	TIME [epoch: 8.21 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2012776382128485		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 3.2012776382128485 | validation: 3.1949733988279094]
	TIME [epoch: 8.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196604167115125		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 3.196604167115125 | validation: 3.190318698158549]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1936497866388014		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 3.1936497866388014 | validation: 3.1962850096728657]
	TIME [epoch: 8.23 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1974015132572493		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 3.1974015132572493 | validation: 3.208216491001826]
	TIME [epoch: 8.21 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202573402110576		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 3.202573402110576 | validation: 3.205739051343346]
	TIME [epoch: 8.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239213850613776		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 3.239213850613776 | validation: 3.576578809517412]
	TIME [epoch: 8.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4416512648210844		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 3.4416512648210844 | validation: 3.3735589604427654]
	TIME [epoch: 8.19 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2613841469824165		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 3.2613841469824165 | validation: 3.2339740574894975]
	TIME [epoch: 8.25 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210495704794768		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 3.210495704794768 | validation: 3.203007556047677]
	TIME [epoch: 8.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1969568600738847		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 3.1969568600738847 | validation: 3.238778522974993]
	TIME [epoch: 8.19 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.272510503087627		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 3.272510503087627 | validation: 3.302422759744001]
	TIME [epoch: 8.19 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2736037035115064		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 3.2736037035115064 | validation: 3.2601157668947676]
	TIME [epoch: 8.19 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2336682251670963		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 3.2336682251670963 | validation: 3.221847673635123]
	TIME [epoch: 8.21 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215673972742359		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 3.215673972742359 | validation: 3.2070100220880855]
	TIME [epoch: 8.23 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205183697089813		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 3.205183697089813 | validation: 3.1950546725542575]
	TIME [epoch: 8.19 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1983934909959206		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 3.1983934909959206 | validation: 3.1911973312362383]
	TIME [epoch: 8.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.195401270177409		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 3.195401270177409 | validation: 3.1915328830559346]
	TIME [epoch: 8.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20297042618565		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 3.20297042618565 | validation: 3.239410360321612]
	TIME [epoch: 8.18 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233937077985175		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 3.233937077985175 | validation: 3.2304078696902336]
	TIME [epoch: 8.22 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2218343364685045		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 3.2218343364685045 | validation: 3.2594736875788826]
	TIME [epoch: 8.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232256062094608		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 3.232256062094608 | validation: 3.243926608648352]
	TIME [epoch: 8.18 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2369338961332694		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 3.2369338961332694 | validation: 3.291057590762531]
	TIME [epoch: 8.19 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2275291183973485		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 3.2275291183973485 | validation: 3.193239352509977]
	TIME [epoch: 8.18 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1949864840743265		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 3.1949864840743265 | validation: 3.1924039441738215]
	TIME [epoch: 8.19 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2011769361374167		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 3.2011769361374167 | validation: 3.207933135051929]
	TIME [epoch: 8.24 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20236251812298		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 3.20236251812298 | validation: 3.215002406468207]
	TIME [epoch: 8.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050401717783035		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 3.2050401717783035 | validation: 3.2008096562592225]
	TIME [epoch: 8.19 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.197850539382574		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 3.197850539382574 | validation: 3.200519134083465]
	TIME [epoch: 8.19 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1998391339268397		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 3.1998391339268397 | validation: 3.190518744202907]
	TIME [epoch: 8.19 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1954237668584833		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 3.1954237668584833 | validation: 3.196829518404668]
	TIME [epoch: 8.21 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218589126555548		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 3.218589126555548 | validation: 3.212120060182743]
	TIME [epoch: 8.24 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2154698880367842		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 3.2154698880367842 | validation: 3.244918951493773]
	TIME [epoch: 8.19 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209859818662263		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 3.209859818662263 | validation: 3.196532055658879]
	TIME [epoch: 8.19 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.198157015554565		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 3.198157015554565 | validation: 3.2064319946464392]
	TIME [epoch: 8.18 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1970282908581193		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 3.1970282908581193 | validation: 3.263043468670432]
	TIME [epoch: 8.19 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.266824461334466		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 3.266824461334466 | validation: 3.275081282284917]
	TIME [epoch: 8.24 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239566141658588		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 3.239566141658588 | validation: 3.218346989479203]
	TIME [epoch: 8.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223232315421402		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 3.223232315421402 | validation: 3.221693002840202]
	TIME [epoch: 8.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217608773774715		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 3.217608773774715 | validation: 3.2064306411727985]
	TIME [epoch: 8.19 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218198775608883		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 3.218198775608883 | validation: 3.24921351608471]
	TIME [epoch: 8.19 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2455611124945563		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 3.2455611124945563 | validation: 3.2356455043639265]
	TIME [epoch: 8.18 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218247862612683		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 3.218247862612683 | validation: 3.1991241582917445]
	TIME [epoch: 8.23 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2018206478183773		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 3.2018206478183773 | validation: 3.1950762876797967]
	TIME [epoch: 8.19 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.194143133306693		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 3.194143133306693 | validation: 3.1920001846010058]
	TIME [epoch: 8.19 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.191347381586729		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 3.191347381586729 | validation: 3.1883278114381683]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240519_152529/states/model_phi1_1a_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1921170594429205		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 3.1921170594429205 | validation: 3.2000431589870573]
	TIME [epoch: 8.22 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.195538163718984		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 3.195538163718984 | validation: 3.2571743672981697]
	TIME [epoch: 8.24 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212821045613679		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 3.212821045613679 | validation: 3.2138264685106437]
	TIME [epoch: 8.23 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2122014784523873		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 3.2122014784523873 | validation: 3.2038815957826636]
	TIME [epoch: 8.22 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2223358050392594		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 3.2223358050392594 | validation: 3.240265192223605]
	TIME [epoch: 8.21 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088963552311727		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 3.2088963552311727 | validation: 3.2128651425181713]
	TIME [epoch: 8.21 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228030802998673		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 3.228030802998673 | validation: 3.2533745261882583]
	TIME [epoch: 8.21 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2403385566938008		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 3.2403385566938008 | validation: 3.2454845582686405]
	TIME [epoch: 8.26 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.242572613607801		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 3.242572613607801 | validation: 3.3152314862984418]
	TIME [epoch: 8.22 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273955182163072		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 3.273955182163072 | validation: 3.2591057175265297]
	TIME [epoch: 8.22 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.245535714614016		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 3.245535714614016 | validation: 3.2312523454929254]
	TIME [epoch: 8.21 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2220864506196025		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 3.2220864506196025 | validation: 3.225844045608011]
	TIME [epoch: 8.21 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215994517462972		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 3.215994517462972 | validation: 3.2208339130763246]
	TIME [epoch: 8.24 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106889574594217		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 3.2106889574594217 | validation: 3.2381226823744993]
	TIME [epoch: 8.25 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2292727128282		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 3.2292727128282 | validation: 3.258484084297498]
	TIME [epoch: 8.22 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239012524858798		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 3.239012524858798 | validation: 3.2506056839511146]
	TIME [epoch: 8.21 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232079529013292		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 3.232079529013292 | validation: 3.2341375503220275]
	TIME [epoch: 8.22 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2171638118155648		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 3.2171638118155648 | validation: 3.2260204937861654]
	TIME [epoch: 8.21 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2219953918548683		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 3.2219953918548683 | validation: 3.2329114670721393]
	TIME [epoch: 8.27 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2185127437969645		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 3.2185127437969645 | validation: 3.2279286898432407]
	TIME [epoch: 8.23 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063947112550144		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 3.2063947112550144 | validation: 3.2059847070952956]
	TIME [epoch: 8.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2166897190136607		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 3.2166897190136607 | validation: 3.199301227839224]
	TIME [epoch: 8.22 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22311782922781		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 3.22311782922781 | validation: 3.2970070084044343]
	TIME [epoch: 8.22 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294524691691887		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 3.294524691691887 | validation: 3.3579269238248433]
	TIME [epoch: 8.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259546004821447		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 3.259546004821447 | validation: 3.2590109172121724]
	TIME [epoch: 8.26 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2198514251719677		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 3.2198514251719677 | validation: 3.207601690317749]
	TIME [epoch: 8.21 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203902434491032		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 3.203902434491032 | validation: 3.215933082919107]
	TIME [epoch: 8.22 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225408845229102		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 3.225408845229102 | validation: 3.313559591568316]
	TIME [epoch: 8.21 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274571237387579		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 3.274571237387579 | validation: 3.29765767184833]
	TIME [epoch: 8.21 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3215624949282776		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 3.3215624949282776 | validation: 3.547637026366646]
	TIME [epoch: 8.23 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.516734761701308		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 3.516734761701308 | validation: 3.8479452047306655]
	TIME [epoch: 8.22 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.591174458259753		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 3.591174458259753 | validation: 3.8368460711059003]
	TIME [epoch: 8.21 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6217470721925262		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 3.6217470721925262 | validation: 3.9349585417946065]
	TIME [epoch: 8.19 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.592869063835988		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 3.592869063835988 | validation: 3.7546757619714866]
	TIME [epoch: 8.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5388025096744045		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 3.5388025096744045 | validation: 3.798920733590051]
	TIME [epoch: 8.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.573826452004199		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 3.573826452004199 | validation: 3.866031966601891]
	TIME [epoch: 8.26 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635449536202979		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 3.635449536202979 | validation: 4.061037520151441]
	TIME [epoch: 8.21 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.710834276998212		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 3.710834276998212 | validation: 3.8801628371850416]
	TIME [epoch: 8.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.612166520436507		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 3.612166520436507 | validation: 4.096607452340018]
	TIME [epoch: 8.21 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6852234571917926		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 3.6852234571917926 | validation: 3.95575400127741]
	TIME [epoch: 8.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6140764380962773		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 3.6140764380962773 | validation: 3.9005780132954295]
	TIME [epoch: 8.22 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6181409519657635		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 3.6181409519657635 | validation: 3.9196142420227087]
	TIME [epoch: 8.25 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6284164970236388		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 3.6284164970236388 | validation: 4.0398041729271235]
	TIME [epoch: 8.21 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.705801941250197		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 3.705801941250197 | validation: 4.132681998283614]
	TIME [epoch: 8.21 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.703860477392305		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 3.703860477392305 | validation: 4.094395384262396]
	TIME [epoch: 8.21 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682450042017863		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 3.682450042017863 | validation: 4.051787483248484]
	TIME [epoch: 8.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.63661124665198		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 3.63661124665198 | validation: 3.7009580179691715]
	TIME [epoch: 8.24 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5185366866288454		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 3.5185366866288454 | validation: 3.640176791851772]
	TIME [epoch: 8.23 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375501530556887		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 3.375501530556887 | validation: 3.478775216713363]
	TIME [epoch: 8.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3880523393557667		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 3.3880523393557667 | validation: 3.6371883394912654]
	TIME [epoch: 8.21 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4553210638503624		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 3.4553210638503624 | validation: 3.73049837591489]
	TIME [epoch: 8.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4840171208849955		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 3.4840171208849955 | validation: 3.681314284550253]
	TIME [epoch: 8.21 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4859924398818896		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 3.4859924398818896 | validation: 3.7383152774354005]
	TIME [epoch: 8.25 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5391132946917296		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 3.5391132946917296 | validation: 3.8715854105535765]
	TIME [epoch: 8.21 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.616835377911894		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 3.616835377911894 | validation: 3.9195305545542665]
	TIME [epoch: 8.21 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6056235022727554		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 3.6056235022727554 | validation: 3.7964280466728084]
	TIME [epoch: 8.21 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.531259260116548		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 3.531259260116548 | validation: 3.7370970563750543]
	TIME [epoch: 8.22 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.527611145241325		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 3.527611145241325 | validation: 3.771978997691239]
	TIME [epoch: 8.24 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5710613884835967		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 3.5710613884835967 | validation: 3.8823182819072315]
	TIME [epoch: 8.23 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6043247716286047		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 3.6043247716286047 | validation: 3.8460086078102385]
	TIME [epoch: 8.21 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.556456625293249		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 3.556456625293249 | validation: 3.8285331090377177]
	TIME [epoch: 8.22 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6276573253596633		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 3.6276573253596633 | validation: 4.0332366978931535]
	TIME [epoch: 8.21 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7274556975976263		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 3.7274556975976263 | validation: 4.140990676061]
	TIME [epoch: 8.21 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.824482207028127		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 3.824482207028127 | validation: 4.28769772279345]
	TIME [epoch: 8.26 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.930657331014338		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 3.930657331014338 | validation: 4.3228921385377]
	TIME [epoch: 8.23 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9173068676074023		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 3.9173068676074023 | validation: 4.281595352154798]
	TIME [epoch: 8.21 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.965148867710046		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 3.965148867710046 | validation: 4.372907346740224]
	TIME [epoch: 8.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.983727745665032		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 3.983727745665032 | validation: 4.330690409373339]
	TIME [epoch: 8.19 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.917456695939058		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 3.917456695939058 | validation: 4.238744638760917]
	TIME [epoch: 8.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8917088234773662		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 3.8917088234773662 | validation: 4.298897980561994]
	TIME [epoch: 8.24 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9526163830201027		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 3.9526163830201027 | validation: 4.402596263362683]
	TIME [epoch: 8.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.028321662314507		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 4.028321662314507 | validation: 4.3790506520851]
	TIME [epoch: 8.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.00508417421051		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 4.00508417421051 | validation: 4.40865900231738]
	TIME [epoch: 8.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.023728771786427		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 4.023728771786427 | validation: 4.385789789151687]
	TIME [epoch: 8.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.041695770585656		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 4.041695770585656 | validation: 4.430318328305842]
	TIME [epoch: 8.22 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.084118436964173		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 4.084118436964173 | validation: 4.464020022836315]
	TIME [epoch: 8.22 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.107847358730725		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 4.107847358730725 | validation: 4.44997022918977]
	TIME [epoch: 8.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.026359954081583		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 4.026359954081583 | validation: 4.339140839560723]
	TIME [epoch: 8.19 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.922710688684106		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 3.922710688684106 | validation: 4.23032692157898]
	TIME [epoch: 8.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857181352807631		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 3.857181352807631 | validation: 4.270520730722687]
	TIME [epoch: 8.19 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8822441003528327		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 3.8822441003528327 | validation: 4.23402171053906]
	TIME [epoch: 8.24 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8707577281527517		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 3.8707577281527517 | validation: 4.210472190584632]
	TIME [epoch: 8.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8695053659127625		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 3.8695053659127625 | validation: 4.238750716214642]
	TIME [epoch: 8.18 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8810160916720875		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 3.8810160916720875 | validation: 4.2191767995376654]
	TIME [epoch: 8.19 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8047491015152914		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 3.8047491015152914 | validation: 4.098515802258361]
	TIME [epoch: 8.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6943680790261713		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 3.6943680790261713 | validation: 3.9228677879123746]
	TIME [epoch: 8.21 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.587717256707154		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 3.587717256707154 | validation: 3.7876441706003434]
	TIME [epoch: 8.22 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.575307537826046		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 3.575307537826046 | validation: 3.861309293772625]
	TIME [epoch: 8.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.659843760590715		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 3.659843760590715 | validation: 3.8328687766174347]
	TIME [epoch: 8.18 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5917720055721563		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 3.5917720055721563 | validation: 3.814595910730473]
	TIME [epoch: 8.19 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.601408129916309		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 3.601408129916309 | validation: 3.818685032057215]
	TIME [epoch: 8.18 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6179305714530665		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 3.6179305714530665 | validation: 3.914864257402241]
	TIME [epoch: 8.23 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7368821578113725		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 3.7368821578113725 | validation: 4.053125439806491]
	TIME [epoch: 8.21 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7049602997179734		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 3.7049602997179734 | validation: 4.007809771349695]
	TIME [epoch: 8.18 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752675078745961		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 3.752675078745961 | validation: 4.04225487970031]
	TIME [epoch: 8.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7801339145845345		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 3.7801339145845345 | validation: 4.150052656113111]
	TIME [epoch: 8.19 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7477374325412054		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 3.7477374325412054 | validation: 3.95578306959062]
	TIME [epoch: 8.19 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.71087958553136		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 3.71087958553136 | validation: 4.146437974881626]
	TIME [epoch: 8.23 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7813954721689775		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 3.7813954721689775 | validation: 4.184485373046615]
	TIME [epoch: 8.19 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7706647730390777		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 3.7706647730390777 | validation: 4.1645122484996815]
	TIME [epoch: 8.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.801960683871357		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 3.801960683871357 | validation: 4.277479206362072]
	TIME [epoch: 8.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.84376301204633		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 3.84376301204633 | validation: 4.24649867924888]
	TIME [epoch: 8.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.749875363277591		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 3.749875363277591 | validation: 4.002819185331776]
	TIME [epoch: 8.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8101699453114737		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 3.8101699453114737 | validation: 4.35952527112008]
	TIME [epoch: 8.22 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.048820196898184		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 4.048820196898184 | validation: 4.329008374844824]
	TIME [epoch: 8.19 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.014398064004378		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 4.014398064004378 | validation: 4.492479794755694]
	TIME [epoch: 8.18 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.046631249648691		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 4.046631249648691 | validation: 4.355894254715689]
	TIME [epoch: 8.19 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9740387037506837		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 3.9740387037506837 | validation: 4.422097496535727]
	TIME [epoch: 8.19 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.022441173835334		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 4.022441173835334 | validation: 4.326037826572816]
	TIME [epoch: 8.22 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.970773702631712		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 3.970773702631712 | validation: 4.270101187528281]
	TIME [epoch: 8.19 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9507102785073887		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 3.9507102785073887 | validation: 4.24869222571296]
	TIME [epoch: 8.18 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.891755826900956		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 3.891755826900956 | validation: 4.238885515200513]
	TIME [epoch: 8.19 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.89932054011772		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 3.89932054011772 | validation: 4.1794629536753085]
	TIME [epoch: 8.18 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8514976120878197		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 3.8514976120878197 | validation: 4.102605157296162]
	TIME [epoch: 8.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7481823840331048		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 3.7481823840331048 | validation: 3.9720579489848094]
	TIME [epoch: 8.24 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.711945006305406		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 3.711945006305406 | validation: 3.994880579059008]
	TIME [epoch: 8.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7714985175872533		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 3.7714985175872533 | validation: 4.23626789780031]
	TIME [epoch: 8.19 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.957838019359397		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 3.957838019359397 | validation: 4.387167692898407]
	TIME [epoch: 8.19 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.035654789771809		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 4.035654789771809 | validation: 4.4388174029869365]
	TIME [epoch: 8.18 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0381867442361505		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 4.0381867442361505 | validation: 4.407986736889331]
	TIME [epoch: 8.22 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.019223239551128		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 4.019223239551128 | validation: 4.3664555172433985]
	TIME [epoch: 8.22 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0488210989204365		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 4.0488210989204365 | validation: 4.414046404106783]
	TIME [epoch: 8.19 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.077954868187619		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 4.077954868187619 | validation: 4.341485507779478]
	TIME [epoch: 8.18 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9133341422048193		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 3.9133341422048193 | validation: 4.0707009958298475]
	TIME [epoch: 8.18 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778894874209039		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 3.778894874209039 | validation: 4.123483138203593]
	TIME [epoch: 8.18 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7329280793717974		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 3.7329280793717974 | validation: 3.975914180379387]
	TIME [epoch: 8.23 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.799166636483242		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 3.799166636483242 | validation: 4.148117426881145]
	TIME [epoch: 8.19 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8048474472038416		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 3.8048474472038416 | validation: 4.063118661767218]
	TIME [epoch: 8.21 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.832910112666936		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 3.832910112666936 | validation: 4.250230517093002]
	TIME [epoch: 8.22 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8997429047188206		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 3.8997429047188206 | validation: 4.096049211144521]
	TIME [epoch: 8.23 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8190542536331114		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 3.8190542536331114 | validation: 4.18639305791979]
	TIME [epoch: 8.24 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8196789014041044		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 3.8196789014041044 | validation: 4.211650836290218]
	TIME [epoch: 8.26 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8149203996113217		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 3.8149203996113217 | validation: 4.078526703118959]
	TIME [epoch: 8.22 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7633323269869394		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 3.7633323269869394 | validation: 4.032584414111831]
	TIME [epoch: 8.21 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81384895943304		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 3.81384895943304 | validation: 4.198829746242748]
	TIME [epoch: 8.22 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.959886149604059		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 3.959886149604059 | validation: 4.450635190645215]
	TIME [epoch: 8.22 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.075718719111133		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 4.075718719111133 | validation: 4.387443675582155]
	TIME [epoch: 8.25 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.986380300816691		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 3.986380300816691 | validation: 4.126216701370064]
	TIME [epoch: 8.23 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.756119869142009		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 3.756119869142009 | validation: 3.9887965605883746]
	TIME [epoch: 8.22 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7298909425502895		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 3.7298909425502895 | validation: 4.026091644834356]
	TIME [epoch: 8.22 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.846837323437154		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 3.846837323437154 | validation: 4.372380702033268]
	TIME [epoch: 8.22 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.999077114112907		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 3.999077114112907 | validation: 4.451320072847878]
	TIME [epoch: 8.22 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.018557444901065		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 4.018557444901065 | validation: 4.337262419084261]
	TIME [epoch: 8.26 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9630162445557615		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 3.9630162445557615 | validation: 4.299805895225919]
	TIME [epoch: 8.22 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9069372436166114		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 3.9069372436166114 | validation: 4.242368869562584]
	TIME [epoch: 8.22 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.906637331774225		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 3.906637331774225 | validation: 4.224805461451337]
	TIME [epoch: 8.21 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9143053659964067		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 3.9143053659964067 | validation: 4.267204812758065]
	TIME [epoch: 8.21 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.952861900621622		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 3.952861900621622 | validation: 4.295364139194353]
	TIME [epoch: 8.24 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9556439295647974		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 3.9556439295647974 | validation: 4.288298460490478]
	TIME [epoch: 8.22 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.948754512461334		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 3.948754512461334 | validation: 4.233762874324371]
	TIME [epoch: 8.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8850992759288205		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 3.8850992759288205 | validation: 4.179144756800428]
	TIME [epoch: 8.22 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8509947127509383		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 3.8509947127509383 | validation: 4.1712109546923095]
	TIME [epoch: 8.21 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.87262170736088		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 3.87262170736088 | validation: 4.220787082464846]
	TIME [epoch: 8.21 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9215618556685947		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 3.9215618556685947 | validation: 4.368833751934216]
	TIME [epoch: 8.26 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.043300475519575		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 4.043300475519575 | validation: 4.3808942213364555]
	TIME [epoch: 8.21 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.994486149634919		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 3.994486149634919 | validation: 4.222684902924529]
	TIME [epoch: 8.22 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9251860677623704		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 3.9251860677623704 | validation: 4.3625419728551655]
	TIME [epoch: 8.21 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.988523144982792		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 3.988523144982792 | validation: 4.353920211690019]
	TIME [epoch: 8.21 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.014126064945348		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 4.014126064945348 | validation: 4.242012357659476]
	TIME [epoch: 8.21 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9295611387646807		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 3.9295611387646807 | validation: 4.241056430654423]
	TIME [epoch: 8.27 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8941444746009637		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 3.8941444746009637 | validation: 4.175311257778505]
	TIME [epoch: 8.21 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8384965742088797		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 3.8384965742088797 | validation: 4.063664082805632]
	TIME [epoch: 8.21 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7771942493540904		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 3.7771942493540904 | validation: 4.038558151101133]
	TIME [epoch: 8.21 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.761940076317718		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 3.761940076317718 | validation: 4.099289420461224]
	TIME [epoch: 8.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8505611429617814		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 3.8505611429617814 | validation: 4.063509501360781]
	TIME [epoch: 8.24 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.795380099659938		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 3.795380099659938 | validation: 4.134681218614828]
	TIME [epoch: 8.23 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9088077225970563		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 3.9088077225970563 | validation: 4.297189887861306]
	TIME [epoch: 8.21 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9384560910036237		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 3.9384560910036237 | validation: 4.17298884790274]
	TIME [epoch: 8.21 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.899543237751698		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 3.899543237751698 | validation: 4.2460325879182115]
	TIME [epoch: 8.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.938489022758908		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 3.938489022758908 | validation: 4.250338678349982]
	TIME [epoch: 8.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.903917201908075		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 3.903917201908075 | validation: 4.284162373544658]
	TIME [epoch: 8.25 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9392807058925996		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 3.9392807058925996 | validation: 4.2640986329730355]
	TIME [epoch: 8.21 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.911393322626086		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 3.911393322626086 | validation: 4.204991542053186]
	TIME [epoch: 8.21 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8759048425380294		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 3.8759048425380294 | validation: 4.170838528224854]
	TIME [epoch: 8.21 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.788571520452853		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 3.788571520452853 | validation: 4.050104647537697]
	TIME [epoch: 8.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8118573713310466		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 3.8118573713310466 | validation: 4.187118023409585]
	TIME [epoch: 8.22 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.869750456190971		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 3.869750456190971 | validation: 4.1807258398247455]
	TIME [epoch: 8.24 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875776444160291		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 3.875776444160291 | validation: 4.201093368109669]
	TIME [epoch: 8.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8602117224520778		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 3.8602117224520778 | validation: 4.152323837558862]
	TIME [epoch: 8.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8035827799649136		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 3.8035827799649136 | validation: 4.1058400303730025]
	TIME [epoch: 8.21 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774516644646834		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 3.774516644646834 | validation: 4.079958632894027]
	TIME [epoch: 8.21 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7332139503884347		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 3.7332139503884347 | validation: 4.034915912594922]
	TIME [epoch: 8.24 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.711650366776263		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 3.711650366776263 | validation: 4.048043513294052]
	TIME [epoch: 8.22 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715700980387387		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 3.715700980387387 | validation: 3.9517771160064497]
	TIME [epoch: 8.21 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.627571073426523		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.627571073426523 | validation: 3.897059514685548]
	TIME [epoch: 8.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.606761534544346		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 3.606761534544346 | validation: 3.9144467356273207]
	TIME [epoch: 8.21 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6801189390756925		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 3.6801189390756925 | validation: 3.998728627184159]
	TIME [epoch: 8.22 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6987154556637734		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 3.6987154556637734 | validation: 3.9557638069364036]
	TIME [epoch: 8.25 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6267515139465494		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 3.6267515139465494 | validation: 3.880068002118053]
	TIME [epoch: 8.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6339874183275365		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 3.6339874183275365 | validation: 3.9736245329089366]
	TIME [epoch: 8.19 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.660431933371382		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 3.660431933371382 | validation: 3.880714130476743]
	TIME [epoch: 8.19 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5812577258627565		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 3.5812577258627565 | validation: 3.8434982280510743]
	TIME [epoch: 8.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5811318392535654		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 3.5811318392535654 | validation: 3.857745528372603]
	TIME [epoch: 8.22 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6194293071557544		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 3.6194293071557544 | validation: 3.9008237016914027]
	TIME [epoch: 8.21 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.618859053018815		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 3.618859053018815 | validation: 3.8886829990006753]
	TIME [epoch: 8.19 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5900925171517346		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 3.5900925171517346 | validation: 3.8512602328389995]
	TIME [epoch: 8.19 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.583845449290869		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 3.583845449290869 | validation: 3.9095103966284785]
	TIME [epoch: 8.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.59103266272256		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 3.59103266272256 | validation: 3.8878800816913675]
	TIME [epoch: 8.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.623543912685028		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 3.623543912685028 | validation: 3.933927873619136]
	TIME [epoch: 8.24 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6503304648719737		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 3.6503304648719737 | validation: 4.012492977038671]
	TIME [epoch: 8.21 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8150303059999833		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 3.8150303059999833 | validation: 4.154673399860459]
	TIME [epoch: 8.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804137713645467		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 3.804137713645467 | validation: 4.197898010868866]
	TIME [epoch: 8.21 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800490300756863		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 3.800490300756863 | validation: 4.1750304565024265]
	TIME [epoch: 8.25 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8128525578957957		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 3.8128525578957957 | validation: 4.144370982067194]
	TIME [epoch: 8.28 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816447512655158		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 3.816447512655158 | validation: 4.181724245276172]
	TIME [epoch: 8.26 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8309041822004577		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 3.8309041822004577 | validation: 4.159748894288386]
	TIME [epoch: 8.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.802606661131713		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 3.802606661131713 | validation: 3.9998268727154476]
	TIME [epoch: 8.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.707337481440929		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 3.707337481440929 | validation: 3.9953515894505234]
	TIME [epoch: 8.29 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7103089610999147		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 3.7103089610999147 | validation: 3.922040466260467]
	TIME [epoch: 8.19 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6877787528211536		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 3.6877787528211536 | validation: 3.993493377625737]
	TIME [epoch: 8.23 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.713708515246003		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 3.713708515246003 | validation: 4.004827171835718]
	TIME [epoch: 8.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6735224172408394		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 3.6735224172408394 | validation: 3.844576737071061]
	TIME [epoch: 8.18 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.605582800943896		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 3.605582800943896 | validation: 3.881195088656211]
	TIME [epoch: 8.19 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6234010682616953		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 3.6234010682616953 | validation: 4.021748448261175]
	TIME [epoch: 8.19 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6591666713756608		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 3.6591666713756608 | validation: 3.9222237735241237]
	TIME [epoch: 8.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.624625281184991		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 3.624625281184991 | validation: 3.9106519929552146]
	TIME [epoch: 8.24 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6378525073396992		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 3.6378525073396992 | validation: 3.8769937623527557]
	TIME [epoch: 8.18 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5978926989157705		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 3.5978926989157705 | validation: 3.928294062729586]
	TIME [epoch: 8.18 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.646897798341132		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 3.646897798341132 | validation: 4.047921341670294]
	TIME [epoch: 8.19 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681719054462395		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 3.681719054462395 | validation: 4.141232294973118]
	TIME [epoch: 8.19 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7502458014213005		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 3.7502458014213005 | validation: 4.153238077290339]
	TIME [epoch: 8.21 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7371204844202417		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 3.7371204844202417 | validation: 4.087828964861033]
	TIME [epoch: 8.22 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6922289674138757		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 3.6922289674138757 | validation: 4.054748250423074]
	TIME [epoch: 8.23 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.660540364225369		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 3.660540364225369 | validation: 4.034649970206351]
	TIME [epoch: 8.26 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6524128704843246		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 3.6524128704843246 | validation: 3.9332574973075705]
	TIME [epoch: 8.22 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.687195595434101		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 3.687195595434101 | validation: 3.989961804939207]
	TIME [epoch: 8.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6889381981870653		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 3.6889381981870653 | validation: 3.964208134974215]
	TIME [epoch: 8.23 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6816301380068044		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 3.6816301380068044 | validation: 4.108020081249213]
	TIME [epoch: 8.31 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.711032072445707		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 3.711032072445707 | validation: 4.02051837002887]
	TIME [epoch: 8.19 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.650661029670729		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 3.650661029670729 | validation: 3.946898105054965]
	TIME [epoch: 8.19 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.63505557031521		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 3.63505557031521 | validation: 3.99518074660406]
	TIME [epoch: 8.19 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6108326409220104		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 3.6108326409220104 | validation: 3.9894174721600386]
	TIME [epoch: 8.19 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.677123722479187		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 3.677123722479187 | validation: 4.132993785667168]
	TIME [epoch: 8.24 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7233843621660254		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 3.7233843621660254 | validation: 4.143898025991639]
	TIME [epoch: 8.19 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7342147542518536		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 3.7342147542518536 | validation: 4.179626588431044]
	TIME [epoch: 8.19 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.75983678201349		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 3.75983678201349 | validation: 4.154009441991984]
	TIME [epoch: 8.18 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.704051142114012		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 3.704051142114012 | validation: 3.997409166228265]
	TIME [epoch: 8.19 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6247622148190435		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 3.6247622148190435 | validation: 3.882405662179168]
	TIME [epoch: 8.22 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.607345073315383		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 3.607345073315383 | validation: 3.958066197019076]
	TIME [epoch: 8.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6165943905681344		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 3.6165943905681344 | validation: 4.005654063203593]
	TIME [epoch: 8.19 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.653767217913754		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 3.653767217913754 | validation: 3.987467536316877]
	TIME [epoch: 8.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.626229767052791		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 3.626229767052791 | validation: 3.9617122980923516]
	TIME [epoch: 8.19 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.619375968569556		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 3.619375968569556 | validation: 3.9882027337819768]
	TIME [epoch: 8.21 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6572255331635963		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 3.6572255331635963 | validation: 4.05707859468982]
	TIME [epoch: 8.33 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6677773300874925		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 3.6677773300874925 | validation: 4.031070917940511]
	TIME [epoch: 8.25 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6573399593778344		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 3.6573399593778344 | validation: 4.0199408870221625]
	TIME [epoch: 8.18 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6479134714643555		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 3.6479134714643555 | validation: 4.015647508200901]
	TIME [epoch: 8.19 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6543949432455225		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 3.6543949432455225 | validation: 3.99522820630584]
	TIME [epoch: 8.28 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.654401147744149		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 3.654401147744149 | validation: 4.048274427460463]
	TIME [epoch: 8.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.661827905690782		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 3.661827905690782 | validation: 3.9991483779901467]
	TIME [epoch: 8.23 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6408586909655574		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 3.6408586909655574 | validation: 3.966730994927538]
	TIME [epoch: 8.19 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6396458379074628		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 3.6396458379074628 | validation: 4.025859294211049]
	TIME [epoch: 8.18 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6727304444126436		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 3.6727304444126436 | validation: 4.072470051003538]
	TIME [epoch: 8.19 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6851769318249845		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 3.6851769318249845 | validation: 4.082734443568729]
	TIME [epoch: 8.19 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6859356002704007		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 3.6859356002704007 | validation: 4.054691376658717]
	TIME [epoch: 8.23 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6671333098338756		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 3.6671333098338756 | validation: 4.004525593754433]
	TIME [epoch: 8.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.646315620649815		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 3.646315620649815 | validation: 3.9846227863089734]
	TIME [epoch: 8.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.644706592130165		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 3.644706592130165 | validation: 4.048122523712129]
	TIME [epoch: 8.18 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.690584949704975		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 3.690584949704975 | validation: 4.11241008793577]
	TIME [epoch: 8.19 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7257263550862727		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 3.7257263550862727 | validation: 4.119996828853868]
	TIME [epoch: 8.19 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7155420568721853		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 3.7155420568721853 | validation: 4.10328899167639]
	TIME [epoch: 8.23 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.727567250833694		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 3.727567250833694 | validation: 4.120049063359293]
	TIME [epoch: 8.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7532297435378554		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 3.7532297435378554 | validation: 4.2104660745238025]
	TIME [epoch: 8.19 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8129451177419362		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 3.8129451177419362 | validation: 4.246584764340878]
	TIME [epoch: 8.24 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8492581584155166		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 3.8492581584155166 | validation: 4.274814608557977]
	TIME [epoch: 8.27 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8767902480981693		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 3.8767902480981693 | validation: 4.31178215650036]
	TIME [epoch: 8.23 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9119479915324593		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 3.9119479915324593 | validation: 4.3387133860886955]
	TIME [epoch: 8.23 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.889152525764805		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 3.889152525764805 | validation: 4.303910967503745]
	TIME [epoch: 8.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8752115870125414		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 3.8752115870125414 | validation: 4.315822300001642]
	TIME [epoch: 8.22 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8906828504218005		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 3.8906828504218005 | validation: 4.299748107993885]
	TIME [epoch: 8.21 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8893222527222506		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 3.8893222527222506 | validation: 4.326149083355912]
	TIME [epoch: 8.21 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9191916345433624		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 3.9191916345433624 | validation: 4.387802017149109]
	TIME [epoch: 8.26 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9637080238687474		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 3.9637080238687474 | validation: 4.405204306283334]
	TIME [epoch: 8.21 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9648786576160386		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 3.9648786576160386 | validation: 4.407935898466613]
	TIME [epoch: 8.21 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.986969017853636		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 3.986969017853636 | validation: 4.415169836776437]
	TIME [epoch: 8.21 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001599963641102		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 4.001599963641102 | validation: 4.439776656076597]
	TIME [epoch: 8.21 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.990964185150297		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 3.990964185150297 | validation: 4.405043941728417]
	TIME [epoch: 8.22 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9853448317138165		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 3.9853448317138165 | validation: 4.4262987130621045]
	TIME [epoch: 8.25 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.009500076156402		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 4.009500076156402 | validation: 4.4910311211413605]
	TIME [epoch: 8.21 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.051143845987662		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 4.051143845987662 | validation: 4.485231905561026]
	TIME [epoch: 8.21 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.049515799549125		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 4.049515799549125 | validation: 4.505666501134959]
	TIME [epoch: 8.21 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.058922073970626		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 4.058922073970626 | validation: 4.506351392229583]
	TIME [epoch: 8.21 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.056287377926886		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 4.056287377926886 | validation: 4.482474338222223]
	TIME [epoch: 8.25 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002423689965792		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 4.002423689965792 | validation: 4.438254068746696]
	TIME [epoch: 8.23 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002243428358249		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 4.002243428358249 | validation: 4.463365899454173]
	TIME [epoch: 8.22 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.013167834398944		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 4.013167834398944 | validation: 4.459145007630942]
	TIME [epoch: 8.21 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.01353138472776		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 4.01353138472776 | validation: 4.466242498979213]
	TIME [epoch: 8.21 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0229382510597755		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 4.0229382510597755 | validation: 4.452312523428766]
	TIME [epoch: 8.21 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.002131878452904		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 4.002131878452904 | validation: 4.418100673294269]
	TIME [epoch: 8.25 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.955888173924599		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 3.955888173924599 | validation: 4.369575690152602]
	TIME [epoch: 8.21 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.924340544466116		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 3.924340544466116 | validation: 4.357859041862811]
	TIME [epoch: 8.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9070196002038355		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 3.9070196002038355 | validation: 4.321624236930653]
	TIME [epoch: 8.21 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875805836084722		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 3.875805836084722 | validation: 4.32627088026072]
	TIME [epoch: 8.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8968357526068		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 3.8968357526068 | validation: 4.308356613607254]
	TIME [epoch: 8.22 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8934861928637288		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 3.8934861928637288 | validation: 4.344297874141328]
	TIME [epoch: 8.24 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.932298085161536		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 3.932298085161536 | validation: 4.411593932106391]
	TIME [epoch: 8.21 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.977223283931279		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 3.977223283931279 | validation: 4.4315284573682225]
	TIME [epoch: 8.22 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.001005019407005		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 4.001005019407005 | validation: 4.432378602654666]
	TIME [epoch: 8.22 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9795375938461763		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 3.9795375938461763 | validation: 4.412123331387615]
	TIME [epoch: 8.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.947013503824432		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 3.947013503824432 | validation: 4.351998709586573]
	TIME [epoch: 8.24 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.942690312077105		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 3.942690312077105 | validation: 4.382437633900444]
	TIME [epoch: 8.22 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.943075064987564		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 3.943075064987564 | validation: 4.367611004569885]
	TIME [epoch: 8.21 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9178154491029638		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 3.9178154491029638 | validation: 4.337710587028903]
	TIME [epoch: 8.21 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9051907891430098		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 3.9051907891430098 | validation: 4.330287281372582]
	TIME [epoch: 8.21 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9064749342136067		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 3.9064749342136067 | validation: 4.355079524228915]
	TIME [epoch: 8.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.937674585070583		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 3.937674585070583 | validation: 4.3488473964901235]
	TIME [epoch: 8.25 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.91883953568631		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 3.91883953568631 | validation: 4.324468143662429]
	TIME [epoch: 8.21 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8913470768576643		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 3.8913470768576643 | validation: 4.296995441453243]
	TIME [epoch: 8.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.862418957394313		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 3.862418957394313 | validation: 4.278836656206979]
	TIME [epoch: 8.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8624445015208684		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 3.8624445015208684 | validation: 4.276085202216618]
	TIME [epoch: 8.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8504633556168812		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 3.8504633556168812 | validation: 4.235696317429594]
	TIME [epoch: 8.23 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8189482441288716		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 3.8189482441288716 | validation: 4.2154181752058655]
	TIME [epoch: 8.22 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800579940392787		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 3.800579940392787 | validation: 4.179827271254022]
	TIME [epoch: 8.21 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.77553127074843		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 3.77553127074843 | validation: 4.187084855558144]
	TIME [epoch: 8.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7907382157177025		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 3.7907382157177025 | validation: 4.219031294159615]
	TIME [epoch: 8.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7897984027805314		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 3.7897984027805314 | validation: 4.183434977616171]
	TIME [epoch: 8.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779175715397497		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 3.779175715397497 | validation: 4.221759183668094]
	TIME [epoch: 8.25 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.802484521081264		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 3.802484521081264 | validation: 4.198006283862529]
	TIME [epoch: 8.21 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7908931669181527		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 3.7908931669181527 | validation: 4.195712798551982]
	TIME [epoch: 8.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7908703875396936		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 3.7908703875396936 | validation: 4.193623325900085]
	TIME [epoch: 8.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778249668293837		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 3.778249668293837 | validation: 4.19978770628539]
	TIME [epoch: 8.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7834985256480467		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 3.7834985256480467 | validation: 4.192440061878353]
	TIME [epoch: 8.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.799303905477392		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 3.799303905477392 | validation: 4.216263921341086]
	TIME [epoch: 8.24 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7962817961077553		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 3.7962817961077553 | validation: 4.207606415953207]
	TIME [epoch: 8.19 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7930573040880966		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 3.7930573040880966 | validation: 4.221926074758821]
	TIME [epoch: 8.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8029890467985616		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 3.8029890467985616 | validation: 4.228836810323514]
	TIME [epoch: 8.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8034723021171657		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 3.8034723021171657 | validation: 4.222886192514827]
	TIME [epoch: 8.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8120640425671093		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 3.8120640425671093 | validation: 4.239657673359943]
	TIME [epoch: 8.32 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7983610151129454		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 3.7983610151129454 | validation: 4.197293130808067]
	TIME [epoch: 8.21 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782719984522		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 3.782719984522 | validation: 4.182379186401349]
	TIME [epoch: 8.19 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.784866540566704		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 3.784866540566704 | validation: 4.200757244892664]
	TIME [epoch: 8.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81980906815953		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 3.81980906815953 | validation: 4.24073820852438]
	TIME [epoch: 8.19 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8415044270791916		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 3.8415044270791916 | validation: 4.267321849988182]
	TIME [epoch: 8.19 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8588245364877096		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 3.8588245364877096 | validation: 4.30184287250353]
	TIME [epoch: 8.24 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.899570635896069		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 3.899570635896069 | validation: 4.338720686022265]
	TIME [epoch: 8.19 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.915951575454816		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 3.915951575454816 | validation: 4.346744236317431]
	TIME [epoch: 8.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9252585897325045		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 3.9252585897325045 | validation: 4.355086629610625]
	TIME [epoch: 8.19 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.924343069818039		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 3.924343069818039 | validation: 4.319053867207892]
	TIME [epoch: 8.19 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9090195006899577		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 3.9090195006899577 | validation: 4.3192185213547365]
	TIME [epoch: 8.22 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8993101923471265		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 3.8993101923471265 | validation: 4.2970096261223105]
	TIME [epoch: 8.23 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863417067697846		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 3.863417067697846 | validation: 4.253505139194212]
	TIME [epoch: 8.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8400442136069017		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 3.8400442136069017 | validation: 4.234873127339264]
	TIME [epoch: 8.2 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8357344837478586		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 3.8357344837478586 | validation: 4.235934392318727]
	TIME [epoch: 8.19 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8373855837726154		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 3.8373855837726154 | validation: 4.232638727684242]
	TIME [epoch: 8.19 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816056041299155		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 3.816056041299155 | validation: 4.217688578995409]
	TIME [epoch: 8.28 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8147137869855907		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 3.8147137869855907 | validation: 4.217568771962569]
	TIME [epoch: 8.27 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8179177405875575		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 3.8179177405875575 | validation: 4.2178871401136275]
	TIME [epoch: 8.22 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.817877074299255		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 3.817877074299255 | validation: 4.21252605121669]
	TIME [epoch: 8.29 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.825750630472329		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 3.825750630472329 | validation: 4.225537751825753]
	TIME [epoch: 8.19 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.804127816831509		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 3.804127816831509 | validation: 4.210445070966052]
	TIME [epoch: 8.19 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8167614400654513		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 3.8167614400654513 | validation: 4.246035336706863]
	TIME [epoch: 8.23 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.837887298409397		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 3.837887298409397 | validation: 4.27000483269418]
	TIME [epoch: 8.19 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8417845275483744		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 3.8417845275483744 | validation: 4.244301930137194]
	TIME [epoch: 8.19 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.820384696918028		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 3.820384696918028 | validation: 4.229595025415859]
	TIME [epoch: 8.19 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8309862514193007		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 3.8309862514193007 | validation: 4.277305311262047]
	TIME [epoch: 8.19 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.862273223254861		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 3.862273223254861 | validation: 4.2924905368997015]
	TIME [epoch: 8.22 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850220159075915		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 3.850220159075915 | validation: 4.247375732932776]
	TIME [epoch: 8.21 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.84076247359252		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 3.84076247359252 | validation: 4.260624134596698]
	TIME [epoch: 8.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8287560979992037		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 3.8287560979992037 | validation: 4.222585333467666]
	TIME [epoch: 8.19 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7968707395574857		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 3.7968707395574857 | validation: 4.192810710262597]
	TIME [epoch: 8.19 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.769647436288625		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 3.769647436288625 | validation: 4.202940406134104]
	TIME [epoch: 8.19 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800069073912989		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 3.800069073912989 | validation: 4.230122018586948]
	TIME [epoch: 8.23 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8099168024937144		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 3.8099168024937144 | validation: 4.22171369115235]
	TIME [epoch: 8.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.807081083991677		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 3.807081083991677 | validation: 4.233136499377695]
	TIME [epoch: 8.19 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816511436639122		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 3.816511436639122 | validation: 4.211283995497909]
	TIME [epoch: 8.22 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7958734680063815		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 3.7958734680063815 | validation: 4.178727531858603]
	TIME [epoch: 8.26 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7634778638908757		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 3.7634778638908757 | validation: 4.145417215631564]
	TIME [epoch: 8.21 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7385300956655154		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 3.7385300956655154 | validation: 4.111233173967031]
	TIME [epoch: 8.23 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.731018574229662		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 3.731018574229662 | validation: 4.126703494030642]
	TIME [epoch: 8.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.734862395732421		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 3.734862395732421 | validation: 4.122279831097098]
	TIME [epoch: 8.18 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7245500806853986		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 3.7245500806853986 | validation: 4.134148184778173]
	TIME [epoch: 8.19 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.738892296494924		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 3.738892296494924 | validation: 4.155251885333604]
	TIME [epoch: 8.2 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768835419077117		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 3.768835419077117 | validation: 4.20928506739026]
	TIME [epoch: 8.23 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8032746237531967		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 3.8032746237531967 | validation: 4.229926800096233]
	TIME [epoch: 8.21 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8095606065364267		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 3.8095606065364267 | validation: 4.222867955809545]
	TIME [epoch: 8.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.806836354059627		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 3.806836354059627 | validation: 4.23639620039955]
	TIME [epoch: 8.19 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8250146702733927		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 3.8250146702733927 | validation: 4.227980265963836]
	TIME [epoch: 8.19 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816144340965705		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 3.816144340965705 | validation: 4.232856765702504]
	TIME [epoch: 8.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.813392619226083		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 3.813392619226083 | validation: 4.244149067265691]
	TIME [epoch: 8.24 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8305877796941887		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 3.8305877796941887 | validation: 4.28232857161291]
	TIME [epoch: 8.19 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842813320149639		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 3.842813320149639 | validation: 4.234300730382529]
	TIME [epoch: 8.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8178842362506327		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 3.8178842362506327 | validation: 4.234657208903897]
	TIME [epoch: 8.19 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8095630829755027		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 3.8095630829755027 | validation: 4.20707965109119]
	TIME [epoch: 8.18 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78104387720897		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 3.78104387720897 | validation: 4.168699907062079]
	TIME [epoch: 8.21 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7798378645018746		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 3.7798378645018746 | validation: 4.19538794051624]
	TIME [epoch: 8.22 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800532091315067		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 3.800532091315067 | validation: 4.227066018468264]
	TIME [epoch: 8.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.830886056383169		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 3.830886056383169 | validation: 4.2575239279974015]
	TIME [epoch: 8.25 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8568151144558978		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 3.8568151144558978 | validation: 4.282213826691832]
	TIME [epoch: 8.28 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.874438621262696		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 3.874438621262696 | validation: 4.310182386402683]
	TIME [epoch: 8.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8973732470777347		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 3.8973732470777347 | validation: 4.328278486244523]
	TIME [epoch: 8.25 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.912019470534293		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 3.912019470534293 | validation: 4.338803382851222]
	TIME [epoch: 8.21 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9088479741506488		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 3.9088479741506488 | validation: 4.321689806583667]
	TIME [epoch: 8.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8885475834848577		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 3.8885475834848577 | validation: 4.297802867320314]
	TIME [epoch: 8.21 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9031174056228557		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 3.9031174056228557 | validation: 4.348541102870055]
	TIME [epoch: 8.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9295628660909334		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 3.9295628660909334 | validation: 4.363624987970695]
	TIME [epoch: 8.21 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9462753488833213		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 3.9462753488833213 | validation: 4.374869305115121]
	TIME [epoch: 8.23 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9628017231434813		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 3.9628017231434813 | validation: 4.410059684500842]
	TIME [epoch: 8.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9765422718031322		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 3.9765422718031322 | validation: 4.399041932586453]
	TIME [epoch: 8.19 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9746543532196608		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 3.9746543532196608 | validation: 4.371383375706192]
	TIME [epoch: 8.19 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.946515831556305		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 3.946515831556305 | validation: 4.346379067008014]
	TIME [epoch: 8.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9031860296037713		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 3.9031860296037713 | validation: 4.286952498677511]
	TIME [epoch: 8.23 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.871253663439214		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 3.871253663439214 | validation: 4.290699687047519]
	TIME [epoch: 8.21 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8789577610243886		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 3.8789577610243886 | validation: 4.276903862667011]
	TIME [epoch: 8.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8745518235730083		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 3.8745518235730083 | validation: 4.301458123046052]
	TIME [epoch: 8.19 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875694164942727		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 3.875694164942727 | validation: 4.260097067866367]
	TIME [epoch: 8.19 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8474046407191675		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 3.8474046407191675 | validation: 4.218838643940778]
	TIME [epoch: 8.2 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8172547062752162		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 3.8172547062752162 | validation: 4.197305017357815]
	TIME [epoch: 8.24 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.800528499874093		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 3.800528499874093 | validation: 4.181473226671333]
	TIME [epoch: 8.21 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7924626937469648		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 3.7924626937469648 | validation: 4.176698558808727]
	TIME [epoch: 8.21 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.781788784759075		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 3.781788784759075 | validation: 4.16292537861965]
	TIME [epoch: 8.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7660009599425663		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 3.7660009599425663 | validation: 4.155492917490284]
	TIME [epoch: 8.19 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777808227364823		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 3.777808227364823 | validation: 4.161113661927246]
	TIME [epoch: 8.22 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779265609793705		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 3.779265609793705 | validation: 4.1846964555152955]
	TIME [epoch: 8.23 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.791742273328195		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 3.791742273328195 | validation: 4.182540716144311]
	TIME [epoch: 8.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7954753449910545		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 3.7954753449910545 | validation: 4.187268927253902]
	TIME [epoch: 8.19 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7981406381186815		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 3.7981406381186815 | validation: 4.190599248775401]
	TIME [epoch: 8.19 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7863512058418025		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 3.7863512058418025 | validation: 4.165989882479305]
	TIME [epoch: 8.19 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776762173803902		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 3.776762173803902 | validation: 4.152826540691385]
	TIME [epoch: 8.23 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752014417198831		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 3.752014417198831 | validation: 4.098666698579251]
	TIME [epoch: 8.21 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7221667208738305		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 3.7221667208738305 | validation: 4.088522353496014]
	TIME [epoch: 8.19 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.71404681736196		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 3.71404681736196 | validation: 4.074234668124172]
	TIME [epoch: 8.18 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7039130072474924		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 3.7039130072474924 | validation: 4.060438191099147]
	TIME [epoch: 8.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.691377221239608		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 3.691377221239608 | validation: 4.046434738700217]
	TIME [epoch: 8.19 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6863832773968968		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 3.6863832773968968 | validation: 4.026928354389263]
	TIME [epoch: 8.24 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.672306183205105		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 3.672306183205105 | validation: 4.039164137428699]
	TIME [epoch: 8.19 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67859997855248		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 3.67859997855248 | validation: 4.044646254278229]
	TIME [epoch: 8.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6925652360642456		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 3.6925652360642456 | validation: 4.053646457937509]
	TIME [epoch: 8.19 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6894590975214907		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 3.6894590975214907 | validation: 4.035698804455279]
	TIME [epoch: 8.25 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682696355766233		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 3.682696355766233 | validation: 4.036606714408059]
	TIME [epoch: 8.32 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.679341831407568		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 3.679341831407568 | validation: 4.036379847186552]
	TIME [epoch: 8.27 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6844999268494103		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 3.6844999268494103 | validation: 4.023913131091396]
	TIME [epoch: 8.21 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683080116856596		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 3.683080116856596 | validation: 4.030814143675137]
	TIME [epoch: 8.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6803336524394124		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 3.6803336524394124 | validation: 4.027511850998776]
	TIME [epoch: 8.2 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6962937578180712		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 3.6962937578180712 | validation: 4.056725172441091]
	TIME [epoch: 8.21 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7069350224952706		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 3.7069350224952706 | validation: 4.058580020002497]
	TIME [epoch: 8.25 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.706982039910229		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 3.706982039910229 | validation: 4.064386012387606]
	TIME [epoch: 8.21 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7270161475582175		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 3.7270161475582175 | validation: 4.076658941572608]
	TIME [epoch: 8.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.723329796619843		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 3.723329796619843 | validation: 4.061507431186309]
	TIME [epoch: 8.21 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718886649655204		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 3.718886649655204 | validation: 4.100003019781031]
	TIME [epoch: 8.21 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7318803627989388		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 3.7318803627989388 | validation: 4.09960397137794]
	TIME [epoch: 8.22 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.753637987594991		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 3.753637987594991 | validation: 4.117179300624224]
	TIME [epoch: 8.25 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7627104478297633		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 3.7627104478297633 | validation: 4.158005119302]
	TIME [epoch: 8.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.801440964734912		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 3.801440964734912 | validation: 4.187306870730264]
	TIME [epoch: 8.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81426859343721		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 3.81426859343721 | validation: 4.213269304411466]
	TIME [epoch: 8.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8296884609233928		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 3.8296884609233928 | validation: 4.227606320838058]
	TIME [epoch: 8.2 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.825269263837943		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 3.825269263837943 | validation: 4.185504403599053]
	TIME [epoch: 8.24 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8194239271782138		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 3.8194239271782138 | validation: 4.203980785767429]
	TIME [epoch: 8.22 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826838968987667		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 3.826838968987667 | validation: 4.221025162749649]
	TIME [epoch: 8.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.854156407230291		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 3.854156407230291 | validation: 4.265683252748003]
	TIME [epoch: 8.28 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8994534909585004		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 3.8994534909585004 | validation: 4.301733920183768]
	TIME [epoch: 8.27 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.89899012535743		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 3.89899012535743 | validation: 4.281732655661556]
	TIME [epoch: 8.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.88738915849187		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 3.88738915849187 | validation: 4.265655287414143]
	TIME [epoch: 8.24 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8680732584978967		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 3.8680732584978967 | validation: 4.209223852687892]
	TIME [epoch: 8.2 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.842832749685556		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 3.842832749685556 | validation: 4.224150738113765]
	TIME [epoch: 8.21 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8428563658992023		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 3.8428563658992023 | validation: 4.210468293518634]
	TIME [epoch: 8.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858222940385999		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 3.858222940385999 | validation: 4.229897536896535]
	TIME [epoch: 8.2 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8632897711813117		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 3.8632897711813117 | validation: 4.229899406323314]
	TIME [epoch: 8.23 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8670078584573044		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 3.8670078584573044 | validation: 4.242559491906775]
	TIME [epoch: 8.21 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870011958582163		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 3.870011958582163 | validation: 4.217989552017771]
	TIME [epoch: 8.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8623712866421718		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 3.8623712866421718 | validation: 4.225188093826946]
	TIME [epoch: 8.2 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8627871578749344		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 3.8627871578749344 | validation: 4.227667134206293]
	TIME [epoch: 8.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8750170974680964		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 3.8750170974680964 | validation: 4.250612907036926]
	TIME [epoch: 8.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8948648880339847		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 3.8948648880339847 | validation: 4.248719966171261]
	TIME [epoch: 8.25 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.89512948870058		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 3.89512948870058 | validation: 4.25880580926539]
	TIME [epoch: 8.21 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9137018832115658		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 3.9137018832115658 | validation: 4.285155478190958]
	TIME [epoch: 8.21 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9283607463545223		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 3.9283607463545223 | validation: 4.317108351013026]
	TIME [epoch: 8.21 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.946835144371204		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 3.946835144371204 | validation: 4.3283359527933705]
	TIME [epoch: 8.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.942154939690778		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 3.942154939690778 | validation: 4.314525759335412]
	TIME [epoch: 8.32 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.936977340920727		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 3.936977340920727 | validation: 4.31386606880522]
	TIME [epoch: 8.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.944345598036348		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 3.944345598036348 | validation: 4.3053264247808904]
	TIME [epoch: 8.27 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9471665073998734		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 3.9471665073998734 | validation: 4.319754210122369]
	TIME [epoch: 8.21 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.95230636264597		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 3.95230636264597 | validation: 4.2967616045433035]
	TIME [epoch: 8.19 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.935777076983093		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 3.935777076983093 | validation: 4.286819198937505]
	TIME [epoch: 8.19 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.909009484601675		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 3.909009484601675 | validation: 4.260309833668902]
	TIME [epoch: 8.24 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.894325746887643		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 3.894325746887643 | validation: 4.266352994410417]
	TIME [epoch: 8.21 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.893351316520712		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 3.893351316520712 | validation: 4.250669472728102]
	TIME [epoch: 8.2 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870955152547139		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 3.870955152547139 | validation: 4.227359241666774]
	TIME [epoch: 8.21 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8685906576330593		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 3.8685906576330593 | validation: 4.212152370937192]
	TIME [epoch: 8.21 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8634945302859407		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 3.8634945302859407 | validation: 4.222331811941206]
	TIME [epoch: 8.2 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8696920255368648		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 3.8696920255368648 | validation: 4.236959404886053]
	TIME [epoch: 8.24 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888158289678768		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 3.888158289678768 | validation: 4.257000484148052]
	TIME [epoch: 8.19 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9076384527776007		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 3.9076384527776007 | validation: 4.296815456426882]
	TIME [epoch: 8.17 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9346901412052278		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 3.9346901412052278 | validation: 4.334897016392802]
	TIME [epoch: 8.19 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9589509538682046		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 3.9589509538682046 | validation: 4.339752622277194]
	TIME [epoch: 8.19 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.960770333777529		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 3.960770333777529 | validation: 4.326126317835065]
	TIME [epoch: 8.21 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9615502100122404		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 3.9615502100122404 | validation: 4.3260867830143015]
	TIME [epoch: 8.23 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.960587640550669		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 3.960587640550669 | validation: 4.332081341261254]
	TIME [epoch: 8.2 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.958463655862173		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 3.958463655862173 | validation: 4.345909405366578]
	TIME [epoch: 8.27 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9659602012701365		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 3.9659602012701365 | validation: 4.341873854839092]
	TIME [epoch: 8.19 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9694453217438546		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 3.9694453217438546 | validation: 4.32800281181653]
	TIME [epoch: 8.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9568701471208527		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 3.9568701471208527 | validation: 4.320754229286823]
	TIME [epoch: 8.24 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9393102023330178		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 3.9393102023330178 | validation: 4.295915142956297]
	TIME [epoch: 8.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.925214011270947		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 3.925214011270947 | validation: 4.2886228338787]
	TIME [epoch: 8.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9153087020748165		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 3.9153087020748165 | validation: 4.2849805152299325]
	TIME [epoch: 8.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.902444628535956		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 3.902444628535956 | validation: 4.263693431085203]
	TIME [epoch: 8.19 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9064229960916776		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 3.9064229960916776 | validation: 4.271638585643319]
	TIME [epoch: 8.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9083334063410122		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 3.9083334063410122 | validation: 4.2710986449815875]
	TIME [epoch: 8.24 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9189226423320385		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 3.9189226423320385 | validation: 4.266095381281451]
	TIME [epoch: 8.2 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.90438102514568		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 3.90438102514568 | validation: 4.269044918030643]
	TIME [epoch: 8.19 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8915259851027906		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 3.8915259851027906 | validation: 4.2390500132257545]
	TIME [epoch: 8.19 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8677449802325103		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 3.8677449802325103 | validation: 4.217516057482668]
	TIME [epoch: 8.19 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.856116740631803		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 3.856116740631803 | validation: 4.2279225934580165]
	TIME [epoch: 8.22 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.857102831784813		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 3.857102831784813 | validation: 4.222612945321059]
	TIME [epoch: 8.21 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.859604332753433		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 3.859604332753433 | validation: 4.225960975424231]
	TIME [epoch: 8.19 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8713827902325275		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 3.8713827902325275 | validation: 4.226410015691161]
	TIME [epoch: 8.19 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.875169748359243		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 3.875169748359243 | validation: 4.240505751963706]
	TIME [epoch: 8.19 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8794352796898144		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 3.8794352796898144 | validation: 4.2416168635822755]
	TIME [epoch: 8.19 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.885469200571305		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 3.885469200571305 | validation: 4.250436974577282]
	TIME [epoch: 8.35 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888279484148198		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 3.888279484148198 | validation: 4.248420619331588]
	TIME [epoch: 8.27 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.877671315364238		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 3.877671315364238 | validation: 4.245783889485897]
	TIME [epoch: 8.26 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8751189564819906		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 3.8751189564819906 | validation: 4.223517357673408]
	TIME [epoch: 8.2 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8673668408944986		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 3.8673668408944986 | validation: 4.2382681198414325]
	TIME [epoch: 8.2 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.867041160320543		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 3.867041160320543 | validation: 4.220121487498373]
	TIME [epoch: 8.2 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.85330625399032		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 3.85330625399032 | validation: 4.202237161648536]
	TIME [epoch: 8.22 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850773608056877		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 3.850773608056877 | validation: 4.214010706452994]
	TIME [epoch: 8.2 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8606625753276385		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 3.8606625753276385 | validation: 4.216420664511328]
	TIME [epoch: 8.2 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.858116123617249		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 3.858116123617249 | validation: 4.224994480805279]
	TIME [epoch: 8.2 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872725036257824		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 3.872725036257824 | validation: 4.2198363033919275]
	TIME [epoch: 8.2 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872597117027011		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 3.872597117027011 | validation: 4.24073114868003]
	TIME [epoch: 8.23 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8685259442302535		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 3.8685259442302535 | validation: 4.2113878412428445]
	TIME [epoch: 8.21 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852499753095758		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 3.852499753095758 | validation: 4.220378995850277]
	TIME [epoch: 8.18 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850718805310959		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 3.850718805310959 | validation: 4.195758158418604]
	TIME [epoch: 8.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.836535612586751		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 3.836535612586751 | validation: 4.197835270798224]
	TIME [epoch: 8.2 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8561739926786096		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 3.8561739926786096 | validation: 4.231646457406172]
	TIME [epoch: 8.19 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.877083610029701		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 3.877083610029701 | validation: 4.248669281339642]
	TIME [epoch: 8.24 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8931327261970496		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 3.8931327261970496 | validation: 4.2500358945727825]
	TIME [epoch: 8.19 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9068397475138594		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 3.9068397475138594 | validation: 4.28762078998729]
	TIME [epoch: 8.19 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9188327296067413		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 3.9188327296067413 | validation: 4.2867459149159295]
	TIME [epoch: 8.29 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9139366166439853		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 3.9139366166439853 | validation: 4.277487230462878]
	TIME [epoch: 8.26 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.901176807077312		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 3.901176807077312 | validation: 4.2613824185864395]
	TIME [epoch: 8.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9040200900924935		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 3.9040200900924935 | validation: 4.295213920321807]
	TIME [epoch: 8.22 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9243195024583053		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 3.9243195024583053 | validation: 4.296207555027296]
	TIME [epoch: 8.19 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9229860403965677		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 3.9229860403965677 | validation: 4.302942192641705]
	TIME [epoch: 8.19 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.923537347494675		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 3.923537347494675 | validation: 4.289890513006133]
	TIME [epoch: 8.18 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9255114053430624		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 3.9255114053430624 | validation: 4.308355982367598]
	TIME [epoch: 8.19 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.930055360080669		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 3.930055360080669 | validation: 4.317267110981432]
	TIME [epoch: 8.24 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9324489409325274		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 3.9324489409325274 | validation: 4.3041836709923]
	TIME [epoch: 8.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9230057889406917		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 3.9230057889406917 | validation: 4.306496066688819]
	TIME [epoch: 8.19 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9290754655197775		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 3.9290754655197775 | validation: 4.3181431221002216]
	TIME [epoch: 8.2 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9339833263553396		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 3.9339833263553396 | validation: 4.305201004972257]
	TIME [epoch: 8.19 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.927729655462076		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 3.927729655462076 | validation: 4.294203951728429]
	TIME [epoch: 8.2 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9208212511654246		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 3.9208212511654246 | validation: 4.313164115983409]
	TIME [epoch: 8.24 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.920055669949827		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 3.920055669949827 | validation: 4.2748539626171596]
	TIME [epoch: 8.2 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9043016187760617		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 3.9043016187760617 | validation: 4.279320509159737]
	TIME [epoch: 8.21 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.894875015658344		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 3.894875015658344 | validation: 4.260023720988295]
	TIME [epoch: 8.19 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.877482772640762		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 3.877482772640762 | validation: 4.237255235701344]
	TIME [epoch: 8.19 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8704657653081025		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 3.8704657653081025 | validation: 4.234941189498169]
	TIME [epoch: 8.22 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8832377267227276		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 3.8832377267227276 | validation: 4.265731835285318]
	TIME [epoch: 8.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9049073525584643		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 3.9049073525584643 | validation: 4.276767824711182]
	TIME [epoch: 8.2 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.907408082043382		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 3.907408082043382 | validation: 4.286512003914961]
	TIME [epoch: 8.26 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9114181782181277		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 3.9114181782181277 | validation: 4.309852051185063]
	TIME [epoch: 8.24 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.930122183673267		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 3.930122183673267 | validation: 4.30344533829895]
	TIME [epoch: 8.19 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.928373290934493		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 3.928373290934493 | validation: 4.2870920043961505]
	TIME [epoch: 8.23 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.914225570246503		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 3.914225570246503 | validation: 4.277054592265081]
	TIME [epoch: 8.19 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9055694774683007		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 3.9055694774683007 | validation: 4.269373326623501]
	TIME [epoch: 8.19 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.90495198299382		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 3.90495198299382 | validation: 4.27386968466284]
	TIME [epoch: 8.19 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9074115907906353		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 3.9074115907906353 | validation: 4.2646353791427405]
	TIME [epoch: 8.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8988256157971573		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 3.8988256157971573 | validation: 4.25593013531494]
	TIME [epoch: 8.21 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.885218825284987		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 3.885218825284987 | validation: 4.241054511748315]
	TIME [epoch: 8.22 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870077324740979		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 3.870077324740979 | validation: 4.2450521638242344]
	TIME [epoch: 8.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8808017337486813		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 3.8808017337486813 | validation: 4.254760700688528]
	TIME [epoch: 8.2 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.883204438754289		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 3.883204438754289 | validation: 4.249600574282367]
	TIME [epoch: 8.19 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8918468983518455		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 3.8918468983518455 | validation: 4.247000465592069]
	TIME [epoch: 8.18 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8745627965305856		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 3.8745627965305856 | validation: 4.220874310758942]
	TIME [epoch: 8.23 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8605230933786245		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 3.8605230933786245 | validation: 4.233487804525916]
	TIME [epoch: 8.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8609981888090417		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 3.8609981888090417 | validation: 4.225815326011655]
	TIME [epoch: 8.19 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.860270750031609		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 3.860270750031609 | validation: 4.218793971871975]
	TIME [epoch: 8.18 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.856455853953982		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 3.856455853953982 | validation: 4.214367084485685]
	TIME [epoch: 8.32 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8477761633414493		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 3.8477761633414493 | validation: 4.2138293049960724]
	TIME [epoch: 8.22 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840049075444642		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 3.840049075444642 | validation: 4.1892868569162784]
	TIME [epoch: 8.33 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8363372824347692		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 3.8363372824347692 | validation: 4.212790017471377]
	TIME [epoch: 8.28 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8434561325245804		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 3.8434561325245804 | validation: 4.200422033622026]
	TIME [epoch: 8.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8551415008870293		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 3.8551415008870293 | validation: 4.219553810457457]
	TIME [epoch: 8.19 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8673505799186456		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 3.8673505799186456 | validation: 4.240215157073407]
	TIME [epoch: 8.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.878456888429122		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 3.878456888429122 | validation: 4.251294594286196]
	TIME [epoch: 8.23 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888725485243738		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 3.888725485243738 | validation: 4.2583365569858564]
	TIME [epoch: 8.21 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8876862967182273		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 3.8876862967182273 | validation: 4.2477938390288354]
	TIME [epoch: 8.21 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.883541464016937		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 3.883541464016937 | validation: 4.243258898110017]
	TIME [epoch: 8.21 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.870815675742688		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 3.870815675742688 | validation: 4.218852853199334]
	TIME [epoch: 8.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8631547770476953		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 3.8631547770476953 | validation: 4.220630771630524]
	TIME [epoch: 8.21 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8548478061588343		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 3.8548478061588343 | validation: 4.212851828612852]
	TIME [epoch: 8.25 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8505861400483417		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 3.8505861400483417 | validation: 4.206386148822782]
	TIME [epoch: 8.21 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.83897061424969		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 3.83897061424969 | validation: 4.19303704218048]
	TIME [epoch: 8.22 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8468514327510546		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 3.8468514327510546 | validation: 4.195024994272112]
	TIME [epoch: 8.21 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.840214138015135		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 3.840214138015135 | validation: 4.212596012470408]
	TIME [epoch: 8.21 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.852550008878673		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 3.852550008878673 | validation: 4.219738748383323]
	TIME [epoch: 8.22 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863733022651124		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 3.863733022651124 | validation: 4.221590564501548]
	TIME [epoch: 8.25 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8608827249487474		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 3.8608827249487474 | validation: 4.207361817374384]
	TIME [epoch: 8.32 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.850767457686407		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 3.850767457686407 | validation: 4.201034244067258]
	TIME [epoch: 8.2 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.84062543332098		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 3.84062543332098 | validation: 4.204638033782501]
	TIME [epoch: 8.23 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8265331495001433		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 3.8265331495001433 | validation: 4.167485876413471]
	TIME [epoch: 8.2 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.80875985176614		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 3.80875985176614 | validation: 4.168852011160429]
	TIME [epoch: 8.24 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8085542949080615		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 3.8085542949080615 | validation: 4.161901684863929]
	TIME [epoch: 8.21 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.803435285239063		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 3.803435285239063 | validation: 4.147087833758063]
	TIME [epoch: 8.21 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8023008011436414		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 3.8023008011436414 | validation: 4.15379489952975]
	TIME [epoch: 8.21 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8064171232029604		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 3.8064171232029604 | validation: 4.156968659435833]
	TIME [epoch: 8.2 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8167137006000775		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 3.8167137006000775 | validation: 4.175181794473957]
	TIME [epoch: 8.2 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8120196669750546		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 3.8120196669750546 | validation: 4.166498201609793]
	TIME [epoch: 8.24 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8133799354821827		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 3.8133799354821827 | validation: 4.168191357034462]
	TIME [epoch: 8.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8201953855643986		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 3.8201953855643986 | validation: 4.176496290834191]
	TIME [epoch: 8.19 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8190298464051247		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 3.8190298464051247 | validation: 4.1725978526442145]
	TIME [epoch: 8.19 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8114628131799932		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 3.8114628131799932 | validation: 4.1622449752181705]
	TIME [epoch: 8.19 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8095557271652143		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 3.8095557271652143 | validation: 4.166134902313841]
	TIME [epoch: 8.22 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8201076090559116		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 3.8201076090559116 | validation: 4.177392228161734]
	TIME [epoch: 8.21 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8276312225762523		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 3.8276312225762523 | validation: 4.1904132212593]
	TIME [epoch: 8.19 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.826119979061161		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 3.826119979061161 | validation: 4.181736969545576]
	TIME [epoch: 8.19 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8270320124551196		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 3.8270320124551196 | validation: 4.179894802153591]
	TIME [epoch: 8.19 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8298903899691044		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 3.8298903899691044 | validation: 4.183183685411177]
	TIME [epoch: 8.21 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8180910494905773		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 3.8180910494905773 | validation: 4.164913557476536]
	TIME [epoch: 8.25 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.823272308907462		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 3.823272308907462 | validation: 4.177611255429458]
	TIME [epoch: 8.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.823022897535399		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 3.823022897535399 | validation: 4.168835673622434]
	TIME [epoch: 8.16 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.814521593572539		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 3.814521593572539 | validation: 4.175988953838477]
	TIME [epoch: 8.15 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8157774713358776		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 3.8157774713358776 | validation: 4.180300211245796]
	TIME [epoch: 8.2 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.81158180632245		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 3.81158180632245 | validation: 4.1671786536727655]
	TIME [epoch: 8.28 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.815533163314166		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 3.815533163314166 | validation: 4.177336345300079]
	TIME [epoch: 8.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8237342840397597		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 3.8237342840397597 | validation: 4.181124654699539]
	TIME [epoch: 8.2 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8219384780496717		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 3.8219384780496717 | validation: 4.176920846386423]
	TIME [epoch: 8.19 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.821169446360124		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 3.821169446360124 | validation: 4.1701802998612525]
	TIME [epoch: 8.19 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8099036252527156		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 3.8099036252527156 | validation: 4.158773832796703]
	TIME [epoch: 8.19 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8075361720802356		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 3.8075361720802356 | validation: 4.149147544245135]
	TIME [epoch: 8.23 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.803971339432467		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 3.803971339432467 | validation: 4.16368009460847]
	TIME [epoch: 8.2 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8153918814992607		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 3.8153918814992607 | validation: 4.157377202940252]
	TIME [epoch: 8.19 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.813388704241852		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 3.813388704241852 | validation: 4.1746054994913635]
	TIME [epoch: 8.19 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.808089574370511		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 3.808089574370511 | validation: 4.156423731510166]
	TIME [epoch: 8.18 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8046496910615297		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 3.8046496910615297 | validation: 4.143515263197152]
	TIME [epoch: 8.19 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.796161999960728		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 3.796161999960728 | validation: 4.132019473591479]
	TIME [epoch: 8.24 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7886612547691287		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 3.7886612547691287 | validation: 4.144644166539959]
	TIME [epoch: 8.19 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7884369178551855		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 3.7884369178551855 | validation: 4.1456305936928715]
	TIME [epoch: 8.29 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7983586967240726		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 3.7983586967240726 | validation: 4.176831423511141]
	TIME [epoch: 8.19 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8099443106450552		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 3.8099443106450552 | validation: 4.16100138778789]
	TIME [epoch: 8.19 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8201759793729275		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 3.8201759793729275 | validation: 4.166392512328327]
	TIME [epoch: 8.23 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.816868297946899		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 3.816868297946899 | validation: 4.165632122568862]
	TIME [epoch: 8.21 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8263766343387418		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 3.8263766343387418 | validation: 4.188801936633814]
	TIME [epoch: 8.21 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.823046128149427		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 3.823046128149427 | validation: 4.162249185765516]
	TIME [epoch: 8.27 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8177798010011097		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 3.8177798010011097 | validation: 4.16314631392814]
	TIME [epoch: 8.25 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8076496763821166		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 3.8076496763821166 | validation: 4.166475862023153]
	TIME [epoch: 8.19 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7974597015939433		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 3.7974597015939433 | validation: 4.146736701947983]
	TIME [epoch: 8.24 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7965371274658186		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 3.7965371274658186 | validation: 4.141546342832065]
	TIME [epoch: 8.19 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8003621641775593		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 3.8003621641775593 | validation: 4.1481987898008565]
	TIME [epoch: 8.19 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7978895596294215		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 3.7978895596294215 | validation: 4.152723482899409]
	TIME [epoch: 8.19 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7966004188146467		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 3.7966004188146467 | validation: 4.1461679702051555]
	TIME [epoch: 8.19 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7973315257420763		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 3.7973315257420763 | validation: 4.150759581286607]
	TIME [epoch: 8.21 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7911823548571304		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 3.7911823548571304 | validation: 4.145573995427014]
	TIME [epoch: 8.23 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7903340422613088		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 3.7903340422613088 | validation: 4.134418630888524]
	TIME [epoch: 8.19 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7785518250353514		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 3.7785518250353514 | validation: 4.13253754502354]
	TIME [epoch: 8.19 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.772980270909115		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 3.772980270909115 | validation: 4.114995954652947]
	TIME [epoch: 8.19 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7710371581053934		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 3.7710371581053934 | validation: 4.121878845650311]
	TIME [epoch: 8.19 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765952780728249		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 3.765952780728249 | validation: 4.110243913072883]
	TIME [epoch: 8.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762123669212718		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 3.762123669212718 | validation: 4.108104342020628]
	TIME [epoch: 8.2 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767656092229191		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 3.767656092229191 | validation: 4.1073216818896965]
	TIME [epoch: 8.19 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.759055571067285		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 3.759055571067285 | validation: 4.097429827180452]
	TIME [epoch: 8.19 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7511999601100943		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 3.7511999601100943 | validation: 4.093462513214111]
	TIME [epoch: 8.18 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7500880888685133		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 3.7500880888685133 | validation: 4.103380478854178]
	TIME [epoch: 8.19 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7524399773113677		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 3.7524399773113677 | validation: 4.0981011880858365]
	TIME [epoch: 8.31 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.746303426534315		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 3.746303426534315 | validation: 4.098620022274213]
	TIME [epoch: 8.26 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752344313317039		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 3.752344313317039 | validation: 4.101381583218405]
	TIME [epoch: 8.2 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752612050697496		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 3.752612050697496 | validation: 4.094548614637482]
	TIME [epoch: 8.19 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.75037328457702		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 3.75037328457702 | validation: 4.08819871440517]
	TIME [epoch: 8.18 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7445805651458586		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 3.7445805651458586 | validation: 4.079662444543525]
	TIME [epoch: 8.2 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7456729614046895		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 3.7456729614046895 | validation: 4.092979391505366]
	TIME [epoch: 8.21 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748328387824105		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 3.748328387824105 | validation: 4.102615986356966]
	TIME [epoch: 8.18 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7520231148710743		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 3.7520231148710743 | validation: 4.103954448114227]
	TIME [epoch: 8.19 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7637625026754233		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 3.7637625026754233 | validation: 4.113289929996784]
	TIME [epoch: 8.19 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7713142938872206		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 3.7713142938872206 | validation: 4.109926591596856]
	TIME [epoch: 8.19 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7677688949078356		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 3.7677688949078356 | validation: 4.109379205181111]
	TIME [epoch: 8.24 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7691110928922384		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 3.7691110928922384 | validation: 4.114153727707735]
	TIME [epoch: 8.21 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7666849235527606		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 3.7666849235527606 | validation: 4.1118081071555785]
	TIME [epoch: 8.19 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7616584406234037		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 3.7616584406234037 | validation: 4.10986899952311]
	TIME [epoch: 8.29 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.755963528446637		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 3.755963528446637 | validation: 4.11001895646358]
	TIME [epoch: 8.17 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.757350973171876		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 3.757350973171876 | validation: 4.103087507448322]
	TIME [epoch: 8.19 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7614855181492186		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 3.7614855181492186 | validation: 4.108706843324155]
	TIME [epoch: 8.23 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7681991500141816		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 3.7681991500141816 | validation: 4.106964068447512]
	TIME [epoch: 8.19 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7656341436011105		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 3.7656341436011105 | validation: 4.118490436483446]
	TIME [epoch: 8.18 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7800338473615653		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 3.7800338473615653 | validation: 4.13537874163476]
	TIME [epoch: 8.23 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.787019960580075		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 3.787019960580075 | validation: 4.1380754284206756]
	TIME [epoch: 8.26 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.796833269785363		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 3.796833269785363 | validation: 4.147311412612951]
	TIME [epoch: 8.26 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7916025661939092		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 3.7916025661939092 | validation: 4.138444650548612]
	TIME [epoch: 8.19 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7889605475329255		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 3.7889605475329255 | validation: 4.132718381163507]
	TIME [epoch: 8.19 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.787340835113642		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 3.787340835113642 | validation: 4.132851575508779]
	TIME [epoch: 8.18 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7774931136927137		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 3.7774931136927137 | validation: 4.104320110816909]
	TIME [epoch: 8.19 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.770825510964322		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 3.770825510964322 | validation: 4.125165919399214]
	TIME [epoch: 8.19 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7783910026148546		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 3.7783910026148546 | validation: 4.134796197159342]
	TIME [epoch: 8.24 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7806822533803714		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 3.7806822533803714 | validation: 4.126867448676088]
	TIME [epoch: 8.2 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782895288442547		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 3.782895288442547 | validation: 4.115925342984367]
	TIME [epoch: 8.18 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7675429236069378		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 3.7675429236069378 | validation: 4.1196449170258225]
	TIME [epoch: 8.19 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7712681018945946		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 3.7712681018945946 | validation: 4.12218107281706]
	TIME [epoch: 8.18 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7713621778227693		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 3.7713621778227693 | validation: 4.117216650367993]
	TIME [epoch: 8.19 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7686572732472783		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 3.7686572732472783 | validation: 4.1168381386913]
	TIME [epoch: 8.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.769489311781104		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 3.769489311781104 | validation: 4.114048973862591]
	TIME [epoch: 8.19 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7714513894027273		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 3.7714513894027273 | validation: 4.120718138621862]
	TIME [epoch: 8.18 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.77098513401235		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 3.77098513401235 | validation: 4.110063635269922]
	TIME [epoch: 8.18 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763709091974347		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 3.763709091974347 | validation: 4.1249036244871995]
	TIME [epoch: 8.18 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774637532414588		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 3.774637532414588 | validation: 4.132452076025816]
	TIME [epoch: 8.22 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774172315390879		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 3.774172315390879 | validation: 4.127236208386651]
	TIME [epoch: 8.23 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7707320110408764		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 3.7707320110408764 | validation: 4.118489684925283]
	TIME [epoch: 8.27 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767023071787152		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 3.767023071787152 | validation: 4.117387757466311]
	TIME [epoch: 8.24 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766064501092379		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 3.766064501092379 | validation: 4.111310797178314]
	TIME [epoch: 8.19 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.760891036457689		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 3.760891036457689 | validation: 4.116873640385693]
	TIME [epoch: 8.19 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7635165084419406		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 3.7635165084419406 | validation: 4.11388559439586]
	TIME [epoch: 8.23 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7682423589465164		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 3.7682423589465164 | validation: 4.1301024532067565]
	TIME [epoch: 8.19 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7665797082485084		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 3.7665797082485084 | validation: 4.1087749965921425]
	TIME [epoch: 8.18 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7630364965334637		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 3.7630364965334637 | validation: 4.1193244650215215]
	TIME [epoch: 8.18 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7680311342997825		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 3.7680311342997825 | validation: 4.1126837792988375]
	TIME [epoch: 8.18 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7628137228136915		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 3.7628137228136915 | validation: 4.105416904351165]
	TIME [epoch: 8.23 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7589700878077084		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 3.7589700878077084 | validation: 4.1003927426153375]
	TIME [epoch: 8.2 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.756995977940618		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 3.756995977940618 | validation: 4.0954519584953815]
	TIME [epoch: 8.19 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7598458281857123		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 3.7598458281857123 | validation: 4.0961910439584415]
	TIME [epoch: 8.17 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.750262786043912		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 3.750262786043912 | validation: 4.105022898163698]
	TIME [epoch: 8.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7541170361986995		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 3.7541170361986995 | validation: 4.095696577472834]
	TIME [epoch: 8.18 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7454600899872528		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 3.7454600899872528 | validation: 4.081831319222276]
	TIME [epoch: 8.24 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74672861189861		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 3.74672861189861 | validation: 4.098751521119959]
	TIME [epoch: 8.19 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7512537905150474		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 3.7512537905150474 | validation: 4.093758519819543]
	TIME [epoch: 8.18 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7487152512367343		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 3.7487152512367343 | validation: 4.098272668838769]
	TIME [epoch: 8.18 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7577394125136365		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 3.7577394125136365 | validation: 4.103245564101243]
	TIME [epoch: 8.19 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7601148944515845		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 3.7601148944515845 | validation: 4.106386064998697]
	TIME [epoch: 8.21 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7632633868433913		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 3.7632633868433913 | validation: 4.100764795135367]
	TIME [epoch: 8.53 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763981356061806		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 3.763981356061806 | validation: 4.117051596385048]
	TIME [epoch: 8.21 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.764171398955523		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 3.764171398955523 | validation: 4.121009105630758]
	TIME [epoch: 8.21 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.764871252627664		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 3.764871252627664 | validation: 4.1196937475098]
	TIME [epoch: 8.21 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7538416970152877		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 3.7538416970152877 | validation: 4.098437876844013]
	TIME [epoch: 8.2 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.748603343903504		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 3.748603343903504 | validation: 4.1016582655439935]
	TIME [epoch: 8.24 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7520979608033445		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 3.7520979608033445 | validation: 4.101815939879237]
	TIME [epoch: 8.22 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7577271643470977		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 3.7577271643470977 | validation: 4.109082908250397]
	TIME [epoch: 8.2 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766568767226058		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 3.766568767226058 | validation: 4.135188813890014]
	TIME [epoch: 8.21 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7720219250068916		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 3.7720219250068916 | validation: 4.1268908772191475]
	TIME [epoch: 8.22 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777749674246471		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 3.777749674246471 | validation: 4.125564688445542]
	TIME [epoch: 8.22 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7742864320640286		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 3.7742864320640286 | validation: 4.125454809868725]
	TIME [epoch: 8.25 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7689158351714633		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 3.7689158351714633 | validation: 4.11045100607282]
	TIME [epoch: 8.21 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7610390337628825		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 3.7610390337628825 | validation: 4.104548444497059]
	TIME [epoch: 8.2 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.758294003977759		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 3.758294003977759 | validation: 4.108843286040846]
	TIME [epoch: 8.2 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7506694145865906		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 3.7506694145865906 | validation: 4.086150300559323]
	TIME [epoch: 8.2 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7404950121651437		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 3.7404950121651437 | validation: 4.093858080873149]
	TIME [epoch: 8.23 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7457809292465214		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 3.7457809292465214 | validation: 4.086922534921953]
	TIME [epoch: 8.22 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74208327189577		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 3.74208327189577 | validation: 4.0921167104221645]
	TIME [epoch: 8.2 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7380970932980144		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 3.7380970932980144 | validation: 4.085329409678854]
	TIME [epoch: 8.26 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.745969475235761		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 3.745969475235761 | validation: 4.088522080832927]
	TIME [epoch: 8.28 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74558922434348		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 3.74558922434348 | validation: 4.087784089547199]
	TIME [epoch: 8.23 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7455664489772738		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 3.7455664489772738 | validation: 4.084353076717607]
	TIME [epoch: 8.25 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7403563495898884		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 3.7403563495898884 | validation: 4.0806056353282525]
	TIME [epoch: 8.21 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7409563141245767		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 3.7409563141245767 | validation: 4.084217825613458]
	TIME [epoch: 8.2 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7396251519533954		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 3.7396251519533954 | validation: 4.095860084409998]
	TIME [epoch: 8.2 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.740263955026361		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 3.740263955026361 | validation: 4.082234175427434]
	TIME [epoch: 8.21 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7381494680526326		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 3.7381494680526326 | validation: 4.063451092179793]
	TIME [epoch: 8.22 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.741521003825455		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 3.741521003825455 | validation: 4.089741641778799]
	TIME [epoch: 8.26 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7401153543764365		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 3.7401153543764365 | validation: 4.076434312940532]
	TIME [epoch: 8.21 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7354770974476743		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 3.7354770974476743 | validation: 4.081549411283639]
	TIME [epoch: 8.21 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.733595815035192		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 3.733595815035192 | validation: 4.0825392216599985]
	TIME [epoch: 8.2 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.72961849242595		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 3.72961849242595 | validation: 4.065509763284997]
	TIME [epoch: 8.28 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7236481565394115		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 3.7236481565394115 | validation: 4.064807914388275]
	TIME [epoch: 8.23 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7284080000035713		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 3.7284080000035713 | validation: 4.07598630116513]
	TIME [epoch: 8.22 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7375700907822145		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 3.7375700907822145 | validation: 4.073666176222508]
	TIME [epoch: 8.2 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7382303044031		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 3.7382303044031 | validation: 4.094168458921949]
	TIME [epoch: 8.19 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7403199846590525		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 3.7403199846590525 | validation: 4.086966346219397]
	TIME [epoch: 8.2 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7451442493182716		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 3.7451442493182716 | validation: 4.096555257617327]
	TIME [epoch: 8.2 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7490050861886224		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 3.7490050861886224 | validation: 4.1117889629234305]
	TIME [epoch: 8.25 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7592484609757886		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 3.7592484609757886 | validation: 4.1254429709239755]
	TIME [epoch: 8.2 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765894697644193		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 3.765894697644193 | validation: 4.119342937860486]
	TIME [epoch: 8.2 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7651249892500993		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 3.7651249892500993 | validation: 4.107581279201362]
	TIME [epoch: 8.19 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7648299467779083		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 3.7648299467779083 | validation: 4.118271790125925]
	TIME [epoch: 8.18 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767357769474252		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 3.767357769474252 | validation: 4.124857723471318]
	TIME [epoch: 8.19 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768535002830633		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 3.768535002830633 | validation: 4.120815041557185]
	TIME [epoch: 8.22 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7683805271556787		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 3.7683805271556787 | validation: 4.118406790642355]
	TIME [epoch: 8.19 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7632426017669007		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 3.7632426017669007 | validation: 4.120252459192711]
	TIME [epoch: 8.2 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.76203734426705		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 3.76203734426705 | validation: 4.1097900431917225]
	TIME [epoch: 8.19 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7609744690483122		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 3.7609744690483122 | validation: 4.107461598438938]
	TIME [epoch: 8.19 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.756366320232041		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 3.756366320232041 | validation: 4.10793251210516]
	TIME [epoch: 8.23 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7590755458107483		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 3.7590755458107483 | validation: 4.110607602859215]
	TIME [epoch: 8.2 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762212041984216		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 3.762212041984216 | validation: 4.119803122120562]
	TIME [epoch: 8.19 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7604674826574715		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 3.7604674826574715 | validation: 4.106969291316416]
	TIME [epoch: 8.2 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7611141422420875		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 3.7611141422420875 | validation: 4.114017424552188]
	TIME [epoch: 8.19 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7593338371913876		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 3.7593338371913876 | validation: 4.1036827314839766]
	TIME [epoch: 8.2 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7535696334271424		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 3.7535696334271424 | validation: 4.097275043803279]
	TIME [epoch: 8.24 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7519586953914503		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 3.7519586953914503 | validation: 4.091534853043007]
	TIME [epoch: 8.2 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7513130842515174		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 3.7513130842515174 | validation: 4.104160019690223]
	TIME [epoch: 8.2 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.742855065789715		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 3.742855065789715 | validation: 4.084431597752449]
	TIME [epoch: 8.19 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.739574371807508		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 3.739574371807508 | validation: 4.073838025079977]
	TIME [epoch: 8.27 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.733602779128221		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 3.733602779128221 | validation: 4.0673263648950355]
	TIME [epoch: 8.31 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.73071067810518		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 3.73071067810518 | validation: 4.065817284055873]
	TIME [epoch: 8.22 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.724044861090896		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 3.724044861090896 | validation: 4.058566921817346]
	TIME [epoch: 8.19 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7279175627924523		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 3.7279175627924523 | validation: 4.07085087778322]
	TIME [epoch: 8.2 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7331668856219204		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 3.7331668856219204 | validation: 4.075410957437578]
	TIME [epoch: 8.2 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7277567057293353		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 3.7277567057293353 | validation: 4.057997774726289]
	TIME [epoch: 8.19 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.728733023671259		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 3.728733023671259 | validation: 4.067475431873532]
	TIME [epoch: 8.24 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.725969494228689		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 3.725969494228689 | validation: 4.064460218331783]
	TIME [epoch: 8.2 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7263607980539057		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 3.7263607980539057 | validation: 4.08267061623486]
	TIME [epoch: 8.2 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.732349941517529		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 3.732349941517529 | validation: 4.086073362995503]
	TIME [epoch: 8.2 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7324237051853344		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 3.7324237051853344 | validation: 4.068060052803688]
	TIME [epoch: 8.19 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7248152480997927		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 3.7248152480997927 | validation: 4.059777444191727]
	TIME [epoch: 8.32 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7148804758742457		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 3.7148804758742457 | validation: 4.046968125114438]
	TIME [epoch: 8.23 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7090867845688344		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 3.7090867845688344 | validation: 4.034066919542106]
	TIME [epoch: 8.19 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.703914537211707		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 3.703914537211707 | validation: 4.032959523894711]
	TIME [epoch: 8.19 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6971006339553067		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 3.6971006339553067 | validation: 4.030056903492849]
	TIME [epoch: 8.19 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7002325996589702		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 3.7002325996589702 | validation: 4.0257478014294135]
	TIME [epoch: 8.19 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6961906211036855		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 3.6961906211036855 | validation: 4.015639220282525]
	TIME [epoch: 8.23 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6872430449257303		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 3.6872430449257303 | validation: 4.018142828487529]
	TIME [epoch: 8.21 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6859440281738984		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 3.6859440281738984 | validation: 4.009108096248729]
	TIME [epoch: 8.26 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6794729977872485		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 3.6794729977872485 | validation: 4.0107411242094155]
	TIME [epoch: 8.27 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67991677117349		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 3.67991677117349 | validation: 4.015122045088823]
	TIME [epoch: 8.22 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6893335592999117		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 3.6893335592999117 | validation: 4.011017106132323]
	TIME [epoch: 8.19 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6886814372698358		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 3.6886814372698358 | validation: 4.0265160716992945]
	TIME [epoch: 8.23 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6950095277381503		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 3.6950095277381503 | validation: 4.033381966226812]
	TIME [epoch: 8.2 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.692557483906283		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 3.692557483906283 | validation: 4.0408801858383265]
	TIME [epoch: 8.19 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7029568252193337		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 3.7029568252193337 | validation: 4.0478645108516575]
	TIME [epoch: 8.19 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7092632276556037		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 3.7092632276556037 | validation: 4.043607555028487]
	TIME [epoch: 8.19 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7111192634082872		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 3.7111192634082872 | validation: 4.046620912031916]
	TIME [epoch: 8.23 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715669083518109		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 3.715669083518109 | validation: 4.047881827374303]
	TIME [epoch: 8.21 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7177218083245074		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 3.7177218083245074 | validation: 4.04817861310581]
	TIME [epoch: 8.2 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7084017291566207		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 3.7084017291566207 | validation: 4.035162341175095]
	TIME [epoch: 8.27 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7034539934000748		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 3.7034539934000748 | validation: 4.035392278932694]
	TIME [epoch: 8.19 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7035935202557337		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 3.7035935202557337 | validation: 4.025172084955267]
	TIME [epoch: 8.19 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7003284303260906		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 3.7003284303260906 | validation: 4.034237280608649]
	TIME [epoch: 8.24 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7041673602307865		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 3.7041673602307865 | validation: 4.023241612027835]
	TIME [epoch: 8.19 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.696341834334102		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 3.696341834334102 | validation: 4.019736243695219]
	TIME [epoch: 8.19 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6944656840672607		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 3.6944656840672607 | validation: 4.016979841982366]
	TIME [epoch: 8.19 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.695024192515215		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 3.695024192515215 | validation: 4.023416937419935]
	TIME [epoch: 8.19 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.694046448317649		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 3.694046448317649 | validation: 4.02377109399672]
	TIME [epoch: 8.24 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.693081705714005		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 3.693081705714005 | validation: 4.020757523753864]
	TIME [epoch: 8.31 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.685811270754713		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 3.685811270754713 | validation: 4.005324898256579]
	TIME [epoch: 8.24 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681263229277632		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 3.681263229277632 | validation: 4.0042133580736365]
	TIME [epoch: 8.19 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6812302345360557		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 3.6812302345360557 | validation: 4.003642428186319]
	TIME [epoch: 8.19 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6795842411186728		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 3.6795842411186728 | validation: 3.9989426599727427]
	TIME [epoch: 8.18 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682294588338479		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 3.682294588338479 | validation: 4.008974490242203]
	TIME [epoch: 8.23 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681949144857632		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 3.681949144857632 | validation: 3.995638851693089]
	TIME [epoch: 8.22 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6812206268709744		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 3.6812206268709744 | validation: 4.011371611950352]
	TIME [epoch: 8.19 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.687386557828432		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 3.687386557828432 | validation: 4.012848535656392]
	TIME [epoch: 8.2 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683925930455586		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 3.683925930455586 | validation: 4.019730508535562]
	TIME [epoch: 8.19 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.686917029484366		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 3.686917029484366 | validation: 4.011053566363135]
	TIME [epoch: 8.19 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6827939795737517		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 3.6827939795737517 | validation: 4.01291672980077]
	TIME [epoch: 8.3 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.678055305427535		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 3.678055305427535 | validation: 3.989166222836047]
	TIME [epoch: 8.18 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6683751103958464		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 3.6683751103958464 | validation: 3.9903214289825275]
	TIME [epoch: 8.18 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.668085219578303		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 3.668085219578303 | validation: 4.003520422427311]
	TIME [epoch: 8.19 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.675355444992042		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 3.675355444992042 | validation: 4.001197517949978]
	TIME [epoch: 8.18 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.675840289649394		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 3.675840289649394 | validation: 4.003995214451841]
	TIME [epoch: 8.22 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6714116383743467		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 3.6714116383743467 | validation: 3.983358126545287]
	TIME [epoch: 8.22 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6633568828324488		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 3.6633568828324488 | validation: 3.9843820614289744]
	TIME [epoch: 8.2 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6616714001399417		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 3.6616714001399417 | validation: 3.97163251759587]
	TIME [epoch: 8.22 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6619130294454214		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 3.6619130294454214 | validation: 3.9968882521312237]
	TIME [epoch: 8.26 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6686020691769765		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 3.6686020691769765 | validation: 3.985575022356835]
	TIME [epoch: 8.25 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6684997171941434		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 3.6684997171941434 | validation: 3.98627897012035]
	TIME [epoch: 8.23 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.671447319454144		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 3.671447319454144 | validation: 3.993159573816449]
	TIME [epoch: 8.19 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6697730592011286		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 3.6697730592011286 | validation: 3.997413809694896]
	TIME [epoch: 8.18 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.672316801919951		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 3.672316801919951 | validation: 4.00107194766443]
	TIME [epoch: 8.2 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6747383105253553		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 3.6747383105253553 | validation: 4.004838432243331]
	TIME [epoch: 8.19 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.680638716675434		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 3.680638716675434 | validation: 4.001723888354419]
	TIME [epoch: 8.19 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.676366912399615		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 3.676366912399615 | validation: 3.9914901074140126]
	TIME [epoch: 8.23 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.673221405278405		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 3.673221405278405 | validation: 3.9955745205500763]
	TIME [epoch: 8.19 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6679939123005054		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 3.6679939123005054 | validation: 3.9852918697302666]
	TIME [epoch: 8.18 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6650133374653366		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 3.6650133374653366 | validation: 3.9869908908381344]
	TIME [epoch: 8.29 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.657479011368619		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 3.657479011368619 | validation: 3.9817685513354037]
	TIME [epoch: 8.19 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6650753342125753		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 3.6650753342125753 | validation: 3.9835772873004345]
	TIME [epoch: 8.22 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6633582602458805		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 3.6633582602458805 | validation: 3.993708278777282]
	TIME [epoch: 8.21 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.670670190450478		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 3.670670190450478 | validation: 4.00547871851245]
	TIME [epoch: 8.18 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6781603489553056		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 3.6781603489553056 | validation: 4.009556381903012]
	TIME [epoch: 8.18 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6807722649341557		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 3.6807722649341557 | validation: 4.014902641252216]
	TIME [epoch: 8.19 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.689010623059774		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 3.689010623059774 | validation: 4.023512486403282]
	TIME [epoch: 8.19 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6860617974621572		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 3.6860617974621572 | validation: 4.017077291675953]
	TIME [epoch: 8.23 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.685750497896147		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 3.685750497896147 | validation: 4.02576143214511]
	TIME [epoch: 8.27 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.688271995002499		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 3.688271995002499 | validation: 4.021663388119174]
	TIME [epoch: 8.26 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.686791240828128		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 3.686791240828128 | validation: 4.018269938610036]
	TIME [epoch: 8.21 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6836714362624754		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 3.6836714362624754 | validation: 4.0209050208638955]
	TIME [epoch: 8.19 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6860669863263746		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 3.6860669863263746 | validation: 4.012671395096757]
	TIME [epoch: 8.2 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6851191099253686		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 3.6851191099253686 | validation: 4.015892681711205]
	TIME [epoch: 8.23 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682518375030334		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 3.682518375030334 | validation: 4.010952777227056]
	TIME [epoch: 8.2 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6829492105288235		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 3.6829492105288235 | validation: 4.010625788699527]
	TIME [epoch: 8.19 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683061278941522		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 3.683061278941522 | validation: 4.020628287023238]
	TIME [epoch: 8.19 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6849871306680297		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 3.6849871306680297 | validation: 4.01527368876331]
	TIME [epoch: 8.18 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6815521497326604		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 3.6815521497326604 | validation: 4.023038599932312]
	TIME [epoch: 8.22 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.68281631120915		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 3.68281631120915 | validation: 4.022686991702294]
	TIME [epoch: 8.32 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6867060468607082		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 3.6867060468607082 | validation: 4.023988619240193]
	TIME [epoch: 8.2 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6911330295179168		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 3.6911330295179168 | validation: 4.026320214772824]
	TIME [epoch: 8.21 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.68979668587049		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 3.68979668587049 | validation: 4.032279125717275]
	TIME [epoch: 8.21 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.69027758591138		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 3.69027758591138 | validation: 4.026007745402071]
	TIME [epoch: 8.21 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.693954030189538		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 3.693954030189538 | validation: 4.027715970885787]
	TIME [epoch: 8.25 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6904051828808817		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 3.6904051828808817 | validation: 4.028775438002798]
	TIME [epoch: 8.22 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.692655419830574		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 3.692655419830574 | validation: 4.026705047879097]
	TIME [epoch: 8.22 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.691609329295471		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 3.691609329295471 | validation: 4.026576822756615]
	TIME [epoch: 8.22 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6880375259519997		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 3.6880375259519997 | validation: 4.022605828508807]
	TIME [epoch: 8.26 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6883280582377185		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 3.6883280582377185 | validation: 4.022326791979106]
	TIME [epoch: 8.28 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.68945863318173		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 3.68945863318173 | validation: 4.012538387931201]
	TIME [epoch: 8.22 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.678615249579936		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 3.678615249579936 | validation: 3.989864903781661]
	TIME [epoch: 8.2 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6729651362237306		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 3.6729651362237306 | validation: 3.996336584626897]
	TIME [epoch: 8.2 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.669377781828274		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 3.669377781828274 | validation: 4.007310179395147]
	TIME [epoch: 8.21 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6756957939403434		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 3.6756957939403434 | validation: 4.012492886229335]
	TIME [epoch: 8.21 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6728624150223768		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 3.6728624150223768 | validation: 3.9984367894197144]
	TIME [epoch: 8.26 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6747819015603245		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 3.6747819015603245 | validation: 4.010938619975103]
	TIME [epoch: 8.21 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.674049512302799		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 3.674049512302799 | validation: 4.010109727092113]
	TIME [epoch: 8.2 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6802120008753474		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 3.6802120008753474 | validation: 4.004142747702616]
	TIME [epoch: 8.2 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.676283297541886		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 3.676283297541886 | validation: 3.9993114553737286]
	TIME [epoch: 8.2 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67207232050117		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 3.67207232050117 | validation: 4.008953188880237]
	TIME [epoch: 8.23 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6771336217666555		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 3.6771336217666555 | validation: 4.015226951545905]
	TIME [epoch: 8.24 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681844216710892		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 3.681844216710892 | validation: 4.014837158557393]
	TIME [epoch: 8.2 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.683946112459607		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 3.683946112459607 | validation: 4.0092536999005395]
	TIME [epoch: 8.21 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.679520193148321		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 3.679520193148321 | validation: 4.009159803770167]
	TIME [epoch: 8.19 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6807388236159317		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 3.6807388236159317 | validation: 4.012169051970302]
	TIME [epoch: 8.2 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6799237641562446		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 3.6799237641562446 | validation: 4.002307418462374]
	TIME [epoch: 8.24 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.67402060669382		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 3.67402060669382 | validation: 4.006169735368042]
	TIME [epoch: 8.21 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6759022084365056		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 3.6759022084365056 | validation: 4.015333575545799]
	TIME [epoch: 8.2 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6843886626099374		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 3.6843886626099374 | validation: 4.012916352094984]
	TIME [epoch: 8.21 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.682812918229057		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 3.682812918229057 | validation: 4.008327792047]
	TIME [epoch: 8.28 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6814731650615595		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 3.6814731650615595 | validation: 4.017749451204556]
	TIME [epoch: 8.26 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.681899701649232		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 3.681899701649232 | validation: 4.023548792443743]
	TIME [epoch: 8.25 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6850187413702913		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 3.6850187413702913 | validation: 4.027270575310823]
	TIME [epoch: 8.2 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6904065221500186		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 3.6904065221500186 | validation: 4.0301216199922205]
	TIME [epoch: 8.2 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.697389463808772		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 3.697389463808772 | validation: 4.03232262566417]
	TIME [epoch: 8.2 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7004762033889054		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 3.7004762033889054 | validation: 4.0421063107029696]
	TIME [epoch: 8.21 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7022108245193146		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 3.7022108245193146 | validation: 4.043311875430384]
	TIME [epoch: 8.24 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7001399854504453		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 3.7001399854504453 | validation: 4.055693422594866]
	TIME [epoch: 8.22 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.700022614161223		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 3.700022614161223 | validation: 4.048514353024692]
	TIME [epoch: 8.31 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.707134946719474		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 3.707134946719474 | validation: 4.04657587603343]
	TIME [epoch: 8.2 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7087425749685057		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 3.7087425749685057 | validation: 4.0438806555704305]
	TIME [epoch: 8.2 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.708917619659874		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 3.708917619659874 | validation: 4.043773200155172]
	TIME [epoch: 8.2 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7020516609151013		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 3.7020516609151013 | validation: 4.038483536485256]
	TIME [epoch: 8.23 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7066332262085253		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 3.7066332262085253 | validation: 4.049262270398588]
	TIME [epoch: 8.2 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.707947864532663		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 3.707947864532663 | validation: 4.052428094395619]
	TIME [epoch: 8.2 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.698834705364063		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 3.698834705364063 | validation: 4.041871685484349]
	TIME [epoch: 8.2 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7011221242118064		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 3.7011221242118064 | validation: 4.040567971862049]
	TIME [epoch: 8.2 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.702998619713244		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 3.702998619713244 | validation: 4.039016703230336]
	TIME [epoch: 8.22 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6990508723939852		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 3.6990508723939852 | validation: 4.031344915237833]
	TIME [epoch: 8.24 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6947350243238546		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 3.6947350243238546 | validation: 4.030390791077446]
	TIME [epoch: 8.27 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6949397268459787		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 3.6949397268459787 | validation: 4.031252420693223]
	TIME [epoch: 8.28 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.692868743408543		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 3.692868743408543 | validation: 4.038391283589448]
	TIME [epoch: 8.2 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.693151246916616		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 3.693151246916616 | validation: 4.026136303120767]
	TIME [epoch: 8.2 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6943724556483333		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 3.6943724556483333 | validation: 4.032209859322855]
	TIME [epoch: 8.23 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.693383669682973		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 3.693383669682973 | validation: 4.038315781032396]
	TIME [epoch: 8.21 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.696195542980493		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 3.696195542980493 | validation: 4.032736073734963]
	TIME [epoch: 8.2 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6946198452753345		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 3.6946198452753345 | validation: 4.039042454998922]
	TIME [epoch: 8.2 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6953005965301493		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 3.6953005965301493 | validation: 4.026983000510583]
	TIME [epoch: 8.2 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.698923415423663		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 3.698923415423663 | validation: 4.038301224171231]
	TIME [epoch: 8.28 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7021828841255155		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 3.7021828841255155 | validation: 4.039271314322635]
	TIME [epoch: 8.25 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7000808924053494		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 3.7000808924053494 | validation: 4.031661582368615]
	TIME [epoch: 8.19 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.699710136793194		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 3.699710136793194 | validation: 4.043585099815806]
	TIME [epoch: 8.2 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7071474943412834		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 3.7071474943412834 | validation: 4.049196348330202]
	TIME [epoch: 8.18 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7122514893441445		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 3.7122514893441445 | validation: 4.055994273231433]
	TIME [epoch: 8.2 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7156633858477695		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 3.7156633858477695 | validation: 4.063538099193486]
	TIME [epoch: 8.23 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719864731209881		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 3.719864731209881 | validation: 4.068843432358214]
	TIME [epoch: 8.2 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7217391076249657		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 3.7217391076249657 | validation: 4.063328381916462]
	TIME [epoch: 8.18 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7236722484298577		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 3.7236722484298577 | validation: 4.066676993929075]
	TIME [epoch: 8.19 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.724450697901241		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 3.724450697901241 | validation: 4.079396302348628]
	TIME [epoch: 8.19 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7175110559777615		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 3.7175110559777615 | validation: 4.065724412440507]
	TIME [epoch: 8.24 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719380685241545		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 3.719380685241545 | validation: 4.070292373322639]
	TIME [epoch: 8.31 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719854677362263		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 3.719854677362263 | validation: 4.076262375395988]
	TIME [epoch: 8.2 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.72303874056549		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 3.72303874056549 | validation: 4.073240912088768]
	TIME [epoch: 8.19 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7176731090953576		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 3.7176731090953576 | validation: 4.051366329658128]
	TIME [epoch: 8.19 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717798348846348		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 3.717798348846348 | validation: 4.0593261977880575]
	TIME [epoch: 8.2 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7150024026800095		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 3.7150024026800095 | validation: 4.057243177746656]
	TIME [epoch: 8.2 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715902167568291		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 3.715902167568291 | validation: 4.052405143108212]
	TIME [epoch: 8.23 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7077432759160427		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 3.7077432759160427 | validation: 4.058577971118655]
	TIME [epoch: 8.18 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7065906828525343		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 3.7065906828525343 | validation: 4.062080223282482]
	TIME [epoch: 8.27 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7092250122458577		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 3.7092250122458577 | validation: 4.055660902509368]
	TIME [epoch: 8.18 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7147670574350364		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 3.7147670574350364 | validation: 4.070218630065909]
	TIME [epoch: 8.19 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718426010597347		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 3.718426010597347 | validation: 4.064076538757298]
	TIME [epoch: 8.21 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7172282627294146		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 3.7172282627294146 | validation: 4.065292387125474]
	TIME [epoch: 8.2 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717908449263261		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 3.717908449263261 | validation: 4.063998325794652]
	TIME [epoch: 8.18 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7157980315891397		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 3.7157980315891397 | validation: 4.0644541456044525]
	TIME [epoch: 8.18 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716926067676086		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 3.716926067676086 | validation: 4.06577513491491]
	TIME [epoch: 8.2 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7203396235110753		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 3.7203396235110753 | validation: 4.054444525456199]
	TIME [epoch: 8.19 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.720843587430375		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 3.720843587430375 | validation: 4.0631760714541825]
	TIME [epoch: 8.24 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.711299725172688		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 3.711299725172688 | validation: 4.051952903835616]
	TIME [epoch: 8.19 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7101402482864745		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 3.7101402482864745 | validation: 4.059893224524856]
	TIME [epoch: 8.22 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7137135538416355		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 3.7137135538416355 | validation: 4.061846348682229]
	TIME [epoch: 8.26 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.715629858957194		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 3.715629858957194 | validation: 4.069141736261532]
	TIME [epoch: 8.23 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.71813998750374		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 3.71813998750374 | validation: 4.059136897969395]
	TIME [epoch: 8.22 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7178882563166376		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 3.7178882563166376 | validation: 4.072612141592115]
	TIME [epoch: 8.21 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7158695225300167		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 3.7158695225300167 | validation: 4.063870842095901]
	TIME [epoch: 8.19 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717331930247031		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 3.717331930247031 | validation: 4.0640871172407955]
	TIME [epoch: 8.2 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7176610255536744		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 3.7176610255536744 | validation: 4.06084010752549]
	TIME [epoch: 8.19 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716352030119822		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 3.716352030119822 | validation: 4.060023783285443]
	TIME [epoch: 8.18 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7160942303111635		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 3.7160942303111635 | validation: 4.060827132584587]
	TIME [epoch: 8.35 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719430805069849		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 3.719430805069849 | validation: 4.064062736369381]
	TIME [epoch: 8.2 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.721358143210823		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 3.721358143210823 | validation: 4.06171831788151]
	TIME [epoch: 8.18 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7189739836398363		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 3.7189739836398363 | validation: 4.071078243556146]
	TIME [epoch: 8.19 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718849415609136		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 3.718849415609136 | validation: 4.076361250798374]
	TIME [epoch: 8.19 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718579279066927		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 3.718579279066927 | validation: 4.060310735904056]
	TIME [epoch: 8.2 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7124519340212387		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 3.7124519340212387 | validation: 4.056626753722431]
	TIME [epoch: 8.23 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.713948469124834		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 3.713948469124834 | validation: 4.067226180224656]
	TIME [epoch: 8.19 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717531956524482		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 3.717531956524482 | validation: 4.070125894997468]
	TIME [epoch: 8.19 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.716265603245907		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 3.716265603245907 | validation: 4.058264379808911]
	TIME [epoch: 8.18 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7144566272294695		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 3.7144566272294695 | validation: 4.067350690612532]
	TIME [epoch: 8.18 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7178205494549146		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 3.7178205494549146 | validation: 4.05903058783915]
	TIME [epoch: 8.22 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7120427190213925		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 3.7120427190213925 | validation: 4.05425326340333]
	TIME [epoch: 8.28 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7127574429285017		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 3.7127574429285017 | validation: 4.061478931940286]
	TIME [epoch: 8.26 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7166009916800924		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 3.7166009916800924 | validation: 4.058196326488272]
	TIME [epoch: 8.2 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7115899404798833		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 3.7115899404798833 | validation: 4.070310495068443]
	TIME [epoch: 8.2 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7210691306024977		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 3.7210691306024977 | validation: 4.059755962269309]
	TIME [epoch: 8.2 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717475181072824		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 3.717475181072824 | validation: 4.073242655543132]
	TIME [epoch: 8.24 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7176332761135438		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 3.7176332761135438 | validation: 4.062820974268796]
	TIME [epoch: 8.19 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7147170685596973		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 3.7147170685596973 | validation: 4.061714284749296]
	TIME [epoch: 8.19 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7149209502044096		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 3.7149209502044096 | validation: 4.057026102680203]
	TIME [epoch: 8.19 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7144175402689106		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 3.7144175402689106 | validation: 4.0622487732279495]
	TIME [epoch: 8.18 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7123819319872133		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 3.7123819319872133 | validation: 4.057726707349325]
	TIME [epoch: 8.22 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.712576736907279		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 3.712576736907279 | validation: 4.05550501617479]
	TIME [epoch: 8.2 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.714117418851879		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 3.714117418851879 | validation: 4.0593757903956]
	TIME [epoch: 8.19 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718477678953647		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 3.718477678953647 | validation: 4.07108706025188]
	TIME [epoch: 8.19 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.722386443758621		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 3.722386443758621 | validation: 4.068641208824612]
	TIME [epoch: 8.19 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7208999531739906		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 3.7208999531739906 | validation: 4.066044375312918]
	TIME [epoch: 8.18 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.724210680959188		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 3.724210680959188 | validation: 4.065327296780364]
	TIME [epoch: 8.24 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.720488785698737		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 3.720488785698737 | validation: 4.082030540625183]
	TIME [epoch: 8.21 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7268215752267677		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 3.7268215752267677 | validation: 4.08637775519944]
	TIME [epoch: 8.19 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7318251018002253		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 3.7318251018002253 | validation: 4.07968738893987]
	TIME [epoch: 8.19 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.731632979925455		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 3.731632979925455 | validation: 4.081220341723078]
	TIME [epoch: 8.19 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7292688007009374		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 3.7292688007009374 | validation: 4.087138681277013]
	TIME [epoch: 8.19 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.734110440481638		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 3.734110440481638 | validation: 4.0950429383613445]
	TIME [epoch: 8.23 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.730091294282391		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 3.730091294282391 | validation: 4.081505715814192]
	TIME [epoch: 8.19 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.72876657659825		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 3.72876657659825 | validation: 4.083297770902455]
	TIME [epoch: 8.19 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.723737525409245		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 3.723737525409245 | validation: 4.0754379972634265]
	TIME [epoch: 8.19 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.727260811164956		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 3.727260811164956 | validation: 4.071665083480855]
	TIME [epoch: 8.2 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7265263669769366		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 3.7265263669769366 | validation: 4.060913848942008]
	TIME [epoch: 8.22 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.725488014451491		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 3.725488014451491 | validation: 4.069028150510935]
	TIME [epoch: 8.3 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7217986660564515		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 3.7217986660564515 | validation: 4.080970287255898]
	TIME [epoch: 8.22 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7309168022389825		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 3.7309168022389825 | validation: 4.083035493483964]
	TIME [epoch: 8.22 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7332255675983195		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 3.7332255675983195 | validation: 4.083291371329193]
	TIME [epoch: 8.22 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.73279695160743		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 3.73279695160743 | validation: 4.082201831868122]
	TIME [epoch: 8.21 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7304033152279783		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 3.7304033152279783 | validation: 4.078562880536565]
	TIME [epoch: 8.26 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7294157270670207		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 3.7294157270670207 | validation: 4.072712892882416]
	TIME [epoch: 8.21 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.73036018481278		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 3.73036018481278 | validation: 4.077015302435708]
	TIME [epoch: 8.21 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.724833953352384		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 3.724833953352384 | validation: 4.07161351109761]
	TIME [epoch: 8.21 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7227830557168384		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 3.7227830557168384 | validation: 4.065210585520081]
	TIME [epoch: 8.21 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.719574576716037		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 3.719574576716037 | validation: 4.060746498417188]
	TIME [epoch: 8.24 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7191115320884345		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 3.7191115320884345 | validation: 4.063079807929986]
	TIME [epoch: 8.24 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718482114915512		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 3.718482114915512 | validation: 4.067779353988968]
	TIME [epoch: 8.24 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7145876300774066		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 3.7145876300774066 | validation: 4.061501793548896]
	TIME [epoch: 8.21 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.718837109876667		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 3.718837109876667 | validation: 4.068466134979144]
	TIME [epoch: 8.21 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.72001650759006		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 3.72001650759006 | validation: 4.06367982061518]
	TIME [epoch: 8.21 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7182954813379463		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 3.7182954813379463 | validation: 4.070156045438935]
	TIME [epoch: 8.26 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.71645553559874		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 3.71645553559874 | validation: 4.061048618927321]
	TIME [epoch: 8.22 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7169345470099717		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 3.7169345470099717 | validation: 4.066392787541732]
	TIME [epoch: 8.21 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7235388883972775		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 3.7235388883972775 | validation: 4.069413119869777]
	TIME [epoch: 8.21 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.722985000759598		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 3.722985000759598 | validation: 4.077264804091177]
	TIME [epoch: 8.2 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7266124605368107		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 3.7266124605368107 | validation: 4.08134442437249]
	TIME [epoch: 8.21 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7301255503978648		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 3.7301255503978648 | validation: 4.0755907073381294]
	TIME [epoch: 8.24 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7301790359632774		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 3.7301790359632774 | validation: 4.073110747422163]
	TIME [epoch: 8.2 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.728905907362348		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 3.728905907362348 | validation: 4.06370940812647]
	TIME [epoch: 8.2 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7253582164407577		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 3.7253582164407577 | validation: 4.077176893612046]
	TIME [epoch: 8.2 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.72002604709233		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 3.72002604709233 | validation: 4.074750231427403]
	TIME [epoch: 8.2 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.726603192673615		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 3.726603192673615 | validation: 4.080165492937393]
	TIME [epoch: 8.24 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.727042502434171		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 3.727042502434171 | validation: 4.070502756236916]
	TIME [epoch: 8.22 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.722015931737128		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 3.722015931737128 | validation: 4.066863169912672]
	TIME [epoch: 8.21 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7261137616641373		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 3.7261137616641373 | validation: 4.062703695678207]
	TIME [epoch: 8.2 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7258168481374825		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 3.7258168481374825 | validation: 4.075052517685316]
	TIME [epoch: 8.2 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7238641937881543		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 3.7238641937881543 | validation: 4.069410253137555]
	TIME [epoch: 8.2 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7239162157003767		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 3.7239162157003767 | validation: 4.074491054946897]
	TIME [epoch: 8.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.725359988291974		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 3.725359988291974 | validation: 4.072122385817353]
	TIME [epoch: 8.27 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7326605920933638		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 3.7326605920933638 | validation: 4.086837637850388]
	TIME [epoch: 8.22 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.736210037255939		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 3.736210037255939 | validation: 4.085525807219473]
	TIME [epoch: 8.2 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.733836299090507		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 3.733836299090507 | validation: 4.078385069477159]
	TIME [epoch: 8.2 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7286480998939053		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 3.7286480998939053 | validation: 4.082615795722472]
	TIME [epoch: 8.21 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7312722970800882		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 3.7312722970800882 | validation: 4.091193883158342]
	TIME [epoch: 8.23 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7310181326185474		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 3.7310181326185474 | validation: 4.093101814205767]
	TIME [epoch: 8.3 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7362006864043056		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 3.7362006864043056 | validation: 4.086241631356906]
	TIME [epoch: 8.2 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.733691723480486		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 3.733691723480486 | validation: 4.084231345194385]
	TIME [epoch: 8.19 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.735400916475364		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 3.735400916475364 | validation: 4.100272697007349]
	TIME [epoch: 8.19 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74301613184585		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 3.74301613184585 | validation: 4.099040285293976]
	TIME [epoch: 8.24 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.742673772782119		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 3.742673772782119 | validation: 4.096057643845527]
	TIME [epoch: 8.2 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7467181336138835		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 3.7467181336138835 | validation: 4.102186032049145]
	TIME [epoch: 8.2 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.745283537262537		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 3.745283537262537 | validation: 4.105064182735889]
	TIME [epoch: 8.2 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.750091200393622		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 3.750091200393622 | validation: 4.117877017473958]
	TIME [epoch: 8.2 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7532500910707185		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 3.7532500910707185 | validation: 4.11423948113929]
	TIME [epoch: 8.2 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7545573101102194		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 3.7545573101102194 | validation: 4.107621809028296]
	TIME [epoch: 8.24 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7554712157247163		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 3.7554712157247163 | validation: 4.117045394784158]
	TIME [epoch: 8.19 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7574073571937867		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 3.7574073571937867 | validation: 4.115238605737917]
	TIME [epoch: 8.19 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7556595503309493		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 3.7556595503309493 | validation: 4.127022516306798]
	TIME [epoch: 8.24 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.757450826160299		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 3.757450826160299 | validation: 4.1173455467756375]
	TIME [epoch: 8.27 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7592232507701207		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 3.7592232507701207 | validation: 4.13683680004581]
	TIME [epoch: 8.26 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7616357452756892		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 3.7616357452756892 | validation: 4.127186322250746]
	TIME [epoch: 8.21 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763325437742954		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 3.763325437742954 | validation: 4.12783546402308]
	TIME [epoch: 8.2 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7629169232683273		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 3.7629169232683273 | validation: 4.117877929327491]
	TIME [epoch: 8.2 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7649453796810337		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 3.7649453796810337 | validation: 4.126263867346399]
	TIME [epoch: 8.2 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7709652051036646		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 3.7709652051036646 | validation: 4.1318533176374626]
	TIME [epoch: 8.21 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768105526961614		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 3.768105526961614 | validation: 4.134051025987283]
	TIME [epoch: 8.24 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767402292511375		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 3.767402292511375 | validation: 4.139214694243973]
	TIME [epoch: 8.19 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7655175425682623		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 3.7655175425682623 | validation: 4.13887131159328]
	TIME [epoch: 8.19 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7697114942571552		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 3.7697114942571552 | validation: 4.138085949704148]
	TIME [epoch: 8.19 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767018018302491		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 3.767018018302491 | validation: 4.134436826593642]
	TIME [epoch: 8.2 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.76962982104848		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 3.76962982104848 | validation: 4.130473871912195]
	TIME [epoch: 8.21 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7676371269046		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 3.7676371269046 | validation: 4.12682411868157]
	TIME [epoch: 8.23 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7621782545864604		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 3.7621782545864604 | validation: 4.119161433344701]
	TIME [epoch: 8.2 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7665618502383484		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 3.7665618502383484 | validation: 4.12218046203491]
	TIME [epoch: 8.19 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7640176193343144		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 3.7640176193343144 | validation: 4.121111218550238]
	TIME [epoch: 8.19 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762297032281424		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 3.762297032281424 | validation: 4.125213337488905]
	TIME [epoch: 8.2 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7603063247145236		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 3.7603063247145236 | validation: 4.1206857408792175]
	TIME [epoch: 8.24 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765748136852613		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 3.765748136852613 | validation: 4.124032644605644]
	TIME [epoch: 8.22 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7665324109452483		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 3.7665324109452483 | validation: 4.143917488546512]
	TIME [epoch: 8.19 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763715555599761		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 3.763715555599761 | validation: 4.138853326700582]
	TIME [epoch: 8.2 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768823300396104		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 3.768823300396104 | validation: 4.126686157765036]
	TIME [epoch: 8.2 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.761226839942919		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 3.761226839942919 | validation: 4.1367424153279835]
	TIME [epoch: 8.19 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765148007476917		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 3.765148007476917 | validation: 4.131499234779884]
	TIME [epoch: 8.24 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7662316829631326		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 3.7662316829631326 | validation: 4.138669302292585]
	TIME [epoch: 8.2 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7664815107804444		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 3.7664815107804444 | validation: 4.141464324233014]
	TIME [epoch: 8.2 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7721363796794205		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 3.7721363796794205 | validation: 4.144335836040172]
	TIME [epoch: 8.19 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768416231875899		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 3.768416231875899 | validation: 4.140455923099088]
	TIME [epoch: 8.2 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7716098053655527		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 3.7716098053655527 | validation: 4.145256175867324]
	TIME [epoch: 8.22 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776415657214289		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 3.776415657214289 | validation: 4.14561790578157]
	TIME [epoch: 8.21 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7791124503145204		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 3.7791124503145204 | validation: 4.162865691277555]
	TIME [epoch: 8.19 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7754971587694497		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 3.7754971587694497 | validation: 4.1440406670671415]
	TIME [epoch: 8.19 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7747142541488525		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 3.7747142541488525 | validation: 4.144992856443391]
	TIME [epoch: 8.19 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.775364140166474		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 3.775364140166474 | validation: 4.142565014149349]
	TIME [epoch: 8.19 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7692373335586185		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 3.7692373335586185 | validation: 4.141190008763516]
	TIME [epoch: 8.24 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766685704895701		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 3.766685704895701 | validation: 4.130233033084658]
	TIME [epoch: 8.19 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767982723663775		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 3.767982723663775 | validation: 4.135344101091261]
	TIME [epoch: 8.18 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.76684206230387		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 3.76684206230387 | validation: 4.1343628356228646]
	TIME [epoch: 8.18 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7657173023630617		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 3.7657173023630617 | validation: 4.134591360011958]
	TIME [epoch: 8.19 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763039143493773		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 3.763039143493773 | validation: 4.123173173468103]
	TIME [epoch: 8.2 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7629666724445965		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 3.7629666724445965 | validation: 4.127883412617068]
	TIME [epoch: 8.23 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765404180427552		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 3.765404180427552 | validation: 4.134297233101057]
	TIME [epoch: 8.19 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7636521987508536		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 3.7636521987508536 | validation: 4.134867501386109]
	TIME [epoch: 8.19 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7699359563946393		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 3.7699359563946393 | validation: 4.13720873848084]
	TIME [epoch: 8.19 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7656617923193085		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 3.7656617923193085 | validation: 4.1252847313090015]
	TIME [epoch: 8.19 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7705553984296896		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 3.7705553984296896 | validation: 4.137272058287614]
	TIME [epoch: 8.24 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7659353410992416		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 3.7659353410992416 | validation: 4.134943490183708]
	TIME [epoch: 8.21 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7705290940224767		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 3.7705290940224767 | validation: 4.143662210843884]
	TIME [epoch: 8.2 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771742550806474		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 3.771742550806474 | validation: 4.130845461531296]
	TIME [epoch: 8.18 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7668024827370825		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 3.7668024827370825 | validation: 4.1378834412201115]
	TIME [epoch: 8.19 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771079025339654		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 3.771079025339654 | validation: 4.1384155761860875]
	TIME [epoch: 8.19 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7696644233757026		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 3.7696644233757026 | validation: 4.138627140196519]
	TIME [epoch: 8.23 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7685666213990867		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 3.7685666213990867 | validation: 4.134453182053633]
	TIME [epoch: 8.19 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767596748466607		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 3.767596748466607 | validation: 4.136542064332971]
	TIME [epoch: 8.19 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7714672178337523		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 3.7714672178337523 | validation: 4.148506952879685]
	TIME [epoch: 8.19 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.773271431039175		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 3.773271431039175 | validation: 4.1391295075008205]
	TIME [epoch: 8.19 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7696318864914002		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 3.7696318864914002 | validation: 4.141676137832212]
	TIME [epoch: 8.21 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7699665365930546		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 3.7699665365930546 | validation: 4.141034484910792]
	TIME [epoch: 8.21 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774491356337712		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 3.774491356337712 | validation: 4.149690216558499]
	TIME [epoch: 8.18 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779257713112376		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 3.779257713112376 | validation: 4.156525049296253]
	TIME [epoch: 8.2 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777281972385599		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 3.777281972385599 | validation: 4.147929641847114]
	TIME [epoch: 8.19 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7778725554563652		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 3.7778725554563652 | validation: 4.14123943597774]
	TIME [epoch: 8.2 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.772342951784923		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 3.772342951784923 | validation: 4.1373093314365015]
	TIME [epoch: 8.23 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771725686835266		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 3.771725686835266 | validation: 4.13610424772755]
	TIME [epoch: 8.19 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7654724290381476		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 3.7654724290381476 | validation: 4.131173665849755]
	TIME [epoch: 8.19 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771078431889405		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 3.771078431889405 | validation: 4.1323998010599485]
	TIME [epoch: 8.19 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7662246513459765		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 3.7662246513459765 | validation: 4.141841928595843]
	TIME [epoch: 8.19 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7665304731520175		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 3.7665304731520175 | validation: 4.144026537943018]
	TIME [epoch: 8.2 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7642541825795828		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 3.7642541825795828 | validation: 4.127287562545377]
	TIME [epoch: 8.23 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766715692400859		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 3.766715692400859 | validation: 4.134746513452821]
	TIME [epoch: 8.19 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7668411339798133		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 3.7668411339798133 | validation: 4.121911662872415]
	TIME [epoch: 8.18 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763082800625576		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 3.763082800625576 | validation: 4.129078397166062]
	TIME [epoch: 8.19 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.764044811942793		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 3.764044811942793 | validation: 4.123890919219009]
	TIME [epoch: 8.19 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7676833472235187		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 3.7676833472235187 | validation: 4.132879896672355]
	TIME [epoch: 8.23 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7637430217655305		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 3.7637430217655305 | validation: 4.132119899962877]
	TIME [epoch: 8.21 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7629831273717755		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 3.7629831273717755 | validation: 4.1372110114116865]
	TIME [epoch: 8.2 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.761904094561103		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 3.761904094561103 | validation: 4.131808469914518]
	TIME [epoch: 8.19 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7611491911492694		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 3.7611491911492694 | validation: 4.124513487958094]
	TIME [epoch: 8.19 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7593031229189773		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 3.7593031229189773 | validation: 4.122571662400956]
	TIME [epoch: 8.19 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.759108220501751		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 3.759108220501751 | validation: 4.11291520958107]
	TIME [epoch: 8.24 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7589759656784585		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 3.7589759656784585 | validation: 4.119818111832046]
	TIME [epoch: 8.19 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762070704569399		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 3.762070704569399 | validation: 4.119372683096954]
	TIME [epoch: 8.19 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.762973359329986		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 3.762973359329986 | validation: 4.117970512442287]
	TIME [epoch: 8.19 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.758864017635394		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 3.758864017635394 | validation: 4.122800492575141]
	TIME [epoch: 8.19 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7599348895011167		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 3.7599348895011167 | validation: 4.12720657910185]
	TIME [epoch: 8.19 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763393506744727		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 3.763393506744727 | validation: 4.119371148981751]
	TIME [epoch: 8.24 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7626885590018615		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 3.7626885590018615 | validation: 4.123851830322042]
	TIME [epoch: 8.19 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7649795902657823		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 3.7649795902657823 | validation: 4.126906467073596]
	TIME [epoch: 8.2 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.766136589509594		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 3.766136589509594 | validation: 4.1241836030604295]
	TIME [epoch: 8.22 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.763730295390973		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 3.763730295390973 | validation: 4.132515036795347]
	TIME [epoch: 8.2 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7705071442407094		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 3.7705071442407094 | validation: 4.1280004165043405]
	TIME [epoch: 8.23 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7684879471568826		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 3.7684879471568826 | validation: 4.123209516482321]
	TIME [epoch: 8.2 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7655092120308127		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 3.7655092120308127 | validation: 4.1302456569803745]
	TIME [epoch: 8.19 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.76533917309428		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 3.76533917309428 | validation: 4.1264421936415365]
	TIME [epoch: 8.5 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7653887703242663		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 3.7653887703242663 | validation: 4.1271995584800205]
	TIME [epoch: 8.2 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.768865873677054		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 3.768865873677054 | validation: 4.142658551370221]
	TIME [epoch: 8.21 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.767082451274921		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 3.767082451274921 | validation: 4.139364463037504]
	TIME [epoch: 8.26 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7722668718201016		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 3.7722668718201016 | validation: 4.144515196246135]
	TIME [epoch: 8.21 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.77252117790463		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 3.77252117790463 | validation: 4.1389244480743805]
	TIME [epoch: 8.21 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7694929106119557		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 3.7694929106119557 | validation: 4.140680266745839]
	TIME [epoch: 8.2 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.773659982109064		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 3.773659982109064 | validation: 4.138632638469373]
	TIME [epoch: 8.21 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7735730055292014		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 3.7735730055292014 | validation: 4.141780516230611]
	TIME [epoch: 8.24 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.773982413767903		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 3.773982413767903 | validation: 4.143031474771204]
	TIME [epoch: 8.23 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7750479007543785		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 3.7750479007543785 | validation: 4.146029608676854]
	TIME [epoch: 8.21 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776295737567027		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 3.776295737567027 | validation: 4.144913266458711]
	TIME [epoch: 8.22 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7764153487604033		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 3.7764153487604033 | validation: 4.151566830570111]
	TIME [epoch: 8.2 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7746689258273847		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 3.7746689258273847 | validation: 4.149949709343533]
	TIME [epoch: 8.21 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7798155434950327		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 3.7798155434950327 | validation: 4.151212156263659]
	TIME [epoch: 8.25 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7814605601571616		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 3.7814605601571616 | validation: 4.159848451392328]
	TIME [epoch: 8.2 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7843661228765626		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 3.7843661228765626 | validation: 4.149678068583421]
	TIME [epoch: 8.2 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780892631047501		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 3.780892631047501 | validation: 4.153221101717979]
	TIME [epoch: 8.19 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.788241945823471		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 3.788241945823471 | validation: 4.151567993556127]
	TIME [epoch: 8.2 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7835928805769443		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 3.7835928805769443 | validation: 4.165927940267808]
	TIME [epoch: 8.22 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.786093955744941		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 3.786093955744941 | validation: 4.157552848398852]
	TIME [epoch: 8.23 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7864942619073925		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 3.7864942619073925 | validation: 4.164594046597616]
	TIME [epoch: 8.2 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.790709897205789		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 3.790709897205789 | validation: 4.1559027382615135]
	TIME [epoch: 8.21 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.789274962709052		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 3.789274962709052 | validation: 4.164259081250086]
	TIME [epoch: 8.2 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7910275289942295		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 3.7910275289942295 | validation: 4.165734266876069]
	TIME [epoch: 8.2 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7883208242130024		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 3.7883208242130024 | validation: 4.165750007064382]
	TIME [epoch: 8.24 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.798206869690258		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 3.798206869690258 | validation: 4.160989437309308]
	TIME [epoch: 8.22 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.788913840455887		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 3.788913840455887 | validation: 4.1624331504444445]
	TIME [epoch: 8.2 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.790520126604292		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 3.790520126604292 | validation: 4.162659906929539]
	TIME [epoch: 8.2 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7923514777047203		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 3.7923514777047203 | validation: 4.161446302695381]
	TIME [epoch: 8.2 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7940623414757915		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 3.7940623414757915 | validation: 4.15448349892542]
	TIME [epoch: 8.2 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.793256863569078		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 3.793256863569078 | validation: 4.170898460297224]
	TIME [epoch: 8.24 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.791492615633002		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 3.791492615633002 | validation: 4.159237446523662]
	TIME [epoch: 8.2 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783757764862898		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 3.783757764862898 | validation: 4.161635134328866]
	TIME [epoch: 8.19 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.784189688224551		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 3.784189688224551 | validation: 4.152917788210043]
	TIME [epoch: 8.2 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783994518437216		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 3.783994518437216 | validation: 4.151743196059703]
	TIME [epoch: 8.19 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7868004911721544		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 3.7868004911721544 | validation: 4.152788503544567]
	TIME [epoch: 8.22 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7861498717465154		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 3.7861498717465154 | validation: 4.163006794690224]
	TIME [epoch: 8.21 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.784419501162159		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 3.784419501162159 | validation: 4.148199603082883]
	TIME [epoch: 8.2 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7814742313139336		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 3.7814742313139336 | validation: 4.1386366802799195]
	TIME [epoch: 8.2 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7793512900376514		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 3.7793512900376514 | validation: 4.152500084071368]
	TIME [epoch: 8.21 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7797190926703617		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 3.7797190926703617 | validation: 4.149764157561069]
	TIME [epoch: 8.21 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78298990378322		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 3.78298990378322 | validation: 4.143228315879586]
	TIME [epoch: 8.23 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7773114487193236		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 3.7773114487193236 | validation: 4.144647597150097]
	TIME [epoch: 8.2 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782109709129529		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 3.782109709129529 | validation: 4.157237495727578]
	TIME [epoch: 8.22 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7816291949648573		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 3.7816291949648573 | validation: 4.1489536641876725]
	TIME [epoch: 8.21 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7828654491205294		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 3.7828654491205294 | validation: 4.153649977451593]
	TIME [epoch: 8.2 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782521583251878		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 3.782521583251878 | validation: 4.145002766098779]
	TIME [epoch: 8.21 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782755557679673		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 3.782755557679673 | validation: 4.139747068017481]
	TIME [epoch: 8.24 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778048987163668		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 3.778048987163668 | validation: 4.136387819898661]
	TIME [epoch: 8.2 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780637319089619		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 3.780637319089619 | validation: 4.137019677879463]
	TIME [epoch: 8.2 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78068617653801		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 3.78068617653801 | validation: 4.142656924562186]
	TIME [epoch: 8.19 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7786094775449173		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 3.7786094775449173 | validation: 4.149835131460351]
	TIME [epoch: 8.2 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7794540989088015		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 3.7794540989088015 | validation: 4.150035802056202]
	TIME [epoch: 8.24 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7816207880617965		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 3.7816207880617965 | validation: 4.149025662172375]
	TIME [epoch: 8.2 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7778575478224616		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 3.7778575478224616 | validation: 4.135290736710915]
	TIME [epoch: 8.18 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776901486582363		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 3.776901486582363 | validation: 4.143839207187243]
	TIME [epoch: 8.18 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7809479012267895		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 3.7809479012267895 | validation: 4.1552029859476765]
	TIME [epoch: 8.19 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7807822777643927		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 3.7807822777643927 | validation: 4.152131245596504]
	TIME [epoch: 8.2 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7791459352188994		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 3.7791459352188994 | validation: 4.143884049561583]
	TIME [epoch: 8.24 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7746073956964996		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 3.7746073956964996 | validation: 4.147930809118083]
	TIME [epoch: 8.21 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7779627166936365		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 3.7779627166936365 | validation: 4.138665065183288]
	TIME [epoch: 8.2 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782025513241085		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 3.782025513241085 | validation: 4.143092795454019]
	TIME [epoch: 8.2 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778551500902406		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 3.778551500902406 | validation: 4.151228312715766]
	TIME [epoch: 8.19 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7835273286247806		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 3.7835273286247806 | validation: 4.147221044169924]
	TIME [epoch: 8.22 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779369838352794		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 3.779369838352794 | validation: 4.148207334923623]
	TIME [epoch: 8.21 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7810219412011863		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 3.7810219412011863 | validation: 4.1537604475574526]
	TIME [epoch: 8.19 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7796475374365417		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 3.7796475374365417 | validation: 4.154310202286958]
	TIME [epoch: 8.19 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7831557155454183		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 3.7831557155454183 | validation: 4.1397936842925995]
	TIME [epoch: 8.2 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7778836815974475		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 3.7778836815974475 | validation: 4.143998704509873]
	TIME [epoch: 8.2 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78056868729756		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 3.78056868729756 | validation: 4.142348169226494]
	TIME [epoch: 8.25 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7785388059842124		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 3.7785388059842124 | validation: 4.1567215991710835]
	TIME [epoch: 8.2 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7791389904901624		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 3.7791389904901624 | validation: 4.144843110277646]
	TIME [epoch: 8.19 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.781208176591361		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 3.781208176591361 | validation: 4.145238062762694]
	TIME [epoch: 8.18 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780053082636627		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 3.780053082636627 | validation: 4.149018433200992]
	TIME [epoch: 8.18 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7784398519147646		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 3.7784398519147646 | validation: 4.143409013076987]
	TIME [epoch: 8.2 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7812196364159143		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 3.7812196364159143 | validation: 4.157846076442964]
	TIME [epoch: 8.21 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7810934630166293		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 3.7810934630166293 | validation: 4.149227485162768]
	TIME [epoch: 8.17 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7837606468577785		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 3.7837606468577785 | validation: 4.152800864837099]
	TIME [epoch: 8.18 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776907492055635		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 3.776907492055635 | validation: 4.1438571989792266]
	TIME [epoch: 8.2 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7833507501525943		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 3.7833507501525943 | validation: 4.148325337726371]
	TIME [epoch: 8.2 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7773643558369816		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 3.7773643558369816 | validation: 4.145335263079266]
	TIME [epoch: 8.23 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7810894947224734		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 3.7810894947224734 | validation: 4.147861394280417]
	TIME [epoch: 8.2 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.778638309126009		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 3.778638309126009 | validation: 4.151770795604181]
	TIME [epoch: 8.19 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.785870346576174		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 3.785870346576174 | validation: 4.154369596770845]
	TIME [epoch: 8.19 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782039060901318		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 3.782039060901318 | validation: 4.155723183405598]
	TIME [epoch: 8.19 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7825925227982515		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 3.7825925227982515 | validation: 4.141625534066767]
	TIME [epoch: 8.19 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7826299847156233		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 3.7826299847156233 | validation: 4.1550796664971745]
	TIME [epoch: 8.24 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7836451944301714		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 3.7836451944301714 | validation: 4.146810112520832]
	TIME [epoch: 8.19 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.78527415425962		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 3.78527415425962 | validation: 4.149186158769385]
	TIME [epoch: 8.19 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783646323932985		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 3.783646323932985 | validation: 4.145343706354706]
	TIME [epoch: 8.19 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7840877665690154		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 3.7840877665690154 | validation: 4.1474813821360685]
	TIME [epoch: 8.19 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7810907648395657		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 3.7810907648395657 | validation: 4.147581010461709]
	TIME [epoch: 8.23 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.783074345202658		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 3.783074345202658 | validation: 4.143604384665007]
	TIME [epoch: 8.2 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780190694047115		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 3.780190694047115 | validation: 4.153082238399541]
	TIME [epoch: 8.19 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7864996424649546		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 3.7864996424649546 | validation: 4.1439733090783655]
	TIME [epoch: 8.19 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7782407734614845		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 3.7782407734614845 | validation: 4.145878216812655]
	TIME [epoch: 8.19 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7764487130010966		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 3.7764487130010966 | validation: 4.142905469660761]
	TIME [epoch: 8.19 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779031066748014		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 3.779031066748014 | validation: 4.1408214075822185]
	TIME [epoch: 8.23 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777919501606572		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 3.777919501606572 | validation: 4.1459310603734245]
	TIME [epoch: 8.19 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7789572963458014		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 3.7789572963458014 | validation: 4.1553498678807355]
	TIME [epoch: 8.18 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7790417971842003		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 3.7790417971842003 | validation: 4.149437083971372]
	TIME [epoch: 8.18 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7776304312325184		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 3.7776304312325184 | validation: 4.14779509087427]
	TIME [epoch: 8.18 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7755670236987875		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 3.7755670236987875 | validation: 4.149623283358363]
	TIME [epoch: 8.19 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7819169894111457		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 3.7819169894111457 | validation: 4.149616350568961]
	TIME [epoch: 8.23 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7823396570327645		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 3.7823396570327645 | validation: 4.143557300271116]
	TIME [epoch: 8.19 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7813033319350637		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 3.7813033319350637 | validation: 4.150758577198658]
	TIME [epoch: 8.19 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7815168863743		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 3.7815168863743 | validation: 4.1595341932943715]
	TIME [epoch: 8.18 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.77997014620403		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 3.77997014620403 | validation: 4.149870862281929]
	TIME [epoch: 8.19 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7783206328986614		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 3.7783206328986614 | validation: 4.145850959859073]
	TIME [epoch: 8.21 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.781517590640729		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 3.781517590640729 | validation: 4.146264763429365]
	TIME [epoch: 8.21 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776502337242621		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 3.776502337242621 | validation: 4.147949374478962]
	TIME [epoch: 8.2 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780330281999006		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 3.780330281999006 | validation: 4.14439134176733]
	TIME [epoch: 8.19 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7791034558408105		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 3.7791034558408105 | validation: 4.146076094096162]
	TIME [epoch: 8.19 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.779580969927853		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 3.779580969927853 | validation: 4.14336697190465]
	TIME [epoch: 8.2 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7786899914267194		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 3.7786899914267194 | validation: 4.148369354696284]
	TIME [epoch: 8.23 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.775324509268193		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 3.775324509268193 | validation: 4.143796568646062]
	TIME [epoch: 8.19 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.780112410498911		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 3.780112410498911 | validation: 4.151015900620409]
	TIME [epoch: 8.18 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777572719720272		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 3.777572719720272 | validation: 4.138161051784568]
	TIME [epoch: 8.2 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776221128750717		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 3.776221128750717 | validation: 4.152247185155165]
	TIME [epoch: 8.18 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7774983294406814		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 3.7774983294406814 | validation: 4.1361369558719385]
	TIME [epoch: 8.2 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.77791136448504		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 3.77791136448504 | validation: 4.1398019522238005]
	TIME [epoch: 8.22 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7743846361504434		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 3.7743846361504434 | validation: 4.144477412756136]
	TIME [epoch: 8.19 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7804149872477106		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 3.7804149872477106 | validation: 4.130939105147687]
	TIME [epoch: 8.19 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774490794653067		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 3.774490794653067 | validation: 4.136884419366779]
	TIME [epoch: 8.19 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7788392673785722		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 3.7788392673785722 | validation: 4.133724778759253]
	TIME [epoch: 8.18 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7784015082220392		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 3.7784015082220392 | validation: 4.139378723912871]
	TIME [epoch: 8.23 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774860472522541		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 3.774860472522541 | validation: 4.134260757232502]
	TIME [epoch: 8.2 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7761566131903157		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 3.7761566131903157 | validation: 4.136684740085135]
	TIME [epoch: 8.19 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.776818252212859		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 3.776818252212859 | validation: 4.140344167310351]
	TIME [epoch: 8.18 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7742289539880156		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 3.7742289539880156 | validation: 4.133265976571103]
	TIME [epoch: 8.18 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7739526219211124		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 3.7739526219211124 | validation: 4.138032830492374]
	TIME [epoch: 8.19 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7758894930322975		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 3.7758894930322975 | validation: 4.147336964191007]
	TIME [epoch: 8.23 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.770857066252498		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 3.770857066252498 | validation: 4.14142022184847]
	TIME [epoch: 8.19 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774378012735327		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 3.774378012735327 | validation: 4.129418492769818]
	TIME [epoch: 8.18 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7741011494455114		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 3.7741011494455114 | validation: 4.153190265848401]
	TIME [epoch: 8.19 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.774814331097973		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 3.774814331097973 | validation: 4.135647707643036]
	TIME [epoch: 8.18 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7704089718759333		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 3.7704089718759333 | validation: 4.133972303750408]
	TIME [epoch: 8.22 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7695107333459887		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 3.7695107333459887 | validation: 4.145849398043585]
	TIME [epoch: 8.21 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7748117692583265		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 3.7748117692583265 | validation: 4.129402870441044]
	TIME [epoch: 8.19 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7729246361389834		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 3.7729246361389834 | validation: 4.138342773670187]
	TIME [epoch: 8.19 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.772058764018123		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 3.772058764018123 | validation: 4.133692227287039]
	TIME [epoch: 8.2 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7745795643647435		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 3.7745795643647435 | validation: 4.138618173600719]
	TIME [epoch: 8.19 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7734465420394425		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 3.7734465420394425 | validation: 4.136148524105041]
	TIME [epoch: 8.23 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.769399858295978		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 3.769399858295978 | validation: 4.1359547601476025]
	TIME [epoch: 8.19 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.773514415046637		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 3.773514415046637 | validation: 4.139071207931144]
	TIME [epoch: 8.18 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7693500568192255		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 3.7693500568192255 | validation: 4.135006734169609]
	TIME [epoch: 8.19 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7705866437953643		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 3.7705866437953643 | validation: 4.1331161486618795]
	TIME [epoch: 8.19 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771736910898986		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 3.771736910898986 | validation: 4.121839798566048]
	TIME [epoch: 8.2 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7711808131416866		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 3.7711808131416866 | validation: 4.138164486432787]
	TIME [epoch: 8.23 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.772402147141396		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 3.772402147141396 | validation: 4.133651746295172]
	TIME [epoch: 8.21 sec]
Finished training in 17395.984 seconds.
