Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise_large', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise_large', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.5, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3958157085

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.475044097933838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.475044097933838 | validation: 3.1492019764511205]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8212681386138465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8212681386138465 | validation: 2.902781118299224]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6022563234194886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6022563234194886 | validation: 2.839151220691693]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4160708405146982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4160708405146982 | validation: 2.794556743720281]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3904415599889277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3904415599889277 | validation: 2.818653125365378]
	TIME [epoch: 7.7 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.286601039976279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.286601039976279 | validation: 2.7577349597730136]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2590817153281026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2590817153281026 | validation: 2.791577107003593]
	TIME [epoch: 7.74 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.241105451414657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.241105451414657 | validation: 2.816419855296101]
	TIME [epoch: 7.73 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.26880257864255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.26880257864255 | validation: 2.681436995291547]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1788008233378626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1788008233378626 | validation: 2.6987765881862633]
	TIME [epoch: 7.74 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.11039518468779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.11039518468779 | validation: 2.692443801335264]
	TIME [epoch: 7.74 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.198907015238751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.198907015238751 | validation: 2.7355160548537825]
	TIME [epoch: 7.76 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1692467389037007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1692467389037007 | validation: 2.6098023186632613]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0910152653814915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0910152653814915 | validation: 2.599232494579116]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.047311216658665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047311216658665 | validation: 2.6005234384354967]
	TIME [epoch: 7.74 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.04745911681366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.04745911681366 | validation: 2.5922904150689243]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.169513332259015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.169513332259015 | validation: 2.646628469594303]
	TIME [epoch: 7.77 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.054647431828639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.054647431828639 | validation: 2.513398988132676]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9494267320058898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9494267320058898 | validation: 2.4963527899029136]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.034751749592892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.034751749592892 | validation: 2.4984016990360054]
	TIME [epoch: 7.76 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.964894975759756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.964894975759756 | validation: 2.4574829318147824]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.928412366510405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.928412366510405 | validation: 2.4376660741835767]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8826474482954172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8826474482954172 | validation: 2.429256950484134]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.835003505802046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.835003505802046 | validation: 2.5694760499702465]
	TIME [epoch: 7.72 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8901135628224444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8901135628224444 | validation: 2.338997502696656]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7652330392744022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7652330392744022 | validation: 2.4672501164787954]
	TIME [epoch: 7.76 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884757741117796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.884757741117796 | validation: 2.3227745491340555]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7993910373749407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7993910373749407 | validation: 2.3881949127586743]
	TIME [epoch: 7.75 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6770256070154221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6770256070154221 | validation: 2.4406839951078316]
	TIME [epoch: 7.72 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7632084195968414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7632084195968414 | validation: 2.258611530937527]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5977063029783933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5977063029783933 | validation: 2.294084827914177]
	TIME [epoch: 7.76 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.664011625564403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.664011625564403 | validation: 2.5491547197372455]
	TIME [epoch: 7.73 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7820519577166167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7820519577166167 | validation: 2.247696023218812]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5936441263364394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5936441263364394 | validation: 2.249323852834925]
	TIME [epoch: 7.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5710090253719837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5710090253719837 | validation: 2.195094401220245]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4980319308690482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4980319308690482 | validation: 2.3124751545366093]
	TIME [epoch: 7.77 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.664009399721513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.664009399721513 | validation: 2.3397074461987537]
	TIME [epoch: 7.76 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5736888440657648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5736888440657648 | validation: 2.299084722694513]
	TIME [epoch: 7.74 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6794818687943944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6794818687943944 | validation: 2.299999702852487]
	TIME [epoch: 7.75 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5700607884237123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5700607884237123 | validation: 2.118117946024606]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4467425233558784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4467425233558784 | validation: 2.1401844012171356]
	TIME [epoch: 7.8 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5660386261798376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5660386261798376 | validation: 2.4798542491081967]
	TIME [epoch: 7.75 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.512908515324032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.512908515324032 | validation: 2.116012719792537]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4910917140997562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4910917140997562 | validation: 2.292193595497304]
	TIME [epoch: 7.67 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5596741056200059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5596741056200059 | validation: 2.084059135448927]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5436189666971836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5436189666971836 | validation: 2.2425926678905483]
	TIME [epoch: 7.75 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5644910547785693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5644910547785693 | validation: 2.1155931679041484]
	TIME [epoch: 7.71 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4322789344872928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4322789344872928 | validation: 2.105243929120754]
	TIME [epoch: 7.72 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4084182454684848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4084182454684848 | validation: 2.254419093111412]
	TIME [epoch: 7.69 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5097867099639193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5097867099639193 | validation: 2.071385758773009]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4631342142594104		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.4631342142594104 | validation: 2.5925966826856808]
	TIME [epoch: 7.75 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7485594459631586		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.7485594459631586 | validation: 2.1190662274119676]
	TIME [epoch: 7.7 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3748117294294708		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.3748117294294708 | validation: 2.1065629126623233]
	TIME [epoch: 7.69 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3708516147075194		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.3708516147075194 | validation: 2.2676865698908326]
	TIME [epoch: 7.7 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4911686902809465		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.4911686902809465 | validation: 2.1229750261248013]
	TIME [epoch: 7.7 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3846916715514075		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.3846916715514075 | validation: 2.201383224262107]
	TIME [epoch: 7.74 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6323434843842568		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.6323434843842568 | validation: 2.2911646044684195]
	TIME [epoch: 7.7 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.463022289802836		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.463022289802836 | validation: 2.1168924475042195]
	TIME [epoch: 7.69 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3671755094149252		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.3671755094149252 | validation: 2.075329687715861]
	TIME [epoch: 7.7 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3391495348297846		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.3391495348297846 | validation: 2.2736944964581163]
	TIME [epoch: 7.71 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4541742566694473		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.4541742566694473 | validation: 2.059473967852476]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3639624392197724		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.3639624392197724 | validation: 2.0724487749774205]
	TIME [epoch: 7.72 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.375190047350236		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.375190047350236 | validation: 2.16929762061633]
	TIME [epoch: 7.72 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3846928892696089		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.3846928892696089 | validation: 2.1273616461031173]
	TIME [epoch: 7.72 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3663901402375112		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.3663901402375112 | validation: 2.1815155942708886]
	TIME [epoch: 7.73 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.427508378457886		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 1.427508378457886 | validation: 2.188429832064661]
	TIME [epoch: 7.78 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3530400150028947		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.3530400150028947 | validation: 2.072833994056829]
	TIME [epoch: 7.72 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.689544095776453		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 2.689544095776453 | validation: 2.4261357063307605]
	TIME [epoch: 7.71 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6408167193835466		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.6408167193835466 | validation: 2.241475440836571]
	TIME [epoch: 7.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4448020836177338		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 1.4448020836177338 | validation: 2.1826904030317165]
	TIME [epoch: 7.73 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3919492290551099		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 1.3919492290551099 | validation: 2.1423468791672304]
	TIME [epoch: 7.76 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3755695082385926		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 1.3755695082385926 | validation: 2.0687476572522474]
	TIME [epoch: 7.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3357503867112615		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.3357503867112615 | validation: 2.079616213075876]
	TIME [epoch: 7.72 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3272415682546368		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 1.3272415682546368 | validation: 2.062503922813449]
	TIME [epoch: 7.72 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4502712335866352		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 1.4502712335866352 | validation: 2.1333066889666137]
	TIME [epoch: 7.75 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4425101567920675		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 1.4425101567920675 | validation: 2.1586330178116455]
	TIME [epoch: 7.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3452258567237674		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 1.3452258567237674 | validation: 2.134152082388354]
	TIME [epoch: 7.72 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3500514950269877		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 1.3500514950269877 | validation: 2.1465455278467385]
	TIME [epoch: 7.72 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4466406884434775		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.4466406884434775 | validation: 2.114669976053797]
	TIME [epoch: 7.71 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3377913403302266		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 1.3377913403302266 | validation: 3.1573666990097777]
	TIME [epoch: 7.71 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6181136334872837		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 1.6181136334872837 | validation: 2.086731270022389]
	TIME [epoch: 7.73 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3315057815369054		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 1.3315057815369054 | validation: 2.0714498596160817]
	TIME [epoch: 7.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3096947473504108		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 1.3096947473504108 | validation: 2.0853098205970975]
	TIME [epoch: 7.69 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3398543084735475		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 1.3398543084735475 | validation: 2.072413009927371]
	TIME [epoch: 7.7 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.299090833572369		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 1.299090833572369 | validation: 2.094315498795523]
	TIME [epoch: 7.72 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.319184120959325		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 1.319184120959325 | validation: 2.1958121847297063]
	TIME [epoch: 7.74 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4145116010777183		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 1.4145116010777183 | validation: 2.0599500745836243]
	TIME [epoch: 7.71 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2863998205780196		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 1.2863998205780196 | validation: 2.108674144874241]
	TIME [epoch: 7.71 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3302194939768488		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 1.3302194939768488 | validation: 2.1380176753077267]
	TIME [epoch: 7.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5175095532156264		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 1.5175095532156264 | validation: 2.245733560553891]
	TIME [epoch: 7.72 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4804893262586019		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 1.4804893262586019 | validation: 2.0736506433164195]
	TIME [epoch: 7.73 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3008742026343516		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 1.3008742026343516 | validation: 2.01169790419805]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.27544672035364		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 1.27544672035364 | validation: 3.215458916924332]
	TIME [epoch: 7.74 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9050696362990571		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 1.9050696362990571 | validation: 2.0000005737138067]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2603550962347059		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 1.2603550962347059 | validation: 1.4685478972785249]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9831415562688779		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.9831415562688779 | validation: 1.2545473402788798]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7421071946714424		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.7421071946714424 | validation: 0.9200920062250498]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5893719766839796		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5893719766839796 | validation: 1.2231272062764993]
	TIME [epoch: 7.7 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8283452953511201		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.8283452953511201 | validation: 0.7296027040933618]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5151729271171133		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.5151729271171133 | validation: 0.6349061103148697]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5612582393589935		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.5612582393589935 | validation: 0.6585263283747076]
	TIME [epoch: 7.74 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4813196269823941		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4813196269823941 | validation: 0.5700734374296471]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4301576979257008		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.4301576979257008 | validation: 0.6226147253683998]
	TIME [epoch: 7.75 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8364358073051659		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.8364358073051659 | validation: 1.0241541610568232]
	TIME [epoch: 7.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7032976942503958		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.7032976942503958 | validation: 0.7456058227726083]
	TIME [epoch: 7.79 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5643016483391188		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.5643016483391188 | validation: 0.6607228987505969]
	TIME [epoch: 7.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49292230545096827		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.49292230545096827 | validation: 0.5918317869157901]
	TIME [epoch: 7.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4470234934108087		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.4470234934108087 | validation: 0.5013214958259298]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4050291391480563		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.4050291391480563 | validation: 0.4666838672291978]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43830944172339165		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.43830944172339165 | validation: 0.4956367612335534]
	TIME [epoch: 7.8 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5418847111613365		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5418847111613365 | validation: 0.563409871104811]
	TIME [epoch: 7.76 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46280336421205226		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.46280336421205226 | validation: 0.5105525977411766]
	TIME [epoch: 7.76 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39246262711742447		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.39246262711742447 | validation: 0.6480407673015007]
	TIME [epoch: 7.76 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41978541386438245		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.41978541386438245 | validation: 0.38641344271036937]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8012302441743802		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 1.8012302441743802 | validation: 2.8954111624396757]
	TIME [epoch: 7.78 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9995879879041851		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 1.9995879879041851 | validation: 1.204221533909633]
	TIME [epoch: 7.76 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7291593242733692		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.7291593242733692 | validation: 0.7271635824976161]
	TIME [epoch: 7.76 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5236757034114643		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.5236757034114643 | validation: 0.6356293603974178]
	TIME [epoch: 7.75 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44548887859691993		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.44548887859691993 | validation: 0.5421407399633994]
	TIME [epoch: 7.78 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38129360050803734		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.38129360050803734 | validation: 0.49057455620788193]
	TIME [epoch: 7.78 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44494401533025507		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.44494401533025507 | validation: 0.509137553766597]
	TIME [epoch: 7.76 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.425849461409281		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.425849461409281 | validation: 0.4699030227656942]
	TIME [epoch: 7.76 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4544186932823968		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.4544186932823968 | validation: 0.5486783618805554]
	TIME [epoch: 7.75 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37216657063658326		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.37216657063658326 | validation: 0.4000338092220798]
	TIME [epoch: 7.8 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8650853164208014		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.8650853164208014 | validation: 0.5447132392587812]
	TIME [epoch: 7.78 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3809637245720092		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.3809637245720092 | validation: 0.3991611099332498]
	TIME [epoch: 7.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33750644946531205		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.33750644946531205 | validation: 0.3952746343158554]
	TIME [epoch: 7.76 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37273595313594887		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.37273595313594887 | validation: 0.3550143469655787]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.524500817386178		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.524500817386178 | validation: 0.4183941092003309]
	TIME [epoch: 7.82 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5756576518123442		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 1.5756576518123442 | validation: 2.513705045517221]
	TIME [epoch: 7.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5618421415680308		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 1.5618421415680308 | validation: 0.5973684594810392]
	TIME [epoch: 7.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44399842275773344		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.44399842275773344 | validation: 0.43135471863122476]
	TIME [epoch: 7.76 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33319120777438677		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.33319120777438677 | validation: 0.3621373768223123]
	TIME [epoch: 7.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28730393043671926		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.28730393043671926 | validation: 0.41284924292552366]
	TIME [epoch: 7.81 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3230661923044087		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3230661923044087 | validation: 0.3755530206699994]
	TIME [epoch: 7.76 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41327715537693044		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.41327715537693044 | validation: 0.4769317708699204]
	TIME [epoch: 7.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5122796752058646		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.5122796752058646 | validation: 0.42093716699063377]
	TIME [epoch: 7.76 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829946937496325		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.2829946937496325 | validation: 0.30322174167683785]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24877731969947858		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.24877731969947858 | validation: 0.3008242648773768]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26837822997985183		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.26837822997985183 | validation: 0.32390200528353025]
	TIME [epoch: 7.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36187607472123473		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.36187607472123473 | validation: 0.3062655087693863]
	TIME [epoch: 7.72 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2935115188253662		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.2935115188253662 | validation: 0.2911467429331791]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268120261159822		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.268120261159822 | validation: 0.6395840786464173]
	TIME [epoch: 7.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36561826866535163		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.36561826866535163 | validation: 0.6953756828038894]
	TIME [epoch: 7.75 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6478427558645252		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.6478427558645252 | validation: 0.4186797947039879]
	TIME [epoch: 7.71 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30924737958960435		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.30924737958960435 | validation: 0.26725585289724935]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22486481568062378		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.22486481568062378 | validation: 0.3255648641470488]
	TIME [epoch: 7.74 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2602454463388458		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.2602454463388458 | validation: 0.2517140654732742]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20985258296306453		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.20985258296306453 | validation: 0.22126281083192129]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22963984134950494		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.22963984134950494 | validation: 0.29587526682916326]
	TIME [epoch: 7.75 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2992199736611526		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2992199736611526 | validation: 0.23831903693134104]
	TIME [epoch: 7.75 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8447198080667928		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.8447198080667928 | validation: 0.8734898666493931]
	TIME [epoch: 7.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5767325777109923		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.5767325777109923 | validation: 1.0993463645509043]
	TIME [epoch: 7.78 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8443341497476371		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.8443341497476371 | validation: 0.8707526072850391]
	TIME [epoch: 7.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5256639799464636		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.5256639799464636 | validation: 0.3821573827732286]
	TIME [epoch: 7.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26604422188932875		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.26604422188932875 | validation: 0.750139147220021]
	TIME [epoch: 7.76 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4235558770601023		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.4235558770601023 | validation: 0.36354925398310356]
	TIME [epoch: 7.76 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.274843841385524		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.274843841385524 | validation: 0.26962291824734164]
	TIME [epoch: 7.78 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2205890570307898		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2205890570307898 | validation: 0.3628769444894787]
	TIME [epoch: 7.78 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24692624509505587		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.24692624509505587 | validation: 0.29262843561990687]
	TIME [epoch: 7.75 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21631103649308492		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.21631103649308492 | validation: 0.231136034233635]
	TIME [epoch: 7.75 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19199562186573266		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.19199562186573266 | validation: 0.2686617750545318]
	TIME [epoch: 7.75 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2472898359124855		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2472898359124855 | validation: 0.2572712365989788]
	TIME [epoch: 7.78 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23864059450821634		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.23864059450821634 | validation: 0.376483867537998]
	TIME [epoch: 7.77 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2663078033249584		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.2663078033249584 | validation: 0.22408202222090198]
	TIME [epoch: 7.75 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19244799763222964		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.19244799763222964 | validation: 0.1853349964378299]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17964689839893075		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.17964689839893075 | validation: 0.9113824253240005]
	TIME [epoch: 7.75 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46538399263216557		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.46538399263216557 | validation: 0.3328412654896662]
	TIME [epoch: 7.8 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23390357179911103		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.23390357179911103 | validation: 0.22005335735241288]
	TIME [epoch: 7.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1935374157192858		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1935374157192858 | validation: 0.20421146659498482]
	TIME [epoch: 7.75 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17319069934012568		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.17319069934012568 | validation: 0.19321304509906656]
	TIME [epoch: 7.75 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.190224629810621		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.190224629810621 | validation: 0.24629547600468624]
	TIME [epoch: 7.75 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18908897394189342		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.18908897394189342 | validation: 0.21431220868864903]
	TIME [epoch: 7.8 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25238126348381323		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.25238126348381323 | validation: 0.2077833853343804]
	TIME [epoch: 7.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1845576814270978		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.1845576814270978 | validation: 0.18874151806786627]
	TIME [epoch: 7.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20837253489887897		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.20837253489887897 | validation: 0.3838021878061273]
	TIME [epoch: 7.75 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25066932101397693		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.25066932101397693 | validation: 0.20748057658450036]
	TIME [epoch: 7.75 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20340621712558948		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.20340621712558948 | validation: 0.19561711712148563]
	TIME [epoch: 7.8 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19288488159265058		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.19288488159265058 | validation: 0.18115884994446974]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1600827955096884		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1600827955096884 | validation: 0.16412257977430927]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7642952473807054		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.7642952473807054 | validation: 1.1894701489569315]
	TIME [epoch: 7.7 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8342121139053178		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.8342121139053178 | validation: 0.39192826213917764]
	TIME [epoch: 7.71 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41394988244138		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.41394988244138 | validation: 0.8862881349836866]
	TIME [epoch: 7.74 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4961894453933594		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.4961894453933594 | validation: 0.3986386543824023]
	TIME [epoch: 7.7 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29659838119311815		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.29659838119311815 | validation: 0.3063153618629556]
	TIME [epoch: 7.7 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23762645534023083		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.23762645534023083 | validation: 0.24619472171469248]
	TIME [epoch: 7.68 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18943168486653164		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.18943168486653164 | validation: 0.1891469836248167]
	TIME [epoch: 7.69 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15462606117340194		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.15462606117340194 | validation: 0.19248156774359076]
	TIME [epoch: 7.73 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1630600923353846		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.1630600923353846 | validation: 0.2228010462419017]
	TIME [epoch: 7.7 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31817517946003526		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.31817517946003526 | validation: 0.22876258042962944]
	TIME [epoch: 7.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2384595095874762		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.2384595095874762 | validation: 0.26218995443582593]
	TIME [epoch: 7.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20459122292723333		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.20459122292723333 | validation: 0.19718003841744774]
	TIME [epoch: 7.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16662225804868125		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.16662225804868125 | validation: 0.17816314974604042]
	TIME [epoch: 7.73 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19138972988123895		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.19138972988123895 | validation: 0.26635187777442515]
	TIME [epoch: 7.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8991320296527411		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.8991320296527411 | validation: 0.5334306608650188]
	TIME [epoch: 7.69 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32733612010600005		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.32733612010600005 | validation: 0.253406270544839]
	TIME [epoch: 7.69 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20773889426768027		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.20773889426768027 | validation: 0.20452835575719908]
	TIME [epoch: 7.7 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17351366156572143		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.17351366156572143 | validation: 0.1803030290878318]
	TIME [epoch: 7.73 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15266865358624265		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.15266865358624265 | validation: 0.17971289167445875]
	TIME [epoch: 7.69 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8882670974649817		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.8882670974649817 | validation: 0.7825255857906082]
	TIME [epoch: 7.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5026429203054165		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.5026429203054165 | validation: 0.36065299310348153]
	TIME [epoch: 7.73 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21059584651515895		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.21059584651515895 | validation: 0.18388561830920352]
	TIME [epoch: 7.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14775729066228666		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.14775729066228666 | validation: 0.17081098474571815]
	TIME [epoch: 7.77 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2105989554957503		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.2105989554957503 | validation: 0.1832069514831075]
	TIME [epoch: 7.72 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42215212079293984		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.42215212079293984 | validation: 0.3361957780495092]
	TIME [epoch: 7.72 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3104163499191126		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.3104163499191126 | validation: 0.22802524203112484]
	TIME [epoch: 7.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19501001107653507		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.19501001107653507 | validation: 0.18383686991286008]
	TIME [epoch: 7.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16947756244602435		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.16947756244602435 | validation: 0.17749485893414968]
	TIME [epoch: 7.75 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18369246451465282		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.18369246451465282 | validation: 0.18453565696559862]
	TIME [epoch: 7.71 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1808237437901576		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1808237437901576 | validation: 0.2795133643926052]
	TIME [epoch: 7.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17757970091057604		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.17757970091057604 | validation: 0.17675850486822295]
	TIME [epoch: 7.73 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14500435204617054		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.14500435204617054 | validation: 0.1676720045678025]
	TIME [epoch: 7.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20643197944675706		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.20643197944675706 | validation: 0.17740747796811374]
	TIME [epoch: 7.77 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16250009170061838		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.16250009170061838 | validation: 0.169968557803248]
	TIME [epoch: 7.73 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16916839023362656		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.16916839023362656 | validation: 0.203356283209001]
	TIME [epoch: 7.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18559653902385703		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.18559653902385703 | validation: 0.1716315730105562]
	TIME [epoch: 7.73 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18488357981957138		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.18488357981957138 | validation: 0.18364133377700842]
	TIME [epoch: 7.75 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.159499789062022		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.159499789062022 | validation: 0.14728686554438522]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15392964399133005		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.15392964399133005 | validation: 0.19051783868173966]
	TIME [epoch: 7.74 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16211698396793825		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.16211698396793825 | validation: 0.16213218049789135]
	TIME [epoch: 7.73 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14902086377987125		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.14902086377987125 | validation: 0.1640798734853115]
	TIME [epoch: 7.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44798850945820345		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.44798850945820345 | validation: 0.2448002213286145]
	TIME [epoch: 7.76 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22245560511411297		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.22245560511411297 | validation: 0.19447798220173768]
	TIME [epoch: 7.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1479371851566113		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.1479371851566113 | validation: 0.15652059394178863]
	TIME [epoch: 7.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13435550490893575		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.13435550490893575 | validation: 0.14917656147897762]
	TIME [epoch: 7.73 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12656613363215358		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.12656613363215358 | validation: 0.14047367142273695]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17737742453020106		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.17737742453020106 | validation: 0.1571725511142528]
	TIME [epoch: 7.78 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16427818148690476		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.16427818148690476 | validation: 0.17214924112473667]
	TIME [epoch: 7.77 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16272645181969167		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.16272645181969167 | validation: 0.15920751705884564]
	TIME [epoch: 7.75 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21413999366860842		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.21413999366860842 | validation: 0.22749524695933387]
	TIME [epoch: 7.76 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17963659787565497		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.17963659787565497 | validation: 0.14671008210221498]
	TIME [epoch: 7.75 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0224642526265275		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 1.0224642526265275 | validation: 2.1578876605255584]
	TIME [epoch: 7.79 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5185405528721334		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 1.5185405528721334 | validation: 2.748418752045099]
	TIME [epoch: 7.77 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6888351354709958		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 1.6888351354709958 | validation: 2.22460219095525]
	TIME [epoch: 7.75 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.327958525918388		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 1.327958525918388 | validation: 2.273323414220102]
	TIME [epoch: 7.75 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.448683288956737		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 1.448683288956737 | validation: 1.458929541054418]
	TIME [epoch: 7.75 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677496996599907		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.5677496996599907 | validation: 0.4285161876992748]
	TIME [epoch: 7.79 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024613862293363		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.3024613862293363 | validation: 0.3388100983572296]
	TIME [epoch: 7.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24863458347106052		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.24863458347106052 | validation: 0.2601222067554355]
	TIME [epoch: 7.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20946792660817048		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.20946792660817048 | validation: 0.4192106760014753]
	TIME [epoch: 7.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28202239216204794		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.28202239216204794 | validation: 0.2617831178096175]
	TIME [epoch: 7.76 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19541611323052455		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.19541611323052455 | validation: 0.21630171079513127]
	TIME [epoch: 7.79 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16080788431839962		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.16080788431839962 | validation: 0.19766992462709396]
	TIME [epoch: 7.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14553286809705387		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.14553286809705387 | validation: 0.18055899950993767]
	TIME [epoch: 7.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14071497177270306		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.14071497177270306 | validation: 0.17140754001341885]
	TIME [epoch: 7.75 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14508405341235187		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.14508405341235187 | validation: 0.16803629850912516]
	TIME [epoch: 7.75 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1351226837955704		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.1351226837955704 | validation: 0.15665549895178518]
	TIME [epoch: 7.81 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13147659941746656		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.13147659941746656 | validation: 0.13974004833157758]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12567238247214044		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.12567238247214044 | validation: 0.13937542335575134]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13648513341493096		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.13648513341493096 | validation: 0.37244989428734365]
	TIME [epoch: 7.76 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2923536341936851		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.2923536341936851 | validation: 0.22990166351834124]
	TIME [epoch: 7.76 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16920661562943864		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.16920661562943864 | validation: 0.20384005359360377]
	TIME [epoch: 7.81 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15607740610687484		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.15607740610687484 | validation: 0.1592182892669875]
	TIME [epoch: 7.75 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13256695670456914		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.13256695670456914 | validation: 0.14149396110192064]
	TIME [epoch: 7.75 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12587820920196477		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.12587820920196477 | validation: 0.1568039618364756]
	TIME [epoch: 7.75 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13342383382817327		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.13342383382817327 | validation: 0.13263477970695384]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12612813110963367		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.12612813110963367 | validation: 0.14582845166190098]
	TIME [epoch: 7.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1428034779768238		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.1428034779768238 | validation: 0.2356866570349907]
	TIME [epoch: 7.75 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15708443044163783		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.15708443044163783 | validation: 0.16085842460250244]
	TIME [epoch: 7.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15052312229634093		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.15052312229634093 | validation: 0.14307871628902125]
	TIME [epoch: 7.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1295212493258875		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.1295212493258875 | validation: 0.13648556137013493]
	TIME [epoch: 7.77 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13627764701269843		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.13627764701269843 | validation: 0.11351647273954696]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12987713898329104		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.12987713898329104 | validation: 0.15105165847594867]
	TIME [epoch: 7.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12495584916161047		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.12495584916161047 | validation: 0.13507786639298286]
	TIME [epoch: 7.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2539029881336836		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.2539029881336836 | validation: 0.18181435395376105]
	TIME [epoch: 7.76 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17575622644888902		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.17575622644888902 | validation: 0.265375305890086]
	TIME [epoch: 7.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16257453182163803		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.16257453182163803 | validation: 0.13379724757727404]
	TIME [epoch: 7.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11985962177005122		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.11985962177005122 | validation: 0.1214714554691056]
	TIME [epoch: 7.76 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13499736445694346		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.13499736445694346 | validation: 0.14520500695802327]
	TIME [epoch: 7.75 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12745552338136468		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.12745552338136468 | validation: 0.1253826753864812]
	TIME [epoch: 7.75 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1157969124674717		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.1157969124674717 | validation: 0.10768251486744898]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20335857186882575		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.20335857186882575 | validation: 0.19309959569864132]
	TIME [epoch: 7.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1478292163344519		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.1478292163344519 | validation: 0.15732572511325268]
	TIME [epoch: 7.72 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16374496398945368		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.16374496398945368 | validation: 1.113763439018321]
	TIME [epoch: 7.72 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5378131092605446		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.5378131092605446 | validation: 0.21891537779955017]
	TIME [epoch: 7.71 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27028593531073053		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.27028593531073053 | validation: 0.22991948478477725]
	TIME [epoch: 7.75 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1768439070124427		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.1768439070124427 | validation: 0.20963691157336428]
	TIME [epoch: 7.73 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15405383982105758		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.15405383982105758 | validation: 0.1661723362020805]
	TIME [epoch: 7.72 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12604354998604475		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.12604354998604475 | validation: 0.14580077939622704]
	TIME [epoch: 7.72 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11727449314981342		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.11727449314981342 | validation: 0.13793705388700953]
	TIME [epoch: 7.72 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.124269440723904		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.124269440723904 | validation: 0.12187481033263574]
	TIME [epoch: 7.75 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11108813914389581		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.11108813914389581 | validation: 0.1316529358039943]
	TIME [epoch: 7.75 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11084755156098339		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.11084755156098339 | validation: 0.10793114536128162]
	TIME [epoch: 7.72 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13541565994165425		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.13541565994165425 | validation: 0.33484369450801743]
	TIME [epoch: 7.72 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33913234642743506		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.33913234642743506 | validation: 0.553346554699671]
	TIME [epoch: 7.72 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32189983933596644		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.32189983933596644 | validation: 0.27784336280101707]
	TIME [epoch: 7.74 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21024482398055133		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.21024482398055133 | validation: 0.2255253321093671]
	TIME [epoch: 7.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17556386513622746		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.17556386513622746 | validation: 0.196424394976292]
	TIME [epoch: 7.72 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15433899307501703		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.15433899307501703 | validation: 0.16503748769787743]
	TIME [epoch: 7.71 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12960433362820847		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.12960433362820847 | validation: 0.2977697658886346]
	TIME [epoch: 7.72 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2823257269045497		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.2823257269045497 | validation: 0.19233074139714554]
	TIME [epoch: 7.75 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1641694576989268		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.1641694576989268 | validation: 0.154542871371065]
	TIME [epoch: 7.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14207108091582019		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.14207108091582019 | validation: 0.13618553849106033]
	TIME [epoch: 7.71 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12702377683665267		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.12702377683665267 | validation: 0.14248611616846668]
	TIME [epoch: 7.71 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13768854157707658		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.13768854157707658 | validation: 0.16693913145941847]
	TIME [epoch: 7.72 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13328208883840748		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.13328208883840748 | validation: 0.13831479255551066]
	TIME [epoch: 7.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11967628338742928		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.11967628338742928 | validation: 0.1567092897369547]
	TIME [epoch: 7.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12301261951310036		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.12301261951310036 | validation: 0.10103589548958505]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1018628672736202		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.1018628672736202 | validation: 0.11885594425621704]
	TIME [epoch: 7.73 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11684334720141686		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.11684334720141686 | validation: 0.8263933935938408]
	TIME [epoch: 7.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2370815608057069		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.2370815608057069 | validation: 0.14327337988613345]
	TIME [epoch: 7.79 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11285607875538858		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.11285607875538858 | validation: 0.10413849727027844]
	TIME [epoch: 7.76 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11530379269416194		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.11530379269416194 | validation: 0.10407476765416834]
	TIME [epoch: 7.74 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14261499801330513		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.14261499801330513 | validation: 0.14386591954079161]
	TIME [epoch: 7.75 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6561333165396666		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.6561333165396666 | validation: 0.665895339741954]
	TIME [epoch: 7.74 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4171254321990393		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.4171254321990393 | validation: 0.18631529595284024]
	TIME [epoch: 7.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22511595147696636		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.22511595147696636 | validation: 0.22350910714857652]
	TIME [epoch: 7.75 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15273021426066288		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.15273021426066288 | validation: 0.14231482561585998]
	TIME [epoch: 7.74 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1114480514058324		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.1114480514058324 | validation: 0.48021831110076335]
	TIME [epoch: 7.74 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29150845208104803		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.29150845208104803 | validation: 0.27022875501208315]
	TIME [epoch: 7.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20158903397481764		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.20158903397481764 | validation: 0.17477922486813632]
	TIME [epoch: 7.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1528590543051692		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.1528590543051692 | validation: 0.1488419686919903]
	TIME [epoch: 7.75 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12816766435293633		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.12816766435293633 | validation: 0.12604140498471042]
	TIME [epoch: 7.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11923464817981275		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.11923464817981275 | validation: 0.16421869842797304]
	TIME [epoch: 7.75 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16054905568190295		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.16054905568190295 | validation: 0.15095607714007042]
	TIME [epoch: 7.75 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1198003335152869		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.1198003335152869 | validation: 0.12053460345715404]
	TIME [epoch: 7.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10519241288546465		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.10519241288546465 | validation: 0.10822683827295813]
	TIME [epoch: 7.75 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4179073220648952		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.4179073220648952 | validation: 1.0314405013097774]
	TIME [epoch: 7.74 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5504256704815162		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.5504256704815162 | validation: 0.3548288932553999]
	TIME [epoch: 7.74 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30513531129695026		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.30513531129695026 | validation: 0.32271668160555383]
	TIME [epoch: 7.74 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2526619852188249		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.2526619852188249 | validation: 0.29212764464368046]
	TIME [epoch: 7.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22121177779530815		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.22121177779530815 | validation: 0.2162883588259853]
	TIME [epoch: 7.75 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1650534874725887		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.1650534874725887 | validation: 0.18575778215219607]
	TIME [epoch: 7.74 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.139261632777949		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.139261632777949 | validation: 0.14653746587383298]
	TIME [epoch: 7.74 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12419867403138367		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.12419867403138367 | validation: 0.12406771543365075]
	TIME [epoch: 7.74 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10979147340033137		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.10979147340033137 | validation: 0.10767281164910786]
	TIME [epoch: 7.81 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16491142782681448		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.16491142782681448 | validation: 0.22780389032519802]
	TIME [epoch: 7.75 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2748080485989708		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.2748080485989708 | validation: 0.24117253656366341]
	TIME [epoch: 7.75 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18541553842626218		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.18541553842626218 | validation: 0.16426350748098473]
	TIME [epoch: 7.75 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12204404709704325		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.12204404709704325 | validation: 0.14081126270979094]
	TIME [epoch: 7.75 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11634308829799171		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.11634308829799171 | validation: 0.14314779120416815]
	TIME [epoch: 7.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1102373659673655		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.1102373659673655 | validation: 0.11100345906072678]
	TIME [epoch: 7.75 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09939818670983315		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.09939818670983315 | validation: 0.09300017945547848]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18091822737919827		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.18091822737919827 | validation: 0.16758341750250205]
	TIME [epoch: 7.75 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15772392418038073		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.15772392418038073 | validation: 0.16544185399682176]
	TIME [epoch: 7.75 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12565582784691698		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.12565582784691698 | validation: 0.14317230996318528]
	TIME [epoch: 7.79 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11662667257159144		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.11662667257159144 | validation: 0.11774844695238024]
	TIME [epoch: 7.75 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11206645252948254		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.11206645252948254 | validation: 0.11646601506264143]
	TIME [epoch: 7.75 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11107996488775747		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.11107996488775747 | validation: 0.11065626425426923]
	TIME [epoch: 7.75 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10770523671844806		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.10770523671844806 | validation: 0.1078296316802732]
	TIME [epoch: 7.77 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09614627568048682		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.09614627568048682 | validation: 0.15724149076543614]
	TIME [epoch: 7.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1242496840323905		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.1242496840323905 | validation: 0.10933613388515856]
	TIME [epoch: 7.75 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2572824660696025		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.2572824660696025 | validation: 1.020605062771387]
	TIME [epoch: 7.75 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5592952086312528		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.5592952086312528 | validation: 0.19362829994886938]
	TIME [epoch: 7.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16393067131515002		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.16393067131515002 | validation: 0.15565748015097453]
	TIME [epoch: 7.76 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13249046891648092		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.13249046891648092 | validation: 0.14137988177080354]
	TIME [epoch: 7.78 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11875713248631942		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.11875713248631942 | validation: 0.13307656149802297]
	TIME [epoch: 7.75 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1116568323955581		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.1116568323955581 | validation: 0.12622903570882443]
	TIME [epoch: 7.75 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10806589626040525		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.10806589626040525 | validation: 0.16595848791259396]
	TIME [epoch: 7.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14586086942670234		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.14586086942670234 | validation: 0.14893140463152782]
	TIME [epoch: 7.76 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11318600740115171		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.11318600740115171 | validation: 0.12520414581176534]
	TIME [epoch: 7.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10864357479148337		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.10864357479148337 | validation: 0.11966671645986732]
	TIME [epoch: 7.76 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1053637342328623		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.1053637342328623 | validation: 0.11477854596177439]
	TIME [epoch: 7.75 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115224932920783		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.10115224932920783 | validation: 0.10549139299459948]
	TIME [epoch: 7.74 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10263233817133866		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.10263233817133866 | validation: 0.1047534235281942]
	TIME [epoch: 7.76 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10563383649623058		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.10563383649623058 | validation: 0.12092546196861984]
	TIME [epoch: 7.78 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11341364575871171		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.11341364575871171 | validation: 0.10241449376343095]
	TIME [epoch: 7.75 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09878050574926592		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.09878050574926592 | validation: 0.09391831197309856]
	TIME [epoch: 7.75 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09083881572101404		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.09083881572101404 | validation: 0.31497284539236764]
	TIME [epoch: 7.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1560073195397541		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.1560073195397541 | validation: 0.09329411736768478]
	TIME [epoch: 7.76 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0953209390608305		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.0953209390608305 | validation: 0.0875874189374935]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09628466617729556		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.09628466617729556 | validation: 0.09543138265035334]
	TIME [epoch: 7.75 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08879807639230407		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.08879807639230407 | validation: 0.10007221129364408]
	TIME [epoch: 7.75 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09420291497801826		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.09420291497801826 | validation: 0.09014233738634489]
	TIME [epoch: 7.75 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10200882386476602		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.10200882386476602 | validation: 0.11130905120195621]
	TIME [epoch: 7.78 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10579736502372246		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.10579736502372246 | validation: 0.10239503398974226]
	TIME [epoch: 7.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09773802327491836		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.09773802327491836 | validation: 0.14529696103000772]
	TIME [epoch: 7.75 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11649209117998993		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.11649209117998993 | validation: 0.10989604678192946]
	TIME [epoch: 7.75 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09357434095607582		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.09357434095607582 | validation: 0.08593812198789427]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09024426864536622		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.09024426864536622 | validation: 0.08730116322269987]
	TIME [epoch: 7.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09000874114355667		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.09000874114355667 | validation: 0.09575163424984683]
	TIME [epoch: 7.78 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.095112384978298		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.095112384978298 | validation: 0.11014864989629214]
	TIME [epoch: 7.75 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11242360118348788		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.11242360118348788 | validation: 0.1063081766471555]
	TIME [epoch: 7.75 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0929358009106958		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.0929358009106958 | validation: 0.11962815734476669]
	TIME [epoch: 7.76 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15182033968858374		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.15182033968858374 | validation: 0.15570937778253288]
	TIME [epoch: 7.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11761946614816414		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.11761946614816414 | validation: 0.16081270021378083]
	TIME [epoch: 7.77 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12427188416635873		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.12427188416635873 | validation: 0.11734140954504792]
	TIME [epoch: 7.75 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1001698278038073		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.1001698278038073 | validation: 0.103473703125672]
	TIME [epoch: 7.75 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09418269036173915		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.09418269036173915 | validation: 0.09028772430538076]
	TIME [epoch: 7.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08697806370453645		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.08697806370453645 | validation: 0.08322077543026263]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0939737742256895		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.0939737742256895 | validation: 0.09182338550679897]
	TIME [epoch: 7.77 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09751978020530229		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.09751978020530229 | validation: 0.08923034256609638]
	TIME [epoch: 7.76 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09530211893069925		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.09530211893069925 | validation: 0.09899626126061004]
	TIME [epoch: 7.76 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16800644206495033		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.16800644206495033 | validation: 0.28704257005061967]
	TIME [epoch: 7.76 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19316160042248715		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.19316160042248715 | validation: 0.17740670312628629]
	TIME [epoch: 7.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3176195127008138		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.3176195127008138 | validation: 0.2786489259018456]
	TIME [epoch: 7.75 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21134810201457208		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.21134810201457208 | validation: 0.23094098145871422]
	TIME [epoch: 7.75 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18902008008579485		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.18902008008579485 | validation: 0.21025010113827405]
	TIME [epoch: 7.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1741281976391691		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.1741281976391691 | validation: 0.18291648260458454]
	TIME [epoch: 7.75 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15690099365281726		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.15690099365281726 | validation: 0.1649031582518056]
	TIME [epoch: 7.81 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14035288084695657		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.14035288084695657 | validation: 0.13747873744469358]
	TIME [epoch: 7.75 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11842186491066838		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.11842186491066838 | validation: 0.1097721031540062]
	TIME [epoch: 7.75 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10105751334755936		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.10105751334755936 | validation: 0.10064569088240298]
	TIME [epoch: 7.75 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09789754196855341		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.09789754196855341 | validation: 0.09367996681704346]
	TIME [epoch: 7.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09395899552038331		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.09395899552038331 | validation: 0.0861642516727772]
	TIME [epoch: 7.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08381015228897251		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.08381015228897251 | validation: 0.0862200964426265]
	TIME [epoch: 7.77 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1147865976705649		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.1147865976705649 | validation: 0.1509432014401022]
	TIME [epoch: 7.75 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13027932462170733		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.13027932462170733 | validation: 0.13091889545048402]
	TIME [epoch: 7.75 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1042284264258816		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.1042284264258816 | validation: 0.11748917903716863]
	TIME [epoch: 7.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09819430162092957		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.09819430162092957 | validation: 0.13301700889310292]
	TIME [epoch: 7.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10871240868109262		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.10871240868109262 | validation: 0.09260563963036228]
	TIME [epoch: 7.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08937815174343641		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.08937815174343641 | validation: 0.08782108064974205]
	TIME [epoch: 7.71 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08435671335912637		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.08435671335912637 | validation: 0.18595091177189887]
	TIME [epoch: 7.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14017602628146314		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.14017602628146314 | validation: 0.1456763426371091]
	TIME [epoch: 7.72 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1097067516174349		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.1097067516174349 | validation: 0.11966006275048285]
	TIME [epoch: 7.75 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10148683630201044		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.10148683630201044 | validation: 0.10816249454219615]
	TIME [epoch: 7.72 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09455143165758705		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.09455143165758705 | validation: 0.12601628187573577]
	TIME [epoch: 7.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09128025153665395		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.09128025153665395 | validation: 0.4147906058964206]
	TIME [epoch: 7.71 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4405627163993881		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.4405627163993881 | validation: 0.313101308704969]
	TIME [epoch: 7.71 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19151648268169383		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.19151648268169383 | validation: 0.13050168116696176]
	TIME [epoch: 7.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13307939094354204		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.13307939094354204 | validation: 0.16126471240291962]
	TIME [epoch: 7.71 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12424719779332778		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.12424719779332778 | validation: 0.11704863993826911]
	TIME [epoch: 7.72 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10744419359404066		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.10744419359404066 | validation: 0.10411360386477471]
	TIME [epoch: 7.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09492306717760222		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.09492306717760222 | validation: 0.09356720908654764]
	TIME [epoch: 7.72 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08152381672779466		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.08152381672779466 | validation: 0.08637187255941536]
	TIME [epoch: 7.77 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08213337953257084		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.08213337953257084 | validation: 0.08497944649973452]
	TIME [epoch: 7.71 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07954205451425132		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.07954205451425132 | validation: 0.10429963995416072]
	TIME [epoch: 7.72 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10980576608354381		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.10980576608354381 | validation: 0.10830938165876006]
	TIME [epoch: 7.72 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09493091353699586		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.09493091353699586 | validation: 0.10894749877720739]
	TIME [epoch: 7.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10096683223550448		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.10096683223550448 | validation: 0.09608330332309833]
	TIME [epoch: 7.76 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08833971189424737		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.08833971189424737 | validation: 0.07966898030306789]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07961650701623627		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.07961650701623627 | validation: 0.08407306623423033]
	TIME [epoch: 7.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08451327291055129		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.08451327291055129 | validation: 0.08926094920528575]
	TIME [epoch: 7.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08327989954490228		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.08327989954490228 | validation: 0.08477776490537879]
	TIME [epoch: 7.74 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18700056810245833		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.18700056810245833 | validation: 0.1364883052824414]
	TIME [epoch: 7.74 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10266241913768323		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.10266241913768323 | validation: 0.11968789155899584]
	TIME [epoch: 7.72 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09335767523938683		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.09335767523938683 | validation: 0.10384272310158332]
	TIME [epoch: 7.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09281145806284592		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.09281145806284592 | validation: 0.08875272903123863]
	TIME [epoch: 7.72 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09190774898788764		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.09190774898788764 | validation: 0.09753716222391534]
	TIME [epoch: 7.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16315160338849363		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.16315160338849363 | validation: 0.16098920294889218]
	TIME [epoch: 7.74 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13928794760973737		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.13928794760973737 | validation: 0.0996500465378939]
	TIME [epoch: 7.71 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09170498758898529		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.09170498758898529 | validation: 0.08966402109547869]
	TIME [epoch: 7.72 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09061442298474687		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.09061442298474687 | validation: 0.09815498920788714]
	TIME [epoch: 7.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15356064148874876		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.15356064148874876 | validation: 0.19309256074989103]
	TIME [epoch: 7.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13394337711313403		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.13394337711313403 | validation: 0.14102733438317827]
	TIME [epoch: 7.75 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10274008174555378		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.10274008174555378 | validation: 0.11503935797721287]
	TIME [epoch: 7.71 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09387629998883129		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.09387629998883129 | validation: 0.11053977505261152]
	TIME [epoch: 7.73 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0901172517502713		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.0901172517502713 | validation: 0.08587474241828782]
	TIME [epoch: 7.71 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07937049360259804		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.07937049360259804 | validation: 0.08364008799051037]
	TIME [epoch: 7.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08511526918922571		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.08511526918922571 | validation: 0.08934593123045623]
	TIME [epoch: 7.74 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08677958000086919		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.08677958000086919 | validation: 0.07578417115142233]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08765819206055203		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.08765819206055203 | validation: 0.08389528741754157]
	TIME [epoch: 7.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08179925132980324		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.08179925132980324 | validation: 0.09102648312527184]
	TIME [epoch: 7.75 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28043510734587357		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.28043510734587357 | validation: 0.17203799337992465]
	TIME [epoch: 7.78 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1310615231391436		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.1310615231391436 | validation: 0.12013111124654369]
	TIME [epoch: 7.77 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09961213707450076		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.09961213707450076 | validation: 0.10180869449088667]
	TIME [epoch: 7.74 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0935760830720443		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.0935760830720443 | validation: 0.09498043287019672]
	TIME [epoch: 7.74 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0882147929533328		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0882147929533328 | validation: 0.094488399811889]
	TIME [epoch: 7.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09183878362175803		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.09183878362175803 | validation: 0.09685746112357117]
	TIME [epoch: 7.77 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08491911928551692		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.08491911928551692 | validation: 0.09298590211605204]
	TIME [epoch: 7.77 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09053482008254993		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.09053482008254993 | validation: 0.09796072576323347]
	TIME [epoch: 7.74 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09130383297181287		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.09130383297181287 | validation: 0.08742271400716098]
	TIME [epoch: 7.74 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07842694658637024		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.07842694658637024 | validation: 0.07639676679949316]
	TIME [epoch: 7.74 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07859890794060757		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.07859890794060757 | validation: 0.07777038789401061]
	TIME [epoch: 7.76 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08016887614424306		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.08016887614424306 | validation: 0.08800375946753902]
	TIME [epoch: 7.76 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0853176721617452		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.0853176721617452 | validation: 0.07582122315876913]
	TIME [epoch: 7.74 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08203207045766203		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.08203207045766203 | validation: 0.0896093619596001]
	TIME [epoch: 7.74 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08756040590980135		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.08756040590980135 | validation: 0.12788274716166714]
	TIME [epoch: 7.74 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09313746523810368		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.09313746523810368 | validation: 0.09355392549550859]
	TIME [epoch: 7.79 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08321807053834214		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.08321807053834214 | validation: 0.11339019127893882]
	TIME [epoch: 7.76 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09954995773372977		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.09954995773372977 | validation: 0.10495713391638054]
	TIME [epoch: 7.74 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08511113710013957		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.08511113710013957 | validation: 0.08621459784389413]
	TIME [epoch: 7.75 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07653735866351624		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.07653735866351624 | validation: 0.08472679570435732]
	TIME [epoch: 7.75 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07993275046437151		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.07993275046437151 | validation: 0.12177257157681394]
	TIME [epoch: 7.79 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0959423095261804		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.0959423095261804 | validation: 0.11966821485592877]
	TIME [epoch: 7.75 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0880210812694401		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.0880210812694401 | validation: 0.08425203530349726]
	TIME [epoch: 7.74 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08076833342356564		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.08076833342356564 | validation: 0.08536605707814238]
	TIME [epoch: 7.75 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0781238963132193		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.0781238963132193 | validation: 0.0876931616916771]
	TIME [epoch: 7.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08061349395467476		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.08061349395467476 | validation: 0.07530936952411274]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07693246786758538		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.07693246786758538 | validation: 0.07399148051371368]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08373566046195025		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.08373566046195025 | validation: 0.0946657296212677]
	TIME [epoch: 7.75 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08754747102384962		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.08754747102384962 | validation: 0.09999206298931879]
	TIME [epoch: 7.76 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08606378697853029		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.08606378697853029 | validation: 0.08897097305874528]
	TIME [epoch: 7.76 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0823482029803456		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.0823482029803456 | validation: 0.08827093290701812]
	TIME [epoch: 7.81 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08116355302813745		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.08116355302813745 | validation: 0.07372126959672633]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07405691880846622		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.07405691880846622 | validation: 0.08235571238269726]
	TIME [epoch: 7.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09045915609601948		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.09045915609601948 | validation: 0.08735690341204354]
	TIME [epoch: 7.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08028417078529156		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.08028417078529156 | validation: 0.09317323726913959]
	TIME [epoch: 7.76 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08225971866467022		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.08225971866467022 | validation: 0.08208735689382654]
	TIME [epoch: 7.79 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10179665457781639		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.10179665457781639 | validation: 0.12879118083661337]
	TIME [epoch: 7.75 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10276272833096328		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.10276272833096328 | validation: 0.09909912729379527]
	TIME [epoch: 7.74 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08190003010058311		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.08190003010058311 | validation: 0.08326617010641245]
	TIME [epoch: 7.75 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07883850501206069		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.07883850501206069 | validation: 0.0778419864081107]
	TIME [epoch: 7.75 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0838231455538781		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0838231455538781 | validation: 0.0884255207309043]
	TIME [epoch: 7.79 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0784433340775662		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.0784433340775662 | validation: 0.08226948298412534]
	TIME [epoch: 7.75 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0776476043050457		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.0776476043050457 | validation: 0.0749201893239883]
	TIME [epoch: 7.75 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07264994270805777		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.07264994270805777 | validation: 0.0810072997430047]
	TIME [epoch: 7.75 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08581267563888105		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.08581267563888105 | validation: 0.08933982399326923]
	TIME [epoch: 7.76 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08201627137819167		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.08201627137819167 | validation: 0.0926659766262508]
	TIME [epoch: 7.79 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11429338451154465		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.11429338451154465 | validation: 0.10258020973903176]
	TIME [epoch: 7.74 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08436972135152149		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.08436972135152149 | validation: 0.08975259591657578]
	TIME [epoch: 7.74 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07446334493284182		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.07446334493284182 | validation: 0.07078002911741042]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07189172450496945		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.07189172450496945 | validation: 0.0724099754050947]
	TIME [epoch: 7.77 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09808629168544675		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.09808629168544675 | validation: 0.11536947974394543]
	TIME [epoch: 7.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09891863519814979		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.09891863519814979 | validation: 0.07522968048031481]
	TIME [epoch: 7.74 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13965986358102056		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.13965986358102056 | validation: 0.12581618976586348]
	TIME [epoch: 7.74 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09882695317187788		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.09882695317187788 | validation: 0.10815028423874368]
	TIME [epoch: 7.74 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08580317527479787		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.08580317527479787 | validation: 0.09571617473299483]
	TIME [epoch: 7.77 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07874272654622863		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.07874272654622863 | validation: 0.10264726487363876]
	TIME [epoch: 7.77 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0924154934198558		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.0924154934198558 | validation: 0.08808063873775272]
	TIME [epoch: 7.75 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07912056053704135		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.07912056053704135 | validation: 0.08853380724992538]
	TIME [epoch: 7.74 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08188917356118593		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.08188917356118593 | validation: 0.08845274258305605]
	TIME [epoch: 7.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07581148287204362		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.07581148287204362 | validation: 0.08330637575170619]
	TIME [epoch: 7.78 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07335408379289948		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.07335408379289948 | validation: 0.08292407523519274]
	TIME [epoch: 7.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07370355110088503		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.07370355110088503 | validation: 0.07991523501533454]
	TIME [epoch: 7.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10389304251426809		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.10389304251426809 | validation: 0.11517243033350266]
	TIME [epoch: 7.75 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09271780662400145		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.09271780662400145 | validation: 0.10717071184838725]
	TIME [epoch: 7.74 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08319512333789938		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.08319512333789938 | validation: 0.0943773158279084]
	TIME [epoch: 7.77 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07955833536188722		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.07955833536188722 | validation: 0.08484155774144614]
	TIME [epoch: 7.77 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07802605694107786		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.07802605694107786 | validation: 0.0744180394173245]
	TIME [epoch: 7.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07203718660154655		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.07203718660154655 | validation: 0.08162646011224241]
	TIME [epoch: 7.74 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18636765826131102		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.18636765826131102 | validation: 0.15969687835659052]
	TIME [epoch: 7.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13132234659875208		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.13132234659875208 | validation: 0.15196392626539168]
	TIME [epoch: 7.77 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12485634439534601		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.12485634439534601 | validation: 0.121055103431746]
	TIME [epoch: 7.77 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10146925780452515		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.10146925780452515 | validation: 0.0990025985019081]
	TIME [epoch: 7.74 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08504475964983584		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.08504475964983584 | validation: 0.08194164474468857]
	TIME [epoch: 7.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07729086381094441		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.07729086381094441 | validation: 0.07523386737118987]
	TIME [epoch: 7.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07113003440231155		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.07113003440231155 | validation: 0.07033138593429356]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07257734151961703		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.07257734151961703 | validation: 0.07101721935245392]
	TIME [epoch: 7.74 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07084688976339427		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.07084688976339427 | validation: 0.07380928906163749]
	TIME [epoch: 7.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0695988403708268		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.0695988403708268 | validation: 0.08126693980568611]
	TIME [epoch: 7.73 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07147299565971885		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.07147299565971885 | validation: 0.0729746632484736]
	TIME [epoch: 7.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07303231075093901		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.07303231075093901 | validation: 0.07224471657148343]
	TIME [epoch: 7.78 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07726558777444242		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.07726558777444242 | validation: 0.07231063696776674]
	TIME [epoch: 7.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07340128853483693		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.07340128853483693 | validation: 0.07510111011534419]
	TIME [epoch: 7.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07731718533799178		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.07731718533799178 | validation: 0.06999047928971283]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07381987168547613		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.07381987168547613 | validation: 0.06742912501980301]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06913866221041232		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.06913866221041232 | validation: 0.08030416011101313]
	TIME [epoch: 7.79 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0825571627767154		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.0825571627767154 | validation: 0.08048095683914949]
	TIME [epoch: 7.74 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07495108946521808		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.07495108946521808 | validation: 0.07584652402819912]
	TIME [epoch: 7.76 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07132387101353879		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.07132387101353879 | validation: 0.07297775416849989]
	TIME [epoch: 7.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08040592146322878		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.08040592146322878 | validation: 0.10218164174082986]
	TIME [epoch: 7.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0800186572243298		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.0800186572243298 | validation: 0.08423683513946789]
	TIME [epoch: 7.78 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07918106488512577		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.07918106488512577 | validation: 0.07399911125358687]
	TIME [epoch: 7.74 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06931430238018245		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.06931430238018245 | validation: 0.0865672918360054]
	TIME [epoch: 7.72 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07333069816585247		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.07333069816585247 | validation: 0.09854011396079124]
	TIME [epoch: 7.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08774998882518012		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.08774998882518012 | validation: 0.07841342814550892]
	TIME [epoch: 7.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0767799653095326		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.0767799653095326 | validation: 0.07600907300337785]
	TIME [epoch: 7.77 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07563218070831537		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.07563218070831537 | validation: 0.07810131155372661]
	TIME [epoch: 7.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07602563362078588		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.07602563362078588 | validation: 0.07258133816216676]
	TIME [epoch: 7.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07083293712464753		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.07083293712464753 | validation: 0.07339023135610674]
	TIME [epoch: 7.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07258762095110553		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.07258762095110553 | validation: 0.07583984083848247]
	TIME [epoch: 7.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07184259971860232		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.07184259971860232 | validation: 0.0791110667542294]
	TIME [epoch: 7.77 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07224119920933648		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.07224119920933648 | validation: 0.09030016489520507]
	TIME [epoch: 7.72 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0855906052233274		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.0855906052233274 | validation: 0.08066364649314864]
	TIME [epoch: 7.72 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07035998041650869		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.07035998041650869 | validation: 0.072057715851478]
	TIME [epoch: 7.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06827968163693125		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.06827968163693125 | validation: 0.07625308785632423]
	TIME [epoch: 7.75 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07254088583679767		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.07254088583679767 | validation: 0.0671805211314949]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06402755926978118		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.06402755926978118 | validation: 0.0678377708533579]
	TIME [epoch: 7.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07561191361574271		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.07561191361574271 | validation: 0.07397240292594748]
	TIME [epoch: 7.75 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07264849598463581		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.07264849598463581 | validation: 0.07868347503361262]
	TIME [epoch: 7.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07066131018840513		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.07066131018840513 | validation: 0.07897714492052377]
	TIME [epoch: 7.77 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08356526932996401		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.08356526932996401 | validation: 0.07723019706397286]
	TIME [epoch: 7.79 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07056277784141947		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.07056277784141947 | validation: 0.07104125423470758]
	TIME [epoch: 7.75 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06568598909492009		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.06568598909492009 | validation: 0.0905877998778252]
	TIME [epoch: 7.75 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11373981581485983		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.11373981581485983 | validation: 0.11681247492954294]
	TIME [epoch: 7.75 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09474345422397353		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.09474345422397353 | validation: 0.10461760878918638]
	TIME [epoch: 7.77 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08290280900124912		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.08290280900124912 | validation: 0.09261883989108138]
	TIME [epoch: 7.78 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07671502450270569		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.07671502450270569 | validation: 0.09001349448790466]
	TIME [epoch: 7.75 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07775937841417066		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.07775937841417066 | validation: 0.08037284809768014]
	TIME [epoch: 7.75 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07296863092657806		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.07296863092657806 | validation: 0.07697184704043407]
	TIME [epoch: 7.75 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07179773414581274		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.07179773414581274 | validation: 0.0789715710042441]
	TIME [epoch: 7.78 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07082399169104506		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.07082399169104506 | validation: 0.07295533484464932]
	TIME [epoch: 7.78 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07346764111610696		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.07346764111610696 | validation: 0.07808006035406029]
	TIME [epoch: 7.75 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07726890649154561		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.07726890649154561 | validation: 0.078695297277908]
	TIME [epoch: 7.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07258119533916656		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.07258119533916656 | validation: 0.06922833514717251]
	TIME [epoch: 7.75 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06538899067736673		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.06538899067736673 | validation: 0.08019765746331114]
	TIME [epoch: 7.77 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07159807197739454		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.07159807197739454 | validation: 0.07513254319578531]
	TIME [epoch: 7.77 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07064827046101894		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.07064827046101894 | validation: 0.06556280203225393]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07144130435609995		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.07144130435609995 | validation: 0.06873292288688188]
	TIME [epoch: 7.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06687885637741686		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.06687885637741686 | validation: 0.07421276148175387]
	TIME [epoch: 7.75 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07056708043013536		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.07056708043013536 | validation: 0.07846617534378908]
	TIME [epoch: 7.78 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07587250490982028		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.07587250490982028 | validation: 0.07536851565097206]
	TIME [epoch: 7.77 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07163764156278404		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.07163764156278404 | validation: 0.06756266394997984]
	TIME [epoch: 7.76 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06681122185153934		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.06681122185153934 | validation: 0.07289558564853289]
	TIME [epoch: 7.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0721516208436223		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.0721516208436223 | validation: 0.06974257684693339]
	TIME [epoch: 7.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07511974426932597		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.07511974426932597 | validation: 0.0694099453085843]
	TIME [epoch: 7.79 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06877536179039004		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.06877536179039004 | validation: 0.07087172707056238]
	TIME [epoch: 7.77 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06705329278961575		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.06705329278961575 | validation: 0.0653372115273788]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06702099446659816		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.06702099446659816 | validation: 0.13202239710752053]
	TIME [epoch: 7.75 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08923608181415503		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.08923608181415503 | validation: 0.09595198138031]
	TIME [epoch: 7.75 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07998255988247262		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.07998255988247262 | validation: 0.08595646391256548]
	TIME [epoch: 7.8 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07432208108247931		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.07432208108247931 | validation: 0.09185259447211115]
	TIME [epoch: 7.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07324060687589933		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.07324060687589933 | validation: 0.07204159742594117]
	TIME [epoch: 7.76 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06990739759602879		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.06990739759602879 | validation: 0.07477499124563214]
	TIME [epoch: 7.75 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06741117020953602		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.06741117020953602 | validation: 0.06756754484467101]
	TIME [epoch: 7.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06485018528322448		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.06485018528322448 | validation: 0.09274002688435898]
	TIME [epoch: 7.8 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07349196383512809		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.07349196383512809 | validation: 0.09297438181785511]
	TIME [epoch: 7.75 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0806512799887866		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.0806512799887866 | validation: 0.07106350979856182]
	TIME [epoch: 7.75 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07054301149073323		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.07054301149073323 | validation: 0.07230186150161406]
	TIME [epoch: 7.75 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640939489532505		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.06640939489532505 | validation: 0.07176300564174862]
	TIME [epoch: 7.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0678919566260881		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.0678919566260881 | validation: 0.07744165125872195]
	TIME [epoch: 7.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07626496507002609		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.07626496507002609 | validation: 0.07344399778640757]
	TIME [epoch: 7.76 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15126621699072978		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.15126621699072978 | validation: 0.1076945967149667]
	TIME [epoch: 7.75 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08491777629165148		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.08491777629165148 | validation: 0.0936269564668171]
	TIME [epoch: 7.75 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07586724852442345		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.07586724852442345 | validation: 0.08317847417537239]
	TIME [epoch: 7.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06986424590297668		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.06986424590297668 | validation: 0.07411802570540474]
	TIME [epoch: 7.79 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06874039544549912		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.06874039544549912 | validation: 0.09121945416052198]
	TIME [epoch: 7.76 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07147142304322426		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.07147142304322426 | validation: 0.07456653537860371]
	TIME [epoch: 7.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07060372784746798		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.07060372784746798 | validation: 0.07313329558887724]
	TIME [epoch: 7.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07338270828471735		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.07338270828471735 | validation: 0.0744911690087983]
	TIME [epoch: 7.76 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07295178916001008		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.07295178916001008 | validation: 0.08277661586097865]
	TIME [epoch: 7.79 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0706633357551583		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.0706633357551583 | validation: 0.07876204079437327]
	TIME [epoch: 7.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09164983883993005		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.09164983883993005 | validation: 0.10600545074026238]
	TIME [epoch: 7.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08670994629681272		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.08670994629681272 | validation: 0.0811428898917534]
	TIME [epoch: 7.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0721031798331377		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.0721031798331377 | validation: 0.07728751567654832]
	TIME [epoch: 7.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06927115787732856		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.06927115787732856 | validation: 0.07253201346355152]
	TIME [epoch: 7.79 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07474867698338247		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.07474867698338247 | validation: 0.06987225347616301]
	TIME [epoch: 7.76 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07164022247543475		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.07164022247543475 | validation: 0.06715807793262604]
	TIME [epoch: 7.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06536966545714079		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.06536966545714079 | validation: 0.06934927210318229]
	TIME [epoch: 7.75 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0634559619593546		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.0634559619593546 | validation: 0.0702958563695565]
	TIME [epoch: 7.76 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07273212802471275		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.07273212802471275 | validation: 0.0783549460341135]
	TIME [epoch: 7.79 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07217721232107156		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.07217721232107156 | validation: 0.07186309056529419]
	TIME [epoch: 7.74 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06579580937553005		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.06579580937553005 | validation: 0.07726010096468734]
	TIME [epoch: 7.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0710648483848231		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.0710648483848231 | validation: 0.07688157109010982]
	TIME [epoch: 7.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08006687286230121		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.08006687286230121 | validation: 0.07538280699864232]
	TIME [epoch: 7.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07395674970955003		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.07395674970955003 | validation: 0.08145792328553941]
	TIME [epoch: 7.79 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06747787715037849		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.06747787715037849 | validation: 0.06790713143684375]
	TIME [epoch: 7.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07069009932594476		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.07069009932594476 | validation: 0.07225728508809268]
	TIME [epoch: 7.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07742528842402757		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.07742528842402757 | validation: 0.08383655665711232]
	TIME [epoch: 7.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08075498912524334		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.08075498912524334 | validation: 0.07761530673242625]
	TIME [epoch: 7.77 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08009304701995916		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.08009304701995916 | validation: 0.07300491955398128]
	TIME [epoch: 7.78 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06996765890336248		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.06996765890336248 | validation: 0.07052434448821776]
	TIME [epoch: 7.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06361528583139042		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.06361528583139042 | validation: 0.06887566602629483]
	TIME [epoch: 7.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06660934523877357		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.06660934523877357 | validation: 0.06470774904171187]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06233886361286394		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.06233886361286394 | validation: 0.06700176761965192]
	TIME [epoch: 7.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.067136269437269		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.067136269437269 | validation: 0.0743536756013658]
	TIME [epoch: 7.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06938247356120204		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.06938247356120204 | validation: 0.06947598160364231]
	TIME [epoch: 7.71 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07647415499649104		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.07647415499649104 | validation: 0.08092699105972631]
	TIME [epoch: 7.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07340787118583128		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.07340787118583128 | validation: 0.07128653350852407]
	TIME [epoch: 7.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06789084594182507		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.06789084594182507 | validation: 0.06602692033632407]
	TIME [epoch: 7.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06290334071398643		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.06290334071398643 | validation: 0.06596603666838777]
	TIME [epoch: 7.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06342191789171396		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.06342191789171396 | validation: 0.06519564046712982]
	TIME [epoch: 7.69 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06534504511122328		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.06534504511122328 | validation: 0.08031040919662111]
	TIME [epoch: 7.71 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06940214846242425		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.06940214846242425 | validation: 0.06796301887560216]
	TIME [epoch: 7.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06433454064607355		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.06433454064607355 | validation: 0.06393480646344243]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06500277142258488		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.06500277142258488 | validation: 0.07647492755961348]
	TIME [epoch: 7.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06912618783192437		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.06912618783192437 | validation: 0.06740893134569133]
	TIME [epoch: 7.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12300802055414671		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.12300802055414671 | validation: 0.08191376048210805]
	TIME [epoch: 7.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07758377661055219		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.07758377661055219 | validation: 0.08256892511749828]
	TIME [epoch: 7.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07013137941318914		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.07013137941318914 | validation: 0.07086521958330723]
	TIME [epoch: 7.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06602177859399178		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.06602177859399178 | validation: 0.06529146179807968]
	TIME [epoch: 7.72 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06345785774360425		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.06345785774360425 | validation: 0.06625723559095188]
	TIME [epoch: 7.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336634100097742		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.06336634100097742 | validation: 0.06869022079908677]
	TIME [epoch: 7.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06392293878347871		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.06392293878347871 | validation: 0.0652619638340353]
	TIME [epoch: 7.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06349417063817372		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.06349417063817372 | validation: 0.06826228868998196]
	TIME [epoch: 7.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06578800037272313		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.06578800037272313 | validation: 0.07159770813876312]
	TIME [epoch: 7.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0647630635760736		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0647630635760736 | validation: 0.06888048194689636]
	TIME [epoch: 7.71 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388862613242795		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.06388862613242795 | validation: 0.0717808836105436]
	TIME [epoch: 7.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06914064295220818		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.06914064295220818 | validation: 0.08960545600389472]
	TIME [epoch: 7.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07941694265341864		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.07941694265341864 | validation: 0.06671033473077945]
	TIME [epoch: 7.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06856141593101589		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.06856141593101589 | validation: 0.06520535781146694]
	TIME [epoch: 7.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06467912947032921		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.06467912947032921 | validation: 0.06419419258054695]
	TIME [epoch: 7.72 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0671305238954664		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.0671305238954664 | validation: 0.0678472861906449]
	TIME [epoch: 7.71 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06853888868911977		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.06853888868911977 | validation: 0.061272656622953656]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0805100818426765		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.0805100818426765 | validation: 0.1064999773196953]
	TIME [epoch: 7.78 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10154206466208611		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.10154206466208611 | validation: 0.0940329702543811]
	TIME [epoch: 7.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08270934744611154		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.08270934744611154 | validation: 0.08234438915009304]
	TIME [epoch: 7.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07264400960210018		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.07264400960210018 | validation: 0.07856075049071187]
	TIME [epoch: 7.74 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07588246145257116		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.07588246145257116 | validation: 0.0689400996485002]
	TIME [epoch: 7.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655592106614463		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0655592106614463 | validation: 0.06386577260583418]
	TIME [epoch: 7.77 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06532583390649833		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.06532583390649833 | validation: 0.08452287172051345]
	TIME [epoch: 7.74 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07510925108611399		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.07510925108611399 | validation: 0.06478168492171738]
	TIME [epoch: 7.73 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06854300288796637		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.06854300288796637 | validation: 0.06088575907474171]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06512331694752437		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.06512331694752437 | validation: 0.0714944110340039]
	TIME [epoch: 7.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06573509496952157		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.06573509496952157 | validation: 0.06118124027906441]
	TIME [epoch: 7.8 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06554367286512439		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.06554367286512439 | validation: 0.06589408229881799]
	TIME [epoch: 7.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06625868652357386		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.06625868652357386 | validation: 0.06635445962891133]
	TIME [epoch: 7.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09075550847974802		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.09075550847974802 | validation: 0.09853838303014428]
	TIME [epoch: 7.73 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08295468385097177		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.08295468385097177 | validation: 0.0759976888919209]
	TIME [epoch: 7.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07410447986173294		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.07410447986173294 | validation: 0.0721907345995649]
	TIME [epoch: 7.79 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06686310664266265		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.06686310664266265 | validation: 0.06994722265963885]
	TIME [epoch: 7.74 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06475591463371741		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.06475591463371741 | validation: 0.07221267163549062]
	TIME [epoch: 7.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06390962720419405		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.06390962720419405 | validation: 0.07109219880021891]
	TIME [epoch: 7.74 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07141610430596657		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.07141610430596657 | validation: 0.07291658986383302]
	TIME [epoch: 7.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09134985990495496		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.09134985990495496 | validation: 0.11951341999451537]
	TIME [epoch: 7.79 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11243440393081465		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.11243440393081465 | validation: 0.08937961152070062]
	TIME [epoch: 7.73 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08442629576826727		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.08442629576826727 | validation: 0.07090298066560163]
	TIME [epoch: 7.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07543909645878108		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.07543909645878108 | validation: 0.06786261273949287]
	TIME [epoch: 7.73 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06778594912846411		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.06778594912846411 | validation: 0.0645980505382227]
	TIME [epoch: 7.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06532509088386743		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.06532509088386743 | validation: 0.05962183244899581]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06462911814589814		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.06462911814589814 | validation: 0.06507193661616598]
	TIME [epoch: 7.74 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06323889179495204		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.06323889179495204 | validation: 0.06438026347560794]
	TIME [epoch: 7.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06287620339931477		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.06287620339931477 | validation: 0.06938023465611214]
	TIME [epoch: 7.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06399695747389811		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.06399695747389811 | validation: 0.07015680095494195]
	TIME [epoch: 7.76 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0633555298912045		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.0633555298912045 | validation: 0.06327390491742442]
	TIME [epoch: 7.79 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06413178391320941		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.06413178391320941 | validation: 0.10939107986616689]
	TIME [epoch: 7.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08067736354830017		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.08067736354830017 | validation: 0.07480632946994137]
	TIME [epoch: 7.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0696076168170919		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.0696076168170919 | validation: 0.06925287481337683]
	TIME [epoch: 7.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06403581990787174		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.06403581990787174 | validation: 0.06558226876342882]
	TIME [epoch: 7.74 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06192772300418717		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.06192772300418717 | validation: 0.06417173214195218]
	TIME [epoch: 7.8 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06391256948302271		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.06391256948302271 | validation: 0.062248354829599714]
	TIME [epoch: 7.75 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06614915053409312		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.06614915053409312 | validation: 0.06296185067160945]
	TIME [epoch: 7.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06396320139205212		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.06396320139205212 | validation: 0.06280252723142206]
	TIME [epoch: 7.75 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06433581804697958		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.06433581804697958 | validation: 0.06805403557437248]
	TIME [epoch: 7.76 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07040143470751861		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.07040143470751861 | validation: 0.06505084674452445]
	TIME [epoch: 7.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06521193338218056		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.06521193338218056 | validation: 0.07028655540982104]
	TIME [epoch: 7.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06592814213064682		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.06592814213064682 | validation: 0.06500686904188425]
	TIME [epoch: 7.75 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0665238539478839		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.0665238539478839 | validation: 0.06656743103147184]
	TIME [epoch: 7.74 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06497219657425896		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.06497219657425896 | validation: 0.0686465690844584]
	TIME [epoch: 7.76 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07310740376763569		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.07310740376763569 | validation: 0.06088945702937264]
	TIME [epoch: 7.79 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06345319852824685		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.06345319852824685 | validation: 0.09025753458118166]
	TIME [epoch: 7.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06962392494008587		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.06962392494008587 | validation: 0.06420247311039283]
	TIME [epoch: 7.75 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06477743546909927		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.06477743546909927 | validation: 0.07101844704990386]
	TIME [epoch: 7.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388448981435826		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.06388448981435826 | validation: 0.06492855818289847]
	TIME [epoch: 7.78 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06451769093407697		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.06451769093407697 | validation: 0.06418719339752045]
	TIME [epoch: 7.77 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06189504372167448		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.06189504372167448 | validation: 0.06254956005215248]
	TIME [epoch: 7.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06187503160279435		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.06187503160279435 | validation: 0.06618327846316086]
	TIME [epoch: 7.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06183648119124498		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.06183648119124498 | validation: 0.06697431262851256]
	TIME [epoch: 7.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.065663122428593		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.065663122428593 | validation: 0.06730686394522858]
	TIME [epoch: 7.78 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06363096053050539		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.06363096053050539 | validation: 0.06436225545838462]
	TIME [epoch: 7.77 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0627493687014824		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.0627493687014824 | validation: 0.06191140371497797]
	TIME [epoch: 7.76 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062479496857096		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.062479496857096 | validation: 0.07033866888566574]
	TIME [epoch: 7.74 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06340255121716812		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.06340255121716812 | validation: 0.0641479854669664]
	TIME [epoch: 7.75 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0654201909849474		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.0654201909849474 | validation: 0.06998710924956938]
	TIME [epoch: 7.77 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06709465782555002		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.06709465782555002 | validation: 0.06759005873414989]
	TIME [epoch: 7.77 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07567714926177668		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.07567714926177668 | validation: 0.067080676525568]
	TIME [epoch: 7.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06353427830775044		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.06353427830775044 | validation: 0.0636804674702365]
	TIME [epoch: 7.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06272431107298014		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.06272431107298014 | validation: 0.06768558895166775]
	TIME [epoch: 7.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06247178407479796		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.06247178407479796 | validation: 0.06685650773296489]
	TIME [epoch: 7.77 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06313920992969503		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.06313920992969503 | validation: 0.06382859522008946]
	TIME [epoch: 7.79 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06649166954878989		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.06649166954878989 | validation: 0.0684511547722587]
	TIME [epoch: 7.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06480752651182024		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.06480752651182024 | validation: 0.07339515663078028]
	TIME [epoch: 7.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07035907176029293		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.07035907176029293 | validation: 0.07936038576776722]
	TIME [epoch: 7.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07145579213521713		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.07145579213521713 | validation: 0.122039003215552]
	TIME [epoch: 7.78 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09659995555836459		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.09659995555836459 | validation: 0.08501867811788501]
	TIME [epoch: 7.77 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07345571171013802		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.07345571171013802 | validation: 0.07681429390014909]
	TIME [epoch: 7.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07170099022378415		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.07170099022378415 | validation: 0.0700433148431025]
	TIME [epoch: 7.76 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0681257097192724		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.0681257097192724 | validation: 0.0702471596621229]
	TIME [epoch: 7.75 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06683850789219449		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.06683850789219449 | validation: 0.07312814579353905]
	TIME [epoch: 7.79 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06557560129750568		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.06557560129750568 | validation: 0.0693376218562538]
	TIME [epoch: 7.77 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07242024152003154		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.07242024152003154 | validation: 0.10374221905337563]
	TIME [epoch: 7.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08961174436899207		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.08961174436899207 | validation: 0.10477674417215344]
	TIME [epoch: 7.76 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0800816113002323		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.0800816113002323 | validation: 0.08370472847450999]
	TIME [epoch: 7.75 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07134497836286527		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.07134497836286527 | validation: 0.07967112348037059]
	TIME [epoch: 7.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07105601628459704		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.07105601628459704 | validation: 0.08127252808021078]
	TIME [epoch: 7.76 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06952023658437527		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.06952023658437527 | validation: 0.0748578069942101]
	TIME [epoch: 7.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06801713657469394		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.06801713657469394 | validation: 0.08167110179593265]
	TIME [epoch: 7.75 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06989126534000165		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.06989126534000165 | validation: 0.0688388170273624]
	TIME [epoch: 7.75 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06737405114669844		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.06737405114669844 | validation: 0.06969473719994523]
	TIME [epoch: 7.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06424516026352206		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.06424516026352206 | validation: 0.070911715415508]
	TIME [epoch: 7.75 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07034468540037152		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.07034468540037152 | validation: 0.07057813133501466]
	TIME [epoch: 7.75 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06507734284497074		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.06507734284497074 | validation: 0.0702500026211347]
	TIME [epoch: 7.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06339279871381412		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.06339279871381412 | validation: 0.06714990680034649]
	TIME [epoch: 7.75 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06396444423774979		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.06396444423774979 | validation: 0.06694162818851356]
	TIME [epoch: 7.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06268609708263054		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.06268609708263054 | validation: 0.06508369468967995]
	TIME [epoch: 7.76 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061915567232971175		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.061915567232971175 | validation: 0.0660763760130209]
	TIME [epoch: 7.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07255610694765635		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.07255610694765635 | validation: 0.08120285038621657]
	TIME [epoch: 7.75 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06900302180792683		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.06900302180792683 | validation: 0.07113443802620528]
	TIME [epoch: 7.75 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06524761333965799		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.06524761333965799 | validation: 0.0713670784841846]
	TIME [epoch: 7.82 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06394913102091779		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.06394913102091779 | validation: 0.06649413706546545]
	TIME [epoch: 7.76 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06688580547921254		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.06688580547921254 | validation: 0.06654487434487424]
	TIME [epoch: 7.76 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06153832837632762		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.06153832837632762 | validation: 0.06294902527774274]
	TIME [epoch: 7.75 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0607683165065645		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.0607683165065645 | validation: 0.06277071615799357]
	TIME [epoch: 7.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06245597794780797		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.06245597794780797 | validation: 0.06717524643621525]
	TIME [epoch: 7.81 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06594046936697037		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.06594046936697037 | validation: 0.06413103488702525]
	TIME [epoch: 7.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06391405723580589		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.06391405723580589 | validation: 0.06853559180725323]
	TIME [epoch: 7.75 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06468070743854715		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.06468070743854715 | validation: 0.07032861293559024]
	TIME [epoch: 7.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.065096539546844		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.065096539546844 | validation: 0.06926340287486733]
	TIME [epoch: 7.76 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06091840567272054		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.06091840567272054 | validation: 0.06016742121915394]
	TIME [epoch: 7.81 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06275181491790312		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.06275181491790312 | validation: 0.06389706230129863]
	TIME [epoch: 7.76 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06409685376628832		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.06409685376628832 | validation: 0.06362052740619624]
	TIME [epoch: 7.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06344419685171154		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.06344419685171154 | validation: 0.06998929945092545]
	TIME [epoch: 7.76 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06219401941342051		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.06219401941342051 | validation: 0.09800563979674337]
	TIME [epoch: 7.76 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07790672022761184		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.07790672022761184 | validation: 0.0680326071155567]
	TIME [epoch: 7.81 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06426513428693958		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.06426513428693958 | validation: 0.05987967586869661]
	TIME [epoch: 7.75 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061618209618837815		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.061618209618837815 | validation: 0.06378613927008499]
	TIME [epoch: 7.75 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060863758851511976		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.060863758851511976 | validation: 0.061566760890445764]
	TIME [epoch: 7.76 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06273912038809915		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.06273912038809915 | validation: 0.06422653523146571]
	TIME [epoch: 7.76 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06160426372171293		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.06160426372171293 | validation: 0.1020301534498301]
	TIME [epoch: 7.81 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07454009702161267		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.07454009702161267 | validation: 0.07502051957836421]
	TIME [epoch: 7.76 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07089405826419541		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.07089405826419541 | validation: 0.07289924203345102]
	TIME [epoch: 7.76 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06957611422151519		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.06957611422151519 | validation: 0.06415190744704252]
	TIME [epoch: 7.77 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06169239409463391		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.06169239409463391 | validation: 0.06411013000368171]
	TIME [epoch: 7.77 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06178043652388017		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.06178043652388017 | validation: 0.059115264122451924]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05968788953930466		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.05968788953930466 | validation: 0.06070418194124932]
	TIME [epoch: 7.76 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059168205741802433		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.059168205741802433 | validation: 0.05974081164370071]
	TIME [epoch: 7.77 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07044062562381767		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.07044062562381767 | validation: 0.06785581741459099]
	TIME [epoch: 7.77 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06944076195429406		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.06944076195429406 | validation: 0.06290007858332719]
	TIME [epoch: 7.77 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06467345636167043		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.06467345636167043 | validation: 0.08185488443712229]
	TIME [epoch: 7.81 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0729274013158076		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0729274013158076 | validation: 0.06569289051216268]
	TIME [epoch: 7.77 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0636686296658221		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0636686296658221 | validation: 0.06283500730755957]
	TIME [epoch: 7.76 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06113212811255052		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.06113212811255052 | validation: 0.05769687739735368]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062340058289231406		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.062340058289231406 | validation: 0.07686021158484234]
	TIME [epoch: 7.79 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07064646988721625		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.07064646988721625 | validation: 0.07022811148888072]
	TIME [epoch: 7.78 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06593976667763737		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.06593976667763737 | validation: 0.06808785689825583]
	TIME [epoch: 7.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06290797173512558		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.06290797173512558 | validation: 0.06887255688704619]
	TIME [epoch: 7.76 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0646110625106353		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.0646110625106353 | validation: 0.060736033782367485]
	TIME [epoch: 7.76 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06089833405107112		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.06089833405107112 | validation: 0.06376797305610295]
	TIME [epoch: 7.79 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07005482934380958		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.07005482934380958 | validation: 0.07858344934498378]
	TIME [epoch: 7.78 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07814133945184551		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.07814133945184551 | validation: 0.06260111497640551]
	TIME [epoch: 7.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0731015731971286		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0731015731971286 | validation: 0.06268476884789985]
	TIME [epoch: 7.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06724605772058344		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.06724605772058344 | validation: 0.06034506452106539]
	TIME [epoch: 7.76 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06297998042530517		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.06297998042530517 | validation: 0.05855582498617498]
	TIME [epoch: 7.78 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060428071135588055		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.060428071135588055 | validation: 0.06306167198039597]
	TIME [epoch: 7.79 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06150301269259578		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.06150301269259578 | validation: 0.060596788943517645]
	TIME [epoch: 7.76 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060404144501731635		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.060404144501731635 | validation: 0.06126163128666883]
	TIME [epoch: 7.76 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0632349601966886		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0632349601966886 | validation: 0.06617382733057736]
	TIME [epoch: 7.76 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06453143167680808		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.06453143167680808 | validation: 0.06386019534086704]
	TIME [epoch: 7.8 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0647967674296219		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0647967674296219 | validation: 0.057817267590598936]
	TIME [epoch: 7.79 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060422929073467635		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.060422929073467635 | validation: 0.058614539396297655]
	TIME [epoch: 7.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058153543923608685		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.058153543923608685 | validation: 0.0595271167599457]
	TIME [epoch: 7.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05975190647275801		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.05975190647275801 | validation: 0.05892513076446158]
	TIME [epoch: 7.76 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06333592861881182		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.06333592861881182 | validation: 0.06045576043071935]
	TIME [epoch: 7.81 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0615877492958274		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0615877492958274 | validation: 0.06011731264948371]
	TIME [epoch: 7.76 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06512222543780535		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.06512222543780535 | validation: 0.06759127120881023]
	TIME [epoch: 7.75 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07511402329113746		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.07511402329113746 | validation: 0.07301210895924519]
	TIME [epoch: 7.75 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07207852502811225		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.07207852502811225 | validation: 0.06915712822586735]
	TIME [epoch: 7.75 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0795695529331308		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0795695529331308 | validation: 0.09621008695570031]
	TIME [epoch: 7.81 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0869553925009457		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.0869553925009457 | validation: 0.08176435729165897]
	TIME [epoch: 7.75 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07867069673503277		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.07867069673503277 | validation: 0.08473504656138474]
	TIME [epoch: 7.76 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07812986517289155		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.07812986517289155 | validation: 0.07820094635073052]
	TIME [epoch: 7.77 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07772056055619554		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.07772056055619554 | validation: 0.07273755215239011]
	TIME [epoch: 7.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07198264378197153		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.07198264378197153 | validation: 0.07602700520408891]
	TIME [epoch: 7.8 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06925083496229728		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.06925083496229728 | validation: 0.0718681154891261]
	TIME [epoch: 7.76 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07168227835068906		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.07168227835068906 | validation: 0.06767442974383112]
	TIME [epoch: 7.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06909777634573101		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.06909777634573101 | validation: 0.0680135039454984]
	TIME [epoch: 7.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06684225721473554		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.06684225721473554 | validation: 0.06776658161238483]
	TIME [epoch: 7.73 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06442371849656778		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.06442371849656778 | validation: 0.062391568390472274]
	TIME [epoch: 7.79 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06441634430909231		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.06441634430909231 | validation: 0.062308772982351955]
	TIME [epoch: 7.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06353291257674641		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.06353291257674641 | validation: 0.06281990094798495]
	TIME [epoch: 7.74 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06422416791216644		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.06422416791216644 | validation: 0.06558632745232802]
	TIME [epoch: 7.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06393446949893555		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.06393446949893555 | validation: 0.08334751975048134]
	TIME [epoch: 7.75 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07429464553619093		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.07429464553619093 | validation: 0.06726324624494233]
	TIME [epoch: 7.78 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06697366035702927		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.06697366035702927 | validation: 0.060873352605793775]
	TIME [epoch: 7.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06497688922138786		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.06497688922138786 | validation: 0.06498482445339156]
	TIME [epoch: 7.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06618725673093166		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.06618725673093166 | validation: 0.05783293999874009]
	TIME [epoch: 7.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061255461492162655		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.061255461492162655 | validation: 0.05554530274466239]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240523_102345/states/model_phi1_1a_v_mmd1_fix_noise_large_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057701717275199176		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.057701717275199176 | validation: 0.059036725646598065]
	TIME [epoch: 7.79 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05985091698802013		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.05985091698802013 | validation: 0.058413191202307216]
	TIME [epoch: 7.75 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05973373071097657		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.05973373071097657 | validation: 0.06346560008476002]
	TIME [epoch: 7.74 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060247401345185234		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.060247401345185234 | validation: 0.059631099849368846]
	TIME [epoch: 7.74 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06218842105123101		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.06218842105123101 | validation: 0.0626060735556445]
	TIME [epoch: 7.75 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06250756825098823		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.06250756825098823 | validation: 0.06460966373312187]
	TIME [epoch: 7.78 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06132866687220516		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.06132866687220516 | validation: 0.06009819068906061]
	TIME [epoch: 7.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06043021302508003		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.06043021302508003 | validation: 0.06071678358565717]
	TIME [epoch: 7.74 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05927373242614468		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.05927373242614468 | validation: 0.06180473821971644]
	TIME [epoch: 7.75 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06102277297600882		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.06102277297600882 | validation: 0.0582049058551333]
	TIME [epoch: 7.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06192286180776105		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.06192286180776105 | validation: 0.06420036098978356]
	TIME [epoch: 7.79 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060022717667969586		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.060022717667969586 | validation: 0.05773656682574729]
	TIME [epoch: 7.74 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06093730127879442		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.06093730127879442 | validation: 0.057638216687219704]
	TIME [epoch: 7.74 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06565429800876117		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.06565429800876117 | validation: 0.06487761048330912]
	TIME [epoch: 7.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06198104115633664		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.06198104115633664 | validation: 0.05782361832282302]
	TIME [epoch: 7.76 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06012955484761087		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.06012955484761087 | validation: 0.06152174007262476]
	TIME [epoch: 7.79 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0644251939732084		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0644251939732084 | validation: 0.06325437025159186]
	TIME [epoch: 7.76 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06533219064712385		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.06533219064712385 | validation: 0.059763118802804514]
	TIME [epoch: 7.74 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06011852408895686		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.06011852408895686 | validation: 0.06399470566959341]
	TIME [epoch: 7.74 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06363668374888891		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.06363668374888891 | validation: 0.06097570037806657]
	TIME [epoch: 7.75 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059978434374033135		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.059978434374033135 | validation: 0.0648163944621191]
	TIME [epoch: 7.78 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06194302120872624		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.06194302120872624 | validation: 0.060029594990356254]
	TIME [epoch: 7.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0625831019984785		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0625831019984785 | validation: 0.0700025606501512]
	TIME [epoch: 7.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06635457283873492		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.06635457283873492 | validation: 0.06835367570106185]
	TIME [epoch: 7.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06481312581575438		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.06481312581575438 | validation: 0.06760389024174518]
	TIME [epoch: 7.76 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06389504498718497		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.06389504498718497 | validation: 0.06781265034222274]
	TIME [epoch: 7.78 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06331629630397198		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.06331629630397198 | validation: 0.06251546693200233]
	TIME [epoch: 7.74 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060443945762876455		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.060443945762876455 | validation: 0.06194049123977208]
	TIME [epoch: 7.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06492133853756535		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.06492133853756535 | validation: 0.06796791615844838]
	TIME [epoch: 7.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06600350514756059		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.06600350514756059 | validation: 0.06985141745606992]
	TIME [epoch: 7.76 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06458988230382841		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.06458988230382841 | validation: 0.0705295485044575]
	TIME [epoch: 7.78 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06435386355109497		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.06435386355109497 | validation: 0.06626939374643727]
	TIME [epoch: 7.75 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06409536720471917		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.06409536720471917 | validation: 0.0670431666001961]
	TIME [epoch: 7.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06451481740810674		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.06451481740810674 | validation: 0.06442491149653352]
	TIME [epoch: 7.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06474874417927909		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.06474874417927909 | validation: 0.06797274917379073]
	TIME [epoch: 7.77 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0632172068355064		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0632172068355064 | validation: 0.06787509234983594]
	TIME [epoch: 7.76 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08795937733283783		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.08795937733283783 | validation: 0.09867610586219785]
	TIME [epoch: 7.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08000594543017735		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.08000594543017735 | validation: 0.0733873575965221]
	TIME [epoch: 7.74 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06785099130323811		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.06785099130323811 | validation: 0.07054328692167633]
	TIME [epoch: 7.75 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06569616623569689		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.06569616623569689 | validation: 0.0702525996366262]
	TIME [epoch: 7.78 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655572994734325		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0655572994734325 | validation: 0.06686144504699063]
	TIME [epoch: 7.78 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06447281264096813		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.06447281264096813 | validation: 0.06516018564006978]
	TIME [epoch: 7.75 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06342657059698686		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.06342657059698686 | validation: 0.06985989386773364]
	TIME [epoch: 7.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06281686930980761		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.06281686930980761 | validation: 0.06682538182809285]
	TIME [epoch: 7.76 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06295727992150824		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.06295727992150824 | validation: 0.0671347008166153]
	TIME [epoch: 7.78 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06385569957354154		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.06385569957354154 | validation: 0.06533773377699621]
	TIME [epoch: 7.78 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06334747081911789		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.06334747081911789 | validation: 0.07022064346584667]
	TIME [epoch: 7.76 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06413837682057795		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.06413837682057795 | validation: 0.06070523862802947]
	TIME [epoch: 7.75 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06335343010975186		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.06335343010975186 | validation: 0.07052842158688592]
	TIME [epoch: 7.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0618081991409401		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0618081991409401 | validation: 0.0671440208743196]
	TIME [epoch: 7.78 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0641169333412826		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.0641169333412826 | validation: 0.06449397341861077]
	TIME [epoch: 7.78 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06307422943418439		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.06307422943418439 | validation: 0.06569882535303481]
	TIME [epoch: 7.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060180463148068124		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.060180463148068124 | validation: 0.059556647121353855]
	TIME [epoch: 7.75 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06254630498025479		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.06254630498025479 | validation: 0.0675693690329873]
	TIME [epoch: 7.75 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061519097376030396		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.061519097376030396 | validation: 0.06556333841640359]
	TIME [epoch: 7.78 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06034218092053441		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.06034218092053441 | validation: 0.06297206534658534]
	TIME [epoch: 7.77 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061157912827828884		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.061157912827828884 | validation: 0.0636339958148332]
	TIME [epoch: 7.75 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0622462967493422		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0622462967493422 | validation: 0.06271187926620118]
	TIME [epoch: 7.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06181925256079015		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.06181925256079015 | validation: 0.06522489396731546]
	TIME [epoch: 7.75 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062148298662308275		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.062148298662308275 | validation: 0.06387488303773284]
	TIME [epoch: 7.79 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06308919265056694		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.06308919265056694 | validation: 0.06685568614758768]
	TIME [epoch: 7.77 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06491167745929356		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.06491167745929356 | validation: 0.0630091043182269]
	TIME [epoch: 7.75 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06342021868338883		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.06342021868338883 | validation: 0.06497033285803741]
	TIME [epoch: 7.75 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06408755521107887		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.06408755521107887 | validation: 0.060623061621076]
	TIME [epoch: 7.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06447483771064613		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.06447483771064613 | validation: 0.15188487080317672]
	TIME [epoch: 7.79 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09016951660253257		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.09016951660253257 | validation: 0.07794790651251064]
	TIME [epoch: 7.76 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07665130052148031		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.07665130052148031 | validation: 0.07481691230005075]
	TIME [epoch: 7.74 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07040805771029139		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.07040805771029139 | validation: 0.06863627062154727]
	TIME [epoch: 7.76 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06862714043153138		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.06862714043153138 | validation: 0.07061204049797232]
	TIME [epoch: 7.75 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0691216141999591		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.0691216141999591 | validation: 0.06914994273959624]
	TIME [epoch: 7.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0683398287570038		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0683398287570038 | validation: 0.07083744824466329]
	TIME [epoch: 7.76 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06442467324234608		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.06442467324234608 | validation: 0.07069413356732615]
	TIME [epoch: 7.74 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06379001303167119		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.06379001303167119 | validation: 0.06385463682947673]
	TIME [epoch: 7.76 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06390873357401196		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.06390873357401196 | validation: 0.0662553594146498]
	TIME [epoch: 7.76 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06452198937494752		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.06452198937494752 | validation: 0.06470660685876171]
	TIME [epoch: 7.81 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06292449019137922		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.06292449019137922 | validation: 0.06419047648783867]
	TIME [epoch: 7.76 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060520286244788904		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.060520286244788904 | validation: 0.066384276512097]
	TIME [epoch: 7.76 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06089567593904962		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.06089567593904962 | validation: 0.06309978731712321]
	TIME [epoch: 7.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06017070289389716		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.06017070289389716 | validation: 0.05900228805774316]
	TIME [epoch: 7.76 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060150552037112726		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.060150552037112726 | validation: 0.06422054789924934]
	TIME [epoch: 7.82 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060455863521074835		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.060455863521074835 | validation: 0.07288958235021353]
	TIME [epoch: 7.76 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062299538143174744		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.062299538143174744 | validation: 0.06325457068732097]
	TIME [epoch: 7.75 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06034035753751939		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.06034035753751939 | validation: 0.06321866989610425]
	TIME [epoch: 7.76 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05873454399126026		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.05873454399126026 | validation: 0.061885818342468556]
	TIME [epoch: 7.76 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06555359360219916		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.06555359360219916 | validation: 0.07150885048938449]
	TIME [epoch: 7.81 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0636400133476992		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0636400133476992 | validation: 0.06359331013047698]
	TIME [epoch: 7.77 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06100661113232908		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.06100661113232908 | validation: 0.06570554745980886]
	TIME [epoch: 7.77 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06285401889180324		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.06285401889180324 | validation: 0.06471418645099106]
	TIME [epoch: 7.76 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06095043545441409		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.06095043545441409 | validation: 0.06467313501455724]
	TIME [epoch: 7.77 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06303350986855688		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.06303350986855688 | validation: 0.06918131636818897]
	TIME [epoch: 7.81 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061595199564521086		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.061595199564521086 | validation: 0.06327748081442264]
	TIME [epoch: 7.76 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0621032597343522		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0621032597343522 | validation: 0.06401704643881988]
	TIME [epoch: 7.76 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061227994144554466		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.061227994144554466 | validation: 0.06127145513093259]
	TIME [epoch: 7.76 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06164857357570726		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.06164857357570726 | validation: 0.09311866727771996]
	TIME [epoch: 7.77 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06727074428050286		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.06727074428050286 | validation: 0.06746249778541444]
	TIME [epoch: 7.81 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06386226576046564		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.06386226576046564 | validation: 0.06820845205052388]
	TIME [epoch: 7.77 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05954072642271197		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.05954072642271197 | validation: 0.06735757175738175]
	TIME [epoch: 7.76 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060661580068900184		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.060661580068900184 | validation: 0.06561781744561229]
	TIME [epoch: 7.77 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06015142019756893		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.06015142019756893 | validation: 0.060298288929122576]
	TIME [epoch: 7.78 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0629212841738853		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0629212841738853 | validation: 0.06263561007547083]
	TIME [epoch: 7.81 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06217560832264686		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.06217560832264686 | validation: 0.0611952507415797]
	TIME [epoch: 7.76 sec]
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7445.518 seconds.
