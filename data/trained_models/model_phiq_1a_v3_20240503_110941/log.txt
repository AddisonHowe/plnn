Args:
Namespace(name='model_phiq_1a_v3', outdir='out/model_training/model_phiq_1a_v3', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.01, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.75, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 404411141

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.848364759955974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.848364759955974 | validation: 7.743583014182727]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.562534657994761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.562534657994761 | validation: 7.531723762645109]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.250252795886411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.250252795886411 | validation: 7.371651695302827]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.797163192908637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.797163192908637 | validation: 7.031046987225121]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.493420079144488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.493420079144488 | validation: 6.692563402743453]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2021050071469075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2021050071469075 | validation: 6.386880333556135]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.89315879086222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.89315879086222 | validation: 5.939420861666101]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.427789528059007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.427789528059007 | validation: 5.162414652502701]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.10988124905195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.10988124905195 | validation: 5.0430011649504385]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.986856984097183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.986856984097183 | validation: 4.870764674759966]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.690542012227823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.690542012227823 | validation: 4.567719777854079]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.5910015533706545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5910015533706545 | validation: 4.628772820255021]
	TIME [epoch: 75.8 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.442783243478438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.442783243478438 | validation: 4.291697372908934]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.35228902057817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35228902057817 | validation: 4.234485549824576]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.268053380158468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.268053380158468 | validation: 4.165697869222493]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1621095470941505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1621095470941505 | validation: 4.08799337094629]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.09354229086223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.09354229086223 | validation: 4.048351090670783]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.037162125631929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.037162125631929 | validation: 3.9022650310190232]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0079960698228465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0079960698228465 | validation: 3.9185208203442574]
	TIME [epoch: 75.7 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.993608942572759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.993608942572759 | validation: 3.9394029050368014]
	TIME [epoch: 75.8 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9323094516492483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9323094516492483 | validation: 3.813129635255327]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.82872991502966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.82872991502966 | validation: 3.6996857636586045]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7885591907345666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7885591907345666 | validation: 3.6967241635709254]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.770951973963498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.770951973963498 | validation: 3.5914974756134876]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7319898777264626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7319898777264626 | validation: 3.578722119906412]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.665955470693264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.665955470693264 | validation: 3.4892508872311288]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.718953209866544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.718953209866544 | validation: 3.6298852214276023]
	TIME [epoch: 75.8 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7010739606886736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7010739606886736 | validation: 3.442738215666936]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.527626262889756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527626262889756 | validation: 3.352925733988079]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.435528196680764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.435528196680764 | validation: 3.252720765115022]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.289115708541286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.289115708541286 | validation: 3.5977034216956487]
	TIME [epoch: 75.8 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5909254006887537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5909254006887537 | validation: 3.9172604036899608]
	TIME [epoch: 75.8 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.521134257456243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.521134257456243 | validation: 3.5134760683823023]
	TIME [epoch: 75.8 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.344224361333059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344224361333059 | validation: 3.2450957804107357]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2219540127904898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2219540127904898 | validation: 3.186882212115718]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0939214593863813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0939214593863813 | validation: 3.6882815538948415]
	TIME [epoch: 75.8 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.074821785190389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074821785190389 | validation: 3.789777144779105]
	TIME [epoch: 75.8 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4418392797426254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4418392797426254 | validation: 3.3127448504057773]
	TIME [epoch: 75.8 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2425217967467073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2425217967467073 | validation: 3.1289563136033345]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0898541193320774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0898541193320774 | validation: 3.069256735867593]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7442604015999743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7442604015999743 | validation: 4.251796579243276]
	TIME [epoch: 75.8 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.231752596935919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.231752596935919 | validation: 3.696620688610529]
	TIME [epoch: 75.8 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5077121428106066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5077121428106066 | validation: 3.4151120379019595]
	TIME [epoch: 75.8 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3192615488885235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3192615488885235 | validation: 3.2893670257203658]
	TIME [epoch: 75.8 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2124247850573937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2124247850573937 | validation: 3.173963523046587]
	TIME [epoch: 75.8 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.235442908856337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.235442908856337 | validation: 3.097781430961079]
	TIME [epoch: 76.1 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0556379778156035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0556379778156035 | validation: 3.0217003617919826]
	TIME [epoch: 76.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1067402593270868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1067402593270868 | validation: 3.090968950087346]
	TIME [epoch: 76.3 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1373299251463695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1373299251463695 | validation: 2.9461381082231046]
	TIME [epoch: 76.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.184962233421733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.184962233421733 | validation: 3.0881890744109706]
	TIME [epoch: 76.2 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.058636630582934		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 3.058636630582934 | validation: 3.2599799992200165]
	TIME [epoch: 75.9 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.308217207486106		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 4.308217207486106 | validation: 5.984531018255097]
	TIME [epoch: 76.3 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.283452036163235		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 5.283452036163235 | validation: 4.305546304365164]
	TIME [epoch: 76 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.4023726095705085		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 4.4023726095705085 | validation: 4.195427715374517]
	TIME [epoch: 76.1 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.542234950628651		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 4.542234950628651 | validation: 3.217786408534147]
	TIME [epoch: 76.1 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1308986697451875		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 3.1308986697451875 | validation: 3.0335568852450665]
	TIME [epoch: 76.3 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9817402102846433		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 2.9817402102846433 | validation: 2.9314299808070774]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9052867852604605		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 2.9052867852604605 | validation: 2.783677513127757]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_58.pth
	Model improved!!!
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.845655966174463		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 2.845655966174463 | validation: 2.7907999977892533]
	TIME [epoch: 76.1 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7941842052424697		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 2.7941842052424697 | validation: 2.766134850101834]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7579455096123393		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 2.7579455096123393 | validation: 2.6305436907265323]
	TIME [epoch: 76.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7544463903402243		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 2.7544463903402243 | validation: 2.8000216657826837]
	TIME [epoch: 76.1 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.731619704919759		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 2.731619704919759 | validation: 2.5733030217595094]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_63.pth
	Model improved!!!
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4921451625983697		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 2.4921451625983697 | validation: 2.4222813276305013]
	TIME [epoch: 76.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_64.pth
	Model improved!!!
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5194247572883137		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 2.5194247572883137 | validation: 2.4327759447581876]
	TIME [epoch: 76 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3532555319541872		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 2.3532555319541872 | validation: 2.6943560223269793]
	TIME [epoch: 76.2 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.454569679530462		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 2.454569679530462 | validation: 2.3721355483808493]
	TIME [epoch: 76.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_67.pth
	Model improved!!!
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.498752296090965		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 2.498752296090965 | validation: 4.105664570846143]
	TIME [epoch: 75.9 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.074944858253637		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 3.074944858253637 | validation: 2.290297926340753]
	TIME [epoch: 76.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_69.pth
	Model improved!!!
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.235680684374661		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 2.235680684374661 | validation: 2.2128185681586916]
	TIME [epoch: 76.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_70.pth
	Model improved!!!
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.254056251858855		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 2.254056251858855 | validation: 2.1784904046346556]
	TIME [epoch: 76.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_71.pth
	Model improved!!!
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.111725145206769		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 2.111725145206769 | validation: 2.191317951236016]
	TIME [epoch: 76.4 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1408757743734577		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 2.1408757743734577 | validation: 2.3189221592840514]
	TIME [epoch: 76.5 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8806766351527653		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 2.8806766351527653 | validation: 2.9544553963507907]
	TIME [epoch: 76.3 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.610381842894589		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 2.610381842894589 | validation: 2.4102838642685493]
	TIME [epoch: 76.2 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.191682657529202		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 2.191682657529202 | validation: 2.1883090173526005]
	TIME [epoch: 76.1 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.93241799136439		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 2.93241799136439 | validation: 4.558702611713974]
	TIME [epoch: 76.2 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.734167806819829		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 3.734167806819829 | validation: 2.8612340409728123]
	TIME [epoch: 76.1 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6416556503554585		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 2.6416556503554585 | validation: 2.4550121014667123]
	TIME [epoch: 76.1 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3739129385033007		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 2.3739129385033007 | validation: 2.235234464086718]
	TIME [epoch: 76.2 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.217946029386191		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 2.217946029386191 | validation: 2.286817215050361]
	TIME [epoch: 76.3 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.264106128751769		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 2.264106128751769 | validation: 2.2240086069658873]
	TIME [epoch: 76.2 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1693984580258703		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 2.1693984580258703 | validation: 2.0741660427918935]
	TIME [epoch: 76.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_83.pth
	Model improved!!!
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9923880885613103		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 1.9923880885613103 | validation: 1.9582360540632124]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_84.pth
	Model improved!!!
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9492772916611811		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.9492772916611811 | validation: 2.5215788969691393]
	TIME [epoch: 76.1 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1242936154962786		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 2.1242936154962786 | validation: 2.3450561976536433]
	TIME [epoch: 76.1 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2340502888021865		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 2.2340502888021865 | validation: 2.4567008800261396]
	TIME [epoch: 76.1 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.442937643391684		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 2.442937643391684 | validation: 3.087349462194471]
	TIME [epoch: 76.2 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4165633566011873		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 2.4165633566011873 | validation: 2.8795169372809477]
	TIME [epoch: 76 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.861336367929161		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 2.861336367929161 | validation: 3.5892030707396985]
	TIME [epoch: 76 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3572433495610072		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 3.3572433495610072 | validation: 2.8396918754566363]
	TIME [epoch: 76.1 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4412688908206395		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 2.4412688908206395 | validation: 2.0268900864898827]
	TIME [epoch: 76 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9239222963590374		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.9239222963590374 | validation: 1.9278496922129822]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_93.pth
	Model improved!!!
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.986038986084285		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.986038986084285 | validation: 3.050169764879443]
	TIME [epoch: 76.1 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2027758533128825		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 2.2027758533128825 | validation: 1.7904261314146117]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9954299396562423		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 1.9954299396562423 | validation: 2.8176589594987718]
	TIME [epoch: 76 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1705623784122663		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 2.1705623784122663 | validation: 1.8021137378397096]
	TIME [epoch: 76.1 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5620419986377163		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 2.5620419986377163 | validation: 3.495228733642394]
	TIME [epoch: 76.1 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1855616005913614		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 3.1855616005913614 | validation: 3.3060263042007634]
	TIME [epoch: 76 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1040173279718872		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 3.1040173279718872 | validation: 2.828854810478882]
	TIME [epoch: 76.1 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7900132273401472		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 2.7900132273401472 | validation: 2.6081383864719827]
	TIME [epoch: 76 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.26877925603758		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 2.26877925603758 | validation: 3.102349358452587]
	TIME [epoch: 76 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.328860801069666		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 3.328860801069666 | validation: 2.609978652281005]
	TIME [epoch: 76.1 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.089165018354207		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 2.089165018354207 | validation: 1.907000572444629]
	TIME [epoch: 76 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7687298176818262		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 1.7687298176818262 | validation: 1.7687639670267576]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_105.pth
	Model improved!!!
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6899425693984997		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 1.6899425693984997 | validation: 1.9032054263839315]
	TIME [epoch: 76 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6891503728580557		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 1.6891503728580557 | validation: 1.736387760082104]
	TIME [epoch: 76 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_107.pth
	Model improved!!!
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5733641610173763		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 1.5733641610173763 | validation: 1.6853326050077837]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_108.pth
	Model improved!!!
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5590597847643832		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 1.5590597847643832 | validation: 1.8283939594652292]
	TIME [epoch: 76.1 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7614482078054303		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 1.7614482078054303 | validation: 1.6766421420304853]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_110.pth
	Model improved!!!
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6185136657536305		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 1.6185136657536305 | validation: 1.558976528638936]
	TIME [epoch: 76.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_111.pth
	Model improved!!!
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.570967082196757		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 1.570967082196757 | validation: 1.856783403112777]
	TIME [epoch: 76.1 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7859431322473298		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 1.7859431322473298 | validation: 1.6120170059840548]
	TIME [epoch: 76 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.407204579427131		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 1.407204579427131 | validation: 1.7929237515191123]
	TIME [epoch: 76.1 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4974166445562962		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 1.4974166445562962 | validation: 1.591231217167157]
	TIME [epoch: 76.1 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4454581098434476		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 1.4454581098434476 | validation: 1.4205959127258816]
	TIME [epoch: 76.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_116.pth
	Model improved!!!
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5728733030710988		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 1.5728733030710988 | validation: 2.0314977271840466]
	TIME [epoch: 76.1 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5344625769554505		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 1.5344625769554505 | validation: 1.454375856360425]
	TIME [epoch: 75.9 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6586391530062659		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 1.6586391530062659 | validation: 2.002272941231558]
	TIME [epoch: 75.9 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.657616235291636		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 1.657616235291636 | validation: 1.9940131124062819]
	TIME [epoch: 76 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8863488686826555		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 1.8863488686826555 | validation: 1.5415157849076477]
	TIME [epoch: 76.1 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4010872876614415		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 1.4010872876614415 | validation: 1.6631992380747553]
	TIME [epoch: 76.1 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.580597832222121		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 2.580597832222121 | validation: 2.8497696968301964]
	TIME [epoch: 76 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6886162298107887		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 2.6886162298107887 | validation: 1.8643752472387871]
	TIME [epoch: 76 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.787179524058913		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 1.787179524058913 | validation: 1.70469966412644]
	TIME [epoch: 76 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4854700199341688		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 1.4854700199341688 | validation: 1.7778710150312476]
	TIME [epoch: 76 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.423680535399145		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 1.423680535399145 | validation: 1.673984556150662]
	TIME [epoch: 76.1 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.430621948485475		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 1.430621948485475 | validation: 1.6808227050003195]
	TIME [epoch: 76 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6327233164100288		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 1.6327233164100288 | validation: 3.3577302334522248]
	TIME [epoch: 76.1 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8140278128028866		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 2.8140278128028866 | validation: 3.4411075358044303]
	TIME [epoch: 76 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1434685667492483		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 3.1434685667492483 | validation: 3.3619505117285855]
	TIME [epoch: 76.2 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7559359282521805		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 2.7559359282521805 | validation: 2.2060066473998488]
	TIME [epoch: 76.1 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.026652155457344		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 2.026652155457344 | validation: 2.4987215849050273]
	TIME [epoch: 76.2 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3199599432862854		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 2.3199599432862854 | validation: 2.898025988955712]
	TIME [epoch: 76.2 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9636428875109742		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 1.9636428875109742 | validation: 1.6765119165049862]
	TIME [epoch: 76.3 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5474146051097497		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 1.5474146051097497 | validation: 1.5224022046700338]
	TIME [epoch: 76.2 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.401716614184842		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 1.401716614184842 | validation: 1.585595996154328]
	TIME [epoch: 76.3 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.447267481354189		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 1.447267481354189 | validation: 2.1783418626650124]
	TIME [epoch: 76.2 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2263837842055962		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 2.2263837842055962 | validation: 1.9023417833816292]
	TIME [epoch: 76.2 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.668638483660703		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 1.668638483660703 | validation: 2.289397895633341]
	TIME [epoch: 75.9 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7173806318397928		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 1.7173806318397928 | validation: 1.6559831940543066]
	TIME [epoch: 76.1 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.790380980629858		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 1.790380980629858 | validation: 2.1102639419901994]
	TIME [epoch: 76.1 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0985260615244337		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 2.0985260615244337 | validation: 2.217926390375536]
	TIME [epoch: 76.3 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2481548661725275		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 2.2481548661725275 | validation: 3.935714639668747]
	TIME [epoch: 75.9 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9075809058478432		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 3.9075809058478432 | validation: 4.157145073561329]
	TIME [epoch: 76 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6779727437644842		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 2.6779727437644842 | validation: 1.7615066189105908]
	TIME [epoch: 75.8 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5891912281425236		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 1.5891912281425236 | validation: 1.7074786815677]
	TIME [epoch: 75.9 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5508864095679749		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 1.5508864095679749 | validation: 1.7426462748319271]
	TIME [epoch: 75.9 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4055189707329603		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 1.4055189707329603 | validation: 1.6015119701971199]
	TIME [epoch: 76 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8882648815620473		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 1.8882648815620473 | validation: 2.0716975860575926]
	TIME [epoch: 75.9 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0795414988632634		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 2.0795414988632634 | validation: 2.1797486370920742]
	TIME [epoch: 76 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8808413961238344		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 1.8808413961238344 | validation: 1.6940729752713377]
	TIME [epoch: 75.9 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.424131251881227		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 1.424131251881227 | validation: 1.514909851267078]
	TIME [epoch: 76 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4987830635778605		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 1.4987830635778605 | validation: 1.3953653289392558]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_154.pth
	Model improved!!!
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.365495076743992		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 1.365495076743992 | validation: 1.5282190506082711]
	TIME [epoch: 76.1 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4337430467483383		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 1.4337430467483383 | validation: 1.505594199027961]
	TIME [epoch: 76.2 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.287920263687385		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 1.287920263687385 | validation: 1.4233536108985034]
	TIME [epoch: 76.3 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4489587215106243		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 1.4489587215106243 | validation: 2.8781719443250355]
	TIME [epoch: 75.9 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.717308244442213		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 1.717308244442213 | validation: 1.3643788658921454]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_159.pth
	Model improved!!!
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2921172940123495		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 1.2921172940123495 | validation: 1.4211481910232502]
	TIME [epoch: 75.8 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5075335368452258		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 1.5075335368452258 | validation: 1.7648710248826176]
	TIME [epoch: 75.8 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5766995625447309		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 1.5766995625447309 | validation: 1.540645731802761]
	TIME [epoch: 75.8 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2999541435868487		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 1.2999541435868487 | validation: 1.4135304280942802]
	TIME [epoch: 75.9 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4764885699796992		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 1.4764885699796992 | validation: 1.3675413891110852]
	TIME [epoch: 75.8 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3622797182557638		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 1.3622797182557638 | validation: 1.4832058528713439]
	TIME [epoch: 75.8 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2858080520325488		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 1.2858080520325488 | validation: 1.7667132370051475]
	TIME [epoch: 75.8 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6673995549883063		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 1.6673995549883063 | validation: 1.7604312551239014]
	TIME [epoch: 75.8 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.581618540096327		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 1.581618540096327 | validation: 2.895805195504954]
	TIME [epoch: 75.8 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6457225568591216		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 2.6457225568591216 | validation: 2.590723832291393]
	TIME [epoch: 75.8 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.363938903091361		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 2.363938903091361 | validation: 2.508696566613658]
	TIME [epoch: 75.8 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0224433145444785		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 2.0224433145444785 | validation: 3.5050472848726333]
	TIME [epoch: 75.8 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9558007546985956		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 2.9558007546985956 | validation: 2.6283058536734663]
	TIME [epoch: 75.8 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1167725870406793		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 2.1167725870406793 | validation: 2.1167061443158914]
	TIME [epoch: 75.8 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.978434786512837		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 1.978434786512837 | validation: 2.4946891724501565]
	TIME [epoch: 75.8 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1931665740427566		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 2.1931665740427566 | validation: 2.0981143629417356]
	TIME [epoch: 75.8 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.118039317622929		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 2.118039317622929 | validation: 2.583752970605113]
	TIME [epoch: 75.8 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4801970024193842		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 2.4801970024193842 | validation: 2.341583504724204]
	TIME [epoch: 75.8 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.264712627345989		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 2.264712627345989 | validation: 2.0331576935617153]
	TIME [epoch: 75.8 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9879812606190446		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 1.9879812606190446 | validation: 1.922383321145018]
	TIME [epoch: 75.8 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5366469774189897		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 1.5366469774189897 | validation: 1.4962096136465508]
	TIME [epoch: 75.8 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4162028591315168		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 1.4162028591315168 | validation: 1.79874279380042]
	TIME [epoch: 75.8 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4270940863967576		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 1.4270940863967576 | validation: 1.5589890392979613]
	TIME [epoch: 75.8 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.313982638158032		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 1.313982638158032 | validation: 1.555158667023444]
	TIME [epoch: 75.8 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.391090834479834		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 1.391090834479834 | validation: 1.514763482902549]
	TIME [epoch: 75.8 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4729843406931726		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 1.4729843406931726 | validation: 1.677048773099753]
	TIME [epoch: 75.8 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3144811072940996		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 1.3144811072940996 | validation: 1.4841604683834317]
	TIME [epoch: 75.8 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4002936309405285		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 1.4002936309405285 | validation: 1.3909175355804027]
	TIME [epoch: 75.8 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2592406754475884		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 1.2592406754475884 | validation: 1.3829024124476963]
	TIME [epoch: 75.8 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3534628990503677		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 1.3534628990503677 | validation: 1.5970235305405514]
	TIME [epoch: 75.8 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5677946746593119		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 1.5677946746593119 | validation: 1.5460882817895314]
	TIME [epoch: 75.8 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4623213586428612		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 1.4623213586428612 | validation: 1.3477285380303814]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_191.pth
	Model improved!!!
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.280724904128969		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 1.280724904128969 | validation: 1.5678012695117038]
	TIME [epoch: 75.8 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1792422725842502		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 1.1792422725842502 | validation: 1.3879822736002803]
	TIME [epoch: 75.8 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1895109224407565		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 1.1895109224407565 | validation: 1.3112392445190357]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_194.pth
	Model improved!!!
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2557843876364392		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 1.2557843876364392 | validation: 1.2556065076337535]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_195.pth
	Model improved!!!
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0939605307374156		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 1.0939605307374156 | validation: 1.6255031221582719]
	TIME [epoch: 75.8 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2643218272884207		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 1.2643218272884207 | validation: 1.5114410576821404]
	TIME [epoch: 75.8 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1591107131424354		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 1.1591107131424354 | validation: 1.2651085170628413]
	TIME [epoch: 75.8 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1367909445899806		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 1.1367909445899806 | validation: 1.3680619018524003]
	TIME [epoch: 75.7 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3624139410255152		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 1.3624139410255152 | validation: 2.0412318668948504]
	TIME [epoch: 75.8 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4500083970839974		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 1.4500083970839974 | validation: 1.388027762008687]
	TIME [epoch: 75.8 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2977509883505538		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 1.2977509883505538 | validation: 1.1817548426358746]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_202.pth
	Model improved!!!
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0734467015113938		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 1.0734467015113938 | validation: 1.161907599076796]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_203.pth
	Model improved!!!
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1886331003911283		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 1.1886331003911283 | validation: 1.3734724087484547]
	TIME [epoch: 75.8 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1875089471607276		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 1.1875089471607276 | validation: 1.1694861756150199]
	TIME [epoch: 75.8 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.138722464592584		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 1.138722464592584 | validation: 1.4092189502049846]
	TIME [epoch: 75.8 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1142149657345009		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 1.1142149657345009 | validation: 1.146529684237369]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_207.pth
	Model improved!!!
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0417055144040641		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 1.0417055144040641 | validation: 1.2013655846316138]
	TIME [epoch: 75.8 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0583685198371346		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 1.0583685198371346 | validation: 1.5442249238413845]
	TIME [epoch: 75.8 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1700666615916004		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 1.1700666615916004 | validation: 1.2812996908323946]
	TIME [epoch: 75.8 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.094596412536429		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 1.094596412536429 | validation: 1.1537670061426866]
	TIME [epoch: 75.8 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0940507142245246		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 1.0940507142245246 | validation: 1.336959407397861]
	TIME [epoch: 75.8 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2094713192784199		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 1.2094713192784199 | validation: 1.6489193605164842]
	TIME [epoch: 75.7 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1507706603269834		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 1.1507706603269834 | validation: 1.802243497537161]
	TIME [epoch: 75.8 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1745799825567325		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 1.1745799825567325 | validation: 1.1672490935325233]
	TIME [epoch: 75.8 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0232503361833873		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 1.0232503361833873 | validation: 1.1052004995186155]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_216.pth
	Model improved!!!
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9449627773375092		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 0.9449627773375092 | validation: 1.1941680223553648]
	TIME [epoch: 75.8 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9060371695863016		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 0.9060371695863016 | validation: 1.4160739272890153]
	TIME [epoch: 75.7 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0396323065409099		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 1.0396323065409099 | validation: 1.0406742557752602]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_219.pth
	Model improved!!!
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9859739309486695		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 0.9859739309486695 | validation: 1.1557708197086722]
	TIME [epoch: 75.8 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0477938597014087		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 1.0477938597014087 | validation: 1.0726951909137648]
	TIME [epoch: 75.7 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1447373042506703		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 1.1447373042506703 | validation: 1.3493415129694264]
	TIME [epoch: 75.8 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9981168667998578		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 0.9981168667998578 | validation: 1.0329266053641204]
	TIME [epoch: 75.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_223.pth
	Model improved!!!
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9788765920981768		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 0.9788765920981768 | validation: 1.1754850500353586]
	TIME [epoch: 75.8 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9898499634949003		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 0.9898499634949003 | validation: 1.1504079082127787]
	TIME [epoch: 75.8 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.091130870712512		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 1.091130870712512 | validation: 1.094743599960756]
	TIME [epoch: 75.8 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.026668362697365		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 1.026668362697365 | validation: 1.3581674205144552]
	TIME [epoch: 75.7 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0890696039358267		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 1.0890696039358267 | validation: 1.1232065048033961]
	TIME [epoch: 75.8 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9173335526987118		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 0.9173335526987118 | validation: 1.0648335743549064]
	TIME [epoch: 75.8 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9180732516560408		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 0.9180732516560408 | validation: 1.0252326711602007]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_230.pth
	Model improved!!!
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9605026724787122		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 0.9605026724787122 | validation: 0.9750947266939394]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_231.pth
	Model improved!!!
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9691425586401938		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 0.9691425586401938 | validation: 1.1183226198317122]
	TIME [epoch: 75.8 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0478443807537494		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 1.0478443807537494 | validation: 1.0241326730445162]
	TIME [epoch: 75.7 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8747588671951175		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 0.8747588671951175 | validation: 1.1321543381470398]
	TIME [epoch: 75.8 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9017469810147765		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 0.9017469810147765 | validation: 1.346045834345219]
	TIME [epoch: 75.8 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0067421119665705		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 1.0067421119665705 | validation: 1.1864362590904456]
	TIME [epoch: 75.8 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.958850579993396		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 0.958850579993396 | validation: 0.9512656704382622]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_237.pth
	Model improved!!!
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9973544063805257		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 0.9973544063805257 | validation: 0.9256084223160204]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_238.pth
	Model improved!!!
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.846896990476508		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 0.846896990476508 | validation: 1.0301988891332492]
	TIME [epoch: 75.8 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.864681047929566		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 0.864681047929566 | validation: 1.141619662280585]
	TIME [epoch: 75.8 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9519923355511983		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 0.9519923355511983 | validation: 0.9376321277614585]
	TIME [epoch: 75.8 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9337936111358369		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 0.9337936111358369 | validation: 1.1941264054499439]
	TIME [epoch: 75.8 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9137919502400742		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 0.9137919502400742 | validation: 0.984191388349845]
	TIME [epoch: 75.8 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8773421595356032		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 0.8773421595356032 | validation: 1.0588826396825237]
	TIME [epoch: 75.8 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9243520696522776		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.9243520696522776 | validation: 0.9457173965198977]
	TIME [epoch: 75.8 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9797996401972995		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 0.9797996401972995 | validation: 1.2600622179910006]
	TIME [epoch: 75.8 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1546756350817482		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 1.1546756350817482 | validation: 0.9876731591659288]
	TIME [epoch: 75.8 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9823038727645801		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 0.9823038727645801 | validation: 0.9302025417581608]
	TIME [epoch: 75.9 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7991115001359694		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 0.7991115001359694 | validation: 0.9917085993660939]
	TIME [epoch: 75.8 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8345657561474882		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 0.8345657561474882 | validation: 0.9297041951501426]
	TIME [epoch: 75.8 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7864123519396975		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 0.7864123519396975 | validation: 0.8815500413161557]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_251.pth
	Model improved!!!
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.830475849752282		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 0.830475849752282 | validation: 0.8125343561864186]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_252.pth
	Model improved!!!
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0492057697141057		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 1.0492057697141057 | validation: 0.9944403054722377]
	TIME [epoch: 75.8 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8542153135203984		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 0.8542153135203984 | validation: 0.878085432837223]
	TIME [epoch: 75.9 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9521996610437935		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 0.9521996610437935 | validation: 1.2439501436551277]
	TIME [epoch: 75.8 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9040456872024799		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 0.9040456872024799 | validation: 1.1933041904171227]
	TIME [epoch: 75.9 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9396850946940922		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 0.9396850946940922 | validation: 1.0657356935859474]
	TIME [epoch: 75.8 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9124229159683924		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 0.9124229159683924 | validation: 1.000367338973798]
	TIME [epoch: 75.8 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8684613486447967		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 0.8684613486447967 | validation: 0.9297066170795483]
	TIME [epoch: 75.9 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7887967902625339		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 0.7887967902625339 | validation: 0.9134354441776247]
	TIME [epoch: 75.8 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8285936940474675		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 0.8285936940474675 | validation: 0.8996970473948307]
	TIME [epoch: 75.8 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8065142533588843		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 0.8065142533588843 | validation: 1.1501867124066996]
	TIME [epoch: 75.8 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0931303386282054		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 1.0931303386282054 | validation: 0.9474219846055566]
	TIME [epoch: 75.8 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7814873124551419		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.7814873124551419 | validation: 0.9714343458143329]
	TIME [epoch: 75.8 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7614639981843749		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 0.7614639981843749 | validation: 1.0241877377142519]
	TIME [epoch: 75.9 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7673742318203343		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 0.7673742318203343 | validation: 0.8515208546188289]
	TIME [epoch: 75.8 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9379783474702816		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 0.9379783474702816 | validation: 0.9894897597150253]
	TIME [epoch: 75.8 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8627169780746728		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 0.8627169780746728 | validation: 1.2006383163516747]
	TIME [epoch: 75.8 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8159874309773242		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 0.8159874309773242 | validation: 1.187014719694926]
	TIME [epoch: 75.8 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8784311285271058		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 0.8784311285271058 | validation: 0.8636560280375616]
	TIME [epoch: 75.8 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7906909132557216		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 0.7906909132557216 | validation: 1.0462084348473148]
	TIME [epoch: 75.8 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7841394389447859		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 0.7841394389447859 | validation: 1.0020380202242098]
	TIME [epoch: 75.8 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.799269327231738		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 0.799269327231738 | validation: 0.9540612403576978]
	TIME [epoch: 75.9 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8342340881285016		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 0.8342340881285016 | validation: 0.9759841525695521]
	TIME [epoch: 75.9 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8089760656959216		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 0.8089760656959216 | validation: 0.9905922999490717]
	TIME [epoch: 75.8 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8915031524308759		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 0.8915031524308759 | validation: 0.9232858921138427]
	TIME [epoch: 75.9 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8091952413774213		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 0.8091952413774213 | validation: 0.8671812983160669]
	TIME [epoch: 75.9 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7970771071137913		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 0.7970771071137913 | validation: 0.9985628418465189]
	TIME [epoch: 75.8 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8198678203893288		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 0.8198678203893288 | validation: 1.12543992887365]
	TIME [epoch: 75.9 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8140962085404841		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 0.8140962085404841 | validation: 0.8463119147846087]
	TIME [epoch: 75.8 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7467337310453805		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 0.7467337310453805 | validation: 0.9746118132700211]
	TIME [epoch: 75.8 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8578004062411618		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 0.8578004062411618 | validation: 0.9080105729327577]
	TIME [epoch: 75.8 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7741668422029153		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.7741668422029153 | validation: 0.8195710082279459]
	TIME [epoch: 75.8 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8199069234519261		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 0.8199069234519261 | validation: 0.8417331080761694]
	TIME [epoch: 75.8 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7595435502396652		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 0.7595435502396652 | validation: 0.8772737357987523]
	TIME [epoch: 75.8 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8044539925566374		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 0.8044539925566374 | validation: 1.013009937034518]
	TIME [epoch: 75.8 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.82157740000141		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.82157740000141 | validation: 0.8988271460695254]
	TIME [epoch: 75.9 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8152584168887794		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.8152584168887794 | validation: 0.9005403181410849]
	TIME [epoch: 75.9 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7618707459204253		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.7618707459204253 | validation: 0.8503317759752685]
	TIME [epoch: 75.8 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7447201273503432		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 0.7447201273503432 | validation: 1.0079802304947023]
	TIME [epoch: 75.9 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8070023703935835		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.8070023703935835 | validation: 0.8928429926522232]
	TIME [epoch: 75.7 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8175568303904999		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 0.8175568303904999 | validation: 0.8401005833569952]
	TIME [epoch: 75.8 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7479999258413704		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 0.7479999258413704 | validation: 0.9228093312255374]
	TIME [epoch: 75.8 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8481016714825302		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 0.8481016714825302 | validation: 0.9104921586527921]
	TIME [epoch: 75.8 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7657841468808193		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 0.7657841468808193 | validation: 0.9121874391298109]
	TIME [epoch: 75.9 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7301177530256664		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.7301177530256664 | validation: 0.77409772914523]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_296.pth
	Model improved!!!
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7951532346934719		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 0.7951532346934719 | validation: 0.7969097132299716]
	TIME [epoch: 75.7 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7756195822625243		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.7756195822625243 | validation: 0.8586160694383462]
	TIME [epoch: 75.8 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8001848670814548		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.8001848670814548 | validation: 0.8706869304138676]
	TIME [epoch: 75.8 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7848863287893747		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 0.7848863287893747 | validation: 1.1147932613780127]
	TIME [epoch: 75.8 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.894189215565449		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 0.894189215565449 | validation: 1.0227428644549708]
	TIME [epoch: 75.8 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8072342649228925		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.8072342649228925 | validation: 0.8786998261665935]
	TIME [epoch: 75.8 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9562660913998392		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 0.9562660913998392 | validation: 0.8068103807150868]
	TIME [epoch: 75.8 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7348455535188649		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 0.7348455535188649 | validation: 0.8923120203011919]
	TIME [epoch: 75.8 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8105947817883439		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 0.8105947817883439 | validation: 0.8734994523064625]
	TIME [epoch: 75.7 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7968638141732589		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.7968638141732589 | validation: 0.7691358937451505]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_306.pth
	Model improved!!!
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7608873721015244		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.7608873721015244 | validation: 0.8778367182732051]
	TIME [epoch: 75.8 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7234498452406753		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.7234498452406753 | validation: 0.9898458181284961]
	TIME [epoch: 75.7 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8353495587366873		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 0.8353495587366873 | validation: 1.0584641601178526]
	TIME [epoch: 75.7 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7929518656670081		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.7929518656670081 | validation: 0.8581607072529454]
	TIME [epoch: 75.8 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7219240408134242		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 0.7219240408134242 | validation: 0.7688991192271173]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_311.pth
	Model improved!!!
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.717321211021162		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 0.717321211021162 | validation: 0.8733351076691281]
	TIME [epoch: 75.8 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7509038250958542		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 0.7509038250958542 | validation: 0.7828244881110653]
	TIME [epoch: 75.8 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8737953309211788		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 0.8737953309211788 | validation: 1.1090724753246257]
	TIME [epoch: 75.7 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8868438007491061		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 0.8868438007491061 | validation: 0.8890166918335863]
	TIME [epoch: 75.7 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.789934095995202		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.789934095995202 | validation: 1.2401502182891275]
	TIME [epoch: 75.7 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8958350802661912		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 0.8958350802661912 | validation: 0.9834182023231541]
	TIME [epoch: 75.7 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7872992239669113		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 0.7872992239669113 | validation: 0.918865445674322]
	TIME [epoch: 75.7 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9109592633952844		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.9109592633952844 | validation: 1.009845968750276]
	TIME [epoch: 75.7 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7597790295592113		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 0.7597790295592113 | validation: 0.8451818924977523]
	TIME [epoch: 75.7 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.862181684538185		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.862181684538185 | validation: 1.5003485209197667]
	TIME [epoch: 75.7 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.052835425682423		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 1.052835425682423 | validation: 0.7972612128414598]
	TIME [epoch: 75.7 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7418973980618214		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 0.7418973980618214 | validation: 0.7733090064876306]
	TIME [epoch: 75.7 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6739932730492887		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 0.6739932730492887 | validation: 0.859099301807857]
	TIME [epoch: 75.7 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8069506428901236		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 0.8069506428901236 | validation: 0.844359667140365]
	TIME [epoch: 75.7 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7594773552474877		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.7594773552474877 | validation: 0.7882110708303233]
	TIME [epoch: 75.7 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.742426702324643		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.742426702324643 | validation: 0.8465577572936187]
	TIME [epoch: 75.7 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6964585121561453		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 0.6964585121561453 | validation: 0.8821782304550438]
	TIME [epoch: 75.8 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7440728452777168		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.7440728452777168 | validation: 0.8313845198932821]
	TIME [epoch: 75.7 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7220523227092663		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.7220523227092663 | validation: 0.8188328128548201]
	TIME [epoch: 75.7 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7364192594195778		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.7364192594195778 | validation: 0.8684224010665165]
	TIME [epoch: 75.8 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8042836328052209		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.8042836328052209 | validation: 0.9639787322781843]
	TIME [epoch: 75.7 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7426901322723385		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.7426901322723385 | validation: 0.8953032203992943]
	TIME [epoch: 75.8 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8236868123165655		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 0.8236868123165655 | validation: 0.850986787897883]
	TIME [epoch: 75.8 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8701326738830749		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 0.8701326738830749 | validation: 0.8643698068697315]
	TIME [epoch: 75.7 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6751149686867313		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 0.6751149686867313 | validation: 0.923113361389162]
	TIME [epoch: 75.8 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7699172382042871		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 0.7699172382042871 | validation: 0.7826556862252528]
	TIME [epoch: 75.7 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7133460454623994		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 0.7133460454623994 | validation: 0.8128586148852976]
	TIME [epoch: 75.7 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7122832288198127		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 0.7122832288198127 | validation: 0.7648678811995695]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_339.pth
	Model improved!!!
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6611613763517372		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.6611613763517372 | validation: 0.864995630534314]
	TIME [epoch: 75.7 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7042299056029365		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 0.7042299056029365 | validation: 0.7995376446365343]
	TIME [epoch: 75.8 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.684941497155297		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 0.684941497155297 | validation: 0.8496388236434453]
	TIME [epoch: 75.8 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8938319760401957		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 0.8938319760401957 | validation: 1.4166351030682374]
	TIME [epoch: 75.8 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0667952149187228		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 1.0667952149187228 | validation: 1.130168025657889]
	TIME [epoch: 75.7 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7931358778499309		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 0.7931358778499309 | validation: 1.0483302775918233]
	TIME [epoch: 75.8 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7522963211978035		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 0.7522963211978035 | validation: 0.839196781675857]
	TIME [epoch: 75.7 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7159115656375786		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 0.7159115656375786 | validation: 0.9050372140618657]
	TIME [epoch: 75.8 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7039748542455829		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.7039748542455829 | validation: 0.7839614367396879]
	TIME [epoch: 75.8 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6570408215467797		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.6570408215467797 | validation: 0.8093819272712212]
	TIME [epoch: 75.8 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7095553734743324		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.7095553734743324 | validation: 0.8197352410773973]
	TIME [epoch: 75.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7306096766135488		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 0.7306096766135488 | validation: 0.7925114790383938]
	TIME [epoch: 75.7 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6647280844856871		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.6647280844856871 | validation: 0.8275588636490074]
	TIME [epoch: 75.8 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7436378387075171		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.7436378387075171 | validation: 0.7733565345479314]
	TIME [epoch: 75.8 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7437457711881093		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.7437457711881093 | validation: 0.7833077865735685]
	TIME [epoch: 75.7 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6614071700194857		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.6614071700194857 | validation: 0.7615257463106943]
	TIME [epoch: 75.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_355.pth
	Model improved!!!
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6976931339011943		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 0.6976931339011943 | validation: 0.8027105681901421]
	TIME [epoch: 75.8 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6820891961183444		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 0.6820891961183444 | validation: 0.9072507832004575]
	TIME [epoch: 75.7 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7422252046447483		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.7422252046447483 | validation: 0.8117504600009615]
	TIME [epoch: 75.7 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7423545324752905		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.7423545324752905 | validation: 0.8282678317219841]
	TIME [epoch: 75.8 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6740452267343564		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.6740452267343564 | validation: 0.8928385652438284]
	TIME [epoch: 75.8 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7556843576308832		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.7556843576308832 | validation: 0.7743078466869152]
	TIME [epoch: 75.8 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6784862703194443		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.6784862703194443 | validation: 0.7402242765345395]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_362.pth
	Model improved!!!
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7251541212945292		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 0.7251541212945292 | validation: 1.2691738844155733]
	TIME [epoch: 75.8 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9895484349798813		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.9895484349798813 | validation: 0.8237685720575989]
	TIME [epoch: 75.8 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7601433152726704		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.7601433152726704 | validation: 0.8332226240035012]
	TIME [epoch: 75.7 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7135022598317509		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.7135022598317509 | validation: 0.7884793833573565]
	TIME [epoch: 75.7 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6648696180583855		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.6648696180583855 | validation: 0.7407066460671048]
	TIME [epoch: 75.8 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6490489132818525		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.6490489132818525 | validation: 0.7014371322438827]
	TIME [epoch: 75.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v3_20240503_110941/states/model_phiq_1a_v3_368.pth
	Model improved!!!
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6396255705078592		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.6396255705078592 | validation: 0.7749625206547565]
	TIME [epoch: 75.8 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6485681247143333		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 0.6485681247143333 | validation: 0.7945903877860278]
	TIME [epoch: 75.8 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6707635177492608		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.6707635177492608 | validation: 0.7907005658136563]
	TIME [epoch: 75.8 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.739671805959744		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.739671805959744 | validation: 0.8203675585523834]
	TIME [epoch: 75.8 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6691730652129729		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 0.6691730652129729 | validation: 0.8531166540124572]
	TIME [epoch: 75.8 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7636156968714498		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.7636156968714498 | validation: 0.7820494738878325]
	TIME [epoch: 75.8 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7877500329015915		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 0.7877500329015915 | validation: 0.8157417774593844]
	TIME [epoch: 75.8 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6864807290210635		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.6864807290210635 | validation: 0.7314090061184836]
	TIME [epoch: 75.8 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6438163211296049		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.6438163211296049 | validation: 0.7377547415652037]
	TIME [epoch: 75.8 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7344728250266345		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.7344728250266345 | validation: 0.9009678064305507]
	TIME [epoch: 75.8 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7648911288397611		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.7648911288397611 | validation: 0.8173949001545326]
	TIME [epoch: 75.8 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6840323773146636		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.6840323773146636 | validation: 0.760142374711268]
	TIME [epoch: 75.8 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6632773433548351		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.6632773433548351 | validation: 0.762252686005677]
	TIME [epoch: 75.6 sec]
EPOCH 382/1000:
	Training over batches...
