Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r4', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r4', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3275235441

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 13.548955355184221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 13.548955355184221 | validation: 11.77775054834373]
	TIME [epoch: 98.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 12.15575619576351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.15575619576351 | validation: 12.39180394861534]
	TIME [epoch: 11.6 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 12.04749099237558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.04749099237558 | validation: 10.289729806792854]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.169334861334354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.169334861334354 | validation: 9.20603135675048]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.992687550465584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.992687550465584 | validation: 8.039311266623413]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.8973119464501025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.8973119464501025 | validation: 8.844949717403038]
	TIME [epoch: 11.5 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.683012498591418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.683012498591418 | validation: 7.61370472064592]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.601452150270994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.601452150270994 | validation: 6.318188976035547]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.975537198380293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.975537198380293 | validation: 5.720550646696076]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.35824297347911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.35824297347911 | validation: 5.701557614906624]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3186607556923775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3186607556923775 | validation: 5.212328065679457]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0068179889673985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0068179889673985 | validation: 5.036252627513581]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.003983652759484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.003983652759484 | validation: 6.351608141283364]
	TIME [epoch: 11.5 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.200590293286924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.200590293286924 | validation: 5.052845630027349]
	TIME [epoch: 11.5 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.521036961095252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.521036961095252 | validation: 4.57307835797997]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.501740347642469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.501740347642469 | validation: 5.295964954236078]
	TIME [epoch: 11.5 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.701384506239007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.701384506239007 | validation: 4.8567342550596555]
	TIME [epoch: 11.5 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.658832873890269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.658832873890269 | validation: 4.682111324501098]
	TIME [epoch: 11.5 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.339750114375462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.339750114375462 | validation: 4.85047217278439]
	TIME [epoch: 11.5 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.354118364644635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.354118364644635 | validation: 4.880945023386146]
	TIME [epoch: 11.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.34008697861608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.34008697861608 | validation: 4.467481689065423]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.301087518911267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.301087518911267 | validation: 4.438167768774284]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.906401379319469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.906401379319469 | validation: 4.90509210550563]
	TIME [epoch: 11.6 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.284878900792338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.284878900792338 | validation: 4.224138203020765]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18311578833282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.18311578833282 | validation: 4.024559277711744]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.485146166097515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.485146166097515 | validation: 4.001925811677624]
	TIME [epoch: 11.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.210881750668087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.210881750668087 | validation: 4.009320950108532]
	TIME [epoch: 11.5 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.871737491844502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.871737491844502 | validation: 4.021968156933283]
	TIME [epoch: 11.5 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7159380288097976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7159380288097976 | validation: 5.171199645173653]
	TIME [epoch: 11.6 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128770554579465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.128770554579465 | validation: 3.9538762283938658]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.881143847531307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.881143847531307 | validation: 4.086294556774346]
	TIME [epoch: 11.5 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.99176062891019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.99176062891019 | validation: 4.187966651341702]
	TIME [epoch: 11.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.250347764525699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.250347764525699 | validation: 3.966277859032149]
	TIME [epoch: 11.5 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7100838900402806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7100838900402806 | validation: 4.32069814331096]
	TIME [epoch: 11.5 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9012674531583156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9012674531583156 | validation: 5.021646130142651]
	TIME [epoch: 11.6 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8655036495642623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8655036495642623 | validation: 5.212462620326369]
	TIME [epoch: 11.5 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158177922709725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.158177922709725 | validation: 7.614330472831688]
	TIME [epoch: 11.5 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.198313741428346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.198313741428346 | validation: 4.744918838949586]
	TIME [epoch: 11.6 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.027822573673909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027822573673909 | validation: 5.123726050648611]
	TIME [epoch: 11.5 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9739757171940853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9739757171940853 | validation: 7.233649345607964]
	TIME [epoch: 11.5 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.830252206095958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.830252206095958 | validation: 4.477227449256195]
	TIME [epoch: 11.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9158374050564784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9158374050564784 | validation: 4.220379173560868]
	TIME [epoch: 11.5 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8754598612758926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8754598612758926 | validation: 4.032654801143184]
	TIME [epoch: 11.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6610026915624747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6610026915624747 | validation: 4.195604548050621]
	TIME [epoch: 11.5 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.663148177905949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.663148177905949 | validation: 4.492431189896685]
	TIME [epoch: 11.5 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9005407703080404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9005407703080404 | validation: 4.338502592068328]
	TIME [epoch: 11.5 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7768337166544708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7768337166544708 | validation: 3.8995250330157014]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.318102071193291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.318102071193291 | validation: 5.23945442274919]
	TIME [epoch: 11.6 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8986264114391096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8986264114391096 | validation: 4.861197859491868]
	TIME [epoch: 11.5 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.950619529031975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.950619529031975 | validation: 3.890234611728712]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4878444178158494		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 3.4878444178158494 | validation: 3.7346053998781246]
	TIME [epoch: 11.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.678982688141156		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 3.678982688141156 | validation: 3.982735206013183]
	TIME [epoch: 11.5 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.616307391758058		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 3.616307391758058 | validation: 4.301916469114475]
	TIME [epoch: 11.5 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1443990329261515		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 4.1443990329261515 | validation: 3.4122270300946647]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4674194194525403		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 3.4674194194525403 | validation: 3.7283410062842717]
	TIME [epoch: 11.5 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5572750005679046		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 3.5572750005679046 | validation: 3.9345806594301873]
	TIME [epoch: 11.5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.605572319788998		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.605572319788998 | validation: 3.8278670147800153]
	TIME [epoch: 11.6 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.698085063551624		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.698085063551624 | validation: 3.4143757355169373]
	TIME [epoch: 11.5 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4961552806558136		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.4961552806558136 | validation: 3.7134688242242535]
	TIME [epoch: 11.5 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6662438596305154		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.6662438596305154 | validation: 4.613253105002002]
	TIME [epoch: 11.5 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5656853653916016		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.5656853653916016 | validation: 3.855372568128637]
	TIME [epoch: 11.6 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.978045647411761		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 4.978045647411761 | validation: 3.8858122842610157]
	TIME [epoch: 11.5 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4473648956630503		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.4473648956630503 | validation: 4.3790257510972985]
	TIME [epoch: 11.5 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6735596829322215		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 3.6735596829322215 | validation: 5.334665644476599]
	TIME [epoch: 11.5 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0543780156731595		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 4.0543780156731595 | validation: 3.434937297793343]
	TIME [epoch: 11.5 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.744853192148964		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 3.744853192148964 | validation: 4.11365422781824]
	TIME [epoch: 11.5 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7784949747703958		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.7784949747703958 | validation: 3.4244569804801124]
	TIME [epoch: 11.5 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.487740630370307		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.487740630370307 | validation: 4.0198784392732385]
	TIME [epoch: 11.5 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6807626128230613		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.6807626128230613 | validation: 3.4916474471071672]
	TIME [epoch: 11.5 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.47953414289485		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.47953414289485 | validation: 3.6431934245185653]
	TIME [epoch: 11.5 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3296624758586257		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 3.3296624758586257 | validation: 3.981816929878978]
	TIME [epoch: 11.5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.631741853194198		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.631741853194198 | validation: 4.5544774133938235]
	TIME [epoch: 11.5 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6440646759458026		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.6440646759458026 | validation: 3.7947811088553887]
	TIME [epoch: 11.5 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4087033689707438		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.4087033689707438 | validation: 3.7027817421530167]
	TIME [epoch: 11.5 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.500029207307543		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.500029207307543 | validation: 3.902252054485473]
	TIME [epoch: 11.5 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4476030619563893		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 3.4476030619563893 | validation: 3.4875084930275237]
	TIME [epoch: 11.5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.592033038146107		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 3.592033038146107 | validation: 4.650841246388533]
	TIME [epoch: 11.5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7260217220382765		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 3.7260217220382765 | validation: 3.6243171422838705]
	TIME [epoch: 11.5 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5142724368328793		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.5142724368328793 | validation: 3.3961319120841416]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.863890150374538		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 3.863890150374538 | validation: 3.682661204108645]
	TIME [epoch: 11.5 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4023802154779705		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 3.4023802154779705 | validation: 5.332889021574329]
	TIME [epoch: 11.5 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8841182257571387		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.8841182257571387 | validation: 3.429103130436273]
	TIME [epoch: 11.5 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3442054796873477		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 3.3442054796873477 | validation: 3.5435400420242646]
	TIME [epoch: 11.5 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6610271035894866		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.6610271035894866 | validation: 3.2734848705499116]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4474203502907566		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.4474203502907566 | validation: 3.32685793360222]
	TIME [epoch: 11.5 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4528579175528797		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.4528579175528797 | validation: 3.3963173133899525]
	TIME [epoch: 11.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2812852504542054		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.2812852504542054 | validation: 3.520853322612856]
	TIME [epoch: 11.5 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.493711930795444		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 3.493711930795444 | validation: 3.7116789697075765]
	TIME [epoch: 11.5 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4901031457376535		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.4901031457376535 | validation: 4.888484658262512]
	TIME [epoch: 11.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8986750395840835		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 3.8986750395840835 | validation: 3.2492846332580694]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.448059365954227		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 4.448059365954227 | validation: 4.28917388530174]
	TIME [epoch: 11.5 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9181973190256634		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 3.9181973190256634 | validation: 3.393112407805648]
	TIME [epoch: 11.5 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.007945774226048		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 4.007945774226048 | validation: 3.9434103372678138]
	TIME [epoch: 11.5 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6579352415928286		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 3.6579352415928286 | validation: 3.413715363559396]
	TIME [epoch: 11.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.417112275559623		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 3.417112275559623 | validation: 3.5514865710572487]
	TIME [epoch: 11.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4823049846444594		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 3.4823049846444594 | validation: 3.3476088943996194]
	TIME [epoch: 11.5 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5881438948537756		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 3.5881438948537756 | validation: 3.27935945662064]
	TIME [epoch: 11.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.197260212763289		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 4.197260212763289 | validation: 3.765675463950207]
	TIME [epoch: 11.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.914961903460859		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.914961903460859 | validation: 3.6426606644606783]
	TIME [epoch: 11.6 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3908873060023725		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 3.3908873060023725 | validation: 3.949955708484187]
	TIME [epoch: 11.5 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.47613517537793		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 3.47613517537793 | validation: 3.414213186363078]
	TIME [epoch: 11.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3299870024160385		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 3.3299870024160385 | validation: 3.640817446083606]
	TIME [epoch: 11.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323451937162693		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 3.323451937162693 | validation: 3.51871246891316]
	TIME [epoch: 11.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3037202281003832		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 3.3037202281003832 | validation: 3.475474004228973]
	TIME [epoch: 11.5 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4536256920251103		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.4536256920251103 | validation: 3.5619373171609756]
	TIME [epoch: 11.5 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3705265250564405		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 3.3705265250564405 | validation: 4.124567179166711]
	TIME [epoch: 11.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5070012096444643		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 3.5070012096444643 | validation: 5.095818409792802]
	TIME [epoch: 11.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8817582890111515		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.8817582890111515 | validation: 3.2882744899071157]
	TIME [epoch: 11.5 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4411286670902674		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 3.4411286670902674 | validation: 4.225014955540997]
	TIME [epoch: 11.5 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3134107059275686		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 3.3134107059275686 | validation: 3.24193497242583]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3498126527435557		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.3498126527435557 | validation: 4.31125429452043]
	TIME [epoch: 11.5 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.630485340978243		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 3.630485340978243 | validation: 3.482369411026085]
	TIME [epoch: 11.5 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271638196822801		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 3.271638196822801 | validation: 3.5428735308696595]
	TIME [epoch: 11.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.343000688421308		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 3.343000688421308 | validation: 3.656739745298509]
	TIME [epoch: 11.5 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3129038823405406		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 3.3129038823405406 | validation: 3.314244517806043]
	TIME [epoch: 11.5 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275000791457594		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 3.275000791457594 | validation: 3.435288194469558]
	TIME [epoch: 11.5 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.112562791879872		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.112562791879872 | validation: 3.2378803463408206]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232214184726111		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 3.232214184726111 | validation: 3.8958227012893425]
	TIME [epoch: 11.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4649064437846313		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.4649064437846313 | validation: 3.3049409553518947]
	TIME [epoch: 11.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0512146569162635		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 3.0512146569162635 | validation: 3.6018892875388575]
	TIME [epoch: 11.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4039850723495046		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 3.4039850723495046 | validation: 5.164276581121997]
	TIME [epoch: 11.5 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.819445007224739		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 3.819445007224739 | validation: 3.381077587715514]
	TIME [epoch: 11.5 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227473962868374		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 3.227473962868374 | validation: 3.3920665082892754]
	TIME [epoch: 11.5 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232111070683765		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 3.232111070683765 | validation: 4.09896356829905]
	TIME [epoch: 11.5 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3868517140410788		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 3.3868517140410788 | validation: 3.8791822114017793]
	TIME [epoch: 11.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2920943199479673		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 3.2920943199479673 | validation: 3.8485130925888487]
	TIME [epoch: 11.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3491626645802715		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 3.3491626645802715 | validation: 3.315282497170029]
	TIME [epoch: 11.5 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2999701220826325		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 3.2999701220826325 | validation: 3.3277039903128496]
	TIME [epoch: 11.5 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3960943818509715		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 3.3960943818509715 | validation: 3.827057098529739]
	TIME [epoch: 11.5 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3746896349290245		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 3.3746896349290245 | validation: 3.208330800959171]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1151618536613617		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 3.1151618536613617 | validation: 3.341820917242982]
	TIME [epoch: 11.5 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.231299271373508		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 3.231299271373508 | validation: 3.453207318312424]
	TIME [epoch: 11.6 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20444838573648		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 3.20444838573648 | validation: 3.6504843314265747]
	TIME [epoch: 11.5 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3058755934943864		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 3.3058755934943864 | validation: 3.1493095866506065]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0853822687071943		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 3.0853822687071943 | validation: 3.606629862367123]
	TIME [epoch: 11.5 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3206775934686403		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 3.3206775934686403 | validation: 3.2567810012820297]
	TIME [epoch: 11.5 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1274688066730696		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 3.1274688066730696 | validation: 3.4560148307843077]
	TIME [epoch: 11.5 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1640612043293084		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 3.1640612043293084 | validation: 3.864084684189926]
	TIME [epoch: 11.5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229842992425592		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 3.229842992425592 | validation: 3.389204700665406]
	TIME [epoch: 11.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3650137649735474		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 3.3650137649735474 | validation: 3.7229664250477343]
	TIME [epoch: 11.5 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3966224950288675		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 3.3966224950288675 | validation: 3.4657203696043424]
	TIME [epoch: 11.6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3395862036166744		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 3.3395862036166744 | validation: 3.1376436878606833]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.990050433327985		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 2.990050433327985 | validation: 3.932679989082034]
	TIME [epoch: 11.5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2938775779753886		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 3.2938775779753886 | validation: 4.3353190165144415]
	TIME [epoch: 11.5 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30440065015105		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 3.30440065015105 | validation: 3.2417332973253736]
	TIME [epoch: 11.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.450651161015686		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 3.450651161015686 | validation: 3.7775428720054016]
	TIME [epoch: 11.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252398325937419		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 3.252398325937419 | validation: 3.299813740608331]
	TIME [epoch: 11.5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2338992038480203		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 3.2338992038480203 | validation: 3.16965861715755]
	TIME [epoch: 11.5 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9948473110855542		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 2.9948473110855542 | validation: 3.32951596362705]
	TIME [epoch: 11.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.200047249781822		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 3.200047249781822 | validation: 3.265600813998602]
	TIME [epoch: 11.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8048453283309973		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 3.8048453283309973 | validation: 3.202484859107508]
	TIME [epoch: 11.5 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.707730878298044		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 3.707730878298044 | validation: 3.339714872556957]
	TIME [epoch: 11.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5681932340779032		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 3.5681932340779032 | validation: 3.423994107545586]
	TIME [epoch: 11.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4175778117902804		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 3.4175778117902804 | validation: 3.2451549986751385]
	TIME [epoch: 11.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326484492592876		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 3.326484492592876 | validation: 3.339783434775646]
	TIME [epoch: 11.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2484633720482994		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 3.2484633720482994 | validation: 3.470748318637934]
	TIME [epoch: 11.5 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1022429441732173		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 3.1022429441732173 | validation: 3.6952735825312777]
	TIME [epoch: 11.5 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6365457129782595		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 3.6365457129782595 | validation: 4.52534109902775]
	TIME [epoch: 11.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5108008327518476		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 3.5108008327518476 | validation: 3.493045278854656]
	TIME [epoch: 11.5 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0924565802483546		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 3.0924565802483546 | validation: 3.294191538671523]
	TIME [epoch: 11.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1106348808499154		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 3.1106348808499154 | validation: 3.2902399678022243]
	TIME [epoch: 11.5 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1040375227021557		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 3.1040375227021557 | validation: 3.348466577597989]
	TIME [epoch: 11.5 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0950682354393058		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 3.0950682354393058 | validation: 3.1330969090928718]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1156058726366043		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 3.1156058726366043 | validation: 3.687474400235883]
	TIME [epoch: 11.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0863666576692257		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 3.0863666576692257 | validation: 4.692510592469857]
	TIME [epoch: 11.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5428145279443224		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 3.5428145279443224 | validation: 4.977377614920166]
	TIME [epoch: 11.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.581764329344127		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 3.581764329344127 | validation: 4.488136181236996]
	TIME [epoch: 11.5 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3821417317606874		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 3.3821417317606874 | validation: 5.500907323503319]
	TIME [epoch: 11.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.973860551617033		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 3.973860551617033 | validation: 4.447887986868753]
	TIME [epoch: 11.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4768307313559217		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 3.4768307313559217 | validation: 3.6158843113678456]
	TIME [epoch: 11.5 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1350724889695414		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 3.1350724889695414 | validation: 3.194941947374257]
	TIME [epoch: 11.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9147932909772005		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 2.9147932909772005 | validation: 3.6162978727460975]
	TIME [epoch: 11.5 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0843141555438534		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 3.0843141555438534 | validation: 3.253421492553201]
	TIME [epoch: 11.5 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0447418197232743		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 3.0447418197232743 | validation: 3.0524642388269583]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.867133045668996		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 2.867133045668996 | validation: 3.833882743344408]
	TIME [epoch: 11.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2177064985227988		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 3.2177064985227988 | validation: 3.151495932781212]
	TIME [epoch: 11.5 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1422860204702396		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 3.1422860204702396 | validation: 3.1559074224533687]
	TIME [epoch: 11.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.899835693634782		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 2.899835693634782 | validation: 3.214231897505281]
	TIME [epoch: 11.5 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0545417144714815		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 3.0545417144714815 | validation: 3.8912184718470946]
	TIME [epoch: 11.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.125806096307275		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 3.125806096307275 | validation: 3.072289246536384]
	TIME [epoch: 11.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.008416626970264		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 3.008416626970264 | validation: 3.574828413546279]
	TIME [epoch: 11.5 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1225677029771157		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 3.1225677029771157 | validation: 3.058563318018731]
	TIME [epoch: 11.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0692373406375406		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 3.0692373406375406 | validation: 3.1971387130787265]
	TIME [epoch: 11.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0158667583967684		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 3.0158667583967684 | validation: 3.06724509467693]
	TIME [epoch: 11.5 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8917763845651843		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 2.8917763845651843 | validation: 4.061502929011546]
	TIME [epoch: 11.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.101985362989651		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 3.101985362989651 | validation: 3.2972530571903373]
	TIME [epoch: 11.5 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.144851340436193		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 3.144851340436193 | validation: 5.221302044102767]
	TIME [epoch: 11.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.067067068557403		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 4.067067068557403 | validation: 4.299998309129189]
	TIME [epoch: 11.5 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.478360372071444		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 3.478360372071444 | validation: 4.217912832118846]
	TIME [epoch: 11.5 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7173124211680846		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 3.7173124211680846 | validation: 3.9019761381792435]
	TIME [epoch: 11.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.122710579542188		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 3.122710579542188 | validation: 3.432494050678274]
	TIME [epoch: 11.5 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1681650881861447		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 3.1681650881861447 | validation: 5.006047039609067]
	TIME [epoch: 11.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3766633954997007		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 3.3766633954997007 | validation: 5.186084755397052]
	TIME [epoch: 11.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8433131928992563		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 3.8433131928992563 | validation: 4.656243631023625]
	TIME [epoch: 11.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5724667740525975		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 3.5724667740525975 | validation: 3.4926858189634675]
	TIME [epoch: 11.5 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0454534199770302		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 3.0454534199770302 | validation: 3.600620740453884]
	TIME [epoch: 11.5 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.007231567018783		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 3.007231567018783 | validation: 3.1667916921269295]
	TIME [epoch: 11.5 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.150042337965633		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 3.150042337965633 | validation: 4.86695977576563]
	TIME [epoch: 11.5 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9171584808585926		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 3.9171584808585926 | validation: 4.089465804613471]
	TIME [epoch: 11.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.516111106537715		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 3.516111106537715 | validation: 4.102025910949855]
	TIME [epoch: 11.5 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.482679441175256		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 3.482679441175256 | validation: 4.090307144218233]
	TIME [epoch: 11.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.428993313982096		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 3.428993313982096 | validation: 4.2804125517550595]
	TIME [epoch: 11.5 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4035916027756423		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 3.4035916027756423 | validation: 3.6719105780145913]
	TIME [epoch: 11.5 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0647991944956012		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 3.0647991944956012 | validation: 3.328428936725772]
	TIME [epoch: 11.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.082779741682774		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 3.082779741682774 | validation: 3.039549688425349]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.920860744269337		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 2.920860744269337 | validation: 3.050824157229432]
	TIME [epoch: 11.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.883398154812194		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 2.883398154812194 | validation: 3.360264590548715]
	TIME [epoch: 11.5 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9188663151155634		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 2.9188663151155634 | validation: 3.977922418778385]
	TIME [epoch: 11.5 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.995565884890394		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 2.995565884890394 | validation: 3.2737846328634963]
	TIME [epoch: 11.5 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9359944209134228		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 2.9359944209134228 | validation: 3.3008323825028993]
	TIME [epoch: 11.5 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.898033346803793		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 2.898033346803793 | validation: 3.4769527535757927]
	TIME [epoch: 11.5 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.057207205154817		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 3.057207205154817 | validation: 7.553314810609041]
	TIME [epoch: 11.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.372408393278382		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 5.372408393278382 | validation: 3.9009749621652503]
	TIME [epoch: 11.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1187265565219144		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 3.1187265565219144 | validation: 3.0122810760942484]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8134611463200017		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 2.8134611463200017 | validation: 3.63912744135003]
	TIME [epoch: 11.5 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.071678724343304		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 3.071678724343304 | validation: 3.7022916945156865]
	TIME [epoch: 11.5 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.031584962744472		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 3.031584962744472 | validation: 3.4847646230941804]
	TIME [epoch: 11.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9289347017975094		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 2.9289347017975094 | validation: 3.3421899224941605]
	TIME [epoch: 11.5 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.820832762619805		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 2.820832762619805 | validation: 3.2404701101231708]
	TIME [epoch: 11.5 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0492004472990577		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 3.0492004472990577 | validation: 3.102609971626951]
	TIME [epoch: 11.5 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8915606662332562		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 2.8915606662332562 | validation: 3.137011749029937]
	TIME [epoch: 11.5 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8794456572879374		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 2.8794456572879374 | validation: 3.0290168097228336]
	TIME [epoch: 11.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7937413319639224		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 2.7937413319639224 | validation: 3.2107824969219565]
	TIME [epoch: 11.5 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7462968214065855		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 3.7462968214065855 | validation: 3.763210937514614]
	TIME [epoch: 11.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.429748463293718		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 3.429748463293718 | validation: 3.0249042886930204]
	TIME [epoch: 11.5 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8983425408241033		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 2.8983425408241033 | validation: 3.1542404926758274]
	TIME [epoch: 11.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.887502433223724		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 2.887502433223724 | validation: 3.139665428670993]
	TIME [epoch: 11.5 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8668682176229807		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 2.8668682176229807 | validation: 3.1262777366692207]
	TIME [epoch: 11.5 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8371587766033293		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 2.8371587766033293 | validation: 3.222428524484739]
	TIME [epoch: 11.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8133368107974257		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 2.8133368107974257 | validation: 2.9931810322416217]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6821970916547966		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 2.6821970916547966 | validation: 4.010907766096683]
	TIME [epoch: 11.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5349990495539227		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 3.5349990495539227 | validation: 4.447111521133093]
	TIME [epoch: 11.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3875741667005737		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 3.3875741667005737 | validation: 3.890675747274914]
	TIME [epoch: 11.5 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.166856354184792		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 3.166856354184792 | validation: 4.047208859940221]
	TIME [epoch: 11.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.124914419960222		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 3.124914419960222 | validation: 3.0872223287397005]
	TIME [epoch: 11.5 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8109669589830846		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 2.8109669589830846 | validation: 2.8940406948676944]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237916051359028		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 3.237916051359028 | validation: 3.1603811259533137]
	TIME [epoch: 11.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2325141198248186		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 3.2325141198248186 | validation: 3.1483730633537927]
	TIME [epoch: 11.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.046243912175342		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 3.046243912175342 | validation: 3.7824999751719766]
	TIME [epoch: 11.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.079651896353915		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 3.079651896353915 | validation: 4.045763657590231]
	TIME [epoch: 11.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.193156652687432		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 3.193156652687432 | validation: 3.6237808810946603]
	TIME [epoch: 11.5 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.900771251911113		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 2.900771251911113 | validation: 2.9507439360082053]
	TIME [epoch: 11.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.103255164372512		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 3.103255164372512 | validation: 3.0405698740713287]
	TIME [epoch: 11.5 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1324506851662957		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 3.1324506851662957 | validation: 3.0898273839189905]
	TIME [epoch: 11.5 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.941797381105199		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 2.941797381105199 | validation: 3.0738709682925123]
	TIME [epoch: 11.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7758915375496094		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 2.7758915375496094 | validation: 3.2244287788862085]
	TIME [epoch: 11.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5169071331291235		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 3.5169071331291235 | validation: 3.0760453078251544]
	TIME [epoch: 11.5 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.04550030074516		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 3.04550030074516 | validation: 2.9576091373660005]
	TIME [epoch: 11.5 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.872929542404173		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 2.872929542404173 | validation: 2.9273565039841016]
	TIME [epoch: 11.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8942716309894125		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 2.8942716309894125 | validation: 3.191914291890453]
	TIME [epoch: 11.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.05333586024197		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 3.05333586024197 | validation: 2.9993583131637314]
	TIME [epoch: 11.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7751515915558587		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 2.7751515915558587 | validation: 3.1870251385427957]
	TIME [epoch: 11.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8010623941171664		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 2.8010623941171664 | validation: 3.221921607906627]
	TIME [epoch: 11.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.835423598782751		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 2.835423598782751 | validation: 3.6483729366279434]
	TIME [epoch: 11.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.945497170191148		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 2.945497170191148 | validation: 3.3875248397332824]
	TIME [epoch: 11.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8056692204417093		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 2.8056692204417093 | validation: 3.0158328094428066]
	TIME [epoch: 11.5 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8678880060889713		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 2.8678880060889713 | validation: 3.077517648332996]
	TIME [epoch: 11.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.840623374111427		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 2.840623374111427 | validation: 4.220286236015452]
	TIME [epoch: 11.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4194921936444604		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 3.4194921936444604 | validation: 3.004618391421088]
	TIME [epoch: 11.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1932572819817673		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 3.1932572819817673 | validation: 3.0336157697789248]
	TIME [epoch: 11.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.112605401392071		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 3.112605401392071 | validation: 3.773655769702638]
	TIME [epoch: 11.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0319117701013196		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 3.0319117701013196 | validation: 3.293982550089442]
	TIME [epoch: 11.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.835005082783516		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 2.835005082783516 | validation: 2.9611483655685307]
	TIME [epoch: 11.5 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8052091803199213		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 2.8052091803199213 | validation: 3.0310986069104535]
	TIME [epoch: 11.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.838838020642191		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 2.838838020642191 | validation: 3.123462254021748]
	TIME [epoch: 11.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7810243709935754		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 2.7810243709935754 | validation: 3.8074981694203234]
	TIME [epoch: 11.5 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3591753727338376		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 3.3591753727338376 | validation: 3.263671149218947]
	TIME [epoch: 11.5 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.85270903667643		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 2.85270903667643 | validation: 3.375163886791117]
	TIME [epoch: 11.5 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.855996023313928		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 2.855996023313928 | validation: 2.9645191236535458]
	TIME [epoch: 11.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.821064601622171		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 2.821064601622171 | validation: 3.0019493004363027]
	TIME [epoch: 11.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.793322374145508		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 2.793322374145508 | validation: 3.130922698398755]
	TIME [epoch: 11.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8752302289385243		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 2.8752302289385243 | validation: 2.909974853395827]
	TIME [epoch: 11.5 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8779906461619804		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 2.8779906461619804 | validation: 2.921801449870667]
	TIME [epoch: 11.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.859282681805994		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 2.859282681805994 | validation: 3.980151470148194]
	TIME [epoch: 11.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9722082876720055		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 2.9722082876720055 | validation: 2.9788561458794254]
	TIME [epoch: 11.5 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0894919994896854		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 3.0894919994896854 | validation: 3.08366330329016]
	TIME [epoch: 11.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0276182271880314		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 3.0276182271880314 | validation: 2.9577246927662264]
	TIME [epoch: 11.5 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.02290315764136		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 3.02290315764136 | validation: 3.04486412793041]
	TIME [epoch: 11.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.888461259069096		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 2.888461259069096 | validation: 2.956713317246392]
	TIME [epoch: 11.5 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7405916416322844		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 2.7405916416322844 | validation: 3.328439778246517]
	TIME [epoch: 11.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.740396883388157		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 2.740396883388157 | validation: 3.506397377408122]
	TIME [epoch: 11.5 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854538457987317		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 2.854538457987317 | validation: 2.9449444203611104]
	TIME [epoch: 11.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.701864843318355		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 2.701864843318355 | validation: 3.3752910571362458]
	TIME [epoch: 11.5 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8118210930544922		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 2.8118210930544922 | validation: 2.923780077921922]
	TIME [epoch: 11.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.720725635282859		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 2.720725635282859 | validation: 3.0087295871396966]
	TIME [epoch: 11.5 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7883010817919165		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 2.7883010817919165 | validation: 3.1197227960414886]
	TIME [epoch: 11.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.799425149328404		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 2.799425149328404 | validation: 2.843204184916168]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.740324169696855		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 2.740324169696855 | validation: 2.95458431550798]
	TIME [epoch: 11.5 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.742566574420424		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 2.742566574420424 | validation: 2.9209045543075387]
	TIME [epoch: 11.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0715813786676573		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 3.0715813786676573 | validation: 2.9682773786184606]
	TIME [epoch: 11.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.075695525634064		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 3.075695525634064 | validation: 3.1726512770724105]
	TIME [epoch: 11.5 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.007157464480316		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 3.007157464480316 | validation: 2.8970839786507674]
	TIME [epoch: 11.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.728090020825676		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 2.728090020825676 | validation: 2.9980775096901096]
	TIME [epoch: 11.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.759835616710567		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 2.759835616710567 | validation: 4.709969397581455]
	TIME [epoch: 11.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.402299616356393		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 3.402299616356393 | validation: 3.4350843500459884]
	TIME [epoch: 11.5 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7760179471029867		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 2.7760179471029867 | validation: 3.3735277229257417]
	TIME [epoch: 11.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.845134942425245		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 2.845134942425245 | validation: 3.1961676366910035]
	TIME [epoch: 11.5 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.750554919447445		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 2.750554919447445 | validation: 2.9705906474316452]
	TIME [epoch: 11.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.778284706750995		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 2.778284706750995 | validation: 3.0161467198291554]
	TIME [epoch: 11.5 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7783654754631404		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 2.7783654754631404 | validation: 2.8953673281409285]
	TIME [epoch: 11.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.783819590495219		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 2.783819590495219 | validation: 2.830612038274686]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7095367156014976		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 2.7095367156014976 | validation: 3.190081528870491]
	TIME [epoch: 11.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7764007965229784		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 2.7764007965229784 | validation: 2.8683623349445204]
	TIME [epoch: 11.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9661907737273916		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 2.9661907737273916 | validation: 3.014575421549633]
	TIME [epoch: 11.5 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9205033124063116		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 2.9205033124063116 | validation: 3.156140679407615]
	TIME [epoch: 11.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6818713541979227		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 2.6818713541979227 | validation: 3.1013289870306733]
	TIME [epoch: 11.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6737303624074507		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 2.6737303624074507 | validation: 3.2502375259547143]
	TIME [epoch: 11.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8891038572016488		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 2.8891038572016488 | validation: 3.555462099172721]
	TIME [epoch: 11.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8847930272115847		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 2.8847930272115847 | validation: 3.388809016157903]
	TIME [epoch: 11.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7291977684858573		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 2.7291977684858573 | validation: 2.863817185015358]
	TIME [epoch: 11.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.661263907125332		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 2.661263907125332 | validation: 3.0825431133498906]
	TIME [epoch: 11.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7157268513454538		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 2.7157268513454538 | validation: 2.810850966551795]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6236897214407815		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 2.6236897214407815 | validation: 2.9928782719963705]
	TIME [epoch: 11.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7277865830785846		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 2.7277865830785846 | validation: 2.9653438786986808]
	TIME [epoch: 11.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6946167020463907		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 2.6946167020463907 | validation: 2.841538999370339]
	TIME [epoch: 11.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.865117913808653		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 2.865117913808653 | validation: 2.848433902576987]
	TIME [epoch: 11.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6487726796814157		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 2.6487726796814157 | validation: 4.161727405703315]
	TIME [epoch: 11.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1740056393387035		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 3.1740056393387035 | validation: 3.012747082435764]
	TIME [epoch: 11.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.825994236448874		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 2.825994236448874 | validation: 3.2082395127133236]
	TIME [epoch: 11.5 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.820981790900211		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 2.820981790900211 | validation: 3.1491760313034023]
	TIME [epoch: 11.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7227412264521846		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 2.7227412264521846 | validation: 3.276028505640998]
	TIME [epoch: 11.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6912918651113937		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 2.6912918651113937 | validation: 2.8498301781253463]
	TIME [epoch: 11.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0608385720455864		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 3.0608385720455864 | validation: 3.1227580433855384]
	TIME [epoch: 11.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.910117646205414		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 2.910117646205414 | validation: 3.425757919886603]
	TIME [epoch: 11.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.813330051442212		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 2.813330051442212 | validation: 3.792010052829067]
	TIME [epoch: 11.5 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.940010838728341		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 2.940010838728341 | validation: 2.8207307917447815]
	TIME [epoch: 11.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.611230881719523		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 2.611230881719523 | validation: 3.091233022956578]
	TIME [epoch: 11.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6648084259346883		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 2.6648084259346883 | validation: 2.8398400409757487]
	TIME [epoch: 11.5 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.646321794750739		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 2.646321794750739 | validation: 2.8909518385628643]
	TIME [epoch: 11.5 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6370020783627073		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 2.6370020783627073 | validation: 2.9461173305226205]
	TIME [epoch: 11.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7328747466685552		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 2.7328747466685552 | validation: 2.931491053376875]
	TIME [epoch: 11.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.948861358034436		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 2.948861358034436 | validation: 2.8684941470141996]
	TIME [epoch: 11.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8357308203185347		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 2.8357308203185347 | validation: 2.844758432153011]
	TIME [epoch: 11.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.829427700666713		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 2.829427700666713 | validation: 2.9969648845251187]
	TIME [epoch: 11.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8300722524618616		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 2.8300722524618616 | validation: 2.9861226064414006]
	TIME [epoch: 11.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.719361740374938		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 2.719361740374938 | validation: 2.995333989502733]
	TIME [epoch: 11.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.693587862551775		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 2.693587862551775 | validation: 2.8128201829421777]
	TIME [epoch: 11.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5978302067576395		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 2.5978302067576395 | validation: 3.1247407276916888]
	TIME [epoch: 11.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6639025332967794		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 2.6639025332967794 | validation: 2.923318415501612]
	TIME [epoch: 11.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6204122567507184		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 2.6204122567507184 | validation: 3.6506554189521943]
	TIME [epoch: 11.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9809283201872026		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 2.9809283201872026 | validation: 3.825530855979772]
	TIME [epoch: 11.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9544655144953875		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 2.9544655144953875 | validation: 3.726222706370835]
	TIME [epoch: 11.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8478370240724025		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 2.8478370240724025 | validation: 3.65460261088145]
	TIME [epoch: 11.5 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854811398401541		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 2.854811398401541 | validation: 2.991853836694825]
	TIME [epoch: 11.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.967442289241973		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 2.967442289241973 | validation: 2.961165663492977]
	TIME [epoch: 11.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.772669034186833		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 2.772669034186833 | validation: 2.822161874945796]
	TIME [epoch: 11.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.654745802620868		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 2.654745802620868 | validation: 2.84667359199854]
	TIME [epoch: 11.5 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.719885210565977		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 2.719885210565977 | validation: 3.041382916636308]
	TIME [epoch: 11.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.627993865193152		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 2.627993865193152 | validation: 3.108899723158911]
	TIME [epoch: 11.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.692698641146335		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 2.692698641146335 | validation: 2.8343142635585346]
	TIME [epoch: 11.5 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.787473424544907		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 2.787473424544907 | validation: 2.841325048316617]
	TIME [epoch: 11.5 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6936657655967924		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 2.6936657655967924 | validation: 5.101912498667806]
	TIME [epoch: 11.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4673642384677894		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 3.4673642384677894 | validation: 3.04931598937815]
	TIME [epoch: 11.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.661271847162074		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 2.661271847162074 | validation: 2.989870149604001]
	TIME [epoch: 11.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.712928133211827		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 2.712928133211827 | validation: 3.4323547616613435]
	TIME [epoch: 11.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8630051457263184		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 2.8630051457263184 | validation: 2.8726174884851634]
	TIME [epoch: 11.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5756667722412523		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 2.5756667722412523 | validation: 3.038881262261784]
	TIME [epoch: 11.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5913905209294152		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 2.5913905209294152 | validation: 3.069319835900433]
	TIME [epoch: 11.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5958655528629624		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 3.5958655528629624 | validation: 2.9718482115064093]
	TIME [epoch: 11.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9243372752184587		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 2.9243372752184587 | validation: 2.8439716901882526]
	TIME [epoch: 11.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.015198616959631		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 3.015198616959631 | validation: 3.558851875356594]
	TIME [epoch: 11.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9126290609828485		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 2.9126290609828485 | validation: 2.9659932155373974]
	TIME [epoch: 11.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7702593421967294		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 2.7702593421967294 | validation: 2.889188300058118]
	TIME [epoch: 11.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.616254038858261		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 2.616254038858261 | validation: 2.8836196248659682]
	TIME [epoch: 11.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.586131574721676		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 2.586131574721676 | validation: 3.0112993103327352]
	TIME [epoch: 11.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6461474318732185		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 2.6461474318732185 | validation: 2.9583619619407044]
	TIME [epoch: 11.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7530845175753043		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 2.7530845175753043 | validation: 2.8304181508360093]
	TIME [epoch: 11.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7788070124508932		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 2.7788070124508932 | validation: 3.596373004567721]
	TIME [epoch: 11.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.808607361954853		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 2.808607361954853 | validation: 3.9679660858184866]
	TIME [epoch: 11.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.960877867738283		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 2.960877867738283 | validation: 3.0498788342876217]
	TIME [epoch: 11.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6811592720150945		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 2.6811592720150945 | validation: 3.640983155756622]
	TIME [epoch: 11.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0663333536527224		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 3.0663333536527224 | validation: 3.041638714817866]
	TIME [epoch: 11.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.695713449690139		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 2.695713449690139 | validation: 2.838529953975201]
	TIME [epoch: 11.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.584392591705935		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 2.584392591705935 | validation: 3.020231265720283]
	TIME [epoch: 11.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.711183112496262		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 2.711183112496262 | validation: 2.797653267170732]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.539550063131649		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 2.539550063131649 | validation: 2.995803996977061]
	TIME [epoch: 11.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6227016945524912		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 2.6227016945524912 | validation: 3.0850070243044474]
	TIME [epoch: 11.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6910976073401582		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 2.6910976073401582 | validation: 3.1103599073444146]
	TIME [epoch: 11.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6409611736516947		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 2.6409611736516947 | validation: 3.6564981123606053]
	TIME [epoch: 11.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.943379152222725		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 2.943379152222725 | validation: 3.0896606104430773]
	TIME [epoch: 11.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.642872823919466		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 2.642872823919466 | validation: 3.1288911345805124]
	TIME [epoch: 11.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6335008436209844		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 2.6335008436209844 | validation: 2.842023482675401]
	TIME [epoch: 11.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6417413495714617		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 2.6417413495714617 | validation: 2.9684632812575837]
	TIME [epoch: 11.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.625445691099949		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 2.625445691099949 | validation: 2.888107548234718]
	TIME [epoch: 11.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7086971134256714		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 2.7086971134256714 | validation: 2.833333808497934]
	TIME [epoch: 11.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7604187582768827		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 2.7604187582768827 | validation: 3.0133773117486977]
	TIME [epoch: 11.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6728338981097943		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 2.6728338981097943 | validation: 3.067574545305275]
	TIME [epoch: 11.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.675056315685738		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 2.675056315685738 | validation: 2.9637016433734913]
	TIME [epoch: 11.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6186531847712597		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 2.6186531847712597 | validation: 2.8962895960410187]
	TIME [epoch: 11.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6145742393815254		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 2.6145742393815254 | validation: 2.994158031815473]
	TIME [epoch: 11.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.587598411326096		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 2.587598411326096 | validation: 3.160494148777065]
	TIME [epoch: 11.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6294840327599847		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 2.6294840327599847 | validation: 2.8094808700915492]
	TIME [epoch: 11.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.123879075480504		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 3.123879075480504 | validation: 3.032799456079541]
	TIME [epoch: 11.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6948255631458515		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 2.6948255631458515 | validation: 3.2326326942560186]
	TIME [epoch: 11.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.66485805726692		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 2.66485805726692 | validation: 3.073210425040315]
	TIME [epoch: 11.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.716837738588041		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 2.716837738588041 | validation: 2.8372419257804755]
	TIME [epoch: 11.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5909773473278124		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 2.5909773473278124 | validation: 2.8157928933682004]
	TIME [epoch: 11.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5509412147489017		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 2.5509412147489017 | validation: 3.5799401724420328]
	TIME [epoch: 11.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7308296330925836		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 2.7308296330925836 | validation: 4.033303588644735]
	TIME [epoch: 11.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.942162615044449		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 2.942162615044449 | validation: 3.043084025776746]
	TIME [epoch: 11.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6681977240657613		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 2.6681977240657613 | validation: 3.444860411920383]
	TIME [epoch: 11.5 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7330823583169828		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 2.7330823583169828 | validation: 2.8514364871102127]
	TIME [epoch: 11.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.626683549709544		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 2.626683549709544 | validation: 2.844016313644598]
	TIME [epoch: 11.5 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8341500487870794		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 2.8341500487870794 | validation: 2.8443073076649057]
	TIME [epoch: 11.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.529870177066994		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 2.529870177066994 | validation: 3.3574587157716405]
	TIME [epoch: 11.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.829490607451873		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 2.829490607451873 | validation: 3.3609116324070296]
	TIME [epoch: 11.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6624201159952494		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 2.6624201159952494 | validation: 3.401019970139225]
	TIME [epoch: 11.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7215058851576193		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 2.7215058851576193 | validation: 3.5874252057608884]
	TIME [epoch: 11.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8085347341371265		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 2.8085347341371265 | validation: 3.1849679902811743]
	TIME [epoch: 11.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.72654201838383		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 2.72654201838383 | validation: 2.8279036396948145]
	TIME [epoch: 11.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.674013068374454		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 2.674013068374454 | validation: 2.952491988582033]
	TIME [epoch: 11.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.706791660632245		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 2.706791660632245 | validation: 3.0614782897297155]
	TIME [epoch: 11.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6926325191553286		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 2.6926325191553286 | validation: 3.0020932244204848]
	TIME [epoch: 11.5 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5663125022910283		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 2.5663125022910283 | validation: 2.7990691034504698]
	TIME [epoch: 11.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6231393120613733		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 2.6231393120613733 | validation: 3.2667347902884067]
	TIME [epoch: 11.5 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.760256377459357		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 2.760256377459357 | validation: 2.830340952278833]
	TIME [epoch: 11.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.747837506729355		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 2.747837506729355 | validation: 3.0038488848461897]
	TIME [epoch: 11.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6214685488990206		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 2.6214685488990206 | validation: 2.777205117512011]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6342053718380196		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 2.6342053718380196 | validation: 2.89737998632838]
	TIME [epoch: 11.5 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6424431560909563		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 2.6424431560909563 | validation: 2.8186507960681912]
	TIME [epoch: 11.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.574540731868508		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 2.574540731868508 | validation: 2.85914355973164]
	TIME [epoch: 11.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5228869954499062		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 2.5228869954499062 | validation: 2.8536618867114307]
	TIME [epoch: 11.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.52469571872584		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 2.52469571872584 | validation: 3.432760378780497]
	TIME [epoch: 11.5 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8177658184557623		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 2.8177658184557623 | validation: 3.002290716739973]
	TIME [epoch: 11.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.565958740948302		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 2.565958740948302 | validation: 3.1275834425533104]
	TIME [epoch: 11.5 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6427873554606194		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 2.6427873554606194 | validation: 2.777956474893271]
	TIME [epoch: 11.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.641118982172094		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 2.641118982172094 | validation: 2.7645383751181787]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7622921522982455		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 2.7622921522982455 | validation: 2.828708862584681]
	TIME [epoch: 11.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.739054271422578		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 2.739054271422578 | validation: 2.8283873953732748]
	TIME [epoch: 11.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5855002994949317		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 2.5855002994949317 | validation: 2.771175241855941]
	TIME [epoch: 11.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6030713795433793		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 2.6030713795433793 | validation: 2.858295107371422]
	TIME [epoch: 11.5 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.630911979034623		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 2.630911979034623 | validation: 2.7892608051160166]
	TIME [epoch: 11.5 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.566485238162797		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 2.566485238162797 | validation: 2.7508626844457185]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.530779661083576		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 2.530779661083576 | validation: 2.9577762351336547]
	TIME [epoch: 11.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5484257777025654		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 2.5484257777025654 | validation: 2.767309787025831]
	TIME [epoch: 11.5 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.473340583432305		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 2.473340583432305 | validation: 3.119137901875115]
	TIME [epoch: 11.5 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6137041508312064		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 2.6137041508312064 | validation: 3.2471494801264464]
	TIME [epoch: 11.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6445326281246784		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 2.6445326281246784 | validation: 3.084020655387544]
	TIME [epoch: 11.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.566582406121378		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 2.566582406121378 | validation: 2.7995421924357284]
	TIME [epoch: 11.5 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5391133383525126		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 2.5391133383525126 | validation: 2.7612253760339196]
	TIME [epoch: 11.5 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5941961881859865		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 2.5941961881859865 | validation: 2.817405762476147]
	TIME [epoch: 11.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5689713768450337		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 2.5689713768450337 | validation: 2.8660487502482432]
	TIME [epoch: 11.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6301272995861606		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 2.6301272995861606 | validation: 2.7604329341538265]
	TIME [epoch: 11.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.614959548426514		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 2.614959548426514 | validation: 2.77561023356276]
	TIME [epoch: 11.5 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.56673325541262		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 2.56673325541262 | validation: 3.3137491108276675]
	TIME [epoch: 11.5 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7164673168122246		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 2.7164673168122246 | validation: 2.9492779770493063]
	TIME [epoch: 11.5 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6387723848487536		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 2.6387723848487536 | validation: 2.975837654004506]
	TIME [epoch: 11.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.731952399004999		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 2.731952399004999 | validation: 2.9908637650522216]
	TIME [epoch: 11.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5885879413208306		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 2.5885879413208306 | validation: 2.850886278370408]
	TIME [epoch: 11.5 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5327109139008384		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 2.5327109139008384 | validation: 2.7628453752060502]
	TIME [epoch: 11.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.591491905443691		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 2.591491905443691 | validation: 2.771083738374648]
	TIME [epoch: 11.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6027448196464023		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 2.6027448196464023 | validation: 2.9581428890287147]
	TIME [epoch: 11.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.563876737175889		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 2.563876737175889 | validation: 3.0881478383726666]
	TIME [epoch: 11.5 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6154182177648972		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 2.6154182177648972 | validation: 3.080645996214893]
	TIME [epoch: 11.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.773019096563344		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 2.773019096563344 | validation: 2.984794497920342]
	TIME [epoch: 11.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5686832791688605		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 2.5686832791688605 | validation: 3.378221582015227]
	TIME [epoch: 11.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6764448842741		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 2.6764448842741 | validation: 2.8730659214942977]
	TIME [epoch: 11.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.565925817579652		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 2.565925817579652 | validation: 2.75164541794034]
	TIME [epoch: 11.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.544237147998329		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 2.544237147998329 | validation: 2.893050175624199]
	TIME [epoch: 11.5 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5090448221354906		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 2.5090448221354906 | validation: 2.8323958443774386]
	TIME [epoch: 11.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.517328600493569		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 2.517328600493569 | validation: 2.823338944966117]
	TIME [epoch: 11.5 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.666906745491661		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 2.666906745491661 | validation: 2.902216433695057]
	TIME [epoch: 11.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.581303958531115		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 2.581303958531115 | validation: 2.7806825319790356]
	TIME [epoch: 11.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5469116007433525		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 2.5469116007433525 | validation: 2.7851804406912666]
	TIME [epoch: 11.5 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.52589660581788		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 2.52589660581788 | validation: 2.828911271910896]
	TIME [epoch: 11.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4948581967124492		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 2.4948581967124492 | validation: 3.2996437884268324]
	TIME [epoch: 11.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6691099869323374		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 2.6691099869323374 | validation: 3.2062110981194714]
	TIME [epoch: 11.5 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6532802319122335		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 2.6532802319122335 | validation: 2.865074425757656]
	TIME [epoch: 11.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.510877125705439		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 2.510877125705439 | validation: 3.631795889945737]
	TIME [epoch: 11.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.836706246624428		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 2.836706246624428 | validation: 2.7773651480424553]
	TIME [epoch: 11.5 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.575658537550655		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 2.575658537550655 | validation: 2.8562576061024134]
	TIME [epoch: 11.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.59510676477795		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 2.59510676477795 | validation: 2.919665421767735]
	TIME [epoch: 11.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5380919716654873		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 2.5380919716654873 | validation: 2.874565323283391]
	TIME [epoch: 11.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.658493696865703		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 2.658493696865703 | validation: 2.918322044502171]
	TIME [epoch: 11.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5492345323891943		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 2.5492345323891943 | validation: 2.857163082360569]
	TIME [epoch: 11.5 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5540780243389323		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 2.5540780243389323 | validation: 3.3223646838611662]
	TIME [epoch: 11.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7600100400902248		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 2.7600100400902248 | validation: 2.7487794376033494]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5414612279089224		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 2.5414612279089224 | validation: 2.763803982610982]
	TIME [epoch: 11.5 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5355651653720894		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 2.5355651653720894 | validation: 2.7610914409709966]
	TIME [epoch: 11.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.505292298561367		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 2.505292298561367 | validation: 2.7871350012819653]
	TIME [epoch: 11.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.465158832745307		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 2.465158832745307 | validation: 2.9797284348826576]
	TIME [epoch: 11.5 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.524057044139057		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 2.524057044139057 | validation: 3.3650185612653374]
	TIME [epoch: 11.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.68984781581374		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 2.68984781581374 | validation: 2.9667618215342038]
	TIME [epoch: 11.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6209003575153655		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 2.6209003575153655 | validation: 3.029776070827639]
	TIME [epoch: 11.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.669284859991796		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 2.669284859991796 | validation: 2.995518908600575]
	TIME [epoch: 11.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6246532763306942		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 2.6246532763306942 | validation: 2.779616963746312]
	TIME [epoch: 11.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4980736256219043		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 2.4980736256219043 | validation: 2.849800411358213]
	TIME [epoch: 11.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.470702014105062		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 2.470702014105062 | validation: 2.903070546550589]
	TIME [epoch: 11.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5259593538740512		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 2.5259593538740512 | validation: 2.724414176486023]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.491475650974648		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 2.491475650974648 | validation: 3.3047151734852696]
	TIME [epoch: 11.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6923476746123844		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 2.6923476746123844 | validation: 2.8265143375273123]
	TIME [epoch: 11.5 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4937683154821793		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 2.4937683154821793 | validation: 2.73536005722905]
	TIME [epoch: 11.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466904152860382		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 2.466904152860382 | validation: 2.7661949863283155]
	TIME [epoch: 11.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4515850213124706		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 2.4515850213124706 | validation: 2.8035500919337544]
	TIME [epoch: 11.5 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.488363935064836		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 2.488363935064836 | validation: 2.756731567653045]
	TIME [epoch: 11.5 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.483878246938025		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 2.483878246938025 | validation: 2.7756678557412036]
	TIME [epoch: 11.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.510742385231616		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 2.510742385231616 | validation: 2.747568048901334]
	TIME [epoch: 11.5 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.593063690055729		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 2.593063690055729 | validation: 3.055046185110166]
	TIME [epoch: 11.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5547538779769945		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 2.5547538779769945 | validation: 3.002337542669814]
	TIME [epoch: 11.5 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5376943343314555		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 2.5376943343314555 | validation: 2.789212766686036]
	TIME [epoch: 11.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5400128542595217		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 2.5400128542595217 | validation: 3.215782914685235]
	TIME [epoch: 11.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6174020641180933		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 2.6174020641180933 | validation: 3.054357373725029]
	TIME [epoch: 11.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5724711998592396		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 2.5724711998592396 | validation: 2.7958750786491353]
	TIME [epoch: 11.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5368608286920713		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 2.5368608286920713 | validation: 2.7556284522710106]
	TIME [epoch: 11.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5507500238142513		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 2.5507500238142513 | validation: 2.819446716461585]
	TIME [epoch: 11.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5266072291676362		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 2.5266072291676362 | validation: 2.830401912779732]
	TIME [epoch: 11.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5160899329069006		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 2.5160899329069006 | validation: 2.796021089953874]
	TIME [epoch: 11.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4882906814669843		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 2.4882906814669843 | validation: 2.782852076230063]
	TIME [epoch: 11.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5525727355345027		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 2.5525727355345027 | validation: 2.8330735302842287]
	TIME [epoch: 11.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.606031983571889		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 2.606031983571889 | validation: 2.9408216098397992]
	TIME [epoch: 11.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5090746472917345		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 2.5090746472917345 | validation: 2.7901672952167904]
	TIME [epoch: 11.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4850645265905973		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 2.4850645265905973 | validation: 2.836933368193204]
	TIME [epoch: 11.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.540429628871431		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 2.540429628871431 | validation: 2.9102353145286894]
	TIME [epoch: 11.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5158854422653163		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 2.5158854422653163 | validation: 3.1422108755293534]
	TIME [epoch: 11.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5866327170107173		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 2.5866327170107173 | validation: 2.8053563835141144]
	TIME [epoch: 11.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.482466233515039		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 2.482466233515039 | validation: 2.814636254777314]
	TIME [epoch: 11.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5053950417743733		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 2.5053950417743733 | validation: 2.7825867108086677]
	TIME [epoch: 11.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5109607460474352		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 2.5109607460474352 | validation: 2.9488029007154086]
	TIME [epoch: 11.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.55492848500504		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 2.55492848500504 | validation: 2.7436004958914206]
	TIME [epoch: 11.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6155173452697476		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 2.6155173452697476 | validation: 2.7779189569338634]
	TIME [epoch: 11.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.469679918958704		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 2.469679918958704 | validation: 2.747969964401784]
	TIME [epoch: 11.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5456627871706314		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 2.5456627871706314 | validation: 2.9803347627963697]
	TIME [epoch: 11.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.591696284974364		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 2.591696284974364 | validation: 2.8182183136679657]
	TIME [epoch: 11.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4441394536208962		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 2.4441394536208962 | validation: 2.9069253136790167]
	TIME [epoch: 11.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.481940911200395		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 2.481940911200395 | validation: 2.8619505263989846]
	TIME [epoch: 11.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.491923759916151		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 2.491923759916151 | validation: 2.7710267481330004]
	TIME [epoch: 11.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437150571588163		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 2.437150571588163 | validation: 2.8553344600314916]
	TIME [epoch: 11.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.561659597462824		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 2.561659597462824 | validation: 2.7425884380478975]
	TIME [epoch: 11.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.64049904912379		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 2.64049904912379 | validation: 2.81120472636536]
	TIME [epoch: 11.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.570696349852235		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 2.570696349852235 | validation: 2.9462690902728164]
	TIME [epoch: 11.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4989224287922616		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 2.4989224287922616 | validation: 2.762980298626877]
	TIME [epoch: 11.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5169886812847815		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 2.5169886812847815 | validation: 3.069569768363757]
	TIME [epoch: 11.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6539173424020417		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 2.6539173424020417 | validation: 2.899962407940312]
	TIME [epoch: 11.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4951541841290865		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 2.4951541841290865 | validation: 2.847247432015935]
	TIME [epoch: 11.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.616271797031082		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 2.616271797031082 | validation: 2.748689252805584]
	TIME [epoch: 11.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6263378641275326		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 2.6263378641275326 | validation: 3.007445840344037]
	TIME [epoch: 11.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.543744511813126		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 2.543744511813126 | validation: 2.8496646848844467]
	TIME [epoch: 11.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4545615114131385		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 2.4545615114131385 | validation: 2.830322006387304]
	TIME [epoch: 11.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.54495038485857		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 2.54495038485857 | validation: 2.7787740770875895]
	TIME [epoch: 11.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.481136831949205		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 2.481136831949205 | validation: 2.9070057964796576]
	TIME [epoch: 11.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4835738139622165		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 2.4835738139622165 | validation: 2.8766269712200354]
	TIME [epoch: 11.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5722482183619384		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 2.5722482183619384 | validation: 2.7575238273110654]
	TIME [epoch: 11.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.581091642798976		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 2.581091642798976 | validation: 2.79009148888941]
	TIME [epoch: 11.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.497044985698013		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 2.497044985698013 | validation: 2.7466030153549137]
	TIME [epoch: 11.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5231124701360903		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 2.5231124701360903 | validation: 2.7680098897624057]
	TIME [epoch: 11.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4777831621506086		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 2.4777831621506086 | validation: 2.758916180485944]
	TIME [epoch: 11.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4417617938351253		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 2.4417617938351253 | validation: 2.927798713174219]
	TIME [epoch: 11.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5112684626714072		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 2.5112684626714072 | validation: 2.8116432625711405]
	TIME [epoch: 11.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5520205446167963		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 2.5520205446167963 | validation: 2.8846533032349346]
	TIME [epoch: 11.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.586973902310838		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 2.586973902310838 | validation: 2.9123633883422544]
	TIME [epoch: 11.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4686941389961117		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 2.4686941389961117 | validation: 2.7959638402150278]
	TIME [epoch: 11.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.538271612739516		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 2.538271612739516 | validation: 2.8092819899018626]
	TIME [epoch: 11.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.699277312662124		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 2.699277312662124 | validation: 2.8340169358059244]
	TIME [epoch: 11.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.50821362976855		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 2.50821362976855 | validation: 2.8442346242872856]
	TIME [epoch: 11.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4740258853212076		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 2.4740258853212076 | validation: 2.812608584062971]
	TIME [epoch: 11.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5068531667767378		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 2.5068531667767378 | validation: 2.8277922049484405]
	TIME [epoch: 11.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.493133529893203		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 2.493133529893203 | validation: 2.757415578098577]
	TIME [epoch: 11.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4707763785876566		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 2.4707763785876566 | validation: 2.936149899556369]
	TIME [epoch: 11.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5605693197002317		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 2.5605693197002317 | validation: 2.810056894766682]
	TIME [epoch: 11.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4637474524842964		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 2.4637474524842964 | validation: 2.950805384590843]
	TIME [epoch: 11.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4480175934602064		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 2.4480175934602064 | validation: 2.8280690881230974]
	TIME [epoch: 11.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4500001567511402		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 2.4500001567511402 | validation: 2.896427974942902]
	TIME [epoch: 11.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4822468856781263		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 2.4822468856781263 | validation: 2.7488200455710103]
	TIME [epoch: 11.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.491337375189105		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 2.491337375189105 | validation: 2.9250110792261568]
	TIME [epoch: 11.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5381532910038516		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 2.5381532910038516 | validation: 2.9641959626909675]
	TIME [epoch: 11.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5551779947108937		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 2.5551779947108937 | validation: 2.7364531103927083]
	TIME [epoch: 11.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.545606988153966		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 2.545606988153966 | validation: 2.8833784551685904]
	TIME [epoch: 11.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5267971580885495		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 2.5267971580885495 | validation: 2.7663863149562076]
	TIME [epoch: 11.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5210210115002663		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 2.5210210115002663 | validation: 2.7576236154775438]
	TIME [epoch: 11.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.492359393424736		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 2.492359393424736 | validation: 2.8992098519682314]
	TIME [epoch: 11.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4698754170499857		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 2.4698754170499857 | validation: 2.796079531872738]
	TIME [epoch: 11.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.464097227612827		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 2.464097227612827 | validation: 2.809016455106963]
	TIME [epoch: 11.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4381552010350434		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 2.4381552010350434 | validation: 2.756616377075643]
	TIME [epoch: 11.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.487740587092735		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 2.487740587092735 | validation: 2.7418926986901226]
	TIME [epoch: 11.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4390544555856533		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 2.4390544555856533 | validation: 2.7355645305917795]
	TIME [epoch: 11.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466589611441827		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 2.466589611441827 | validation: 2.766881978156031]
	TIME [epoch: 11.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4508875275109396		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 2.4508875275109396 | validation: 2.7443431049718936]
	TIME [epoch: 11.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.462849530066235		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 2.462849530066235 | validation: 2.727716273627783]
	TIME [epoch: 11.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.432107487416957		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 2.432107487416957 | validation: 2.866352299557433]
	TIME [epoch: 11.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.507191628067239		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 2.507191628067239 | validation: 3.028287738389788]
	TIME [epoch: 11.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5683086502406214		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 2.5683086502406214 | validation: 2.887778375293714]
	TIME [epoch: 11.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.532965583207951		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 2.532965583207951 | validation: 2.946924894304151]
	TIME [epoch: 11.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5196046253653366		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 2.5196046253653366 | validation: 2.7651504665669715]
	TIME [epoch: 11.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5581566702328975		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 2.5581566702328975 | validation: 2.8967325476693366]
	TIME [epoch: 11.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.480571582426492		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 2.480571582426492 | validation: 2.7511644517424707]
	TIME [epoch: 11.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4387839497792747		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 2.4387839497792747 | validation: 2.782720292008767]
	TIME [epoch: 11.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4989565628375363		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 2.4989565628375363 | validation: 2.74133119363756]
	TIME [epoch: 11.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4427913094146425		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 2.4427913094146425 | validation: 2.9906021563975758]
	TIME [epoch: 11.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.468758779510482		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 2.468758779510482 | validation: 2.7178849298689305]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4631750313947984		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 2.4631750313947984 | validation: 2.767437557775445]
	TIME [epoch: 11.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.429347226834466		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 2.429347226834466 | validation: 3.0295892578449237]
	TIME [epoch: 11.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5482754622384225		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 2.5482754622384225 | validation: 2.8574034948438656]
	TIME [epoch: 11.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.468859055893444		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 2.468859055893444 | validation: 2.7486608495053337]
	TIME [epoch: 11.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5244260687919065		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 2.5244260687919065 | validation: 2.7635302685162664]
	TIME [epoch: 11.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.674474798084167		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 2.674474798084167 | validation: 3.3467402223521048]
	TIME [epoch: 11.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5544043289273857		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 2.5544043289273857 | validation: 2.788468253778954]
	TIME [epoch: 11.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5015059418576744		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 2.5015059418576744 | validation: 2.8660307617877163]
	TIME [epoch: 11.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4684887244900535		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 2.4684887244900535 | validation: 2.75082942390913]
	TIME [epoch: 11.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4243209348459303		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 2.4243209348459303 | validation: 2.7898642966765683]
	TIME [epoch: 11.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4430087231513506		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 2.4430087231513506 | validation: 2.8711684091117786]
	TIME [epoch: 11.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.460876571684917		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 2.460876571684917 | validation: 2.7265372217185315]
	TIME [epoch: 11.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.41278669345905		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 2.41278669345905 | validation: 2.8064438996936456]
	TIME [epoch: 11.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5518542359387157		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 2.5518542359387157 | validation: 2.769888881476023]
	TIME [epoch: 11.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4654441006083907		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 2.4654441006083907 | validation: 2.7785194698707802]
	TIME [epoch: 11.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.459626604037239		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 2.459626604037239 | validation: 2.7076029420013947]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.514352303480204		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 2.514352303480204 | validation: 2.848760508083282]
	TIME [epoch: 11.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4580357081355673		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 2.4580357081355673 | validation: 2.7315209701159966]
	TIME [epoch: 11.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.416895802605612		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 2.416895802605612 | validation: 3.027307802951473]
	TIME [epoch: 11.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.569154016869735		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 2.569154016869735 | validation: 2.7252017963625215]
	TIME [epoch: 11.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4873527762454914		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 2.4873527762454914 | validation: 2.7311088613997048]
	TIME [epoch: 11.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.454894961467019		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 2.454894961467019 | validation: 3.0136709448857255]
	TIME [epoch: 11.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5114551114473533		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 2.5114551114473533 | validation: 2.756745772423057]
	TIME [epoch: 11.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.422114185379333		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 2.422114185379333 | validation: 2.752906564789721]
	TIME [epoch: 11.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.473025766516984		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 2.473025766516984 | validation: 2.774605418260037]
	TIME [epoch: 11.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4733395255583934		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 2.4733395255583934 | validation: 2.8673224439861578]
	TIME [epoch: 11.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.448802180968513		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 2.448802180968513 | validation: 2.797580018852752]
	TIME [epoch: 11.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4861123083843277		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 2.4861123083843277 | validation: 2.8052275599620726]
	TIME [epoch: 11.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4794472455416288		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 2.4794472455416288 | validation: 2.7653394089758536]
	TIME [epoch: 11.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.443851699525511		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 2.443851699525511 | validation: 2.761151365104067]
	TIME [epoch: 11.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4482746003280864		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 2.4482746003280864 | validation: 2.7063085128699202]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4405178568627846		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 2.4405178568627846 | validation: 2.723167904506631]
	TIME [epoch: 11.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.501008817750617		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 2.501008817750617 | validation: 2.7382496543153274]
	TIME [epoch: 11.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4511309835712187		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 2.4511309835712187 | validation: 2.7528059109464778]
	TIME [epoch: 11.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4472748624392215		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 2.4472748624392215 | validation: 2.806855031353196]
	TIME [epoch: 11.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5099708258006617		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 2.5099708258006617 | validation: 2.7416564326275803]
	TIME [epoch: 11.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.511362056671241		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 2.511362056671241 | validation: 2.8648625234394514]
	TIME [epoch: 11.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4495943803254754		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 2.4495943803254754 | validation: 2.703363764022833]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4202973857723533		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 2.4202973857723533 | validation: 2.7812891050465827]
	TIME [epoch: 11.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4572689593629518		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 2.4572689593629518 | validation: 2.7527272900732944]
	TIME [epoch: 11.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.426871546533863		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 2.426871546533863 | validation: 2.7679002349057056]
	TIME [epoch: 11.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4638815076505374		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 2.4638815076505374 | validation: 2.768756253792725]
	TIME [epoch: 11.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4348820167062675		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 2.4348820167062675 | validation: 2.712384128689158]
	TIME [epoch: 11.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3973705684318594		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 2.3973705684318594 | validation: 2.7633283392974226]
	TIME [epoch: 11.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.440517962368482		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 2.440517962368482 | validation: 2.817302601341317]
	TIME [epoch: 11.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.484202267394836		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 2.484202267394836 | validation: 2.8477757204702376]
	TIME [epoch: 11.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4497868726586662		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 2.4497868726586662 | validation: 2.869099985371994]
	TIME [epoch: 11.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4911684056196073		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 2.4911684056196073 | validation: 2.721048638249249]
	TIME [epoch: 11.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4905592913950403		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 2.4905592913950403 | validation: 2.7080196869818027]
	TIME [epoch: 11.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.448680403408786		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 2.448680403408786 | validation: 2.809458948303809]
	TIME [epoch: 11.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4184512803410922		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 2.4184512803410922 | validation: 2.7309099340434804]
	TIME [epoch: 11.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4912029592005065		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 2.4912029592005065 | validation: 2.8017580428760893]
	TIME [epoch: 11.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.471398078264807		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 2.471398078264807 | validation: 2.9214112998933284]
	TIME [epoch: 11.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6119413245703944		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 2.6119413245703944 | validation: 3.1392839294183834]
	TIME [epoch: 11.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.62044788293789		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 2.62044788293789 | validation: 2.7377355809075232]
	TIME [epoch: 11.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4721926152755653		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 2.4721926152755653 | validation: 2.7893826598452978]
	TIME [epoch: 11.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4605538399313502		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 2.4605538399313502 | validation: 2.758795760658062]
	TIME [epoch: 11.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437780963540635		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 2.437780963540635 | validation: 2.713815246625763]
	TIME [epoch: 11.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419555075479743		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 2.419555075479743 | validation: 2.7787880861601573]
	TIME [epoch: 11.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4388472704849296		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 2.4388472704849296 | validation: 2.8125156270372624]
	TIME [epoch: 11.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5048486941695716		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 2.5048486941695716 | validation: 2.757473090502302]
	TIME [epoch: 11.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.431367806597203		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 2.431367806597203 | validation: 2.8162223288803307]
	TIME [epoch: 11.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.415951664740692		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 2.415951664740692 | validation: 2.7191530190955087]
	TIME [epoch: 11.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4019839602468185		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 2.4019839602468185 | validation: 2.9365959217550266]
	TIME [epoch: 11.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4572414357524		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 2.4572414357524 | validation: 2.816693625528144]
	TIME [epoch: 11.5 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4904520446082783		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 2.4904520446082783 | validation: 2.7513029903818107]
	TIME [epoch: 11.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.45169191695253		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 2.45169191695253 | validation: 2.717352115557126]
	TIME [epoch: 11.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4687871773222434		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 2.4687871773222434 | validation: 2.7948346699298288]
	TIME [epoch: 11.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4843730839545146		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 2.4843730839545146 | validation: 2.7597592263508477]
	TIME [epoch: 11.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466450430043004		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 2.466450430043004 | validation: 2.8644635465007093]
	TIME [epoch: 11.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.433725697337718		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 2.433725697337718 | validation: 2.7952287425059774]
	TIME [epoch: 11.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4413117467043812		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 2.4413117467043812 | validation: 2.7208296422356764]
	TIME [epoch: 11.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.469056805134964		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 2.469056805134964 | validation: 2.770602112280685]
	TIME [epoch: 11.5 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4139201185116836		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 2.4139201185116836 | validation: 2.6985289934430265]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.413073710851341		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 2.413073710851341 | validation: 2.7488513424859464]
	TIME [epoch: 11.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.437307050530251		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 2.437307050530251 | validation: 2.7052026392289052]
	TIME [epoch: 11.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4127567281198767		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 2.4127567281198767 | validation: 2.720616231078358]
	TIME [epoch: 11.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4258125547920812		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 2.4258125547920812 | validation: 2.727340648496897]
	TIME [epoch: 11.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4237753916831424		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 2.4237753916831424 | validation: 2.7013780748319745]
	TIME [epoch: 11.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4053331620087866		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 2.4053331620087866 | validation: 2.7362986224129644]
	TIME [epoch: 11.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4244273814119834		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 2.4244273814119834 | validation: 2.9189502725004304]
	TIME [epoch: 11.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.445415069578334		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 2.445415069578334 | validation: 2.7440682530288565]
	TIME [epoch: 11.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4382056703874313		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 2.4382056703874313 | validation: 2.747527881636512]
	TIME [epoch: 11.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419249482100447		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 2.419249482100447 | validation: 2.833239863651729]
	TIME [epoch: 11.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.460019823474485		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 2.460019823474485 | validation: 2.783212705741753]
	TIME [epoch: 11.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.43345107294971		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 2.43345107294971 | validation: 2.728642189214785]
	TIME [epoch: 11.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.459051071432239		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 2.459051071432239 | validation: 2.711144574856964]
	TIME [epoch: 11.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.455103431829531		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 2.455103431829531 | validation: 2.740990548880791]
	TIME [epoch: 11.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4320620603750926		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 2.4320620603750926 | validation: 2.7105725785703076]
	TIME [epoch: 11.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4243923498396254		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 2.4243923498396254 | validation: 2.7930409646062593]
	TIME [epoch: 11.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.403671840443912		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 2.403671840443912 | validation: 2.7139505556022856]
	TIME [epoch: 11.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.423734879712202		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 2.423734879712202 | validation: 2.830155281699215]
	TIME [epoch: 11.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.494207672844996		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 2.494207672844996 | validation: 2.7714783248596264]
	TIME [epoch: 11.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4288201553379682		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 2.4288201553379682 | validation: 2.7037699023918607]
	TIME [epoch: 11.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4423313493289593		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 2.4423313493289593 | validation: 2.7188458241838465]
	TIME [epoch: 11.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.416452049589431		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 2.416452049589431 | validation: 2.7173674985085587]
	TIME [epoch: 11.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3977164085044027		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 2.3977164085044027 | validation: 2.7787728808692687]
	TIME [epoch: 11.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.415906012252493		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 2.415906012252493 | validation: 2.7157013313193286]
	TIME [epoch: 11.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4268016722624988		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 2.4268016722624988 | validation: 2.7654152313718607]
	TIME [epoch: 11.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4266754583290675		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 2.4266754583290675 | validation: 2.7355644461773365]
	TIME [epoch: 11.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4105726230272753		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 2.4105726230272753 | validation: 2.741727082277894]
	TIME [epoch: 11.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4126340557914023		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 2.4126340557914023 | validation: 2.914164810428872]
	TIME [epoch: 11.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4625299697298186		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 2.4625299697298186 | validation: 2.7215213572650065]
	TIME [epoch: 11.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4166265894435046		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 2.4166265894435046 | validation: 2.758835732015167]
	TIME [epoch: 11.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4971650857096406		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 2.4971650857096406 | validation: 2.780185755460092]
	TIME [epoch: 11.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4432824545229455		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 2.4432824545229455 | validation: 2.7128728278568155]
	TIME [epoch: 11.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3991172350166243		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 2.3991172350166243 | validation: 2.7320189517303435]
	TIME [epoch: 11.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.426484912339183		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 2.426484912339183 | validation: 2.766075005673607]
	TIME [epoch: 11.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4260962879755885		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 2.4260962879755885 | validation: 2.6820100776471443]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4115556258080115		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 2.4115556258080115 | validation: 2.7065818954276106]
	TIME [epoch: 11.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.429109993062627		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 2.429109993062627 | validation: 2.6967327921408555]
	TIME [epoch: 11.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4081324596775553		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 2.4081324596775553 | validation: 2.7233442305949938]
	TIME [epoch: 11.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4289765821969693		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 2.4289765821969693 | validation: 2.714849700546765]
	TIME [epoch: 11.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.442488144973915		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 2.442488144973915 | validation: 2.7678210419964087]
	TIME [epoch: 11.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4302350031512203		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 2.4302350031512203 | validation: 2.7361186457770263]
	TIME [epoch: 11.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4108152003868444		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 2.4108152003868444 | validation: 2.8179970249333195]
	TIME [epoch: 11.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466800710409216		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 2.466800710409216 | validation: 2.722650527274209]
	TIME [epoch: 11.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4680639125762722		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 2.4680639125762722 | validation: 2.7367245219979486]
	TIME [epoch: 11.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4157294238909337		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 2.4157294238909337 | validation: 2.7029896378781757]
	TIME [epoch: 11.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3930028379570505		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 2.3930028379570505 | validation: 2.7240980132081103]
	TIME [epoch: 11.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.448692577223893		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 2.448692577223893 | validation: 2.79944475660257]
	TIME [epoch: 11.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4108744274810663		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 2.4108744274810663 | validation: 2.9034738357498875]
	TIME [epoch: 11.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4666763717489895		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 2.4666763717489895 | validation: 2.7359923367061105]
	TIME [epoch: 11.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.395412624704294		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 2.395412624704294 | validation: 2.768333551812069]
	TIME [epoch: 11.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.43517829434263		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 2.43517829434263 | validation: 2.7741237425254783]
	TIME [epoch: 11.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4238982317569615		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 2.4238982317569615 | validation: 2.712256773148857]
	TIME [epoch: 11.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4131526226446454		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 2.4131526226446454 | validation: 2.8124820254591523]
	TIME [epoch: 11.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.420904500618419		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 2.420904500618419 | validation: 2.8105133557772284]
	TIME [epoch: 11.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4571424393070886		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 2.4571424393070886 | validation: 2.7182383953479006]
	TIME [epoch: 11.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.478740539148788		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 2.478740539148788 | validation: 2.7190395350448706]
	TIME [epoch: 11.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387398784157308		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 2.387398784157308 | validation: 2.758632612021062]
	TIME [epoch: 11.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419841423507987		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 2.419841423507987 | validation: 2.8660623728259265]
	TIME [epoch: 11.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4445473550118066		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 2.4445473550118066 | validation: 2.7331435311932086]
	TIME [epoch: 11.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4234809214975357		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 2.4234809214975357 | validation: 2.7309010676741834]
	TIME [epoch: 11.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399761795962582		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 2.399761795962582 | validation: 2.7105744785084074]
	TIME [epoch: 11.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4046756434980385		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 2.4046756434980385 | validation: 2.9152795801639115]
	TIME [epoch: 11.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.434155737433025		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 2.434155737433025 | validation: 2.847136606421226]
	TIME [epoch: 11.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6614265781552375		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 2.6614265781552375 | validation: 2.697356495535978]
	TIME [epoch: 11.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390820649613675		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 2.390820649613675 | validation: 2.725884968641316]
	TIME [epoch: 11.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.42914447030639		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 2.42914447030639 | validation: 2.738169732062687]
	TIME [epoch: 11.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4115671942767043		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 2.4115671942767043 | validation: 2.7059571244403413]
	TIME [epoch: 11.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4491391721715665		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 2.4491391721715665 | validation: 2.850388737531431]
	TIME [epoch: 11.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4117061798077173		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 2.4117061798077173 | validation: 2.7824261140773006]
	TIME [epoch: 11.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.401401069808635		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 2.401401069808635 | validation: 2.716785741641089]
	TIME [epoch: 11.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4006848778838146		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 2.4006848778838146 | validation: 2.709891119893782]
	TIME [epoch: 11.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4298139820677536		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 2.4298139820677536 | validation: 2.7479872454941496]
	TIME [epoch: 11.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4308901573434056		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 2.4308901573434056 | validation: 2.780269340766513]
	TIME [epoch: 11.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.466417689458302		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 2.466417689458302 | validation: 2.7025364817646507]
	TIME [epoch: 11.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3905614490716447		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 2.3905614490716447 | validation: 2.841048569137249]
	TIME [epoch: 11.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4427800874432566		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 2.4427800874432566 | validation: 2.7114107236050677]
	TIME [epoch: 11.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4338032360287927		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 2.4338032360287927 | validation: 2.70405293669779]
	TIME [epoch: 11.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.459161051229053		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 2.459161051229053 | validation: 2.7050877119911076]
	TIME [epoch: 11.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387047282584297		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 2.387047282584297 | validation: 2.766474336231984]
	TIME [epoch: 11.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.412974351128261		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 2.412974351128261 | validation: 2.6967393858158597]
	TIME [epoch: 11.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.398447884802246		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 2.398447884802246 | validation: 2.731028244186009]
	TIME [epoch: 11.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4501446947659655		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 2.4501446947659655 | validation: 2.72647660489083]
	TIME [epoch: 11.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4029625958370318		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 2.4029625958370318 | validation: 2.699590075336864]
	TIME [epoch: 11.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4203608418670504		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 2.4203608418670504 | validation: 2.784956337673049]
	TIME [epoch: 11.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.408386281702674		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 2.408386281702674 | validation: 2.7233236386066015]
	TIME [epoch: 11.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4206659382395834		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 2.4206659382395834 | validation: 2.804402031704918]
	TIME [epoch: 11.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4836204382771347		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 2.4836204382771347 | validation: 2.728611005828226]
	TIME [epoch: 11.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4255263058881287		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 2.4255263058881287 | validation: 2.6919473360325186]
	TIME [epoch: 11.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3920879209273327		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 2.3920879209273327 | validation: 2.7112112328968534]
	TIME [epoch: 11.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4036759228010816		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 2.4036759228010816 | validation: 2.7011487751201786]
	TIME [epoch: 11.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.493779338472252		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 2.493779338472252 | validation: 2.78276154975301]
	TIME [epoch: 11.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.408470464045643		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 2.408470464045643 | validation: 2.701140230585503]
	TIME [epoch: 11.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39637043998687		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 2.39637043998687 | validation: 2.7503740230152993]
	TIME [epoch: 11.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4372134451812775		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 2.4372134451812775 | validation: 2.7087009355866214]
	TIME [epoch: 11.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4327957339295847		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 2.4327957339295847 | validation: 2.6913999094911616]
	TIME [epoch: 11.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.404385993088675		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 2.404385993088675 | validation: 2.7303331229372705]
	TIME [epoch: 11.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4448051680703258		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 2.4448051680703258 | validation: 2.7043741407489112]
	TIME [epoch: 11.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.426339385809412		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 2.426339385809412 | validation: 2.7080453164420057]
	TIME [epoch: 11.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3954233623331027		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 2.3954233623331027 | validation: 2.7020773944384167]
	TIME [epoch: 11.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4277790258845275		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 2.4277790258845275 | validation: 2.6954464688124347]
	TIME [epoch: 11.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381477537314595		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 2.381477537314595 | validation: 2.7207729113611605]
	TIME [epoch: 11.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4557708855523046		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 2.4557708855523046 | validation: 2.752259131808973]
	TIME [epoch: 11.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4142823848531325		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 2.4142823848531325 | validation: 2.6955352553084833]
	TIME [epoch: 11.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3980998419888797		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 2.3980998419888797 | validation: 2.7348120603844244]
	TIME [epoch: 11.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390052919759821		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 2.390052919759821 | validation: 2.6955883537702765]
	TIME [epoch: 11.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.433924043569883		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 2.433924043569883 | validation: 2.6932979778247246]
	TIME [epoch: 11.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.403220457913199		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 2.403220457913199 | validation: 2.728764387816127]
	TIME [epoch: 11.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4265668938934377		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 2.4265668938934377 | validation: 2.737131224502079]
	TIME [epoch: 11.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399181583539126		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 2.399181583539126 | validation: 2.7064541938719153]
	TIME [epoch: 11.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3786911328616056		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 2.3786911328616056 | validation: 2.721480465026389]
	TIME [epoch: 11.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3924690773840367		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 2.3924690773840367 | validation: 2.7113694434235005]
	TIME [epoch: 11.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4287096787107934		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 2.4287096787107934 | validation: 2.8045936342162054]
	TIME [epoch: 11.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.408020140062102		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 2.408020140062102 | validation: 2.6845927671229637]
	TIME [epoch: 11.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3905917332022057		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 2.3905917332022057 | validation: 2.7537148083570515]
	TIME [epoch: 11.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4471467497201704		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 2.4471467497201704 | validation: 2.704656870945312]
	TIME [epoch: 11.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.384116513013751		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 2.384116513013751 | validation: 2.7078522513544057]
	TIME [epoch: 11.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.396635113192891		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 2.396635113192891 | validation: 2.8060412338091716]
	TIME [epoch: 11.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4179569106173133		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 2.4179569106173133 | validation: 2.7371560383421047]
	TIME [epoch: 11.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4016448202173613		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 2.4016448202173613 | validation: 2.734518212997439]
	TIME [epoch: 11.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3955745144331257		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 2.3955745144331257 | validation: 2.7754775037603747]
	TIME [epoch: 11.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.424685853812461		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 2.424685853812461 | validation: 2.7593694396316653]
	TIME [epoch: 11.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39781821941329		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 2.39781821941329 | validation: 2.756053697932799]
	TIME [epoch: 11.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4259846377613705		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 2.4259846377613705 | validation: 2.7177025146877645]
	TIME [epoch: 11.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3816146079494507		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 2.3816146079494507 | validation: 2.7075605923397923]
	TIME [epoch: 11.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.415753077670588		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 2.415753077670588 | validation: 2.752739954118936]
	TIME [epoch: 11.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381620320636944		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 2.381620320636944 | validation: 2.757134532636867]
	TIME [epoch: 11.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3998534625224943		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 2.3998534625224943 | validation: 2.723964420653102]
	TIME [epoch: 11.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.406384732646533		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 2.406384732646533 | validation: 2.7674709791747727]
	TIME [epoch: 11.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387053899953981		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 2.387053899953981 | validation: 2.7621897292232096]
	TIME [epoch: 11.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4066779534220197		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 2.4066779534220197 | validation: 2.7098452472379955]
	TIME [epoch: 11.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.388613258476935		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 2.388613258476935 | validation: 2.8403737880998334]
	TIME [epoch: 11.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4342680258922624		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 2.4342680258922624 | validation: 2.7718052738702403]
	TIME [epoch: 11.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.402086226304025		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 2.402086226304025 | validation: 2.7216497966212634]
	TIME [epoch: 11.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390164934913554		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 2.390164934913554 | validation: 2.6867892773827196]
	TIME [epoch: 11.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4091585594706495		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 2.4091585594706495 | validation: 2.7236098442352956]
	TIME [epoch: 11.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.424657781718445		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 2.424657781718445 | validation: 2.712988510408157]
	TIME [epoch: 11.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.382766620162945		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 2.382766620162945 | validation: 2.696082312548242]
	TIME [epoch: 11.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3800166340932702		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 2.3800166340932702 | validation: 2.7222518597284657]
	TIME [epoch: 11.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3943437637993776		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 2.3943437637993776 | validation: 2.785135758453231]
	TIME [epoch: 11.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.422414461740191		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 2.422414461740191 | validation: 2.746317854381469]
	TIME [epoch: 11.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3998473467463906		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 2.3998473467463906 | validation: 2.6962014325043904]
	TIME [epoch: 11.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3807920190582763		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 2.3807920190582763 | validation: 2.7043578834020514]
	TIME [epoch: 11.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.381196568484256		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 2.381196568484256 | validation: 2.7027530011252625]
	TIME [epoch: 11.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3978786033465713		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 2.3978786033465713 | validation: 2.728870994768022]
	TIME [epoch: 11.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.418566475373844		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 2.418566475373844 | validation: 2.6916359876464147]
	TIME [epoch: 11.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3975031672883054		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 2.3975031672883054 | validation: 2.7242572379360213]
	TIME [epoch: 11.5 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.458706878573866		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 2.458706878573866 | validation: 2.695189196793767]
	TIME [epoch: 11.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.433951857571002		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 2.433951857571002 | validation: 2.6996962004162857]
	TIME [epoch: 11.5 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4031114059172864		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 2.4031114059172864 | validation: 2.7074451612826556]
	TIME [epoch: 11.5 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372054262079208		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 2.372054262079208 | validation: 2.601976705335837]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4074307566155126		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 2.4074307566155126 | validation: 2.6964964576719224]
	TIME [epoch: 11.8 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3907236303117045		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 2.3907236303117045 | validation: 2.6885028415391377]
	TIME [epoch: 11.5 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39482806438326		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 2.39482806438326 | validation: 2.7015768067243666]
	TIME [epoch: 11.5 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.383886231497326		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 2.383886231497326 | validation: 2.72412519264048]
	TIME [epoch: 11.5 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4092196679742663		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 2.4092196679742663 | validation: 2.7631498715013527]
	TIME [epoch: 11.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4078000548540075		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 2.4078000548540075 | validation: 2.725905673533912]
	TIME [epoch: 11.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3910536881195634		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 2.3910536881195634 | validation: 2.7077625252469386]
	TIME [epoch: 11.5 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419599246384374		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 2.419599246384374 | validation: 2.695606502338066]
	TIME [epoch: 11.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4062920714195037		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 2.4062920714195037 | validation: 2.745004112145127]
	TIME [epoch: 11.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.433367424610592		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 2.433367424610592 | validation: 2.7345701768559416]
	TIME [epoch: 11.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4495690244434662		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 2.4495690244434662 | validation: 2.7168777085809293]
	TIME [epoch: 11.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374061647675285		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 2.374061647675285 | validation: 2.814421284202715]
	TIME [epoch: 11.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4154666641315683		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 2.4154666641315683 | validation: 2.7759675303100164]
	TIME [epoch: 11.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4334518570949615		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 2.4334518570949615 | validation: 2.6985237485209006]
	TIME [epoch: 11.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3710410835908298		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 2.3710410835908298 | validation: 2.760483461697675]
	TIME [epoch: 11.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3808264038108677		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 2.3808264038108677 | validation: 2.6909415818626043]
	TIME [epoch: 11.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3971915114072733		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 2.3971915114072733 | validation: 2.711317317817892]
	TIME [epoch: 11.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390994969597661		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 2.390994969597661 | validation: 2.7023098240873105]
	TIME [epoch: 11.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3946962621244774		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 2.3946962621244774 | validation: 2.826911005672146]
	TIME [epoch: 11.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4372955722842065		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 2.4372955722842065 | validation: 2.7563907826021863]
	TIME [epoch: 11.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.413832847412202		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 2.413832847412202 | validation: 2.7079075944524447]
	TIME [epoch: 11.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3848332387569418		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 2.3848332387569418 | validation: 2.7140982242736515]
	TIME [epoch: 11.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375126440320356		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 2.375126440320356 | validation: 2.699688933982188]
	TIME [epoch: 11.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4149968599044156		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 2.4149968599044156 | validation: 2.711745865233479]
	TIME [epoch: 11.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399429600095631		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 2.399429600095631 | validation: 2.7124335770830323]
	TIME [epoch: 11.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.395697883404619		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 2.395697883404619 | validation: 2.7116289844874384]
	TIME [epoch: 11.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.427749020704319		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 2.427749020704319 | validation: 2.7227590228989373]
	TIME [epoch: 11.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3877988811124076		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 2.3877988811124076 | validation: 2.6832131979339375]
	TIME [epoch: 11.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.418271991213605		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 2.418271991213605 | validation: 2.7002030734191496]
	TIME [epoch: 11.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3770258137287414		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 2.3770258137287414 | validation: 2.696728388879693]
	TIME [epoch: 11.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3831042929479054		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 2.3831042929479054 | validation: 2.719611195347428]
	TIME [epoch: 11.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.423614782108363		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 2.423614782108363 | validation: 2.7061279606570543]
	TIME [epoch: 11.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.388171167406577		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 2.388171167406577 | validation: 2.697371408804019]
	TIME [epoch: 11.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3948114923879134		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 2.3948114923879134 | validation: 2.7059477339261515]
	TIME [epoch: 11.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387368369725393		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 2.387368369725393 | validation: 2.733245290710223]
	TIME [epoch: 11.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.389501773405467		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 2.389501773405467 | validation: 2.7261816800822034]
	TIME [epoch: 11.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4219016025596645		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 2.4219016025596645 | validation: 2.6982382177421567]
	TIME [epoch: 11.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.443459878675008		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 2.443459878675008 | validation: 2.738465294990011]
	TIME [epoch: 11.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3870135032592223		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 2.3870135032592223 | validation: 2.6902960817794757]
	TIME [epoch: 11.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.41570402716279		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 2.41570402716279 | validation: 2.686381860071648]
	TIME [epoch: 11.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374661242439197		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 2.374661242439197 | validation: 2.684774967028072]
	TIME [epoch: 11.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.489009325185495		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 2.489009325185495 | validation: 2.683508494690279]
	TIME [epoch: 11.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.447831549281621		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 2.447831549281621 | validation: 2.759353605156343]
	TIME [epoch: 11.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378441570123064		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 2.378441570123064 | validation: 2.6870496052196944]
	TIME [epoch: 11.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.393921395467107		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 2.393921395467107 | validation: 2.6866775066343873]
	TIME [epoch: 11.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387174723249967		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 2.387174723249967 | validation: 2.771826101528618]
	TIME [epoch: 11.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.386296962079169		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 2.386296962079169 | validation: 2.73379366287897]
	TIME [epoch: 11.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.386032542775226		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 2.386032542775226 | validation: 2.7605859341220778]
	TIME [epoch: 11.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4070068572294607		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 2.4070068572294607 | validation: 2.7286555252606814]
	TIME [epoch: 11.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4048889609578046		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 2.4048889609578046 | validation: 2.69714726901224]
	TIME [epoch: 11.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.394028473092867		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 2.394028473092867 | validation: 2.698447424584894]
	TIME [epoch: 11.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.432024646785645		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 2.432024646785645 | validation: 2.6849840996160443]
	TIME [epoch: 11.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3675613029533817		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 2.3675613029533817 | validation: 2.744349455967049]
	TIME [epoch: 11.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.412252964205536		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 2.412252964205536 | validation: 2.700023244229793]
	TIME [epoch: 11.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3928059784603612		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 2.3928059784603612 | validation: 2.76678961923605]
	TIME [epoch: 11.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379390057644687		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 2.379390057644687 | validation: 2.7016056148862457]
	TIME [epoch: 11.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3778967595524705		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 2.3778967595524705 | validation: 2.796520912641168]
	TIME [epoch: 11.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4427878692765814		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 2.4427878692765814 | validation: 2.6949996799175766]
	TIME [epoch: 11.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4117575847996062		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 2.4117575847996062 | validation: 2.683246818184257]
	TIME [epoch: 11.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4011272565804864		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 2.4011272565804864 | validation: 2.6889610076509096]
	TIME [epoch: 11.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375438655407022		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 2.375438655407022 | validation: 2.689014534135477]
	TIME [epoch: 11.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.406600473573315		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 2.406600473573315 | validation: 2.798797538913626]
	TIME [epoch: 11.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.450778057985123		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 2.450778057985123 | validation: 2.710610716174955]
	TIME [epoch: 11.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4139629588888747		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 2.4139629588888747 | validation: 2.7269433740811624]
	TIME [epoch: 11.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.383525416389887		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 2.383525416389887 | validation: 2.756582436239577]
	TIME [epoch: 11.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4024647875033427		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 2.4024647875033427 | validation: 2.7125421276127177]
	TIME [epoch: 11.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4011538217474815		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 2.4011538217474815 | validation: 2.708994166770362]
	TIME [epoch: 11.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.376833403302465		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 2.376833403302465 | validation: 2.736506456098082]
	TIME [epoch: 11.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4062735766199292		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 2.4062735766199292 | validation: 2.691342515992575]
	TIME [epoch: 11.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.384059709469042		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 2.384059709469042 | validation: 2.708921494593834]
	TIME [epoch: 11.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3721897216447565		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 2.3721897216447565 | validation: 2.7361199443318514]
	TIME [epoch: 11.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4049726838604015		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 2.4049726838604015 | validation: 2.706258392984848]
	TIME [epoch: 11.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379684186608441		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 2.379684186608441 | validation: 2.7032713190300526]
	TIME [epoch: 11.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3788661928904267		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 2.3788661928904267 | validation: 2.7103377073150847]
	TIME [epoch: 11.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3771488191794696		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 2.3771488191794696 | validation: 2.693797184487625]
	TIME [epoch: 11.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.418590513895959		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 2.418590513895959 | validation: 2.6906344536066364]
	TIME [epoch: 11.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4093823750255963		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 2.4093823750255963 | validation: 2.6923032428372573]
	TIME [epoch: 11.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.393182795027392		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 2.393182795027392 | validation: 2.7344436128000273]
	TIME [epoch: 11.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.411517443248935		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 2.411517443248935 | validation: 2.722807033808191]
	TIME [epoch: 11.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.386325951815351		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 2.386325951815351 | validation: 2.7089345834129355]
	TIME [epoch: 11.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3776693823902173		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 2.3776693823902173 | validation: 2.75114243122333]
	TIME [epoch: 11.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3860200960741045		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 2.3860200960741045 | validation: 2.707598224358717]
	TIME [epoch: 11.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3923579024593984		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 2.3923579024593984 | validation: 2.8877499258039085]
	TIME [epoch: 11.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.417119153257007		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 2.417119153257007 | validation: 2.6829061015430065]
	TIME [epoch: 11.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3751192974574695		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 2.3751192974574695 | validation: 2.728990633308886]
	TIME [epoch: 11.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372934600901256		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 2.372934600901256 | validation: 2.7156513150853674]
	TIME [epoch: 11.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.403458205522531		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 2.403458205522531 | validation: 2.736279699791294]
	TIME [epoch: 11.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.389816310648793		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 2.389816310648793 | validation: 2.6836340338418365]
	TIME [epoch: 11.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3963904960778013		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 2.3963904960778013 | validation: 2.683817588219122]
	TIME [epoch: 11.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4028995903674994		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 2.4028995903674994 | validation: 2.802756086321516]
	TIME [epoch: 11.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4237386451663845		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 2.4237386451663845 | validation: 2.721186479126377]
	TIME [epoch: 11.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.408028294982289		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 2.408028294982289 | validation: 2.7181834699271237]
	TIME [epoch: 11.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.396354474554763		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 2.396354474554763 | validation: 2.683317258206703]
	TIME [epoch: 11.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3684498546149504		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 2.3684498546149504 | validation: 2.7661802825481185]
	TIME [epoch: 11.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3849407082040788		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 2.3849407082040788 | validation: 2.7149671052820255]
	TIME [epoch: 11.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3876342042419085		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 2.3876342042419085 | validation: 2.785599951212522]
	TIME [epoch: 11.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4266956092597365		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 2.4266956092597365 | validation: 2.7228041615695178]
	TIME [epoch: 11.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.383383894513682		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 2.383383894513682 | validation: 2.6953057497246515]
	TIME [epoch: 11.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3740495491513305		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 2.3740495491513305 | validation: 2.6867647044494674]
	TIME [epoch: 11.5 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.368026173951758		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 2.368026173951758 | validation: 2.7116779847976953]
	TIME [epoch: 11.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3698691723214087		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 2.3698691723214087 | validation: 2.6973764337896036]
	TIME [epoch: 11.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.37675263926203		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 2.37675263926203 | validation: 2.687439550540882]
	TIME [epoch: 11.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390146567783296		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 2.390146567783296 | validation: 2.686217424482505]
	TIME [epoch: 11.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4213845985649023		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 2.4213845985649023 | validation: 2.7375316776345664]
	TIME [epoch: 11.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3906617770812955		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 2.3906617770812955 | validation: 2.718180860426617]
	TIME [epoch: 11.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.383432467148942		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 2.383432467148942 | validation: 2.7031818411665824]
	TIME [epoch: 11.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4181194133221893		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 2.4181194133221893 | validation: 2.694133084610328]
	TIME [epoch: 11.5 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.376652596154696		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 2.376652596154696 | validation: 2.6842870842995956]
	TIME [epoch: 11.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.36524593408945		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 2.36524593408945 | validation: 2.7253898920745088]
	TIME [epoch: 11.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3704543566310012		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 2.3704543566310012 | validation: 2.741001558155134]
	TIME [epoch: 11.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4395313773515452		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 2.4395313773515452 | validation: 2.730234525281288]
	TIME [epoch: 11.5 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4248191620384976		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 2.4248191620384976 | validation: 2.75609247998088]
	TIME [epoch: 11.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.404025912973256		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 2.404025912973256 | validation: 2.6888676346377607]
	TIME [epoch: 11.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3655137393807015		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 2.3655137393807015 | validation: 2.716328209756929]
	TIME [epoch: 11.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375230558570398		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 2.375230558570398 | validation: 2.7096915930034657]
	TIME [epoch: 11.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3659147170583896		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 2.3659147170583896 | validation: 2.691528225591127]
	TIME [epoch: 11.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4327988476702767		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 2.4327988476702767 | validation: 2.7154555647092957]
	TIME [epoch: 11.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3878890213530424		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 2.3878890213530424 | validation: 2.759336479539261]
	TIME [epoch: 11.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3864490171842765		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 2.3864490171842765 | validation: 2.690683552008962]
	TIME [epoch: 11.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3750602200756497		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 2.3750602200756497 | validation: 2.685932742758749]
	TIME [epoch: 11.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379714542846491		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 2.379714542846491 | validation: 2.6981543646735715]
	TIME [epoch: 11.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4072374320056307		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 2.4072374320056307 | validation: 2.7424967479398332]
	TIME [epoch: 11.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4055064568731717		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 2.4055064568731717 | validation: 2.7276102229974764]
	TIME [epoch: 11.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3643381352026815		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 2.3643381352026815 | validation: 2.6971426644735574]
	TIME [epoch: 11.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3624421545243877		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 2.3624421545243877 | validation: 2.7978077222675877]
	TIME [epoch: 11.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.414248327930656		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 2.414248327930656 | validation: 2.603412864806187]
	TIME [epoch: 11.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32847937231411		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 2.32847937231411 | validation: 2.4548256989354402]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_940.pth
	Model improved!!!
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.277478091847853		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 2.277478091847853 | validation: 2.5020150546668605]
	TIME [epoch: 11.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322451639324905		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 2.322451639324905 | validation: 2.7258179443717734]
	TIME [epoch: 11.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.407101922809037		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 2.407101922809037 | validation: 2.7432375716215405]
	TIME [epoch: 11.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3672919044803384		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 2.3672919044803384 | validation: 2.6947383895231214]
	TIME [epoch: 11.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3721163358589226		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 2.3721163358589226 | validation: 2.684373541187233]
	TIME [epoch: 11.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3763418467423016		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 2.3763418467423016 | validation: 2.7024371970148064]
	TIME [epoch: 11.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3844918961523955		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 2.3844918961523955 | validation: 2.699464595633785]
	TIME [epoch: 11.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3906353467757877		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 2.3906353467757877 | validation: 2.7077876431827446]
	TIME [epoch: 11.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.376211017738287		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 2.376211017738287 | validation: 2.6898754043867705]
	TIME [epoch: 11.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3790658843373786		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 2.3790658843373786 | validation: 2.688884595760601]
	TIME [epoch: 11.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374007958373304		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 2.374007958373304 | validation: 2.700123353950244]
	TIME [epoch: 11.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.382914837751957		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 2.382914837751957 | validation: 2.6818793620199926]
	TIME [epoch: 11.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370882793224353		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 2.370882793224353 | validation: 2.7048942353882843]
	TIME [epoch: 11.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370731965686651		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 2.370731965686651 | validation: 2.7213736587349127]
	TIME [epoch: 11.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3712801274964845		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 2.3712801274964845 | validation: 2.6916156207605093]
	TIME [epoch: 11.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3633427659994886		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 2.3633427659994886 | validation: 2.6888081363491345]
	TIME [epoch: 11.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.364149316036146		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 2.364149316036146 | validation: 2.7282972693286434]
	TIME [epoch: 11.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4319597334810283		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 2.4319597334810283 | validation: 2.7077892042664167]
	TIME [epoch: 11.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378798243150311		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 2.378798243150311 | validation: 2.7159661278168215]
	TIME [epoch: 11.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3783872480442376		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 2.3783872480442376 | validation: 2.7402505397079393]
	TIME [epoch: 11.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375736959455723		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 2.375736959455723 | validation: 2.7079630389007328]
	TIME [epoch: 11.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.385643365999133		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 2.385643365999133 | validation: 2.6872915914433775]
	TIME [epoch: 11.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.392391245451183		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 2.392391245451183 | validation: 2.695103098121558]
	TIME [epoch: 11.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3671557186593146		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 2.3671557186593146 | validation: 2.676924126325859]
	TIME [epoch: 11.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3799330468748634		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 2.3799330468748634 | validation: 2.7687944907051225]
	TIME [epoch: 11.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.390284599821559		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 2.390284599821559 | validation: 2.7074058518529256]
	TIME [epoch: 11.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.368319390069424		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 2.368319390069424 | validation: 2.6893693032737596]
	TIME [epoch: 11.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.407457170144096		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 2.407457170144096 | validation: 2.7155896128673795]
	TIME [epoch: 11.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3755318784635957		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 2.3755318784635957 | validation: 2.7066912599490185]
	TIME [epoch: 11.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3799170162886845		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 2.3799170162886845 | validation: 2.6800966974980454]
	TIME [epoch: 11.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3697215686321194		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 2.3697215686321194 | validation: 2.68487542230019]
	TIME [epoch: 11.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3768276730303173		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 2.3768276730303173 | validation: 2.6838122635506636]
	TIME [epoch: 11.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3631088737122825		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 2.3631088737122825 | validation: 2.686169689177482]
	TIME [epoch: 11.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3677077102285855		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 2.3677077102285855 | validation: 2.6969940944269895]
	TIME [epoch: 11.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3792037739804606		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 2.3792037739804606 | validation: 2.690921800378485]
	TIME [epoch: 11.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3607665673177554		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 2.3607665673177554 | validation: 2.6915337166152336]
	TIME [epoch: 11.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3697075546628352		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 2.3697075546628352 | validation: 2.711355307109022]
	TIME [epoch: 11.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369699512446174		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 2.369699512446174 | validation: 2.6911446181395284]
	TIME [epoch: 11.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3759704331794467		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 2.3759704331794467 | validation: 2.6906062262140336]
	TIME [epoch: 11.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.371936875898184		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 2.371936875898184 | validation: 2.6939439136015473]
	TIME [epoch: 11.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3752383145678033		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 2.3752383145678033 | validation: 2.6984264361486487]
	TIME [epoch: 11.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3744727530677525		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 2.3744727530677525 | validation: 2.6869711665250966]
	TIME [epoch: 11.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3769653914027398		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 2.3769653914027398 | validation: 2.7812409857732745]
	TIME [epoch: 11.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3852828428533686		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 2.3852828428533686 | validation: 2.6918642485467723]
	TIME [epoch: 11.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3807429205475055		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 2.3807429205475055 | validation: 2.7362756923978724]
	TIME [epoch: 11.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3672924748515767		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 2.3672924748515767 | validation: 2.700084802134702]
	TIME [epoch: 11.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3836602291929205		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 2.3836602291929205 | validation: 2.7405795734221248]
	TIME [epoch: 11.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4005754622011812		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 2.4005754622011812 | validation: 2.769503525228155]
	TIME [epoch: 11.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3744657798819198		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 2.3744657798819198 | validation: 2.7200911862582]
	TIME [epoch: 11.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4040835270110787		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 2.4040835270110787 | validation: 2.6925244805716684]
	TIME [epoch: 11.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4144876645447058		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 2.4144876645447058 | validation: 2.876161303460881]
	TIME [epoch: 11.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4100209301584217		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 2.4100209301584217 | validation: 2.6956796942628007]
	TIME [epoch: 11.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3683598998615514		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 2.3683598998615514 | validation: 2.68654780219973]
	TIME [epoch: 11.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3677509757315693		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 2.3677509757315693 | validation: 2.775200565715642]
	TIME [epoch: 11.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4091214516996264		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 2.4091214516996264 | validation: 2.6933094233199757]
	TIME [epoch: 11.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3713617009174377		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 2.3713617009174377 | validation: 2.725498833514081]
	TIME [epoch: 11.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374210769588553		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 2.374210769588553 | validation: 2.694167736616024]
	TIME [epoch: 11.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4344007947543895		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 2.4344007947543895 | validation: 2.704682246728514]
	TIME [epoch: 11.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4119090968588965		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 2.4119090968588965 | validation: 2.6959434020574156]
	TIME [epoch: 11.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3906943726553243		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 2.3906943726553243 | validation: 2.8021703963916127]
	TIME [epoch: 11.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3906197429288136		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 2.3906197429288136 | validation: 2.705624833792059]
	TIME [epoch: 11.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.38506428616168		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 2.38506428616168 | validation: 2.688885067927742]
	TIME [epoch: 11.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3735494377768385		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 2.3735494377768385 | validation: 2.6889245549442196]
	TIME [epoch: 11.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372227112376014		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 2.372227112376014 | validation: 2.690705035130011]
	TIME [epoch: 11.5 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3699294073692023		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 2.3699294073692023 | validation: 2.751778623551294]
	TIME [epoch: 11.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3654531532329934		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 2.3654531532329934 | validation: 2.6925251340782923]
	TIME [epoch: 11.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3976714123176377		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 2.3976714123176377 | validation: 2.7002075166761785]
	TIME [epoch: 11.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.388516763575222		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 2.388516763575222 | validation: 2.6801517735545963]
	TIME [epoch: 11.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3589178448554815		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 2.3589178448554815 | validation: 2.6870158757254705]
	TIME [epoch: 11.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.419587198424717		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 2.419587198424717 | validation: 2.689830742442565]
	TIME [epoch: 11.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.368353408480119		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 2.368353408480119 | validation: 2.722951717122123]
	TIME [epoch: 11.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3713644332249535		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 2.3713644332249535 | validation: 2.6964410703910717]
	TIME [epoch: 11.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3837731162309304		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 2.3837731162309304 | validation: 2.71609547159362]
	TIME [epoch: 11.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.377359659934352		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 2.377359659934352 | validation: 2.6836975189272105]
	TIME [epoch: 11.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.363386253753296		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 2.363386253753296 | validation: 2.732682745223875]
	TIME [epoch: 11.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3943648764234324		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 2.3943648764234324 | validation: 2.7148589667649183]
	TIME [epoch: 11.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3781959377842448		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 2.3781959377842448 | validation: 2.69867617146784]
	TIME [epoch: 11.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.371426367404855		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 2.371426367404855 | validation: 2.714010171128907]
	TIME [epoch: 11.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369495192311087		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 2.369495192311087 | validation: 2.698794041882294]
	TIME [epoch: 11.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360372935762481		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 2.360372935762481 | validation: 2.707186672880208]
	TIME [epoch: 11.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.356661169591559		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 2.356661169591559 | validation: 2.626157652163895]
	TIME [epoch: 11.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3694574662178507		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 2.3694574662178507 | validation: 2.728337002900564]
	TIME [epoch: 11.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3649189347910315		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 2.3649189347910315 | validation: 2.6075536014499203]
	TIME [epoch: 11.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3454564363443526		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 2.3454564363443526 | validation: 2.681447077154737]
	TIME [epoch: 11.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3612628546726557		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 2.3612628546726557 | validation: 2.701233199902148]
	TIME [epoch: 11.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343466216435645		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 2.343466216435645 | validation: 2.698534255829856]
	TIME [epoch: 11.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.351452158495943		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 2.351452158495943 | validation: 2.688965726165163]
	TIME [epoch: 11.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3643372320963034		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 2.3643372320963034 | validation: 2.703032371725529]
	TIME [epoch: 11.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3748725306226373		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 2.3748725306226373 | validation: 2.6754347846625786]
	TIME [epoch: 11.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3642817344464726		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 2.3642817344464726 | validation: 2.7155954108057854]
	TIME [epoch: 11.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3718766816768175		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 2.3718766816768175 | validation: 2.7001393409900345]
	TIME [epoch: 11.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3630326110595403		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 2.3630326110595403 | validation: 2.6963739715926884]
	TIME [epoch: 11.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3658682647010005		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 2.3658682647010005 | validation: 2.729476923283546]
	TIME [epoch: 11.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.375321990897031		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 2.375321990897031 | validation: 2.711278842265217]
	TIME [epoch: 11.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374953463411826		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 2.374953463411826 | validation: 2.6951473280830487]
	TIME [epoch: 11.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3586328035968593		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 2.3586328035968593 | validation: 2.6853786056899493]
	TIME [epoch: 11.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.363515716790376		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 2.363515716790376 | validation: 2.701387787725127]
	TIME [epoch: 11.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3661772692248535		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 2.3661772692248535 | validation: 2.7114235340434285]
	TIME [epoch: 11.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.358862556603331		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 2.358862556603331 | validation: 2.7329059972165646]
	TIME [epoch: 11.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369567656660628		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 2.369567656660628 | validation: 2.684191293458965]
	TIME [epoch: 11.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3613928360344034		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 2.3613928360344034 | validation: 2.6837741099855474]
	TIME [epoch: 11.5 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3594701551532276		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 2.3594701551532276 | validation: 2.7603215650634563]
	TIME [epoch: 11.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4145531293357996		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 2.4145531293357996 | validation: 2.712253886397773]
	TIME [epoch: 11.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3646721359819933		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 2.3646721359819933 | validation: 2.6901844584755064]
	TIME [epoch: 11.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3656960255715718		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 2.3656960255715718 | validation: 2.7329622005101375]
	TIME [epoch: 11.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3951654966890206		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 2.3951654966890206 | validation: 2.687061934113741]
	TIME [epoch: 11.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3509648434884145		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 2.3509648434884145 | validation: 2.6211236997510623]
	TIME [epoch: 11.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336638308468943		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 2.336638308468943 | validation: 2.499774998484667]
	TIME [epoch: 11.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3004076463304806		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 2.3004076463304806 | validation: 2.4638009746202494]
	TIME [epoch: 11.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3025700031111076		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 2.3025700031111076 | validation: 2.633324178619457]
	TIME [epoch: 11.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.351317588032162		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 2.351317588032162 | validation: 2.6823594385077967]
	TIME [epoch: 11.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3652807362533017		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 2.3652807362533017 | validation: 2.66282160349596]
	TIME [epoch: 11.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.353686826908904		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 2.353686826908904 | validation: 2.687595775757484]
	TIME [epoch: 11.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3374508120968427		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 2.3374508120968427 | validation: 2.5646831990250245]
	TIME [epoch: 11.5 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3197410814594006		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 2.3197410814594006 | validation: 2.4203256997066727]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1055.pth
	Model improved!!!
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2513282815584224		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 2.2513282815584224 | validation: 2.5292589369909098]
	TIME [epoch: 11.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.329241989456858		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 2.329241989456858 | validation: 2.7167429337598072]
	TIME [epoch: 11.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3533673932707684		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 2.3533673932707684 | validation: 2.6431171074953466]
	TIME [epoch: 11.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3582280986451676		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 2.3582280986451676 | validation: 2.694564134442245]
	TIME [epoch: 11.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3816963942558775		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 2.3816963942558775 | validation: 2.7046718465463098]
	TIME [epoch: 11.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3752060154861496		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 2.3752060154861496 | validation: 2.690962873781715]
	TIME [epoch: 11.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3716840363161467		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 2.3716840363161467 | validation: 2.703595651571218]
	TIME [epoch: 11.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.361958055577361		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 2.361958055577361 | validation: 2.6916103865668455]
	TIME [epoch: 11.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3697448655915805		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 2.3697448655915805 | validation: 2.691042504988859]
	TIME [epoch: 11.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3743114379502033		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 2.3743114379502033 | validation: 2.702583690575571]
	TIME [epoch: 11.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3639253816076105		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 2.3639253816076105 | validation: 2.6855589784988694]
	TIME [epoch: 11.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3706219943451585		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 2.3706219943451585 | validation: 2.702866547520238]
	TIME [epoch: 11.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.373206280701728		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 2.373206280701728 | validation: 2.6770824359926015]
	TIME [epoch: 11.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3751269366230927		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 2.3751269366230927 | validation: 2.6829111449703347]
	TIME [epoch: 11.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3580310037090064		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 2.3580310037090064 | validation: 2.703268368038205]
	TIME [epoch: 11.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3730113884648016		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 2.3730113884648016 | validation: 2.686851329263151]
	TIME [epoch: 11.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3438003505616396		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 2.3438003505616396 | validation: 2.709276876570441]
	TIME [epoch: 11.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3625825749295397		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 2.3625825749295397 | validation: 2.701988341384141]
	TIME [epoch: 11.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3776836208389187		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 2.3776836208389187 | validation: 2.704234749242492]
	TIME [epoch: 11.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.39289658708159		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 2.39289658708159 | validation: 2.7059217320968494]
	TIME [epoch: 11.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3602405701816074		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 2.3602405701816074 | validation: 2.7018916053962494]
	TIME [epoch: 11.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359970210260755		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 2.359970210260755 | validation: 2.704939106476098]
	TIME [epoch: 11.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3619654754264823		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 2.3619654754264823 | validation: 2.6872760062119254]
	TIME [epoch: 11.5 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.361795454738644		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 2.361795454738644 | validation: 2.7183989774125688]
	TIME [epoch: 11.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.393885770638339		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 2.393885770638339 | validation: 2.682687219865693]
	TIME [epoch: 11.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3655929997329053		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 2.3655929997329053 | validation: 2.695175519170651]
	TIME [epoch: 11.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3632950273421054		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 2.3632950273421054 | validation: 2.680903992212043]
	TIME [epoch: 11.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.358652991897352		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 2.358652991897352 | validation: 2.6993822484558656]
	TIME [epoch: 11.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3774382398943823		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 2.3774382398943823 | validation: 2.721418536704413]
	TIME [epoch: 11.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3723490949592034		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 2.3723490949592034 | validation: 2.6853273497109833]
	TIME [epoch: 11.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3634616105244843		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 2.3634616105244843 | validation: 2.6888186559520455]
	TIME [epoch: 11.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3507701559264707		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 2.3507701559264707 | validation: 2.658255143555141]
	TIME [epoch: 11.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3456145004246443		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 2.3456145004246443 | validation: 2.6929796918017486]
	TIME [epoch: 11.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3551869737457793		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 2.3551869737457793 | validation: 2.584761536064257]
	TIME [epoch: 11.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3162800914951114		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 2.3162800914951114 | validation: 2.5219264408464954]
	TIME [epoch: 11.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3179581758359724		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 2.3179581758359724 | validation: 2.574635797359349]
	TIME [epoch: 11.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3211578414859773		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 2.3211578414859773 | validation: 2.5701085748628394]
	TIME [epoch: 11.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32119625302815		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 2.32119625302815 | validation: 2.630061355014328]
	TIME [epoch: 11.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3356861541600935		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 2.3356861541600935 | validation: 2.6253296053360407]
	TIME [epoch: 11.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3505482724906472		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 2.3505482724906472 | validation: 2.681950700642836]
	TIME [epoch: 11.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3472996990110815		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 2.3472996990110815 | validation: 2.6920990296207608]
	TIME [epoch: 11.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.384082406624477		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 2.384082406624477 | validation: 2.6974177200289327]
	TIME [epoch: 11.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.401120943959574		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 2.401120943959574 | validation: 2.681721167985312]
	TIME [epoch: 11.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.366467476872651		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 2.366467476872651 | validation: 2.694250407441464]
	TIME [epoch: 11.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3602691554466655		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 2.3602691554466655 | validation: 2.6980678992552396]
	TIME [epoch: 11.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3714756206517986		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 2.3714756206517986 | validation: 2.72333241830285]
	TIME [epoch: 11.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.36256627099029		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 2.36256627099029 | validation: 2.676608209178386]
	TIME [epoch: 11.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3728200481515502		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 2.3728200481515502 | validation: 2.684765534137771]
	TIME [epoch: 11.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3599580544723615		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 2.3599580544723615 | validation: 2.6895703308991843]
	TIME [epoch: 11.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.365997553888591		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 2.365997553888591 | validation: 2.7103550867697837]
	TIME [epoch: 11.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3806441770237976		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 2.3806441770237976 | validation: 2.7078298398102723]
	TIME [epoch: 11.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.365761157262715		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 2.365761157262715 | validation: 2.69037435164829]
	TIME [epoch: 11.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.357829059500662		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 2.357829059500662 | validation: 2.7101087749921]
	TIME [epoch: 11.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.364748819166932		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 2.364748819166932 | validation: 2.682348965665136]
	TIME [epoch: 11.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3691649869419695		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 2.3691649869419695 | validation: 2.692451377311855]
	TIME [epoch: 11.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3491438573860557		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 2.3491438573860557 | validation: 2.7071665536936145]
	TIME [epoch: 11.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3693982494450254		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 2.3693982494450254 | validation: 2.699539594653927]
	TIME [epoch: 11.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3677433007127693		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 2.3677433007127693 | validation: 2.6764037413439157]
	TIME [epoch: 11.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3706012805003835		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 2.3706012805003835 | validation: 2.6959654864756364]
	TIME [epoch: 11.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3899951731188187		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 2.3899951731188187 | validation: 2.6856744189980737]
	TIME [epoch: 11.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360234373746719		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 2.360234373746719 | validation: 2.708978892095604]
	TIME [epoch: 11.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.352311828564048		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 2.352311828564048 | validation: 2.6482382099629227]
	TIME [epoch: 11.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3489617387504436		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 2.3489617387504436 | validation: 2.727456061146005]
	TIME [epoch: 11.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3557843226663273		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 2.3557843226663273 | validation: 2.6978741433762514]
	TIME [epoch: 11.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3666069439910142		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 2.3666069439910142 | validation: 2.694923340596059]
	TIME [epoch: 11.5 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3383626695997983		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 2.3383626695997983 | validation: 2.6635664234040637]
	TIME [epoch: 11.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3366601582528963		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 2.3366601582528963 | validation: 2.589143907737621]
	TIME [epoch: 11.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3467169902588907		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 2.3467169902588907 | validation: 2.741042769359741]
	TIME [epoch: 11.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3708056184605795		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 2.3708056184605795 | validation: 2.6840669526909062]
	TIME [epoch: 11.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3542981323461807		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 2.3542981323461807 | validation: 2.7452302735017406]
	TIME [epoch: 11.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379891802325691		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 2.379891802325691 | validation: 2.7086033336384236]
	TIME [epoch: 11.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.364035279241229		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 2.364035279241229 | validation: 2.682086134954024]
	TIME [epoch: 11.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3444295944639575		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 2.3444295944639575 | validation: 2.6860391351229773]
	TIME [epoch: 11.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355432070118984		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 2.355432070118984 | validation: 2.687296805099343]
	TIME [epoch: 11.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3518576794578174		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 2.3518576794578174 | validation: 2.668477938470029]
	TIME [epoch: 11.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3435647880454438		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 2.3435647880454438 | validation: 2.6964462521972554]
	TIME [epoch: 11.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.35184120356027		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 2.35184120356027 | validation: 2.6553760973259597]
	TIME [epoch: 11.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3432935752501898		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 2.3432935752501898 | validation: 2.6486010528395387]
	TIME [epoch: 11.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3337910183021986		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 2.3337910183021986 | validation: 2.663672062546583]
	TIME [epoch: 11.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3525184873694163		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 2.3525184873694163 | validation: 2.6872885958072823]
	TIME [epoch: 11.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3508174109942033		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 2.3508174109942033 | validation: 2.6458514296538027]
	TIME [epoch: 11.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3338578188572505		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 2.3338578188572505 | validation: 2.617446399418071]
	TIME [epoch: 11.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33718799982073		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 2.33718799982073 | validation: 2.590086581737388]
	TIME [epoch: 11.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334217982776472		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 2.334217982776472 | validation: 2.5350461976414005]
	TIME [epoch: 11.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319384023457148		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 2.319384023457148 | validation: 2.5539859920055608]
	TIME [epoch: 11.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.280636146012753		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 2.280636146012753 | validation: 2.4128627954248056]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2110128959957915		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 2.2110128959957915 | validation: 2.361981352715367]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1142.pth
	Model improved!!!
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2047977490566977		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 2.2047977490566977 | validation: 2.3861269182976566]
	TIME [epoch: 11.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213224842979839		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 2.213224842979839 | validation: 2.4190091184365468]
	TIME [epoch: 11.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.209779018692735		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 2.209779018692735 | validation: 2.373701798031041]
	TIME [epoch: 11.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.218175372826205		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 2.218175372826205 | validation: 2.3939255943948146]
	TIME [epoch: 11.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2130109049225584		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 2.2130109049225584 | validation: 2.3583537450947603]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1147.pth
	Model improved!!!
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2177136233090393		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 2.2177136233090393 | validation: 2.3768743521996116]
	TIME [epoch: 11.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2586372324190167		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 2.2586372324190167 | validation: 2.3610755129093888]
	TIME [epoch: 11.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.236645759285308		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 2.236645759285308 | validation: 2.402716048097309]
	TIME [epoch: 11.5 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2551101338426225		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 2.2551101338426225 | validation: 2.402947556338264]
	TIME [epoch: 11.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2791157603195034		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 2.2791157603195034 | validation: 2.400370932309658]
	TIME [epoch: 11.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.259415555421979		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 2.259415555421979 | validation: 2.517168467626714]
	TIME [epoch: 11.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3143736796219434		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 2.3143736796219434 | validation: 2.4530788870363054]
	TIME [epoch: 11.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.301687982754773		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 2.301687982754773 | validation: 2.4986758282308914]
	TIME [epoch: 11.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312610204185468		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 2.312610204185468 | validation: 2.498790316163251]
	TIME [epoch: 11.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316726107099739		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 2.316726107099739 | validation: 2.5352185421852047]
	TIME [epoch: 11.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3182402546610463		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 2.3182402546610463 | validation: 2.450148267302871]
	TIME [epoch: 11.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3080724843575675		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 2.3080724843575675 | validation: 2.5557496331482277]
	TIME [epoch: 11.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3159902403678583		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 2.3159902403678583 | validation: 2.4081715590452326]
	TIME [epoch: 11.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2489721118783885		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 2.2489721118783885 | validation: 2.3872537120122685]
	TIME [epoch: 11.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.245271256074428		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 2.245271256074428 | validation: 2.4856916253102423]
	TIME [epoch: 11.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3099280348235864		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 2.3099280348235864 | validation: 2.531951654925911]
	TIME [epoch: 11.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324138666988882		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 2.324138666988882 | validation: 2.3997120893476778]
	TIME [epoch: 11.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2559885511815576		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 2.2559885511815576 | validation: 2.3858045188933175]
	TIME [epoch: 11.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2503703656364182		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 2.2503703656364182 | validation: 2.4483147144964432]
	TIME [epoch: 11.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2403035108637246		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 2.2403035108637246 | validation: 2.4135363917016983]
	TIME [epoch: 11.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2497855133479194		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 2.2497855133479194 | validation: 2.401631147523697]
	TIME [epoch: 11.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2419527222928366		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 2.2419527222928366 | validation: 2.3929005777567705]
	TIME [epoch: 11.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2560370428173977		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 2.2560370428173977 | validation: 2.3858465014059296]
	TIME [epoch: 11.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2859283178334047		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 2.2859283178334047 | validation: 2.5081592140945195]
	TIME [epoch: 11.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.301640701546632		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 2.301640701546632 | validation: 2.4481902256694874]
	TIME [epoch: 11.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3131730919185456		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 2.3131730919185456 | validation: 2.520322872168061]
	TIME [epoch: 11.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313768512232312		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 2.313768512232312 | validation: 2.5296905563915364]
	TIME [epoch: 11.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337027512836111		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 2.337027512836111 | validation: 2.603761625518286]
	TIME [epoch: 11.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3325947009766885		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 2.3325947009766885 | validation: 2.5710272149110005]
	TIME [epoch: 11.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3253982854905413		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 2.3253982854905413 | validation: 2.5467638487268625]
	TIME [epoch: 11.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3212619230376412		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 2.3212619230376412 | validation: 2.596154459491008]
	TIME [epoch: 11.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3239264581029375		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 2.3239264581029375 | validation: 2.563350242293683]
	TIME [epoch: 11.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326890508060015		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 2.326890508060015 | validation: 2.6143056590190357]
	TIME [epoch: 11.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33423253366122		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 2.33423253366122 | validation: 2.688925050191995]
	TIME [epoch: 11.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.359212606634193		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 2.359212606634193 | validation: 2.5947011636473567]
	TIME [epoch: 11.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3305148188047458		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 2.3305148188047458 | validation: 2.6310440347098227]
	TIME [epoch: 11.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332601868443434		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 2.332601868443434 | validation: 2.6040276178915054]
	TIME [epoch: 11.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335326995603824		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 2.335326995603824 | validation: 2.6329350277599723]
	TIME [epoch: 11.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3234910614697877		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 2.3234910614697877 | validation: 2.601748428577536]
	TIME [epoch: 11.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3427233793296134		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 2.3427233793296134 | validation: 2.6494072403276347]
	TIME [epoch: 11.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3436926848079604		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 2.3436926848079604 | validation: 2.5756895338307992]
	TIME [epoch: 11.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3395191766947865		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 2.3395191766947865 | validation: 2.458429832469276]
	TIME [epoch: 11.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3124022339290518		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 2.3124022339290518 | validation: 2.5807971793751903]
	TIME [epoch: 11.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326926190349187		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 2.326926190349187 | validation: 2.581427372342187]
	TIME [epoch: 11.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3176386277972405		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 2.3176386277972405 | validation: 2.579384091517165]
	TIME [epoch: 11.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3320045609104127		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 2.3320045609104127 | validation: 2.6038909511952295]
	TIME [epoch: 11.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3368683788351565		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 2.3368683788351565 | validation: 2.578001594752611]
	TIME [epoch: 11.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3291895171110646		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 2.3291895171110646 | validation: 2.5837492643217694]
	TIME [epoch: 11.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323619843657255		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 2.323619843657255 | validation: 2.4851384949237945]
	TIME [epoch: 11.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.299847757058385		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 2.299847757058385 | validation: 2.427220923399811]
	TIME [epoch: 11.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.303665612641966		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 2.303665612641966 | validation: 2.5525565701316313]
	TIME [epoch: 11.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.329201062507217		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 2.329201062507217 | validation: 2.406690338296144]
	TIME [epoch: 11.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220523917725833		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 2.220523917725833 | validation: 2.3850207475739684]
	TIME [epoch: 11.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.290831271009217		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 2.290831271009217 | validation: 2.5431113016538855]
	TIME [epoch: 11.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3403992222699594		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 2.3403992222699594 | validation: 2.6313856623730225]
	TIME [epoch: 11.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3389956879164076		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 2.3389956879164076 | validation: 2.603685597582927]
	TIME [epoch: 11.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334764935658449		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 2.334764935658449 | validation: 2.6947566991750285]
	TIME [epoch: 11.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324040601166802		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 2.324040601166802 | validation: 2.6660175800635844]
	TIME [epoch: 11.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33923306491519		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 2.33923306491519 | validation: 2.598665394213417]
	TIME [epoch: 11.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.35447577867933		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 2.35447577867933 | validation: 2.6408672160860975]
	TIME [epoch: 11.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3433587824158346		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 2.3433587824158346 | validation: 2.6642368245666286]
	TIME [epoch: 11.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3475037480560026		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 2.3475037480560026 | validation: 2.648668328232956]
	TIME [epoch: 11.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3261446173238407		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 2.3261446173238407 | validation: 2.6271866784311784]
	TIME [epoch: 11.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3246766490976487		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 2.3246766490976487 | validation: 2.6483467551699245]
	TIME [epoch: 11.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3274569891100576		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 2.3274569891100576 | validation: 2.603450441822315]
	TIME [epoch: 11.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3207177772373475		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 2.3207177772373475 | validation: 2.560150918522227]
	TIME [epoch: 11.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3166544412166847		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 2.3166544412166847 | validation: 2.637348698122064]
	TIME [epoch: 11.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328611757001843		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 2.328611757001843 | validation: 2.6249104397391045]
	TIME [epoch: 11.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.325380441403564		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 2.325380441403564 | validation: 2.6068810022314315]
	TIME [epoch: 11.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.338021248563424		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 2.338021248563424 | validation: 2.5998850621695637]
	TIME [epoch: 11.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336507541229061		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 2.336507541229061 | validation: 2.628448922867975]
	TIME [epoch: 11.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32736842235989		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 2.32736842235989 | validation: 2.6589150049188675]
	TIME [epoch: 11.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3346276447712024		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 2.3346276447712024 | validation: 2.651779681909844]
	TIME [epoch: 11.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3414435597059695		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 2.3414435597059695 | validation: 2.7087787292298917]
	TIME [epoch: 11.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3377939892476074		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 2.3377939892476074 | validation: 2.5643555319727502]
	TIME [epoch: 11.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330973862603814		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 2.330973862603814 | validation: 2.6629284382137453]
	TIME [epoch: 11.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.331440398061282		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 2.331440398061282 | validation: 2.6511475197610217]
	TIME [epoch: 11.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3448861023252516		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 2.3448861023252516 | validation: 2.553696946304667]
	TIME [epoch: 11.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321033203230302		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 2.321033203230302 | validation: 2.6052363712165234]
	TIME [epoch: 11.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342038418073801		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 2.342038418073801 | validation: 2.5849295828630896]
	TIME [epoch: 11.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330421832145027		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 2.330421832145027 | validation: 2.65254261897708]
	TIME [epoch: 11.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3370098035087206		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 2.3370098035087206 | validation: 2.6585127846262004]
	TIME [epoch: 11.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3376144254244844		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 2.3376144254244844 | validation: 2.611348920775937]
	TIME [epoch: 11.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342616906683062		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 2.342616906683062 | validation: 2.692378285850595]
	TIME [epoch: 11.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355287885416347		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 2.355287885416347 | validation: 2.7145082945438084]
	TIME [epoch: 11.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355442684117823		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 2.355442684117823 | validation: 2.680260627914363]
	TIME [epoch: 11.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3450254158821893		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 2.3450254158821893 | validation: 2.6689106165225414]
	TIME [epoch: 11.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3423230914934643		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 2.3423230914934643 | validation: 2.645514703741579]
	TIME [epoch: 11.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3334792590292905		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 2.3334792590292905 | validation: 2.5163916986470802]
	TIME [epoch: 11.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2686638092058384		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 2.2686638092058384 | validation: 2.364570045881866]
	TIME [epoch: 11.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2386033961206104		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 2.2386033961206104 | validation: 2.3368432509214605]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2506968617421914		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 2.2506968617421914 | validation: 2.3844712330159235]
	TIME [epoch: 11.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2399526609612366		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 2.2399526609612366 | validation: 2.3686708411772464]
	TIME [epoch: 11.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2614549504035204		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 2.2614549504035204 | validation: 2.4184811392008196]
	TIME [epoch: 11.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3019676048807978		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 2.3019676048807978 | validation: 2.5616093075980264]
	TIME [epoch: 11.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3220510129900527		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 2.3220510129900527 | validation: 2.6078047341846435]
	TIME [epoch: 11.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33066484436506		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 2.33066484436506 | validation: 2.6489472391496474]
	TIME [epoch: 11.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332954956734709		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 2.332954956734709 | validation: 2.6608032309901692]
	TIME [epoch: 11.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3326529106561207		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 2.3326529106561207 | validation: 2.6860478711282463]
	TIME [epoch: 11.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3287526616026675		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 2.3287526616026675 | validation: 2.5960841524165756]
	TIME [epoch: 11.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321310250358028		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 2.321310250358028 | validation: 2.56481905443629]
	TIME [epoch: 11.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3145574657514203		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 2.3145574657514203 | validation: 2.529234514412048]
	TIME [epoch: 11.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3107515820944444		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 2.3107515820944444 | validation: 2.4929283457853155]
	TIME [epoch: 11.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3142979462632853		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 2.3142979462632853 | validation: 2.408662419836622]
	TIME [epoch: 11.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2950449682591736		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 2.2950449682591736 | validation: 2.384861869612676]
	TIME [epoch: 11.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.28801625083249		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 2.28801625083249 | validation: 2.526939297955637]
	TIME [epoch: 11.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.303414417168835		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 2.303414417168835 | validation: 2.4064498751840104]
	TIME [epoch: 11.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2749227465758537		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 2.2749227465758537 | validation: 2.456266167301194]
	TIME [epoch: 11.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291034284875897		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 2.291034284875897 | validation: 2.5223443844053177]
	TIME [epoch: 11.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270642571801585		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 2.270642571801585 | validation: 2.3853712870813037]
	TIME [epoch: 11.5 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.233003854010459		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 2.233003854010459 | validation: 2.4760191342681708]
	TIME [epoch: 11.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.211484242721133		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 2.211484242721133 | validation: 2.4370708841863453]
	TIME [epoch: 11.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2034490220696554		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 2.2034490220696554 | validation: 2.3957852451064325]
	TIME [epoch: 11.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2039725955184		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 2.2039725955184 | validation: 2.3398314338129653]
	TIME [epoch: 11.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2251081605528626		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 2.2251081605528626 | validation: 2.368414866429893]
	TIME [epoch: 11.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231581721268089		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 2.231581721268089 | validation: 2.390623864343273]
	TIME [epoch: 11.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2445195244025764		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 2.2445195244025764 | validation: 2.386190421140698]
	TIME [epoch: 11.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2232409426157713		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 2.2232409426157713 | validation: 2.3784982133408015]
	TIME [epoch: 11.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.250563360224521		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 2.250563360224521 | validation: 2.367992565096723]
	TIME [epoch: 11.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2354732031912716		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 2.2354732031912716 | validation: 2.3611200341439487]
	TIME [epoch: 11.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2217626353902866		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 2.2217626353902866 | validation: 2.3570766471558535]
	TIME [epoch: 11.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.236674245891851		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 2.236674245891851 | validation: 2.402665348965238]
	TIME [epoch: 11.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2867693428715006		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 2.2867693428715006 | validation: 2.383501073921084]
	TIME [epoch: 11.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2904116621481405		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 2.2904116621481405 | validation: 2.5165133925162224]
	TIME [epoch: 11.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3302588669565107		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 2.3302588669565107 | validation: 2.522174035522985]
	TIME [epoch: 11.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3115300052814445		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 2.3115300052814445 | validation: 2.542873685850802]
	TIME [epoch: 11.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330387952708809		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 2.330387952708809 | validation: 2.6265955945750172]
	TIME [epoch: 11.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328483690032778		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 2.328483690032778 | validation: 2.681967132152953]
	TIME [epoch: 11.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3260312466439834		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 2.3260312466439834 | validation: 2.5906752243426285]
	TIME [epoch: 11.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3311300857315778		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 2.3311300857315778 | validation: 2.615366081311828]
	TIME [epoch: 11.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328985347339786		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 2.328985347339786 | validation: 2.6275167164490916]
	TIME [epoch: 11.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3392723553449413		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 2.3392723553449413 | validation: 2.721772093419529]
	TIME [epoch: 11.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3528904668574113		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 2.3528904668574113 | validation: 2.6841481114874113]
	TIME [epoch: 11.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326157094796909		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 2.326157094796909 | validation: 2.67755765875141]
	TIME [epoch: 11.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.356918321026378		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 2.356918321026378 | validation: 2.6993705505812575]
	TIME [epoch: 11.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.344896215173723		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 2.344896215173723 | validation: 2.6859983796585865]
	TIME [epoch: 11.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3497053463037343		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 2.3497053463037343 | validation: 2.6790523408437696]
	TIME [epoch: 11.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3555490425690504		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 2.3555490425690504 | validation: 2.6953847189642195]
	TIME [epoch: 11.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343102200464524		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 2.343102200464524 | validation: 2.7041578784714573]
	TIME [epoch: 11.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.356366655893105		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 2.356366655893105 | validation: 2.698641346764567]
	TIME [epoch: 11.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3527343882046248		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 2.3527343882046248 | validation: 2.687661577152915]
	TIME [epoch: 11.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3458801656586825		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 2.3458801656586825 | validation: 2.65042139668255]
	TIME [epoch: 11.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.329306480491718		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 2.329306480491718 | validation: 2.5347345152659115]
	TIME [epoch: 11.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332708558243367		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 2.332708558243367 | validation: 2.621744088037366]
	TIME [epoch: 11.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.341961352657871		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 2.341961352657871 | validation: 2.6960556578845867]
	TIME [epoch: 11.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3338928831122225		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 2.3338928831122225 | validation: 2.592539570389008]
	TIME [epoch: 11.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3349737562426136		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 2.3349737562426136 | validation: 2.5944721231442305]
	TIME [epoch: 11.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3275093066691395		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 2.3275093066691395 | validation: 2.5979390689483655]
	TIME [epoch: 11.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3208399843079572		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 2.3208399843079572 | validation: 2.5348217212017987]
	TIME [epoch: 11.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3206621378931125		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 2.3206621378931125 | validation: 2.5462344228229523]
	TIME [epoch: 11.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3056812057807594		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 2.3056812057807594 | validation: 2.423062901239571]
	TIME [epoch: 11.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.256679130258573		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 2.256679130258573 | validation: 2.413742847115093]
	TIME [epoch: 11.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2858840308846338		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 2.2858840308846338 | validation: 2.5259117313446193]
	TIME [epoch: 11.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313977734224699		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 2.313977734224699 | validation: 2.589861769981212]
	TIME [epoch: 11.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3270984485688633		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 2.3270984485688633 | validation: 2.567948969158513]
	TIME [epoch: 11.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316511171072615		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 2.316511171072615 | validation: 2.470797076550585]
	TIME [epoch: 11.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2565876580086863		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 2.2565876580086863 | validation: 2.376942206494326]
	TIME [epoch: 11.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.248700676165025		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 2.248700676165025 | validation: 2.3538017209900413]
	TIME [epoch: 11.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.230357759106137		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 2.230357759106137 | validation: 2.366595962245224]
	TIME [epoch: 11.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.199306055434196		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 2.199306055434196 | validation: 2.3933505281417347]
	TIME [epoch: 11.5 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2019787016502566		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 2.2019787016502566 | validation: 2.4467866120401154]
	TIME [epoch: 11.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2122330315833256		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 2.2122330315833256 | validation: 2.3969444371024515]
	TIME [epoch: 11.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.203003080564846		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 2.203003080564846 | validation: 2.3740673890927764]
	TIME [epoch: 11.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1864806875041705		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 2.1864806875041705 | validation: 2.3866874196183567]
	TIME [epoch: 11.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1950494302530057		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 2.1950494302530057 | validation: 2.37554960199638]
	TIME [epoch: 11.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1924004989483237		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 2.1924004989483237 | validation: 2.4056726916620805]
	TIME [epoch: 11.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.191117337281341		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 2.191117337281341 | validation: 2.4110295068425183]
	TIME [epoch: 11.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1845582945028865		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 2.1845582945028865 | validation: 2.4042123246852016]
	TIME [epoch: 11.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.192033238236223		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 2.192033238236223 | validation: 2.408338444410586]
	TIME [epoch: 11.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.193154498046706		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 2.193154498046706 | validation: 2.43193903069951]
	TIME [epoch: 11.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1866467711162176		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 2.1866467711162176 | validation: 2.4007867706113775]
	TIME [epoch: 11.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1946988704654804		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 2.1946988704654804 | validation: 2.3608369026950764]
	TIME [epoch: 11.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.194973320114441		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 2.194973320114441 | validation: 2.395019413878691]
	TIME [epoch: 11.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.214176909531491		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 2.214176909531491 | validation: 2.37163074473727]
	TIME [epoch: 11.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2387326190236623		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 2.2387326190236623 | validation: 2.3652673519514655]
	TIME [epoch: 11.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2261506637439		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 2.2261506637439 | validation: 2.358533667119886]
	TIME [epoch: 11.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2460081781161683		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 2.2460081781161683 | validation: 2.3642695100447115]
	TIME [epoch: 11.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.239418526346111		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 2.239418526346111 | validation: 2.355728658248977]
	TIME [epoch: 11.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.262669571994901		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 2.262669571994901 | validation: 2.415241591971475]
	TIME [epoch: 11.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3058768667544993		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 2.3058768667544993 | validation: 2.379023353550788]
	TIME [epoch: 11.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235087904942607		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 2.235087904942607 | validation: 2.3838575857383613]
	TIME [epoch: 11.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.246755952944828		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 2.246755952944828 | validation: 2.413005243811142]
	TIME [epoch: 11.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2588958344453296		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 2.2588958344453296 | validation: 2.3954129792789054]
	TIME [epoch: 11.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2581901273594522		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 2.2581901273594522 | validation: 2.389927525754241]
	TIME [epoch: 11.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2687870104453376		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 2.2687870104453376 | validation: 2.367229039147515]
	TIME [epoch: 11.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2374938049963706		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 2.2374938049963706 | validation: 2.3818676536341066]
	TIME [epoch: 11.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2160830088276278		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 2.2160830088276278 | validation: 2.3594691224918076]
	TIME [epoch: 11.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2130369014060554		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 2.2130369014060554 | validation: 2.4488429609210587]
	TIME [epoch: 11.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.218042734393194		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 2.218042734393194 | validation: 2.4832204939591778]
	TIME [epoch: 11.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2082516750893704		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 2.2082516750893704 | validation: 2.4094169719830205]
	TIME [epoch: 11.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.195872056555426		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 2.195872056555426 | validation: 2.3503430064571313]
	TIME [epoch: 11.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1906220703343		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 2.1906220703343 | validation: 2.4260083905699994]
	TIME [epoch: 11.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1816666744097044		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 2.1816666744097044 | validation: 2.381382021506182]
	TIME [epoch: 11.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.205616632130993		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 2.205616632130993 | validation: 2.350072184409517]
	TIME [epoch: 11.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2031082740232755		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 2.2031082740232755 | validation: 2.3853670993872127]
	TIME [epoch: 11.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1913206556822007		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 2.1913206556822007 | validation: 2.382980034562159]
	TIME [epoch: 11.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.19327714087144		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 2.19327714087144 | validation: 2.359233929971883]
	TIME [epoch: 11.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.202922506076936		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 2.202922506076936 | validation: 2.3685799493656554]
	TIME [epoch: 11.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2200849734056294		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 2.2200849734056294 | validation: 2.380968671822267]
	TIME [epoch: 11.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2428871885792363		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 2.2428871885792363 | validation: 2.400741954382131]
	TIME [epoch: 11.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.287852520470617		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 2.287852520470617 | validation: 2.436584428258948]
	TIME [epoch: 11.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.302698165784985		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 2.302698165784985 | validation: 2.454614149420486]
	TIME [epoch: 11.5 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.287831438506075		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 2.287831438506075 | validation: 2.5382624332128696]
	TIME [epoch: 11.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322835178377847		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 2.322835178377847 | validation: 2.592100009770306]
	TIME [epoch: 11.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322318684633037		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 2.322318684633037 | validation: 2.5537279178910994]
	TIME [epoch: 11.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322305106562149		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 2.322305106562149 | validation: 2.504810399208143]
	TIME [epoch: 11.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3135409130941165		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 2.3135409130941165 | validation: 2.5220931008900056]
	TIME [epoch: 11.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3018831147881533		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 2.3018831147881533 | validation: 2.530450652898832]
	TIME [epoch: 11.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312923947861635		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 2.312923947861635 | validation: 2.5771321320089435]
	TIME [epoch: 11.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3375924586461614		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 2.3375924586461614 | validation: 2.579787365538747]
	TIME [epoch: 11.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3293418931407723		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 2.3293418931407723 | validation: 2.630135523486924]
	TIME [epoch: 11.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32808371132688		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 2.32808371132688 | validation: 2.659109541458773]
	TIME [epoch: 11.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326021640840528		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 2.326021640840528 | validation: 2.598942864628166]
	TIME [epoch: 11.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3301232809809003		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 2.3301232809809003 | validation: 2.642968770878998]
	TIME [epoch: 11.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3244426002807073		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 2.3244426002807073 | validation: 2.6096165359837027]
	TIME [epoch: 11.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3432168488995284		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 2.3432168488995284 | validation: 2.644523192569711]
	TIME [epoch: 11.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.354380170039996		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 2.354380170039996 | validation: 2.7080312416130425]
	TIME [epoch: 11.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3481024021675294		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 2.3481024021675294 | validation: 2.7022272187767276]
	TIME [epoch: 11.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.333945273264534		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 2.333945273264534 | validation: 2.631151320611296]
	TIME [epoch: 11.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.338341051713588		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 2.338341051713588 | validation: 2.643011680400389]
	TIME [epoch: 11.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3257293321091574		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 2.3257293321091574 | validation: 2.6675018041139316]
	TIME [epoch: 11.5 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3437182073761846		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 2.3437182073761846 | validation: 2.695121122431264]
	TIME [epoch: 11.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33709955162313		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 2.33709955162313 | validation: 2.6334124038101185]
	TIME [epoch: 11.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330962812652867		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 2.330962812652867 | validation: 2.6078873316648354]
	TIME [epoch: 11.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3240480362678513		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 2.3240480362678513 | validation: 2.54753536722043]
	TIME [epoch: 11.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.320111628953268		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 2.320111628953268 | validation: 2.5791319393893133]
	TIME [epoch: 11.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326060785604735		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 2.326060785604735 | validation: 2.5955697725486493]
	TIME [epoch: 11.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.333931539722558		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 2.333931539722558 | validation: 2.634565806416286]
	TIME [epoch: 11.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330552020609603		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 2.330552020609603 | validation: 2.6440531658772977]
	TIME [epoch: 11.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334159149944097		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 2.334159149944097 | validation: 2.5788868436783883]
	TIME [epoch: 11.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.324288134070112		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 2.324288134070112 | validation: 2.603991485648431]
	TIME [epoch: 11.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32926783398805		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 2.32926783398805 | validation: 2.572641943780354]
	TIME [epoch: 11.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321847757274036		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 2.321847757274036 | validation: 2.625279497133819]
	TIME [epoch: 11.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3249252637663784		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 2.3249252637663784 | validation: 2.535820765850072]
	TIME [epoch: 11.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3217858446557464		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 2.3217858446557464 | validation: 2.5611353241805137]
	TIME [epoch: 11.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.314658702058864		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 2.314658702058864 | validation: 2.4830479874803353]
	TIME [epoch: 11.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3095211907945696		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 2.3095211907945696 | validation: 2.5580640072098193]
	TIME [epoch: 11.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330296826787904		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 2.330296826787904 | validation: 2.587529413707475]
	TIME [epoch: 11.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339424755541333		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 2.339424755541333 | validation: 2.628122830413646]
	TIME [epoch: 11.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335845331377523		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 2.335845331377523 | validation: 2.607752386967904]
	TIME [epoch: 11.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334499901531978		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 2.334499901531978 | validation: 2.5822829584611746]
	TIME [epoch: 11.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3116696934518783		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 2.3116696934518783 | validation: 2.6128692477458264]
	TIME [epoch: 11.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3242517414799098		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 2.3242517414799098 | validation: 2.605476064571072]
	TIME [epoch: 11.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3304265280129046		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 2.3304265280129046 | validation: 2.5711418780211988]
	TIME [epoch: 11.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3259657276042036		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 2.3259657276042036 | validation: 2.6309744487880726]
	TIME [epoch: 11.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3316759569516745		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 2.3316759569516745 | validation: 2.6424554903549904]
	TIME [epoch: 11.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330897835120249		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 2.330897835120249 | validation: 2.615290781351457]
	TIME [epoch: 11.5 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3282218668280024		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 2.3282218668280024 | validation: 2.63172449345703]
	TIME [epoch: 11.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328761606681962		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 2.328761606681962 | validation: 2.6897590299394354]
	TIME [epoch: 11.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.34081892620912		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 2.34081892620912 | validation: 2.5786278106002274]
	TIME [epoch: 11.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3303637168241154		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 2.3303637168241154 | validation: 2.636401409404262]
	TIME [epoch: 11.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3288916038795504		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 2.3288916038795504 | validation: 2.647818075511418]
	TIME [epoch: 11.5 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3271533817712604		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 2.3271533817712604 | validation: 2.6422662118777436]
	TIME [epoch: 11.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3378007006997716		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 2.3378007006997716 | validation: 2.6321509407277075]
	TIME [epoch: 11.5 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3329138436320034		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 2.3329138436320034 | validation: 2.661837406508796]
	TIME [epoch: 11.5 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328564090996238		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 2.328564090996238 | validation: 2.603843075065544]
	TIME [epoch: 11.5 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3267615531734434		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 2.3267615531734434 | validation: 2.654936522097929]
	TIME [epoch: 11.5 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3380866803482006		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 2.3380866803482006 | validation: 2.587476052950786]
	TIME [epoch: 11.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335259403847938		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 2.335259403847938 | validation: 2.6102433165837]
	TIME [epoch: 11.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3277674096544025		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 2.3277674096544025 | validation: 2.645518709938329]
	TIME [epoch: 11.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3268344625195168		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 2.3268344625195168 | validation: 2.676372530516072]
	TIME [epoch: 11.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3405627392171		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 2.3405627392171 | validation: 2.649324759459879]
	TIME [epoch: 11.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3388009195107133		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 2.3388009195107133 | validation: 2.6793021698618817]
	TIME [epoch: 11.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3405218734538735		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 2.3405218734538735 | validation: 2.6509927203711268]
	TIME [epoch: 11.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340507478150123		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 2.340507478150123 | validation: 2.6855195310681843]
	TIME [epoch: 11.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.340943566870338		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 2.340943566870338 | validation: 2.678321579983464]
	TIME [epoch: 11.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32889349838985		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 2.32889349838985 | validation: 2.606455599598886]
	TIME [epoch: 11.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323053055924522		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 2.323053055924522 | validation: 2.6207744344265285]
	TIME [epoch: 11.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3231018813339617		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 2.3231018813339617 | validation: 2.629132868448963]
	TIME [epoch: 11.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.338309702050389		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 2.338309702050389 | validation: 2.6497434595418077]
	TIME [epoch: 11.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3365514888568018		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 2.3365514888568018 | validation: 2.693019848968472]
	TIME [epoch: 11.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.335429195032756		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 2.335429195032756 | validation: 2.5860444551501582]
	TIME [epoch: 11.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.33072102725328		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 2.33072102725328 | validation: 2.5634570283552933]
	TIME [epoch: 11.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.309318872023277		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 2.309318872023277 | validation: 2.566941334036287]
	TIME [epoch: 11.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3177002588201345		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 2.3177002588201345 | validation: 2.5799762780927287]
	TIME [epoch: 11.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.332192936170831		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 2.332192936170831 | validation: 2.5716652813051373]
	TIME [epoch: 11.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3202924885939264		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 2.3202924885939264 | validation: 2.5536501980268738]
	TIME [epoch: 11.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.30708022891223		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 2.30708022891223 | validation: 2.4270750642785326]
	TIME [epoch: 11.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291202990628231		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 2.291202990628231 | validation: 2.481768252658607]
	TIME [epoch: 11.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3014808233055355		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 2.3014808233055355 | validation: 2.4873226717600083]
	TIME [epoch: 11.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291744270472215		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 2.291744270472215 | validation: 2.3866933797861494]
	TIME [epoch: 11.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2635498096030116		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 2.2635498096030116 | validation: 2.42911351740498]
	TIME [epoch: 11.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2770358828876196		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 2.2770358828876196 | validation: 2.4046958889648984]
	TIME [epoch: 11.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2971323668728054		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 2.2971323668728054 | validation: 2.451149424342559]
	TIME [epoch: 11.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.288685172099401		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 2.288685172099401 | validation: 2.4453867187106604]
	TIME [epoch: 11.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.292204362629325		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 2.292204362629325 | validation: 2.5181915466394593]
	TIME [epoch: 11.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2858533427746632		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 2.2858533427746632 | validation: 2.441924671891551]
	TIME [epoch: 11.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2959735295283306		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 2.2959735295283306 | validation: 2.448156768990699]
	TIME [epoch: 11.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2887536013135925		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 2.2887536013135925 | validation: 2.5408503235179714]
	TIME [epoch: 11.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3122730792571655		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 2.3122730792571655 | validation: 2.55199156736462]
	TIME [epoch: 11.5 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2900834767588956		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 2.2900834767588956 | validation: 2.4489638152818705]
	TIME [epoch: 11.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2775130103480374		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 2.2775130103480374 | validation: 2.4528288267529406]
	TIME [epoch: 11.5 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2995274151160485		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 2.2995274151160485 | validation: 2.42484125452093]
	TIME [epoch: 11.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3031849286452846		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 2.3031849286452846 | validation: 2.5419326223049628]
	TIME [epoch: 11.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3213825529551677		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 2.3213825529551677 | validation: 2.5418593706546635]
	TIME [epoch: 11.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.327044922658236		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 2.327044922658236 | validation: 2.548148395155689]
	TIME [epoch: 11.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3130662553488976		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 2.3130662553488976 | validation: 2.5488667489746413]
	TIME [epoch: 11.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3230774855498364		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 2.3230774855498364 | validation: 2.549753265295798]
	TIME [epoch: 11.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313356099456885		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 2.313356099456885 | validation: 2.509742535230209]
	TIME [epoch: 11.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.288931822699491		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 2.288931822699491 | validation: 2.470959678021015]
	TIME [epoch: 11.5 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291977804368297		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 2.291977804368297 | validation: 2.450347854814973]
	TIME [epoch: 11.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3019981101107785		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 2.3019981101107785 | validation: 2.5317166879358193]
	TIME [epoch: 11.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.307668483938551		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 2.307668483938551 | validation: 2.556148811997013]
	TIME [epoch: 11.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3170976872367257		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 2.3170976872367257 | validation: 2.590210264004672]
	TIME [epoch: 11.5 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.313585802806532		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 2.313585802806532 | validation: 2.5376474952021755]
	TIME [epoch: 11.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318734884251863		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 2.318734884251863 | validation: 2.457011464911455]
	TIME [epoch: 11.5 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.305468834773616		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 2.305468834773616 | validation: 2.4562874238496155]
	TIME [epoch: 11.5 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2918622256779466		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 2.2918622256779466 | validation: 2.4085352944195173]
	TIME [epoch: 11.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2677865256944263		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 2.2677865256944263 | validation: 2.395892498276276]
	TIME [epoch: 11.5 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.283495687086098		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 2.283495687086098 | validation: 2.39229493416709]
	TIME [epoch: 11.5 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.260956303531211		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 2.260956303531211 | validation: 2.359080016921614]
	TIME [epoch: 11.5 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2320664422700514		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 2.2320664422700514 | validation: 2.3598653023102174]
	TIME [epoch: 11.5 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.241402611654463		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 2.241402611654463 | validation: 2.386834188617317]
	TIME [epoch: 11.5 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.233386668915999		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 2.233386668915999 | validation: 2.3856291925525377]
	TIME [epoch: 11.5 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270689816261466		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 2.270689816261466 | validation: 2.50245801581186]
	TIME [epoch: 11.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2927496225014306		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 2.2927496225014306 | validation: 2.46731466683961]
	TIME [epoch: 11.5 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2558065520664377		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 2.2558065520664377 | validation: 2.379335703899656]
	TIME [epoch: 11.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2767474457490695		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 2.2767474457490695 | validation: 2.3963187010272042]
	TIME [epoch: 11.5 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2922500917996156		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 2.2922500917996156 | validation: 2.4895711027730854]
	TIME [epoch: 11.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3086201556076924		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 2.3086201556076924 | validation: 2.5177228657634276]
	TIME [epoch: 11.5 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.295618456812112		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 2.295618456812112 | validation: 2.5210542696717164]
	TIME [epoch: 11.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.290545556084573		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 2.290545556084573 | validation: 2.4581222648700645]
	TIME [epoch: 11.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2924816442116023		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 2.2924816442116023 | validation: 2.466667431263447]
	TIME [epoch: 11.5 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2931240090288965		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 2.2931240090288965 | validation: 2.5421808451265444]
	TIME [epoch: 11.5 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.30510311705892		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 2.30510311705892 | validation: 2.5119124425358486]
	TIME [epoch: 11.5 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3145197326501825		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 2.3145197326501825 | validation: 2.5412487658902347]
	TIME [epoch: 11.5 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.317426821523439		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 2.317426821523439 | validation: 2.5189793044889375]
	TIME [epoch: 11.5 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3193946487838963		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 2.3193946487838963 | validation: 2.5828581420129684]
	TIME [epoch: 11.5 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3309102201405096		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 2.3309102201405096 | validation: 2.547459561838346]
	TIME [epoch: 11.5 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3283779097479442		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 2.3283779097479442 | validation: 2.5831344928919076]
	TIME [epoch: 11.5 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319555493583429		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 2.319555493583429 | validation: 2.563805957483651]
	TIME [epoch: 11.5 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3296502560043173		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 2.3296502560043173 | validation: 2.622687030717512]
	TIME [epoch: 11.5 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.31839150937871		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 2.31839150937871 | validation: 2.5586516433662214]
	TIME [epoch: 11.5 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2995745599395057		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 2.2995745599395057 | validation: 2.456410916388252]
	TIME [epoch: 11.5 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3072624881365575		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 2.3072624881365575 | validation: 2.53196656190201]
	TIME [epoch: 11.5 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3191238273548227		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 2.3191238273548227 | validation: 2.5616203960577115]
	TIME [epoch: 11.5 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323768156179496		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 2.323768156179496 | validation: 2.588740258610055]
	TIME [epoch: 11.5 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3304606012186486		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 2.3304606012186486 | validation: 2.557344097791353]
	TIME [epoch: 11.5 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3251035060926424		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 2.3251035060926424 | validation: 2.5935719978077056]
	TIME [epoch: 11.5 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3293711269417567		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 2.3293711269417567 | validation: 2.6462759556642044]
	TIME [epoch: 11.5 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.325302247854814		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 2.325302247854814 | validation: 2.583506467904332]
	TIME [epoch: 11.5 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3280526715492837		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 2.3280526715492837 | validation: 2.617852560137544]
	TIME [epoch: 11.5 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3305360525639682		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 2.3305360525639682 | validation: 2.648503890777954]
	TIME [epoch: 11.5 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328946976211327		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 2.328946976211327 | validation: 2.596949765430926]
	TIME [epoch: 11.5 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3269832334531033		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 2.3269832334531033 | validation: 2.6418356629690116]
	TIME [epoch: 11.5 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3240022096016517		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 2.3240022096016517 | validation: 2.5578807088715134]
	TIME [epoch: 11.5 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323664443918723		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 2.323664443918723 | validation: 2.550773243371816]
	TIME [epoch: 11.5 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.317190777158739		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 2.317190777158739 | validation: 2.586272197662546]
	TIME [epoch: 11.5 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3232656885765346		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 2.3232656885765346 | validation: 2.617907309636594]
	TIME [epoch: 11.5 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334589406064482		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 2.334589406064482 | validation: 2.6030504633603386]
	TIME [epoch: 11.5 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328942947690898		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 2.328942947690898 | validation: 2.560771129405673]
	TIME [epoch: 11.5 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3381896296615903		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 2.3381896296615903 | validation: 2.6245162774957294]
	TIME [epoch: 11.5 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3350304259560635		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 2.3350304259560635 | validation: 2.5914517634368304]
	TIME [epoch: 11.5 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3315986593093294		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 2.3315986593093294 | validation: 2.6188204634371703]
	TIME [epoch: 11.5 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.336636417800233		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 2.336636417800233 | validation: 2.631332654493724]
	TIME [epoch: 11.5 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3340472313547953		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 2.3340472313547953 | validation: 2.603108556257772]
	TIME [epoch: 11.5 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3305465097080758		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 2.3305465097080758 | validation: 2.6059656424324085]
	TIME [epoch: 11.5 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326486904376017		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 2.326486904376017 | validation: 2.600417559285233]
	TIME [epoch: 11.5 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3241362681080573		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 2.3241362681080573 | validation: 2.6221407115273703]
	TIME [epoch: 11.5 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323471473003946		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 2.323471473003946 | validation: 2.5672547497999814]
	TIME [epoch: 11.5 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319907207975673		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 2.319907207975673 | validation: 2.5730342248906832]
	TIME [epoch: 11.5 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312804577155374		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 2.312804577155374 | validation: 2.549670960267139]
	TIME [epoch: 11.5 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3126041896593565		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 2.3126041896593565 | validation: 2.559665962811392]
	TIME [epoch: 11.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321274274041		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 2.321274274041 | validation: 2.6729435398982333]
	TIME [epoch: 11.5 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3342489521168734		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 2.3342489521168734 | validation: 2.6003611195015277]
	TIME [epoch: 11.5 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3345144641250757		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 2.3345144641250757 | validation: 2.64786685826385]
	TIME [epoch: 11.5 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3261319849444106		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 2.3261319849444106 | validation: 2.6611990271919392]
	TIME [epoch: 11.5 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3328013983169855		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 2.3328013983169855 | validation: 2.652653202393824]
	TIME [epoch: 11.5 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3367806946291343		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 2.3367806946291343 | validation: 2.6190129265627626]
	TIME [epoch: 11.5 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.333965284470647		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 2.333965284470647 | validation: 2.6773437361419656]
	TIME [epoch: 11.5 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3473377012601304		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 2.3473377012601304 | validation: 2.6492685445247903]
	TIME [epoch: 11.5 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3244406482619966		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 2.3244406482619966 | validation: 2.6682384533357193]
	TIME [epoch: 11.5 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322817071670496		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 2.322817071670496 | validation: 2.6640186917662265]
	TIME [epoch: 11.5 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3394504931850895		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 2.3394504931850895 | validation: 2.6298803570235227]
	TIME [epoch: 11.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321630502014057		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 2.321630502014057 | validation: 2.605720899806196]
	TIME [epoch: 11.5 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321300081000185		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 2.321300081000185 | validation: 2.5907159476758705]
	TIME [epoch: 11.5 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326133578782968		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 2.326133578782968 | validation: 2.5724650595655425]
	TIME [epoch: 11.5 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3160383726512364		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 2.3160383726512364 | validation: 2.594885771093305]
	TIME [epoch: 11.5 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3222870701502405		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 2.3222870701502405 | validation: 2.584302970422724]
	TIME [epoch: 11.5 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32491765120393		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 2.32491765120393 | validation: 2.665456692802408]
	TIME [epoch: 11.5 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330752389862371		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 2.330752389862371 | validation: 2.6559721052294663]
	TIME [epoch: 11.5 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.333068143240791		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 2.333068143240791 | validation: 2.651636049902977]
	TIME [epoch: 11.5 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330541613852748		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 2.330541613852748 | validation: 2.5966722531449067]
	TIME [epoch: 11.5 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.326777298426919		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 2.326777298426919 | validation: 2.6295446976288583]
	TIME [epoch: 11.5 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3237729109244882		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 2.3237729109244882 | validation: 2.6257217263743127]
	TIME [epoch: 11.5 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3264043422927747		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 2.3264043422927747 | validation: 2.62228379038514]
	TIME [epoch: 11.5 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330595367438871		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 2.330595367438871 | validation: 2.6568055651860316]
	TIME [epoch: 11.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3405607039579484		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 2.3405607039579484 | validation: 2.6527770497877396]
	TIME [epoch: 11.5 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3259740679757437		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 2.3259740679757437 | validation: 2.62883657564362]
	TIME [epoch: 11.5 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3289191797591675		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 2.3289191797591675 | validation: 2.6247320334762474]
	TIME [epoch: 11.5 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3275466695290694		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 2.3275466695290694 | validation: 2.603876840853752]
	TIME [epoch: 11.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3178814926183726		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 2.3178814926183726 | validation: 2.622771497332473]
	TIME [epoch: 11.5 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.34103853561052		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 2.34103853561052 | validation: 2.6150115715360966]
	TIME [epoch: 11.5 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3308326228305134		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 2.3308326228305134 | validation: 2.5982021239568103]
	TIME [epoch: 11.5 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3162978352278376		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 2.3162978352278376 | validation: 2.6594704467823034]
	TIME [epoch: 11.5 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3242913192542494		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 2.3242913192542494 | validation: 2.6061262828764327]
	TIME [epoch: 11.5 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3206335383377112		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 2.3206335383377112 | validation: 2.6893028073843577]
	TIME [epoch: 11.5 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319843049579701		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 2.319843049579701 | validation: 2.569822035305419]
	TIME [epoch: 11.5 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3195759513812595		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 2.3195759513812595 | validation: 2.5843497679314145]
	TIME [epoch: 11.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3226529496034143		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 2.3226529496034143 | validation: 2.5662490573373367]
	TIME [epoch: 11.5 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3172961894139323		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 2.3172961894139323 | validation: 2.525638360232086]
	TIME [epoch: 11.5 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.316258541979357		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 2.316258541979357 | validation: 2.557692468549676]
	TIME [epoch: 11.5 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321586964336503		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 2.321586964336503 | validation: 2.539179983308026]
	TIME [epoch: 11.5 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3121447891543596		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 2.3121447891543596 | validation: 2.527769829548217]
	TIME [epoch: 11.5 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3145310539268382		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 2.3145310539268382 | validation: 2.4558892736486575]
	TIME [epoch: 11.5 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.306695263849913		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 2.306695263849913 | validation: 2.5037614748816677]
	TIME [epoch: 11.5 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2972919467555553		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 2.2972919467555553 | validation: 2.4593232370570166]
	TIME [epoch: 11.5 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2854769044554626		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 2.2854769044554626 | validation: 2.4390347076049648]
	TIME [epoch: 11.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.296901593127969		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 2.296901593127969 | validation: 2.5374688931107445]
	TIME [epoch: 11.5 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319802201608047		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 2.319802201608047 | validation: 2.576566605455927]
	TIME [epoch: 11.5 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3154409037730725		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 2.3154409037730725 | validation: 2.5226234661753346]
	TIME [epoch: 11.5 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.306723335380126		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 2.306723335380126 | validation: 2.5524544038703216]
	TIME [epoch: 11.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3163573089614014		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 2.3163573089614014 | validation: 2.5625011329559255]
	TIME [epoch: 11.5 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3186679657378306		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 2.3186679657378306 | validation: 2.5696890791941667]
	TIME [epoch: 11.5 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.311502609246511		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 2.311502609246511 | validation: 2.4812019655860498]
	TIME [epoch: 11.5 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3119990300562807		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 2.3119990300562807 | validation: 2.5458011927677533]
	TIME [epoch: 11.5 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2868217596860942		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 2.2868217596860942 | validation: 2.4189110680217505]
	TIME [epoch: 11.5 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.27523815670031		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 2.27523815670031 | validation: 2.449477993865792]
	TIME [epoch: 11.5 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2908098134806374		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 2.2908098134806374 | validation: 2.500158468742467]
	TIME [epoch: 11.5 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.287148668023185		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 2.287148668023185 | validation: 2.444450462518604]
	TIME [epoch: 11.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3047679929167426		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 2.3047679929167426 | validation: 2.459833732563946]
	TIME [epoch: 11.5 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.305944964569685		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 2.305944964569685 | validation: 2.5521587470318905]
	TIME [epoch: 11.5 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3122767982091803		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 2.3122767982091803 | validation: 2.562530607014686]
	TIME [epoch: 11.5 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.310659545603592		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 2.310659545603592 | validation: 2.5630798440469915]
	TIME [epoch: 11.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318122457617169		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 2.318122457617169 | validation: 2.5556483703509962]
	TIME [epoch: 11.5 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318989151180601		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 2.318989151180601 | validation: 2.54732930231873]
	TIME [epoch: 11.5 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3213852433561653		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 2.3213852433561653 | validation: 2.5883247175042823]
	TIME [epoch: 11.5 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322522995802147		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 2.322522995802147 | validation: 2.5788794381675406]
	TIME [epoch: 11.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3122382194417597		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 2.3122382194417597 | validation: 2.6133149213956153]
	TIME [epoch: 11.5 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3221102165024217		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 2.3221102165024217 | validation: 2.6318498487122817]
	TIME [epoch: 11.5 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3194043240516997		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 2.3194043240516997 | validation: 2.571876873264013]
	TIME [epoch: 11.5 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3180432783550593		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 2.3180432783550593 | validation: 2.633329101074572]
	TIME [epoch: 11.5 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318027623677284		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 2.318027623677284 | validation: 2.6392359361970885]
	TIME [epoch: 11.5 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3284598738798112		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 2.3284598738798112 | validation: 2.6706396975535656]
	TIME [epoch: 11.5 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3281978648473505		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 2.3281978648473505 | validation: 2.627744030637704]
	TIME [epoch: 11.5 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3292948926718378		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 2.3292948926718378 | validation: 2.664371953439913]
	TIME [epoch: 11.5 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3311164024342648		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 2.3311164024342648 | validation: 2.655488012563257]
	TIME [epoch: 11.5 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3329450241082066		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 2.3329450241082066 | validation: 2.603578351676425]
	TIME [epoch: 11.5 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3212745432236437		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 2.3212745432236437 | validation: 2.627539720113972]
	TIME [epoch: 11.5 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318225536647189		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 2.318225536647189 | validation: 2.64110693589343]
	TIME [epoch: 11.5 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322589628832908		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 2.322589628832908 | validation: 2.6486282166907142]
	TIME [epoch: 11.5 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.330267582270207		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 2.330267582270207 | validation: 2.5755800798304382]
	TIME [epoch: 11.5 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.314897384959647		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 2.314897384959647 | validation: 2.58473125366648]
	TIME [epoch: 11.5 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.325163747612553		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 2.325163747612553 | validation: 2.5991108113964576]
	TIME [epoch: 11.5 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.312636047939158		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 2.312636047939158 | validation: 2.6134203714256374]
	TIME [epoch: 11.5 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3182913243543846		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 2.3182913243543846 | validation: 2.5638820241507103]
	TIME [epoch: 11.5 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322655066602631		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 2.322655066602631 | validation: 2.569647716727191]
	TIME [epoch: 11.5 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321516916518122		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 2.321516916518122 | validation: 2.5522492507745613]
	TIME [epoch: 11.5 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3106197916858857		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 2.3106197916858857 | validation: 2.549279946299139]
	TIME [epoch: 11.5 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3169390040434923		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 2.3169390040434923 | validation: 2.5395608911260625]
	TIME [epoch: 11.5 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.314597054466304		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 2.314597054466304 | validation: 2.560709188327489]
	TIME [epoch: 11.5 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3138679248715786		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 2.3138679248715786 | validation: 2.5493001842166643]
	TIME [epoch: 11.5 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3176347240296895		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 2.3176347240296895 | validation: 2.53951716323449]
	TIME [epoch: 11.5 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.296223035339665		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 2.296223035339665 | validation: 2.5538499849152694]
	TIME [epoch: 11.5 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.310702854956859		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 2.310702854956859 | validation: 2.510062589199222]
	TIME [epoch: 11.5 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.310882700190571		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 2.310882700190571 | validation: 2.487703795961914]
	TIME [epoch: 11.5 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.298011959193377		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 2.298011959193377 | validation: 2.4928644639682678]
	TIME [epoch: 11.5 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2954282656096607		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 2.2954282656096607 | validation: 2.429105312354723]
	TIME [epoch: 11.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.290260186600024		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 2.290260186600024 | validation: 2.501952962854066]
	TIME [epoch: 11.5 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3164156526587014		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 2.3164156526587014 | validation: 2.492109046347722]
	TIME [epoch: 11.5 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2975419319512653		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 2.2975419319512653 | validation: 2.408279424112749]
	TIME [epoch: 11.5 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2892670841478537		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 2.2892670841478537 | validation: 2.4393579994783257]
	TIME [epoch: 11.5 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.283313418547244		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 2.283313418547244 | validation: 2.4387356017264894]
	TIME [epoch: 11.5 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.298635973012051		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 2.298635973012051 | validation: 2.4655403633046764]
	TIME [epoch: 11.5 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3019928048963565		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 2.3019928048963565 | validation: 2.4098779963072348]
	TIME [epoch: 11.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.28884117841533		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 2.28884117841533 | validation: 2.409382712216906]
	TIME [epoch: 11.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2690049315671605		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 2.2690049315671605 | validation: 2.4554570029385174]
	TIME [epoch: 11.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.286516795219179		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 2.286516795219179 | validation: 2.4212383135716875]
	TIME [epoch: 11.5 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2689453036234224		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 2.2689453036234224 | validation: 2.373939128525983]
	TIME [epoch: 11.5 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.242130037778492		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 2.242130037778492 | validation: 2.3612406252508205]
	TIME [epoch: 11.5 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2344741993267334		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 2.2344741993267334 | validation: 2.3808493278077294]
	TIME [epoch: 11.5 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2311633131999447		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 2.2311633131999447 | validation: 2.3645057440473547]
	TIME [epoch: 11.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2177003928686374		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 2.2177003928686374 | validation: 2.385419470626802]
	TIME [epoch: 11.5 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220669991587534		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 2.220669991587534 | validation: 2.3477074185097426]
	TIME [epoch: 11.5 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213592402796978		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 2.213592402796978 | validation: 2.360454021963815]
	TIME [epoch: 11.5 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2232750799357364		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 2.2232750799357364 | validation: 2.3789679113490556]
	TIME [epoch: 11.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2122162174856856		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 2.2122162174856856 | validation: 2.3743213736872497]
	TIME [epoch: 11.5 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2116274089409833		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 2.2116274089409833 | validation: 2.362779784256484]
	TIME [epoch: 11.5 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2029754235700065		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 2.2029754235700065 | validation: 2.3464144598776326]
	TIME [epoch: 11.5 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2112500962352306		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 2.2112500962352306 | validation: 2.356664023250199]
	TIME [epoch: 11.5 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2236135130249948		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 2.2236135130249948 | validation: 2.323436249474515]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1628.pth
	Model improved!!!
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2165146699165312		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 2.2165146699165312 | validation: 2.350000420952846]
	TIME [epoch: 11.5 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.202961132056713		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 2.202961132056713 | validation: 2.3408705783409216]
	TIME [epoch: 11.5 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2043813132335126		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 2.2043813132335126 | validation: 2.3579033573446257]
	TIME [epoch: 11.5 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1834260600939888		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 2.1834260600939888 | validation: 2.338112466593121]
	TIME [epoch: 11.5 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.191009619732612		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 2.191009619732612 | validation: 2.350915870662642]
	TIME [epoch: 11.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2012261315990584		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 2.2012261315990584 | validation: 2.3348360673567106]
	TIME [epoch: 11.5 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1962428801960256		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 2.1962428801960256 | validation: 2.33292315802807]
	TIME [epoch: 11.5 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2021994065975314		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 2.2021994065975314 | validation: 2.349913466369258]
	TIME [epoch: 11.5 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.188646760079066		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 2.188646760079066 | validation: 2.351536181392081]
	TIME [epoch: 11.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2083193252530107		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 2.2083193252530107 | validation: 2.353190420149513]
	TIME [epoch: 11.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1918358594833984		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 2.1918358594833984 | validation: 2.3594446350697242]
	TIME [epoch: 11.5 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.193781388450922		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 2.193781388450922 | validation: 2.3644365557897684]
	TIME [epoch: 11.5 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.187777667115452		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 2.187777667115452 | validation: 2.353124939435955]
	TIME [epoch: 11.5 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.214007504613838		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 2.214007504613838 | validation: 2.3309052609011376]
	TIME [epoch: 11.5 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2009910473674084		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 2.2009910473674084 | validation: 2.3308356276246958]
	TIME [epoch: 11.5 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.209655221964616		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 2.209655221964616 | validation: 2.3299048813893193]
	TIME [epoch: 11.5 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.205551192982854		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 2.205551192982854 | validation: 2.3620695724010825]
	TIME [epoch: 11.5 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2122482293057306		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 2.2122482293057306 | validation: 2.359271719395889]
	TIME [epoch: 11.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213387737791449		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 2.213387737791449 | validation: 2.3348737942106585]
	TIME [epoch: 11.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.216094807884067		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 2.216094807884067 | validation: 2.3568829454776927]
	TIME [epoch: 11.5 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.201923346878828		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 2.201923346878828 | validation: 2.3412312132847823]
	TIME [epoch: 11.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2047215472140183		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 2.2047215472140183 | validation: 2.358140469493081]
	TIME [epoch: 11.5 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.199692702289214		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 2.199692702289214 | validation: 2.3045055545780553]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r4_20240310_044606/states/model_tr_study203_1651.pth
	Model improved!!!
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.201030960259494		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 2.201030960259494 | validation: 2.37570752123302]
	TIME [epoch: 11.5 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.204396893043426		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 2.204396893043426 | validation: 2.3475242265642238]
	TIME [epoch: 11.5 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1957993779546663		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 2.1957993779546663 | validation: 2.325875780054622]
	TIME [epoch: 11.5 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1990703509198415		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 2.1990703509198415 | validation: 2.3514006675688974]
	TIME [epoch: 11.5 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.219220076261281		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 2.219220076261281 | validation: 2.3479758080115354]
	TIME [epoch: 11.5 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.192831912019009		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 2.192831912019009 | validation: 2.350901018157729]
	TIME [epoch: 11.5 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.22035764520788		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 2.22035764520788 | validation: 2.36689139339291]
	TIME [epoch: 11.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2316873703968696		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 2.2316873703968696 | validation: 2.3674404770647737]
	TIME [epoch: 11.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2328184630750973		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 2.2328184630750973 | validation: 2.3759676237071052]
	TIME [epoch: 11.5 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2319884778091126		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 2.2319884778091126 | validation: 2.364467751641084]
	TIME [epoch: 11.5 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2272469588327013		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 2.2272469588327013 | validation: 2.3726861114801427]
	TIME [epoch: 11.5 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215923176999936		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 2.215923176999936 | validation: 2.3727032597491458]
	TIME [epoch: 11.5 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.229284994415673		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 2.229284994415673 | validation: 2.339385908087651]
	TIME [epoch: 11.5 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215293045378308		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 2.215293045378308 | validation: 2.357018683530136]
	TIME [epoch: 11.5 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2218804657450404		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 2.2218804657450404 | validation: 2.36354045594668]
	TIME [epoch: 11.5 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.22751118234439		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 2.22751118234439 | validation: 2.363462517114978]
	TIME [epoch: 11.5 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.245068999112025		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 2.245068999112025 | validation: 2.3682972175789265]
	TIME [epoch: 11.5 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.261581319667549		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 2.261581319667549 | validation: 2.3780784624271294]
	TIME [epoch: 11.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.267851786121072		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 2.267851786121072 | validation: 2.3822568714697927]
	TIME [epoch: 11.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2690274417632277		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 2.2690274417632277 | validation: 2.3800702471352144]
	TIME [epoch: 11.5 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2637592393722317		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 2.2637592393722317 | validation: 2.341275406040054]
	TIME [epoch: 11.5 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2394571909977827		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 2.2394571909977827 | validation: 2.369033609363515]
	TIME [epoch: 11.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2313495742636102		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 2.2313495742636102 | validation: 2.362774790191031]
	TIME [epoch: 11.5 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.217469991346373		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 2.217469991346373 | validation: 2.3884158873583705]
	TIME [epoch: 11.5 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2279015570411183		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 2.2279015570411183 | validation: 2.3651495643803395]
	TIME [epoch: 11.5 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2144367311960997		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 2.2144367311960997 | validation: 2.3764397451526924]
	TIME [epoch: 11.5 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213915252612014		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 2.213915252612014 | validation: 2.359522747997376]
	TIME [epoch: 11.5 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.209368523787133		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 2.209368523787133 | validation: 2.3694571696023865]
	TIME [epoch: 11.5 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2231366866754705		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 2.2231366866754705 | validation: 2.370602801108]
	TIME [epoch: 11.5 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.223131429657797		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 2.223131429657797 | validation: 2.343163386941688]
	TIME [epoch: 11.5 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.225249809115864		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 2.225249809115864 | validation: 2.349393934596138]
	TIME [epoch: 11.5 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231525760812553		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 2.231525760812553 | validation: 2.326991350277133]
	TIME [epoch: 11.5 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2042835626233788		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 2.2042835626233788 | validation: 2.3701275280066723]
	TIME [epoch: 11.5 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.208697064162505		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 2.208697064162505 | validation: 2.3645866793845682]
	TIME [epoch: 11.5 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2078693534487566		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 2.2078693534487566 | validation: 2.3317887759383087]
	TIME [epoch: 11.5 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2127367888146545		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 2.2127367888146545 | validation: 2.3626918609461676]
	TIME [epoch: 11.5 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215944425276142		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 2.215944425276142 | validation: 2.3508149984739934]
	TIME [epoch: 11.5 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21512165408525		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 2.21512165408525 | validation: 2.3600482309267354]
	TIME [epoch: 11.5 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2246125242134527		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 2.2246125242134527 | validation: 2.3663367803736115]
	TIME [epoch: 11.5 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.224685645381804		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 2.224685645381804 | validation: 2.3593978131776847]
	TIME [epoch: 11.5 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2413908896449826		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 2.2413908896449826 | validation: 2.37631421409021]
	TIME [epoch: 11.5 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.237241804859078		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 2.237241804859078 | validation: 2.3660681357199134]
	TIME [epoch: 11.5 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2343113324112642		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 2.2343113324112642 | validation: 2.3466995993205457]
	TIME [epoch: 11.5 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2285483636179837		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 2.2285483636179837 | validation: 2.3540837530528553]
	TIME [epoch: 11.5 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.214343061066047		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 2.214343061066047 | validation: 2.350079223226795]
	TIME [epoch: 11.5 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2225514503792794		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 2.2225514503792794 | validation: 2.342577050304302]
	TIME [epoch: 11.5 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2294707448391584		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 2.2294707448391584 | validation: 2.3636162343745366]
	TIME [epoch: 11.5 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.222339992004292		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 2.222339992004292 | validation: 2.370149768726497]
	TIME [epoch: 11.5 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235545411860019		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 2.235545411860019 | validation: 2.3465732206228727]
	TIME [epoch: 11.5 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.236174134874464		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 2.236174134874464 | validation: 2.3416888545333094]
	TIME [epoch: 11.5 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2332690510473427		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 2.2332690510473427 | validation: 2.357370897495544]
	TIME [epoch: 11.5 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2293939654977977		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 2.2293939654977977 | validation: 2.3622034797837834]
	TIME [epoch: 11.5 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2217440794458096		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 2.2217440794458096 | validation: 2.3405841524426307]
	TIME [epoch: 11.5 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2235321591281445		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 2.2235321591281445 | validation: 2.355300426845421]
	TIME [epoch: 11.5 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.242918578765587		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 2.242918578765587 | validation: 2.3910881235436645]
	TIME [epoch: 11.5 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2708239728129582		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 2.2708239728129582 | validation: 2.3622634204362405]
	TIME [epoch: 11.5 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2716142172359977		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 2.2716142172359977 | validation: 2.3798857230755566]
	TIME [epoch: 11.5 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2756053896075095		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 2.2756053896075095 | validation: 2.4110708027867087]
	TIME [epoch: 11.5 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2845290902297846		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 2.2845290902297846 | validation: 2.3958790936115544]
	TIME [epoch: 11.5 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.265493103051344		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 2.265493103051344 | validation: 2.4620183787666847]
	TIME [epoch: 11.5 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2816308994303256		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 2.2816308994303256 | validation: 2.4331717621061153]
	TIME [epoch: 11.5 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2892437509420906		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 2.2892437509420906 | validation: 2.5057825172086923]
	TIME [epoch: 11.5 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.279105606632094		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 2.279105606632094 | validation: 2.4604096204223946]
	TIME [epoch: 11.5 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2854234025877957		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 2.2854234025877957 | validation: 2.4474406876911674]
	TIME [epoch: 11.5 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.287936428199654		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 2.287936428199654 | validation: 2.3796393668183584]
	TIME [epoch: 11.5 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.288774341344781		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 2.288774341344781 | validation: 2.435807797402725]
	TIME [epoch: 11.5 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.279056156417751		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 2.279056156417751 | validation: 2.414288243157584]
	TIME [epoch: 11.5 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270636639736358		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 2.270636639736358 | validation: 2.396171440016681]
	TIME [epoch: 11.5 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2887192912019403		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 2.2887192912019403 | validation: 2.4668218090773553]
	TIME [epoch: 11.5 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2715153282128546		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 2.2715153282128546 | validation: 2.420368437400399]
	TIME [epoch: 11.5 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2841034151744		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 2.2841034151744 | validation: 2.4534145837160057]
	TIME [epoch: 11.5 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3013058726544764		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 2.3013058726544764 | validation: 2.4154551041674193]
	TIME [epoch: 11.5 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.288158100432205		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 2.288158100432205 | validation: 2.4748051672704094]
	TIME [epoch: 11.5 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.287573130799113		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 2.287573130799113 | validation: 2.405926228824123]
	TIME [epoch: 11.5 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.294803578604971		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 2.294803578604971 | validation: 2.423338590363666]
	TIME [epoch: 11.5 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2487793835962786		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 2.2487793835962786 | validation: 2.359959116894069]
	TIME [epoch: 11.5 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.268080304493439		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 2.268080304493439 | validation: 2.366744519164394]
	TIME [epoch: 11.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2856192047120176		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 2.2856192047120176 | validation: 2.3904871512804693]
	TIME [epoch: 11.5 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2496446880402443		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 2.2496446880402443 | validation: 2.3769981880209197]
	TIME [epoch: 11.5 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2599243817604044		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 2.2599243817604044 | validation: 2.3578064597680966]
	TIME [epoch: 11.5 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.273225930719974		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 2.273225930719974 | validation: 2.428517402975399]
	TIME [epoch: 11.5 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2568636604308066		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 2.2568636604308066 | validation: 2.3868537245302766]
	TIME [epoch: 11.5 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.256464293003616		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 2.256464293003616 | validation: 2.377940627716453]
	TIME [epoch: 11.5 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235004971403303		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 2.235004971403303 | validation: 2.3745330376883405]
	TIME [epoch: 11.5 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.232868063794011		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 2.232868063794011 | validation: 2.3829442045767992]
	TIME [epoch: 11.5 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2748775075858103		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 2.2748775075858103 | validation: 2.359881279477915]
	TIME [epoch: 11.5 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2420361056042215		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 2.2420361056042215 | validation: 2.412749531813293]
	TIME [epoch: 11.5 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.261260252619973		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 2.261260252619973 | validation: 2.3577951964070905]
	TIME [epoch: 11.5 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2551733547204806		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 2.2551733547204806 | validation: 2.3698524150082005]
	TIME [epoch: 11.5 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2904993121587083		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 2.2904993121587083 | validation: 2.400573253487903]
	TIME [epoch: 11.5 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2817053431173937		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 2.2817053431173937 | validation: 2.4383589185087873]
	TIME [epoch: 11.5 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291430257709484		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 2.291430257709484 | validation: 2.4357693619304435]
	TIME [epoch: 11.5 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.284290944927311		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 2.284290944927311 | validation: 2.397012476601549]
	TIME [epoch: 11.5 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2828400094671486		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 2.2828400094671486 | validation: 2.374794291754136]
	TIME [epoch: 11.5 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270354140616229		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 2.270354140616229 | validation: 2.3685374504309546]
	TIME [epoch: 11.5 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.258856882022493		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 2.258856882022493 | validation: 2.3575580453801575]
	TIME [epoch: 11.5 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2404519589932392		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 2.2404519589932392 | validation: 2.3572967953564254]
	TIME [epoch: 11.5 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2412831814017777		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 2.2412831814017777 | validation: 2.358476029525794]
	TIME [epoch: 11.5 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2460373813744745		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 2.2460373813744745 | validation: 2.3688068165821528]
	TIME [epoch: 11.5 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2680068660618313		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 2.2680068660618313 | validation: 2.3761040198320096]
	TIME [epoch: 11.5 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.240053805322308		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 2.240053805322308 | validation: 2.388115207084031]
	TIME [epoch: 11.5 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2627461047668724		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 2.2627461047668724 | validation: 2.345929292621067]
	TIME [epoch: 11.5 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2430533670159787		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 2.2430533670159787 | validation: 2.3707290541440735]
	TIME [epoch: 11.5 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2494729209953026		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 2.2494729209953026 | validation: 2.3894348967067645]
	TIME [epoch: 11.5 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2312716563086195		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 2.2312716563086195 | validation: 2.3813600773607995]
	TIME [epoch: 11.5 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.281758580181592		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 2.281758580181592 | validation: 2.4114343975019183]
	TIME [epoch: 11.5 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2576026618314917		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 2.2576026618314917 | validation: 2.3966094792605346]
	TIME [epoch: 11.5 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2531590660545877		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 2.2531590660545877 | validation: 2.379585980495377]
	TIME [epoch: 11.5 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2400657467011853		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 2.2400657467011853 | validation: 2.388454267109267]
	TIME [epoch: 11.5 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2553437727230383		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 2.2553437727230383 | validation: 2.388704459402612]
	TIME [epoch: 11.5 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2568599024029035		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 2.2568599024029035 | validation: 2.420754024528752]
	TIME [epoch: 11.5 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.279624771769503		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 2.279624771769503 | validation: 2.4088987209748254]
	TIME [epoch: 11.5 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2673051267643642		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 2.2673051267643642 | validation: 2.4058243104445514]
	TIME [epoch: 11.5 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.272576941840166		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 2.272576941840166 | validation: 2.381453351340687]
	TIME [epoch: 11.5 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291815353718057		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 2.291815353718057 | validation: 2.365906665293411]
	TIME [epoch: 11.5 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270418849763989		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 2.270418849763989 | validation: 2.373663354601327]
	TIME [epoch: 11.5 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.280007710750594		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 2.280007710750594 | validation: 2.4326435795321895]
	TIME [epoch: 11.5 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2839079703513496		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 2.2839079703513496 | validation: 2.425018596958992]
	TIME [epoch: 11.5 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.273213345119373		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 2.273213345119373 | validation: 2.4029942943444316]
	TIME [epoch: 11.5 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2895879415233025		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 2.2895879415233025 | validation: 2.4078766887127125]
	TIME [epoch: 11.5 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2790340670328817		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 2.2790340670328817 | validation: 2.41186130997259]
	TIME [epoch: 11.5 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.26932229659414		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 2.26932229659414 | validation: 2.3712803262002686]
	TIME [epoch: 11.5 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2484622398213		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 2.2484622398213 | validation: 2.3708034573590973]
	TIME [epoch: 11.5 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.254262251373876		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 2.254262251373876 | validation: 2.3795044140991943]
	TIME [epoch: 11.5 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2548702290312788		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 2.2548702290312788 | validation: 2.3606032623995272]
	TIME [epoch: 11.5 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2522622992758237		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 2.2522622992758237 | validation: 2.371684296374576]
	TIME [epoch: 11.5 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2312250716941975		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 2.2312250716941975 | validation: 2.341533484000795]
	TIME [epoch: 11.5 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235931265401032		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 2.235931265401032 | validation: 2.380470761233655]
	TIME [epoch: 11.5 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2299049763769636		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 2.2299049763769636 | validation: 2.369134712060241]
	TIME [epoch: 11.5 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2426602481753766		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 2.2426602481753766 | validation: 2.338835776940748]
	TIME [epoch: 11.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.248429019682952		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 2.248429019682952 | validation: 2.3897652557412403]
	TIME [epoch: 11.5 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2557405815730145		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 2.2557405815730145 | validation: 2.4191780638289453]
	TIME [epoch: 11.5 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2743478793255205		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 2.2743478793255205 | validation: 2.3770041652159954]
	TIME [epoch: 11.5 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.274265621447335		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 2.274265621447335 | validation: 2.3722934338623385]
	TIME [epoch: 11.5 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2983866138775166		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 2.2983866138775166 | validation: 2.430766447273125]
	TIME [epoch: 11.5 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2649735930882864		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 2.2649735930882864 | validation: 2.3792571579286115]
	TIME [epoch: 11.5 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.260864775366717		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 2.260864775366717 | validation: 2.375724996634133]
	TIME [epoch: 11.5 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2572480222423357		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 2.2572480222423357 | validation: 2.3756112810074694]
	TIME [epoch: 11.5 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.258627061150409		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 2.258627061150409 | validation: 2.356359061021101]
	TIME [epoch: 11.5 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2580473256022686		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 2.2580473256022686 | validation: 2.3778980579374025]
	TIME [epoch: 11.5 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2467671140998466		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 2.2467671140998466 | validation: 2.381448947150933]
	TIME [epoch: 11.5 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.240882709482793		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 2.240882709482793 | validation: 2.384099289689167]
	TIME [epoch: 11.5 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.251664120368563		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 2.251664120368563 | validation: 2.3751503391953883]
	TIME [epoch: 11.5 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.24861346031176		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 2.24861346031176 | validation: 2.3741292107137126]
	TIME [epoch: 11.5 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2314500578227876		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 2.2314500578227876 | validation: 2.3353015752676556]
	TIME [epoch: 11.5 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2201645259316893		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 2.2201645259316893 | validation: 2.3246696210289817]
	TIME [epoch: 11.5 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2166144568914463		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 2.2166144568914463 | validation: 2.356775966106128]
	TIME [epoch: 11.5 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.223679948935005		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 2.223679948935005 | validation: 2.364036828993655]
	TIME [epoch: 11.5 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2292639632891946		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 2.2292639632891946 | validation: 2.3758782537733962]
	TIME [epoch: 11.5 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.239003636722172		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 2.239003636722172 | validation: 2.356644206889559]
	TIME [epoch: 11.5 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231329582221408		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 2.231329582221408 | validation: 2.342340023668568]
	TIME [epoch: 11.5 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2481927279395286		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 2.2481927279395286 | validation: 2.376181378942713]
	TIME [epoch: 11.5 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2671912820149327		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 2.2671912820149327 | validation: 2.3781998087221776]
	TIME [epoch: 11.5 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2652756737687816		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 2.2652756737687816 | validation: 2.3930832744694044]
	TIME [epoch: 11.5 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2891550934037834		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 2.2891550934037834 | validation: 2.399106591681825]
	TIME [epoch: 11.5 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.276023604449583		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 2.276023604449583 | validation: 2.420242403751105]
	TIME [epoch: 11.5 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2872993033266424		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 2.2872993033266424 | validation: 2.4402243230444087]
	TIME [epoch: 11.5 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.281376523327521		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 2.281376523327521 | validation: 2.3951104606085254]
	TIME [epoch: 11.5 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2869153511078357		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 2.2869153511078357 | validation: 2.42358664160821]
	TIME [epoch: 11.5 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2866866355526225		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 2.2866866355526225 | validation: 2.4429353363215305]
	TIME [epoch: 11.5 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.284988316001559		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 2.284988316001559 | validation: 2.40749705709901]
	TIME [epoch: 11.5 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2611660253095076		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 2.2611660253095076 | validation: 2.409971345997417]
	TIME [epoch: 11.5 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2791970944643976		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 2.2791970944643976 | validation: 2.3721618152623427]
	TIME [epoch: 11.5 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.266609555913365		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 2.266609555913365 | validation: 2.4095508410278517]
	TIME [epoch: 11.5 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2451026938336796		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 2.2451026938336796 | validation: 2.3719192666438675]
	TIME [epoch: 11.5 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.262056224138269		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 2.262056224138269 | validation: 2.3838434603372605]
	TIME [epoch: 11.5 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2635089548307397		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 2.2635089548307397 | validation: 2.3756716368504436]
	TIME [epoch: 11.5 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2673407877285454		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 2.2673407877285454 | validation: 2.40634943440294]
	TIME [epoch: 11.5 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2655089428678217		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 2.2655089428678217 | validation: 2.3983892961141287]
	TIME [epoch: 11.5 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2707082215730923		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 2.2707082215730923 | validation: 2.353777159609814]
	TIME [epoch: 11.5 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2774536573601103		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 2.2774536573601103 | validation: 2.3910666980529416]
	TIME [epoch: 11.5 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.293156906902524		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 2.293156906902524 | validation: 2.3741506083786423]
	TIME [epoch: 11.5 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.274256664360266		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 2.274256664360266 | validation: 2.4063381052518613]
	TIME [epoch: 11.5 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2766290903597044		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 2.2766290903597044 | validation: 2.394870873498717]
	TIME [epoch: 11.5 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2537123099811662		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 2.2537123099811662 | validation: 2.434883052652171]
	TIME [epoch: 11.5 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2630112607324295		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 2.2630112607324295 | validation: 2.403258545614501]
	TIME [epoch: 11.5 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.263647131419333		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 2.263647131419333 | validation: 2.3843067242075247]
	TIME [epoch: 11.5 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2736597783793364		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 2.2736597783793364 | validation: 2.3564708717662004]
	TIME [epoch: 11.5 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.251041652384125		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 2.251041652384125 | validation: 2.39116094385912]
	TIME [epoch: 11.5 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2519365822388875		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 2.2519365822388875 | validation: 2.368294952750641]
	TIME [epoch: 11.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2443921768347392		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 2.2443921768347392 | validation: 2.364315668532798]
	TIME [epoch: 11.5 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.266718119020773		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 2.266718119020773 | validation: 2.4039474217700367]
	TIME [epoch: 11.5 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2575202756086985		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 2.2575202756086985 | validation: 2.4084603632703296]
	TIME [epoch: 11.5 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2746215479386613		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 2.2746215479386613 | validation: 2.371004772525519]
	TIME [epoch: 11.5 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2740239047024984		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 2.2740239047024984 | validation: 2.371088979266328]
	TIME [epoch: 11.5 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.261882058553173		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 2.261882058553173 | validation: 2.400610375256674]
	TIME [epoch: 11.5 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.260728627212191		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 2.260728627212191 | validation: 2.374041287912107]
	TIME [epoch: 11.5 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2338410114702567		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 2.2338410114702567 | validation: 2.3803849274447004]
	TIME [epoch: 11.5 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2585976259799914		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 2.2585976259799914 | validation: 2.3747742787066644]
	TIME [epoch: 11.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.236079279999615		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 2.236079279999615 | validation: 2.3711040078830767]
	TIME [epoch: 11.5 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.240855936154972		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 2.240855936154972 | validation: 2.3779697774665403]
	TIME [epoch: 11.5 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2440477903621687		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 2.2440477903621687 | validation: 2.355536582927976]
	TIME [epoch: 11.5 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2269710492058055		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 2.2269710492058055 | validation: 2.3791484490870234]
	TIME [epoch: 11.5 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235021768318177		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 2.235021768318177 | validation: 2.3640681483320147]
	TIME [epoch: 11.5 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.239899275696573		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 2.239899275696573 | validation: 2.360331082985417]
	TIME [epoch: 11.5 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2453197598228125		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 2.2453197598228125 | validation: 2.3456826700405733]
	TIME [epoch: 11.5 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2216340920929096		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 2.2216340920929096 | validation: 2.3726270565481387]
	TIME [epoch: 11.5 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.226083386458316		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 2.226083386458316 | validation: 2.3356412526043613]
	TIME [epoch: 11.5 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.222021077425735		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 2.222021077425735 | validation: 2.3507833622614704]
	TIME [epoch: 11.5 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.228092555914561		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 2.228092555914561 | validation: 2.342345703011578]
	TIME [epoch: 11.5 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235157027980682		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 2.235157027980682 | validation: 2.35919251175549]
	TIME [epoch: 11.5 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2190312844385383		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 2.2190312844385383 | validation: 2.371730371386904]
	TIME [epoch: 11.5 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2255298252161335		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 2.2255298252161335 | validation: 2.359488290159004]
	TIME [epoch: 11.5 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2223051893930847		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 2.2223051893930847 | validation: 2.362880814768469]
	TIME [epoch: 11.5 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2270408417596506		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 2.2270408417596506 | validation: 2.3712710998245634]
	TIME [epoch: 11.5 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2359885113689812		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 2.2359885113689812 | validation: 2.3496268009451193]
	TIME [epoch: 11.5 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.234441056185584		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 2.234441056185584 | validation: 2.339620630350826]
	TIME [epoch: 11.5 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2193823246937043		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 2.2193823246937043 | validation: 2.35719548528101]
	TIME [epoch: 11.5 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.221819527712399		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 2.221819527712399 | validation: 2.341948309327991]
	TIME [epoch: 11.5 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2290370049530503		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 2.2290370049530503 | validation: 2.3367077306936146]
	TIME [epoch: 11.5 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.227649388914548		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 2.227649388914548 | validation: 2.3602235643906644]
	TIME [epoch: 11.5 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.222164158227007		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 2.222164158227007 | validation: 2.35443022959757]
	TIME [epoch: 11.5 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.226987131254372		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 2.226987131254372 | validation: 2.3627745967176437]
	TIME [epoch: 11.5 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.209158051843616		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 2.209158051843616 | validation: 2.3570977055892706]
	TIME [epoch: 11.5 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2177533998539687		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 2.2177533998539687 | validation: 2.350546641577639]
	TIME [epoch: 11.5 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2199078278701534		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 2.2199078278701534 | validation: 2.3568455153801984]
	TIME [epoch: 11.5 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.225929124241803		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 2.225929124241803 | validation: 2.342633824264914]
	TIME [epoch: 11.5 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.228066472006628		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 2.228066472006628 | validation: 2.347042416212228]
	TIME [epoch: 11.5 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21316629784122		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 2.21316629784122 | validation: 2.3535953970784536]
	TIME [epoch: 11.5 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2110646356975536		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 2.2110646356975536 | validation: 2.3621245058190703]
	TIME [epoch: 11.5 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2388215922355594		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 2.2388215922355594 | validation: 2.3419970166531106]
	TIME [epoch: 11.5 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.268803664418862		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 2.268803664418862 | validation: 2.4057104233687574]
	TIME [epoch: 11.5 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2387183944612876		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 2.2387183944612876 | validation: 2.340919641964529]
	TIME [epoch: 11.5 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.239334695432641		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 2.239334695432641 | validation: 2.3338731717998287]
	TIME [epoch: 11.5 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2524529715106887		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 2.2524529715106887 | validation: 2.3445444938329096]
	TIME [epoch: 11.5 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2311487888125683		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 2.2311487888125683 | validation: 2.360404858827446]
	TIME [epoch: 11.5 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2474107229452835		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 2.2474107229452835 | validation: 2.362672890791954]
	TIME [epoch: 11.5 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.247282591492701		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 2.247282591492701 | validation: 2.347877886374283]
	TIME [epoch: 11.5 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.24035003295748		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 2.24035003295748 | validation: 2.360354464617226]
	TIME [epoch: 11.5 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2408948430339426		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 2.2408948430339426 | validation: 2.39380390615372]
	TIME [epoch: 11.5 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.234119294878729		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 2.234119294878729 | validation: 2.350969053096614]
	TIME [epoch: 11.5 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2456438250826074		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 2.2456438250826074 | validation: 2.3185398270893818]
	TIME [epoch: 11.5 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2403849319295635		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 2.2403849319295635 | validation: 2.373630676071461]
	TIME [epoch: 11.5 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231824982224684		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 2.231824982224684 | validation: 2.352730060772487]
	TIME [epoch: 11.5 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2393314967746543		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 2.2393314967746543 | validation: 2.3536822320337905]
	TIME [epoch: 11.5 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2323228891651095		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 2.2323228891651095 | validation: 2.3785994681032214]
	TIME [epoch: 11.5 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2421544529226463		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 2.2421544529226463 | validation: 2.368854263964407]
	TIME [epoch: 11.5 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2329890264445744		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 2.2329890264445744 | validation: 2.4010191525006315]
	TIME [epoch: 11.5 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2430127077453323		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 2.2430127077453323 | validation: 2.3392578513177127]
	TIME [epoch: 11.5 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2489373168432865		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 2.2489373168432865 | validation: 2.3526953939698423]
	TIME [epoch: 11.5 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.247654152044807		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 2.247654152044807 | validation: 2.387338363592474]
	TIME [epoch: 11.5 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.242580843672784		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 2.242580843672784 | validation: 2.3927226381627884]
	TIME [epoch: 11.5 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2404548613807505		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 2.2404548613807505 | validation: 2.3323328720809657]
	TIME [epoch: 11.5 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.236126047382364		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 2.236126047382364 | validation: 2.3907783671667686]
	TIME [epoch: 11.5 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2467673708739273		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 2.2467673708739273 | validation: 2.3933215437724096]
	TIME [epoch: 11.5 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.265653574653768		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 2.265653574653768 | validation: 2.3792373340605644]
	TIME [epoch: 11.5 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2448783941450254		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 2.2448783941450254 | validation: 2.386272020217214]
	TIME [epoch: 11.5 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2513514653652433		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 2.2513514653652433 | validation: 2.3545053108281797]
	TIME [epoch: 11.5 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.254721702395355		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 2.254721702395355 | validation: 2.366278633740785]
	TIME [epoch: 11.5 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2568475479796497		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 2.2568475479796497 | validation: 2.344939150521348]
	TIME [epoch: 11.5 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2592920872066915		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 2.2592920872066915 | validation: 2.358854147345771]
	TIME [epoch: 11.5 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.266703315718292		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 2.266703315718292 | validation: 2.348328257805297]
	TIME [epoch: 11.5 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.247496519998707		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 2.247496519998707 | validation: 2.357634778011787]
	TIME [epoch: 11.5 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2629890934504733		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 2.2629890934504733 | validation: 2.357074379954206]
	TIME [epoch: 11.5 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2747148375765986		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 2.2747148375765986 | validation: 2.379318044972791]
	TIME [epoch: 11.5 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2591824394732685		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 2.2591824394732685 | validation: 2.4035955445670023]
	TIME [epoch: 11.5 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2688764322505817		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 2.2688764322505817 | validation: 2.390606864266137]
	TIME [epoch: 11.5 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.250535953260736		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 2.250535953260736 | validation: 2.3890629339657106]
	TIME [epoch: 11.5 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2497299853808235		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 2.2497299853808235 | validation: 2.4003127798262716]
	TIME [epoch: 11.5 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.235974413690566		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 2.235974413690566 | validation: 2.3665348016063854]
	TIME [epoch: 11.5 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2324919567074053		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 2.2324919567074053 | validation: 2.3590454911262477]
	TIME [epoch: 11.5 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2363502868711356		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 2.2363502868711356 | validation: 2.3778860872065333]
	TIME [epoch: 11.5 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2265802088421838		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 2.2265802088421838 | validation: 2.3886441205833577]
	TIME [epoch: 11.5 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2391858865048198		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 2.2391858865048198 | validation: 2.3771532672522087]
	TIME [epoch: 11.5 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2254074977112754		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 2.2254074977112754 | validation: 2.384353353531871]
	TIME [epoch: 11.5 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.219524201315493		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 2.219524201315493 | validation: 2.385150446822958]
	TIME [epoch: 11.5 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220569362650242		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 2.220569362650242 | validation: 2.3788302921153197]
	TIME [epoch: 11.5 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2177317820442948		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 2.2177317820442948 | validation: 2.3790967072027334]
	TIME [epoch: 11.5 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2264211885275285		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 2.2264211885275285 | validation: 2.3694508124228872]
	TIME [epoch: 11.5 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213480831073076		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 2.213480831073076 | validation: 2.363312193344798]
	TIME [epoch: 11.5 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.228304382920879		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 2.228304382920879 | validation: 2.3695036429939007]
	TIME [epoch: 11.5 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2221546720061496		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 2.2221546720061496 | validation: 2.3690589513679288]
	TIME [epoch: 11.5 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2400452942089935		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 2.2400452942089935 | validation: 2.377514815470936]
	TIME [epoch: 11.5 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2334405993949256		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 2.2334405993949256 | validation: 2.326290305205588]
	TIME [epoch: 11.5 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.229623808031676		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 2.229623808031676 | validation: 2.374586650443631]
	TIME [epoch: 11.5 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.238706039304922		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 2.238706039304922 | validation: 2.3620191647112763]
	TIME [epoch: 11.5 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.22800157016125		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 2.22800157016125 | validation: 2.355513613992883]
	TIME [epoch: 11.5 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.219082982083622		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 2.219082982083622 | validation: 2.3575252667069906]
	TIME [epoch: 11.5 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2126565313790936		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 2.2126565313790936 | validation: 2.3717707002017416]
	TIME [epoch: 11.5 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2270397923441845		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 2.2270397923441845 | validation: 2.350151947882519]
	TIME [epoch: 11.5 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.225167769555749		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 2.225167769555749 | validation: 2.347724851209581]
	TIME [epoch: 11.5 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231788549325221		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 2.231788549325221 | validation: 2.367362368265433]
	TIME [epoch: 11.5 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2318240103140625		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 2.2318240103140625 | validation: 2.3668975860334354]
	TIME [epoch: 11.5 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.227901315577372		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 2.227901315577372 | validation: 2.368558752684908]
	TIME [epoch: 11.5 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.228864675207641		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 2.228864675207641 | validation: 2.3844493848298534]
	TIME [epoch: 11.5 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2141608021063734		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 2.2141608021063734 | validation: 2.363847862041872]
	TIME [epoch: 11.5 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2225865841452666		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 2.2225865841452666 | validation: 2.3643553338250776]
	TIME [epoch: 11.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2230489026044045		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 2.2230489026044045 | validation: 2.3699285691787266]
	TIME [epoch: 11.5 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.230182877611453		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 2.230182877611453 | validation: 2.3454280684050834]
	TIME [epoch: 11.5 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231322814060567		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 2.231322814060567 | validation: 2.354227248404004]
	TIME [epoch: 11.5 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2405872606395945		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 2.2405872606395945 | validation: 2.3808216466223535]
	TIME [epoch: 11.5 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.232595197415625		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 2.232595197415625 | validation: 2.3493616526833656]
	TIME [epoch: 11.5 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2451344941674365		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 2.2451344941674365 | validation: 2.375361414538935]
	TIME [epoch: 11.5 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.251407502214929		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 2.251407502214929 | validation: 2.358060697265088]
	TIME [epoch: 11.5 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2345347385998244		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 2.2345347385998244 | validation: 2.337248833883375]
	TIME [epoch: 11.5 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.239129634720619		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 2.239129634720619 | validation: 2.366617513630206]
	TIME [epoch: 11.5 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.246132095558326		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 2.246132095558326 | validation: 2.3670315362451584]
	TIME [epoch: 11.5 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231644815665053		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 2.231644815665053 | validation: 2.359386603285168]
	TIME [epoch: 11.5 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2258728376157975		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 2.2258728376157975 | validation: 2.360912556170906]
	TIME [epoch: 11.5 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2184508999273347		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 2.2184508999273347 | validation: 2.340528829911919]
	TIME [epoch: 11.5 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21346617088983		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 2.21346617088983 | validation: 2.337687800677279]
	TIME [epoch: 11.5 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2211889603484165		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 2.2211889603484165 | validation: 2.3467768059864946]
	TIME [epoch: 11.5 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.208462064212173		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 2.208462064212173 | validation: 2.3824261590203086]
	TIME [epoch: 11.5 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2273483853088574		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 2.2273483853088574 | validation: 2.3572793587551684]
	TIME [epoch: 11.5 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.225941693581561		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 2.225941693581561 | validation: 2.357305754253769]
	TIME [epoch: 11.5 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21076330958712		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 2.21076330958712 | validation: 2.361193144964114]
	TIME [epoch: 11.5 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.210621566108522		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 2.210621566108522 | validation: 2.369415307515512]
	TIME [epoch: 11.5 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2095700034772348		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 2.2095700034772348 | validation: 2.3517268362867334]
	TIME [epoch: 11.5 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215355781944511		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 2.215355781944511 | validation: 2.334555437432103]
	TIME [epoch: 11.5 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2167441539422352		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 2.2167441539422352 | validation: 2.3436114701666004]
	TIME [epoch: 11.5 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2239093472949163		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 2.2239093472949163 | validation: 2.347746430068257]
	TIME [epoch: 11.5 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21101904617719		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 2.21101904617719 | validation: 2.3549434955068103]
	TIME [epoch: 11.5 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2198008894199295		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 2.2198008894199295 | validation: 2.3255376954569966]
	TIME [epoch: 11.5 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2138006649893094		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 2.2138006649893094 | validation: 2.36643750688151]
	TIME [epoch: 11.5 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220084675321625		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 2.220084675321625 | validation: 2.361973736462017]
	TIME [epoch: 11.5 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21266384922538		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 2.21266384922538 | validation: 2.3781487920083153]
	TIME [epoch: 11.5 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220722081122168		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 2.220722081122168 | validation: 2.3382662260318727]
	TIME [epoch: 11.5 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2078130418169497		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 2.2078130418169497 | validation: 2.3677257113232693]
	TIME [epoch: 11.5 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2198746628833277		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 2.2198746628833277 | validation: 2.3487384693684557]
	TIME [epoch: 11.5 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2202710456715478		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 2.2202710456715478 | validation: 2.367347496473127]
	TIME [epoch: 11.5 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2165467388629976		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 2.2165467388629976 | validation: 2.345924885129896]
	TIME [epoch: 11.5 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.220861878251814		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 2.220861878251814 | validation: 2.327186946454196]
	TIME [epoch: 11.5 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2099949368744736		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 2.2099949368744736 | validation: 2.3201987488663933]
	TIME [epoch: 11.5 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2202987989076037		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 2.2202987989076037 | validation: 2.361653630988207]
	TIME [epoch: 11.5 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.214619967087418		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 2.214619967087418 | validation: 2.365190634305058]
	TIME [epoch: 11.5 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.224811271937099		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 2.224811271937099 | validation: 2.354067646520327]
	TIME [epoch: 11.5 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2101269385183007		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 2.2101269385183007 | validation: 2.3729890530912843]
	TIME [epoch: 11.5 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2159977051623696		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 2.2159977051623696 | validation: 2.3550747130329213]
	TIME [epoch: 11.5 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215351968482677		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 2.215351968482677 | validation: 2.3675930267746677]
	TIME [epoch: 11.5 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.231450667417742		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 2.231450667417742 | validation: 2.3378251690013583]
	TIME [epoch: 11.5 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2082092254851218		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 2.2082092254851218 | validation: 2.3720007625569934]
	TIME [epoch: 11.5 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2259679518081574		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 2.2259679518081574 | validation: 2.377366129196849]
	TIME [epoch: 11.5 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215309856970956		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 2.215309856970956 | validation: 2.369749120348399]
	TIME [epoch: 11.5 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2104078399083953		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 2.2104078399083953 | validation: 2.3542944983988554]
	TIME [epoch: 11.5 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2164966114194784		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 2.2164966114194784 | validation: 2.3524737406438176]
	TIME [epoch: 11.5 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.206333513683384		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 2.206333513683384 | validation: 2.3664441866011297]
	TIME [epoch: 11.5 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2121681469963694		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 2.2121681469963694 | validation: 2.3427060999763842]
	TIME [epoch: 11.5 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.212455646060589		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 2.212455646060589 | validation: 2.3537075692950005]
	TIME [epoch: 11.5 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2169974881043464		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 2.2169974881043464 | validation: 2.373783406207289]
	TIME [epoch: 11.5 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2080863444724876		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 2.2080863444724876 | validation: 2.3561314134391758]
	TIME [epoch: 11.5 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1984820189886127		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 2.1984820189886127 | validation: 2.36043221608682]
	TIME [epoch: 11.5 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.210584235196422		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 2.210584235196422 | validation: 2.3531437649025406]
	TIME [epoch: 11.5 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2051671388112064		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 2.2051671388112064 | validation: 2.381109006485584]
	TIME [epoch: 11.5 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.209503267488853		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 2.209503267488853 | validation: 2.3697322387916744]
	TIME [epoch: 11.5 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2081985725910043		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 2.2081985725910043 | validation: 2.3718584063412282]
	TIME [epoch: 11.5 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.215942907871144		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 2.215942907871144 | validation: 2.3491693153008613]
	TIME [epoch: 11.5 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.203847293795069		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 2.203847293795069 | validation: 2.3712278399363735]
	TIME [epoch: 11.5 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.210817943396523		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 2.210817943396523 | validation: 2.3729790322814677]
	TIME [epoch: 11.5 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.206604612699117		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 2.206604612699117 | validation: 2.353466271659429]
	TIME [epoch: 11.5 sec]
Finished training in 23173.135 seconds.
