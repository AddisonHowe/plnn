Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r2', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1314470754

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.929196288141426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.929196288141426 | validation: 10.175395585170804]
	TIME [epoch: 111 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.943082062191758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.943082062191758 | validation: 10.264663751938775]
	TIME [epoch: 24.9 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.44147817868661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.44147817868661 | validation: 10.219924490013462]
	TIME [epoch: 24.8 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.36621021418877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.36621021418877 | validation: 9.31237519379287]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.616819558903927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.616819558903927 | validation: 8.668590414249252]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.3219402206786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.3219402206786 | validation: 8.610521050862713]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.886332848541162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.886332848541162 | validation: 7.251496053373338]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.317602256329113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.317602256329113 | validation: 6.536071995648436]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.529948607810938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.529948607810938 | validation: 6.673865648848712]
	TIME [epoch: 24.8 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.154972148091135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.154972148091135 | validation: 5.696715534366226]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.856627352878696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.856627352878696 | validation: 5.394344186076217]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.347116578962991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.347116578962991 | validation: 5.586466018837289]
	TIME [epoch: 24.8 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.604197260909038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.604197260909038 | validation: 5.477014836889059]
	TIME [epoch: 24.8 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.593927342464333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.593927342464333 | validation: 5.472776350918482]
	TIME [epoch: 24.7 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.399991522486356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.399991522486356 | validation: 6.219330201812209]
	TIME [epoch: 24.8 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.746218489533727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.746218489533727 | validation: 6.437827835314254]
	TIME [epoch: 24.8 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.581798881575299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.581798881575299 | validation: 5.240679000315659]
	TIME [epoch: 24.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.524998043025029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.524998043025029 | validation: 5.328248704125122]
	TIME [epoch: 24.8 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.563125263641015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.563125263641015 | validation: 5.497126141992335]
	TIME [epoch: 24.8 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.616587115366346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.616587115366346 | validation: 5.144495400624423]
	TIME [epoch: 24.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.432861095736657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.432861095736657 | validation: 5.1584791698698425]
	TIME [epoch: 24.9 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.388668071757878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.388668071757878 | validation: 5.517752515630465]
	TIME [epoch: 24.8 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.486492822292204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.486492822292204 | validation: 5.866256794776636]
	TIME [epoch: 24.8 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3929194182589875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3929194182589875 | validation: 5.992581667656346]
	TIME [epoch: 24.9 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.445371224439898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.445371224439898 | validation: 5.2213559444543085]
	TIME [epoch: 24.8 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.6519104705060315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6519104705060315 | validation: 5.109960014047178]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.491557772920473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.491557772920473 | validation: 5.0218671957289365]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.341322852942451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.341322852942451 | validation: 5.065754526893667]
	TIME [epoch: 24.8 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1255991519065365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1255991519065365 | validation: 5.645997246087648]
	TIME [epoch: 24.8 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.215440689615768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.215440689615768 | validation: 5.466917890362205]
	TIME [epoch: 24.8 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.4708163276365065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4708163276365065 | validation: 5.151570836486034]
	TIME [epoch: 24.8 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.39913945599293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.39913945599293 | validation: 5.685633119012926]
	TIME [epoch: 24.8 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.241722763080353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.241722763080353 | validation: 5.334087291057658]
	TIME [epoch: 24.8 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.46909147816032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.46909147816032 | validation: 5.230603163970153]
	TIME [epoch: 24.8 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.160025154040259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.160025154040259 | validation: 5.013262543646473]
	TIME [epoch: 24.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.123964273672379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.123964273672379 | validation: 5.983160821888707]
	TIME [epoch: 24.8 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.297267399350603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.297267399350603 | validation: 5.824177304122709]
	TIME [epoch: 24.8 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.156923525017713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.156923525017713 | validation: 5.031424512354475]
	TIME [epoch: 24.8 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.410894197783664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.410894197783664 | validation: 5.19927014845979]
	TIME [epoch: 24.8 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.077577113982272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.077577113982272 | validation: 5.528342442232758]
	TIME [epoch: 24.8 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.14316034275719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.14316034275719 | validation: 4.9460074755412045]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1118686429692906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1118686429692906 | validation: 4.765603204594636]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.08728824544751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.08728824544751 | validation: 4.854027382187901]
	TIME [epoch: 24.8 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1158404489527625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1158404489527625 | validation: 5.034666306386546]
	TIME [epoch: 24.8 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.077442437017654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.077442437017654 | validation: 4.860851580825207]
	TIME [epoch: 24.8 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.13751740010894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.13751740010894 | validation: 4.855562225577879]
	TIME [epoch: 24.8 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.317769490510494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.317769490510494 | validation: 5.269821010629244]
	TIME [epoch: 24.8 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.189422646713133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.189422646713133 | validation: 4.9768782753585645]
	TIME [epoch: 24.8 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.912594500763045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.912594500763045 | validation: 5.48898631539706]
	TIME [epoch: 24.8 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.132779780843565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.132779780843565 | validation: 4.6823547592304475]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.998448616046371		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 4.998448616046371 | validation: 4.984071123527009]
	TIME [epoch: 24.8 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.966778423556762		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 4.966778423556762 | validation: 5.082925856238571]
	TIME [epoch: 24.8 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.993209760834435		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 4.993209760834435 | validation: 5.083832074097253]
	TIME [epoch: 24.8 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.002475370382885		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 5.002475370382885 | validation: 5.368221841599308]
	TIME [epoch: 24.8 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0555049425848315		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 5.0555049425848315 | validation: 5.624726015520938]
	TIME [epoch: 24.8 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.037938601719299		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 5.037938601719299 | validation: 5.436045757076553]
	TIME [epoch: 24.8 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.045677269301614		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 5.045677269301614 | validation: 5.034102860451334]
	TIME [epoch: 24.8 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.881759311069153		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 4.881759311069153 | validation: 4.622911600766583]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.023664857348416		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 5.023664857348416 | validation: 5.549672695060608]
	TIME [epoch: 24.8 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.093235502683785		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 5.093235502683785 | validation: 5.385738333795546]
	TIME [epoch: 24.8 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.013591562583308		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 5.013591562583308 | validation: 5.0333138001931115]
	TIME [epoch: 24.8 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.811058373620653		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 4.811058373620653 | validation: 5.047442712992325]
	TIME [epoch: 24.8 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0520804365017735		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 5.0520804365017735 | validation: 4.680888053635113]
	TIME [epoch: 24.8 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9532074201611636		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 4.9532074201611636 | validation: 4.915085115021738]
	TIME [epoch: 24.8 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.882148742769379		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 4.882148742769379 | validation: 4.855295278310082]
	TIME [epoch: 24.8 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871705946156082		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 4.871705946156082 | validation: 4.763189651315333]
	TIME [epoch: 24.8 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.004168952235411		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 5.004168952235411 | validation: 4.937869110834368]
	TIME [epoch: 24.8 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.956316070579814		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 4.956316070579814 | validation: 5.088300124536714]
	TIME [epoch: 24.8 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.909799686324513		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 4.909799686324513 | validation: 5.23363455169426]
	TIME [epoch: 24.8 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906610855885262		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 4.906610855885262 | validation: 5.3442994732838365]
	TIME [epoch: 24.8 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.893282094566176		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 4.893282094566176 | validation: 5.19690443314272]
	TIME [epoch: 24.8 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826415442296938		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 4.826415442296938 | validation: 5.018523302283074]
	TIME [epoch: 24.8 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9117784579003185		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 4.9117784579003185 | validation: 5.102998587253845]
	TIME [epoch: 24.8 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.802809185987446		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 4.802809185987446 | validation: 5.098840742267659]
	TIME [epoch: 24.8 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.898032962945275		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 4.898032962945275 | validation: 5.704587446980217]
	TIME [epoch: 24.8 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.899294784333598		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 4.899294784333598 | validation: 4.7576874188106375]
	TIME [epoch: 24.8 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.936005045044837		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 4.936005045044837 | validation: 4.550400043893293]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.65099118986333		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 4.65099118986333 | validation: 5.4306338755421075]
	TIME [epoch: 24.8 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.98761342792366		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 4.98761342792366 | validation: 5.697644191976817]
	TIME [epoch: 24.8 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.123036474685183		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 5.123036474685183 | validation: 4.773136956792654]
	TIME [epoch: 24.8 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.958754438492358		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 4.958754438492358 | validation: 4.466684628366697]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.904655781132204		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 4.904655781132204 | validation: 4.5355584021656625]
	TIME [epoch: 24.8 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.806780999393028		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 4.806780999393028 | validation: 6.36104642584209]
	TIME [epoch: 24.8 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.298064678596891		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 5.298064678596891 | validation: 4.5517289430937256]
	TIME [epoch: 24.8 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.916509862671472		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 4.916509862671472 | validation: 4.5910811938857385]
	TIME [epoch: 24.8 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.77483720843193		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 4.77483720843193 | validation: 4.699461161224373]
	TIME [epoch: 24.8 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.83567499788498		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 4.83567499788498 | validation: 4.684816929332727]
	TIME [epoch: 24.8 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.901344596959372		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 4.901344596959372 | validation: 4.710536985612165]
	TIME [epoch: 24.8 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.751297625027005		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 4.751297625027005 | validation: 4.868113097132878]
	TIME [epoch: 24.8 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.801576402511798		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 4.801576402511798 | validation: 4.990537709260335]
	TIME [epoch: 24.8 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.943909765029638		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 4.943909765029638 | validation: 5.127865948561593]
	TIME [epoch: 24.8 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.012031215456499		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 5.012031215456499 | validation: 4.7343881567042105]
	TIME [epoch: 24.8 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.743880457135048		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 4.743880457135048 | validation: 4.896977240785254]
	TIME [epoch: 24.8 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.797883751425847		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 4.797883751425847 | validation: 4.745007349248187]
	TIME [epoch: 24.8 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.726821521518742		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 4.726821521518742 | validation: 6.51497510516637]
	TIME [epoch: 24.8 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5734643199397595		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 5.5734643199397595 | validation: 6.4527696581289495]
	TIME [epoch: 24.8 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.4226728990061615		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 5.4226728990061615 | validation: 5.044994841283823]
	TIME [epoch: 24.8 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.893417419352206		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 4.893417419352206 | validation: 4.5526691867099816]
	TIME [epoch: 24.8 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.747862831825135		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 4.747862831825135 | validation: 4.608082176852039]
	TIME [epoch: 24.8 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.827382575372718		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 4.827382575372718 | validation: 6.353203771212065]
	TIME [epoch: 24.8 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.993548441537188		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 4.993548441537188 | validation: 5.264913927367177]
	TIME [epoch: 24.8 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8089520563614725		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 4.8089520563614725 | validation: 4.585946404760751]
	TIME [epoch: 24.8 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.659333435635567		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 4.659333435635567 | validation: 5.297950147684653]
	TIME [epoch: 24.8 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.850169806793097		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 4.850169806793097 | validation: 4.9966616606622765]
	TIME [epoch: 24.8 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.81933155038321		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 4.81933155038321 | validation: 5.13845209273893]
	TIME [epoch: 24.8 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.732014742237886		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 4.732014742237886 | validation: 5.027778943187385]
	TIME [epoch: 24.8 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8032909534063615		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 4.8032909534063615 | validation: 4.570507544882823]
	TIME [epoch: 24.8 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.743974536013338		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 4.743974536013338 | validation: 4.479339726471253]
	TIME [epoch: 24.8 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.623392241651247		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 4.623392241651247 | validation: 4.536221144803035]
	TIME [epoch: 24.8 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.714518102865785		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 4.714518102865785 | validation: 4.861588056192406]
	TIME [epoch: 24.8 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9277801594643575		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 4.9277801594643575 | validation: 4.797976002294152]
	TIME [epoch: 24.8 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.676483913950736		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 4.676483913950736 | validation: 4.391089346377113]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.64574403566927		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 4.64574403566927 | validation: 4.609830439101039]
	TIME [epoch: 24.8 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6683102233994		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 4.6683102233994 | validation: 4.7329407858777754]
	TIME [epoch: 24.8 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.623010446614172		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 4.623010446614172 | validation: 4.477440267571956]
	TIME [epoch: 24.9 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.75799726668573		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 4.75799726668573 | validation: 4.658984875480847]
	TIME [epoch: 24.8 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.711822221744857		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 4.711822221744857 | validation: 4.436149349449452]
	TIME [epoch: 24.8 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.630718462417531		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 4.630718462417531 | validation: 4.545857696231564]
	TIME [epoch: 24.9 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.704563962680876		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 4.704563962680876 | validation: 4.533093105550413]
	TIME [epoch: 24.8 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6754138972735735		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 4.6754138972735735 | validation: 4.507108446082524]
	TIME [epoch: 24.8 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.601224634786198		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 4.601224634786198 | validation: 4.472618251044156]
	TIME [epoch: 24.9 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.597627770279966		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 4.597627770279966 | validation: 4.477500029396916]
	TIME [epoch: 24.8 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.551249358214028		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 4.551249358214028 | validation: 4.522931924471938]
	TIME [epoch: 24.8 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.632761392117936		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 4.632761392117936 | validation: 4.956307310516268]
	TIME [epoch: 24.9 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.59116248926386		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 4.59116248926386 | validation: 4.471078345529616]
	TIME [epoch: 24.8 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.591469206148837		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 4.591469206148837 | validation: 4.896789952514549]
	TIME [epoch: 24.8 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.527058662141639		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 4.527058662141639 | validation: 4.346804306030269]
	TIME [epoch: 24.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.65093657567343		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 4.65093657567343 | validation: 4.770343585718041]
	TIME [epoch: 24.8 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.757278379043287		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 4.757278379043287 | validation: 4.30041081976921]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.575699380195024		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 4.575699380195024 | validation: 4.506967601286196]
	TIME [epoch: 24.8 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.601344901409105		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 4.601344901409105 | validation: 4.708555774972861]
	TIME [epoch: 24.8 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.548246805884959		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 4.548246805884959 | validation: 4.715567742882779]
	TIME [epoch: 24.8 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.532560951005647		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 4.532560951005647 | validation: 4.504689266354061]
	TIME [epoch: 24.8 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.630674585636426		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 4.630674585636426 | validation: 4.779604094898998]
	TIME [epoch: 24.8 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.730948618790212		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 4.730948618790212 | validation: 4.836726234266467]
	TIME [epoch: 24.8 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.66592527032284		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 4.66592527032284 | validation: 4.377570848490818]
	TIME [epoch: 24.8 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.556704433244343		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 4.556704433244343 | validation: 4.566687270334348]
	TIME [epoch: 24.8 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.619057038490473		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 4.619057038490473 | validation: 4.543119107312032]
	TIME [epoch: 24.8 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.543615356898956		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 4.543615356898956 | validation: 4.37839474869012]
	TIME [epoch: 24.8 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.58603893192824		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 4.58603893192824 | validation: 4.394792505765033]
	TIME [epoch: 24.8 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5347713456402206		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 4.5347713456402206 | validation: 4.416056785421485]
	TIME [epoch: 24.8 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.373401438570128		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 5.373401438570128 | validation: 5.021576601587052]
	TIME [epoch: 24.8 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6639999808853645		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 4.6639999808853645 | validation: 5.055849065728731]
	TIME [epoch: 24.8 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.630326103887368		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 4.630326103887368 | validation: 4.42278609240593]
	TIME [epoch: 24.8 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.451316116665937		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 4.451316116665937 | validation: 4.375798805199796]
	TIME [epoch: 24.8 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.489658286507941		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 4.489658286507941 | validation: 4.684185599225603]
	TIME [epoch: 24.8 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.450006612903257		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 4.450006612903257 | validation: 5.326601368206735]
	TIME [epoch: 24.8 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7959099828603255		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 4.7959099828603255 | validation: 5.147404985506196]
	TIME [epoch: 24.8 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.595507524878991		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 4.595507524878991 | validation: 4.506591316631097]
	TIME [epoch: 24.8 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.511996544369997		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 4.511996544369997 | validation: 4.426910645301545]
	TIME [epoch: 24.8 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.01666342676095		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 5.01666342676095 | validation: 4.5342116118441185]
	TIME [epoch: 24.8 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.712008815245023		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 4.712008815245023 | validation: 4.45728962453185]
	TIME [epoch: 24.8 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.501821588424705		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 4.501821588424705 | validation: 4.316158042053082]
	TIME [epoch: 24.8 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4396887350066265		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 4.4396887350066265 | validation: 4.363204887119111]
	TIME [epoch: 24.8 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.580524552985766		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 4.580524552985766 | validation: 4.264620596223351]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.47932955471472		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 4.47932955471472 | validation: 4.614793549819271]
	TIME [epoch: 24.8 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.513800148540595		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 4.513800148540595 | validation: 4.3138475432094205]
	TIME [epoch: 24.8 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.48920288128737		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 4.48920288128737 | validation: 4.626907500380268]
	TIME [epoch: 24.8 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.445765720212745		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 4.445765720212745 | validation: 4.503840470833608]
	TIME [epoch: 24.8 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.45822771077055		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 4.45822771077055 | validation: 4.35523571548528]
	TIME [epoch: 24.8 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.731336597125111		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 4.731336597125111 | validation: 4.302458637229009]
	TIME [epoch: 24.8 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.751113387678416		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 4.751113387678416 | validation: 4.812284748357586]
	TIME [epoch: 24.8 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.70306103133156		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 4.70306103133156 | validation: 4.551961251396731]
	TIME [epoch: 24.8 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.589323944986702		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 4.589323944986702 | validation: 4.299364330885801]
	TIME [epoch: 24.8 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.436436728039933		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 4.436436728039933 | validation: 4.439540151620609]
	TIME [epoch: 24.8 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.402774225779406		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 4.402774225779406 | validation: 4.328745734464202]
	TIME [epoch: 24.8 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.430950918564738		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 4.430950918564738 | validation: 4.970935815583893]
	TIME [epoch: 24.8 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.543981988925007		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 4.543981988925007 | validation: 4.355605688317701]
	TIME [epoch: 24.8 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.429198021320321		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 4.429198021320321 | validation: 4.6077364426332785]
	TIME [epoch: 24.8 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.568100060440047		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 4.568100060440047 | validation: 4.663963190365438]
	TIME [epoch: 24.8 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.473475533594295		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 4.473475533594295 | validation: 4.614918291221832]
	TIME [epoch: 24.8 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.416736273209265		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 4.416736273209265 | validation: 4.667573614014673]
	TIME [epoch: 24.8 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5473738911272275		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 4.5473738911272275 | validation: 4.261520220458809]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.483681277584468		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 4.483681277584468 | validation: 4.288927310569208]
	TIME [epoch: 24.8 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.494409301867135		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 4.494409301867135 | validation: 4.46985204655208]
	TIME [epoch: 24.8 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.674921146807882		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 4.674921146807882 | validation: 4.3261321481695765]
	TIME [epoch: 24.8 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.38859188077654		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 4.38859188077654 | validation: 4.3411859646734]
	TIME [epoch: 24.8 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.421218262653312		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 4.421218262653312 | validation: 4.661454045337088]
	TIME [epoch: 24.8 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.473869689697812		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 4.473869689697812 | validation: 4.672389991393872]
	TIME [epoch: 24.8 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.412674304563673		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 4.412674304563673 | validation: 4.675724408670601]
	TIME [epoch: 24.8 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.381750349271895		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 4.381750349271895 | validation: 4.370832454731688]
	TIME [epoch: 24.8 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.31361764966846		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 4.31361764966846 | validation: 5.193514460641762]
	TIME [epoch: 24.8 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.721253884619769		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 4.721253884619769 | validation: 4.52346779624689]
	TIME [epoch: 24.8 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.484158934410274		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 4.484158934410274 | validation: 4.40904132642917]
	TIME [epoch: 24.8 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.424647372238966		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 4.424647372238966 | validation: 5.106190822591587]
	TIME [epoch: 24.8 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.6818911720755985		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 4.6818911720755985 | validation: 4.25460199753289]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.388448765067833		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 4.388448765067833 | validation: 4.338728165573232]
	TIME [epoch: 24.8 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.371906855091636		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 4.371906855091636 | validation: 4.640661877492042]
	TIME [epoch: 24.8 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4019285717717915		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 4.4019285717717915 | validation: 4.844105848842632]
	TIME [epoch: 24.8 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.510876453746226		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 4.510876453746226 | validation: 4.352263991740421]
	TIME [epoch: 24.8 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.373362266310176		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 4.373362266310176 | validation: 4.305808303905341]
	TIME [epoch: 24.8 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.455289938407013		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 4.455289938407013 | validation: 4.25978942743039]
	TIME [epoch: 24.8 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7513247814499096		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 4.7513247814499096 | validation: 4.526203842439271]
	TIME [epoch: 24.8 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.361841244858932		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 4.361841244858932 | validation: 4.360036816431805]
	TIME [epoch: 24.8 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.539956386678769		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 4.539956386678769 | validation: 4.383256113054355]
	TIME [epoch: 24.8 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.455098188597238		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 4.455098188597238 | validation: 4.338125214134894]
	TIME [epoch: 24.8 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.403125007738149		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 4.403125007738149 | validation: 4.430133585908501]
	TIME [epoch: 24.8 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.419636837344485		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 4.419636837344485 | validation: 4.246179799256708]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.421499029670828		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 4.421499029670828 | validation: 4.617569463305124]
	TIME [epoch: 24.8 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.398669777959588		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 4.398669777959588 | validation: 4.4706080784723]
	TIME [epoch: 24.8 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.549268517451324		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 4.549268517451324 | validation: 4.820845112195269]
	TIME [epoch: 24.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.548212650679234		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 4.548212650679234 | validation: 4.285697330333427]
	TIME [epoch: 24.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815896314255606		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 4.815896314255606 | validation: 5.413650068015555]
	TIME [epoch: 24.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5886681144214165		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 4.5886681144214165 | validation: 4.324100284837278]
	TIME [epoch: 24.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4251850161989505		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 4.4251850161989505 | validation: 4.464802606228473]
	TIME [epoch: 24.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.385547513240734		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 4.385547513240734 | validation: 4.53023570153886]
	TIME [epoch: 24.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.556616845072985		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 4.556616845072985 | validation: 4.645025013835862]
	TIME [epoch: 24.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5143169308240765		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 4.5143169308240765 | validation: 4.427814373613953]
	TIME [epoch: 24.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.408076404268376		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 4.408076404268376 | validation: 4.271277861253897]
	TIME [epoch: 24.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.388906395557915		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 4.388906395557915 | validation: 4.600498892270155]
	TIME [epoch: 24.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.954423705704347		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 4.954423705704347 | validation: 4.604793109253908]
	TIME [epoch: 24.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.463686542080651		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 4.463686542080651 | validation: 5.424443623655748]
	TIME [epoch: 24.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.762374811192771		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 4.762374811192771 | validation: 4.595501099185251]
	TIME [epoch: 24.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.422783932077061		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 4.422783932077061 | validation: 4.499458332016441]
	TIME [epoch: 24.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.534195408030047		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 4.534195408030047 | validation: 4.266381650331128]
	TIME [epoch: 24.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.357756268218575		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 4.357756268218575 | validation: 4.25307297543638]
	TIME [epoch: 24.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.553628921871994		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 4.553628921871994 | validation: 4.745926381809965]
	TIME [epoch: 24.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.530481235562121		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 4.530481235562121 | validation: 4.295274943368856]
	TIME [epoch: 24.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.381789655064953		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 4.381789655064953 | validation: 4.318130815880318]
	TIME [epoch: 24.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.31558202745869		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 4.31558202745869 | validation: 4.640928230672478]
	TIME [epoch: 24.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.481721334104195		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 4.481721334104195 | validation: 4.465237238433783]
	TIME [epoch: 24.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.380887932960825		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 4.380887932960825 | validation: 4.353789355482128]
	TIME [epoch: 24.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.39087341385573		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 4.39087341385573 | validation: 4.543450844646133]
	TIME [epoch: 24.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.447183859400157		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 4.447183859400157 | validation: 4.461414126771495]
	TIME [epoch: 24.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.390815690548614		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 4.390815690548614 | validation: 4.5718345846267345]
	TIME [epoch: 24.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.372526433779416		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 4.372526433779416 | validation: 4.465974348138734]
	TIME [epoch: 24.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.416277621768195		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 4.416277621768195 | validation: 4.2824198911755715]
	TIME [epoch: 24.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.402011813514266		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 4.402011813514266 | validation: 4.3742214348174695]
	TIME [epoch: 24.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.329633597595231		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 4.329633597595231 | validation: 4.2461181467871105]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.46204861270697		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 4.46204861270697 | validation: 4.229679045821032]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.363580001599208		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 4.363580001599208 | validation: 4.221218471822956]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.369988713775431		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 4.369988713775431 | validation: 4.580731766422826]
	TIME [epoch: 24.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4450238082726505		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 4.4450238082726505 | validation: 4.222627381624361]
	TIME [epoch: 24.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.333635807286672		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 4.333635807286672 | validation: 4.277525755090514]
	TIME [epoch: 24.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3957434875278825		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 4.3957434875278825 | validation: 4.506106019804147]
	TIME [epoch: 24.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.369171237271064		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 4.369171237271064 | validation: 4.3015429332722235]
	TIME [epoch: 24.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5247403491809255		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 4.5247403491809255 | validation: 4.356584214928957]
	TIME [epoch: 24.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.503610391805317		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 4.503610391805317 | validation: 4.342549431740588]
	TIME [epoch: 24.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.388316633406976		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 4.388316633406976 | validation: 4.3027034840545495]
	TIME [epoch: 24.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300877108695751		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 4.300877108695751 | validation: 4.398050583013039]
	TIME [epoch: 24.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.406964399348363		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 4.406964399348363 | validation: 4.2477006277197304]
	TIME [epoch: 24.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3508229434995		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 4.3508229434995 | validation: 4.36469314194284]
	TIME [epoch: 24.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.356041800332622		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 4.356041800332622 | validation: 4.2642813782851245]
	TIME [epoch: 24.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3724724824707035		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 4.3724724824707035 | validation: 4.302371940747882]
	TIME [epoch: 24.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.395529281526553		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 4.395529281526553 | validation: 4.251346011740572]
	TIME [epoch: 24.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3374448528712835		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 4.3374448528712835 | validation: 4.255095005569057]
	TIME [epoch: 24.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.650950100280605		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 4.650950100280605 | validation: 4.291216769702439]
	TIME [epoch: 24.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.41859928575856		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 4.41859928575856 | validation: 4.247548179726492]
	TIME [epoch: 24.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.364225900011016		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 4.364225900011016 | validation: 4.595876334615333]
	TIME [epoch: 24.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.460863680580184		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 4.460863680580184 | validation: 4.2537297588609615]
	TIME [epoch: 24.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.344136689592387		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 4.344136689592387 | validation: 4.487186895544995]
	TIME [epoch: 24.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.354995825187028		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 4.354995825187028 | validation: 4.2350001241634025]
	TIME [epoch: 24.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.287239050213943		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 4.287239050213943 | validation: 4.410204302689883]
	TIME [epoch: 24.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.332006433542288		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 4.332006433542288 | validation: 4.3671445359852905]
	TIME [epoch: 24.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.339088333703148		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 4.339088333703148 | validation: 4.3811041851015355]
	TIME [epoch: 24.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3699208189843235		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 4.3699208189843235 | validation: 4.656815285254144]
	TIME [epoch: 24.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.366710018123138		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 4.366710018123138 | validation: 4.33297172099969]
	TIME [epoch: 24.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.314128054332406		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 4.314128054332406 | validation: 4.498007922497386]
	TIME [epoch: 24.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.425641549625657		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 4.425641549625657 | validation: 4.2143655509973215]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.301100189287443		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 4.301100189287443 | validation: 4.261640555507848]
	TIME [epoch: 24.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.324498680921161		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 4.324498680921161 | validation: 4.365230959779249]
	TIME [epoch: 24.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.364325822549311		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 4.364325822549311 | validation: 4.253962229675928]
	TIME [epoch: 24.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.296355077000261		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 4.296355077000261 | validation: 4.851189787008325]
	TIME [epoch: 24.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.556411074883001		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 4.556411074883001 | validation: 4.258276796633178]
	TIME [epoch: 24.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.290510601155061		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 4.290510601155061 | validation: 4.205087207877186]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.257642629956629		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 4.257642629956629 | validation: 4.302010202378255]
	TIME [epoch: 24.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.876590137360881		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 4.876590137360881 | validation: 4.524352343931948]
	TIME [epoch: 24.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.390032652299138		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 4.390032652299138 | validation: 4.353647297614148]
	TIME [epoch: 24.9 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.295707025494212		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 4.295707025494212 | validation: 4.304134965837416]
	TIME [epoch: 24.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.306010403753296		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 4.306010403753296 | validation: 4.295904636920563]
	TIME [epoch: 24.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.354631162398108		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 4.354631162398108 | validation: 4.249182535867032]
	TIME [epoch: 24.9 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.357228837581238		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 4.357228837581238 | validation: 4.4124612741322435]
	TIME [epoch: 24.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.308477101527563		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 4.308477101527563 | validation: 4.24165590918091]
	TIME [epoch: 24.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.397435562690154		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 4.397435562690154 | validation: 4.276289126392107]
	TIME [epoch: 24.9 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.292483381682173		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 4.292483381682173 | validation: 4.19326355575984]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.337444637875345		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 4.337444637875345 | validation: 4.227696000463066]
	TIME [epoch: 24.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3569662799075735		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 4.3569662799075735 | validation: 4.2644840665371335]
	TIME [epoch: 24.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3486334231798995		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 4.3486334231798995 | validation: 4.397231981132993]
	TIME [epoch: 24.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2676501623623135		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 4.2676501623623135 | validation: 4.239551530760049]
	TIME [epoch: 24.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.316643471670297		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 4.316643471670297 | validation: 4.219135350130546]
	TIME [epoch: 24.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.292272696317375		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 4.292272696317375 | validation: 4.223313714438389]
	TIME [epoch: 24.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.355550362388535		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 4.355550362388535 | validation: 4.225462907462716]
	TIME [epoch: 24.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225498018410033		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 4.225498018410033 | validation: 4.344159621628076]
	TIME [epoch: 24.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2859499170520365		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 4.2859499170520365 | validation: 4.55556532608671]
	TIME [epoch: 24.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.363808484904158		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 4.363808484904158 | validation: 4.204018676273312]
	TIME [epoch: 24.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.481906155420551		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 4.481906155420551 | validation: 4.226823124336317]
	TIME [epoch: 24.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.312157068120757		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 4.312157068120757 | validation: 4.2154372771441135]
	TIME [epoch: 24.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.343789445306918		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 4.343789445306918 | validation: 4.273433266554576]
	TIME [epoch: 24.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3187225189563225		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 4.3187225189563225 | validation: 4.261663366691706]
	TIME [epoch: 24.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.302639624883593		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 4.302639624883593 | validation: 4.232742255951107]
	TIME [epoch: 24.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.260019632006637		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 4.260019632006637 | validation: 4.410912885352355]
	TIME [epoch: 24.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4101601585496635		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 4.4101601585496635 | validation: 4.257409377859471]
	TIME [epoch: 24.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.522503276434152		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 4.522503276434152 | validation: 4.2999995696906295]
	TIME [epoch: 24.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.328039222545402		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 4.328039222545402 | validation: 4.244476737057454]
	TIME [epoch: 24.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.265376267824051		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 4.265376267824051 | validation: 4.552122261395483]
	TIME [epoch: 24.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.334198781765581		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 4.334198781765581 | validation: 4.526693998310136]
	TIME [epoch: 24.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3503200585100466		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 4.3503200585100466 | validation: 4.205125711801233]
	TIME [epoch: 24.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.283356479386572		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 4.283356479386572 | validation: 4.3745449272886106]
	TIME [epoch: 24.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.281552517539556		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 4.281552517539556 | validation: 4.4065908678254555]
	TIME [epoch: 24.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285879421190447		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 4.285879421190447 | validation: 4.363527370391464]
	TIME [epoch: 24.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.339397294650842		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 4.339397294650842 | validation: 4.2846135551819575]
	TIME [epoch: 24.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.340957904294368		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 4.340957904294368 | validation: 4.333460492132676]
	TIME [epoch: 24.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.322772364395763		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 4.322772364395763 | validation: 4.18420245798822]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5471403338193435		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 4.5471403338193435 | validation: 4.597022197683904]
	TIME [epoch: 24.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.343230851682694		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 4.343230851682694 | validation: 4.212319445173306]
	TIME [epoch: 24.8 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.248934978872842		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 4.248934978872842 | validation: 4.298632474424793]
	TIME [epoch: 24.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.330708070133576		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 4.330708070133576 | validation: 4.231869151319258]
	TIME [epoch: 24.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300728840278053		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 4.300728840278053 | validation: 4.360560222991515]
	TIME [epoch: 24.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.28696240738768		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 4.28696240738768 | validation: 4.417205500836396]
	TIME [epoch: 24.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.37726574369767		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 4.37726574369767 | validation: 4.2524565505439424]
	TIME [epoch: 24.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.248458683824586		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 4.248458683824586 | validation: 4.190506483560695]
	TIME [epoch: 24.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.384898509250805		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 4.384898509250805 | validation: 4.241191629971272]
	TIME [epoch: 24.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.246371748038273		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 4.246371748038273 | validation: 4.349978159717807]
	TIME [epoch: 24.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.251630597720294		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 4.251630597720294 | validation: 4.266483461979523]
	TIME [epoch: 24.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.31077937800131		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 4.31077937800131 | validation: 4.181573740402675]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2318252208463925		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 4.2318252208463925 | validation: 4.324601556412274]
	TIME [epoch: 24.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.314479676838615		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 4.314479676838615 | validation: 4.397264398573701]
	TIME [epoch: 24.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.34622070769043		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 4.34622070769043 | validation: 4.264583192143515]
	TIME [epoch: 24.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.335577271684898		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 4.335577271684898 | validation: 4.253852710662793]
	TIME [epoch: 24.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.267808836538668		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 4.267808836538668 | validation: 4.274002434899835]
	TIME [epoch: 24.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300195154043407		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 4.300195154043407 | validation: 4.241508547229951]
	TIME [epoch: 24.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.25446328805848		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 4.25446328805848 | validation: 4.2392365522248]
	TIME [epoch: 24.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8309124396396825		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 4.8309124396396825 | validation: 4.298874243197836]
	TIME [epoch: 24.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.299800628697852		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 4.299800628697852 | validation: 4.25059696527214]
	TIME [epoch: 24.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.308206835561246		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 4.308206835561246 | validation: 4.198927157974147]
	TIME [epoch: 24.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.274272252699224		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 4.274272252699224 | validation: 4.203900834285019]
	TIME [epoch: 24.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.461554925091521		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 4.461554925091521 | validation: 4.513393283224904]
	TIME [epoch: 24.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3977431640232565		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 4.3977431640232565 | validation: 4.412051450370995]
	TIME [epoch: 24.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285474413068627		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 4.285474413068627 | validation: 4.324438134727171]
	TIME [epoch: 24.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.252106797506265		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 4.252106797506265 | validation: 4.24641171449695]
	TIME [epoch: 24.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.332917611412178		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 4.332917611412178 | validation: 4.223435963382771]
	TIME [epoch: 24.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285553224167838		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 4.285553224167838 | validation: 4.203474466451657]
	TIME [epoch: 24.8 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.222591311265298		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 4.222591311265298 | validation: 4.302568998389851]
	TIME [epoch: 24.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.305543801262398		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 4.305543801262398 | validation: 4.3001471362028365]
	TIME [epoch: 24.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.385491386756831		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 4.385491386756831 | validation: 4.303091654036251]
	TIME [epoch: 24.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.372452261866345		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 4.372452261866345 | validation: 4.2371046968842565]
	TIME [epoch: 24.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3210204574773705		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 4.3210204574773705 | validation: 4.265566529142318]
	TIME [epoch: 24.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.319687413847586		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 4.319687413847586 | validation: 4.3040912091985675]
	TIME [epoch: 24.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.425828517388744		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 4.425828517388744 | validation: 4.273359217005746]
	TIME [epoch: 24.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.262908992371524		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 4.262908992371524 | validation: 4.215679417670107]
	TIME [epoch: 24.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.228386706711568		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 4.228386706711568 | validation: 4.518533984153855]
	TIME [epoch: 24.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.327038685429092		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 4.327038685429092 | validation: 4.571395018709191]
	TIME [epoch: 24.8 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.42521836528009		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 4.42521836528009 | validation: 4.271385822270396]
	TIME [epoch: 24.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.256975273749565		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 4.256975273749565 | validation: 4.20753821579856]
	TIME [epoch: 24.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.241548747411063		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 4.241548747411063 | validation: 4.200200901043408]
	TIME [epoch: 24.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2270949853472315		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 4.2270949853472315 | validation: 4.1865771939165795]
	TIME [epoch: 24.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.295328939083795		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 4.295328939083795 | validation: 4.182510422681829]
	TIME [epoch: 24.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.258077009653773		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 4.258077009653773 | validation: 4.255854794205261]
	TIME [epoch: 24.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.417948130309473		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 4.417948130309473 | validation: 4.448728760239281]
	TIME [epoch: 24.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.445449137077673		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 4.445449137077673 | validation: 4.306700787426573]
	TIME [epoch: 24.8 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.260623869265212		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 4.260623869265212 | validation: 4.360450679977751]
	TIME [epoch: 24.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.286303307264558		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 4.286303307264558 | validation: 4.430130178741916]
	TIME [epoch: 24.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.302511204443611		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 4.302511204443611 | validation: 4.250678188296036]
	TIME [epoch: 24.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2260506661598		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 4.2260506661598 | validation: 4.211338332269967]
	TIME [epoch: 24.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.296626349546394		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 4.296626349546394 | validation: 4.395490976685468]
	TIME [epoch: 24.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.286573886405016		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 4.286573886405016 | validation: 4.208981345812091]
	TIME [epoch: 24.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4403756723588925		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 4.4403756723588925 | validation: 4.299574493424959]
	TIME [epoch: 24.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2924622288578975		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 4.2924622288578975 | validation: 4.529563898069021]
	TIME [epoch: 24.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.350653649165269		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 4.350653649165269 | validation: 4.515032329088681]
	TIME [epoch: 24.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.348859592623916		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 4.348859592623916 | validation: 4.244895139258614]
	TIME [epoch: 24.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268803314763534		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 4.268803314763534 | validation: 4.493309310952439]
	TIME [epoch: 24.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.499629789178023		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 4.499629789178023 | validation: 4.337331516305453]
	TIME [epoch: 24.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.233648965743496		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 4.233648965743496 | validation: 4.236122235835448]
	TIME [epoch: 24.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.213434174323796		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 4.213434174323796 | validation: 4.218076179486678]
	TIME [epoch: 24.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2314782924634144		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 4.2314782924634144 | validation: 4.198771049761492]
	TIME [epoch: 24.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.230395701829936		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 4.230395701829936 | validation: 4.175901040695868]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.207137537982196		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 4.207137537982196 | validation: 4.290654716971804]
	TIME [epoch: 24.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.300138704553792		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 4.300138704553792 | validation: 4.399909120652171]
	TIME [epoch: 24.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.287044609682322		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 4.287044609682322 | validation: 4.191421732751709]
	TIME [epoch: 24.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.258758752497984		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 4.258758752497984 | validation: 4.2184468017792485]
	TIME [epoch: 24.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.280656158804609		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 4.280656158804609 | validation: 4.277337019887915]
	TIME [epoch: 24.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.291656437748276		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 4.291656437748276 | validation: 4.240990198373886]
	TIME [epoch: 24.8 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.256048927142865		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 4.256048927142865 | validation: 4.1805838492907155]
	TIME [epoch: 24.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.317023579650476		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 4.317023579650476 | validation: 4.212229925481248]
	TIME [epoch: 24.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.297321490515216		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 4.297321490515216 | validation: 4.186521828678918]
	TIME [epoch: 24.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.241597308369014		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 4.241597308369014 | validation: 4.229381826757632]
	TIME [epoch: 24.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268584527936521		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 4.268584527936521 | validation: 4.279038424349966]
	TIME [epoch: 24.8 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1973670461359		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 4.1973670461359 | validation: 4.236012127840497]
	TIME [epoch: 24.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249973786344032		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 4.249973786344032 | validation: 4.237198744836548]
	TIME [epoch: 24.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.235930599533431		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 4.235930599533431 | validation: 4.813672625560042]
	TIME [epoch: 24.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.416402050225061		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 4.416402050225061 | validation: 4.267349649565457]
	TIME [epoch: 24.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.30714718260954		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 4.30714718260954 | validation: 4.357148920980377]
	TIME [epoch: 24.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.26158277672835		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 4.26158277672835 | validation: 4.233194946830707]
	TIME [epoch: 24.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.307492661702657		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 4.307492661702657 | validation: 4.366467106215432]
	TIME [epoch: 24.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.264381410577467		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 4.264381410577467 | validation: 4.187426443436854]
	TIME [epoch: 24.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.277475731677363		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 4.277475731677363 | validation: 4.351793619162868]
	TIME [epoch: 24.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.228442881557323		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 4.228442881557323 | validation: 4.178016153235343]
	TIME [epoch: 24.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2195139851243315		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 4.2195139851243315 | validation: 4.216274366148054]
	TIME [epoch: 24.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22396635098525		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 4.22396635098525 | validation: 4.311164300392677]
	TIME [epoch: 24.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.256767914105744		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 4.256767914105744 | validation: 4.171674720392856]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268469389502198		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 4.268469389502198 | validation: 4.320038942857734]
	TIME [epoch: 24.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.232325999098503		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 4.232325999098503 | validation: 4.1727104750004775]
	TIME [epoch: 24.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.208625197691436		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 4.208625197691436 | validation: 4.174338704751711]
	TIME [epoch: 24.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.197773672894759		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 4.197773672894759 | validation: 4.261140289139211]
	TIME [epoch: 24.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.237985330091572		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 4.237985330091572 | validation: 4.240496210118705]
	TIME [epoch: 24.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.199723568593949		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 4.199723568593949 | validation: 4.265510245582321]
	TIME [epoch: 24.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195867290639018		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 4.195867290639018 | validation: 4.4061221863307525]
	TIME [epoch: 24.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.325748010042328		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 4.325748010042328 | validation: 4.331354565232501]
	TIME [epoch: 24.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.211368274854397		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 4.211368274854397 | validation: 4.453534232658479]
	TIME [epoch: 24.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.285719629577919		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 4.285719629577919 | validation: 4.198450854281583]
	TIME [epoch: 24.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.210612754345197		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 4.210612754345197 | validation: 4.219509242246953]
	TIME [epoch: 24.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.288418920468779		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 4.288418920468779 | validation: 4.1901677584278465]
	TIME [epoch: 24.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.211750553486564		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 4.211750553486564 | validation: 4.241922509683856]
	TIME [epoch: 24.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.260237335475357		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 4.260237335475357 | validation: 4.186434770088723]
	TIME [epoch: 24.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.179096843627965		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 4.179096843627965 | validation: 4.2267995369618285]
	TIME [epoch: 24.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.219929568438642		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 4.219929568438642 | validation: 4.43435567183008]
	TIME [epoch: 24.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.364805403921637		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 4.364805403921637 | validation: 4.2964408025428025]
	TIME [epoch: 24.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.23008602208497		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 4.23008602208497 | validation: 4.164977219804363]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.192068070628138		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 4.192068070628138 | validation: 4.235236401713982]
	TIME [epoch: 24.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.215852795372013		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 4.215852795372013 | validation: 4.306652931305838]
	TIME [epoch: 24.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.219581271467356		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 4.219581271467356 | validation: 4.196561336743101]
	TIME [epoch: 24.8 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.266988451588943		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 4.266988451588943 | validation: 4.223155641645771]
	TIME [epoch: 24.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21796422029287		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 4.21796422029287 | validation: 4.228030388104662]
	TIME [epoch: 24.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.200487498990976		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 4.200487498990976 | validation: 4.751187479935771]
	TIME [epoch: 24.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.369331909201721		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 4.369331909201721 | validation: 4.227389994738745]
	TIME [epoch: 24.8 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.233393041642651		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 4.233393041642651 | validation: 4.183787283596543]
	TIME [epoch: 24.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21515756016996		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 4.21515756016996 | validation: 4.216607449962198]
	TIME [epoch: 24.8 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.209312612608565		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 4.209312612608565 | validation: 4.2710731332023775]
	TIME [epoch: 24.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.269503026843649		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 4.269503026843649 | validation: 4.361045394314398]
	TIME [epoch: 24.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2481100978037265		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 4.2481100978037265 | validation: 4.203425130105901]
	TIME [epoch: 24.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2894199523152015		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 4.2894199523152015 | validation: 4.184579499575587]
	TIME [epoch: 24.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2167001202691035		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 4.2167001202691035 | validation: 4.178340404578267]
	TIME [epoch: 24.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.246484042559266		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 4.246484042559266 | validation: 4.237850340604161]
	TIME [epoch: 24.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.229900183295754		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 4.229900183295754 | validation: 4.202265903890785]
	TIME [epoch: 24.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.19458693341916		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 4.19458693341916 | validation: 4.175614089549813]
	TIME [epoch: 24.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.304931486042962		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 4.304931486042962 | validation: 4.2088504294633236]
	TIME [epoch: 24.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.308634234604105		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 4.308634234604105 | validation: 4.292925553946776]
	TIME [epoch: 24.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2646114568161435		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 4.2646114568161435 | validation: 4.270033561673538]
	TIME [epoch: 24.8 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.227103847426867		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 4.227103847426867 | validation: 4.43326125917237]
	TIME [epoch: 24.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2873356571982475		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 4.2873356571982475 | validation: 4.17370350205002]
	TIME [epoch: 24.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.271511911102178		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 4.271511911102178 | validation: 4.188714046457418]
	TIME [epoch: 24.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.257548811147902		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 4.257548811147902 | validation: 4.19181761720995]
	TIME [epoch: 24.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.259216190309764		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 4.259216190309764 | validation: 4.240256549488003]
	TIME [epoch: 24.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.282810860915947		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 4.282810860915947 | validation: 4.313507783139178]
	TIME [epoch: 24.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.277454571060222		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 4.277454571060222 | validation: 4.273412498093079]
	TIME [epoch: 24.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.202049935774175		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 4.202049935774175 | validation: 4.404985533765979]
	TIME [epoch: 24.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.240611420890153		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 4.240611420890153 | validation: 4.172547961994046]
	TIME [epoch: 24.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.232112011504594		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 4.232112011504594 | validation: 4.2068106294426295]
	TIME [epoch: 24.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.306295243379759		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 4.306295243379759 | validation: 4.443273036941281]
	TIME [epoch: 24.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.275607348622053		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 4.275607348622053 | validation: 4.194003146760101]
	TIME [epoch: 24.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.248521094313906		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 4.248521094313906 | validation: 4.275715498147097]
	TIME [epoch: 24.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.236523626294101		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 4.236523626294101 | validation: 4.261564208590433]
	TIME [epoch: 24.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.196743503152314		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 4.196743503152314 | validation: 4.170171011441527]
	TIME [epoch: 24.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.224048623661826		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 4.224048623661826 | validation: 4.419992588578439]
	TIME [epoch: 24.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3079278139262165		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 4.3079278139262165 | validation: 4.278526994238523]
	TIME [epoch: 24.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.228462235875007		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 4.228462235875007 | validation: 4.224090930173431]
	TIME [epoch: 24.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225571645392884		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 4.225571645392884 | validation: 4.252804865055428]
	TIME [epoch: 24.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.207511575986277		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 4.207511575986277 | validation: 4.198367303601796]
	TIME [epoch: 24.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.213297837160242		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 4.213297837160242 | validation: 4.166587699296323]
	TIME [epoch: 24.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.194041752501665		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 4.194041752501665 | validation: 4.169357905585025]
	TIME [epoch: 24.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.193090032900148		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 4.193090032900148 | validation: 4.182224955814067]
	TIME [epoch: 24.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.206325574906142		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 4.206325574906142 | validation: 4.162359140408497]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187653538072217		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 4.187653538072217 | validation: 4.193410303536282]
	TIME [epoch: 24.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2246009030438785		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 4.2246009030438785 | validation: 4.2313748727636]
	TIME [epoch: 24.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.24216579724216		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 4.24216579724216 | validation: 4.193107980553468]
	TIME [epoch: 24.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.215147388143681		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 4.215147388143681 | validation: 4.165679946540619]
	TIME [epoch: 24.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18448030842187		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 4.18448030842187 | validation: 4.223706708254769]
	TIME [epoch: 24.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1773175209677635		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 4.1773175209677635 | validation: 4.321738970959542]
	TIME [epoch: 24.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3380905125798		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 4.3380905125798 | validation: 4.17151246332661]
	TIME [epoch: 24.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.244678824331757		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 4.244678824331757 | validation: 4.253232958669989]
	TIME [epoch: 24.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.210504364620923		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 4.210504364620923 | validation: 4.333387059166236]
	TIME [epoch: 24.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.226518058718818		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 4.226518058718818 | validation: 4.315986583200983]
	TIME [epoch: 24.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.234324944096412		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 4.234324944096412 | validation: 4.273913566657399]
	TIME [epoch: 24.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.219078765496117		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 4.219078765496117 | validation: 4.287464786267048]
	TIME [epoch: 24.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.227296963894861		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 4.227296963894861 | validation: 4.171514524377281]
	TIME [epoch: 24.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.176411522188453		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 4.176411522188453 | validation: 4.209120013383377]
	TIME [epoch: 24.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.26178523961614		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 4.26178523961614 | validation: 4.183604124873793]
	TIME [epoch: 24.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.192726945700327		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 4.192726945700327 | validation: 4.2193052932893815]
	TIME [epoch: 24.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.209229600303723		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 4.209229600303723 | validation: 4.243900640393191]
	TIME [epoch: 24.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.205415883900353		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 4.205415883900353 | validation: 4.198334901765058]
	TIME [epoch: 24.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.217009476924516		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 4.217009476924516 | validation: 4.26961710875188]
	TIME [epoch: 24.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22350901638624		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 4.22350901638624 | validation: 4.210353191311188]
	TIME [epoch: 24.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.196602261525891		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 4.196602261525891 | validation: 4.157757886049248]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.21396739391439		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 4.21396739391439 | validation: 4.2040939366278165]
	TIME [epoch: 24.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.269706410672377		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 4.269706410672377 | validation: 4.186651679282738]
	TIME [epoch: 24.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187922056229824		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 4.187922056229824 | validation: 4.200163319680159]
	TIME [epoch: 24.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1975558247288935		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 4.1975558247288935 | validation: 4.18526630946364]
	TIME [epoch: 24.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178128303437513		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 4.178128303437513 | validation: 4.1914418612444235]
	TIME [epoch: 24.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.205532593017136		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 4.205532593017136 | validation: 4.218692414731452]
	TIME [epoch: 24.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.185429113816528		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 4.185429113816528 | validation: 4.260400718771308]
	TIME [epoch: 24.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.259774452977091		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 4.259774452977091 | validation: 4.2376172563574315]
	TIME [epoch: 24.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.347438141015187		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 4.347438141015187 | validation: 4.173932704469366]
	TIME [epoch: 24.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.222232200937452		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 4.222232200937452 | validation: 4.204716180451982]
	TIME [epoch: 24.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.191425692200335		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 4.191425692200335 | validation: 4.187669127477959]
	TIME [epoch: 24.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182766557002645		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 4.182766557002645 | validation: 4.150872933743346]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183370970360663		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 4.183370970360663 | validation: 4.204159520287376]
	TIME [epoch: 24.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1844740755690015		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 4.1844740755690015 | validation: 4.23230287188083]
	TIME [epoch: 24.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1875415735134585		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 4.1875415735134585 | validation: 4.18212175669584]
	TIME [epoch: 24.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.181060979497234		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 4.181060979497234 | validation: 4.164545891951277]
	TIME [epoch: 24.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155583914627943		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 4.155583914627943 | validation: 4.193346792044402]
	TIME [epoch: 24.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.230628369053765		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 4.230628369053765 | validation: 4.281656626294698]
	TIME [epoch: 24.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2069559882591445		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 4.2069559882591445 | validation: 4.176458380006664]
	TIME [epoch: 24.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.217940857321276		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 4.217940857321276 | validation: 4.171713614479992]
	TIME [epoch: 24.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187089462139541		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 4.187089462139541 | validation: 4.1880805511143855]
	TIME [epoch: 24.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.225230118548246		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 4.225230118548246 | validation: 4.168725289337281]
	TIME [epoch: 24.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184742558501035		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 4.184742558501035 | validation: 4.668872646723943]
	TIME [epoch: 24.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.373542352158938		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 4.373542352158938 | validation: 4.150555674994299]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1785066868800875		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 4.1785066868800875 | validation: 4.227745577389011]
	TIME [epoch: 24.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.186360695196929		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 4.186360695196929 | validation: 4.19819370025996]
	TIME [epoch: 24.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.185081065429234		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 4.185081065429234 | validation: 4.197929246779613]
	TIME [epoch: 24.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.279119349515609		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 4.279119349515609 | validation: 4.175688865265534]
	TIME [epoch: 24.8 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.162433596177715		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 4.162433596177715 | validation: 4.158886238431664]
	TIME [epoch: 24.8 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.203091890112743		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 4.203091890112743 | validation: 4.585359311789223]
	TIME [epoch: 24.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2968996715669245		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 4.2968996715669245 | validation: 4.2115291967467225]
	TIME [epoch: 24.8 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.181841535817266		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 4.181841535817266 | validation: 4.183571539734733]
	TIME [epoch: 24.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.19824637684437		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 4.19824637684437 | validation: 4.172046221373426]
	TIME [epoch: 24.8 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156933803317602		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 4.156933803317602 | validation: 4.230946357336315]
	TIME [epoch: 24.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.238939839462128		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 4.238939839462128 | validation: 4.25585339535744]
	TIME [epoch: 24.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.192660008202417		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 4.192660008202417 | validation: 4.269137828272055]
	TIME [epoch: 24.8 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2228164345261945		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 4.2228164345261945 | validation: 4.240874082512282]
	TIME [epoch: 24.8 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.206686971078319		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 4.206686971078319 | validation: 4.191962638814431]
	TIME [epoch: 24.8 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183500136344534		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 4.183500136344534 | validation: 4.150765328500597]
	TIME [epoch: 24.8 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.19239120386003		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 4.19239120386003 | validation: 4.16940390137097]
	TIME [epoch: 24.8 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.231081137740359		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 4.231081137740359 | validation: 4.246520800546737]
	TIME [epoch: 24.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2739403729949785		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 4.2739403729949785 | validation: 4.271665046364715]
	TIME [epoch: 24.8 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2601330348476765		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 4.2601330348476765 | validation: 4.153969782183327]
	TIME [epoch: 24.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178026117305557		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 4.178026117305557 | validation: 4.217251686067134]
	TIME [epoch: 24.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18838847765705		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 4.18838847765705 | validation: 4.205010235671534]
	TIME [epoch: 24.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.217106294360237		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 4.217106294360237 | validation: 4.195263581936331]
	TIME [epoch: 24.8 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.228039686454568		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 4.228039686454568 | validation: 4.19745626666208]
	TIME [epoch: 24.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187991794514044		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 4.187991794514044 | validation: 4.149561142874387]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22737896289281		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 4.22737896289281 | validation: 4.252335756941797]
	TIME [epoch: 24.8 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.223313411232595		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 4.223313411232595 | validation: 4.239720650711501]
	TIME [epoch: 24.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.169916473013112		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 4.169916473013112 | validation: 4.1465654432706]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187456271090386		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 4.187456271090386 | validation: 4.3445614838466105]
	TIME [epoch: 24.8 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249702714489378		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 4.249702714489378 | validation: 4.21237143679488]
	TIME [epoch: 24.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.270343638400568		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 4.270343638400568 | validation: 4.148527855418669]
	TIME [epoch: 24.8 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.226430180138316		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 4.226430180138316 | validation: 4.159272782943305]
	TIME [epoch: 24.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.194593707738161		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 4.194593707738161 | validation: 4.19935432021345]
	TIME [epoch: 24.8 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184684369271581		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 4.184684369271581 | validation: 4.18851607699952]
	TIME [epoch: 24.8 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160135145331261		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 4.160135145331261 | validation: 4.169845887707341]
	TIME [epoch: 24.8 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.218336642141935		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 4.218336642141935 | validation: 4.267502853704055]
	TIME [epoch: 24.8 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249954606524008		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 4.249954606524008 | validation: 4.2148603012439105]
	TIME [epoch: 24.8 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175239727995674		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 4.175239727995674 | validation: 4.171004709616678]
	TIME [epoch: 24.8 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161845567459743		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 4.161845567459743 | validation: 4.158541090236984]
	TIME [epoch: 24.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.197955514211692		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 4.197955514211692 | validation: 4.158255140463645]
	TIME [epoch: 24.8 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18413589453937		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 4.18413589453937 | validation: 4.1501000396408205]
	TIME [epoch: 24.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.205961119741139		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 4.205961119741139 | validation: 4.193399792991027]
	TIME [epoch: 24.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2103138404819855		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 4.2103138404819855 | validation: 4.16505292323684]
	TIME [epoch: 24.8 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.179666949584409		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 4.179666949584409 | validation: 4.17014762698448]
	TIME [epoch: 24.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157904674065027		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 4.157904674065027 | validation: 4.162662802724884]
	TIME [epoch: 24.8 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178709104588551		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 4.178709104588551 | validation: 4.237663879858235]
	TIME [epoch: 24.8 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.189267732866773		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 4.189267732866773 | validation: 4.1641612140987405]
	TIME [epoch: 24.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.212474616933462		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 4.212474616933462 | validation: 4.153330251883049]
	TIME [epoch: 24.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170477276752976		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 4.170477276752976 | validation: 4.2108616413694735]
	TIME [epoch: 24.8 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.181300197387349		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 4.181300197387349 | validation: 4.219547790240792]
	TIME [epoch: 24.8 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183507299990951		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 4.183507299990951 | validation: 4.157650993948776]
	TIME [epoch: 24.8 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165288072992177		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 4.165288072992177 | validation: 4.154728056011289]
	TIME [epoch: 24.8 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174814908726214		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 4.174814908726214 | validation: 4.159277619248729]
	TIME [epoch: 24.8 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172777113795346		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 4.172777113795346 | validation: 4.161062098780842]
	TIME [epoch: 24.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167325038313858		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 4.167325038313858 | validation: 4.142934614406756]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172072424516116		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 4.172072424516116 | validation: 4.156275089492194]
	TIME [epoch: 24.8 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158501163578838		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 4.158501163578838 | validation: 4.163096474510894]
	TIME [epoch: 24.8 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178808136257711		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 4.178808136257711 | validation: 4.202485644302873]
	TIME [epoch: 24.8 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.190123482937009		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 4.190123482937009 | validation: 4.153209429535952]
	TIME [epoch: 24.8 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148618136612024		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 4.148618136612024 | validation: 4.17506832818576]
	TIME [epoch: 24.8 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.171815163935895		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 4.171815163935895 | validation: 4.250158329663353]
	TIME [epoch: 24.8 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.173850458288228		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 4.173850458288228 | validation: 4.2225897310000935]
	TIME [epoch: 24.8 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.169646513548136		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 4.169646513548136 | validation: 4.2392705229596235]
	TIME [epoch: 24.8 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.193055580136665		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 4.193055580136665 | validation: 4.163705020352545]
	TIME [epoch: 24.8 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2085634741350955		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 4.2085634741350955 | validation: 4.316586609910525]
	TIME [epoch: 24.8 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.212588087117423		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 4.212588087117423 | validation: 4.186245730030703]
	TIME [epoch: 24.8 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.247031976174235		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 4.247031976174235 | validation: 4.160768503497907]
	TIME [epoch: 24.8 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2122067708366675		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 4.2122067708366675 | validation: 4.3128600358343645]
	TIME [epoch: 24.8 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.233783148398558		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 4.233783148398558 | validation: 4.1843304829638415]
	TIME [epoch: 24.8 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170922006547023		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 4.170922006547023 | validation: 4.285461567987515]
	TIME [epoch: 24.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.223227282606073		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 4.223227282606073 | validation: 4.20864116370793]
	TIME [epoch: 24.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182756695098158		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 4.182756695098158 | validation: 4.185852366711727]
	TIME [epoch: 24.8 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.168182464012969		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 4.168182464012969 | validation: 4.192129914660846]
	TIME [epoch: 24.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1795215297403185		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 4.1795215297403185 | validation: 4.332618404578844]
	TIME [epoch: 24.8 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.214331160866226		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 4.214331160866226 | validation: 4.173600314699076]
	TIME [epoch: 24.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.196579933221544		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 4.196579933221544 | validation: 4.185854156381035]
	TIME [epoch: 24.8 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184370411664094		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 4.184370411664094 | validation: 4.230358820939171]
	TIME [epoch: 24.8 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182730044525197		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 4.182730044525197 | validation: 4.146813789737423]
	TIME [epoch: 24.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.162337307070078		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 4.162337307070078 | validation: 4.159013190047709]
	TIME [epoch: 24.8 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151382752825016		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 4.151382752825016 | validation: 4.17232661497763]
	TIME [epoch: 24.8 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184968299467802		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 4.184968299467802 | validation: 4.212797492435643]
	TIME [epoch: 24.8 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.169623550358221		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 4.169623550358221 | validation: 4.442016169537309]
	TIME [epoch: 24.8 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268603772616469		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 4.268603772616469 | validation: 4.154605878806811]
	TIME [epoch: 24.8 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144526086324152		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 4.144526086324152 | validation: 4.188230040044827]
	TIME [epoch: 24.8 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164268892263827		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 4.164268892263827 | validation: 4.135152091401962]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156946197375678		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 4.156946197375678 | validation: 4.2595825766156885]
	TIME [epoch: 24.8 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2030772348163765		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 4.2030772348163765 | validation: 4.1832966829320615]
	TIME [epoch: 24.8 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160962253442415		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 4.160962253442415 | validation: 4.227608696872423]
	TIME [epoch: 24.8 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184139129340771		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 4.184139129340771 | validation: 4.177409579219512]
	TIME [epoch: 24.8 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.249791410035657		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 4.249791410035657 | validation: 4.26547727670595]
	TIME [epoch: 24.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.286455382068061		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 4.286455382068061 | validation: 4.1496160182529245]
	TIME [epoch: 24.8 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.192369941246619		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 4.192369941246619 | validation: 4.159628972196889]
	TIME [epoch: 24.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155396419149517		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 4.155396419149517 | validation: 4.26611841836569]
	TIME [epoch: 24.8 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.217669124016222		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 4.217669124016222 | validation: 4.161855760452879]
	TIME [epoch: 24.8 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175125961912983		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 4.175125961912983 | validation: 4.20218644277105]
	TIME [epoch: 24.8 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159891245157699		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 4.159891245157699 | validation: 4.144566719998168]
	TIME [epoch: 24.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151017439013012		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 4.151017439013012 | validation: 4.190521457491088]
	TIME [epoch: 24.8 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.17903467425089		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 4.17903467425089 | validation: 4.147491105321094]
	TIME [epoch: 24.8 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.16430761043792		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 4.16430761043792 | validation: 4.1656899485259]
	TIME [epoch: 24.8 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1582745761866695		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 4.1582745761866695 | validation: 4.157100138739726]
	TIME [epoch: 24.8 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1881936276953535		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 4.1881936276953535 | validation: 4.217117821898458]
	TIME [epoch: 24.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.22946599045232		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 4.22946599045232 | validation: 4.215660388960867]
	TIME [epoch: 24.8 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165234111487434		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 4.165234111487434 | validation: 4.182743550642988]
	TIME [epoch: 24.8 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.162805155105297		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 4.162805155105297 | validation: 4.155509578911954]
	TIME [epoch: 24.8 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155670249203164		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 4.155670249203164 | validation: 4.161534658172233]
	TIME [epoch: 24.8 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2191909910675935		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 4.2191909910675935 | validation: 4.2258112420739335]
	TIME [epoch: 24.8 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183964335100749		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 4.183964335100749 | validation: 4.239118335288874]
	TIME [epoch: 24.8 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.186800980969726		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 4.186800980969726 | validation: 4.208391827437674]
	TIME [epoch: 24.8 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.188876374180367		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 4.188876374180367 | validation: 4.257677042731056]
	TIME [epoch: 24.8 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1787892014268		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 4.1787892014268 | validation: 4.160411899714718]
	TIME [epoch: 24.8 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142157417553658		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 4.142157417553658 | validation: 4.350034477032742]
	TIME [epoch: 24.8 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.235731354521554		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 4.235731354521554 | validation: 4.152513488892866]
	TIME [epoch: 24.8 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178110683935165		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 4.178110683935165 | validation: 4.149067390940659]
	TIME [epoch: 24.8 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154769840619276		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 4.154769840619276 | validation: 4.168049629922157]
	TIME [epoch: 24.8 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160841728145526		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 4.160841728145526 | validation: 4.180000367752027]
	TIME [epoch: 24.8 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155324941681422		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 4.155324941681422 | validation: 4.149502541410447]
	TIME [epoch: 24.8 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15040428614995		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 4.15040428614995 | validation: 4.154798927929233]
	TIME [epoch: 24.8 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160289325049518		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 4.160289325049518 | validation: 4.198320118505461]
	TIME [epoch: 24.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1781149796363		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 4.1781149796363 | validation: 4.196136370002902]
	TIME [epoch: 24.8 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.200059705014187		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 4.200059705014187 | validation: 4.179139971001849]
	TIME [epoch: 24.8 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195978698560982		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 4.195978698560982 | validation: 4.220607482242697]
	TIME [epoch: 24.8 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.169445432701532		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 4.169445432701532 | validation: 4.158327000728283]
	TIME [epoch: 24.8 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.186281237876823		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 4.186281237876823 | validation: 4.143782623066776]
	TIME [epoch: 24.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.160896014609706		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 4.160896014609706 | validation: 4.173757778538332]
	TIME [epoch: 24.8 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146700152365912		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 4.146700152365912 | validation: 4.147582578804436]
	TIME [epoch: 24.8 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158541910343784		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 4.158541910343784 | validation: 4.2131040669960385]
	TIME [epoch: 24.8 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175298366314281		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 4.175298366314281 | validation: 4.145177451164932]
	TIME [epoch: 24.8 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195239741348736		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 4.195239741348736 | validation: 4.148646635297754]
	TIME [epoch: 24.8 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148827013295453		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 4.148827013295453 | validation: 4.165605876349319]
	TIME [epoch: 24.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156283400668219		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 4.156283400668219 | validation: 4.213612147442694]
	TIME [epoch: 24.8 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167607632344615		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 4.167607632344615 | validation: 4.172489435666647]
	TIME [epoch: 24.8 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164475942020689		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 4.164475942020689 | validation: 4.1636117679036815]
	TIME [epoch: 24.8 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.16763078380051		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 4.16763078380051 | validation: 4.158029925184616]
	TIME [epoch: 24.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145607631295253		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 4.145607631295253 | validation: 4.164226949664915]
	TIME [epoch: 24.8 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.180272003810939		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 4.180272003810939 | validation: 4.237570277059326]
	TIME [epoch: 24.8 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182115604627375		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 4.182115604627375 | validation: 4.16283158825909]
	TIME [epoch: 24.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14388167339523		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 4.14388167339523 | validation: 4.1765945975674565]
	TIME [epoch: 24.8 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159029679360238		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 4.159029679360238 | validation: 4.157974281323219]
	TIME [epoch: 24.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175188448207226		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 4.175188448207226 | validation: 4.165852820576964]
	TIME [epoch: 24.8 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158963585247033		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 4.158963585247033 | validation: 4.144137030389578]
	TIME [epoch: 24.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.184357389753535		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 4.184357389753535 | validation: 4.280326077256781]
	TIME [epoch: 24.8 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.178529967830947		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 4.178529967830947 | validation: 4.19280597807142]
	TIME [epoch: 24.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170345707932607		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 4.170345707932607 | validation: 4.16235949364638]
	TIME [epoch: 24.8 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1504363466102925		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 4.1504363466102925 | validation: 4.227895393919331]
	TIME [epoch: 24.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1619010246367525		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 4.1619010246367525 | validation: 4.147572789781683]
	TIME [epoch: 24.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147317649689201		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 4.147317649689201 | validation: 4.159599483053593]
	TIME [epoch: 24.8 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158172693266328		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 4.158172693266328 | validation: 4.213028692613066]
	TIME [epoch: 24.8 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165058620948156		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 4.165058620948156 | validation: 4.200118875479149]
	TIME [epoch: 24.8 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167004670181896		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 4.167004670181896 | validation: 4.1630001955143925]
	TIME [epoch: 24.8 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149347813181397		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 4.149347813181397 | validation: 4.222375377991478]
	TIME [epoch: 24.8 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164184264530408		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 4.164184264530408 | validation: 4.157714399528429]
	TIME [epoch: 24.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164381663870539		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 4.164381663870539 | validation: 4.189991056064783]
	TIME [epoch: 24.8 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166892676063769		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 4.166892676063769 | validation: 4.19047530112734]
	TIME [epoch: 24.8 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183798995284667		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 4.183798995284667 | validation: 4.282210660347851]
	TIME [epoch: 24.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.236946992247051		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 4.236946992247051 | validation: 4.215180397829521]
	TIME [epoch: 24.8 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15369896737607		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 4.15369896737607 | validation: 4.156034367080276]
	TIME [epoch: 24.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166506757472979		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 4.166506757472979 | validation: 4.150578339480788]
	TIME [epoch: 24.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152092995224882		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 4.152092995224882 | validation: 4.187836936591153]
	TIME [epoch: 24.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170189357676342		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 4.170189357676342 | validation: 4.175000281444803]
	TIME [epoch: 24.8 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157461751113781		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 4.157461751113781 | validation: 4.224877494615414]
	TIME [epoch: 24.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.222972489646129		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 4.222972489646129 | validation: 4.331759396672611]
	TIME [epoch: 24.8 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1928770601813685		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 4.1928770601813685 | validation: 4.207797186118613]
	TIME [epoch: 24.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.191437753063221		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 4.191437753063221 | validation: 4.151999190928972]
	TIME [epoch: 24.8 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141361010272185		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 4.141361010272185 | validation: 4.155203655115736]
	TIME [epoch: 24.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.19507368729979		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 4.19507368729979 | validation: 4.209298371057445]
	TIME [epoch: 24.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.189558142612439		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 4.189558142612439 | validation: 4.191468720387489]
	TIME [epoch: 24.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158794590876072		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 4.158794590876072 | validation: 4.150361258971383]
	TIME [epoch: 24.8 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167615928946435		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 4.167615928946435 | validation: 4.17563371038914]
	TIME [epoch: 24.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152876778895685		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 4.152876778895685 | validation: 4.162931545982563]
	TIME [epoch: 24.8 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1703508741693796		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 4.1703508741693796 | validation: 4.160351961768375]
	TIME [epoch: 24.8 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.163690680015009		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 4.163690680015009 | validation: 4.29279039851959]
	TIME [epoch: 24.8 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.206515126078073		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 4.206515126078073 | validation: 4.185053207748942]
	TIME [epoch: 24.8 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1487877403924145		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 4.1487877403924145 | validation: 4.144266911426394]
	TIME [epoch: 24.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154997863081487		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 4.154997863081487 | validation: 4.200254865128145]
	TIME [epoch: 24.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150963508810288		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 4.150963508810288 | validation: 4.1499870776245915]
	TIME [epoch: 24.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1787804641664845		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 4.1787804641664845 | validation: 4.1701635232251535]
	TIME [epoch: 24.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149800840174546		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 4.149800840174546 | validation: 4.202380499652272]
	TIME [epoch: 24.8 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174107991743599		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 4.174107991743599 | validation: 4.209608996845766]
	TIME [epoch: 24.8 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175554528432821		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 4.175554528432821 | validation: 4.139438416514787]
	TIME [epoch: 24.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154888468988505		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 4.154888468988505 | validation: 4.153408377557301]
	TIME [epoch: 24.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165042492589531		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 4.165042492589531 | validation: 4.1421315843739634]
	TIME [epoch: 24.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147855335786905		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 4.147855335786905 | validation: 4.157422227166008]
	TIME [epoch: 24.8 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157402683420523		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 4.157402683420523 | validation: 4.178637318069322]
	TIME [epoch: 24.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175326719236941		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 4.175326719236941 | validation: 4.160616061719798]
	TIME [epoch: 24.8 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183678855448866		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 4.183678855448866 | validation: 4.18992544099457]
	TIME [epoch: 24.8 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159448783669323		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 4.159448783669323 | validation: 4.155963450180249]
	TIME [epoch: 24.8 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.236406430380617		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 4.236406430380617 | validation: 4.217574095377045]
	TIME [epoch: 24.8 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18516359573069		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 4.18516359573069 | validation: 4.25390875637773]
	TIME [epoch: 24.8 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1800699084257475		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 4.1800699084257475 | validation: 4.220484792865609]
	TIME [epoch: 24.8 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156933494024461		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 4.156933494024461 | validation: 4.21797613161552]
	TIME [epoch: 24.8 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.16920220752322		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 4.16920220752322 | validation: 4.156202265132193]
	TIME [epoch: 24.8 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.179363489161189		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 4.179363489161189 | validation: 4.222577348222961]
	TIME [epoch: 24.8 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172417648978331		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 4.172417648978331 | validation: 4.166815809977875]
	TIME [epoch: 24.8 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148086574414831		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 4.148086574414831 | validation: 4.16213952565462]
	TIME [epoch: 24.8 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143340222887543		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 4.143340222887543 | validation: 4.151513603345936]
	TIME [epoch: 24.8 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146440186275646		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 4.146440186275646 | validation: 4.155870559640615]
	TIME [epoch: 24.8 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139153256659272		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 4.139153256659272 | validation: 4.166878828333692]
	TIME [epoch: 24.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149437201922221		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 4.149437201922221 | validation: 4.18205034933293]
	TIME [epoch: 24.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151493964485864		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 4.151493964485864 | validation: 4.206944583608629]
	TIME [epoch: 24.8 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.18305254576941		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 4.18305254576941 | validation: 4.146453042432174]
	TIME [epoch: 24.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133313431219097		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 4.133313431219097 | validation: 4.149227290078295]
	TIME [epoch: 24.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.168228035969665		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 4.168228035969665 | validation: 4.169979261602094]
	TIME [epoch: 24.8 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157148275728613		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 4.157148275728613 | validation: 4.148795080264397]
	TIME [epoch: 24.8 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1628423673178885		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 4.1628423673178885 | validation: 4.171770881989989]
	TIME [epoch: 24.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153562418775533		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 4.153562418775533 | validation: 4.1614279376401635]
	TIME [epoch: 24.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170279758795441		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 4.170279758795441 | validation: 4.152827431680355]
	TIME [epoch: 24.8 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1583667678267		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 4.1583667678267 | validation: 4.170940779123414]
	TIME [epoch: 24.8 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144107174258953		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 4.144107174258953 | validation: 4.156060111675357]
	TIME [epoch: 24.8 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138247517561019		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 4.138247517561019 | validation: 4.144847892626899]
	TIME [epoch: 24.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.193462615681241		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 4.193462615681241 | validation: 4.164702129094389]
	TIME [epoch: 24.8 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.187522028291989		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 4.187522028291989 | validation: 4.227797498417935]
	TIME [epoch: 24.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1649663073736995		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 4.1649663073736995 | validation: 4.162297361538174]
	TIME [epoch: 24.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158934973859418		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 4.158934973859418 | validation: 4.1629303301672715]
	TIME [epoch: 24.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143400350319538		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 4.143400350319538 | validation: 4.213132070066187]
	TIME [epoch: 24.8 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.182046109608796		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 4.182046109608796 | validation: 4.145275622799223]
	TIME [epoch: 24.8 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135506799751713		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 4.135506799751713 | validation: 4.1366763834603795]
	TIME [epoch: 24.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135427016796655		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 4.135427016796655 | validation: 4.153653957438947]
	TIME [epoch: 24.8 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139117979416658		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 4.139117979416658 | validation: 4.148796212103667]
	TIME [epoch: 24.8 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2038965850508285		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 4.2038965850508285 | validation: 4.145601063947886]
	TIME [epoch: 24.8 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14184941316506		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 4.14184941316506 | validation: 4.1426143270727405]
	TIME [epoch: 24.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14532847273559		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 4.14532847273559 | validation: 4.183727309398467]
	TIME [epoch: 24.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155761356352992		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 4.155761356352992 | validation: 4.193338908219794]
	TIME [epoch: 24.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1551807781927454		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 4.1551807781927454 | validation: 4.142145010126605]
	TIME [epoch: 24.8 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145228201728524		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 4.145228201728524 | validation: 4.1471071630369]
	TIME [epoch: 24.8 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141978951888204		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 4.141978951888204 | validation: 4.148518213851788]
	TIME [epoch: 24.8 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172158403299609		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 4.172158403299609 | validation: 4.201341385622761]
	TIME [epoch: 24.8 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161123956554119		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 4.161123956554119 | validation: 4.156131364942511]
	TIME [epoch: 24.8 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159324315663513		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 4.159324315663513 | validation: 4.183159995915706]
	TIME [epoch: 24.8 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155657056208839		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 4.155657056208839 | validation: 4.144835546435779]
	TIME [epoch: 24.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15264918937976		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 4.15264918937976 | validation: 4.165072689985036]
	TIME [epoch: 24.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155762120280365		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 4.155762120280365 | validation: 4.171978487576949]
	TIME [epoch: 24.8 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1514599940452		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 4.1514599940452 | validation: 4.1624931002200665]
	TIME [epoch: 24.8 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144078447600762		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 4.144078447600762 | validation: 4.156210364663939]
	TIME [epoch: 24.8 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148663101634322		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 4.148663101634322 | validation: 4.158426710213794]
	TIME [epoch: 24.8 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1363674898942975		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 4.1363674898942975 | validation: 4.142229175298115]
	TIME [epoch: 24.8 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151430238160918		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 4.151430238160918 | validation: 4.139108864753311]
	TIME [epoch: 24.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154450895209663		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 4.154450895209663 | validation: 4.211347147186818]
	TIME [epoch: 24.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183417536357972		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 4.183417536357972 | validation: 4.155468849071888]
	TIME [epoch: 24.8 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175158246649414		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 4.175158246649414 | validation: 4.142170151891467]
	TIME [epoch: 24.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143078419564628		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 4.143078419564628 | validation: 4.188880994970782]
	TIME [epoch: 24.8 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165730778333607		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 4.165730778333607 | validation: 4.163393942229642]
	TIME [epoch: 24.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140977219484108		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 4.140977219484108 | validation: 4.154118862566911]
	TIME [epoch: 24.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140185760973289		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 4.140185760973289 | validation: 4.154486009843231]
	TIME [epoch: 24.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143350083916746		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 4.143350083916746 | validation: 4.2338953395001395]
	TIME [epoch: 24.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170344726478298		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 4.170344726478298 | validation: 4.150910916554554]
	TIME [epoch: 24.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150076956850091		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 4.150076956850091 | validation: 4.139049646021232]
	TIME [epoch: 24.8 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154873365500046		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 4.154873365500046 | validation: 4.134213173867695]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138177901940473		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 4.138177901940473 | validation: 4.157382985489981]
	TIME [epoch: 24.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161418438185387		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 4.161418438185387 | validation: 4.147663015864945]
	TIME [epoch: 24.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140142866788187		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 4.140142866788187 | validation: 4.187530644300538]
	TIME [epoch: 24.8 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167372532995748		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 4.167372532995748 | validation: 4.166578675214799]
	TIME [epoch: 24.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155971729575826		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 4.155971729575826 | validation: 4.168162482168608]
	TIME [epoch: 24.8 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150907945924168		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 4.150907945924168 | validation: 4.158973750808826]
	TIME [epoch: 24.8 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1386817745179		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 4.1386817745179 | validation: 4.199522618014554]
	TIME [epoch: 24.8 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.188619800494873		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 4.188619800494873 | validation: 4.179666782410339]
	TIME [epoch: 24.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144101032337462		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 4.144101032337462 | validation: 4.1773144505345945]
	TIME [epoch: 24.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158387995689179		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 4.158387995689179 | validation: 4.131507505536817]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157029747812646		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 4.157029747812646 | validation: 4.146472570776429]
	TIME [epoch: 24.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170648473194413		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 4.170648473194413 | validation: 4.216820807920735]
	TIME [epoch: 24.8 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148973385185329		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 4.148973385185329 | validation: 4.142559221248513]
	TIME [epoch: 24.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147319720233137		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 4.147319720233137 | validation: 4.1543729909101]
	TIME [epoch: 24.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136612988450677		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 4.136612988450677 | validation: 4.171652383725355]
	TIME [epoch: 24.8 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143017543914048		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 4.143017543914048 | validation: 4.165883235018404]
	TIME [epoch: 24.8 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145269200601639		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 4.145269200601639 | validation: 4.165811330749161]
	TIME [epoch: 24.8 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149541543827877		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 4.149541543827877 | validation: 4.1596297930593655]
	TIME [epoch: 24.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139777587855659		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 4.139777587855659 | validation: 4.14453481782706]
	TIME [epoch: 24.8 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1400135860857965		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 4.1400135860857965 | validation: 4.139513596475231]
	TIME [epoch: 24.8 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145372838450922		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 4.145372838450922 | validation: 4.177489991599819]
	TIME [epoch: 24.8 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159867923833184		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 4.159867923833184 | validation: 4.145661010791896]
	TIME [epoch: 24.8 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152766050733134		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 4.152766050733134 | validation: 4.143031506880466]
	TIME [epoch: 24.8 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15001331970042		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 4.15001331970042 | validation: 4.147422674451556]
	TIME [epoch: 24.8 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141323634506836		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 4.141323634506836 | validation: 4.241705886376541]
	TIME [epoch: 24.8 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.175877684830661		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 4.175877684830661 | validation: 4.243182348898901]
	TIME [epoch: 24.8 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1785391114425465		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 4.1785391114425465 | validation: 4.232545974080226]
	TIME [epoch: 24.8 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.181354451841635		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 4.181354451841635 | validation: 4.138150221155594]
	TIME [epoch: 24.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136528492641254		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 4.136528492641254 | validation: 4.186769744552244]
	TIME [epoch: 24.8 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161162669588228		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 4.161162669588228 | validation: 4.151115876566271]
	TIME [epoch: 24.8 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141145988668307		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 4.141145988668307 | validation: 4.161010646285507]
	TIME [epoch: 24.8 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.163108052435234		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 4.163108052435234 | validation: 4.156332591721371]
	TIME [epoch: 24.8 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14829965664288		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 4.14829965664288 | validation: 4.136942919333616]
	TIME [epoch: 24.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149893287249338		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 4.149893287249338 | validation: 4.171580976737777]
	TIME [epoch: 24.8 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159177708793674		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 4.159177708793674 | validation: 4.177145256550891]
	TIME [epoch: 24.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1441442666032255		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 4.1441442666032255 | validation: 4.177102751160792]
	TIME [epoch: 24.8 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.168152799232837		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 4.168152799232837 | validation: 4.151740271205093]
	TIME [epoch: 24.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.165852234629094		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 4.165852234629094 | validation: 4.257308323708702]
	TIME [epoch: 24.8 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.173023552762969		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 4.173023552762969 | validation: 4.177996785584413]
	TIME [epoch: 24.8 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147185560736835		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 4.147185560736835 | validation: 4.164210146392151]
	TIME [epoch: 24.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167799567683474		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 4.167799567683474 | validation: 4.213746188283409]
	TIME [epoch: 24.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158105329253597		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 4.158105329253597 | validation: 4.155930816526783]
	TIME [epoch: 24.8 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14526645437344		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 4.14526645437344 | validation: 4.143500478558674]
	TIME [epoch: 24.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151746117263287		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 4.151746117263287 | validation: 4.147388546064969]
	TIME [epoch: 24.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132778007858612		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 4.132778007858612 | validation: 4.1430357386263585]
	TIME [epoch: 24.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134669914072721		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 4.134669914072721 | validation: 4.146250627801139]
	TIME [epoch: 24.8 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136165955786295		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 4.136165955786295 | validation: 4.147050057053511]
	TIME [epoch: 24.8 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154532781217739		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 4.154532781217739 | validation: 4.1934347673270445]
	TIME [epoch: 24.8 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148709665625021		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 4.148709665625021 | validation: 4.155977976505223]
	TIME [epoch: 24.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154425254660958		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 4.154425254660958 | validation: 4.167315510256737]
	TIME [epoch: 24.8 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149367305485278		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 4.149367305485278 | validation: 4.151937467200441]
	TIME [epoch: 24.8 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.154581945937926		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 4.154581945937926 | validation: 4.158127374354823]
	TIME [epoch: 24.8 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151550572859452		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 4.151550572859452 | validation: 4.147639560458015]
	TIME [epoch: 24.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147992110871357		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 4.147992110871357 | validation: 4.168563972327137]
	TIME [epoch: 24.8 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140435444558522		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 4.140435444558522 | validation: 4.205017837728997]
	TIME [epoch: 24.8 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174632545445892		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 4.174632545445892 | validation: 4.166325657088189]
	TIME [epoch: 24.8 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142517453636705		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 4.142517453636705 | validation: 4.153123724234783]
	TIME [epoch: 24.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156380314221173		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 4.156380314221173 | validation: 4.153095487977991]
	TIME [epoch: 24.8 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1386414869468915		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 4.1386414869468915 | validation: 4.154817289238651]
	TIME [epoch: 24.8 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138573351585792		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 4.138573351585792 | validation: 4.175402989207182]
	TIME [epoch: 24.8 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143489971232194		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 4.143489971232194 | validation: 4.154548668253477]
	TIME [epoch: 24.8 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149528841917389		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 4.149528841917389 | validation: 4.162650225179348]
	TIME [epoch: 24.8 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141753708337201		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 4.141753708337201 | validation: 4.2063352706194275]
	TIME [epoch: 24.8 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166515631591463		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 4.166515631591463 | validation: 4.145985080664441]
	TIME [epoch: 24.8 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14207921520155		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 4.14207921520155 | validation: 4.1447668376674764]
	TIME [epoch: 24.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1459075415572535		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 4.1459075415572535 | validation: 4.150307036638951]
	TIME [epoch: 24.8 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.157649290555533		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 4.157649290555533 | validation: 4.15722165276899]
	TIME [epoch: 24.8 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140146536732516		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 4.140146536732516 | validation: 4.165638293506569]
	TIME [epoch: 24.8 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145929529790763		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 4.145929529790763 | validation: 4.1819680784716615]
	TIME [epoch: 24.8 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150973270073655		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 4.150973270073655 | validation: 4.150903977499008]
	TIME [epoch: 24.8 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132959217102602		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 4.132959217102602 | validation: 4.147767847216992]
	TIME [epoch: 24.8 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140781779198247		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 4.140781779198247 | validation: 4.174395402707125]
	TIME [epoch: 24.8 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1577587409389025		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 4.1577587409389025 | validation: 4.179243723168187]
	TIME [epoch: 24.8 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155659622304291		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 4.155659622304291 | validation: 4.189632230973892]
	TIME [epoch: 24.8 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146772984617751		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 4.146772984617751 | validation: 4.172124828061441]
	TIME [epoch: 24.8 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147359122673885		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 4.147359122673885 | validation: 4.197182274998307]
	TIME [epoch: 24.8 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152423567377493		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 4.152423567377493 | validation: 4.148026073938205]
	TIME [epoch: 24.8 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13394318407576		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 4.13394318407576 | validation: 4.18490976965893]
	TIME [epoch: 24.8 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149563407534176		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 4.149563407534176 | validation: 4.1821308520581235]
	TIME [epoch: 24.8 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.168213128926499		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 4.168213128926499 | validation: 4.179828489225555]
	TIME [epoch: 24.8 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151789820541581		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 4.151789820541581 | validation: 4.169028745903314]
	TIME [epoch: 24.8 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142168586689138		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 4.142168586689138 | validation: 4.197121648079995]
	TIME [epoch: 24.8 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166532737437722		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 4.166532737437722 | validation: 4.14204893952579]
	TIME [epoch: 24.8 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138960299626328		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 4.138960299626328 | validation: 4.172668792186935]
	TIME [epoch: 24.8 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.16020447848936		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 4.16020447848936 | validation: 4.19717926290784]
	TIME [epoch: 24.8 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1560597261065		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 4.1560597261065 | validation: 4.144592917069903]
	TIME [epoch: 24.8 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143774099374849		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 4.143774099374849 | validation: 4.1812485535973]
	TIME [epoch: 24.8 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158925257566797		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 4.158925257566797 | validation: 4.142433887464344]
	TIME [epoch: 24.8 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139839652276668		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 4.139839652276668 | validation: 4.172627918618501]
	TIME [epoch: 24.8 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174121732705384		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 4.174121732705384 | validation: 4.229665610038455]
	TIME [epoch: 24.8 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15983074299341		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 4.15983074299341 | validation: 4.143358852213442]
	TIME [epoch: 24.8 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137435850178919		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 4.137435850178919 | validation: 4.151110923853354]
	TIME [epoch: 24.8 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134884972946151		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 4.134884972946151 | validation: 4.154201733207566]
	TIME [epoch: 24.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152962805247446		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 4.152962805247446 | validation: 4.160222720590467]
	TIME [epoch: 24.8 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151110369131357		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 4.151110369131357 | validation: 4.21174708633156]
	TIME [epoch: 24.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153456482421575		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 4.153456482421575 | validation: 4.143699878697182]
	TIME [epoch: 24.8 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141858944509067		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 4.141858944509067 | validation: 4.149623050504682]
	TIME [epoch: 24.8 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143070891309498		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 4.143070891309498 | validation: 4.169915209368557]
	TIME [epoch: 24.8 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1380633905422535		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 4.1380633905422535 | validation: 4.156099675073659]
	TIME [epoch: 24.8 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140795816088025		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 4.140795816088025 | validation: 4.151812414169272]
	TIME [epoch: 24.8 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.173997274632851		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 4.173997274632851 | validation: 4.174001456537321]
	TIME [epoch: 24.8 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152450881096135		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 4.152450881096135 | validation: 4.1600509355459305]
	TIME [epoch: 24.8 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1383844296132475		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 4.1383844296132475 | validation: 4.143337075632997]
	TIME [epoch: 24.8 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139753934418099		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 4.139753934418099 | validation: 4.156395562740067]
	TIME [epoch: 24.8 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1399920979141305		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 4.1399920979141305 | validation: 4.160724184286978]
	TIME [epoch: 24.8 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137387906474487		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 4.137387906474487 | validation: 4.172526965727508]
	TIME [epoch: 24.8 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148482968289386		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 4.148482968289386 | validation: 4.158587884427499]
	TIME [epoch: 24.8 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1573553708642645		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 4.1573553708642645 | validation: 4.182482731813621]
	TIME [epoch: 24.8 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.164452471719764		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 4.164452471719764 | validation: 4.1523165794110515]
	TIME [epoch: 24.8 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132124659552642		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 4.132124659552642 | validation: 4.169592775644129]
	TIME [epoch: 24.8 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137011575337016		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 4.137011575337016 | validation: 4.139382018415083]
	TIME [epoch: 24.8 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133993466529964		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 4.133993466529964 | validation: 4.1497500406828935]
	TIME [epoch: 24.8 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145792503687821		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 4.145792503687821 | validation: 4.19942320163473]
	TIME [epoch: 24.8 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159946957024714		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 4.159946957024714 | validation: 4.147469740871465]
	TIME [epoch: 24.8 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144732411311135		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 4.144732411311135 | validation: 4.160535059436593]
	TIME [epoch: 24.8 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134285404525159		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 4.134285404525159 | validation: 4.147600533544311]
	TIME [epoch: 24.8 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128319337447624		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 4.128319337447624 | validation: 4.14778711345576]
	TIME [epoch: 24.8 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140081925236148		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 4.140081925236148 | validation: 4.150291952299751]
	TIME [epoch: 24.8 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138978271456621		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 4.138978271456621 | validation: 4.146853261512804]
	TIME [epoch: 24.8 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132888183791653		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 4.132888183791653 | validation: 4.1533848920893055]
	TIME [epoch: 24.8 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132428126060039		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 4.132428126060039 | validation: 4.15463776610566]
	TIME [epoch: 24.8 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131508905870948		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 4.131508905870948 | validation: 4.153855943280356]
	TIME [epoch: 24.8 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134682495325237		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 4.134682495325237 | validation: 4.148601740047423]
	TIME [epoch: 24.8 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139947268711383		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 4.139947268711383 | validation: 4.169674763891276]
	TIME [epoch: 24.8 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146398700386665		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 4.146398700386665 | validation: 4.152247358470493]
	TIME [epoch: 24.8 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134498798238144		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 4.134498798238144 | validation: 4.146855388794085]
	TIME [epoch: 24.8 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131182281748763		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 4.131182281748763 | validation: 4.155943896888267]
	TIME [epoch: 24.8 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.185753528865708		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 4.185753528865708 | validation: 4.232566224159548]
	TIME [epoch: 24.8 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.167849014441957		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 4.167849014441957 | validation: 4.175985174283444]
	TIME [epoch: 24.8 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.170940456875408		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 4.170940456875408 | validation: 4.135175825724627]
	TIME [epoch: 24.8 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132154859434935		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 4.132154859434935 | validation: 4.138626675340543]
	TIME [epoch: 24.8 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1376443706221995		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 4.1376443706221995 | validation: 4.144771502546376]
	TIME [epoch: 24.8 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134104160424824		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 4.134104160424824 | validation: 4.1309294356627415]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_876.pth
	Model improved!!!
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141129620730109		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 4.141129620730109 | validation: 4.147647098142644]
	TIME [epoch: 24.8 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134664329454889		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 4.134664329454889 | validation: 4.142280089280749]
	TIME [epoch: 24.8 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137493865773162		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 4.137493865773162 | validation: 4.1829599114870595]
	TIME [epoch: 24.8 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155801041428527		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 4.155801041428527 | validation: 4.184582423358789]
	TIME [epoch: 24.8 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1529126434452515		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 4.1529126434452515 | validation: 4.140058420487563]
	TIME [epoch: 24.8 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1278515790327095		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 4.1278515790327095 | validation: 4.161759682211664]
	TIME [epoch: 24.8 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1630925805745065		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 4.1630925805745065 | validation: 4.147023144722617]
	TIME [epoch: 24.8 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134988582351147		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 4.134988582351147 | validation: 4.148860859440341]
	TIME [epoch: 24.8 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143972149404647		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 4.143972149404647 | validation: 4.1586042305711475]
	TIME [epoch: 24.8 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136784777136946		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 4.136784777136946 | validation: 4.151797763832869]
	TIME [epoch: 24.8 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137330932246696		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 4.137330932246696 | validation: 4.206110015165589]
	TIME [epoch: 24.8 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.209313049333343		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 4.209313049333343 | validation: 4.150893832322248]
	TIME [epoch: 24.8 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142181878130097		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 4.142181878130097 | validation: 4.164642633357564]
	TIME [epoch: 24.8 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140843586988882		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 4.140843586988882 | validation: 4.158308976952949]
	TIME [epoch: 24.8 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138603010810703		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 4.138603010810703 | validation: 4.145000759920088]
	TIME [epoch: 24.8 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136065712981009		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 4.136065712981009 | validation: 4.147220099169086]
	TIME [epoch: 24.8 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131648272984705		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 4.131648272984705 | validation: 4.149325229614021]
	TIME [epoch: 24.8 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129405296104211		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 4.129405296104211 | validation: 4.147983419217312]
	TIME [epoch: 24.8 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150585244137732		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 4.150585244137732 | validation: 4.147503366951945]
	TIME [epoch: 24.8 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135237946567794		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 4.135237946567794 | validation: 4.154204011294807]
	TIME [epoch: 24.8 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152568602292721		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 4.152568602292721 | validation: 4.144491935955159]
	TIME [epoch: 24.8 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134689191600796		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 4.134689191600796 | validation: 4.144141291630237]
	TIME [epoch: 24.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147520364433815		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 4.147520364433815 | validation: 4.14415259942542]
	TIME [epoch: 24.8 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126837051243125		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 4.126837051243125 | validation: 4.143067027354738]
	TIME [epoch: 24.8 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141309890422271		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 4.141309890422271 | validation: 4.14408699076438]
	TIME [epoch: 24.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136163345266711		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 4.136163345266711 | validation: 4.139961807219479]
	TIME [epoch: 24.8 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136072616844276		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 4.136072616844276 | validation: 4.153170043824357]
	TIME [epoch: 24.8 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144386770075131		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 4.144386770075131 | validation: 4.166577913699314]
	TIME [epoch: 24.8 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.161371275838077		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 4.161371275838077 | validation: 4.190899909500766]
	TIME [epoch: 24.8 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148370442849879		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 4.148370442849879 | validation: 4.139881090008636]
	TIME [epoch: 24.8 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147698915758486		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 4.147698915758486 | validation: 4.167642764126551]
	TIME [epoch: 24.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137956910937211		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 4.137956910937211 | validation: 4.141176561559815]
	TIME [epoch: 24.8 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141775588313198		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 4.141775588313198 | validation: 4.147823268229657]
	TIME [epoch: 24.8 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132869585740194		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 4.132869585740194 | validation: 4.138198721037841]
	TIME [epoch: 24.8 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127534202523054		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 4.127534202523054 | validation: 4.148701894165639]
	TIME [epoch: 24.8 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135649207818557		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 4.135649207818557 | validation: 4.13508402561583]
	TIME [epoch: 24.8 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149568185553257		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 4.149568185553257 | validation: 4.15075401260922]
	TIME [epoch: 24.8 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138253977747592		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 4.138253977747592 | validation: 4.143223541638659]
	TIME [epoch: 24.8 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136791456221141		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 4.136791456221141 | validation: 4.174162544487568]
	TIME [epoch: 24.8 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1521134127350185		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 4.1521134127350185 | validation: 4.202796645648657]
	TIME [epoch: 24.8 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.166878361377337		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 4.166878361377337 | validation: 4.168524972713527]
	TIME [epoch: 24.8 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1396695139933835		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 4.1396695139933835 | validation: 4.145113068849155]
	TIME [epoch: 24.8 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141350151394795		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 4.141350151394795 | validation: 4.1994124692377]
	TIME [epoch: 24.8 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172316808585339		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 4.172316808585339 | validation: 4.205378706916709]
	TIME [epoch: 24.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153302376082404		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 4.153302376082404 | validation: 4.132614504002778]
	TIME [epoch: 24.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130025652181264		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 4.130025652181264 | validation: 4.136471206780543]
	TIME [epoch: 24.8 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13006579205212		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 4.13006579205212 | validation: 4.1470671694403105]
	TIME [epoch: 24.8 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149015012692853		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 4.149015012692853 | validation: 4.1425510295847525]
	TIME [epoch: 24.8 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1354624046388775		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 4.1354624046388775 | validation: 4.138937655328026]
	TIME [epoch: 24.8 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132686820801198		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 4.132686820801198 | validation: 4.15453314326273]
	TIME [epoch: 24.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151739239136361		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 4.151739239136361 | validation: 4.15790287033534]
	TIME [epoch: 24.8 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1331198509598615		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 4.1331198509598615 | validation: 4.142878128404821]
	TIME [epoch: 24.8 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1469439943613295		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 4.1469439943613295 | validation: 4.1598650817591905]
	TIME [epoch: 24.8 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153318458741257		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 4.153318458741257 | validation: 4.1361679266097005]
	TIME [epoch: 24.8 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129450982118932		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 4.129450982118932 | validation: 4.138278503057584]
	TIME [epoch: 24.8 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134956021318838		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 4.134956021318838 | validation: 4.151265924649228]
	TIME [epoch: 24.8 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14382171111265		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 4.14382171111265 | validation: 4.141123260199272]
	TIME [epoch: 24.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12898569243998		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 4.12898569243998 | validation: 4.150854010953108]
	TIME [epoch: 24.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131310815808636		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 4.131310815808636 | validation: 4.154455170479861]
	TIME [epoch: 24.8 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133101861584753		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 4.133101861584753 | validation: 4.163905504458301]
	TIME [epoch: 24.8 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135126073953426		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 4.135126073953426 | validation: 4.143188129381657]
	TIME [epoch: 24.8 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131547367840755		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 4.131547367840755 | validation: 4.1475213486658475]
	TIME [epoch: 24.8 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130465608071034		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 4.130465608071034 | validation: 4.145644421388919]
	TIME [epoch: 24.8 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133798878204117		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 4.133798878204117 | validation: 4.16766257147953]
	TIME [epoch: 24.8 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13776871234033		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 4.13776871234033 | validation: 4.14868444515523]
	TIME [epoch: 24.8 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130511815218539		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 4.130511815218539 | validation: 4.147707091549799]
	TIME [epoch: 24.8 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141106351516333		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 4.141106351516333 | validation: 4.142177540244595]
	TIME [epoch: 24.8 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.15309341953748		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 4.15309341953748 | validation: 4.155310383514468]
	TIME [epoch: 24.8 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132793386986648		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 4.132793386986648 | validation: 4.160021856362752]
	TIME [epoch: 24.8 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137733051600729		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 4.137733051600729 | validation: 4.140701092016778]
	TIME [epoch: 24.8 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.148100646456063		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 4.148100646456063 | validation: 4.157482009916613]
	TIME [epoch: 24.8 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136720297951136		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 4.136720297951136 | validation: 4.141026982131076]
	TIME [epoch: 24.8 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133048655578767		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 4.133048655578767 | validation: 4.164946632562317]
	TIME [epoch: 24.8 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136438595829348		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 4.136438595829348 | validation: 4.153001481972667]
	TIME [epoch: 24.8 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132613318403659		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 4.132613318403659 | validation: 4.150247905271467]
	TIME [epoch: 24.8 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1318362336716685		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 4.1318362336716685 | validation: 4.147869377555069]
	TIME [epoch: 24.8 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134914841984965		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 4.134914841984965 | validation: 4.159749646009694]
	TIME [epoch: 24.8 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134269837009034		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 4.134269837009034 | validation: 4.158179048572899]
	TIME [epoch: 24.8 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.150652036149416		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 4.150652036149416 | validation: 4.143860152660826]
	TIME [epoch: 24.8 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14604592994875		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 4.14604592994875 | validation: 4.172271959852732]
	TIME [epoch: 24.8 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141485055541363		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 4.141485055541363 | validation: 4.131700472660279]
	TIME [epoch: 24.8 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125790414042366		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 4.125790414042366 | validation: 4.155583694417073]
	TIME [epoch: 24.8 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138315742440871		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 4.138315742440871 | validation: 4.150879308692331]
	TIME [epoch: 24.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134887480094077		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 4.134887480094077 | validation: 4.137843125496668]
	TIME [epoch: 24.8 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126951807389581		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 4.126951807389581 | validation: 4.135696296973566]
	TIME [epoch: 24.8 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129601286197462		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 4.129601286197462 | validation: 4.1486820125687265]
	TIME [epoch: 24.8 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141675281346084		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 4.141675281346084 | validation: 4.13594117505054]
	TIME [epoch: 24.8 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129628013452104		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 4.129628013452104 | validation: 4.145685470685462]
	TIME [epoch: 24.8 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138038234549624		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 4.138038234549624 | validation: 4.1740747413519275]
	TIME [epoch: 24.8 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.181824059865779		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 4.181824059865779 | validation: 4.153211414241096]
	TIME [epoch: 24.8 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13882061369738		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 4.13882061369738 | validation: 4.147552372693868]
	TIME [epoch: 24.8 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1344217702351065		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 4.1344217702351065 | validation: 4.142438763999309]
	TIME [epoch: 24.8 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133709153321856		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 4.133709153321856 | validation: 4.152728700668878]
	TIME [epoch: 24.8 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130611258797133		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 4.130611258797133 | validation: 4.146908047462013]
	TIME [epoch: 24.8 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128479543446119		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 4.128479543446119 | validation: 4.169726877728549]
	TIME [epoch: 24.8 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141974508003156		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 4.141974508003156 | validation: 4.149498935185925]
	TIME [epoch: 24.8 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136507458227337		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 4.136507458227337 | validation: 4.149323453194318]
	TIME [epoch: 24.8 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132924920169545		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 4.132924920169545 | validation: 4.167833205821159]
	TIME [epoch: 24.8 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152868199169966		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 4.152868199169966 | validation: 4.143406265370575]
	TIME [epoch: 24.8 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1300116623170755		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 4.1300116623170755 | validation: 4.149260900319214]
	TIME [epoch: 24.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135261184175268		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 4.135261184175268 | validation: 4.139685465218248]
	TIME [epoch: 24.8 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132887799420697		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 4.132887799420697 | validation: 4.1456757516795735]
	TIME [epoch: 24.8 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138524691593428		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 4.138524691593428 | validation: 4.153834818073235]
	TIME [epoch: 24.8 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141982159895953		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 4.141982159895953 | validation: 4.172916468234925]
	TIME [epoch: 24.8 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140430625666729		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 4.140430625666729 | validation: 4.1460396486916835]
	TIME [epoch: 24.8 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133243109227468		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 4.133243109227468 | validation: 4.140473622651358]
	TIME [epoch: 24.8 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134550230042352		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 4.134550230042352 | validation: 4.147715087744398]
	TIME [epoch: 24.8 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137545106768526		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 4.137545106768526 | validation: 4.138409756977684]
	TIME [epoch: 24.8 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.147282140573458		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 4.147282140573458 | validation: 4.1665984453452936]
	TIME [epoch: 24.8 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135532764783086		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 4.135532764783086 | validation: 4.138601722070548]
	TIME [epoch: 24.8 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126764571155515		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 4.126764571155515 | validation: 4.148874789376217]
	TIME [epoch: 24.8 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137674547389517		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 4.137674547389517 | validation: 4.14018979255639]
	TIME [epoch: 24.8 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1389578360575126		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 4.1389578360575126 | validation: 4.169092034692245]
	TIME [epoch: 24.8 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141858894641729		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 4.141858894641729 | validation: 4.143113281890605]
	TIME [epoch: 24.8 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129770908181429		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 4.129770908181429 | validation: 4.147958989158029]
	TIME [epoch: 24.8 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13948040189638		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 4.13948040189638 | validation: 4.1531452961824735]
	TIME [epoch: 24.8 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127810542314573		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 4.127810542314573 | validation: 4.143982888581798]
	TIME [epoch: 24.8 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136780271218762		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 4.136780271218762 | validation: 4.162514695645071]
	TIME [epoch: 24.8 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134513641361516		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 4.134513641361516 | validation: 4.155984137356531]
	TIME [epoch: 24.8 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.156173831360537		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 4.156173831360537 | validation: 4.159070699200762]
	TIME [epoch: 24.8 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133853633700614		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 4.133853633700614 | validation: 4.142107768426902]
	TIME [epoch: 24.8 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129252978459549		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 4.129252978459549 | validation: 4.133815209641555]
	TIME [epoch: 24.8 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129746796647854		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 4.129746796647854 | validation: 4.145407834503701]
	TIME [epoch: 24.8 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129260129116376		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 4.129260129116376 | validation: 4.136639908419791]
	TIME [epoch: 24.8 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12686319327841		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 4.12686319327841 | validation: 4.161652639600233]
	TIME [epoch: 24.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.153485148936042		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 4.153485148936042 | validation: 4.143588051455913]
	TIME [epoch: 24.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133454263348809		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 4.133454263348809 | validation: 4.143864001553266]
	TIME [epoch: 24.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129975127704122		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 4.129975127704122 | validation: 4.132174515907821]
	TIME [epoch: 24.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12930451904578		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 4.12930451904578 | validation: 4.139158365515036]
	TIME [epoch: 24.8 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129862519404119		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 4.129862519404119 | validation: 4.142789727791193]
	TIME [epoch: 24.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134537567669211		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 4.134537567669211 | validation: 4.146401228516013]
	TIME [epoch: 24.8 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130944778184519		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 4.130944778184519 | validation: 4.150433736591985]
	TIME [epoch: 24.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1460095361883464		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 4.1460095361883464 | validation: 4.1528085657229346]
	TIME [epoch: 24.8 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131447659016462		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 4.131447659016462 | validation: 4.151640864416722]
	TIME [epoch: 24.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144345656020018		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 4.144345656020018 | validation: 4.1955711918406235]
	TIME [epoch: 24.8 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145398875342117		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 4.145398875342117 | validation: 4.148242098445411]
	TIME [epoch: 24.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1290584718471575		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 4.1290584718471575 | validation: 4.148304329161378]
	TIME [epoch: 24.8 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128424278307714		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 4.128424278307714 | validation: 4.154285439726033]
	TIME [epoch: 24.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140428874523752		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 4.140428874523752 | validation: 4.136591784067876]
	TIME [epoch: 24.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127761951651523		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 4.127761951651523 | validation: 4.1626269381194865]
	TIME [epoch: 24.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1499266403870685		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 4.1499266403870685 | validation: 4.141606534862793]
	TIME [epoch: 24.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13543612578373		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 4.13543612578373 | validation: 4.165665034788209]
	TIME [epoch: 24.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143687443987915		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 4.143687443987915 | validation: 4.139981733252629]
	TIME [epoch: 24.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139091008774124		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 4.139091008774124 | validation: 4.161471236382402]
	TIME [epoch: 24.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139357261489838		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 4.139357261489838 | validation: 4.144030661252304]
	TIME [epoch: 24.8 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135855491647353		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 4.135855491647353 | validation: 4.142416967533233]
	TIME [epoch: 24.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131934844755211		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 4.131934844755211 | validation: 4.151200325839302]
	TIME [epoch: 24.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129174187098364		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 4.129174187098364 | validation: 4.143836520767082]
	TIME [epoch: 24.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141750378625121		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 4.141750378625121 | validation: 4.176085829615687]
	TIME [epoch: 24.8 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.14381240500848		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 4.14381240500848 | validation: 4.1461773655569285]
	TIME [epoch: 24.8 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140442751597726		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 4.140442751597726 | validation: 4.157548934715112]
	TIME [epoch: 24.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134329349877254		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 4.134329349877254 | validation: 4.15329274210878]
	TIME [epoch: 24.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135001334762946		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 4.135001334762946 | validation: 4.155561140087386]
	TIME [epoch: 24.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13379771423373		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 4.13379771423373 | validation: 4.1571847588655295]
	TIME [epoch: 24.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133011025693041		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 4.133011025693041 | validation: 4.144289143931085]
	TIME [epoch: 24.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129040862918838		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 4.129040862918838 | validation: 4.141326077098937]
	TIME [epoch: 24.8 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135485912259183		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 4.135485912259183 | validation: 4.181715489401434]
	TIME [epoch: 24.8 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1570617476799745		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 4.1570617476799745 | validation: 4.162123794757811]
	TIME [epoch: 24.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.141168542812505		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 4.141168542812505 | validation: 4.141906048789808]
	TIME [epoch: 24.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1492050536552645		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 4.1492050536552645 | validation: 4.1762687690390035]
	TIME [epoch: 24.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159619856543726		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 4.159619856543726 | validation: 4.137388719183357]
	TIME [epoch: 24.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133154463373427		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 4.133154463373427 | validation: 4.147169679292631]
	TIME [epoch: 24.8 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130515580947255		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 4.130515580947255 | validation: 4.144326967987871]
	TIME [epoch: 24.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12938937768537		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 4.12938937768537 | validation: 4.145384147844931]
	TIME [epoch: 24.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134851878127183		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 4.134851878127183 | validation: 4.137579885044464]
	TIME [epoch: 24.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127418548034598		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 4.127418548034598 | validation: 4.143742214122695]
	TIME [epoch: 24.8 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133890965174301		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 4.133890965174301 | validation: 4.149012801000996]
	TIME [epoch: 24.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132198839229803		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 4.132198839229803 | validation: 4.137581927429476]
	TIME [epoch: 24.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13078541156263		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 4.13078541156263 | validation: 4.1369589219696765]
	TIME [epoch: 24.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129810213554973		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 4.129810213554973 | validation: 4.145448123693974]
	TIME [epoch: 24.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127516122536882		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 4.127516122536882 | validation: 4.146911391081383]
	TIME [epoch: 24.8 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128055294820122		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 4.128055294820122 | validation: 4.167875711163011]
	TIME [epoch: 24.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.155279000460352		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 4.155279000460352 | validation: 4.141805689874549]
	TIME [epoch: 24.8 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132199249345664		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 4.132199249345664 | validation: 4.146177682638555]
	TIME [epoch: 24.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.144325746327561		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 4.144325746327561 | validation: 4.1488556565549395]
	TIME [epoch: 24.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130073143007287		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 4.130073143007287 | validation: 4.153103195556019]
	TIME [epoch: 24.8 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137806580212329		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 4.137806580212329 | validation: 4.170068140196388]
	TIME [epoch: 24.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140798963539125		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 4.140798963539125 | validation: 4.140827864124939]
	TIME [epoch: 24.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125916379653775		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 4.125916379653775 | validation: 4.142792375779614]
	TIME [epoch: 24.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131857288645288		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 4.131857288645288 | validation: 4.141005212444204]
	TIME [epoch: 24.8 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1304007114638095		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 4.1304007114638095 | validation: 4.150416043899884]
	TIME [epoch: 24.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137013015067593		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 4.137013015067593 | validation: 4.155668881822821]
	TIME [epoch: 24.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1268304129672515		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 4.1268304129672515 | validation: 4.152052724029216]
	TIME [epoch: 24.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129533722166771		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 4.129533722166771 | validation: 4.1394439051141605]
	TIME [epoch: 24.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1335306238845115		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 4.1335306238845115 | validation: 4.140859314766784]
	TIME [epoch: 24.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131212474638587		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 4.131212474638587 | validation: 4.145495134317395]
	TIME [epoch: 24.8 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134361011214784		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 4.134361011214784 | validation: 4.140894101810047]
	TIME [epoch: 24.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135014203395379		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 4.135014203395379 | validation: 4.14741547070859]
	TIME [epoch: 24.8 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127427211550929		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 4.127427211550929 | validation: 4.145999026834966]
	TIME [epoch: 24.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142581832980321		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 4.142581832980321 | validation: 4.1556617435316365]
	TIME [epoch: 24.8 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130824952433128		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 4.130824952433128 | validation: 4.1408106406126]
	TIME [epoch: 24.8 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125266470260293		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 4.125266470260293 | validation: 4.138225810858406]
	TIME [epoch: 24.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12974889361215		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 4.12974889361215 | validation: 4.150738144396347]
	TIME [epoch: 24.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.138341746871207		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 4.138341746871207 | validation: 4.140893530770449]
	TIME [epoch: 24.8 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129648719759145		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 4.129648719759145 | validation: 4.140553229957422]
	TIME [epoch: 24.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12143673793814		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 4.12143673793814 | validation: 4.144608342206985]
	TIME [epoch: 24.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126112418742034		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 4.126112418742034 | validation: 4.142434916138983]
	TIME [epoch: 24.8 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13018013306805		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 4.13018013306805 | validation: 4.152051449217218]
	TIME [epoch: 24.8 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137019193845628		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 4.137019193845628 | validation: 4.153765853638961]
	TIME [epoch: 24.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1341778090717325		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 4.1341778090717325 | validation: 4.144788880823576]
	TIME [epoch: 24.8 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128597568324756		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 4.128597568324756 | validation: 4.1444944655538025]
	TIME [epoch: 24.8 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12822565194825		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 4.12822565194825 | validation: 4.154081890426069]
	TIME [epoch: 24.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137481501423588		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 4.137481501423588 | validation: 4.151570672348812]
	TIME [epoch: 24.8 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128345839970709		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 4.128345839970709 | validation: 4.15302417663362]
	TIME [epoch: 24.8 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142171349765324		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 4.142171349765324 | validation: 4.134066451113538]
	TIME [epoch: 24.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13007571370283		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 4.13007571370283 | validation: 4.144391357988794]
	TIME [epoch: 24.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130597685537718		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 4.130597685537718 | validation: 4.136926839377713]
	TIME [epoch: 24.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123779255832196		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 4.123779255832196 | validation: 4.145266996095204]
	TIME [epoch: 24.8 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125835558231542		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 4.125835558231542 | validation: 4.136861948717365]
	TIME [epoch: 24.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1243752029977845		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 4.1243752029977845 | validation: 4.1595009648625725]
	TIME [epoch: 24.8 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146857747701064		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 4.146857747701064 | validation: 4.137455414287663]
	TIME [epoch: 24.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131079759494987		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 4.131079759494987 | validation: 4.133584758716074]
	TIME [epoch: 24.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128138969699686		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 4.128138969699686 | validation: 4.140015914793509]
	TIME [epoch: 24.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1255136625565		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 4.1255136625565 | validation: 4.147730786168952]
	TIME [epoch: 24.8 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132749313514753		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 4.132749313514753 | validation: 4.144012009849453]
	TIME [epoch: 24.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13350933338398		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 4.13350933338398 | validation: 4.14344421018928]
	TIME [epoch: 24.8 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131424746319606		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 4.131424746319606 | validation: 4.138010699052658]
	TIME [epoch: 24.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134167112342683		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 4.134167112342683 | validation: 4.174347616430185]
	TIME [epoch: 24.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.151704183102528		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 4.151704183102528 | validation: 4.136705325375574]
	TIME [epoch: 24.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133137995418341		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 4.133137995418341 | validation: 4.140961125228247]
	TIME [epoch: 24.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129186559975576		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 4.129186559975576 | validation: 4.143124260014765]
	TIME [epoch: 24.8 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127349557364268		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 4.127349557364268 | validation: 4.143137699981162]
	TIME [epoch: 24.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1278694999611965		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 4.1278694999611965 | validation: 4.144362296642644]
	TIME [epoch: 24.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127951233018216		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 4.127951233018216 | validation: 4.1365562136089675]
	TIME [epoch: 24.8 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122522770776848		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 4.122522770776848 | validation: 4.135096590328451]
	TIME [epoch: 24.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129700508804966		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 4.129700508804966 | validation: 4.153528100648777]
	TIME [epoch: 24.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134705668759605		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 4.134705668759605 | validation: 4.141194265466372]
	TIME [epoch: 24.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128885642595938		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 4.128885642595938 | validation: 4.132252402917506]
	TIME [epoch: 24.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127735879413332		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 4.127735879413332 | validation: 4.146899307519394]
	TIME [epoch: 24.8 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135085729550877		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 4.135085729550877 | validation: 4.143389282195799]
	TIME [epoch: 24.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132174402197436		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 4.132174402197436 | validation: 4.1392981211503646]
	TIME [epoch: 24.8 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1273664964697705		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 4.1273664964697705 | validation: 4.152084959665018]
	TIME [epoch: 24.8 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.137855538181244		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 4.137855538181244 | validation: 4.14379314037391]
	TIME [epoch: 24.8 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.135535944997023		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 4.135535944997023 | validation: 4.1620641968089584]
	TIME [epoch: 24.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.142646127604127		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 4.142646127604127 | validation: 4.1457777696888325]
	TIME [epoch: 24.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134736789376509		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 4.134736789376509 | validation: 4.164934118182868]
	TIME [epoch: 24.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1320814112758715		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 4.1320814112758715 | validation: 4.137041561469131]
	TIME [epoch: 24.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127573575585357		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 4.127573575585357 | validation: 4.146715168803826]
	TIME [epoch: 24.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134521111278634		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 4.134521111278634 | validation: 4.139107077255519]
	TIME [epoch: 24.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136322226472087		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 4.136322226472087 | validation: 4.151659461778133]
	TIME [epoch: 24.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125058652194235		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 4.125058652194235 | validation: 4.147526979376598]
	TIME [epoch: 24.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129261134156644		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 4.129261134156644 | validation: 4.153680290160427]
	TIME [epoch: 24.8 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125893911252328		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 4.125893911252328 | validation: 4.14430822732575]
	TIME [epoch: 24.8 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12605867730738		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 4.12605867730738 | validation: 4.143751497317949]
	TIME [epoch: 24.8 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12393465548902		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 4.12393465548902 | validation: 4.142509012982486]
	TIME [epoch: 24.8 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132499695875566		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 4.132499695875566 | validation: 4.149785436103537]
	TIME [epoch: 24.8 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13221452650548		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 4.13221452650548 | validation: 4.141793280130814]
	TIME [epoch: 24.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130746644831052		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 4.130746644831052 | validation: 4.1484915016578965]
	TIME [epoch: 24.8 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128736950801072		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 4.128736950801072 | validation: 4.138784426134526]
	TIME [epoch: 24.8 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129623053946544		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 4.129623053946544 | validation: 4.160820758407186]
	TIME [epoch: 24.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.145765000416587		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 4.145765000416587 | validation: 4.151860750392301]
	TIME [epoch: 24.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134797149790655		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 4.134797149790655 | validation: 4.146704622951218]
	TIME [epoch: 24.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1323099752299175		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 4.1323099752299175 | validation: 4.150115180839282]
	TIME [epoch: 24.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1360495653429235		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 4.1360495653429235 | validation: 4.146756023380794]
	TIME [epoch: 24.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123448882781235		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 4.123448882781235 | validation: 4.139903961132402]
	TIME [epoch: 24.8 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12378888893874		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 4.12378888893874 | validation: 4.141581355727957]
	TIME [epoch: 24.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129266863712049		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 4.129266863712049 | validation: 4.142750942010704]
	TIME [epoch: 24.8 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131428649643135		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 4.131428649643135 | validation: 4.141446714552651]
	TIME [epoch: 24.8 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134848812755953		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 4.134848812755953 | validation: 4.145767502482767]
	TIME [epoch: 24.8 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125702798032861		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 4.125702798032861 | validation: 4.146968986021017]
	TIME [epoch: 24.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132107485566695		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 4.132107485566695 | validation: 4.140203083192176]
	TIME [epoch: 24.8 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129025700386626		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 4.129025700386626 | validation: 4.1506668705756935]
	TIME [epoch: 24.8 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1351856266474885		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 4.1351856266474885 | validation: 4.1422019464482025]
	TIME [epoch: 24.8 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131807017439747		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 4.131807017439747 | validation: 4.139246472905032]
	TIME [epoch: 24.8 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124599331727439		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 4.124599331727439 | validation: 4.135941544235762]
	TIME [epoch: 24.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125864593132527		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 4.125864593132527 | validation: 4.143422087774458]
	TIME [epoch: 24.8 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1257227538347		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 4.1257227538347 | validation: 4.137937863193052]
	TIME [epoch: 24.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127144285224973		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 4.127144285224973 | validation: 4.139011647080328]
	TIME [epoch: 24.8 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1263719132946965		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 4.1263719132946965 | validation: 4.1427945264724775]
	TIME [epoch: 24.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136913078107862		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 4.136913078107862 | validation: 4.1564716460487485]
	TIME [epoch: 24.8 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134245694958357		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 4.134245694958357 | validation: 4.141309717986314]
	TIME [epoch: 24.8 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12828810401094		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 4.12828810401094 | validation: 4.15106993640084]
	TIME [epoch: 24.8 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1302284411007815		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 4.1302284411007815 | validation: 4.14374107816372]
	TIME [epoch: 24.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134518124933157		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 4.134518124933157 | validation: 4.144208862964568]
	TIME [epoch: 24.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128393540195962		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 4.128393540195962 | validation: 4.142407853330308]
	TIME [epoch: 24.8 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125908168381514		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 4.125908168381514 | validation: 4.139553374926258]
	TIME [epoch: 24.8 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126456624522745		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 4.126456624522745 | validation: 4.141913896598561]
	TIME [epoch: 24.8 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136729591931857		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 4.136729591931857 | validation: 4.146794228645332]
	TIME [epoch: 24.8 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132372843481068		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 4.132372843481068 | validation: 4.142694279911088]
	TIME [epoch: 24.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130695981891663		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 4.130695981891663 | validation: 4.146887925284756]
	TIME [epoch: 24.8 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1325645697984825		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 4.1325645697984825 | validation: 4.135836299385625]
	TIME [epoch: 24.8 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130100930319977		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 4.130100930319977 | validation: 4.144210005845036]
	TIME [epoch: 24.8 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.143708555428432		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 4.143708555428432 | validation: 4.155021714182356]
	TIME [epoch: 24.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12781376292984		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 4.12781376292984 | validation: 4.14324894465552]
	TIME [epoch: 24.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132176543581277		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 4.132176543581277 | validation: 4.141636912520981]
	TIME [epoch: 24.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12518130270842		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 4.12518130270842 | validation: 4.139124389229077]
	TIME [epoch: 24.8 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124381687121604		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 4.124381687121604 | validation: 4.135593407411163]
	TIME [epoch: 24.8 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125940782541827		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 4.125940782541827 | validation: 4.141308030081716]
	TIME [epoch: 24.8 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127441339737563		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 4.127441339737563 | validation: 4.145788338886532]
	TIME [epoch: 24.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128704232205588		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 4.128704232205588 | validation: 4.13921936272173]
	TIME [epoch: 24.8 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125327578160759		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 4.125327578160759 | validation: 4.139447554418897]
	TIME [epoch: 24.8 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12879632567064		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 4.12879632567064 | validation: 4.137457295737006]
	TIME [epoch: 24.8 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1270548180442495		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 4.1270548180442495 | validation: 4.151771322994559]
	TIME [epoch: 24.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128488345658471		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 4.128488345658471 | validation: 4.134253477866348]
	TIME [epoch: 24.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134137887337939		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 4.134137887337939 | validation: 4.1440946545753246]
	TIME [epoch: 24.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128361535542995		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 4.128361535542995 | validation: 4.14743706650843]
	TIME [epoch: 24.8 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129391610715237		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 4.129391610715237 | validation: 4.140379632606876]
	TIME [epoch: 24.8 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122726940815933		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 4.122726940815933 | validation: 4.136542904871041]
	TIME [epoch: 24.8 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124193380596104		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 4.124193380596104 | validation: 4.137764923947655]
	TIME [epoch: 24.8 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128314227905658		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 4.128314227905658 | validation: 4.146672194310323]
	TIME [epoch: 24.8 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139334693702716		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 4.139334693702716 | validation: 4.141286954305515]
	TIME [epoch: 24.8 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131720981674391		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 4.131720981674391 | validation: 4.149425418584057]
	TIME [epoch: 24.8 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131262647494763		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 4.131262647494763 | validation: 4.137249491070033]
	TIME [epoch: 24.8 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128698818489708		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 4.128698818489708 | validation: 4.141011607529223]
	TIME [epoch: 24.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130133332816378		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 4.130133332816378 | validation: 4.143178215586884]
	TIME [epoch: 24.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130381172959112		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 4.130381172959112 | validation: 4.130823476585278]
	TIME [epoch: 24.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240310_060415/states/model_tr_study206_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123969052455689		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 4.123969052455689 | validation: 4.131755264486424]
	TIME [epoch: 24.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122221858980223		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 4.122221858980223 | validation: 4.137286364587398]
	TIME [epoch: 24.8 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.140044332211696		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 4.140044332211696 | validation: 4.154528457784473]
	TIME [epoch: 24.8 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1328972523039305		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 4.1328972523039305 | validation: 4.139984841000385]
	TIME [epoch: 24.8 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130400290567628		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 4.130400290567628 | validation: 4.146009036449272]
	TIME [epoch: 24.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134497722037402		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 4.134497722037402 | validation: 4.137166303350737]
	TIME [epoch: 24.8 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122577677186973		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 4.122577677186973 | validation: 4.1363707014415745]
	TIME [epoch: 24.8 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125447522961606		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 4.125447522961606 | validation: 4.132345383552632]
	TIME [epoch: 24.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12981891421279		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 4.12981891421279 | validation: 4.1387804058208975]
	TIME [epoch: 24.8 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1228771422803945		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 4.1228771422803945 | validation: 4.144524440304577]
	TIME [epoch: 24.8 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126810855875255		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 4.126810855875255 | validation: 4.140711637608395]
	TIME [epoch: 24.8 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128623719851749		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 4.128623719851749 | validation: 4.140658639964847]
	TIME [epoch: 24.8 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.136224236273764		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 4.136224236273764 | validation: 4.142387186637129]
	TIME [epoch: 24.8 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131829151886867		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 4.131829151886867 | validation: 4.137594482341131]
	TIME [epoch: 24.8 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13207202728383		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 4.13207202728383 | validation: 4.134127814265834]
	TIME [epoch: 24.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1251292332991145		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 4.1251292332991145 | validation: 4.13740496094452]
	TIME [epoch: 24.8 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124831180806874		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 4.124831180806874 | validation: 4.138119073929895]
	TIME [epoch: 24.8 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1243242688976895		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 4.1243242688976895 | validation: 4.139618490773689]
	TIME [epoch: 24.8 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133502629967769		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 4.133502629967769 | validation: 4.135987469682596]
	TIME [epoch: 24.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124051577970071		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 4.124051577970071 | validation: 4.132286152008882]
	TIME [epoch: 24.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125368693359007		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 4.125368693359007 | validation: 4.133540840197727]
	TIME [epoch: 24.8 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126846259572586		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 4.126846259572586 | validation: 4.134067282757347]
	TIME [epoch: 24.8 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126589695867666		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 4.126589695867666 | validation: 4.140888514992581]
	TIME [epoch: 24.8 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129371016351058		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 4.129371016351058 | validation: 4.140051635021079]
	TIME [epoch: 24.8 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1285340558125085		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 4.1285340558125085 | validation: 4.1486134602115445]
	TIME [epoch: 24.8 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125083975040878		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 4.125083975040878 | validation: 4.1514576010816215]
	TIME [epoch: 24.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12733791436529		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 4.12733791436529 | validation: 4.144734640434803]
	TIME [epoch: 24.8 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.139251515441738		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 4.139251515441738 | validation: 4.149815273650872]
	TIME [epoch: 24.8 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13032358083211		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 4.13032358083211 | validation: 4.138342254904362]
	TIME [epoch: 24.8 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124477743352849		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 4.124477743352849 | validation: 4.133593767550361]
	TIME [epoch: 24.8 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1249425496818475		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 4.1249425496818475 | validation: 4.141438997062706]
	TIME [epoch: 24.8 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127816212734192		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 4.127816212734192 | validation: 4.145513127055983]
	TIME [epoch: 24.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123767863372264		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 4.123767863372264 | validation: 4.140216273696224]
	TIME [epoch: 24.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133709070197186		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 4.133709070197186 | validation: 4.135941394183906]
	TIME [epoch: 24.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124736482645517		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 4.124736482645517 | validation: 4.14128480943232]
	TIME [epoch: 24.8 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129742027395712		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 4.129742027395712 | validation: 4.154739120233912]
	TIME [epoch: 24.8 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126308359706424		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 4.126308359706424 | validation: 4.141757034650031]
	TIME [epoch: 24.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126111810634014		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 4.126111810634014 | validation: 4.134190621617387]
	TIME [epoch: 24.8 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127604345927942		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 4.127604345927942 | validation: 4.137633721373042]
	TIME [epoch: 24.8 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128008578806031		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 4.128008578806031 | validation: 4.135146266768646]
	TIME [epoch: 24.8 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131295892777058		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 4.131295892777058 | validation: 4.146744828337059]
	TIME [epoch: 24.8 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131196149782305		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 4.131196149782305 | validation: 4.1335379977522315]
	TIME [epoch: 24.8 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123203056090277		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 4.123203056090277 | validation: 4.144165779519108]
	TIME [epoch: 24.8 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13138292155804		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 4.13138292155804 | validation: 4.148623740374473]
	TIME [epoch: 24.8 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13067108689922		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 4.13067108689922 | validation: 4.140391805732165]
	TIME [epoch: 24.8 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124022946322118		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 4.124022946322118 | validation: 4.13348189943758]
	TIME [epoch: 24.8 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126324257799448		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 4.126324257799448 | validation: 4.133783851153543]
	TIME [epoch: 24.8 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122773870617856		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 4.122773870617856 | validation: 4.136917482686796]
	TIME [epoch: 24.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.130482113915601		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 4.130482113915601 | validation: 4.134261537766576]
	TIME [epoch: 24.8 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129679905141133		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 4.129679905141133 | validation: 4.13744018958448]
	TIME [epoch: 24.8 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125897272754307		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 4.125897272754307 | validation: 4.152059953989455]
	TIME [epoch: 24.8 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132609263712121		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 4.132609263712121 | validation: 4.142909878319398]
	TIME [epoch: 24.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127315334657306		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 4.127315334657306 | validation: 4.138005621937312]
	TIME [epoch: 24.8 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126865241930543		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 4.126865241930543 | validation: 4.140932146160925]
	TIME [epoch: 24.8 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.120781181009963		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 4.120781181009963 | validation: 4.130908013694467]
	TIME [epoch: 24.8 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123132317508035		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 4.123132317508035 | validation: 4.14318508515821]
	TIME [epoch: 24.8 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1291467585649535		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 4.1291467585649535 | validation: 4.145090647608359]
	TIME [epoch: 24.8 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124913976517771		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 4.124913976517771 | validation: 4.142308341595912]
	TIME [epoch: 24.8 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124032550369488		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 4.124032550369488 | validation: 4.140410161767883]
	TIME [epoch: 24.8 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126427725161034		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 4.126427725161034 | validation: 4.14275950916713]
	TIME [epoch: 24.8 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126985132434841		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 4.126985132434841 | validation: 4.143501628434049]
	TIME [epoch: 24.8 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122647164292817		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 4.122647164292817 | validation: 4.140989936718255]
	TIME [epoch: 24.8 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133235080788543		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 4.133235080788543 | validation: 4.146343716965058]
	TIME [epoch: 24.8 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1290532137408835		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 4.1290532137408835 | validation: 4.139417745354877]
	TIME [epoch: 24.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125961675867466		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 4.125961675867466 | validation: 4.1448868616942045]
	TIME [epoch: 24.8 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125401666882547		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 4.125401666882547 | validation: 4.141964822476557]
	TIME [epoch: 24.8 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128546467260745		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 4.128546467260745 | validation: 4.133257225590463]
	TIME [epoch: 24.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12331012856758		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 4.12331012856758 | validation: 4.141899031669007]
	TIME [epoch: 24.8 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122724339271494		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 4.122724339271494 | validation: 4.137781186822544]
	TIME [epoch: 24.8 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124585750458763		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 4.124585750458763 | validation: 4.142839505450799]
	TIME [epoch: 24.8 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131631866687406		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 4.131631866687406 | validation: 4.137312617893811]
	TIME [epoch: 24.8 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127521282752172		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 4.127521282752172 | validation: 4.154283634095736]
	TIME [epoch: 24.8 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134805756196423		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 4.134805756196423 | validation: 4.1450766442426135]
	TIME [epoch: 24.8 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126688948913037		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 4.126688948913037 | validation: 4.138357573394988]
	TIME [epoch: 24.8 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128447222574253		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 4.128447222574253 | validation: 4.137927970850134]
	TIME [epoch: 24.8 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1249352541079505		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 4.1249352541079505 | validation: 4.1516110289300405]
	TIME [epoch: 24.8 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1380116541848455		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 4.1380116541848455 | validation: 4.1423370803981285]
	TIME [epoch: 24.8 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1245776054250225		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 4.1245776054250225 | validation: 4.140095257820825]
	TIME [epoch: 24.8 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124805181376491		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 4.124805181376491 | validation: 4.138075942438007]
	TIME [epoch: 24.8 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12579794003609		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 4.12579794003609 | validation: 4.140837076584628]
	TIME [epoch: 24.8 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1266334604137365		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 4.1266334604137365 | validation: 4.140922961806455]
	TIME [epoch: 24.8 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12644402870749		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 4.12644402870749 | validation: 4.141789209137829]
	TIME [epoch: 24.8 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133048724847925		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 4.133048724847925 | validation: 4.142473322303799]
	TIME [epoch: 24.8 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127376809434249		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 4.127376809434249 | validation: 4.141169043581831]
	TIME [epoch: 24.8 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.133811507342108		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 4.133811507342108 | validation: 4.149618972820942]
	TIME [epoch: 24.8 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128018030300696		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 4.128018030300696 | validation: 4.146946699783558]
	TIME [epoch: 24.8 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128379392789519		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 4.128379392789519 | validation: 4.135982709984575]
	TIME [epoch: 24.8 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126391563440533		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 4.126391563440533 | validation: 4.139231110278428]
	TIME [epoch: 24.8 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123665068257014		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 4.123665068257014 | validation: 4.140886532463494]
	TIME [epoch: 24.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12582039054771		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 4.12582039054771 | validation: 4.13963222302914]
	TIME [epoch: 24.8 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.121332202519882		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 4.121332202519882 | validation: 4.139329082231364]
	TIME [epoch: 24.8 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12568287959271		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 4.12568287959271 | validation: 4.141182138320828]
	TIME [epoch: 24.8 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122679166327187		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 4.122679166327187 | validation: 4.140585413305615]
	TIME [epoch: 24.8 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124094022032807		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 4.124094022032807 | validation: 4.14047175872456]
	TIME [epoch: 24.8 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131155346865929		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 4.131155346865929 | validation: 4.136133476289867]
	TIME [epoch: 24.8 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122667423983805		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 4.122667423983805 | validation: 4.144719806389335]
	TIME [epoch: 24.8 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126460088934375		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 4.126460088934375 | validation: 4.139167410100503]
	TIME [epoch: 24.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128951091844365		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 4.128951091844365 | validation: 4.136569262364427]
	TIME [epoch: 24.8 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.128339958071567		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 4.128339958071567 | validation: 4.138727149037825]
	TIME [epoch: 24.8 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.12551532977956		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 4.12551532977956 | validation: 4.139303118527855]
	TIME [epoch: 24.8 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.121895942790167		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 4.121895942790167 | validation: 4.135250014501313]
	TIME [epoch: 24.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124471942959527		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 4.124471942959527 | validation: 4.135620893655971]
	TIME [epoch: 24.8 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124161310795805		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 4.124161310795805 | validation: 4.1409816171307]
	TIME [epoch: 24.8 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.134829012814385		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 4.134829012814385 | validation: 4.145293568096298]
	TIME [epoch: 24.8 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1299128535217955		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 4.1299128535217955 | validation: 4.134182401103279]
	TIME [epoch: 24.8 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1230488631457805		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 4.1230488631457805 | validation: 4.138370968489724]
	TIME [epoch: 24.8 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122118686546733		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 4.122118686546733 | validation: 4.141142560762916]
	TIME [epoch: 24.8 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.132022032739175		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 4.132022032739175 | validation: 4.137432363822044]
	TIME [epoch: 24.8 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13018444118955		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 4.13018444118955 | validation: 4.15519570591268]
	TIME [epoch: 24.8 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1380600431240335		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 4.1380600431240335 | validation: 4.151687605880926]
	TIME [epoch: 24.8 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125743725268801		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 4.125743725268801 | validation: 4.137540878530108]
	TIME [epoch: 24.8 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123738966737065		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 4.123738966737065 | validation: 4.136656558143507]
	TIME [epoch: 24.8 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126939963323782		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 4.126939963323782 | validation: 4.1462209431271]
	TIME [epoch: 24.8 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13085707513807		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 4.13085707513807 | validation: 4.141868300549188]
	TIME [epoch: 24.8 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.129221186207263		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 4.129221186207263 | validation: 4.131508751186713]
	TIME [epoch: 24.8 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1244070479279795		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 4.1244070479279795 | validation: 4.141849259445333]
	TIME [epoch: 24.8 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.131139364487183		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 4.131139364487183 | validation: 4.141979912634817]
	TIME [epoch: 24.8 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124515607250533		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 4.124515607250533 | validation: 4.140891284676647]
	TIME [epoch: 24.8 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.120451546506521		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 4.120451546506521 | validation: 4.139719257744366]
	TIME [epoch: 24.8 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.122098837365316		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 4.122098837365316 | validation: 4.137185729975374]
	TIME [epoch: 24.8 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126049054319013		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 4.126049054319013 | validation: 4.13904532544362]
	TIME [epoch: 24.8 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124008074076068		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 4.124008074076068 | validation: 4.137062048846726]
	TIME [epoch: 24.8 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125017022964775		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 4.125017022964775 | validation: 4.133439731187653]
	TIME [epoch: 24.8 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1277847433070365		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 4.1277847433070365 | validation: 4.140671452346248]
	TIME [epoch: 24.8 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.124316294782788		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 4.124316294782788 | validation: 4.13683798190229]
	TIME [epoch: 24.8 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126221486596928		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 4.126221486596928 | validation: 4.136174128765857]
	TIME [epoch: 24.8 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.126101043358071		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 4.126101043358071 | validation: 4.136572015849956]
	TIME [epoch: 24.8 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1242657455137754		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 4.1242657455137754 | validation: 4.135940812230813]
	TIME [epoch: 24.8 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127698608135139		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 4.127698608135139 | validation: 4.142771565250659]
	TIME [epoch: 24.8 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.127264220967681		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 4.127264220967681 | validation: 4.132881764205183]
	TIME [epoch: 24.8 sec]
EPOCH 1313/2000:
	Training over batches...
