Args:
Namespace(name='model_tr_study205', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5', training_data='data/transition_rate_studies/tr_study205/tr_study205_training/r5', validation_data='data/transition_rate_studies/tr_study205/tr_study205_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1926777637

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 13.02820526329432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 13.02820526329432 | validation: 13.495749328592847]
	TIME [epoch: 68.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.240173323623486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.240173323623486 | validation: 11.214925261058657]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.592740621192366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.592740621192366 | validation: 12.051945865879643]
	TIME [epoch: 10.2 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.312181723898757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.312181723898757 | validation: 11.113277175303802]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.44727246967244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.44727246967244 | validation: 10.538409483385871]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.8772377775925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.8772377775925 | validation: 7.880481747263472]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.051430838154056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.051430838154056 | validation: 10.193082897114321]
	TIME [epoch: 10.2 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.233976577626308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.233976577626308 | validation: 7.890276859475399]
	TIME [epoch: 10.2 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.085322855215676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.085322855215676 | validation: 8.71998716870633]
	TIME [epoch: 10.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.816329549160793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.816329549160793 | validation: 7.345249428905709]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.598352356618822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.598352356618822 | validation: 8.164580911424396]
	TIME [epoch: 10.2 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.118557587624354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.118557587624354 | validation: 9.82805447102978]
	TIME [epoch: 10.2 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.708279811783404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.708279811783404 | validation: 7.7207350661162994]
	TIME [epoch: 10.2 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.295585827665525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.295585827665525 | validation: 10.383373992589878]
	TIME [epoch: 10.2 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.009142197733926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.009142197733926 | validation: 8.58394989393868]
	TIME [epoch: 10.2 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.033555466371203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.033555466371203 | validation: 7.248221584880765]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.140030202633247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.140030202633247 | validation: 7.429668128525747]
	TIME [epoch: 10.2 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.017240726326435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.017240726326435 | validation: 9.264926059650438]
	TIME [epoch: 10.2 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.368997566073688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.368997566073688 | validation: 7.096642741154532]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.012790905270418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.012790905270418 | validation: 10.025745017680174]
	TIME [epoch: 10.2 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.49590245318949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.49590245318949 | validation: 7.0574106474635325]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.64178877761087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.64178877761087 | validation: 6.893409142742457]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.987229550335066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.987229550335066 | validation: 7.089551663401046]
	TIME [epoch: 10.2 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.420715107036504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.420715107036504 | validation: 8.904414543924068]
	TIME [epoch: 10.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.837002815379982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.837002815379982 | validation: 9.152081205298057]
	TIME [epoch: 10.2 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.062711216078778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.062711216078778 | validation: 6.677064986420255]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.37177818973759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.37177818973759 | validation: 6.762756621438045]
	TIME [epoch: 10.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.278898663609702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.278898663609702 | validation: 10.328390805129347]
	TIME [epoch: 10.2 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.502730925889963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.502730925889963 | validation: 7.760629086293391]
	TIME [epoch: 10.2 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.362876396434396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.362876396434396 | validation: 6.972016883965671]
	TIME [epoch: 10.2 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.790259243866494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.790259243866494 | validation: 8.216618686004399]
	TIME [epoch: 10.2 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.707716398337032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.707716398337032 | validation: 7.527867468098557]
	TIME [epoch: 10.2 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.760629734709719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.760629734709719 | validation: 6.558036120977038]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.447218338134723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.447218338134723 | validation: 8.334770445290086]
	TIME [epoch: 10.2 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.041060866372755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.041060866372755 | validation: 9.913907209268965]
	TIME [epoch: 10.2 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.112660200582273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.112660200582273 | validation: 5.798155193801676]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.380075882190998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.380075882190998 | validation: 6.227976141063888]
	TIME [epoch: 10.2 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.064254874597008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.064254874597008 | validation: 5.831466131364355]
	TIME [epoch: 10.2 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.389050236354858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.389050236354858 | validation: 7.169384586084816]
	TIME [epoch: 10.2 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.662591505675772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.662591505675772 | validation: 7.57758132559761]
	TIME [epoch: 10.2 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.530219827655051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.530219827655051 | validation: 5.088261830269176]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.309986208901076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.309986208901076 | validation: 8.172979242010534]
	TIME [epoch: 10.2 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.925067855016235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.925067855016235 | validation: 9.110896496123353]
	TIME [epoch: 10.2 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.018460914349985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.018460914349985 | validation: 8.541739464745548]
	TIME [epoch: 10.2 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.685907047529286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.685907047529286 | validation: 5.490294292707538]
	TIME [epoch: 10.2 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.779582673790641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.779582673790641 | validation: 8.979463708449792]
	TIME [epoch: 10.2 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.818212989147035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.818212989147035 | validation: 8.038758791324623]
	TIME [epoch: 10.2 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.479444550242169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.479444550242169 | validation: 9.614172509518886]
	TIME [epoch: 10.2 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.591096026891018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.591096026891018 | validation: 5.812342740060274]
	TIME [epoch: 10.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.342273245647986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.342273245647986 | validation: 4.267348419670878]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.855267606481944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.855267606481944 | validation: 6.143306500573161]
	TIME [epoch: 10.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.737877195028487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.737877195028487 | validation: 5.612640342323844]
	TIME [epoch: 10.2 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.689088093483744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.689088093483744 | validation: 6.261958247643319]
	TIME [epoch: 10.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.762609171395527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.762609171395527 | validation: 5.361839340193917]
	TIME [epoch: 10.2 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.684970052325997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.684970052325997 | validation: 5.786954598046776]
	TIME [epoch: 10.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.730291053132614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.730291053132614 | validation: 4.705855384975358]
	TIME [epoch: 10.2 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.529320074279788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.529320074279788 | validation: 5.617049262356077]
	TIME [epoch: 10.2 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5973631008046265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5973631008046265 | validation: 4.816341325690557]
	TIME [epoch: 10.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.56598125093722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.56598125093722 | validation: 5.805666968334343]
	TIME [epoch: 10.2 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.577450566555275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.577450566555275 | validation: 5.045503166605548]
	TIME [epoch: 10.2 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.609436981838493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.609436981838493 | validation: 5.30884503427689]
	TIME [epoch: 10.2 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.484390760523983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.484390760523983 | validation: 4.510438191562561]
	TIME [epoch: 10.2 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.407767013177612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.407767013177612 | validation: 6.001759060615225]
	TIME [epoch: 10.2 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.37961029139902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.37961029139902 | validation: 5.443011754112063]
	TIME [epoch: 10.2 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.570982700570807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.570982700570807 | validation: 5.896328596686373]
	TIME [epoch: 10.2 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.377760458382284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.377760458382284 | validation: 4.968977193320456]
	TIME [epoch: 10.2 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.339135032444598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.339135032444598 | validation: 5.57768977380478]
	TIME [epoch: 10.2 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260602144119199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.260602144119199 | validation: 4.796150987695783]
	TIME [epoch: 10.2 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.446556336985517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.446556336985517 | validation: 4.682559001698524]
	TIME [epoch: 10.2 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265812402529867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.265812402529867 | validation: 5.059935391073767]
	TIME [epoch: 10.2 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177067605126254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.177067605126254 | validation: 5.423014437872962]
	TIME [epoch: 10.2 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.280312372448227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.280312372448227 | validation: 4.676189014401722]
	TIME [epoch: 10.2 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.126882537876321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.126882537876321 | validation: 6.386517570783469]
	TIME [epoch: 10.2 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241384255131755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.241384255131755 | validation: 5.258114598883716]
	TIME [epoch: 10.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192984393582249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.192984393582249 | validation: 5.35712947621117]
	TIME [epoch: 10.2 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.224563242886801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.224563242886801 | validation: 4.456395405477284]
	TIME [epoch: 10.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.107190461426474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.107190461426474 | validation: 5.784619177290965]
	TIME [epoch: 10.2 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.122646821491827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.122646821491827 | validation: 5.166305956480605]
	TIME [epoch: 10.2 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13916530266561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.13916530266561 | validation: 4.976312060006194]
	TIME [epoch: 10.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.303604092415351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.303604092415351 | validation: 4.114559169882426]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.046332321827299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.046332321827299 | validation: 5.518417832984669]
	TIME [epoch: 10.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2359590797601845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2359590797601845 | validation: 3.9636743628847557]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.707867442642878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.707867442642878 | validation: 5.8888364193454485]
	TIME [epoch: 10.2 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421752247220921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.421752247220921 | validation: 4.244540522068378]
	TIME [epoch: 10.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.025192239107784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.025192239107784 | validation: 5.181919280773587]
	TIME [epoch: 10.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3364173412994775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3364173412994775 | validation: 5.454552493333436]
	TIME [epoch: 10.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204266982050423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.204266982050423 | validation: 5.122327674957549]
	TIME [epoch: 10.2 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.937204324284016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.937204324284016 | validation: 5.401754492172207]
	TIME [epoch: 10.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9331314985357695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9331314985357695 | validation: 4.723303089152806]
	TIME [epoch: 10.2 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8503842846894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8503842846894 | validation: 5.330460949212706]
	TIME [epoch: 10.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085454234298481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.085454234298481 | validation: 4.532983581158463]
	TIME [epoch: 10.2 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.913423832159513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.913423832159513 | validation: 5.311139455238843]
	TIME [epoch: 10.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888038614461015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.888038614461015 | validation: 4.410465084523661]
	TIME [epoch: 10.2 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.860903002955043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.860903002955043 | validation: 4.380833227815457]
	TIME [epoch: 10.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.990156710325224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.990156710325224 | validation: 3.2879107879901097]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3946756939269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3946756939269 | validation: 3.605062370989426]
	TIME [epoch: 10.2 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.479154543048267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.479154543048267 | validation: 3.8744831787035765]
	TIME [epoch: 10.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.145958013375838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145958013375838 | validation: 3.267313189347651]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9981535684148044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9981535684148044 | validation: 3.978928840267745]
	TIME [epoch: 10.2 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.099192833584489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.099192833584489 | validation: 4.9362571932993395]
	TIME [epoch: 10.2 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9950300234625065		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.9950300234625065 | validation: 5.139913009204483]
	TIME [epoch: 10.2 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.834912069413041		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.834912069413041 | validation: 4.851869339115245]
	TIME [epoch: 10.2 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810156451307425		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.810156451307425 | validation: 5.300497320117828]
	TIME [epoch: 10.2 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824910627242514		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 4.824910627242514 | validation: 4.690135700375865]
	TIME [epoch: 10.2 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.868815577766261		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.868815577766261 | validation: 4.846413843764441]
	TIME [epoch: 10.2 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.40117211768828		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 4.40117211768828 | validation: 5.330599695454735]
	TIME [epoch: 10.2 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.415709964964385		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 4.415709964964385 | validation: 3.4564386192271614]
	TIME [epoch: 10.2 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500897369782636		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.500897369782636 | validation: 4.760141867585023]
	TIME [epoch: 10.2 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.071106395089907		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 4.071106395089907 | validation: 4.5415965495508095]
	TIME [epoch: 10.2 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.072535064755681		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 4.072535064755681 | validation: 5.517339408403104]
	TIME [epoch: 10.2 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.83625769964579		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 4.83625769964579 | validation: 4.971377127296494]
	TIME [epoch: 10.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794699624237703		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 4.794699624237703 | validation: 6.219464542985284]
	TIME [epoch: 10.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.831991835878301		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 4.831991835878301 | validation: 5.876646215504675]
	TIME [epoch: 10.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903669479171937		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 4.903669479171937 | validation: 4.928849152940189]
	TIME [epoch: 10.2 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808458380333818		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 4.808458380333818 | validation: 4.355169413469177]
	TIME [epoch: 10.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835165849029961		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 4.835165849029961 | validation: 5.093206559961732]
	TIME [epoch: 10.2 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.327976366221359		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 4.327976366221359 | validation: 4.0252929593099385]
	TIME [epoch: 10.2 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.844518678603572		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.844518678603572 | validation: 4.616698012294129]
	TIME [epoch: 10.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.805043613541082		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.805043613541082 | validation: 4.316036180921511]
	TIME [epoch: 10.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9674903393035037		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.9674903393035037 | validation: 4.059441812929347]
	TIME [epoch: 10.2 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5264342197839698		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.5264342197839698 | validation: 4.2644559253338]
	TIME [epoch: 10.2 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6652588867821168		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.6652588867821168 | validation: 4.609077149437489]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018171386593485		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.6018171386593485 | validation: 4.86114322010133]
	TIME [epoch: 10.2 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.249966413705485		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.249966413705485 | validation: 3.500841963129368]
	TIME [epoch: 10.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494856662707407		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.494856662707407 | validation: 4.634443540581539]
	TIME [epoch: 10.2 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.755533018141444		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.755533018141444 | validation: 3.8090974272799816]
	TIME [epoch: 10.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3845653389837635		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.3845653389837635 | validation: 3.6552912182625024]
	TIME [epoch: 10.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6522336105058555		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.6522336105058555 | validation: 3.958492792395016]
	TIME [epoch: 10.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975985462581006		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.5975985462581006 | validation: 3.8150933124170696]
	TIME [epoch: 10.2 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.390289081154857		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.390289081154857 | validation: 4.370954915730339]
	TIME [epoch: 10.2 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4315997692214197		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.4315997692214197 | validation: 3.5646215804139207]
	TIME [epoch: 10.2 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5713311319936922		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.5713311319936922 | validation: 4.402209606805726]
	TIME [epoch: 10.2 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.454447711665267		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.454447711665267 | validation: 4.162527225425518]
	TIME [epoch: 10.2 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.401748293835317		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.401748293835317 | validation: 3.174584975365401]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.20706156198674		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.20706156198674 | validation: 3.7980874174231993]
	TIME [epoch: 10.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501837963857844		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.501837963857844 | validation: 3.5285527084974593]
	TIME [epoch: 10.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.263819149306696		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.263819149306696 | validation: 3.7482486391746432]
	TIME [epoch: 10.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.317378504917566		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.317378504917566 | validation: 3.624198184773503]
	TIME [epoch: 10.2 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2418715314404603		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.2418715314404603 | validation: 3.6503801451290925]
	TIME [epoch: 10.2 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1759541572597088		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.1759541572597088 | validation: 4.247360065020116]
	TIME [epoch: 10.2 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.207983284141271		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.207983284141271 | validation: 5.524713906390912]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3282537430293635		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.3282537430293635 | validation: 4.3683457176605645]
	TIME [epoch: 10.2 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4260897484957757		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.4260897484957757 | validation: 3.718386211442793]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1492356240841293		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.1492356240841293 | validation: 4.0700254396261935]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.251270275943425		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.251270275943425 | validation: 3.318679493118227]
	TIME [epoch: 10.2 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1066183443195627		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.1066183443195627 | validation: 3.987015119045478]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5357018658476775		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.5357018658476775 | validation: 3.853307320555813]
	TIME [epoch: 10.2 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.167164749772933		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.167164749772933 | validation: 3.5177235921438057]
	TIME [epoch: 10.2 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1406210481885894		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.1406210481885894 | validation: 3.5199052011268397]
	TIME [epoch: 10.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.121080221013955		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.121080221013955 | validation: 3.6636282034440057]
	TIME [epoch: 10.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1318134527600607		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.1318134527600607 | validation: 3.492918949904381]
	TIME [epoch: 10.2 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.859866243550351		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 2.859866243550351 | validation: 3.4700550412320084]
	TIME [epoch: 10.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.202098564349452		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.202098564349452 | validation: 4.19789172727903]
	TIME [epoch: 10.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0481866473744517		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.0481866473744517 | validation: 3.6093709904488036]
	TIME [epoch: 10.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9477155425008026		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 2.9477155425008026 | validation: 3.452562702737403]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9921938370605354		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 2.9921938370605354 | validation: 3.3954001732060157]
	TIME [epoch: 10.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0230264480457443		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.0230264480457443 | validation: 3.2465549262643765]
	TIME [epoch: 10.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8895990225059904		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 2.8895990225059904 | validation: 3.9718542861110486]
	TIME [epoch: 10.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.88818903816722		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 2.88818903816722 | validation: 3.3034261925253174]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.131960257246355		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.131960257246355 | validation: 3.968010804174295]
	TIME [epoch: 10.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.149629316169288		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.149629316169288 | validation: 3.986418470985778]
	TIME [epoch: 10.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1030096507356344		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.1030096507356344 | validation: 3.337043874244323]
	TIME [epoch: 10.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9445060730770374		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 2.9445060730770374 | validation: 3.152274386243436]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029578103518195		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.029578103518195 | validation: 3.596767625103224]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0659841599790356		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 3.0659841599790356 | validation: 3.2468843716690436]
	TIME [epoch: 10.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9915201852320576		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 2.9915201852320576 | validation: 3.7718359326659776]
	TIME [epoch: 10.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9100808938457603		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 2.9100808938457603 | validation: 3.598620166176805]
	TIME [epoch: 10.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965725324297989		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 2.965725324297989 | validation: 3.5204616741923305]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8739377165068922		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 2.8739377165068922 | validation: 3.2024904648260746]
	TIME [epoch: 10.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6385303011929295		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 2.6385303011929295 | validation: 4.23960300769823]
	TIME [epoch: 10.2 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.077498366023207		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.077498366023207 | validation: 3.6770498119682613]
	TIME [epoch: 10.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0160581336905086		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.0160581336905086 | validation: 3.3506081485133006]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8167632049777125		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 2.8167632049777125 | validation: 3.252702514768868]
	TIME [epoch: 10.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.783928145538497		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 2.783928145538497 | validation: 3.797640523352523]
	TIME [epoch: 10.2 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8357412650233216		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 2.8357412650233216 | validation: 3.917938426647538]
	TIME [epoch: 10.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0464395287655974		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.0464395287655974 | validation: 3.3598316144624607]
	TIME [epoch: 10.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8268544803964635		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 2.8268544803964635 | validation: 3.1372307507736434]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8575089204334816		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 2.8575089204334816 | validation: 3.171030261798253]
	TIME [epoch: 10.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.781811209587585		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 2.781811209587585 | validation: 2.985214311700093]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8188905808262366		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 2.8188905808262366 | validation: 3.3538345959101434]
	TIME [epoch: 10.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.729771031040165		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 2.729771031040165 | validation: 3.3041720464783557]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7428008377798023		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 2.7428008377798023 | validation: 2.86978233461395]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6896946487951277		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 2.6896946487951277 | validation: 4.150844009470692]
	TIME [epoch: 10.2 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.849740672234545		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 2.849740672234545 | validation: 3.260170854859085]
	TIME [epoch: 10.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6814246490433438		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 2.6814246490433438 | validation: 3.3202462105779853]
	TIME [epoch: 10.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7621684298542477		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 2.7621684298542477 | validation: 3.0409988447442933]
	TIME [epoch: 10.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.694722269757643		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 2.694722269757643 | validation: 3.186838043120947]
	TIME [epoch: 10.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8286058801297926		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 2.8286058801297926 | validation: 2.9604673715923253]
	TIME [epoch: 10.2 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.546259439770851		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 2.546259439770851 | validation: 3.0595551939131895]
	TIME [epoch: 10.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.733716968765129		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 2.733716968765129 | validation: 3.501224334532677]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.720420155658868		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 2.720420155658868 | validation: 3.2012958332892976]
	TIME [epoch: 10.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9604818725191118		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 2.9604818725191118 | validation: 3.0543390444148564]
	TIME [epoch: 10.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5572379572457518		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 2.5572379572457518 | validation: 3.887095466200271]
	TIME [epoch: 10.2 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956105842097691		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 2.956105842097691 | validation: 3.082221051733486]
	TIME [epoch: 10.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.618481105468584		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.618481105468584 | validation: 3.34159670298791]
	TIME [epoch: 10.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7258548535147114		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 2.7258548535147114 | validation: 3.124703881549608]
	TIME [epoch: 10.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6362680838070682		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 2.6362680838070682 | validation: 3.3538971250391674]
	TIME [epoch: 10.2 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8503365611906233		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 2.8503365611906233 | validation: 2.929484157151959]
	TIME [epoch: 10.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0760815425882093		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 3.0760815425882093 | validation: 3.2664064257595844]
	TIME [epoch: 10.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.871780497993528		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 3.871780497993528 | validation: 4.145575615935223]
	TIME [epoch: 10.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8266158123705902		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 2.8266158123705902 | validation: 3.0651294829016003]
	TIME [epoch: 10.2 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6491076932096043		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 2.6491076932096043 | validation: 3.4433357755339866]
	TIME [epoch: 10.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7937221958618057		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 2.7937221958618057 | validation: 3.20359363354569]
	TIME [epoch: 10.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5540363049116386		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 2.5540363049116386 | validation: 3.569436484362914]
	TIME [epoch: 10.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5507559073300117		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 2.5507559073300117 | validation: 2.9180362050288515]
	TIME [epoch: 10.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3792895744086797		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 2.3792895744086797 | validation: 3.0741303135952442]
	TIME [epoch: 10.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.672129922773126		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 2.672129922773126 | validation: 3.8941612046032157]
	TIME [epoch: 10.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8196427211442185		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 2.8196427211442185 | validation: 3.0947325844255515]
	TIME [epoch: 10.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4135957767525924		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 2.4135957767525924 | validation: 3.7082148211673776]
	TIME [epoch: 10.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6650363845885945		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 2.6650363845885945 | validation: 3.050457732838952]
	TIME [epoch: 10.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542843682563311		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 2.542843682563311 | validation: 3.084483825496183]
	TIME [epoch: 10.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3851928308659263		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.3851928308659263 | validation: 3.201104669006464]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.83798333058044		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 2.83798333058044 | validation: 3.1791736724675306]
	TIME [epoch: 10.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5464503198290305		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 2.5464503198290305 | validation: 3.5997510051145527]
	TIME [epoch: 10.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6108469550479523		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 2.6108469550479523 | validation: 2.880440880444906]
	TIME [epoch: 10.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6301775890447168		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 2.6301775890447168 | validation: 3.2648807335135883]
	TIME [epoch: 10.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.500881515073973		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 2.500881515073973 | validation: 3.314373822946328]
	TIME [epoch: 10.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.552639206510011		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 2.552639206510011 | validation: 2.924215975069117]
	TIME [epoch: 10.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.332748231276318		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 2.332748231276318 | validation: 3.668669683615804]
	TIME [epoch: 10.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.495630781025893		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 2.495630781025893 | validation: 3.7693268193019267]
	TIME [epoch: 10.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.706270235616013		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 2.706270235616013 | validation: 2.829555153082535]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5794902861731934		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 2.5794902861731934 | validation: 2.863260595890083]
	TIME [epoch: 10.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4732729145438888		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 2.4732729145438888 | validation: 2.8447643303997014]
	TIME [epoch: 10.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.436736969344282		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 2.436736969344282 | validation: 2.9077020175076753]
	TIME [epoch: 10.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4496827299581474		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 2.4496827299581474 | validation: 3.310421675307543]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4319237485855973		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 2.4319237485855973 | validation: 3.14588699382991]
	TIME [epoch: 10.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5352522830790667		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 2.5352522830790667 | validation: 3.054677789520933]
	TIME [epoch: 10.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5526010333881075		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 2.5526010333881075 | validation: 2.9137449891162475]
	TIME [epoch: 10.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4705102863507915		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 2.4705102863507915 | validation: 3.0383329934630554]
	TIME [epoch: 10.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.395228257320634		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 2.395228257320634 | validation: 3.6820897057614577]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4040491052618123		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 2.4040491052618123 | validation: 3.3425879241728933]
	TIME [epoch: 10.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.558388448562395		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.558388448562395 | validation: 3.5069313654883842]
	TIME [epoch: 10.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5359252604486007		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 2.5359252604486007 | validation: 3.2356644738409694]
	TIME [epoch: 10.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.510409944349624		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 2.510409944349624 | validation: 2.8028512710144744]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3647814203836703		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 2.3647814203836703 | validation: 2.878629512412298]
	TIME [epoch: 10.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4591833400063736		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 2.4591833400063736 | validation: 3.141734854488391]
	TIME [epoch: 10.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3500126805211146		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 2.3500126805211146 | validation: 3.0690513413914204]
	TIME [epoch: 10.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5500310569809628		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 2.5500310569809628 | validation: 2.7734855346665257]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4253181427327206		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 2.4253181427327206 | validation: 2.7831110899104403]
	TIME [epoch: 10.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.29260133963279		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 2.29260133963279 | validation: 3.3175346360140656]
	TIME [epoch: 10.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7008697520892166		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 2.7008697520892166 | validation: 2.999960183344115]
	TIME [epoch: 10.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.492708423483019		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 2.492708423483019 | validation: 2.9679172665134588]
	TIME [epoch: 10.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3958832603847924		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 2.3958832603847924 | validation: 3.280494685815903]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.414125416856499		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 2.414125416856499 | validation: 3.7358375026741606]
	TIME [epoch: 10.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5583741122968866		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 2.5583741122968866 | validation: 2.7778380440060553]
	TIME [epoch: 10.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3181065552874665		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 2.3181065552874665 | validation: 3.672241144409893]
	TIME [epoch: 10.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.647387317510406		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 2.647387317510406 | validation: 2.8631421544832745]
	TIME [epoch: 10.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3575328899396046		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 2.3575328899396046 | validation: 2.9947270330017837]
	TIME [epoch: 10.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.489621048725181		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 2.489621048725181 | validation: 2.7518193677310006]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.431550325448451		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 2.431550325448451 | validation: 3.9334088918755867]
	TIME [epoch: 10.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.534911937889438		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 2.534911937889438 | validation: 2.886370999042016]
	TIME [epoch: 10.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.345325413073362		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 2.345325413073362 | validation: 2.788180160944146]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0138554257764265		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.0138554257764265 | validation: 3.5305988389103184]
	TIME [epoch: 10.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.870981439843678		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 2.870981439843678 | validation: 3.2625082435244708]
	TIME [epoch: 10.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3573964050327616		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 2.3573964050327616 | validation: 2.8970225546274295]
	TIME [epoch: 10.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2783351238902574		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 2.2783351238902574 | validation: 3.057856437067959]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3984265404420597		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 2.3984265404420597 | validation: 3.0153523276693006]
	TIME [epoch: 10.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4212181578667398		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 2.4212181578667398 | validation: 3.387946232661674]
	TIME [epoch: 10.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.380863690595606		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 2.380863690595606 | validation: 2.99272636405146]
	TIME [epoch: 10.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4129216703032186		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 2.4129216703032186 | validation: 2.8140448163310263]
	TIME [epoch: 10.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.366139455409157		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 2.366139455409157 | validation: 2.7654144935224934]
	TIME [epoch: 10.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4769795227546547		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 2.4769795227546547 | validation: 2.7864771108127546]
	TIME [epoch: 10.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.364533711368059		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 2.364533711368059 | validation: 3.018582134524844]
	TIME [epoch: 10.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.363397832044489		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 2.363397832044489 | validation: 2.7425498181342056]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298418413800878		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 2.298418413800878 | validation: 2.8717657041359956]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1536782905968144		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.1536782905968144 | validation: 3.206456486384795]
	TIME [epoch: 10.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.334122557642833		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 2.334122557642833 | validation: 2.9680811051394183]
	TIME [epoch: 10.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3977719006872777		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 2.3977719006872777 | validation: 2.7670358663424]
	TIME [epoch: 10.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3793887114234664		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 2.3793887114234664 | validation: 3.1478510524814323]
	TIME [epoch: 10.2 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4332697172488738		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 2.4332697172488738 | validation: 2.7316107187972865]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.416261226784003		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 2.416261226784003 | validation: 3.2523454883133445]
	TIME [epoch: 10.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220941957263614		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 2.220941957263614 | validation: 2.843943054741824]
	TIME [epoch: 10.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4292372232991264		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 2.4292372232991264 | validation: 2.8337361348551826]
	TIME [epoch: 10.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195204178062453		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 2.195204178062453 | validation: 3.248646736336267]
	TIME [epoch: 10.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2907452013236353		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 2.2907452013236353 | validation: 3.2371983288920934]
	TIME [epoch: 10.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.585481214980338		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 2.585481214980338 | validation: 3.3336178067146256]
	TIME [epoch: 10.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247632813148077		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 2.247632813148077 | validation: 2.983995297113479]
	TIME [epoch: 10.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.319848026011651		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 2.319848026011651 | validation: 3.117635319419577]
	TIME [epoch: 10.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331484060891779		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 2.331484060891779 | validation: 2.934402994764156]
	TIME [epoch: 10.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2836732047184953		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 2.2836732047184953 | validation: 2.646370319430241]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.216602919361506		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 2.216602919361506 | validation: 3.2681513315663278]
	TIME [epoch: 10.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1974864125196913		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 2.1974864125196913 | validation: 2.7388531182376483]
	TIME [epoch: 10.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.428588043181974		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 2.428588043181974 | validation: 2.9800855435845848]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1421875012751714		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 2.1421875012751714 | validation: 2.6366120006871285]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.746075145259723		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 2.746075145259723 | validation: 2.5801164617899692]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2345321718220585		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 2.2345321718220585 | validation: 2.793086968089647]
	TIME [epoch: 10.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359088668950947		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 2.359088668950947 | validation: 2.682029025515192]
	TIME [epoch: 10.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2631839813975767		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 2.2631839813975767 | validation: 2.6538889733516204]
	TIME [epoch: 10.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.579183630895		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 2.579183630895 | validation: 2.81100361083353]
	TIME [epoch: 10.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1964136608866367		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 2.1964136608866367 | validation: 3.2197154605383207]
	TIME [epoch: 10.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2675322763854084		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 2.2675322763854084 | validation: 2.8964340333651863]
	TIME [epoch: 10.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2018593802516504		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 2.2018593802516504 | validation: 2.8713807278103087]
	TIME [epoch: 10.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1458356351255814		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 2.1458356351255814 | validation: 2.8328119283071267]
	TIME [epoch: 10.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266630810809849		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 2.266630810809849 | validation: 2.6628830535142924]
	TIME [epoch: 10.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0656314903092374		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 2.0656314903092374 | validation: 2.552437926352203]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2447235073306318		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 2.2447235073306318 | validation: 2.7453127705224394]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211933680553476		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 2.211933680553476 | validation: 2.806432922915955]
	TIME [epoch: 10.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1449702000654205		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 2.1449702000654205 | validation: 2.8646530001314057]
	TIME [epoch: 10.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.170201825112531		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 2.170201825112531 | validation: 2.808228974113563]
	TIME [epoch: 10.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2805831176475295		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 2.2805831176475295 | validation: 2.545903287695763]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0307560228542956		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 2.0307560228542956 | validation: 2.6654334745558743]
	TIME [epoch: 10.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0651246553126525		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 2.0651246553126525 | validation: 2.5689476876339334]
	TIME [epoch: 10.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.98150156787156		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 1.98150156787156 | validation: 2.3164950092769283]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9830136041179123		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 1.9830136041179123 | validation: 1.9808699823893]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9140677508855035		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 1.9140677508855035 | validation: 1.874214078756797]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.698613248353349		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 1.698613248353349 | validation: 2.1557511286519575]
	TIME [epoch: 10.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.803584962453891		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 1.803584962453891 | validation: 1.7970156380601718]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1270100638430853		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 2.1270100638430853 | validation: 1.7415170602598955]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8033392068525917		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 1.8033392068525917 | validation: 1.8725521101987403]
	TIME [epoch: 10.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.616460032722348		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 1.616460032722348 | validation: 1.9053403683558008]
	TIME [epoch: 10.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4911016636028562		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 1.4911016636028562 | validation: 1.8270074332973076]
	TIME [epoch: 10.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7410327005218966		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 1.7410327005218966 | validation: 1.5484734018083026]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7267880742765844		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 1.7267880742765844 | validation: 1.8428893542628453]
	TIME [epoch: 10.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8573105525720077		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 1.8573105525720077 | validation: 1.5301153110212795]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6310722583758561		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 1.6310722583758561 | validation: 2.087300649440379]
	TIME [epoch: 10.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6610306030327329		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 1.6610306030327329 | validation: 1.8337011172781326]
	TIME [epoch: 10.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4857911020016723		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 1.4857911020016723 | validation: 1.7017288988539152]
	TIME [epoch: 10.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.418526305774495		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 2.418526305774495 | validation: 2.574380485682497]
	TIME [epoch: 10.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8731206317157518		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 1.8731206317157518 | validation: 1.328359323705823]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.498097524774959		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 1.498097524774959 | validation: 1.5151852335776455]
	TIME [epoch: 10.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7064516515513197		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 1.7064516515513197 | validation: 1.5858976828799638]
	TIME [epoch: 10.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5777543964403549		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 1.5777543964403549 | validation: 1.9636292760775755]
	TIME [epoch: 10.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5210599535832716		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 1.5210599535832716 | validation: 1.7407202970904307]
	TIME [epoch: 10.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4428565348291542		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 1.4428565348291542 | validation: 1.4742246785668318]
	TIME [epoch: 10.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4306386715312485		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 1.4306386715312485 | validation: 1.4229969403008116]
	TIME [epoch: 10.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3480962169769957		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 1.3480962169769957 | validation: 1.911952836781759]
	TIME [epoch: 10.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4256076519932122		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 1.4256076519932122 | validation: 1.3403818519030737]
	TIME [epoch: 10.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3949158412233857		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 1.3949158412233857 | validation: 1.578196027239739]
	TIME [epoch: 10.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3649844595770184		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 1.3649844595770184 | validation: 1.6357707982521472]
	TIME [epoch: 10.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3533219372596712		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 1.3533219372596712 | validation: 1.634414982677975]
	TIME [epoch: 10.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3271078840393702		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 1.3271078840393702 | validation: 1.724232259251328]
	TIME [epoch: 10.2 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.41814920824149		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 1.41814920824149 | validation: 1.6787942268582066]
	TIME [epoch: 10.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2791465942867262		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 1.2791465942867262 | validation: 1.5115409196528062]
	TIME [epoch: 10.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3226568552985818		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 1.3226568552985818 | validation: 1.3120849009453897]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2312337998868719		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 1.2312337998868719 | validation: 1.65988447866573]
	TIME [epoch: 10.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4703657002285406		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 1.4703657002285406 | validation: 1.6486388703628536]
	TIME [epoch: 10.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3438587681174063		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 1.3438587681174063 | validation: 1.7105567349640782]
	TIME [epoch: 10.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4657398816843046		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 1.4657398816843046 | validation: 2.5401333644989887]
	TIME [epoch: 10.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.544475453777362		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 1.544475453777362 | validation: 1.2745546422159604]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2349837102230417		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 1.2349837102230417 | validation: 1.328508311190712]
	TIME [epoch: 10.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1558828704991813		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 1.1558828704991813 | validation: 1.406933097955098]
	TIME [epoch: 10.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2790833288183907		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 1.2790833288183907 | validation: 1.3099576871915195]
	TIME [epoch: 10.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.621247148922728		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 1.621247148922728 | validation: 1.2213804030911406]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.132130431478558		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 1.132130431478558 | validation: 1.367490535327391]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.354693116634237		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 1.354693116634237 | validation: 1.5381102009716574]
	TIME [epoch: 10.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294257820851082		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 1.294257820851082 | validation: 1.3127416714902262]
	TIME [epoch: 10.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1256263138505922		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 1.1256263138505922 | validation: 1.4527472212106418]
	TIME [epoch: 10.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0814927788965036		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 1.0814927788965036 | validation: 1.3325187586980314]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1470664088959714		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 1.1470664088959714 | validation: 1.1899339611370354]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.127748820442506		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 1.127748820442506 | validation: 1.189941058361312]
	TIME [epoch: 10.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2119755249993578		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 1.2119755249993578 | validation: 1.505375388128323]
	TIME [epoch: 10.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.239343314147532		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 1.239343314147532 | validation: 1.1716692069322934]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2150694074385169		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 1.2150694074385169 | validation: 1.2027551906545138]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1166386034736444		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 1.1166386034736444 | validation: 1.1601898594239162]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0283825105053794		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 1.0283825105053794 | validation: 1.278752430691044]
	TIME [epoch: 10.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0524806670607112		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 1.0524806670607112 | validation: 1.2120017357616066]
	TIME [epoch: 10.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.23630748684211		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 1.23630748684211 | validation: 1.2304026802511043]
	TIME [epoch: 10.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2169907136112235		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 1.2169907136112235 | validation: 1.2870122787566547]
	TIME [epoch: 10.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1019513359587167		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 1.1019513359587167 | validation: 1.2897841746154557]
	TIME [epoch: 10.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.272433525507346		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 1.272433525507346 | validation: 1.3583769035683309]
	TIME [epoch: 10.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0750899413535864		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 1.0750899413535864 | validation: 1.1337139007994084]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0358959746312428		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 1.0358959746312428 | validation: 1.4260756524613651]
	TIME [epoch: 10.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1140564438977605		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 1.1140564438977605 | validation: 1.3748166308345833]
	TIME [epoch: 10.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0237305125003395		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 1.0237305125003395 | validation: 1.457102330651007]
	TIME [epoch: 10.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1228079497782386		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 1.1228079497782386 | validation: 1.534459192948231]
	TIME [epoch: 10.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.35495014385583		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 1.35495014385583 | validation: 1.3470776971399414]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2470887236892378		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 1.2470887236892378 | validation: 1.158597336083165]
	TIME [epoch: 10.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1747765630423128		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 1.1747765630423128 | validation: 1.2899567070990443]
	TIME [epoch: 10.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1671087721824815		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 1.1671087721824815 | validation: 1.1293705360953012]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9833607808105722		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 0.9833607808105722 | validation: 1.1083034350967615]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9888162024071093		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 0.9888162024071093 | validation: 1.2001779455145891]
	TIME [epoch: 10.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9619852384733376		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 0.9619852384733376 | validation: 1.166595961775429]
	TIME [epoch: 10.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9702417610426476		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 0.9702417610426476 | validation: 1.3164187542089707]
	TIME [epoch: 10.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1679338774722448		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 1.1679338774722448 | validation: 1.2031358209480636]
	TIME [epoch: 10.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0011050524031588		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 1.0011050524031588 | validation: 1.167558880220105]
	TIME [epoch: 10.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9457000328699655		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 0.9457000328699655 | validation: 1.311874963715988]
	TIME [epoch: 10.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2292447063192549		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 1.2292447063192549 | validation: 1.4136886923354115]
	TIME [epoch: 10.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9988587128893457		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 0.9988587128893457 | validation: 1.111508027355698]
	TIME [epoch: 10.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9909548871492865		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 0.9909548871492865 | validation: 1.1262417979244643]
	TIME [epoch: 10.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9295725919388704		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 0.9295725919388704 | validation: 1.2467833828161385]
	TIME [epoch: 10.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9620175681615908		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 0.9620175681615908 | validation: 1.128851369165248]
	TIME [epoch: 10.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9548505228218168		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 0.9548505228218168 | validation: 1.0575360948427515]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9566484958094629		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 0.9566484958094629 | validation: 1.2313516445932084]
	TIME [epoch: 10.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9823966951261351		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 0.9823966951261351 | validation: 1.1869567849643003]
	TIME [epoch: 10.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9110942259430036		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 0.9110942259430036 | validation: 1.4391427493341495]
	TIME [epoch: 10.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0288986095854993		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 1.0288986095854993 | validation: 1.0404098228019663]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9430707537190124		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 0.9430707537190124 | validation: 1.1931570147347978]
	TIME [epoch: 10.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8889961576328084		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 0.8889961576328084 | validation: 1.2705189696953354]
	TIME [epoch: 10.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9173473700721366		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 0.9173473700721366 | validation: 1.025605518731406]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.048873233360279		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 1.048873233360279 | validation: 1.0773025638316822]
	TIME [epoch: 10.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4691661576867223		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 1.4691661576867223 | validation: 1.2852865704816074]
	TIME [epoch: 10.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9885744277348376		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 0.9885744277348376 | validation: 1.1595306115468977]
	TIME [epoch: 10.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9464560051943082		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 0.9464560051943082 | validation: 1.096936782668454]
	TIME [epoch: 10.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9262143704241772		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 0.9262143704241772 | validation: 1.099072028485307]
	TIME [epoch: 10.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8761642075725377		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 0.8761642075725377 | validation: 1.1301893175405462]
	TIME [epoch: 10.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9805687652641726		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 0.9805687652641726 | validation: 1.1169147890982905]
	TIME [epoch: 10.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9503090810994694		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 0.9503090810994694 | validation: 1.2734468126701601]
	TIME [epoch: 10.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0137308453412364		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 1.0137308453412364 | validation: 1.0383060838471492]
	TIME [epoch: 10.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9335026760189825		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 0.9335026760189825 | validation: 1.0583632987807396]
	TIME [epoch: 10.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8528480745674163		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 0.8528480745674163 | validation: 1.3320729124335131]
	TIME [epoch: 10.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9479011219673577		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 0.9479011219673577 | validation: 1.0979401057316371]
	TIME [epoch: 10.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1285441965846545		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 1.1285441965846545 | validation: 2.1857072895875214]
	TIME [epoch: 10.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3354608389243265		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 1.3354608389243265 | validation: 1.063051783338602]
	TIME [epoch: 10.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9552298488751327		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 0.9552298488751327 | validation: 1.1744647679544205]
	TIME [epoch: 10.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.968685973663054		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 0.968685973663054 | validation: 1.0500422569104766]
	TIME [epoch: 10.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8576706872906728		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 0.8576706872906728 | validation: 0.973844400294197]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9126404502707672		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 0.9126404502707672 | validation: 0.9734144036692254]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8407279918726651		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 0.8407279918726651 | validation: 0.948822312285019]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8643851486049702		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 0.8643851486049702 | validation: 1.1162736254985297]
	TIME [epoch: 10.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9896977660824398		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 0.9896977660824398 | validation: 1.1118375936300653]
	TIME [epoch: 10.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8800519790813428		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 0.8800519790813428 | validation: 1.1944884173621764]
	TIME [epoch: 10.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9484617118208615		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 0.9484617118208615 | validation: 1.0861635337390922]
	TIME [epoch: 10.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8996646855016678		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 0.8996646855016678 | validation: 0.9694642326361215]
	TIME [epoch: 10.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8508275057237		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 0.8508275057237 | validation: 1.1024816683654801]
	TIME [epoch: 10.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8915674297231737		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 0.8915674297231737 | validation: 1.1848527474631896]
	TIME [epoch: 10.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8648693672702041		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 0.8648693672702041 | validation: 1.2643017320234902]
	TIME [epoch: 10.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.864700157017516		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 0.864700157017516 | validation: 1.1802927137918322]
	TIME [epoch: 10.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.895209224426942		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 0.895209224426942 | validation: 1.261203391053633]
	TIME [epoch: 10.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8781235613479563		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 0.8781235613479563 | validation: 1.0849837186135642]
	TIME [epoch: 10.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9323640493688796		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 0.9323640493688796 | validation: 1.1313257341413352]
	TIME [epoch: 10.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.882985986181468		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 0.882985986181468 | validation: 1.047837046195717]
	TIME [epoch: 10.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1442378552948398		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 1.1442378552948398 | validation: 0.9330277437083592]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.897924329082288		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 0.897924329082288 | validation: 1.0750841283373498]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8198223773366244		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 0.8198223773366244 | validation: 0.9738692621670967]
	TIME [epoch: 10.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8174488407864361		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 0.8174488407864361 | validation: 0.9412470323441983]
	TIME [epoch: 10.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8362358794190132		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 0.8362358794190132 | validation: 1.0980808644392324]
	TIME [epoch: 10.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9366620222691366		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 0.9366620222691366 | validation: 1.0537203539346667]
	TIME [epoch: 10.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8612622990830683		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 0.8612622990830683 | validation: 1.0970594523019168]
	TIME [epoch: 10.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8847733372773299		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 0.8847733372773299 | validation: 1.1162285309439477]
	TIME [epoch: 10.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8731933374355257		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 0.8731933374355257 | validation: 0.8695670304851288]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.774722200464186		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 0.774722200464186 | validation: 0.9335672716357496]
	TIME [epoch: 10.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8829875132101112		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 0.8829875132101112 | validation: 1.5637135090810004]
	TIME [epoch: 10.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9920099547521877		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 0.9920099547521877 | validation: 0.861025663931901]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.806566232978762		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 0.806566232978762 | validation: 0.9026888040962903]
	TIME [epoch: 10.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7678145977464512		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 0.7678145977464512 | validation: 1.2268892464271586]
	TIME [epoch: 10.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9115851324691079		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 0.9115851324691079 | validation: 1.0609530889828098]
	TIME [epoch: 10.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8672793314034873		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 0.8672793314034873 | validation: 1.0302024603918505]
	TIME [epoch: 10.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9539916513963492		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 0.9539916513963492 | validation: 0.7740255576144912]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8217049836339312		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 0.8217049836339312 | validation: 1.4093859383102105]
	TIME [epoch: 10.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0500218929953185		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 1.0500218929953185 | validation: 0.8024007368472953]
	TIME [epoch: 10.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9939618202631598		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 0.9939618202631598 | validation: 0.9421188705355101]
	TIME [epoch: 10.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8345652411255445		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 0.8345652411255445 | validation: 0.9421902298113327]
	TIME [epoch: 10.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7943832162681409		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 0.7943832162681409 | validation: 0.8982681312069262]
	TIME [epoch: 10.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7327777834025503		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 0.7327777834025503 | validation: 0.9245594399996069]
	TIME [epoch: 10.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8009708379419915		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 0.8009708379419915 | validation: 0.9974267812516101]
	TIME [epoch: 10.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7046677685518405		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 0.7046677685518405 | validation: 1.183376076786827]
	TIME [epoch: 10.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8380244134242751		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 0.8380244134242751 | validation: 0.798114222886257]
	TIME [epoch: 10.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7668740767654267		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 0.7668740767654267 | validation: 0.9555882433467167]
	TIME [epoch: 10.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.73027757885041		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 0.73027757885041 | validation: 1.050325440201523]
	TIME [epoch: 10.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7514848382634396		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 0.7514848382634396 | validation: 1.2579541801349126]
	TIME [epoch: 10.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7671708159225973		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 0.7671708159225973 | validation: 1.0099984502907193]
	TIME [epoch: 10.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7313977103609614		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 0.7313977103609614 | validation: 1.0994739476963618]
	TIME [epoch: 10.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8967341312748441		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 0.8967341312748441 | validation: 0.7443536510508794]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.897465441752078		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 0.897465441752078 | validation: 1.0491844555077574]
	TIME [epoch: 10.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7657530955091257		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 0.7657530955091257 | validation: 0.9740481189543726]
	TIME [epoch: 10.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7589347016556319		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 0.7589347016556319 | validation: 0.998517755204061]
	TIME [epoch: 10.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8508833500926734		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 0.8508833500926734 | validation: 0.7315111169542644]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7710917579557741		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 0.7710917579557741 | validation: 0.8235031879418463]
	TIME [epoch: 10.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8743394579443601		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 0.8743394579443601 | validation: 1.001955808658837]
	TIME [epoch: 10.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7256034544133219		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 0.7256034544133219 | validation: 0.986985842179473]
	TIME [epoch: 10.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7759803886765594		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 0.7759803886765594 | validation: 0.7883789150364316]
	TIME [epoch: 10.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7140863634669316		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 0.7140863634669316 | validation: 0.8286010251154617]
	TIME [epoch: 10.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8193524808474258		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 0.8193524808474258 | validation: 0.9708425315367959]
	TIME [epoch: 10.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8322040926132583		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 0.8322040926132583 | validation: 0.813902636609952]
	TIME [epoch: 10.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7640778515425091		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 0.7640778515425091 | validation: 0.8887700786486229]
	TIME [epoch: 10.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7087039597746225		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 0.7087039597746225 | validation: 1.0077112553280332]
	TIME [epoch: 10.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.792076056949641		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 0.792076056949641 | validation: 1.0643300225032235]
	TIME [epoch: 10.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7562096669356971		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 0.7562096669356971 | validation: 0.8781889624872591]
	TIME [epoch: 10.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7826281297441195		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 0.7826281297441195 | validation: 0.9097173923207669]
	TIME [epoch: 10.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.788755380448545		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 0.788755380448545 | validation: 0.8857373615404356]
	TIME [epoch: 10.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7733514463845613		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 0.7733514463845613 | validation: 0.8613117259570459]
	TIME [epoch: 10.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.762556218264278		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 0.762556218264278 | validation: 0.8504347959597917]
	TIME [epoch: 10.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.729427817031086		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 0.729427817031086 | validation: 0.9165548411549225]
	TIME [epoch: 10.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8905535593749644		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 0.8905535593749644 | validation: 0.9289572882723647]
	TIME [epoch: 10.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7085074666914124		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 0.7085074666914124 | validation: 0.7807689386381094]
	TIME [epoch: 10.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.781438463431618		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 0.781438463431618 | validation: 0.9285241237942684]
	TIME [epoch: 10.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8202217064340017		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 0.8202217064340017 | validation: 0.9334051426865985]
	TIME [epoch: 10.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.72031258241534		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 0.72031258241534 | validation: 0.8481611889729586]
	TIME [epoch: 10.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971283966760728		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 0.6971283966760728 | validation: 0.8334724107117418]
	TIME [epoch: 10.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7234242776633926		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 0.7234242776633926 | validation: 0.8266975063776506]
	TIME [epoch: 10.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7236279109503879		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 0.7236279109503879 | validation: 0.8737086138957029]
	TIME [epoch: 10.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.715705655626081		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 0.715705655626081 | validation: 1.0248614591066418]
	TIME [epoch: 10.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8369975406830381		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 0.8369975406830381 | validation: 1.020307146945521]
	TIME [epoch: 10.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723185642853032		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 0.723185642853032 | validation: 0.9876873734595166]
	TIME [epoch: 10.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6784625921675179		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 0.6784625921675179 | validation: 0.6735950397244588]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6486593404011709		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 0.6486593404011709 | validation: 0.7286856146645359]
	TIME [epoch: 10.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7435205768195472		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 0.7435205768195472 | validation: 0.804533295642065]
	TIME [epoch: 10.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7818075605013849		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 0.7818075605013849 | validation: 1.0302496013021984]
	TIME [epoch: 10.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7563710303750529		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 0.7563710303750529 | validation: 1.001783431172704]
	TIME [epoch: 10.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7844331198584508		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 0.7844331198584508 | validation: 0.7609532383691109]
	TIME [epoch: 10.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6610273341117752		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 0.6610273341117752 | validation: 0.9390730506218986]
	TIME [epoch: 10.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8088443699339134		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 0.8088443699339134 | validation: 0.7630301715146751]
	TIME [epoch: 10.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6901352996128358		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 0.6901352996128358 | validation: 0.7457463710157101]
	TIME [epoch: 10.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6488509560450532		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 0.6488509560450532 | validation: 0.835154461901035]
	TIME [epoch: 10.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7180281309720691		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 0.7180281309720691 | validation: 1.3575004384503278]
	TIME [epoch: 10.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8138814461324021		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 0.8138814461324021 | validation: 0.808123255640111]
	TIME [epoch: 10.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7144695347211846		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 0.7144695347211846 | validation: 0.8821740797983162]
	TIME [epoch: 10.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7719033816297863		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 0.7719033816297863 | validation: 0.8132536362214668]
	TIME [epoch: 10.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7097817519162116		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 0.7097817519162116 | validation: 0.8307773134920861]
	TIME [epoch: 10.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.745801536177746		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 0.745801536177746 | validation: 0.8294481169786093]
	TIME [epoch: 10.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7523793968378226		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 0.7523793968378226 | validation: 0.8504301128514072]
	TIME [epoch: 10.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.734235562991553		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 0.734235562991553 | validation: 0.7992454077294747]
	TIME [epoch: 10.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697922186319271		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 0.697922186319271 | validation: 0.9842120515751788]
	TIME [epoch: 10.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7935423118189318		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 0.7935423118189318 | validation: 0.7435667510773957]
	TIME [epoch: 10.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.653908558427177		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 0.653908558427177 | validation: 1.0420274240575524]
	TIME [epoch: 10.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.716318815087333		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 0.716318815087333 | validation: 0.7921212144541997]
	TIME [epoch: 10.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7684541298733993		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 0.7684541298733993 | validation: 0.922973557901118]
	TIME [epoch: 10.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.745477214553708		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 0.745477214553708 | validation: 0.7200981127484342]
	TIME [epoch: 10.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.683051876709144		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 0.683051876709144 | validation: 0.7326465828763905]
	TIME [epoch: 10.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.729600327152923		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 0.729600327152923 | validation: 0.9327709232416129]
	TIME [epoch: 10.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6989923717519522		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 0.6989923717519522 | validation: 0.7807922515325414]
	TIME [epoch: 10.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6723332245906315		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 0.6723332245906315 | validation: 0.8751786824484563]
	TIME [epoch: 10.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7301636610719949		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 0.7301636610719949 | validation: 0.9119617702513341]
	TIME [epoch: 10.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7046126479108092		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 0.7046126479108092 | validation: 0.7851098313493854]
	TIME [epoch: 10.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6261680935018071		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 0.6261680935018071 | validation: 0.8063314006275505]
	TIME [epoch: 10.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6505031955955451		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 0.6505031955955451 | validation: 1.0064439229669622]
	TIME [epoch: 10.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7058852465407564		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 0.7058852465407564 | validation: 1.0500884142722768]
	TIME [epoch: 10.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7374154678764073		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 0.7374154678764073 | validation: 1.306620299461876]
	TIME [epoch: 10.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9337613589258484		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 0.9337613589258484 | validation: 0.7741678443221582]
	TIME [epoch: 10.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6500837773758829		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 0.6500837773758829 | validation: 0.8992761551866714]
	TIME [epoch: 10.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7286053779758378		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 0.7286053779758378 | validation: 0.7143569251883414]
	TIME [epoch: 10.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.628539682204649		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 0.628539682204649 | validation: 0.7612866926126006]
	TIME [epoch: 10.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442859534429906		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 0.6442859534429906 | validation: 0.9561309903324663]
	TIME [epoch: 10.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7401757896632651		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 0.7401757896632651 | validation: 0.9934933264133394]
	TIME [epoch: 10.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6542345931027407		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 0.6542345931027407 | validation: 0.7466120317823141]
	TIME [epoch: 10.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6704385088757803		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 0.6704385088757803 | validation: 0.9930882132978133]
	TIME [epoch: 10.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.683975776102962		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 0.683975776102962 | validation: 0.7336958642702066]
	TIME [epoch: 10.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6223374992759492		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 0.6223374992759492 | validation: 0.7072262002474292]
	TIME [epoch: 10.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6288673065117326		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 0.6288673065117326 | validation: 0.627512898012179]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6945920854199057		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 0.6945920854199057 | validation: 0.6947740665649025]
	TIME [epoch: 10.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8408127631128508		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 0.8408127631128508 | validation: 0.7161309804981667]
	TIME [epoch: 10.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7371690953992963		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 0.7371690953992963 | validation: 0.8895588781531695]
	TIME [epoch: 10.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6814221429460909		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 0.6814221429460909 | validation: 0.7554990065726991]
	TIME [epoch: 10.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6543481020701675		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 0.6543481020701675 | validation: 0.7634877442932236]
	TIME [epoch: 10.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6466627759212875		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 0.6466627759212875 | validation: 0.9202625196396522]
	TIME [epoch: 10.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981044606555007		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 0.6981044606555007 | validation: 0.7654719750604926]
	TIME [epoch: 10.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6430492831382593		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 0.6430492831382593 | validation: 1.3577054574400973]
	TIME [epoch: 10.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0336392603627147		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 1.0336392603627147 | validation: 0.6939694876111067]
	TIME [epoch: 10.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6847942186273345		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 0.6847942186273345 | validation: 0.6617823709925985]
	TIME [epoch: 10.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6510371842574376		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 0.6510371842574376 | validation: 0.729202951569402]
	TIME [epoch: 10.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.633840667173814		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 0.633840667173814 | validation: 0.62708105447563]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6702671802104226		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 0.6702671802104226 | validation: 0.6406685446344795]
	TIME [epoch: 10.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6464188059126273		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 0.6464188059126273 | validation: 0.6195567094487809]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5888732837558213		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 0.5888732837558213 | validation: 0.5793965469491658]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6599221899038292		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 0.6599221899038292 | validation: 0.6255311550512523]
	TIME [epoch: 10.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0187814605319263		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 1.0187814605319263 | validation: 0.7031033999516433]
	TIME [epoch: 10.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6471596038596537		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 0.6471596038596537 | validation: 0.6108250310570223]
	TIME [epoch: 10.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6105661520120839		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 0.6105661520120839 | validation: 0.6686519831727068]
	TIME [epoch: 10.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926028533946414		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 0.6926028533946414 | validation: 0.9785832569305837]
	TIME [epoch: 10.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.648583309727627		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 0.648583309727627 | validation: 0.7138216214344227]
	TIME [epoch: 10.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7211789411994698		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 0.7211789411994698 | validation: 1.0185407751943023]
	TIME [epoch: 10.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7334179356872423		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 0.7334179356872423 | validation: 0.7495778636385515]
	TIME [epoch: 10.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6884263777895774		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 0.6884263777895774 | validation: 0.8698053022193952]
	TIME [epoch: 10.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6585076064118491		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 0.6585076064118491 | validation: 0.7495924183274955]
	TIME [epoch: 10.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.743964081577019		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 0.743964081577019 | validation: 0.730249541736325]
	TIME [epoch: 10.2 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7726672968377724		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 0.7726672968377724 | validation: 0.6979711906908487]
	TIME [epoch: 10.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.620802076245887		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 0.620802076245887 | validation: 0.8536894585038973]
	TIME [epoch: 10.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6634171595945058		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 0.6634171595945058 | validation: 0.804116434028768]
	TIME [epoch: 10.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6148818379509055		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 0.6148818379509055 | validation: 0.7448093912631696]
	TIME [epoch: 10.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6876533752936239		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 0.6876533752936239 | validation: 0.6618945672940291]
	TIME [epoch: 10.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010504825502726		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 0.7010504825502726 | validation: 0.6488489166110355]
	TIME [epoch: 10.2 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6710116084209625		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 0.6710116084209625 | validation: 1.0086221158503452]
	TIME [epoch: 10.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6472286533444656		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 0.6472286533444656 | validation: 0.787413310257357]
	TIME [epoch: 10.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5992595508002385		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 0.5992595508002385 | validation: 0.776801582269855]
	TIME [epoch: 10.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6239573106744077		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 0.6239573106744077 | validation: 0.6856901397756355]
	TIME [epoch: 10.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6269182244797272		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 0.6269182244797272 | validation: 0.6939229889376329]
	TIME [epoch: 10.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6780022419890617		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 0.6780022419890617 | validation: 0.7173710409410492]
	TIME [epoch: 10.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6795221611506366		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 0.6795221611506366 | validation: 0.7402373628706311]
	TIME [epoch: 10.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.671430489209899		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 0.671430489209899 | validation: 0.9949583963370665]
	TIME [epoch: 10.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215952381090283		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 0.7215952381090283 | validation: 0.697741987386253]
	TIME [epoch: 10.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6085639223786241		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 0.6085639223786241 | validation: 0.7709232240210928]
	TIME [epoch: 10.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6796595240095864		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 0.6796595240095864 | validation: 0.5314481630585365]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5709829351839476		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 0.5709829351839476 | validation: 0.6198938486717813]
	TIME [epoch: 10.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6272170890146236		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 0.6272170890146236 | validation: 0.8073886689712865]
	TIME [epoch: 10.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6277119819828572		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 0.6277119819828572 | validation: 0.8741561125908354]
	TIME [epoch: 10.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6760230575003936		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 0.6760230575003936 | validation: 0.7540352581759799]
	TIME [epoch: 10.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6497636874930354		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 0.6497636874930354 | validation: 0.8152532287488816]
	TIME [epoch: 10.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6223852370192405		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 0.6223852370192405 | validation: 0.9375347140268372]
	TIME [epoch: 10.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993556493772026		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 0.6993556493772026 | validation: 0.712130951654876]
	TIME [epoch: 10.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6658208211295576		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 0.6658208211295576 | validation: 1.053614918862152]
	TIME [epoch: 10.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7978904147777872		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 0.7978904147777872 | validation: 0.6807279552151507]
	TIME [epoch: 10.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6393385774061416		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 0.6393385774061416 | validation: 0.896773158544122]
	TIME [epoch: 10.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6326025026019302		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 0.6326025026019302 | validation: 0.567351785037944]
	TIME [epoch: 10.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708440627510743		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 0.708440627510743 | validation: 0.7081491325379909]
	TIME [epoch: 10.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7801390719987364		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 0.7801390719987364 | validation: 0.8530210694033139]
	TIME [epoch: 10.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6304475800733637		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 0.6304475800733637 | validation: 0.6945266126445838]
	TIME [epoch: 10.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5773463894887524		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 0.5773463894887524 | validation: 0.7122478728545159]
	TIME [epoch: 10.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6132540548924977		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 0.6132540548924977 | validation: 0.537834261661235]
	TIME [epoch: 10.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5888117201376886		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 0.5888117201376886 | validation: 0.5686626092414098]
	TIME [epoch: 10.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5533228336262294		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 0.5533228336262294 | validation: 0.610918117305056]
	TIME [epoch: 10.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.590904408447283		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 0.590904408447283 | validation: 0.6850704369164402]
	TIME [epoch: 10.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6160740825818032		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 0.6160740825818032 | validation: 0.6277007048466327]
	TIME [epoch: 10.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6022630717489055		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 0.6022630717489055 | validation: 0.7630455855166765]
	TIME [epoch: 10.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6796882330338115		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 0.6796882330338115 | validation: 0.6049266777970371]
	TIME [epoch: 10.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5630803460277773		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 0.5630803460277773 | validation: 0.688952647697983]
	TIME [epoch: 10.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5787228208307659		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 0.5787228208307659 | validation: 0.6596286673834364]
	TIME [epoch: 10.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6566890113814382		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 0.6566890113814382 | validation: 0.7736100975479008]
	TIME [epoch: 10.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6862367315310581		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 0.6862367315310581 | validation: 0.6513465262200575]
	TIME [epoch: 10.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6056690291921176		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 0.6056690291921176 | validation: 0.5319394226023472]
	TIME [epoch: 10.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5787328096344642		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 0.5787328096344642 | validation: 0.6513711576048075]
	TIME [epoch: 10.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.643249135671867		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 0.643249135671867 | validation: 0.5701727926588825]
	TIME [epoch: 10.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6092104637799866		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 0.6092104637799866 | validation: 0.6101643712763819]
	TIME [epoch: 10.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6097256724534822		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 0.6097256724534822 | validation: 0.6374981837470147]
	TIME [epoch: 10.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6203865757817884		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 0.6203865757817884 | validation: 0.6029929901613179]
	TIME [epoch: 10.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5686330784190275		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 0.5686330784190275 | validation: 0.6290722425705025]
	TIME [epoch: 10.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6012872190442359		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 0.6012872190442359 | validation: 0.7219112499966426]
	TIME [epoch: 10.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5864996296439673		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 0.5864996296439673 | validation: 0.7854459300301386]
	TIME [epoch: 10.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6084555386078142		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 0.6084555386078142 | validation: 0.5354129483034852]
	TIME [epoch: 10.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6274353877504668		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 0.6274353877504668 | validation: 0.7782069121756193]
	TIME [epoch: 10.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6096416775250901		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 0.6096416775250901 | validation: 0.5502204208138197]
	TIME [epoch: 10.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6499043094134226		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 0.6499043094134226 | validation: 0.8905123313902001]
	TIME [epoch: 10.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6045442463291152		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 0.6045442463291152 | validation: 0.5907812668119768]
	TIME [epoch: 10.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6042816438041779		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 0.6042816438041779 | validation: 0.5471174957789914]
	TIME [epoch: 10.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5565765417873656		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 0.5565765417873656 | validation: 0.6158773521072197]
	TIME [epoch: 10.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5870471416846311		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 0.5870471416846311 | validation: 0.4812241903902064]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6016002946390193		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 0.6016002946390193 | validation: 0.5596075975468511]
	TIME [epoch: 10.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5690080709960613		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 0.5690080709960613 | validation: 0.5547226417916264]
	TIME [epoch: 10.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5598482213950434		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 0.5598482213950434 | validation: 0.6433779630324729]
	TIME [epoch: 10.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5676014389898418		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 0.5676014389898418 | validation: 0.6209723339517812]
	TIME [epoch: 10.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6248515737437307		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 0.6248515737437307 | validation: 0.5547382910585593]
	TIME [epoch: 10.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6417342437173567		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 0.6417342437173567 | validation: 0.5894201317453254]
	TIME [epoch: 10.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5799637203619185		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 0.5799637203619185 | validation: 0.589489938142148]
	TIME [epoch: 10.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5571510903617386		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 0.5571510903617386 | validation: 0.5599298741165266]
	TIME [epoch: 10.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5966547824087922		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 0.5966547824087922 | validation: 0.6645086416233019]
	TIME [epoch: 10.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5558735691017155		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 0.5558735691017155 | validation: 0.6674387441130581]
	TIME [epoch: 10.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.570776664190198		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 0.570776664190198 | validation: 0.8731748427197564]
	TIME [epoch: 10.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332918538314635		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 0.6332918538314635 | validation: 0.6717179340423627]
	TIME [epoch: 10.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5538127818641094		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 0.5538127818641094 | validation: 0.6401326349605404]
	TIME [epoch: 10.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6221586296816355		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 0.6221586296816355 | validation: 0.767571622423841]
	TIME [epoch: 10.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.62993641906076		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 0.62993641906076 | validation: 1.1422364350908794]
	TIME [epoch: 10.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7419847132396765		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 0.7419847132396765 | validation: 0.5225595104910417]
	TIME [epoch: 10.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6394453729528602		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 0.6394453729528602 | validation: 0.617260775985468]
	TIME [epoch: 10.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6349870646673944		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 0.6349870646673944 | validation: 0.59399838449867]
	TIME [epoch: 10.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5810844741844086		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 0.5810844741844086 | validation: 0.6045525165572341]
	TIME [epoch: 10.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5435135596644163		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 0.5435135596644163 | validation: 0.6444997505169872]
	TIME [epoch: 10.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5921154729186698		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 0.5921154729186698 | validation: 0.5549367528902629]
	TIME [epoch: 10.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6217416458139015		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 0.6217416458139015 | validation: 0.5603286224533841]
	TIME [epoch: 10.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6019866302327583		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 0.6019866302327583 | validation: 0.6999974212503909]
	TIME [epoch: 10.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5800414693061603		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 0.5800414693061603 | validation: 0.7330850796062373]
	TIME [epoch: 10.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6638914645781812		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 0.6638914645781812 | validation: 0.7391067245758837]
	TIME [epoch: 10.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5966976446412746		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 0.5966976446412746 | validation: 0.5379272848943191]
	TIME [epoch: 10.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6239005105226803		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 0.6239005105226803 | validation: 0.5984349019194676]
	TIME [epoch: 10.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6548314460710832		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 0.6548314460710832 | validation: 0.5261131826468326]
	TIME [epoch: 10.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7562560102607678		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 0.7562560102607678 | validation: 0.7140374180949646]
	TIME [epoch: 10.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5658568237235319		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 0.5658568237235319 | validation: 0.5845931647305028]
	TIME [epoch: 10.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6109853378441372		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 0.6109853378441372 | validation: 0.6838909440702817]
	TIME [epoch: 10.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5363181665279807		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 0.5363181665279807 | validation: 0.6643356207701641]
	TIME [epoch: 10.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5770366258200041		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 0.5770366258200041 | validation: 1.000320668381372]
	TIME [epoch: 10.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7132719338630891		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 0.7132719338630891 | validation: 0.7758640881016423]
	TIME [epoch: 10.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5551080418094652		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 0.5551080418094652 | validation: 0.6449864752405597]
	TIME [epoch: 10.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.570005027201335		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 0.570005027201335 | validation: 0.5139637646988069]
	TIME [epoch: 10.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5643533356546971		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 0.5643533356546971 | validation: 0.6409178483867949]
	TIME [epoch: 10.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.552063520027821		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 0.552063520027821 | validation: 0.7833865896186448]
	TIME [epoch: 10.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5373838171783794		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 0.5373838171783794 | validation: 0.6896751341193167]
	TIME [epoch: 10.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.572035222376791		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 0.572035222376791 | validation: 0.618844555988115]
	TIME [epoch: 10.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5570680319481089		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 0.5570680319481089 | validation: 0.6541573185211362]
	TIME [epoch: 10.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7286225521869893		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 0.7286225521869893 | validation: 0.8199782202015695]
	TIME [epoch: 10.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6278710853993978		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 0.6278710853993978 | validation: 0.46302598426893654]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5266960979129013		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 0.5266960979129013 | validation: 0.5445779271972114]
	TIME [epoch: 10.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.498089716151067		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 0.498089716151067 | validation: 0.6702426992036487]
	TIME [epoch: 10.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5438876236291599		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 0.5438876236291599 | validation: 0.5227260366338181]
	TIME [epoch: 10.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6012126280503984		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 0.6012126280503984 | validation: 0.5136669717636932]
	TIME [epoch: 10.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5233030836693449		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 0.5233030836693449 | validation: 0.5652775321617182]
	TIME [epoch: 10.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5199813510863768		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 0.5199813510863768 | validation: 0.5155596200549492]
	TIME [epoch: 10.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5388507627251414		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 0.5388507627251414 | validation: 0.6789614394310377]
	TIME [epoch: 10.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6324561395743002		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 0.6324561395743002 | validation: 0.5872209165115292]
	TIME [epoch: 10.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5261576547544891		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 0.5261576547544891 | validation: 0.602447816006383]
	TIME [epoch: 10.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5797942927658939		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 0.5797942927658939 | validation: 0.5694501947661907]
	TIME [epoch: 10.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5772693350107311		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 0.5772693350107311 | validation: 0.6226609715091014]
	TIME [epoch: 10.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5449611896835361		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 0.5449611896835361 | validation: 0.6868048381986002]
	TIME [epoch: 10.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5787325020971226		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 0.5787325020971226 | validation: 0.6384279949775933]
	TIME [epoch: 10.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5809927047456718		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 0.5809927047456718 | validation: 0.7874610004960664]
	TIME [epoch: 10.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6225919071457977		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 0.6225919071457977 | validation: 0.595213772406634]
	TIME [epoch: 10.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.57148409231764		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 0.57148409231764 | validation: 0.5864561528984312]
	TIME [epoch: 10.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5576456933483984		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 0.5576456933483984 | validation: 0.7724343081368117]
	TIME [epoch: 10.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69504004820699		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 0.69504004820699 | validation: 0.5111024798965402]
	TIME [epoch: 10.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5299178809765962		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 0.5299178809765962 | validation: 0.517839598321317]
	TIME [epoch: 10.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47441546764001397		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 0.47441546764001397 | validation: 0.5340289815392272]
	TIME [epoch: 10.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5274463327020016		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 0.5274463327020016 | validation: 0.5480448357267182]
	TIME [epoch: 10.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6261355089835434		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 0.6261355089835434 | validation: 0.801045682071374]
	TIME [epoch: 10.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5980338904635042		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 0.5980338904635042 | validation: 0.4831536483044015]
	TIME [epoch: 10.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5415092620471786		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 0.5415092620471786 | validation: 0.6034107209676393]
	TIME [epoch: 10.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5997139683257358		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 0.5997139683257358 | validation: 0.5398734856294216]
	TIME [epoch: 10.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5624050923715443		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 0.5624050923715443 | validation: 0.8182822714092014]
	TIME [epoch: 10.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6052259091464608		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 0.6052259091464608 | validation: 0.7076666927167884]
	TIME [epoch: 10.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5767322095978487		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 0.5767322095978487 | validation: 0.766330172834994]
	TIME [epoch: 10.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5527791824262891		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 0.5527791824262891 | validation: 0.7297080753773261]
	TIME [epoch: 10.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.585621763988571		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 0.585621763988571 | validation: 0.6484273616538171]
	TIME [epoch: 10.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5605596585803035		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 0.5605596585803035 | validation: 0.717073939664472]
	TIME [epoch: 10.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5295992620285151		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 0.5295992620285151 | validation: 0.6808197643695819]
	TIME [epoch: 10.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5032704299775365		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 0.5032704299775365 | validation: 0.5864847285893863]
	TIME [epoch: 10.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6550210258842428		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 0.6550210258842428 | validation: 0.5740579490524729]
	TIME [epoch: 10.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5584082720526533		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 0.5584082720526533 | validation: 0.6851602653439758]
	TIME [epoch: 10.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5468836353599648		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 0.5468836353599648 | validation: 0.7363383977440668]
	TIME [epoch: 10.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5422093887977885		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 0.5422093887977885 | validation: 0.5602194276578422]
	TIME [epoch: 10.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.571127058250058		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 0.571127058250058 | validation: 0.6385092386011086]
	TIME [epoch: 10.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5265167374796015		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 0.5265167374796015 | validation: 0.6894586636176838]
	TIME [epoch: 10.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5348779218802611		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 0.5348779218802611 | validation: 0.5777360492921922]
	TIME [epoch: 10.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5826284014317589		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 0.5826284014317589 | validation: 0.7123958216883601]
	TIME [epoch: 10.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5402660918989257		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 0.5402660918989257 | validation: 0.610862111567256]
	TIME [epoch: 10.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5193446090489064		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 0.5193446090489064 | validation: 0.6756435185304153]
	TIME [epoch: 10.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5387934995793047		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 0.5387934995793047 | validation: 0.6198762531376139]
	TIME [epoch: 10.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5146875240680405		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 0.5146875240680405 | validation: 0.667934962367842]
	TIME [epoch: 10.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5477884283029205		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 0.5477884283029205 | validation: 0.6964930553592626]
	TIME [epoch: 10.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5371027292378129		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 0.5371027292378129 | validation: 0.4718280444570774]
	TIME [epoch: 10.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.518954445986852		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 0.518954445986852 | validation: 0.6561596988831937]
	TIME [epoch: 10.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5204530083094342		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 0.5204530083094342 | validation: 0.591396043395522]
	TIME [epoch: 10.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5190214699495594		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 0.5190214699495594 | validation: 0.5618745912139067]
	TIME [epoch: 10.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6454411314310212		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 0.6454411314310212 | validation: 0.5440682757532038]
	TIME [epoch: 10.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5333980845307738		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 0.5333980845307738 | validation: 0.4376257905401473]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6803182890102224		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 0.6803182890102224 | validation: 0.5447965127309482]
	TIME [epoch: 10.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5200395243324131		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 0.5200395243324131 | validation: 0.4424538368382926]
	TIME [epoch: 10.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150228314545175		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 0.6150228314545175 | validation: 0.7602151519717927]
	TIME [epoch: 10.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5843601653097836		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 0.5843601653097836 | validation: 0.6281105215848096]
	TIME [epoch: 10.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5761496563098445		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 0.5761496563098445 | validation: 0.6133362122170937]
	TIME [epoch: 10.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4854518425858697		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 0.4854518425858697 | validation: 0.4875346413752029]
	TIME [epoch: 10.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5128147699137194		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 0.5128147699137194 | validation: 0.6186830516672596]
	TIME [epoch: 10.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5084163671966002		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 0.5084163671966002 | validation: 0.5725401577693822]
	TIME [epoch: 10.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5373470581156099		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 0.5373470581156099 | validation: 0.5471490985322491]
	TIME [epoch: 10.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4954575721465817		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 0.4954575721465817 | validation: 0.6110220970161035]
	TIME [epoch: 10.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.543232160328707		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 0.543232160328707 | validation: 0.5104340775059056]
	TIME [epoch: 10.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5147695523343465		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 0.5147695523343465 | validation: 0.6566905117152143]
	TIME [epoch: 10.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.552639836381142		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 0.552639836381142 | validation: 0.6016535018209358]
	TIME [epoch: 10.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.516344158435984		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 0.516344158435984 | validation: 0.5145680889242505]
	TIME [epoch: 10.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.511966457453924		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 0.511966457453924 | validation: 0.5667646555964128]
	TIME [epoch: 10.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5054698157462413		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 0.5054698157462413 | validation: 0.4880519846283697]
	TIME [epoch: 10.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.533611118585372		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 0.533611118585372 | validation: 0.5076648066968913]
	TIME [epoch: 10.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4906075098419144		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 0.4906075098419144 | validation: 0.4722673955100497]
	TIME [epoch: 10.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5415357998997969		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 0.5415357998997969 | validation: 0.6592586491066698]
	TIME [epoch: 10.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5260396697822605		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 0.5260396697822605 | validation: 0.5371543349020823]
	TIME [epoch: 10.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6649668889507071		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 0.6649668889507071 | validation: 1.391229555948395]
	TIME [epoch: 10.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8127635685017667		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 0.8127635685017667 | validation: 0.49726415408046976]
	TIME [epoch: 10.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4962928532698422		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 0.4962928532698422 | validation: 0.5291768227285789]
	TIME [epoch: 10.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45341992399449466		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 0.45341992399449466 | validation: 0.4416935799142749]
	TIME [epoch: 10.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5173730942427092		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 0.5173730942427092 | validation: 0.5077744377080963]
	TIME [epoch: 10.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5310513368393053		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 0.5310513368393053 | validation: 0.6103795235919396]
	TIME [epoch: 10.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4934669755980254		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 0.4934669755980254 | validation: 0.5063713519646373]
	TIME [epoch: 10.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4914265769795696		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 0.4914265769795696 | validation: 0.5551258457220429]
	TIME [epoch: 10.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48125029159328897		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 0.48125029159328897 | validation: 0.49733419262221673]
	TIME [epoch: 10.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5385520303218181		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 0.5385520303218181 | validation: 0.5846722740057549]
	TIME [epoch: 10.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.531956036570738		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 0.531956036570738 | validation: 0.5378773704002171]
	TIME [epoch: 10.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4731374661871288		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 0.4731374661871288 | validation: 0.49386008519030455]
	TIME [epoch: 10.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4815703055178265		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 0.4815703055178265 | validation: 0.47411919722668655]
	TIME [epoch: 10.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5314552176759676		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 0.5314552176759676 | validation: 0.5486252014337123]
	TIME [epoch: 10.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5175858094462716		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 0.5175858094462716 | validation: 0.553224160838207]
	TIME [epoch: 10.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4671288119543342		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 0.4671288119543342 | validation: 0.4433163934523197]
	TIME [epoch: 10.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5257769713498625		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 0.5257769713498625 | validation: 0.5516780950837393]
	TIME [epoch: 10.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5004549552651186		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 0.5004549552651186 | validation: 0.4359214136256116]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4544340006712009		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 0.4544340006712009 | validation: 0.4551621941551224]
	TIME [epoch: 10.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5143830461320389		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 0.5143830461320389 | validation: 0.6015677524508041]
	TIME [epoch: 10.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4991619936639896		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 0.4991619936639896 | validation: 0.5094112101633708]
	TIME [epoch: 10.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5105461536713788		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 0.5105461536713788 | validation: 0.5282565017841578]
	TIME [epoch: 10.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5153854712001631		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 0.5153854712001631 | validation: 0.6833124837359804]
	TIME [epoch: 10.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5013848025790589		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 0.5013848025790589 | validation: 0.7285262361496057]
	TIME [epoch: 10.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5704961225856339		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 0.5704961225856339 | validation: 0.7466360768411127]
	TIME [epoch: 10.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5618333353103324		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 0.5618333353103324 | validation: 0.7473952680824472]
	TIME [epoch: 10.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5907410153670425		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 0.5907410153670425 | validation: 0.6125430568538685]
	TIME [epoch: 10.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5128988315175338		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 0.5128988315175338 | validation: 0.6902391025382479]
	TIME [epoch: 10.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5308405379683216		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 0.5308405379683216 | validation: 0.6474439071993612]
	TIME [epoch: 10.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49194582312903934		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 0.49194582312903934 | validation: 0.6706817485241955]
	TIME [epoch: 10.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5387267465246002		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 0.5387267465246002 | validation: 0.6719711583315333]
	TIME [epoch: 10.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5364412850989811		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 0.5364412850989811 | validation: 0.7135314938604294]
	TIME [epoch: 10.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5296539920665637		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 0.5296539920665637 | validation: 0.6878253981882617]
	TIME [epoch: 10.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5064703507247764		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 0.5064703507247764 | validation: 0.5947121427399767]
	TIME [epoch: 10.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5471351205733552		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 0.5471351205733552 | validation: 0.5811141799457419]
	TIME [epoch: 10.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5398493902115291		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 0.5398493902115291 | validation: 0.5889727595602317]
	TIME [epoch: 10.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5134182752447225		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 0.5134182752447225 | validation: 0.7015282402080593]
	TIME [epoch: 10.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.520841249307849		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 0.520841249307849 | validation: 0.626880055235716]
	TIME [epoch: 10.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5034826158807546		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 0.5034826158807546 | validation: 0.5307047561480862]
	TIME [epoch: 10.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5326434649191231		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 0.5326434649191231 | validation: 0.6218735715800298]
	TIME [epoch: 10.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48637823672813935		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 0.48637823672813935 | validation: 0.558306435139083]
	TIME [epoch: 10.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5235767505290402		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 0.5235767505290402 | validation: 0.632185637340886]
	TIME [epoch: 10.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5279161875260804		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 0.5279161875260804 | validation: 0.6283181492642171]
	TIME [epoch: 10.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.553388368184099		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 0.553388368184099 | validation: 0.612776029230139]
	TIME [epoch: 10.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5588038768540098		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 0.5588038768540098 | validation: 0.5371911424309818]
	TIME [epoch: 10.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5293819865284681		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 0.5293819865284681 | validation: 0.5092269925713324]
	TIME [epoch: 10.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47350327422685395		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 0.47350327422685395 | validation: 0.47904042796642593]
	TIME [epoch: 10.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48723410976839315		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 0.48723410976839315 | validation: 0.5148063982518497]
	TIME [epoch: 10.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48018413313541836		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 0.48018413313541836 | validation: 0.5213828925597749]
	TIME [epoch: 10.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47506143066821876		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 0.47506143066821876 | validation: 0.5293328101236402]
	TIME [epoch: 10.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.474952237579061		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 0.474952237579061 | validation: 0.5485302230890655]
	TIME [epoch: 10.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5312602819605253		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 0.5312602819605253 | validation: 0.7429640674332854]
	TIME [epoch: 10.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5509344491720002		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 0.5509344491720002 | validation: 0.4788865763167726]
	TIME [epoch: 10.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4959892709038517		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 0.4959892709038517 | validation: 0.5293338674658041]
	TIME [epoch: 10.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5010181901107019		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 0.5010181901107019 | validation: 0.47355644604256747]
	TIME [epoch: 10.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4609188120495939		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 0.4609188120495939 | validation: 0.5377562923578327]
	TIME [epoch: 10.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4651581295178742		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 0.4651581295178742 | validation: 0.52184365403713]
	TIME [epoch: 10.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5000530854147828		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 0.5000530854147828 | validation: 0.43013670927365655]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47760134995904957		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 0.47760134995904957 | validation: 0.5054279374489338]
	TIME [epoch: 10.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5213404642065771		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 0.5213404642065771 | validation: 0.5579704822380849]
	TIME [epoch: 10.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5056165950273931		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 0.5056165950273931 | validation: 0.47014521926881314]
	TIME [epoch: 10.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4451716345840911		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 0.4451716345840911 | validation: 0.4817316320406805]
	TIME [epoch: 10.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48200388697183233		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 0.48200388697183233 | validation: 0.4545735084978236]
	TIME [epoch: 10.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4617818234652731		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 0.4617818234652731 | validation: 0.47256690569116727]
	TIME [epoch: 10.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5156738244167707		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 0.5156738244167707 | validation: 0.4578466096827436]
	TIME [epoch: 10.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5050376965658252		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 0.5050376965658252 | validation: 0.47077841387879543]
	TIME [epoch: 10.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46066796190192266		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 0.46066796190192266 | validation: 0.450806205807028]
	TIME [epoch: 10.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4421752917910903		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 0.4421752917910903 | validation: 0.4526318749392777]
	TIME [epoch: 10.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46506120752269897		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 0.46506120752269897 | validation: 0.48069980085716907]
	TIME [epoch: 10.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.539139969086021		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 0.539139969086021 | validation: 0.5063715436624212]
	TIME [epoch: 10.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4652751128706905		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 0.4652751128706905 | validation: 0.46942794150344497]
	TIME [epoch: 10.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45999213457355753		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 0.45999213457355753 | validation: 0.5103243210442575]
	TIME [epoch: 10.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.449858520760693		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 0.449858520760693 | validation: 0.4624597863713471]
	TIME [epoch: 10.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4996764425820378		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 0.4996764425820378 | validation: 0.4388526123007144]
	TIME [epoch: 10.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45231803695217626		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 0.45231803695217626 | validation: 0.5096848509644163]
	TIME [epoch: 10.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46141202844182355		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 0.46141202844182355 | validation: 0.4490503523025488]
	TIME [epoch: 10.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47267081748042117		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 0.47267081748042117 | validation: 0.46257528754906874]
	TIME [epoch: 10.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5291556004125718		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 0.5291556004125718 | validation: 0.4567644328668552]
	TIME [epoch: 10.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6538396336882303		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 0.6538396336882303 | validation: 0.559257073946958]
	TIME [epoch: 10.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5578311689379276		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 0.5578311689379276 | validation: 0.6168559722439269]
	TIME [epoch: 10.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4803902094449949		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 0.4803902094449949 | validation: 0.44887474390122395]
	TIME [epoch: 10.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41885406024159855		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 0.41885406024159855 | validation: 0.5118215344707645]
	TIME [epoch: 10.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49749904928995103		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 0.49749904928995103 | validation: 0.43405784876672804]
	TIME [epoch: 10.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4637200648212719		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 0.4637200648212719 | validation: 0.5204911010659392]
	TIME [epoch: 10.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45969838339571656		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 0.45969838339571656 | validation: 0.44386828319586835]
	TIME [epoch: 10.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4840352247374059		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 0.4840352247374059 | validation: 0.5325697999313493]
	TIME [epoch: 10.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4648420304432744		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 0.4648420304432744 | validation: 0.4269697989337074]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4421753535986559		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 0.4421753535986559 | validation: 0.4194343353843026]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45459399228505626		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 0.45459399228505626 | validation: 0.5097715141835697]
	TIME [epoch: 10.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4655483978395819		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 0.4655483978395819 | validation: 0.4224648321740256]
	TIME [epoch: 10.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45846388636657026		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 0.45846388636657026 | validation: 0.4659876395763267]
	TIME [epoch: 10.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44820049245256266		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 0.44820049245256266 | validation: 0.43503543816205054]
	TIME [epoch: 10.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5309197115080895		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 0.5309197115080895 | validation: 0.5410886209316403]
	TIME [epoch: 10.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46567789950989935		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 0.46567789950989935 | validation: 0.4540645871224811]
	TIME [epoch: 10.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.535453855054296		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 0.535453855054296 | validation: 0.4998646471805824]
	TIME [epoch: 10.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48973358494043245		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 0.48973358494043245 | validation: 0.6021541898308052]
	TIME [epoch: 10.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4984194200689199		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 0.4984194200689199 | validation: 0.5590367857995783]
	TIME [epoch: 10.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4671247976747174		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 0.4671247976747174 | validation: 0.44635813749660913]
	TIME [epoch: 10.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47318650295339665		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 0.47318650295339665 | validation: 0.5459239874914692]
	TIME [epoch: 10.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5168663839306762		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 0.5168663839306762 | validation: 0.5845149654310484]
	TIME [epoch: 10.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4509656207595544		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 0.4509656207595544 | validation: 0.4671531370902633]
	TIME [epoch: 10.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44278280827479605		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 0.44278280827479605 | validation: 0.5045225288387649]
	TIME [epoch: 10.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47925968216585846		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 0.47925968216585846 | validation: 0.4181306339622563]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45239767924940094		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 0.45239767924940094 | validation: 0.4431058070322493]
	TIME [epoch: 10.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5004567879894501		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 0.5004567879894501 | validation: 0.47159624316231125]
	TIME [epoch: 10.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4438972810458832		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 0.4438972810458832 | validation: 0.4022044292292948]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45292365431622106		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 0.45292365431622106 | validation: 0.4569378428279675]
	TIME [epoch: 10.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45083346479361824		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 0.45083346479361824 | validation: 0.39263572340743513]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48132239436898827		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 0.48132239436898827 | validation: 0.6238903128374481]
	TIME [epoch: 10.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4772061284620822		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 0.4772061284620822 | validation: 0.5018528586557972]
	TIME [epoch: 10.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.497708797222983		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 0.497708797222983 | validation: 0.4901297611032077]
	TIME [epoch: 10.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45207246955560115		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 0.45207246955560115 | validation: 0.5717346235910066]
	TIME [epoch: 10.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45595720245511073		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 0.45595720245511073 | validation: 0.46608050168014886]
	TIME [epoch: 10.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4344705698888681		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 0.4344705698888681 | validation: 0.4331766172170968]
	TIME [epoch: 10.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4701180734082512		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 0.4701180734082512 | validation: 0.42267707202241284]
	TIME [epoch: 10.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8277695388477463		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 0.8277695388477463 | validation: 1.0384447771107044]
	TIME [epoch: 10.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8919654697698748		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 0.8919654697698748 | validation: 0.45970348236109415]
	TIME [epoch: 10.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4819885613124826		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 0.4819885613124826 | validation: 0.42947092523772235]
	TIME [epoch: 10.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4747402956072827		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 0.4747402956072827 | validation: 0.5135675214515032]
	TIME [epoch: 10.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4671704626924306		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 0.4671704626924306 | validation: 0.5441607071396399]
	TIME [epoch: 10.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.437196310979483		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 0.437196310979483 | validation: 0.5143102494915054]
	TIME [epoch: 10.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4496573249905538		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 0.4496573249905538 | validation: 0.43729846981568515]
	TIME [epoch: 10.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42803564591061527		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 0.42803564591061527 | validation: 0.4092077956132978]
	TIME [epoch: 10.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4423509969700127		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 0.4423509969700127 | validation: 0.461982638340434]
	TIME [epoch: 10.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4630898908780991		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 0.4630898908780991 | validation: 0.5096698924761085]
	TIME [epoch: 10.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4421419824486783		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 0.4421419824486783 | validation: 0.43637160248913853]
	TIME [epoch: 10.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47190874356443846		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 0.47190874356443846 | validation: 0.42334749439993474]
	TIME [epoch: 10.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4734781801635702		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 0.4734781801635702 | validation: 0.42954590444359825]
	TIME [epoch: 10.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4782460705043232		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 0.4782460705043232 | validation: 0.474389941739481]
	TIME [epoch: 10.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47700190144008003		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 0.47700190144008003 | validation: 0.6728318389483635]
	TIME [epoch: 10.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49967136527959577		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 0.49967136527959577 | validation: 0.4858543544171292]
	TIME [epoch: 10.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4694245396166556		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 0.4694245396166556 | validation: 0.4026149064852817]
	TIME [epoch: 10.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4438968695320285		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 0.4438968695320285 | validation: 0.47305657086422026]
	TIME [epoch: 10.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4921769292949475		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 0.4921769292949475 | validation: 0.5194983473412811]
	TIME [epoch: 10.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49981891784421417		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 0.49981891784421417 | validation: 0.7521546271410003]
	TIME [epoch: 10.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7120163165753667		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 0.7120163165753667 | validation: 0.40875663455644146]
	TIME [epoch: 10.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5145679128793332		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 0.5145679128793332 | validation: 0.4001755896463102]
	TIME [epoch: 10.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.445516332412346		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 0.445516332412346 | validation: 0.46198302623668325]
	TIME [epoch: 10.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44392025242324895		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 0.44392025242324895 | validation: 0.4551633859160205]
	TIME [epoch: 10.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43404264149053107		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 0.43404264149053107 | validation: 0.42894274920806624]
	TIME [epoch: 10.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4441550749073455		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 0.4441550749073455 | validation: 0.4271996869016625]
	TIME [epoch: 10.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41659140285498736		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 0.41659140285498736 | validation: 0.5752263568917856]
	TIME [epoch: 10.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4870906864779349		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 0.4870906864779349 | validation: 0.5359584223792365]
	TIME [epoch: 10.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5061410236895079		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 0.5061410236895079 | validation: 0.5308892440941985]
	TIME [epoch: 10.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46224598998695166		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 0.46224598998695166 | validation: 0.5606512149110541]
	TIME [epoch: 10.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4766204524513829		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 0.4766204524513829 | validation: 0.550864433213486]
	TIME [epoch: 10.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46110096186228844		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 0.46110096186228844 | validation: 0.49420298619125447]
	TIME [epoch: 10.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4513813206988531		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 0.4513813206988531 | validation: 0.43899095878072003]
	TIME [epoch: 10.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46311274150367226		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 0.46311274150367226 | validation: 0.47157396079925834]
	TIME [epoch: 10.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45854258086463606		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 0.45854258086463606 | validation: 0.4657680593118299]
	TIME [epoch: 10.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44899247395548836		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 0.44899247395548836 | validation: 0.4658394322601957]
	TIME [epoch: 10.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46672640475834204		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 0.46672640475834204 | validation: 0.6095993759907283]
	TIME [epoch: 10.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5140511771857864		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 0.5140511771857864 | validation: 0.5157784531389271]
	TIME [epoch: 10.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46560278789381665		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 0.46560278789381665 | validation: 0.42056258701435084]
	TIME [epoch: 10.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43157044184388643		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 0.43157044184388643 | validation: 0.3986647253871138]
	TIME [epoch: 10.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4580774085067757		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 0.4580774085067757 | validation: 0.6155343728941461]
	TIME [epoch: 10.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49379857158826657		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 0.49379857158826657 | validation: 0.512417118861389]
	TIME [epoch: 10.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45716414306175024		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 0.45716414306175024 | validation: 0.5604143852488632]
	TIME [epoch: 10.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48344646139825376		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 0.48344646139825376 | validation: 0.4197819648543504]
	TIME [epoch: 10.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4690311221397424		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 0.4690311221397424 | validation: 0.6316398761747636]
	TIME [epoch: 10.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49716194345037756		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 0.49716194345037756 | validation: 0.41934252294658464]
	TIME [epoch: 10.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4385402113953022		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 0.4385402113953022 | validation: 0.4589144509944721]
	TIME [epoch: 10.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4262407901463803		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 0.4262407901463803 | validation: 0.44489568972252935]
	TIME [epoch: 10.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4230019592592549		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 0.4230019592592549 | validation: 0.4649928595987575]
	TIME [epoch: 10.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45629268630226705		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 0.45629268630226705 | validation: 0.4900492989625605]
	TIME [epoch: 10.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4544998714380536		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 0.4544998714380536 | validation: 0.5087800247366954]
	TIME [epoch: 10.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43506702148837484		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 0.43506702148837484 | validation: 0.5025179632599978]
	TIME [epoch: 10.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4416243223977247		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 0.4416243223977247 | validation: 0.4229075804975052]
	TIME [epoch: 10.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48516271347747464		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 0.48516271347747464 | validation: 0.49375947921771846]
	TIME [epoch: 10.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45715478859628955		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 0.45715478859628955 | validation: 0.5978536385687703]
	TIME [epoch: 10.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49781532354722985		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 0.49781532354722985 | validation: 0.45680422125117665]
	TIME [epoch: 10.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.451865585211401		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 0.451865585211401 | validation: 0.4752659709922818]
	TIME [epoch: 10.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42245092197949274		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 0.42245092197949274 | validation: 0.4178478936819225]
	TIME [epoch: 10.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46949974895871166		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 0.46949974895871166 | validation: 0.4381917331480203]
	TIME [epoch: 10.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43243504689636375		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 0.43243504689636375 | validation: 0.4365172010635836]
	TIME [epoch: 10.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44251531029593244		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 0.44251531029593244 | validation: 0.5830690679536157]
	TIME [epoch: 10.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46569242519122556		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 0.46569242519122556 | validation: 0.47353491071633963]
	TIME [epoch: 10.2 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4376417990487281		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 0.4376417990487281 | validation: 0.523448775342378]
	TIME [epoch: 10.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45151547935937586		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 0.45151547935937586 | validation: 0.47632499935572553]
	TIME [epoch: 10.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47493124576327705		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 0.47493124576327705 | validation: 0.45687435994474557]
	TIME [epoch: 10.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48073065877487303		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 0.48073065877487303 | validation: 0.4740830333685348]
	TIME [epoch: 10.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43137889178934535		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 0.43137889178934535 | validation: 0.47625514207100816]
	TIME [epoch: 10.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47861605893706455		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 0.47861605893706455 | validation: 0.5303069788818554]
	TIME [epoch: 10.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44183909101767993		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 0.44183909101767993 | validation: 0.42257845233778424]
	TIME [epoch: 10.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4215622418065075		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 0.4215622418065075 | validation: 0.5161811825309188]
	TIME [epoch: 10.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5209659005457434		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 0.5209659005457434 | validation: 0.5927544005138862]
	TIME [epoch: 10.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4908927279887809		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 0.4908927279887809 | validation: 0.6136603440136608]
	TIME [epoch: 10.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5035667676062014		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 0.5035667676062014 | validation: 0.5260895086183272]
	TIME [epoch: 10.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4756085297030225		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 0.4756085297030225 | validation: 0.5701267021352936]
	TIME [epoch: 10.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4645913789105281		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 0.4645913789105281 | validation: 0.5919512868739917]
	TIME [epoch: 10.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.453916629651265		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 0.453916629651265 | validation: 0.5515388582336845]
	TIME [epoch: 10.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49806149115520737		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 0.49806149115520737 | validation: 0.48599802075525533]
	TIME [epoch: 10.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4696021037054763		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 0.4696021037054763 | validation: 0.5327445382527265]
	TIME [epoch: 10.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4719555066292266		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 0.4719555066292266 | validation: 0.47635920086377936]
	TIME [epoch: 10.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47954379146277837		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 0.47954379146277837 | validation: 0.5163720243148818]
	TIME [epoch: 10.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46775893339584834		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 0.46775893339584834 | validation: 0.5091194218176753]
	TIME [epoch: 10.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47048323705832534		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 0.47048323705832534 | validation: 0.46896716342647443]
	TIME [epoch: 10.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45924835583178825		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 0.45924835583178825 | validation: 0.6015940884560549]
	TIME [epoch: 10.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46374806097155863		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 0.46374806097155863 | validation: 0.5096648375740203]
	TIME [epoch: 10.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45293498140783883		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 0.45293498140783883 | validation: 0.560466984829587]
	TIME [epoch: 10.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43935397486089034		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 0.43935397486089034 | validation: 0.42506696807071026]
	TIME [epoch: 10.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.435585848135461		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 0.435585848135461 | validation: 0.5240803153987744]
	TIME [epoch: 10.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43753135980555635		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 0.43753135980555635 | validation: 0.3931859992299355]
	TIME [epoch: 10.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44145182592728743		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 0.44145182592728743 | validation: 0.4178496942125238]
	TIME [epoch: 10.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4157494317957286		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 0.4157494317957286 | validation: 0.5094182772069422]
	TIME [epoch: 10.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4483830086731344		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 0.4483830086731344 | validation: 0.4205362482662381]
	TIME [epoch: 10.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3956604819653059		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 0.3956604819653059 | validation: 0.4557482123605287]
	TIME [epoch: 10.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4931899985330658		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 0.4931899985330658 | validation: 0.40883216616502843]
	TIME [epoch: 10.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4498676060441042		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 0.4498676060441042 | validation: 0.4582187531316221]
	TIME [epoch: 10.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44679120976929515		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 0.44679120976929515 | validation: 0.4677381915528506]
	TIME [epoch: 10.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45824668235897387		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 0.45824668235897387 | validation: 0.39657114090440404]
	TIME [epoch: 10.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42233553508982274		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 0.42233553508982274 | validation: 0.46532704731889296]
	TIME [epoch: 10.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45900355196956094		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 0.45900355196956094 | validation: 0.4306393354697272]
	TIME [epoch: 10.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4665185329456637		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 0.4665185329456637 | validation: 0.4153786184820068]
	TIME [epoch: 10.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43235948492559173		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 0.43235948492559173 | validation: 0.37614953030809417]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5143110518498597		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 0.5143110518498597 | validation: 0.4829897251513299]
	TIME [epoch: 10.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4723766857969853		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 0.4723766857969853 | validation: 0.5544510595916176]
	TIME [epoch: 10.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4588047173173114		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 0.4588047173173114 | validation: 0.4248118948717663]
	TIME [epoch: 10.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4587021871238031		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 0.4587021871238031 | validation: 0.49800567941505036]
	TIME [epoch: 10.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4675560391582293		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 0.4675560391582293 | validation: 0.4583111091014024]
	TIME [epoch: 10.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4380412188408049		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 0.4380412188408049 | validation: 0.4411615994526622]
	TIME [epoch: 10.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45092009429201624		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 0.45092009429201624 | validation: 0.44080046615407265]
	TIME [epoch: 10.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4388333501891989		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 0.4388333501891989 | validation: 0.47952506602088873]
	TIME [epoch: 10.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47249094045303863		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 0.47249094045303863 | validation: 0.4697596503867382]
	TIME [epoch: 10.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46512849687854124		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 0.46512849687854124 | validation: 0.3951082181902081]
	TIME [epoch: 10.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42767738615090956		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 0.42767738615090956 | validation: 0.43879379509542604]
	TIME [epoch: 10.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47534035597277047		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 0.47534035597277047 | validation: 0.4922016110659777]
	TIME [epoch: 10.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44489269671955023		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 0.44489269671955023 | validation: 0.39358847839259026]
	TIME [epoch: 10.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42306607183421396		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 0.42306607183421396 | validation: 0.4125884420623294]
	TIME [epoch: 10.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4308859373336461		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 0.4308859373336461 | validation: 0.4620924269142036]
	TIME [epoch: 10.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4589227015848854		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 0.4589227015848854 | validation: 0.41526960229598714]
	TIME [epoch: 10.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4273780622812053		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 0.4273780622812053 | validation: 0.45450626217299983]
	TIME [epoch: 10.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4338750263576732		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 0.4338750263576732 | validation: 0.4165549499984575]
	TIME [epoch: 10.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4201559242112901		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 0.4201559242112901 | validation: 0.4016056834237395]
	TIME [epoch: 10.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42919295948121416		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 0.42919295948121416 | validation: 0.46072294264608304]
	TIME [epoch: 10.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4679694384967566		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 0.4679694384967566 | validation: 0.45885736411083844]
	TIME [epoch: 10.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4435440068842045		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 0.4435440068842045 | validation: 0.43858289750026813]
	TIME [epoch: 10.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4289290498239032		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 0.4289290498239032 | validation: 0.4125334968912964]
	TIME [epoch: 10.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41522871033090175		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 0.41522871033090175 | validation: 0.4182855457389147]
	TIME [epoch: 10.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4338447088344862		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 0.4338447088344862 | validation: 0.4444502902263641]
	TIME [epoch: 10.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41559851807650644		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 0.41559851807650644 | validation: 0.47855363518951927]
	TIME [epoch: 10.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4149879363262302		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 0.4149879363262302 | validation: 0.451347426017513]
	TIME [epoch: 10.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4229894625294056		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 0.4229894625294056 | validation: 0.4221890513453893]
	TIME [epoch: 10.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41368755358301834		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 0.41368755358301834 | validation: 0.401305344167286]
	TIME [epoch: 10.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43123720471289895		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 0.43123720471289895 | validation: 0.47389750565460664]
	TIME [epoch: 10.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42185864841492443		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 0.42185864841492443 | validation: 0.46422634672859064]
	TIME [epoch: 10.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43046672056100305		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 0.43046672056100305 | validation: 0.41603000689265435]
	TIME [epoch: 10.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5187309891191098		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 0.5187309891191098 | validation: 0.6068432309764636]
	TIME [epoch: 10.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5114141970176826		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 0.5114141970176826 | validation: 0.4353277055538466]
	TIME [epoch: 10.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41738682700865554		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 0.41738682700865554 | validation: 0.4235341423269709]
	TIME [epoch: 10.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4071429446715715		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 0.4071429446715715 | validation: 0.41073103947611944]
	TIME [epoch: 10.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4139796891218504		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 0.4139796891218504 | validation: 0.4013198044714916]
	TIME [epoch: 10.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42350028093624437		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 0.42350028093624437 | validation: 0.45098399653765336]
	TIME [epoch: 10.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39559189871836814		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 0.39559189871836814 | validation: 0.40710368074808784]
	TIME [epoch: 10.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3907956262774718		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 0.3907956262774718 | validation: 0.39698274307207754]
	TIME [epoch: 10.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4136618522929488		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 0.4136618522929488 | validation: 0.434409608068931]
	TIME [epoch: 10.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44475417268303985		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 0.44475417268303985 | validation: 0.39695254296961346]
	TIME [epoch: 10.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41049602751226927		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 0.41049602751226927 | validation: 0.40442745933253066]
	TIME [epoch: 10.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4148445865278122		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 0.4148445865278122 | validation: 0.4925181055100549]
	TIME [epoch: 10.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.441136193319896		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 0.441136193319896 | validation: 0.4070485902193246]
	TIME [epoch: 10.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41650355318688687		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 0.41650355318688687 | validation: 0.3971043786848016]
	TIME [epoch: 10.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40857021283979716		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 0.40857021283979716 | validation: 0.49791644453034695]
	TIME [epoch: 10.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42508131093259144		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 0.42508131093259144 | validation: 0.41029120563755656]
	TIME [epoch: 10.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41704090796241766		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 0.41704090796241766 | validation: 0.38185328152588477]
	TIME [epoch: 10.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4057126940908552		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 0.4057126940908552 | validation: 0.39466884785480716]
	TIME [epoch: 10.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40796411064626026		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 0.40796411064626026 | validation: 0.38087410282598333]
	TIME [epoch: 10.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43064429661023074		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 0.43064429661023074 | validation: 0.3874947839660476]
	TIME [epoch: 10.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4201627867969582		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 0.4201627867969582 | validation: 0.4022300730270827]
	TIME [epoch: 10.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39891611146368466		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 0.39891611146368466 | validation: 0.4135056055890789]
	TIME [epoch: 10.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4010602183142673		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 0.4010602183142673 | validation: 0.3748797659485362]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4039745657981449		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 0.4039745657981449 | validation: 0.3991247916507579]
	TIME [epoch: 10.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4217191215987005		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 0.4217191215987005 | validation: 0.3951839464231347]
	TIME [epoch: 10.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4032272751775715		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 0.4032272751775715 | validation: 0.3853681484494373]
	TIME [epoch: 10.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41264009688974185		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 0.41264009688974185 | validation: 0.3756555294008831]
	TIME [epoch: 10.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38583935312414974		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 0.38583935312414974 | validation: 0.4472380519111055]
	TIME [epoch: 10.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4293839167598305		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 0.4293839167598305 | validation: 0.5001489382439048]
	TIME [epoch: 10.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4403949979697632		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 0.4403949979697632 | validation: 0.37080280393405174]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_1007.pth
	Model improved!!!
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4204929665669619		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 0.4204929665669619 | validation: 0.3891192767838802]
	TIME [epoch: 10.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4001441384723539		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 0.4001441384723539 | validation: 0.4110950184122487]
	TIME [epoch: 10.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42034251989967436		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 0.42034251989967436 | validation: 0.4168576363502084]
	TIME [epoch: 10.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40885302890810254		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 0.40885302890810254 | validation: 0.4171397189372328]
	TIME [epoch: 10.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4183981508657208		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 0.4183981508657208 | validation: 0.3760089667678223]
	TIME [epoch: 10.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42085242549253793		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 0.42085242549253793 | validation: 0.3615800810224215]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_1013.pth
	Model improved!!!
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37811261394915097		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 0.37811261394915097 | validation: 0.41998451561348793]
	TIME [epoch: 10.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39546319756482545		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 0.39546319756482545 | validation: 0.38678758509947386]
	TIME [epoch: 10.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4000656994866104		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 0.4000656994866104 | validation: 0.38403595053065226]
	TIME [epoch: 10.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3940204681983287		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 0.3940204681983287 | validation: 0.38860529880766176]
	TIME [epoch: 10.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4009829449079353		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 0.4009829449079353 | validation: 0.40488199478456416]
	TIME [epoch: 10.2 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3870502812024341		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 0.3870502812024341 | validation: 0.45011235166688524]
	TIME [epoch: 10.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43769776138866867		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 0.43769776138866867 | validation: 0.4020285380172141]
	TIME [epoch: 10.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43828973978382796		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 0.43828973978382796 | validation: 0.4878306626640243]
	TIME [epoch: 10.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4553863346963431		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 0.4553863346963431 | validation: 0.40486807557476284]
	TIME [epoch: 10.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4094434924954983		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 0.4094434924954983 | validation: 0.38844169348875535]
	TIME [epoch: 10.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4186128322379692		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 0.4186128322379692 | validation: 0.3947624283301864]
	TIME [epoch: 10.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39769673228271935		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 0.39769673228271935 | validation: 0.45471189656775307]
	TIME [epoch: 10.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4497418509508434		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 0.4497418509508434 | validation: 0.4101632002475946]
	TIME [epoch: 10.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.461645313769408		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 0.461645313769408 | validation: 0.41705993945475855]
	TIME [epoch: 10.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44865952318000046		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 0.44865952318000046 | validation: 0.46832476876308166]
	TIME [epoch: 10.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44993731738743914		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 0.44993731738743914 | validation: 0.4132167855604033]
	TIME [epoch: 10.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41262610097903496		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 0.41262610097903496 | validation: 0.4189560171081334]
	TIME [epoch: 10.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42997574518012255		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 0.42997574518012255 | validation: 0.4087066637715823]
	TIME [epoch: 10.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40729418634919606		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 0.40729418634919606 | validation: 0.4367889936553371]
	TIME [epoch: 10.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4116759919394635		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 0.4116759919394635 | validation: 0.4258071508470137]
	TIME [epoch: 10.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.488279590761287		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 0.488279590761287 | validation: 0.4307436990951088]
	TIME [epoch: 10.2 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4165937608890456		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 0.4165937608890456 | validation: 0.39980849689304826]
	TIME [epoch: 10.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4147063228037937		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 0.4147063228037937 | validation: 0.42027036183986655]
	TIME [epoch: 10.2 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41925458723245324		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 0.41925458723245324 | validation: 0.42847739522073836]
	TIME [epoch: 10.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42683704444067594		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 0.42683704444067594 | validation: 0.44481125783140985]
	TIME [epoch: 10.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45243113188882367		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 0.45243113188882367 | validation: 0.4776932889586756]
	TIME [epoch: 10.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44193073091804613		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 0.44193073091804613 | validation: 0.3819795385910711]
	TIME [epoch: 10.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43889903734795743		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 0.43889903734795743 | validation: 0.4351294589704227]
	TIME [epoch: 10.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4044422258342862		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 0.4044422258342862 | validation: 0.4206131568695093]
	TIME [epoch: 10.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41005106637000166		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 0.41005106637000166 | validation: 0.3672523320218694]
	TIME [epoch: 10.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38983588196585117		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 0.38983588196585117 | validation: 0.4207156283536525]
	TIME [epoch: 10.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3959190959591299		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 0.3959190959591299 | validation: 0.4192705148373576]
	TIME [epoch: 10.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3995857611961665		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 0.3995857611961665 | validation: 0.4501664986000853]
	TIME [epoch: 10.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.433931924247398		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 0.433931924247398 | validation: 0.4480025525946428]
	TIME [epoch: 10.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44449211489103213		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 0.44449211489103213 | validation: 0.3988905452641731]
	TIME [epoch: 10.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45043768864565104		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 0.45043768864565104 | validation: 0.42343676311212053]
	TIME [epoch: 10.2 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38156718654529254		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 0.38156718654529254 | validation: 0.3869672808530447]
	TIME [epoch: 10.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40853144972383576		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 0.40853144972383576 | validation: 0.4387220767426178]
	TIME [epoch: 10.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4386372799863002		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 0.4386372799863002 | validation: 0.39485235242528455]
	TIME [epoch: 10.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3991793675089517		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 0.3991793675089517 | validation: 0.2657076539607773]
	TIME [epoch: 10.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r5_20240219_235428/states/model_tr_study205_1053.pth
	Model improved!!!
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39535842749190275		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 0.39535842749190275 | validation: 0.4172353695868085]
	TIME [epoch: 10.2 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41353222824618563		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 0.41353222824618563 | validation: 0.4745716608925829]
	TIME [epoch: 10.2 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41036110766753736		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 0.41036110766753736 | validation: 0.4089654888699923]
	TIME [epoch: 10.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4358248936669872		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 0.4358248936669872 | validation: 0.44614994202488767]
	TIME [epoch: 10.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4291797652345082		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 0.4291797652345082 | validation: 0.4597413690665704]
	TIME [epoch: 10.2 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42358737251696227		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 0.42358737251696227 | validation: 0.3761681819518392]
	TIME [epoch: 10.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3977435558672429		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 0.3977435558672429 | validation: 0.36997044503711635]
	TIME [epoch: 10.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38112383996874605		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 0.38112383996874605 | validation: 0.3964886043503713]
	TIME [epoch: 10.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4038275099946381		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 0.4038275099946381 | validation: 0.39165332534405495]
	TIME [epoch: 10.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37803482324341064		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 0.37803482324341064 | validation: 0.4054117810371457]
	TIME [epoch: 10.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4173339248079261		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 0.4173339248079261 | validation: 0.40481830359698806]
	TIME [epoch: 10.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40635160864861886		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 0.40635160864861886 | validation: 0.4883206277691046]
	TIME [epoch: 10.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42169175884542714		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 0.42169175884542714 | validation: 0.40901591184735]
	TIME [epoch: 10.2 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39159245600621395		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 0.39159245600621395 | validation: 0.3851886088420314]
	TIME [epoch: 10.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38674726535434556		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 0.38674726535434556 | validation: 0.3546919633918785]
	TIME [epoch: 10.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4231009000564903		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 0.4231009000564903 | validation: 0.3750613584196163]
	TIME [epoch: 10.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3996700498475063		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 0.3996700498475063 | validation: 0.40529915758478835]
	TIME [epoch: 10.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.401379631397781		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 0.401379631397781 | validation: 0.38265973734690534]
	TIME [epoch: 10.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39158494470909083		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 0.39158494470909083 | validation: 0.3601847402710514]
	TIME [epoch: 10.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3806421212141831		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 0.3806421212141831 | validation: 0.41927989304169083]
	TIME [epoch: 10.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4029427857058357		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 0.4029427857058357 | validation: 0.39397942475687103]
	TIME [epoch: 10.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3807963640831695		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 0.3807963640831695 | validation: 0.39290420116366415]
	TIME [epoch: 10.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4277012402605262		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 0.4277012402605262 | validation: 0.4068815914735736]
	TIME [epoch: 10.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4166998722707403		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 0.4166998722707403 | validation: 0.4304411908988475]
	TIME [epoch: 10.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4120786544664442		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 0.4120786544664442 | validation: 0.4388845872138459]
	TIME [epoch: 10.2 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4155235927616321		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 0.4155235927616321 | validation: 0.44585891332634475]
	TIME [epoch: 10.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40318496528232817		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 0.40318496528232817 | validation: 0.37645164285139393]
	TIME [epoch: 10.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40478513942937655		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 0.40478513942937655 | validation: 0.406093658872621]
	TIME [epoch: 10.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39929587584711085		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 0.39929587584711085 | validation: 0.3830571963510579]
	TIME [epoch: 10.2 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38677756511392586		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 0.38677756511392586 | validation: 0.38877058887921934]
	TIME [epoch: 10.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38583961373184356		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 0.38583961373184356 | validation: 0.37741591619223797]
	TIME [epoch: 10.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3940391387094621		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 0.3940391387094621 | validation: 0.4064856670761414]
	TIME [epoch: 10.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4183873384868713		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 0.4183873384868713 | validation: 0.5003480353419899]
	TIME [epoch: 10.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4904566636937012		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 0.4904566636937012 | validation: 0.38352973307550914]
	TIME [epoch: 10.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4039855040494163		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 0.4039855040494163 | validation: 0.38099342531063934]
	TIME [epoch: 10.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38822725163351735		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 0.38822725163351735 | validation: 0.399955210337109]
	TIME [epoch: 10.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4349483377246437		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 0.4349483377246437 | validation: 0.45650075415277164]
	TIME [epoch: 10.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40358393905622825		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 0.40358393905622825 | validation: 0.4299596506724647]
	TIME [epoch: 10.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39894982033746795		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 0.39894982033746795 | validation: 0.3717808519526454]
	TIME [epoch: 10.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4149032785987938		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 0.4149032785987938 | validation: 0.4170420398413344]
	TIME [epoch: 10.2 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4382757027170546		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 0.4382757027170546 | validation: 0.4138452496692013]
	TIME [epoch: 10.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41100478719001765		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 0.41100478719001765 | validation: 0.4196013379531948]
	TIME [epoch: 10.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40583359993995416		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 0.40583359993995416 | validation: 0.45836143848431904]
	TIME [epoch: 10.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3925874202215024		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 0.3925874202215024 | validation: 0.3623960672147715]
	TIME [epoch: 10.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3979458171591019		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 0.3979458171591019 | validation: 0.428496679224421]
	TIME [epoch: 10.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41649281667458704		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 0.41649281667458704 | validation: 0.4053647571523098]
	TIME [epoch: 10.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40132005801785037		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 0.40132005801785037 | validation: 0.3875106114649094]
	TIME [epoch: 10.2 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3958855076889307		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 0.3958855076889307 | validation: 0.40118379550095534]
	TIME [epoch: 10.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40146277218219373		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 0.40146277218219373 | validation: 0.406360094551363]
	TIME [epoch: 10.2 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4079797987378539		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 0.4079797987378539 | validation: 0.3667797074095999]
	TIME [epoch: 10.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3757773364445244		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 0.3757773364445244 | validation: 0.381332780251235]
	TIME [epoch: 10.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42005905675646504		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 0.42005905675646504 | validation: 0.40888483342763693]
	TIME [epoch: 10.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4229165563348478		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 0.4229165563348478 | validation: 0.41753502121092084]
	TIME [epoch: 10.2 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4222708231239616		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 0.4222708231239616 | validation: 0.40064088060065317]
	TIME [epoch: 10.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3926069467657415		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 0.3926069467657415 | validation: 0.4194913094699358]
	TIME [epoch: 10.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42896871147922144		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 0.42896871147922144 | validation: 0.3779830650145115]
	TIME [epoch: 10.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40473953351655345		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 0.40473953351655345 | validation: 0.5015526718635448]
	TIME [epoch: 10.2 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41261824084764476		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 0.41261824084764476 | validation: 0.4504064465497995]
	TIME [epoch: 10.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4250843686085588		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 0.4250843686085588 | validation: 0.4333433507935577]
	TIME [epoch: 10.2 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43515121810273794		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 0.43515121810273794 | validation: 0.43594826017685623]
	TIME [epoch: 10.2 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4005181845661282		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 0.4005181845661282 | validation: 0.4007378444923517]
	TIME [epoch: 10.2 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39354776498025623		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 0.39354776498025623 | validation: 0.37849634815941313]
	TIME [epoch: 10.2 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43838096563039536		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 0.43838096563039536 | validation: 0.4538826821850755]
	TIME [epoch: 10.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4856325456164791		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 0.4856325456164791 | validation: 0.4799623724547973]
	TIME [epoch: 10.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4307422880604748		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 0.4307422880604748 | validation: 0.4317402750665733]
	TIME [epoch: 10.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4244026621683058		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 0.4244026621683058 | validation: 0.4186080797402563]
	TIME [epoch: 10.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4046445090633048		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 0.4046445090633048 | validation: 0.4472880010675223]
	TIME [epoch: 10.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45552391008970805		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 0.45552391008970805 | validation: 0.4990222605801492]
	TIME [epoch: 10.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46822867932834705		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 0.46822867932834705 | validation: 0.4482330960318288]
	TIME [epoch: 10.2 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41106057811951213		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 0.41106057811951213 | validation: 0.43089242781209336]
	TIME [epoch: 10.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42619587993471625		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 0.42619587993471625 | validation: 0.40737982332011585]
	TIME [epoch: 10.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40085326627745543		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 0.40085326627745543 | validation: 0.42714479219150714]
	TIME [epoch: 10.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40153255449549674		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 0.40153255449549674 | validation: 0.46042176367352033]
	TIME [epoch: 10.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4092451473726017		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 0.4092451473726017 | validation: 0.392847338833116]
	TIME [epoch: 10.2 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4122305322096113		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 0.4122305322096113 | validation: 0.3815504339322307]
	TIME [epoch: 10.2 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3915547091622174		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 0.3915547091622174 | validation: 0.41796503448981215]
	TIME [epoch: 10.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4076424876250672		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 0.4076424876250672 | validation: 0.3924706503362354]
	TIME [epoch: 10.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39770347176528625		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 0.39770347176528625 | validation: 0.37778569817476837]
	TIME [epoch: 10.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40900371150822945		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 0.40900371150822945 | validation: 0.4059168506584978]
	TIME [epoch: 10.2 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3983595117158814		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 0.3983595117158814 | validation: 0.4370585426067604]
	TIME [epoch: 10.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41594705015613814		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 0.41594705015613814 | validation: 0.41370006090725947]
	TIME [epoch: 10.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39665580052859395		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 0.39665580052859395 | validation: 0.37577335559327124]
	TIME [epoch: 10.2 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4242908136633452		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 0.4242908136633452 | validation: 0.37745065593671123]
	TIME [epoch: 10.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39065061427105024		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 0.39065061427105024 | validation: 0.408327602449899]
	TIME [epoch: 10.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3951682112213589		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 0.3951682112213589 | validation: 0.4126829103141776]
	TIME [epoch: 10.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3824228196484534		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 0.3824228196484534 | validation: 0.38093335850136895]
	TIME [epoch: 10.2 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4100074512761719		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 0.4100074512761719 | validation: 0.4024334315569281]
	TIME [epoch: 10.2 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3871466608778561		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 0.3871466608778561 | validation: 0.4012015208052492]
	TIME [epoch: 10.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3952977459711628		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 0.3952977459711628 | validation: 0.4224099598078376]
	TIME [epoch: 10.2 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.396287576683822		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 0.396287576683822 | validation: 0.3873140098467113]
	TIME [epoch: 10.2 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3934536525971192		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 0.3934536525971192 | validation: 0.41274491261096385]
	TIME [epoch: 10.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39598900024932415		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 0.39598900024932415 | validation: 0.42359997880787637]
	TIME [epoch: 10.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4472253761819257		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 0.4472253761819257 | validation: 0.3936748309532162]
	TIME [epoch: 10.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4063003392259363		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 0.4063003392259363 | validation: 0.4202425710907559]
	TIME [epoch: 10.2 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4115716625718536		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 0.4115716625718536 | validation: 0.5464226944373318]
	TIME [epoch: 10.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5298348507100126		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 0.5298348507100126 | validation: 0.39132172055690645]
	TIME [epoch: 10.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42991654085204534		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 0.42991654085204534 | validation: 0.3758723123425989]
	TIME [epoch: 10.2 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41143448348306466		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 0.41143448348306466 | validation: 0.38312057669172583]
	TIME [epoch: 10.2 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4068329800679756		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 0.4068329800679756 | validation: 0.3789330816012743]
	TIME [epoch: 10.2 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38967039294723904		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 0.38967039294723904 | validation: 0.37105392540301146]
	TIME [epoch: 10.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4128919899644388		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 0.4128919899644388 | validation: 0.45773212126378]
	TIME [epoch: 10.2 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4124511175273414		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 0.4124511175273414 | validation: 0.39054305828927155]
	TIME [epoch: 10.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40817549704536377		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 0.40817549704536377 | validation: 0.4159687336805314]
	TIME [epoch: 10.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41010824346285		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 0.41010824346285 | validation: 0.3956018687444845]
	TIME [epoch: 10.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39716053894470793		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 0.39716053894470793 | validation: 0.36325529511035315]
	TIME [epoch: 10.2 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3949822763374574		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 0.3949822763374574 | validation: 0.4419580421097395]
	TIME [epoch: 10.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3955288402359258		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 0.3955288402359258 | validation: 0.39099587114751116]
	TIME [epoch: 10.2 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3920307233009207		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 0.3920307233009207 | validation: 0.3907580722820015]
	TIME [epoch: 10.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38443497623163214		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 0.38443497623163214 | validation: 0.4069105376442432]
	TIME [epoch: 10.2 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3848670366833077		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 0.3848670366833077 | validation: 0.4011372255092703]
	TIME [epoch: 10.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3792384311400972		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 0.3792384311400972 | validation: 0.40193972111000487]
	TIME [epoch: 10.2 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38914976065576906		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 0.38914976065576906 | validation: 0.42639928816280487]
	TIME [epoch: 10.2 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853263768731323		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 0.3853263768731323 | validation: 0.4218663675202139]
	TIME [epoch: 10.2 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40512594517670664		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 0.40512594517670664 | validation: 0.3865232930299618]
	TIME [epoch: 10.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40062084673398557		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 0.40062084673398557 | validation: 0.38169773552954217]
	TIME [epoch: 10.2 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38988977895637456		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 0.38988977895637456 | validation: 0.3909751668503862]
	TIME [epoch: 10.2 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.379392324471135		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 0.379392324471135 | validation: 0.37490067627374374]
	TIME [epoch: 10.2 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39378415636101965		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 0.39378415636101965 | validation: 0.4251966605401307]
	TIME [epoch: 10.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3818396736791751		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 0.3818396736791751 | validation: 0.38288631449154736]
	TIME [epoch: 10.2 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38541246356132774		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 0.38541246356132774 | validation: 0.37009032697337474]
	TIME [epoch: 10.2 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38093391765627976		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 0.38093391765627976 | validation: 0.376613591405568]
	TIME [epoch: 10.2 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3827869942659607		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 0.3827869942659607 | validation: 0.3852960670344707]
	TIME [epoch: 10.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.369202549134294		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 0.369202549134294 | validation: 0.4019510040306476]
	TIME [epoch: 10.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40101096143762743		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 0.40101096143762743 | validation: 0.42145189831133223]
	TIME [epoch: 10.2 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41779153319606044		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 0.41779153319606044 | validation: 0.39750229153375627]
	TIME [epoch: 10.2 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4019771912051384		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 0.4019771912051384 | validation: 0.3784246708739657]
	TIME [epoch: 10.2 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3901326601872334		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 0.3901326601872334 | validation: 0.40222288524375194]
	TIME [epoch: 10.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40570728417295204		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 0.40570728417295204 | validation: 0.40438027070156396]
	TIME [epoch: 10.2 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4129111080102679		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 0.4129111080102679 | validation: 0.41268726180179705]
	TIME [epoch: 10.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4014487115997524		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 0.4014487115997524 | validation: 0.3615110534016354]
	TIME [epoch: 10.2 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3834357969651218		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 0.3834357969651218 | validation: 0.4115470827108774]
	TIME [epoch: 10.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3854844182906998		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 0.3854844182906998 | validation: 0.3873670945066096]
	TIME [epoch: 10.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3896939085138678		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 0.3896939085138678 | validation: 0.3991366996631134]
	TIME [epoch: 10.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38055563757135735		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 0.38055563757135735 | validation: 0.39324527571870305]
	TIME [epoch: 10.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3944095345049717		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 0.3944095345049717 | validation: 0.4238868205360153]
	TIME [epoch: 10.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41696805052346014		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 0.41696805052346014 | validation: 0.38872785185470793]
	TIME [epoch: 10.2 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39807100036907983		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 0.39807100036907983 | validation: 0.37897023581299055]
	TIME [epoch: 10.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3831714028497649		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 0.3831714028497649 | validation: 0.3672983279573043]
	TIME [epoch: 10.2 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36315470402160493		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 0.36315470402160493 | validation: 0.35757030654673716]
	TIME [epoch: 10.2 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37349988083463165		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 0.37349988083463165 | validation: 0.38790865190464024]
	TIME [epoch: 10.2 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38145765129682446		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 0.38145765129682446 | validation: 0.3625505676801461]
	TIME [epoch: 10.2 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38745637217301604		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 0.38745637217301604 | validation: 0.4204258041195803]
	TIME [epoch: 10.2 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3895761780411987		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 0.3895761780411987 | validation: 0.3629055294963385]
	TIME [epoch: 10.2 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39036794799596386		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 0.39036794799596386 | validation: 0.3929738279783969]
	TIME [epoch: 10.2 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37452222026079995		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 0.37452222026079995 | validation: 0.3506097636333689]
	TIME [epoch: 10.2 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38612382847121424		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 0.38612382847121424 | validation: 0.3668344680958632]
	TIME [epoch: 10.2 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3844538118060924		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 0.3844538118060924 | validation: 0.40133271565249945]
	TIME [epoch: 10.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38836496361407014		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 0.38836496361407014 | validation: 0.3836511397033882]
	TIME [epoch: 10.2 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38554233607020916		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 0.38554233607020916 | validation: 0.41363146095533265]
	TIME [epoch: 10.2 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853336908279873		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 0.3853336908279873 | validation: 0.36987721872252066]
	TIME [epoch: 10.2 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37536810985869035		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 0.37536810985869035 | validation: 0.4150027745403134]
	TIME [epoch: 10.2 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37543020376194114		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 0.37543020376194114 | validation: 0.39394769220268033]
	TIME [epoch: 10.2 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3902637959822499		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 0.3902637959822499 | validation: 0.38451981519910716]
	TIME [epoch: 10.2 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.390603775017479		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 0.390603775017479 | validation: 0.4025554857080188]
	TIME [epoch: 10.2 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42207431018098474		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 0.42207431018098474 | validation: 0.3989637660137747]
	TIME [epoch: 10.2 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4161724372906499		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 0.4161724372906499 | validation: 0.40538414190561206]
	TIME [epoch: 10.2 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40249080284877714		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 0.40249080284877714 | validation: 0.3796612299733904]
	TIME [epoch: 10.2 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39317155344431753		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 0.39317155344431753 | validation: 0.40175082936076323]
	TIME [epoch: 10.2 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40779326224035284		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 0.40779326224035284 | validation: 0.3643470229562803]
	TIME [epoch: 10.2 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3862741044021176		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 0.3862741044021176 | validation: 0.3674084387864002]
	TIME [epoch: 10.2 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38698774798957886		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 0.38698774798957886 | validation: 0.4105532785961751]
	TIME [epoch: 10.2 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3796362087981786		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 0.3796362087981786 | validation: 0.39276615175252205]
	TIME [epoch: 10.2 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4167245889803045		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 0.4167245889803045 | validation: 0.4216322114960704]
	TIME [epoch: 10.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42584679499567885		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 0.42584679499567885 | validation: 0.41442915959476323]
	TIME [epoch: 10.2 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4058553818788065		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 0.4058553818788065 | validation: 0.36558717863835183]
	TIME [epoch: 10.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3773468210010863		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 0.3773468210010863 | validation: 0.3728815595757243]
	TIME [epoch: 10.2 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39850956925168973		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 0.39850956925168973 | validation: 0.40218864356668255]
	TIME [epoch: 10.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39281520087834754		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 0.39281520087834754 | validation: 0.3889216231299919]
	TIME [epoch: 10.2 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3931098932173385		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 0.3931098932173385 | validation: 0.380129860038601]
	TIME [epoch: 10.2 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3786786085562854		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 0.3786786085562854 | validation: 0.40078315488178845]
	TIME [epoch: 10.2 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37303813715983863		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 0.37303813715983863 | validation: 0.3601878588381079]
	TIME [epoch: 10.2 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3797822398808902		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 0.3797822398808902 | validation: 0.3956240693607251]
	TIME [epoch: 10.2 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37828374281382765		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 0.37828374281382765 | validation: 0.3570857473187714]
	TIME [epoch: 10.2 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3888708912298108		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 0.3888708912298108 | validation: 0.4028795118989349]
	TIME [epoch: 10.2 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3953096642182955		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 0.3953096642182955 | validation: 0.37377469413012165]
	TIME [epoch: 10.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38154180487879785		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 0.38154180487879785 | validation: 0.36847619582341723]
	TIME [epoch: 10.2 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3877571842735034		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 0.3877571842735034 | validation: 0.40776055903734615]
	TIME [epoch: 10.2 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39567403947506463		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 0.39567403947506463 | validation: 0.46428670566712044]
	TIME [epoch: 10.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4054938000013795		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 0.4054938000013795 | validation: 0.3677991364413923]
	TIME [epoch: 10.2 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37939822062049167		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 0.37939822062049167 | validation: 0.3808092764112434]
	TIME [epoch: 10.2 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39221643966341624		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 0.39221643966341624 | validation: 0.36957744134576304]
	TIME [epoch: 10.2 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3742121744362197		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 0.3742121744362197 | validation: 0.40194331833169394]
	TIME [epoch: 10.2 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3966367929089859		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 0.3966367929089859 | validation: 0.4205142259089875]
	TIME [epoch: 10.2 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3824977319678797		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 0.3824977319678797 | validation: 0.3906972996211799]
	TIME [epoch: 10.2 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38287011767257784		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 0.38287011767257784 | validation: 0.39300834499834864]
	TIME [epoch: 10.2 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3944388676037127		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 0.3944388676037127 | validation: 0.35567677616510934]
	TIME [epoch: 10.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3948622572646303		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 0.3948622572646303 | validation: 0.38151738900889476]
	TIME [epoch: 10.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41549319496228165		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 0.41549319496228165 | validation: 0.3924113571212535]
	TIME [epoch: 10.2 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38498159921787767		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 0.38498159921787767 | validation: 0.36633822400280613]
	TIME [epoch: 10.2 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3822711510566259		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 0.3822711510566259 | validation: 0.3727269091509047]
	TIME [epoch: 10.2 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3835580170260632		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 0.3835580170260632 | validation: 0.36591648932677073]
	TIME [epoch: 10.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36826868908171184		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 0.36826868908171184 | validation: 0.40291297157538253]
	TIME [epoch: 10.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39208289484737746		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 0.39208289484737746 | validation: 0.39156128843212357]
	TIME [epoch: 10.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40981421137434715		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 0.40981421137434715 | validation: 0.38083435161110646]
	TIME [epoch: 10.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4093937817060298		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 0.4093937817060298 | validation: 0.4078734384996217]
	TIME [epoch: 10.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3897327378614725		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 0.3897327378614725 | validation: 0.4008550630788909]
	TIME [epoch: 10.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38877098225618967		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 0.38877098225618967 | validation: 0.3581112372308574]
	TIME [epoch: 10.2 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39276817243274753		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 0.39276817243274753 | validation: 0.39442086051748887]
	TIME [epoch: 10.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40499461488843524		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 0.40499461488843524 | validation: 0.3666291876700592]
	TIME [epoch: 10.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3835792149028146		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 0.3835792149028146 | validation: 0.37544860881594994]
	TIME [epoch: 10.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37293347652898845		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 0.37293347652898845 | validation: 0.35515656487007957]
	TIME [epoch: 10.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37645966749323617		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 0.37645966749323617 | validation: 0.37172441320036825]
	TIME [epoch: 10.2 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3934114760964839		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 0.3934114760964839 | validation: 0.3750029654523642]
	TIME [epoch: 10.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3774695165354952		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 0.3774695165354952 | validation: 0.3616444810072554]
	TIME [epoch: 10.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3810756476263406		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 0.3810756476263406 | validation: 0.39018725555948286]
	TIME [epoch: 10.2 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3786177220498014		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 0.3786177220498014 | validation: 0.376932737070881]
	TIME [epoch: 10.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4063890002209365		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 0.4063890002209365 | validation: 0.3781771079795416]
	TIME [epoch: 10.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38917542484157		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 0.38917542484157 | validation: 0.4046677145326534]
	TIME [epoch: 10.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3899395812015786		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 0.3899395812015786 | validation: 0.39000099762669094]
	TIME [epoch: 10.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4018333087891685		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 0.4018333087891685 | validation: 0.41634214217343635]
	TIME [epoch: 10.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38045876307578164		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 0.38045876307578164 | validation: 0.3814424231360638]
	TIME [epoch: 10.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35774113775424815		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 0.35774113775424815 | validation: 0.3818303667662962]
	TIME [epoch: 10.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38178820371248656		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 0.38178820371248656 | validation: 0.3841841508482302]
	TIME [epoch: 10.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3948342547514055		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 0.3948342547514055 | validation: 0.3742750062641101]
	TIME [epoch: 10.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3983577403858848		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.3983577403858848 | validation: 0.38445510582250725]
	TIME [epoch: 10.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3887921859276297		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 0.3887921859276297 | validation: 0.3703710330714944]
	TIME [epoch: 10.2 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38907440529388226		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 0.38907440529388226 | validation: 0.42440202298310115]
	TIME [epoch: 10.2 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3835827028515867		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 0.3835827028515867 | validation: 0.39090515162663564]
	TIME [epoch: 10.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37253824300986593		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 0.37253824300986593 | validation: 0.3806912342843636]
	TIME [epoch: 10.2 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.387601815241772		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 0.387601815241772 | validation: 0.4001216191723721]
	TIME [epoch: 10.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38856632635694277		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 0.38856632635694277 | validation: 0.3764598262177745]
	TIME [epoch: 10.2 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37115093883011896		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 0.37115093883011896 | validation: 0.3708357931711872]
	TIME [epoch: 10.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3965483876008832		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 0.3965483876008832 | validation: 0.38078711560196327]
	TIME [epoch: 10.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38156781621597174		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 0.38156781621597174 | validation: 0.35447271175547795]
	TIME [epoch: 10.2 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3856349194508517		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 0.3856349194508517 | validation: 0.35281718296733006]
	TIME [epoch: 10.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3827248401663924		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 0.3827248401663924 | validation: 0.3925488410367601]
	TIME [epoch: 10.2 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3890783991266796		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 0.3890783991266796 | validation: 0.3709009957131677]
	TIME [epoch: 10.2 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3806883354722199		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 0.3806883354722199 | validation: 0.4029174208148528]
	TIME [epoch: 10.2 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3854339184983603		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 0.3854339184983603 | validation: 0.3554948197496481]
	TIME [epoch: 10.2 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4067394528060896		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 0.4067394528060896 | validation: 0.3494822124482853]
	TIME [epoch: 10.2 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38125848087537473		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 0.38125848087537473 | validation: 0.37752747122482677]
	TIME [epoch: 10.2 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39312041322609603		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 0.39312041322609603 | validation: 0.4120119115460418]
	TIME [epoch: 10.2 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3859583743450788		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 0.3859583743450788 | validation: 0.4070078084597274]
	TIME [epoch: 10.2 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4137821793617095		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 0.4137821793617095 | validation: 0.37652532295035246]
	TIME [epoch: 10.2 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36766600260589455		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 0.36766600260589455 | validation: 0.3949999866749188]
	TIME [epoch: 10.2 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3983597691019476		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 0.3983597691019476 | validation: 0.4156019918869606]
	TIME [epoch: 10.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39589562639898557		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 0.39589562639898557 | validation: 0.4593536707678899]
	TIME [epoch: 10.2 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39387815920999014		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 0.39387815920999014 | validation: 0.42585668312970126]
	TIME [epoch: 10.2 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38961479203600874		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 0.38961479203600874 | validation: 0.39864035670656905]
	TIME [epoch: 10.2 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4139890262126789		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 0.4139890262126789 | validation: 0.4122717646763727]
	TIME [epoch: 10.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3810225622082654		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 0.3810225622082654 | validation: 0.40283179304294775]
	TIME [epoch: 10.2 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39790363735741757		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 0.39790363735741757 | validation: 0.41506184071018853]
	TIME [epoch: 10.2 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41633012068391756		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 0.41633012068391756 | validation: 0.3998351512124473]
	TIME [epoch: 10.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40740698147471177		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 0.40740698147471177 | validation: 0.3883018398898992]
	TIME [epoch: 10.2 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3823306150657324		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 0.3823306150657324 | validation: 0.3844401945128837]
	TIME [epoch: 10.2 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3886547137272659		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 0.3886547137272659 | validation: 0.41129485011411715]
	TIME [epoch: 10.2 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38894640647587125		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 0.38894640647587125 | validation: 0.41045820341500333]
	TIME [epoch: 10.2 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3920686513544104		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 0.3920686513544104 | validation: 0.4089619474349128]
	TIME [epoch: 10.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40445945449070775		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 0.40445945449070775 | validation: 0.41187172594648747]
	TIME [epoch: 10.2 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40045444966118565		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 0.40045444966118565 | validation: 0.4007692920922791]
	TIME [epoch: 10.2 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38093808716970357		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 0.38093808716970357 | validation: 0.3687725267722006]
	TIME [epoch: 10.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.413174400714193		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 0.413174400714193 | validation: 0.374133133128268]
	TIME [epoch: 10.2 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3896496051394543		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 0.3896496051394543 | validation: 0.3750038918346171]
	TIME [epoch: 10.2 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4106538576628158		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 0.4106538576628158 | validation: 0.39610649261952957]
	TIME [epoch: 10.2 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4167079291709686		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 0.4167079291709686 | validation: 0.3880858641533422]
	TIME [epoch: 10.2 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3982701825748059		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 0.3982701825748059 | validation: 0.37763660602692184]
	TIME [epoch: 10.2 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4113553421620444		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 0.4113553421620444 | validation: 0.40923497606976095]
	TIME [epoch: 10.2 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38889173745642547		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 0.38889173745642547 | validation: 0.3740250836705905]
	TIME [epoch: 10.2 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.391045574452917		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 0.391045574452917 | validation: 0.40776511941716126]
	TIME [epoch: 10.2 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891783826994558		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 0.3891783826994558 | validation: 0.4054333016544234]
	TIME [epoch: 10.2 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39468748292169076		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 0.39468748292169076 | validation: 0.39430796402544177]
	TIME [epoch: 10.2 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39055257642576646		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 0.39055257642576646 | validation: 0.37500136221140906]
	TIME [epoch: 10.2 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39188570656197597		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 0.39188570656197597 | validation: 0.3949205815640918]
	TIME [epoch: 10.2 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38493774329591923		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 0.38493774329591923 | validation: 0.3742285956678995]
	TIME [epoch: 10.2 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3717643261415927		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 0.3717643261415927 | validation: 0.3808016140375965]
	TIME [epoch: 10.2 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3868707058264683		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 0.3868707058264683 | validation: 0.37413368793764973]
	TIME [epoch: 10.2 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853199524975087		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 0.3853199524975087 | validation: 0.3852546801805679]
	TIME [epoch: 10.2 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3706798035274949		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 0.3706798035274949 | validation: 0.3642646581846292]
	TIME [epoch: 10.2 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38668087786712163		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 0.38668087786712163 | validation: 0.35746945656848456]
	TIME [epoch: 10.2 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3783534424959131		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 0.3783534424959131 | validation: 0.41062849081052777]
	TIME [epoch: 10.2 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39603833810835565		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 0.39603833810835565 | validation: 0.3588544316503355]
	TIME [epoch: 10.2 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3786621995965467		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 0.3786621995965467 | validation: 0.4491851327832231]
	TIME [epoch: 10.2 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3936153824446239		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 0.3936153824446239 | validation: 0.41575896140663304]
	TIME [epoch: 10.2 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38216017728332097		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 0.38216017728332097 | validation: 0.39350824017346725]
	TIME [epoch: 10.2 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38346619953101313		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 0.38346619953101313 | validation: 0.4006016608252192]
	TIME [epoch: 10.2 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3803477203008314		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 0.3803477203008314 | validation: 0.39626396257650953]
	TIME [epoch: 10.2 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37667823660092276		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 0.37667823660092276 | validation: 0.37074347683570735]
	TIME [epoch: 10.2 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38221649455697343		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 0.38221649455697343 | validation: 0.37598400289079964]
	TIME [epoch: 10.2 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38322869172253055		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 0.38322869172253055 | validation: 0.38587826196359026]
	TIME [epoch: 10.2 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40428519935356305		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 0.40428519935356305 | validation: 0.43483888218630784]
	TIME [epoch: 10.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3882920739882117		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 0.3882920739882117 | validation: 0.36417323344213914]
	TIME [epoch: 10.2 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3944319203610299		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 0.3944319203610299 | validation: 0.3855638184536728]
	TIME [epoch: 10.2 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38695566563774275		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 0.38695566563774275 | validation: 0.39753822170337044]
	TIME [epoch: 10.2 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829397303445886		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 0.3829397303445886 | validation: 0.4053383047676083]
	TIME [epoch: 10.2 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37238079218352804		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 0.37238079218352804 | validation: 0.4074673398927082]
	TIME [epoch: 10.2 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3802185295987842		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 0.3802185295987842 | validation: 0.42504352813866436]
	TIME [epoch: 10.2 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4148346266656864		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 0.4148346266656864 | validation: 0.4001707937535278]
	TIME [epoch: 10.2 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4047878767747635		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 0.4047878767747635 | validation: 0.42537021790041146]
	TIME [epoch: 10.2 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40169316645418107		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 0.40169316645418107 | validation: 0.4213707928766777]
	TIME [epoch: 10.2 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4256962901226847		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 0.4256962901226847 | validation: 0.4394283338536046]
	TIME [epoch: 10.2 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4215619331245133		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 0.4215619331245133 | validation: 0.3985627549422543]
	TIME [epoch: 10.2 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4097374775482945		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 0.4097374775482945 | validation: 0.3805873689471082]
	TIME [epoch: 10.2 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.390184212109162		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 0.390184212109162 | validation: 0.4255120348465797]
	TIME [epoch: 10.2 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38500822226878284		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 0.38500822226878284 | validation: 0.4009267476015821]
	TIME [epoch: 10.2 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3931011894152404		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 0.3931011894152404 | validation: 0.3877042428646766]
	TIME [epoch: 10.2 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3831661039460598		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 0.3831661039460598 | validation: 0.38180545444982683]
	TIME [epoch: 10.2 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3828681255824056		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 0.3828681255824056 | validation: 0.38700397090857364]
	TIME [epoch: 10.2 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3919243943202043		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 0.3919243943202043 | validation: 0.39562869517152216]
	TIME [epoch: 10.2 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39061002230791403		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 0.39061002230791403 | validation: 0.3952600292800447]
	TIME [epoch: 10.2 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3659145388704042		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 0.3659145388704042 | validation: 0.3638817362390138]
	TIME [epoch: 10.2 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37179188822011505		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 0.37179188822011505 | validation: 0.3719728759045847]
	TIME [epoch: 10.2 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3865534954418314		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 0.3865534954418314 | validation: 0.36683592130538784]
	TIME [epoch: 10.2 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3777224214872605		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 0.3777224214872605 | validation: 0.4055939333508995]
	TIME [epoch: 10.2 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37626467118404766		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 0.37626467118404766 | validation: 0.4039853912438238]
	TIME [epoch: 10.2 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3988040116070363		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 0.3988040116070363 | validation: 0.39271326008639984]
	TIME [epoch: 10.2 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3807252463307659		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 0.3807252463307659 | validation: 0.37777538603044464]
	TIME [epoch: 10.2 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38938667404479194		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 0.38938667404479194 | validation: 0.4122876059273565]
	TIME [epoch: 10.2 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39620070785326356		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 0.39620070785326356 | validation: 0.3682798154445087]
	TIME [epoch: 10.2 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3931104898770205		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 0.3931104898770205 | validation: 0.4034860539469969]
	TIME [epoch: 10.2 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40904749084699354		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 0.40904749084699354 | validation: 0.3885821537106311]
	TIME [epoch: 10.2 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37038598139036943		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 0.37038598139036943 | validation: 0.37688519043129953]
	TIME [epoch: 10.2 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3918813301798818		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 0.3918813301798818 | validation: 0.37851734963073624]
	TIME [epoch: 10.2 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3783621351491516		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 0.3783621351491516 | validation: 0.3664365348789052]
	TIME [epoch: 10.2 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3786649902954453		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 0.3786649902954453 | validation: 0.3660764252454753]
	TIME [epoch: 10.2 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917767751666549		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 0.3917767751666549 | validation: 0.37768975840171815]
	TIME [epoch: 10.2 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3774989495212365		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 0.3774989495212365 | validation: 0.35961283680700606]
	TIME [epoch: 10.2 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38369354603180794		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 0.38369354603180794 | validation: 0.3860251861445376]
	TIME [epoch: 10.2 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4097834818022614		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 0.4097834818022614 | validation: 0.36649946996462823]
	TIME [epoch: 10.2 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3934722188925607		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 0.3934722188925607 | validation: 0.3720533229705341]
	TIME [epoch: 10.2 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37055965375415356		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 0.37055965375415356 | validation: 0.3763001347672714]
	TIME [epoch: 10.2 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.417244417064087		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 0.417244417064087 | validation: 0.37268720001239575]
	TIME [epoch: 10.2 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4175727115376121		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 0.4175727115376121 | validation: 0.44788861270612035]
	TIME [epoch: 10.2 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3970414131461456		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 0.3970414131461456 | validation: 0.4195121597254985]
	TIME [epoch: 10.2 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3951361958781773		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 0.3951361958781773 | validation: 0.3774409622003031]
	TIME [epoch: 10.2 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41035717103898356		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 0.41035717103898356 | validation: 0.3798513353047083]
	TIME [epoch: 10.2 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4097019052625052		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 0.4097019052625052 | validation: 0.426818664870588]
	TIME [epoch: 10.2 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41893004832361563		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 0.41893004832361563 | validation: 0.38347336741582766]
	TIME [epoch: 10.2 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4067863444341109		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 0.4067863444341109 | validation: 0.422161710379275]
	TIME [epoch: 10.2 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3931343024986968		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 0.3931343024986968 | validation: 0.3746190272677448]
	TIME [epoch: 10.2 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40266104718730433		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 0.40266104718730433 | validation: 0.37592151990704664]
	TIME [epoch: 10.2 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37848864841027813		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 0.37848864841027813 | validation: 0.4251836716293114]
	TIME [epoch: 10.2 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3834722203346355		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 0.3834722203346355 | validation: 0.39373124479444715]
	TIME [epoch: 10.2 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4055565252614885		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 0.4055565252614885 | validation: 0.36599593990271834]
	TIME [epoch: 10.2 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.398848922584114		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 0.398848922584114 | validation: 0.35739222817737853]
	TIME [epoch: 10.2 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38489211051512023		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 0.38489211051512023 | validation: 0.3967519858070692]
	TIME [epoch: 10.2 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38867752497814834		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 0.38867752497814834 | validation: 0.36521809625806584]
	TIME [epoch: 10.2 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891409551233661		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 0.3891409551233661 | validation: 0.3596574478760664]
	TIME [epoch: 10.2 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36254152708688353		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 0.36254152708688353 | validation: 0.3892339681413932]
	TIME [epoch: 10.2 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39217086468197876		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 0.39217086468197876 | validation: 0.36761365757797315]
	TIME [epoch: 10.2 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3918522311280556		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 0.3918522311280556 | validation: 0.36696772535482347]
	TIME [epoch: 10.2 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3737944734744355		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 0.3737944734744355 | validation: 0.42377662857264853]
	TIME [epoch: 10.2 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3961587485473728		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 0.3961587485473728 | validation: 0.367278424095524]
	TIME [epoch: 10.2 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3952972392247096		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 0.3952972392247096 | validation: 0.44857242288911553]
	TIME [epoch: 10.2 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917677496441744		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 0.3917677496441744 | validation: 0.37814864104745405]
	TIME [epoch: 10.2 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3749952202399052		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 0.3749952202399052 | validation: 0.4203656468108241]
	TIME [epoch: 10.2 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3911333830012356		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 0.3911333830012356 | validation: 0.4141880349855525]
	TIME [epoch: 10.2 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4016035072135843		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 0.4016035072135843 | validation: 0.3768651047376648]
	TIME [epoch: 10.2 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3940885302947216		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 0.3940885302947216 | validation: 0.3946192899973486]
	TIME [epoch: 10.2 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.401801355410176		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 0.401801355410176 | validation: 0.3990277695738019]
	TIME [epoch: 10.2 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3814914340414918		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 0.3814914340414918 | validation: 0.36886223248648037]
	TIME [epoch: 10.2 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3768194399941612		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 0.3768194399941612 | validation: 0.3960309143295179]
	TIME [epoch: 10.2 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38724177569490775		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 0.38724177569490775 | validation: 0.4112164406974804]
	TIME [epoch: 10.2 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3743438484641275		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 0.3743438484641275 | validation: 0.38304749779122255]
	TIME [epoch: 10.2 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.374622073484563		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 0.374622073484563 | validation: 0.35617097494673017]
	TIME [epoch: 10.2 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3750987328957128		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 0.3750987328957128 | validation: 0.4125171271768555]
	TIME [epoch: 10.2 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37699192459999364		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 0.37699192459999364 | validation: 0.3670135141887394]
	TIME [epoch: 10.2 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3730589900744691		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 0.3730589900744691 | validation: 0.4117310508893185]
	TIME [epoch: 10.2 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37356129905138996		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 0.37356129905138996 | validation: 0.4100518860322606]
	TIME [epoch: 10.2 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37023975687799726		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 0.37023975687799726 | validation: 0.40656188897985324]
	TIME [epoch: 10.2 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3852826515155555		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 0.3852826515155555 | validation: 0.37079828203674103]
	TIME [epoch: 10.2 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39180804924744855		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 0.39180804924744855 | validation: 0.42961378250063176]
	TIME [epoch: 10.2 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3998671041017776		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 0.3998671041017776 | validation: 0.4054557216828565]
	TIME [epoch: 10.2 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38050585493423067		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 0.38050585493423067 | validation: 0.40481784420140854]
	TIME [epoch: 10.2 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39010449168833927		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 0.39010449168833927 | validation: 0.369854757185545]
	TIME [epoch: 10.2 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3790889403576201		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 0.3790889403576201 | validation: 0.36157257676983107]
	TIME [epoch: 10.2 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3914689806574737		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 0.3914689806574737 | validation: 0.4031945014189606]
	TIME [epoch: 10.2 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3928872610201295		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 0.3928872610201295 | validation: 0.39998924612136194]
	TIME [epoch: 10.2 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39200639166412843		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 0.39200639166412843 | validation: 0.34620087422213247]
	TIME [epoch: 10.2 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3699588920188398		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 0.3699588920188398 | validation: 0.4030769663595004]
	TIME [epoch: 10.2 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37427814819903593		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 0.37427814819903593 | validation: 0.42763968906783645]
	TIME [epoch: 10.2 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3895133931878915		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 0.3895133931878915 | validation: 0.4142793137559356]
	TIME [epoch: 10.2 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829492931086734		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 0.3829492931086734 | validation: 0.3778072517764768]
	TIME [epoch: 10.2 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3968411280351777		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 0.3968411280351777 | validation: 0.4080575370044822]
	TIME [epoch: 10.2 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3751554736654451		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 0.3751554736654451 | validation: 0.37248748852283053]
	TIME [epoch: 10.2 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37542211715908486		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 0.37542211715908486 | validation: 0.37856418836602984]
	TIME [epoch: 10.2 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.385064831200685		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 0.385064831200685 | validation: 0.3908077601586028]
	TIME [epoch: 10.2 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3794872547419302		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 0.3794872547419302 | validation: 0.3838776510941944]
	TIME [epoch: 10.2 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36850632035106734		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 0.36850632035106734 | validation: 0.36947293237477397]
	TIME [epoch: 10.2 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4022971407338384		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 0.4022971407338384 | validation: 0.3847197588709679]
	TIME [epoch: 10.2 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3821635607903339		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 0.3821635607903339 | validation: 0.4056003603794248]
	TIME [epoch: 10.2 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3889775846481083		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 0.3889775846481083 | validation: 0.3774161859871458]
	TIME [epoch: 10.2 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3817846373362487		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 0.3817846373362487 | validation: 0.4098232626120405]
	TIME [epoch: 10.2 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37821491013074826		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 0.37821491013074826 | validation: 0.3673650750314155]
	TIME [epoch: 10.2 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3847776636784618		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 0.3847776636784618 | validation: 0.39224605680044755]
	TIME [epoch: 10.2 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3880802196991017		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 0.3880802196991017 | validation: 0.4082819136140634]
	TIME [epoch: 10.2 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37498567333930694		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 0.37498567333930694 | validation: 0.40080146453814053]
	TIME [epoch: 10.2 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36177852618310596		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 0.36177852618310596 | validation: 0.39392886580001574]
	TIME [epoch: 10.2 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3639270002986354		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 0.3639270002986354 | validation: 0.3642798911786524]
	TIME [epoch: 10.2 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.373483862239571		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 0.373483862239571 | validation: 0.38090246270881184]
	TIME [epoch: 10.2 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3956483504654153		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 0.3956483504654153 | validation: 0.4310033628859148]
	TIME [epoch: 10.2 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3831435338318099		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 0.3831435338318099 | validation: 0.3807166300942822]
	TIME [epoch: 10.2 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3740446468001203		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 0.3740446468001203 | validation: 0.39994865170340377]
	TIME [epoch: 10.2 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3686300197408143		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 0.3686300197408143 | validation: 0.410257721902713]
	TIME [epoch: 10.2 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.371372113423604		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 0.371372113423604 | validation: 0.3612355943663086]
	TIME [epoch: 10.2 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3821684338538692		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 0.3821684338538692 | validation: 0.395572413201033]
	TIME [epoch: 10.2 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39329102927008347		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 0.39329102927008347 | validation: 0.3870764894161395]
	TIME [epoch: 10.2 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829640006795464		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 0.3829640006795464 | validation: 0.37408290212735806]
	TIME [epoch: 10.2 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3842928644250855		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 0.3842928644250855 | validation: 0.3812551879094957]
	TIME [epoch: 10.2 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3680577605007618		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 0.3680577605007618 | validation: 0.4069145180535352]
	TIME [epoch: 10.2 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38333630441599953		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 0.38333630441599953 | validation: 0.3735386324585952]
	TIME [epoch: 10.2 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37244360783427277		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 0.37244360783427277 | validation: 0.35573764663524865]
	TIME [epoch: 10.2 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3831755785826979		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 0.3831755785826979 | validation: 0.39848574958016614]
	TIME [epoch: 10.2 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3813243808022614		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 0.3813243808022614 | validation: 0.4189460783002732]
	TIME [epoch: 10.2 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3777658404518468		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 0.3777658404518468 | validation: 0.38076985097466975]
	TIME [epoch: 10.2 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37129801521715783		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 0.37129801521715783 | validation: 0.3818881683176404]
	TIME [epoch: 10.2 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3716336217070082		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 0.3716336217070082 | validation: 0.3923948919361709]
	TIME [epoch: 10.2 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3588366765132406		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 0.3588366765132406 | validation: 0.40634867985661216]
	TIME [epoch: 10.2 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3809350705519955		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 0.3809350705519955 | validation: 0.37351830195108465]
	TIME [epoch: 10.2 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3649376327405792		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 0.3649376327405792 | validation: 0.38034534679698256]
	TIME [epoch: 10.2 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3694924781224687		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 0.3694924781224687 | validation: 0.3583999472159786]
	TIME [epoch: 10.2 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3868630308113975		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 0.3868630308113975 | validation: 0.35859192711502147]
	TIME [epoch: 10.2 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3739288572115223		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 0.3739288572115223 | validation: 0.4046853547470907]
	TIME [epoch: 10.2 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37222095232612523		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 0.37222095232612523 | validation: 0.40924498126360564]
	TIME [epoch: 10.2 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3960260193576211		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 0.3960260193576211 | validation: 0.36119244100577164]
	TIME [epoch: 10.2 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3719606459538204		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 0.3719606459538204 | validation: 0.3812984602595536]
	TIME [epoch: 10.2 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3722795983226486		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 0.3722795983226486 | validation: 0.37707412864223677]
	TIME [epoch: 10.2 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3840489693458483		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 0.3840489693458483 | validation: 0.3515566021512295]
	TIME [epoch: 10.2 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3642161995690697		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 0.3642161995690697 | validation: 0.37103136281318067]
	TIME [epoch: 10.2 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.384192915327073		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 0.384192915327073 | validation: 0.36780126150710774]
	TIME [epoch: 10.2 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37151927791853245		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 0.37151927791853245 | validation: 0.3504253156741792]
	TIME [epoch: 10.2 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3607732562239846		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 0.3607732562239846 | validation: 0.37614902227839336]
	TIME [epoch: 10.2 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3896683101505333		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 0.3896683101505333 | validation: 0.37741270304951147]
	TIME [epoch: 10.2 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37412138065862804		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 0.37412138065862804 | validation: 0.38731419449356075]
	TIME [epoch: 10.2 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.381158945393843		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 0.381158945393843 | validation: 0.4209966490965563]
	TIME [epoch: 10.2 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3683999642533425		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 0.3683999642533425 | validation: 0.3574061621519474]
	TIME [epoch: 10.2 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37386162290285796		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 0.37386162290285796 | validation: 0.3762382474806305]
	TIME [epoch: 10.2 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37607901055097587		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 0.37607901055097587 | validation: 0.36744488587696855]
	TIME [epoch: 10.2 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3634094356299773		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 0.3634094356299773 | validation: 0.3528116036346065]
	TIME [epoch: 10.2 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3685701996182628		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 0.3685701996182628 | validation: 0.4119427246493602]
	TIME [epoch: 10.2 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37217287479580624		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 0.37217287479580624 | validation: 0.37051414126686655]
	TIME [epoch: 10.2 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3964524905038774		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 0.3964524905038774 | validation: 0.40844956364289675]
	TIME [epoch: 10.2 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41119179463898037		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 0.41119179463898037 | validation: 0.3951209872715882]
	TIME [epoch: 10.2 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3909253430587656		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 0.3909253430587656 | validation: 0.3696648014818241]
	TIME [epoch: 10.2 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38735208933018445		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 0.38735208933018445 | validation: 0.3674446510631869]
	TIME [epoch: 10.2 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3899650131566305		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 0.3899650131566305 | validation: 0.3454811228741637]
	TIME [epoch: 10.2 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37251244384251814		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 0.37251244384251814 | validation: 0.4375875469290293]
	TIME [epoch: 10.2 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3657368311212647		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 0.3657368311212647 | validation: 0.36812900904933893]
	TIME [epoch: 10.2 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37582655322106434		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 0.37582655322106434 | validation: 0.39170118868248643]
	TIME [epoch: 10.2 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35473249134224477		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 0.35473249134224477 | validation: 0.3747109437173313]
	TIME [epoch: 10.2 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39121830750501463		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 0.39121830750501463 | validation: 0.4094810593314515]
	TIME [epoch: 10.2 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37878420203307445		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 0.37878420203307445 | validation: 0.3603905529229076]
	TIME [epoch: 10.2 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37955449879109543		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 0.37955449879109543 | validation: 0.39389668989137167]
	TIME [epoch: 10.2 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36536763461619826		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 0.36536763461619826 | validation: 0.36956825452780606]
	TIME [epoch: 10.2 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37020484001913745		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 0.37020484001913745 | validation: 0.3838669190491814]
	TIME [epoch: 10.2 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38652587309561326		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 0.38652587309561326 | validation: 0.37005702038937605]
	TIME [epoch: 10.2 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4074926941968281		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 0.4074926941968281 | validation: 0.37319157814507314]
	TIME [epoch: 10.2 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38593098487168986		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 0.38593098487168986 | validation: 0.37792132141474527]
	TIME [epoch: 10.2 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3901974758248861		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 0.3901974758248861 | validation: 0.364712083896502]
	TIME [epoch: 10.2 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38993132010126297		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 0.38993132010126297 | validation: 0.3990462652999966]
	TIME [epoch: 10.2 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3766902172822829		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 0.3766902172822829 | validation: 0.3926167134179368]
	TIME [epoch: 10.2 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38858933409419966		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 0.38858933409419966 | validation: 0.36084843101109004]
	TIME [epoch: 10.2 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38075948175979024		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 0.38075948175979024 | validation: 0.37817975619650157]
	TIME [epoch: 10.2 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3872108655210906		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 0.3872108655210906 | validation: 0.35434074289718587]
	TIME [epoch: 10.2 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36195771883960093		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 0.36195771883960093 | validation: 0.40106826335368745]
	TIME [epoch: 10.2 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3773536761195457		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 0.3773536761195457 | validation: 0.3622912186414991]
	TIME [epoch: 10.2 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3869828803019405		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 0.3869828803019405 | validation: 0.3982766930761287]
	TIME [epoch: 10.2 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3687947584824664		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 0.3687947584824664 | validation: 0.36200331423057797]
	TIME [epoch: 10.2 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3840319922546981		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 0.3840319922546981 | validation: 0.3635505993965952]
	TIME [epoch: 10.2 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37925017662696114		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 0.37925017662696114 | validation: 0.347844109499518]
	TIME [epoch: 10.2 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37836607929032623		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 0.37836607929032623 | validation: 0.37209180915869283]
	TIME [epoch: 10.2 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3837388533130496		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 0.3837388533130496 | validation: 0.39313473068032767]
	TIME [epoch: 10.2 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3788433019755421		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 0.3788433019755421 | validation: 0.411085729305149]
	TIME [epoch: 10.2 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37782464866864635		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 0.37782464866864635 | validation: 0.4075228449327241]
	TIME [epoch: 10.2 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38488316909126674		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 0.38488316909126674 | validation: 0.36458110287263906]
	TIME [epoch: 10.2 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3898933902944045		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 0.3898933902944045 | validation: 0.3769425104688658]
	TIME [epoch: 10.2 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37330053045662687		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 0.37330053045662687 | validation: 0.4342473630521205]
	TIME [epoch: 10.2 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37832002497992895		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 0.37832002497992895 | validation: 0.37025806677868206]
	TIME [epoch: 10.2 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3777330703368186		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 0.3777330703368186 | validation: 0.36443316553655103]
	TIME [epoch: 10.2 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3730908511923058		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 0.3730908511923058 | validation: 0.3894911386696174]
	TIME [epoch: 10.2 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36343910198097995		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 0.36343910198097995 | validation: 0.35700101763382813]
	TIME [epoch: 10.2 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38744797754112215		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 0.38744797754112215 | validation: 0.3595982078224046]
	TIME [epoch: 10.2 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829177578150406		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 0.3829177578150406 | validation: 0.358539306477751]
	TIME [epoch: 10.2 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38200715115093226		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 0.38200715115093226 | validation: 0.4118547505955041]
	TIME [epoch: 10.2 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3793382212780999		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 0.3793382212780999 | validation: 0.4378179681248288]
	TIME [epoch: 10.2 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3712590035635198		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 0.3712590035635198 | validation: 0.4030908932976777]
	TIME [epoch: 10.2 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37264525007050575		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 0.37264525007050575 | validation: 0.38995837726183913]
	TIME [epoch: 10.2 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38020727473430455		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 0.38020727473430455 | validation: 0.3610169742836274]
	TIME [epoch: 10.2 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38491405011657465		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 0.38491405011657465 | validation: 0.39217855693318265]
	TIME [epoch: 10.2 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3860235413387104		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 0.3860235413387104 | validation: 0.4145013998367138]
	TIME [epoch: 10.2 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3769619991525872		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 0.3769619991525872 | validation: 0.39000038931165504]
	TIME [epoch: 10.2 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38081735071165734		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 0.38081735071165734 | validation: 0.39360958127561924]
	TIME [epoch: 10.2 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37346939586969263		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 0.37346939586969263 | validation: 0.391398502236581]
	TIME [epoch: 10.2 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37568243106845517		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 0.37568243106845517 | validation: 0.42908670603361976]
	TIME [epoch: 10.2 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.387966848593405		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 0.387966848593405 | validation: 0.3656546660184038]
	TIME [epoch: 10.2 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36064834474849183		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 0.36064834474849183 | validation: 0.3622600717287217]
	TIME [epoch: 10.2 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36943980908936985		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 0.36943980908936985 | validation: 0.4092473641346224]
	TIME [epoch: 10.2 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38410855439543345		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 0.38410855439543345 | validation: 0.35706142044240335]
	TIME [epoch: 10.2 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38315900947406456		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 0.38315900947406456 | validation: 0.3920785008942626]
	TIME [epoch: 10.2 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3759785786959263		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 0.3759785786959263 | validation: 0.36918268568080725]
	TIME [epoch: 10.2 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38391664800494896		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 0.38391664800494896 | validation: 0.3715369678438338]
	TIME [epoch: 10.2 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3782946228230401		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 0.3782946228230401 | validation: 0.36827094348845507]
	TIME [epoch: 10.2 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3907723204353473		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 0.3907723204353473 | validation: 0.3701087049331845]
	TIME [epoch: 10.2 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4016517459041832		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 0.4016517459041832 | validation: 0.37821301914839006]
	TIME [epoch: 10.2 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38885751076566216		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 0.38885751076566216 | validation: 0.3811103783397332]
	TIME [epoch: 10.2 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4015121640695202		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 0.4015121640695202 | validation: 0.4186062221162103]
	TIME [epoch: 10.2 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40629577395966054		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 0.40629577395966054 | validation: 0.4154916740790216]
	TIME [epoch: 10.2 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39461821965290644		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 0.39461821965290644 | validation: 0.3992706030732305]
	TIME [epoch: 10.2 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4120341653214097		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 0.4120341653214097 | validation: 0.36694143994932277]
	TIME [epoch: 10.2 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3665769878023673		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 0.3665769878023673 | validation: 0.382009736300395]
	TIME [epoch: 10.2 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38392507482089766		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 0.38392507482089766 | validation: 0.3681750125197779]
	TIME [epoch: 10.2 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.384785007048453		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 0.384785007048453 | validation: 0.4062130653806549]
	TIME [epoch: 10.2 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3840631938704961		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 0.3840631938704961 | validation: 0.399088347438266]
	TIME [epoch: 10.2 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38275321412920593		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 0.38275321412920593 | validation: 0.3979663807696073]
	TIME [epoch: 10.2 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38992868956664484		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 0.38992868956664484 | validation: 0.38364023147316145]
	TIME [epoch: 10.2 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40260110127841664		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 0.40260110127841664 | validation: 0.3664212953189404]
	TIME [epoch: 10.2 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37720748842594787		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 0.37720748842594787 | validation: 0.3769877937035986]
	TIME [epoch: 10.2 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37766391908928443		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 0.37766391908928443 | validation: 0.37461436606421844]
	TIME [epoch: 10.2 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37763932485262286		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 0.37763932485262286 | validation: 0.39817975341364714]
	TIME [epoch: 10.2 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38939634516051963		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 0.38939634516051963 | validation: 0.4109462142096792]
	TIME [epoch: 10.2 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38194900291138423		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 0.38194900291138423 | validation: 0.36721221460038506]
	TIME [epoch: 10.2 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36993056776782085		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 0.36993056776782085 | validation: 0.36163881795472874]
	TIME [epoch: 10.2 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39324573923201017		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 0.39324573923201017 | validation: 0.3850088931771579]
	TIME [epoch: 10.2 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3823847677421369		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 0.3823847677421369 | validation: 0.38034803648800675]
	TIME [epoch: 10.2 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3968204543013125		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 0.3968204543013125 | validation: 0.378373221878066]
	TIME [epoch: 10.2 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836214965912149		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 0.3836214965912149 | validation: 0.4002948932190657]
	TIME [epoch: 10.2 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3801572157049419		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 0.3801572157049419 | validation: 0.3609238536146357]
	TIME [epoch: 10.2 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3745875198809998		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 0.3745875198809998 | validation: 0.3763119301341744]
	TIME [epoch: 10.2 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40232062703040883		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 0.40232062703040883 | validation: 0.41525066637944813]
	TIME [epoch: 10.2 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4007548715061115		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 0.4007548715061115 | validation: 0.3535280753578562]
	TIME [epoch: 10.2 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3800331064694133		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 0.3800331064694133 | validation: 0.3598120856619759]
	TIME [epoch: 10.2 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38627119052200704		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 0.38627119052200704 | validation: 0.3652111069052817]
	TIME [epoch: 10.2 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917878602997998		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 0.3917878602997998 | validation: 0.37856973042097053]
	TIME [epoch: 10.2 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3664199252181344		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 0.3664199252181344 | validation: 0.38040528661045414]
	TIME [epoch: 10.2 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3950202486664309		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 0.3950202486664309 | validation: 0.36145053861517296]
	TIME [epoch: 10.2 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.380904663219777		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 0.380904663219777 | validation: 0.3969692226642563]
	TIME [epoch: 10.2 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3792775608327782		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 0.3792775608327782 | validation: 0.37392066844055594]
	TIME [epoch: 10.2 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40598228527198826		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 0.40598228527198826 | validation: 0.39861121725603044]
	TIME [epoch: 10.2 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41220001104609355		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 0.41220001104609355 | validation: 0.39272255771985043]
	TIME [epoch: 10.2 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39479282094816476		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 0.39479282094816476 | validation: 0.377113584352739]
	TIME [epoch: 10.2 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40183467789039196		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 0.40183467789039196 | validation: 0.36235103910291905]
	TIME [epoch: 10.2 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38443496629526513		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 0.38443496629526513 | validation: 0.44118814208932383]
	TIME [epoch: 10.2 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3919920366541068		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 0.3919920366541068 | validation: 0.37008371492110675]
	TIME [epoch: 10.2 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37218087426914703		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 0.37218087426914703 | validation: 0.41708644036336195]
	TIME [epoch: 10.2 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37346149085213554		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 0.37346149085213554 | validation: 0.41640137217654555]
	TIME [epoch: 10.2 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39135381276458875		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 0.39135381276458875 | validation: 0.39662348393289143]
	TIME [epoch: 10.2 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3976591705711279		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 0.3976591705711279 | validation: 0.3721495927525633]
	TIME [epoch: 10.2 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38148601709333263		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 0.38148601709333263 | validation: 0.40636489240827145]
	TIME [epoch: 10.2 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4022813192991679		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 0.4022813192991679 | validation: 0.44655649511939133]
	TIME [epoch: 10.2 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3742088750928354		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 0.3742088750928354 | validation: 0.3641195558856935]
	TIME [epoch: 10.2 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3761148481710226		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 0.3761148481710226 | validation: 0.35774314849102906]
	TIME [epoch: 10.2 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3984480948222702		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 0.3984480948222702 | validation: 0.3712028484336929]
	TIME [epoch: 10.2 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38396543518536574		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 0.38396543518536574 | validation: 0.3571571419454368]
	TIME [epoch: 10.2 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3773867273490724		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 0.3773867273490724 | validation: 0.36217341255073515]
	TIME [epoch: 10.2 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3752781228404394		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 0.3752781228404394 | validation: 0.4443909660252644]
	TIME [epoch: 10.2 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38186912628178693		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 0.38186912628178693 | validation: 0.37257895166064275]
	TIME [epoch: 10.2 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.396208354084587		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 0.396208354084587 | validation: 0.41345426033432114]
	TIME [epoch: 10.2 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39435055468721636		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 0.39435055468721636 | validation: 0.3576063163111629]
	TIME [epoch: 10.2 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38007650169307317		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 0.38007650169307317 | validation: 0.3770343542105853]
	TIME [epoch: 10.2 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3758962586999896		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 0.3758962586999896 | validation: 0.4576954221581432]
	TIME [epoch: 10.2 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4055021692689508		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 0.4055021692689508 | validation: 0.45755031288773523]
	TIME [epoch: 10.2 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39241974407927704		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 0.39241974407927704 | validation: 0.4201995894544915]
	TIME [epoch: 10.2 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38135631870261155		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 0.38135631870261155 | validation: 0.42100638446899963]
	TIME [epoch: 10.2 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3948172190582345		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 0.3948172190582345 | validation: 0.41112426160666604]
	TIME [epoch: 10.2 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40856269951602464		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 0.40856269951602464 | validation: 0.4177919315461023]
	TIME [epoch: 10.2 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3906456112847352		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 0.3906456112847352 | validation: 0.43392544027140545]
	TIME [epoch: 10.2 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38759830575540366		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 0.38759830575540366 | validation: 0.4010552425763514]
	TIME [epoch: 10.2 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3889221661439142		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 0.3889221661439142 | validation: 0.44572883052221707]
	TIME [epoch: 10.2 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38100747153066933		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 0.38100747153066933 | validation: 0.36685640368309247]
	TIME [epoch: 10.2 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3901241449100413		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 0.3901241449100413 | validation: 0.36722664354734624]
	TIME [epoch: 10.2 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3974432660824164		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 0.3974432660824164 | validation: 0.4110509305983725]
	TIME [epoch: 10.2 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41031833577007665		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 0.41031833577007665 | validation: 0.3738008063656835]
	TIME [epoch: 10.2 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38101317604681834		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 0.38101317604681834 | validation: 0.3764132701271625]
	TIME [epoch: 10.2 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3698412509822589		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 0.3698412509822589 | validation: 0.4238656087380621]
	TIME [epoch: 10.2 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38735627475730006		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 0.38735627475730006 | validation: 0.4060296562629222]
	TIME [epoch: 10.2 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3746562791049553		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 0.3746562791049553 | validation: 0.40542974894332323]
	TIME [epoch: 10.2 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37574240470638365		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 0.37574240470638365 | validation: 0.36013891593838154]
	TIME [epoch: 10.2 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39219955645965704		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 0.39219955645965704 | validation: 0.41368816242985235]
	TIME [epoch: 10.2 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38293393337828624		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 0.38293393337828624 | validation: 0.3566754439022267]
	TIME [epoch: 10.2 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38703265650335095		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 0.38703265650335095 | validation: 0.40633801490098537]
	TIME [epoch: 10.2 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3696735832200003		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 0.3696735832200003 | validation: 0.41513895041244253]
	TIME [epoch: 10.2 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3711513209957008		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 0.3711513209957008 | validation: 0.3732544520967227]
	TIME [epoch: 10.2 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39793462643108257		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 0.39793462643108257 | validation: 0.41098224221252383]
	TIME [epoch: 10.2 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3739620480662828		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 0.3739620480662828 | validation: 0.3969234087707633]
	TIME [epoch: 10.2 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37291307650121913		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 0.37291307650121913 | validation: 0.3801235926024332]
	TIME [epoch: 10.2 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39715764451701474		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 0.39715764451701474 | validation: 0.37842900152715514]
	TIME [epoch: 10.2 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36696077115397474		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 0.36696077115397474 | validation: 0.3637270258697652]
	TIME [epoch: 10.2 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3740781156138618		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 0.3740781156138618 | validation: 0.38687455827082035]
	TIME [epoch: 10.2 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3787634364109326		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 0.3787634364109326 | validation: 0.43789050117120776]
	TIME [epoch: 10.2 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3729047510906821		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 0.3729047510906821 | validation: 0.4079126910588164]
	TIME [epoch: 10.2 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3928563391028822		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 0.3928563391028822 | validation: 0.3477929156333582]
	TIME [epoch: 10.2 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37950181922872445		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 0.37950181922872445 | validation: 0.35242336010041736]
	TIME [epoch: 10.2 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4048534529877097		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 0.4048534529877097 | validation: 0.3962682515059116]
	TIME [epoch: 10.2 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39163029945504924		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 0.39163029945504924 | validation: 0.4045403565031176]
	TIME [epoch: 10.2 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39028685613870234		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 0.39028685613870234 | validation: 0.39272806569787017]
	TIME [epoch: 10.2 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36968139420221424		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 0.36968139420221424 | validation: 0.3937159024484087]
	TIME [epoch: 10.2 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38301369753167214		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 0.38301369753167214 | validation: 0.41012909697850214]
	TIME [epoch: 10.2 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39941929201568416		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 0.39941929201568416 | validation: 0.3599717706041593]
	TIME [epoch: 10.2 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36952093603251723		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 0.36952093603251723 | validation: 0.4074094154291589]
	TIME [epoch: 10.2 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38656858763067514		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 0.38656858763067514 | validation: 0.3754419389589577]
	TIME [epoch: 10.2 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3881090886849174		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 0.3881090886849174 | validation: 0.38512279551301265]
	TIME [epoch: 10.2 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37050405549281756		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 0.37050405549281756 | validation: 0.4229196501972893]
	TIME [epoch: 10.2 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39450002790854843		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 0.39450002790854843 | validation: 0.372182919464282]
	TIME [epoch: 10.2 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3741396216141859		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 0.3741396216141859 | validation: 0.3709686134942673]
	TIME [epoch: 10.2 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38445838848085334		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 0.38445838848085334 | validation: 0.40023064342483256]
	TIME [epoch: 10.2 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.385436361308388		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 0.385436361308388 | validation: 0.37856173280866323]
	TIME [epoch: 10.2 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853685589805643		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 0.3853685589805643 | validation: 0.4547834686063628]
	TIME [epoch: 10.2 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3780349219056468		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 0.3780349219056468 | validation: 0.3578667843557439]
	TIME [epoch: 10.2 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39218047935806444		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 0.39218047935806444 | validation: 0.35423514201230716]
	TIME [epoch: 10.2 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3855766508265136		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 0.3855766508265136 | validation: 0.36048826738099626]
	TIME [epoch: 10.2 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40491654828486123		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 0.40491654828486123 | validation: 0.36214246177491105]
	TIME [epoch: 10.2 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38355599563206155		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 0.38355599563206155 | validation: 0.35785273424752123]
	TIME [epoch: 10.2 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38297862018981066		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 0.38297862018981066 | validation: 0.3669903636739699]
	TIME [epoch: 10.2 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3760387505943525		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 0.3760387505943525 | validation: 0.39429620366211]
	TIME [epoch: 10.2 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4019419112289194		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 0.4019419112289194 | validation: 0.3688908266310243]
	TIME [epoch: 10.2 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37174763497744273		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 0.37174763497744273 | validation: 0.3860134527924123]
	TIME [epoch: 10.2 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39423178713156615		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 0.39423178713156615 | validation: 0.3760628741913866]
	TIME [epoch: 10.2 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3771901977188837		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 0.3771901977188837 | validation: 0.3732148542300909]
	TIME [epoch: 10.2 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917798098735422		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 0.3917798098735422 | validation: 0.39796594669666185]
	TIME [epoch: 10.2 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.382874687431487		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 0.382874687431487 | validation: 0.4278188921700374]
	TIME [epoch: 10.2 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3907130693281774		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 0.3907130693281774 | validation: 0.3683007343912428]
	TIME [epoch: 10.2 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3866509576494228		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 0.3866509576494228 | validation: 0.40637534730223607]
	TIME [epoch: 10.2 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37617066668314686		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 0.37617066668314686 | validation: 0.40029821313962777]
	TIME [epoch: 10.2 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39829529473106284		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 0.39829529473106284 | validation: 0.36349815625997794]
	TIME [epoch: 10.2 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3860054006522277		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 0.3860054006522277 | validation: 0.40117799990359737]
	TIME [epoch: 10.2 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38466441438572546		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 0.38466441438572546 | validation: 0.3649608274476239]
	TIME [epoch: 10.2 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37108168467259806		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 0.37108168467259806 | validation: 0.38448331726696]
	TIME [epoch: 10.2 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3875908775553123		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 0.3875908775553123 | validation: 0.35936957221752996]
	TIME [epoch: 10.2 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39190163114235066		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 0.39190163114235066 | validation: 0.38426525958535107]
	TIME [epoch: 10.2 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37932212756205425		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 0.37932212756205425 | validation: 0.4128029421510967]
	TIME [epoch: 10.2 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.390208770404631		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 0.390208770404631 | validation: 0.37621188352708634]
	TIME [epoch: 10.2 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39170181623092404		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 0.39170181623092404 | validation: 0.36625262378918744]
	TIME [epoch: 10.2 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3915235744095874		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 0.3915235744095874 | validation: 0.3495382406495726]
	TIME [epoch: 10.2 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3704922009770627		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 0.3704922009770627 | validation: 0.3840676794952894]
	TIME [epoch: 10.2 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38891258355626984		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 0.38891258355626984 | validation: 0.37436181237484656]
	TIME [epoch: 10.2 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.375140339773266		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 0.375140339773266 | validation: 0.3664649187688784]
	TIME [epoch: 10.2 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3790420433299951		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 0.3790420433299951 | validation: 0.37531448598871014]
	TIME [epoch: 10.2 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38326289256566437		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 0.38326289256566437 | validation: 0.36722193044045326]
	TIME [epoch: 10.2 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38773501150622136		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 0.38773501150622136 | validation: 0.4238307210390113]
	TIME [epoch: 10.2 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39983720165208636		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 0.39983720165208636 | validation: 0.37700871200610037]
	TIME [epoch: 10.2 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38027820948090374		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 0.38027820948090374 | validation: 0.37993724626845277]
	TIME [epoch: 10.2 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38880432811320126		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 0.38880432811320126 | validation: 0.3745807669359579]
	TIME [epoch: 10.2 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3825413362747176		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 0.3825413362747176 | validation: 0.3793246686346518]
	TIME [epoch: 10.2 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4154493257196078		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 0.4154493257196078 | validation: 0.3729756641571177]
	TIME [epoch: 10.2 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38659275553700506		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 0.38659275553700506 | validation: 0.3785136557817515]
	TIME [epoch: 10.2 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3886146790899533		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 0.3886146790899533 | validation: 0.3608283138747622]
	TIME [epoch: 10.2 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38326579145836953		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 0.38326579145836953 | validation: 0.37125192157886544]
	TIME [epoch: 10.2 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829095520023741		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 0.3829095520023741 | validation: 0.3736830434256658]
	TIME [epoch: 10.2 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3901890715465893		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 0.3901890715465893 | validation: 0.37785257612192824]
	TIME [epoch: 10.2 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3800473975074895		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 0.3800473975074895 | validation: 0.3611648663641737]
	TIME [epoch: 10.2 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3876251900650517		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 0.3876251900650517 | validation: 0.3632116088417613]
	TIME [epoch: 10.2 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38494041183609456		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 0.38494041183609456 | validation: 0.41540124044595933]
	TIME [epoch: 10.2 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3923421882110234		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 0.3923421882110234 | validation: 0.40950646008586]
	TIME [epoch: 10.2 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37984482597967784		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 0.37984482597967784 | validation: 0.3896346645654375]
	TIME [epoch: 10.2 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891066255605713		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 0.3891066255605713 | validation: 0.35819170670668987]
	TIME [epoch: 10.2 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38326919740783516		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 0.38326919740783516 | validation: 0.3955442736936739]
	TIME [epoch: 10.2 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.372882503884605		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 0.372882503884605 | validation: 0.39101415117430804]
	TIME [epoch: 10.2 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39129935800364857		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 0.39129935800364857 | validation: 0.3865223639515778]
	TIME [epoch: 10.2 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3871045428820651		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 0.3871045428820651 | validation: 0.3861281759044132]
	TIME [epoch: 10.2 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3901807486173966		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 0.3901807486173966 | validation: 0.4075240032498117]
	TIME [epoch: 10.2 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3839982927070937		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 0.3839982927070937 | validation: 0.4074542728351375]
	TIME [epoch: 10.2 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3730422333404563		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 0.3730422333404563 | validation: 0.3926480244233245]
	TIME [epoch: 10.2 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3875197821402324		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 0.3875197821402324 | validation: 0.3985217635113946]
	TIME [epoch: 10.2 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3747327391087244		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 0.3747327391087244 | validation: 0.38033782962228146]
	TIME [epoch: 10.2 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3809194780546129		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 0.3809194780546129 | validation: 0.35865040558580275]
	TIME [epoch: 10.2 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3762897062625869		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 0.3762897062625869 | validation: 0.38026151339211367]
	TIME [epoch: 10.2 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3762922954577237		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 0.3762922954577237 | validation: 0.36678245290657707]
	TIME [epoch: 10.2 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3768937483162148		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 0.3768937483162148 | validation: 0.40485757317922266]
	TIME [epoch: 10.2 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37017467220023687		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 0.37017467220023687 | validation: 0.36859474348579724]
	TIME [epoch: 10.2 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3745818136361652		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 0.3745818136361652 | validation: 0.4028596053683941]
	TIME [epoch: 10.2 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39015253574853354		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 0.39015253574853354 | validation: 0.3696212951292066]
	TIME [epoch: 10.2 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3920841998678235		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 0.3920841998678235 | validation: 0.4124461320328387]
	TIME [epoch: 10.2 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4002613198943859		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 0.4002613198943859 | validation: 0.3752642280406864]
	TIME [epoch: 10.2 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3930412999732991		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 0.3930412999732991 | validation: 0.3792358737048957]
	TIME [epoch: 10.2 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836797485170151		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 0.3836797485170151 | validation: 0.3904220529885091]
	TIME [epoch: 10.2 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37937355508056736		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 0.37937355508056736 | validation: 0.36111110171901317]
	TIME [epoch: 10.2 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3772667371748953		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 0.3772667371748953 | validation: 0.3730013022296951]
	TIME [epoch: 10.2 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3867110936384309		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 0.3867110936384309 | validation: 0.3928376212383511]
	TIME [epoch: 10.2 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37915695561093116		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 0.37915695561093116 | validation: 0.4224255603468691]
	TIME [epoch: 10.2 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38647680576596966		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 0.38647680576596966 | validation: 0.4078767176436689]
	TIME [epoch: 10.2 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.385906144413018		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 0.385906144413018 | validation: 0.3860612253901888]
	TIME [epoch: 10.2 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3839223893332836		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 0.3839223893332836 | validation: 0.4087652573069079]
	TIME [epoch: 10.2 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3863596234690781		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 0.3863596234690781 | validation: 0.36635458281274025]
	TIME [epoch: 10.2 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37443736399340644		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 0.37443736399340644 | validation: 0.40054446152043205]
	TIME [epoch: 10.2 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3860496279397127		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 0.3860496279397127 | validation: 0.3622702992948205]
	TIME [epoch: 10.2 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3768322795313674		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 0.3768322795313674 | validation: 0.4494800956171842]
	TIME [epoch: 10.2 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39723668541340207		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 0.39723668541340207 | validation: 0.3623610249641382]
	TIME [epoch: 10.2 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37945608869281666		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 0.37945608869281666 | validation: 0.37425567410876753]
	TIME [epoch: 10.2 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3756077226832054		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 0.3756077226832054 | validation: 0.37269291921853975]
	TIME [epoch: 10.2 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36824617368300594		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 0.36824617368300594 | validation: 0.37105232983959313]
	TIME [epoch: 10.2 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37176115285996575		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 0.37176115285996575 | validation: 0.40756882370513803]
	TIME [epoch: 10.2 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37510875346704264		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 0.37510875346704264 | validation: 0.3908238574146408]
	TIME [epoch: 10.2 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3687877080991024		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 0.3687877080991024 | validation: 0.4001594453815519]
	TIME [epoch: 10.2 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.379210096163272		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 0.379210096163272 | validation: 0.432973190194827]
	TIME [epoch: 10.2 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3749574617482853		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 0.3749574617482853 | validation: 0.36711344023713394]
	TIME [epoch: 10.2 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3884695149646728		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 0.3884695149646728 | validation: 0.3992887934549281]
	TIME [epoch: 10.2 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3803793024521936		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 0.3803793024521936 | validation: 0.41933424512829137]
	TIME [epoch: 10.2 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37414477108122934		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 0.37414477108122934 | validation: 0.36852247786051545]
	TIME [epoch: 10.2 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38710953608147824		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 0.38710953608147824 | validation: 0.44093394233325256]
	TIME [epoch: 10.2 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37467924416876375		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 0.37467924416876375 | validation: 0.45781215802828323]
	TIME [epoch: 10.2 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3802688530489597		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 0.3802688530489597 | validation: 0.3974611658889197]
	TIME [epoch: 10.2 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37205677939188186		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 0.37205677939188186 | validation: 0.38693351487998884]
	TIME [epoch: 10.2 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3998116031678226		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 0.3998116031678226 | validation: 0.41096745722985517]
	TIME [epoch: 10.2 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37241722271778754		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 0.37241722271778754 | validation: 0.41139517655931435]
	TIME [epoch: 10.2 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.392817588659724		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 0.392817588659724 | validation: 0.4053124458778462]
	TIME [epoch: 10.2 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39837386166444066		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 0.39837386166444066 | validation: 0.37442894940809995]
	TIME [epoch: 10.2 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853494491295078		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 0.3853494491295078 | validation: 0.4039269042053288]
	TIME [epoch: 10.2 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3839276366433225		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 0.3839276366433225 | validation: 0.3958847335454713]
	TIME [epoch: 10.2 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3875794154960267		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 0.3875794154960267 | validation: 0.3608323034227397]
	TIME [epoch: 10.2 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39626303730006845		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 0.39626303730006845 | validation: 0.3791655416595968]
	TIME [epoch: 10.2 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3876090988265843		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 0.3876090988265843 | validation: 0.36274850351611493]
	TIME [epoch: 10.2 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37520947769030677		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 0.37520947769030677 | validation: 0.4189837620655797]
	TIME [epoch: 10.2 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891675100311558		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 0.3891675100311558 | validation: 0.37839867885583944]
	TIME [epoch: 10.2 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3751148213503226		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 0.3751148213503226 | validation: 0.3800205576588309]
	TIME [epoch: 10.2 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39595312464827936		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 0.39595312464827936 | validation: 0.3716941423739958]
	TIME [epoch: 10.2 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36666501923473227		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 0.36666501923473227 | validation: 0.3612991222365431]
	TIME [epoch: 10.2 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3824721224120054		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 0.3824721224120054 | validation: 0.3580363976973955]
	TIME [epoch: 10.2 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38312918747839914		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 0.38312918747839914 | validation: 0.3809860455246268]
	TIME [epoch: 10.2 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36866969523361304		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 0.36866969523361304 | validation: 0.3635501199698233]
	TIME [epoch: 10.2 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3610006694560133		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 0.3610006694560133 | validation: 0.39692273000596945]
	TIME [epoch: 10.2 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3834815370750241		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 0.3834815370750241 | validation: 0.382960678130965]
	TIME [epoch: 10.2 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3677354499767992		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 0.3677354499767992 | validation: 0.4061264308564412]
	TIME [epoch: 10.2 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37797289277926943		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 0.37797289277926943 | validation: 0.37483990115795296]
	TIME [epoch: 10.2 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38436598825863993		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 0.38436598825863993 | validation: 0.35687131677900513]
	TIME [epoch: 10.2 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39601993383845374		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 0.39601993383845374 | validation: 0.3906051478648589]
	TIME [epoch: 10.2 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3850467046487403		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 0.3850467046487403 | validation: 0.42528177741531425]
	TIME [epoch: 10.2 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37781862859820015		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 0.37781862859820015 | validation: 0.37101674276082774]
	TIME [epoch: 10.2 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37549592454906344		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 0.37549592454906344 | validation: 0.35360542629681746]
	TIME [epoch: 10.2 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3743555678002498		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 0.3743555678002498 | validation: 0.38988846687703826]
	TIME [epoch: 10.2 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3848932862731759		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 0.3848932862731759 | validation: 0.38962913449364517]
	TIME [epoch: 10.2 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37475164893099217		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 0.37475164893099217 | validation: 0.3904226233371836]
	TIME [epoch: 10.2 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38300841874063535		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 0.38300841874063535 | validation: 0.3926665254929966]
	TIME [epoch: 10.2 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3756733321102256		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 0.3756733321102256 | validation: 0.38120950214028354]
	TIME [epoch: 10.2 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3794976597440587		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 0.3794976597440587 | validation: 0.3982750544695442]
	TIME [epoch: 10.2 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3644591564197067		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 0.3644591564197067 | validation: 0.3595860542234701]
	TIME [epoch: 10.2 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37309948068633575		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 0.37309948068633575 | validation: 0.34984290863578765]
	TIME [epoch: 10.2 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3656493888200743		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 0.3656493888200743 | validation: 0.3716259772051352]
	TIME [epoch: 10.2 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3930422947385758		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 0.3930422947385758 | validation: 0.39581874377699827]
	TIME [epoch: 10.2 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3938012852540871		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 0.3938012852540871 | validation: 0.3932308675648089]
	TIME [epoch: 10.2 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917726307446471		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 0.3917726307446471 | validation: 0.3868043067533566]
	TIME [epoch: 10.2 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3790338277315675		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 0.3790338277315675 | validation: 0.44987380725831344]
	TIME [epoch: 10.2 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38771139749356326		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 0.38771139749356326 | validation: 0.3713073788591009]
	TIME [epoch: 10.2 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3856685018424209		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 0.3856685018424209 | validation: 0.39709377109096294]
	TIME [epoch: 10.2 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37451576798617237		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 0.37451576798617237 | validation: 0.3708528799599384]
	TIME [epoch: 10.2 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3724138422254318		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 0.3724138422254318 | validation: 0.3880501478144764]
	TIME [epoch: 10.2 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38520654255075293		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 0.38520654255075293 | validation: 0.3680738514216139]
	TIME [epoch: 10.2 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.371459847138806		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 0.371459847138806 | validation: 0.40015590750588415]
	TIME [epoch: 10.2 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3765400351458907		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 0.3765400351458907 | validation: 0.37913215064692546]
	TIME [epoch: 10.2 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3684275438713329		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 0.3684275438713329 | validation: 0.35800654911111734]
	TIME [epoch: 10.2 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3680885863829911		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 0.3680885863829911 | validation: 0.35656286544110954]
	TIME [epoch: 10.2 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3603373270621762		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 0.3603373270621762 | validation: 0.36752930379887544]
	TIME [epoch: 10.2 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3832169615967276		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 0.3832169615967276 | validation: 0.39162825488736497]
	TIME [epoch: 10.2 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36145308851671853		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 0.36145308851671853 | validation: 0.39477364310614094]
	TIME [epoch: 10.2 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38846973539812424		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 0.38846973539812424 | validation: 0.39863853281806044]
	TIME [epoch: 10.2 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38462170987921596		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 0.38462170987921596 | validation: 0.3698098899492635]
	TIME [epoch: 10.2 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3879644125390235		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 0.3879644125390235 | validation: 0.4033286957528738]
	TIME [epoch: 10.2 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3847700630372885		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 0.3847700630372885 | validation: 0.37041689683939977]
	TIME [epoch: 10.2 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38416864977686266		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 0.38416864977686266 | validation: 0.39678476831235077]
	TIME [epoch: 10.2 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3712535608576609		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 0.3712535608576609 | validation: 0.36674534776342504]
	TIME [epoch: 10.2 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37476808950106744		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 0.37476808950106744 | validation: 0.3664125656735568]
	TIME [epoch: 10.2 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3776863287329411		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 0.3776863287329411 | validation: 0.3925048497487418]
	TIME [epoch: 10.2 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3764154916638303		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 0.3764154916638303 | validation: 0.3805596296135869]
	TIME [epoch: 10.2 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37263814883221685		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 0.37263814883221685 | validation: 0.3716350891557562]
	TIME [epoch: 10.2 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38111566944216296		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 0.38111566944216296 | validation: 0.4176623741863888]
	TIME [epoch: 10.2 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3812123284403889		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 0.3812123284403889 | validation: 0.3998443871961485]
	TIME [epoch: 10.2 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3665869759126116		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 0.3665869759126116 | validation: 0.3571060265663911]
	TIME [epoch: 10.2 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3848514136208376		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 0.3848514136208376 | validation: 0.42117852284890306]
	TIME [epoch: 10.2 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3930569940896526		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 0.3930569940896526 | validation: 0.38988955858344676]
	TIME [epoch: 10.3 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3700447228585834		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 0.3700447228585834 | validation: 0.35889087825710275]
	TIME [epoch: 10.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853541493129716		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 0.3853541493129716 | validation: 0.3753057751906267]
	TIME [epoch: 10.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4007337834267613		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 0.4007337834267613 | validation: 0.414903783272924]
	TIME [epoch: 10.2 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3760796921714783		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 0.3760796921714783 | validation: 0.36259757029009493]
	TIME [epoch: 10.2 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37426640514046516		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 0.37426640514046516 | validation: 0.4287054911390703]
	TIME [epoch: 10.2 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39444816736694605		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 0.39444816736694605 | validation: 0.38490674791470386]
	TIME [epoch: 10.2 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3724694703223028		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 0.3724694703223028 | validation: 0.3762907717214799]
	TIME [epoch: 10.2 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37160330514459844		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 0.37160330514459844 | validation: 0.4000669676816137]
	TIME [epoch: 10.2 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3685275411922315		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 0.3685275411922315 | validation: 0.39645040349958666]
	TIME [epoch: 10.2 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3878880168830159		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 0.3878880168830159 | validation: 0.35236977922667506]
	TIME [epoch: 10.2 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3827745987012273		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 0.3827745987012273 | validation: 0.3640079030936147]
	TIME [epoch: 10.2 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3794165105739108		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 0.3794165105739108 | validation: 0.35491518099708785]
	TIME [epoch: 10.2 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853473817533853		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 0.3853473817533853 | validation: 0.4027287102122658]
	TIME [epoch: 10.2 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3789550784662581		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 0.3789550784662581 | validation: 0.36065813431381294]
	TIME [epoch: 10.2 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3866150243777523		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 0.3866150243777523 | validation: 0.37767092064442304]
	TIME [epoch: 10.2 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39414059470871277		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 0.39414059470871277 | validation: 0.40719542900046135]
	TIME [epoch: 10.2 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3734363892763619		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 0.3734363892763619 | validation: 0.42090913016262443]
	TIME [epoch: 10.2 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36794284810338607		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 0.36794284810338607 | validation: 0.4026282919103275]
	TIME [epoch: 10.2 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37295900783867153		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 0.37295900783867153 | validation: 0.44164721178847816]
	TIME [epoch: 10.2 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38241385662812044		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 0.38241385662812044 | validation: 0.38389739998836747]
	TIME [epoch: 10.2 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36763573208718364		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 0.36763573208718364 | validation: 0.440096291211232]
	TIME [epoch: 10.2 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38072415556730516		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 0.38072415556730516 | validation: 0.37134434445864145]
	TIME [epoch: 10.2 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38607790302443135		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 0.38607790302443135 | validation: 0.3818486178724966]
	TIME [epoch: 10.2 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3886439035434192		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 0.3886439035434192 | validation: 0.3808272828795549]
	TIME [epoch: 10.2 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38729027913789943		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 0.38729027913789943 | validation: 0.36610392851662893]
	TIME [epoch: 10.2 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3893297084994608		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 0.3893297084994608 | validation: 0.4041713979414926]
	TIME [epoch: 10.2 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37520320272027274		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 0.37520320272027274 | validation: 0.40636000342786477]
	TIME [epoch: 10.2 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3835284293165529		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 0.3835284293165529 | validation: 0.3612025665493364]
	TIME [epoch: 10.2 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3814766675899248		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 0.3814766675899248 | validation: 0.3638583522996342]
	TIME [epoch: 10.2 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36859410444334234		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 0.36859410444334234 | validation: 0.38019344760953594]
	TIME [epoch: 10.2 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.376692114833115		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 0.376692114833115 | validation: 0.38145672557129834]
	TIME [epoch: 10.2 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3626165768408642		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 0.3626165768408642 | validation: 0.4003096314520532]
	TIME [epoch: 10.2 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3792907266751276		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 0.3792907266751276 | validation: 0.419792136806826]
	TIME [epoch: 10.2 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3697234376981362		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 0.3697234376981362 | validation: 0.3642226197591745]
	TIME [epoch: 10.2 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3833662720399186		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 0.3833662720399186 | validation: 0.39516471377730705]
	TIME [epoch: 10.2 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3740926572265553		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 0.3740926572265553 | validation: 0.3747030198376301]
	TIME [epoch: 10.2 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3928810563644223		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 0.3928810563644223 | validation: 0.3634339195386938]
	TIME [epoch: 10.2 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3844461040705083		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 0.3844461040705083 | validation: 0.37810478938015735]
	TIME [epoch: 10.2 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3802388451496365		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 0.3802388451496365 | validation: 0.3713413800754135]
	TIME [epoch: 10.2 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3909809521050982		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 0.3909809521050982 | validation: 0.39192404160538497]
	TIME [epoch: 10.2 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3733898840795731		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 0.3733898840795731 | validation: 0.37451952521755566]
	TIME [epoch: 10.2 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3957762520729925		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 0.3957762520729925 | validation: 0.3499197036225432]
	TIME [epoch: 10.2 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3739989600729291		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 0.3739989600729291 | validation: 0.3803518449447698]
	TIME [epoch: 10.2 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3730750977176287		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 0.3730750977176287 | validation: 0.39274891680869867]
	TIME [epoch: 10.2 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3779498517977829		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 0.3779498517977829 | validation: 0.37802428254463494]
	TIME [epoch: 10.2 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3674021442251042		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 0.3674021442251042 | validation: 0.3902320359731924]
	TIME [epoch: 10.2 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38417730395030963		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 0.38417730395030963 | validation: 0.4203235023364127]
	TIME [epoch: 10.2 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.395054056084328		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 0.395054056084328 | validation: 0.35701161711931134]
	TIME [epoch: 10.2 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38319851682297484		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 0.38319851682297484 | validation: 0.40050899638447396]
	TIME [epoch: 10.2 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38765932776227024		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 0.38765932776227024 | validation: 0.3608991923471869]
	TIME [epoch: 10.2 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38715262551263374		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 0.38715262551263374 | validation: 0.3626796072800195]
	TIME [epoch: 10.2 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36174890978752744		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 0.36174890978752744 | validation: 0.3952507231744032]
	TIME [epoch: 10.2 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37035224586629206		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 0.37035224586629206 | validation: 0.3613698874019032]
	TIME [epoch: 10.2 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38223136070670344		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 0.38223136070670344 | validation: 0.3726000179455899]
	TIME [epoch: 10.2 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38091690512001886		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 0.38091690512001886 | validation: 0.3722689560968054]
	TIME [epoch: 10.2 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39448165422513765		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 0.39448165422513765 | validation: 0.36476725511238245]
	TIME [epoch: 10.2 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3788405394861624		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 0.3788405394861624 | validation: 0.40667630962018314]
	TIME [epoch: 10.2 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3856067820711789		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 0.3856067820711789 | validation: 0.37873685681957]
	TIME [epoch: 10.2 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3957274265045814		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 0.3957274265045814 | validation: 0.4459244709035254]
	TIME [epoch: 10.2 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3892735475989449		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 0.3892735475989449 | validation: 0.4129513800410331]
	TIME [epoch: 10.2 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3843927264400382		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 0.3843927264400382 | validation: 0.38061796697424466]
	TIME [epoch: 10.2 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38229374193443777		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 0.38229374193443777 | validation: 0.39612845467336516]
	TIME [epoch: 10.2 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3816042249752002		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 0.3816042249752002 | validation: 0.4040732469532638]
	TIME [epoch: 10.2 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38794858474702804		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 0.38794858474702804 | validation: 0.3615419977167329]
	TIME [epoch: 10.2 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3663097145089938		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 0.3663097145089938 | validation: 0.3847717057411788]
	TIME [epoch: 10.2 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37758992781620243		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 0.37758992781620243 | validation: 0.40487912019359507]
	TIME [epoch: 10.2 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3757920083094827		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 0.3757920083094827 | validation: 0.3720840158237488]
	TIME [epoch: 10.2 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37804635611163584		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 0.37804635611163584 | validation: 0.3985377646772578]
	TIME [epoch: 10.2 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3929457574472758		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 0.3929457574472758 | validation: 0.37846085879997843]
	TIME [epoch: 10.2 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36429340279668754		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 0.36429340279668754 | validation: 0.3511701518124273]
	TIME [epoch: 10.2 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3766202553494112		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 0.3766202553494112 | validation: 0.3974023369256671]
	TIME [epoch: 10.2 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38412574580852255		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 0.38412574580852255 | validation: 0.35241515635066506]
	TIME [epoch: 10.2 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37155762426110456		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 0.37155762426110456 | validation: 0.363218430344829]
	TIME [epoch: 10.2 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38811521145695804		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 0.38811521145695804 | validation: 0.40428577258358495]
	TIME [epoch: 10.2 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3803233725693537		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 0.3803233725693537 | validation: 0.40181780660235106]
	TIME [epoch: 10.2 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3891928366621674		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 0.3891928366621674 | validation: 0.3838764542369216]
	TIME [epoch: 10.2 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37919346456977127		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 0.37919346456977127 | validation: 0.3684551944988749]
	TIME [epoch: 10.2 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3733168470359868		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 0.3733168470359868 | validation: 0.38816980057251826]
	TIME [epoch: 10.2 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3762585060825336		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 0.3762585060825336 | validation: 0.36974535370199335]
	TIME [epoch: 10.2 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3742947993839343		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 0.3742947993839343 | validation: 0.37967170963294694]
	TIME [epoch: 10.2 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38792603882940335		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 0.38792603882940335 | validation: 0.37791131282629864]
	TIME [epoch: 10.2 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37492434735716273		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 0.37492434735716273 | validation: 0.37780566094098256]
	TIME [epoch: 10.2 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38338407151577253		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 0.38338407151577253 | validation: 0.3886185164746563]
	TIME [epoch: 10.2 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37149641393111366		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 0.37149641393111366 | validation: 0.3947304613117959]
	TIME [epoch: 10.2 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3812066709635232		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 0.3812066709635232 | validation: 0.40528783732770124]
	TIME [epoch: 10.2 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4019899593175212		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 0.4019899593175212 | validation: 0.3640653448930093]
	TIME [epoch: 10.2 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3988837828720448		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 0.3988837828720448 | validation: 0.3893814423029056]
	TIME [epoch: 10.2 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38322860852447127		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 0.38322860852447127 | validation: 0.3931998030834713]
	TIME [epoch: 10.2 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38895604923169047		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 0.38895604923169047 | validation: 0.4114836974096573]
	TIME [epoch: 10.2 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.372220070872993		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 0.372220070872993 | validation: 0.38123697291808684]
	TIME [epoch: 10.2 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3839214559162053		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 0.3839214559162053 | validation: 0.36964050992138897]
	TIME [epoch: 10.2 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37391616065225863		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 0.37391616065225863 | validation: 0.39589113536029774]
	TIME [epoch: 10.2 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3847969724641059		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 0.3847969724641059 | validation: 0.42472805721715623]
	TIME [epoch: 10.2 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37247746585045105		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 0.37247746585045105 | validation: 0.39090869253412175]
	TIME [epoch: 10.2 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37737905475662564		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 0.37737905475662564 | validation: 0.43637297406881614]
	TIME [epoch: 10.2 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3903040639866576		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 0.3903040639866576 | validation: 0.37155359341053756]
	TIME [epoch: 10.2 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853888893035025		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 0.3853888893035025 | validation: 0.40383124100039186]
	TIME [epoch: 10.2 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4124258659637202		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 0.4124258659637202 | validation: 0.3820584801014803]
	TIME [epoch: 10.2 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3906027839173136		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 0.3906027839173136 | validation: 0.3891284898148078]
	TIME [epoch: 10.2 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829591023710882		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 0.3829591023710882 | validation: 0.3692480551805193]
	TIME [epoch: 10.2 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37731181141462067		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 0.37731181141462067 | validation: 0.3616902429477885]
	TIME [epoch: 10.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.378798159911459		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 0.378798159911459 | validation: 0.4005536699205726]
	TIME [epoch: 10.2 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3830001220729587		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 0.3830001220729587 | validation: 0.36715655958148774]
	TIME [epoch: 10.2 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3768799660211376		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 0.3768799660211376 | validation: 0.36990181760351704]
	TIME [epoch: 10.2 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3739126308287274		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 0.3739126308287274 | validation: 0.38085505231869093]
	TIME [epoch: 10.2 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3673191901289648		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 0.3673191901289648 | validation: 0.3651364883950835]
	TIME [epoch: 10.2 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37826235830810095		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 0.37826235830810095 | validation: 0.3778177851530423]
	TIME [epoch: 10.2 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3710792942227618		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 0.3710792942227618 | validation: 0.39630056974870576]
	TIME [epoch: 10.2 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38648376560005226		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 0.38648376560005226 | validation: 0.39255462369000427]
	TIME [epoch: 10.2 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38384972836821696		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 0.38384972836821696 | validation: 0.40653133590848567]
	TIME [epoch: 10.2 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38557074589872775		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 0.38557074589872775 | validation: 0.37841113562929873]
	TIME [epoch: 10.2 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38531125765747265		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 0.38531125765747265 | validation: 0.4013013587017706]
	TIME [epoch: 10.2 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.395051550073655		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 0.395051550073655 | validation: 0.3608523887494542]
	TIME [epoch: 10.2 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36982918493000727		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 0.36982918493000727 | validation: 0.3980727611707663]
	TIME [epoch: 10.2 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38596467031304316		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 0.38596467031304316 | validation: 0.4019773044046579]
	TIME [epoch: 10.2 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3681179336035771		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 0.3681179336035771 | validation: 0.3766062592553998]
	TIME [epoch: 10.2 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3797519443835048		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 0.3797519443835048 | validation: 0.3684048688500664]
	TIME [epoch: 10.2 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3795342922728667		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 0.3795342922728667 | validation: 0.3651651641886039]
	TIME [epoch: 10.2 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37870406282368263		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 0.37870406282368263 | validation: 0.40727058548453826]
	TIME [epoch: 10.2 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39002481133384687		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 0.39002481133384687 | validation: 0.376449614013723]
	TIME [epoch: 10.2 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3774899586857713		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 0.3774899586857713 | validation: 0.39874793371328937]
	TIME [epoch: 10.2 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37536656445566674		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 0.37536656445566674 | validation: 0.37195012590469506]
	TIME [epoch: 10.2 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38927984037451374		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 0.38927984037451374 | validation: 0.3962552682169055]
	TIME [epoch: 10.2 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3947448207957331		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 0.3947448207957331 | validation: 0.3675286803877671]
	TIME [epoch: 10.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36725742068644446		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 0.36725742068644446 | validation: 0.36839846553376554]
	TIME [epoch: 10.2 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36294481189139716		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 0.36294481189139716 | validation: 0.4553832437811824]
	TIME [epoch: 10.2 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37775387034338437		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 0.37775387034338437 | validation: 0.40075032690873413]
	TIME [epoch: 10.2 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38218997347780165		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 0.38218997347780165 | validation: 0.35739077628622684]
	TIME [epoch: 10.2 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3663939846140823		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 0.3663939846140823 | validation: 0.3784678658694419]
	TIME [epoch: 10.2 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3748940347315225		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 0.3748940347315225 | validation: 0.4081261790476651]
	TIME [epoch: 10.2 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3847458762936565		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 0.3847458762936565 | validation: 0.36128644872589283]
	TIME [epoch: 10.2 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3785599362833049		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 0.3785599362833049 | validation: 0.40055256132561967]
	TIME [epoch: 10.2 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39899406206880683		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 0.39899406206880683 | validation: 0.3681183391311182]
	TIME [epoch: 10.2 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3811343684862825		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 0.3811343684862825 | validation: 0.3661129946654363]
	TIME [epoch: 10.2 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38333784176074037		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 0.38333784176074037 | validation: 0.3908619578170017]
	TIME [epoch: 10.2 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38327591317390325		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 0.38327591317390325 | validation: 0.35900565947273083]
	TIME [epoch: 10.2 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3765872968323929		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 0.3765872968323929 | validation: 0.37763303685451355]
	TIME [epoch: 10.2 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3960567203295312		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 0.3960567203295312 | validation: 0.3733686194050538]
	TIME [epoch: 10.2 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3855130329686344		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 0.3855130329686344 | validation: 0.4061784984810856]
	TIME [epoch: 10.2 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3825630759385767		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 0.3825630759385767 | validation: 0.36240765142991876]
	TIME [epoch: 10.2 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39245845275976776		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 0.39245845275976776 | validation: 0.37542308692094467]
	TIME [epoch: 10.2 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39473321220330193		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 0.39473321220330193 | validation: 0.3691021031403813]
	TIME [epoch: 10.2 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3884125273668585		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 0.3884125273668585 | validation: 0.3844171455810312]
	TIME [epoch: 10.2 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4083226383756317		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 0.4083226383756317 | validation: 0.3899404771351669]
	TIME [epoch: 10.2 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.368195906738402		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 0.368195906738402 | validation: 0.4270157059672805]
	TIME [epoch: 10.2 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37779448084349243		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 0.37779448084349243 | validation: 0.3642624400306174]
	TIME [epoch: 10.2 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38026641504702996		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 0.38026641504702996 | validation: 0.40295999586711356]
	TIME [epoch: 10.2 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37353769710346085		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 0.37353769710346085 | validation: 0.4153574882980524]
	TIME [epoch: 10.2 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3853649656278394		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 0.3853649656278394 | validation: 0.40744671465529264]
	TIME [epoch: 10.2 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37400186399786994		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 0.37400186399786994 | validation: 0.3924780398128678]
	TIME [epoch: 10.2 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3860269365184174		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 0.3860269365184174 | validation: 0.392431499926985]
	TIME [epoch: 10.2 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3854714595157722		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 0.3854714595157722 | validation: 0.3937685021446004]
	TIME [epoch: 10.2 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3806453301174999		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 0.3806453301174999 | validation: 0.3719179562681123]
	TIME [epoch: 10.2 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3706654706901859		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 0.3706654706901859 | validation: 0.4553974525152924]
	TIME [epoch: 10.2 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.378906075280456		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 0.378906075280456 | validation: 0.3819359508911758]
	TIME [epoch: 10.2 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3821309033578172		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 0.3821309033578172 | validation: 0.3801744327199526]
	TIME [epoch: 10.2 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3718471113607097		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 0.3718471113607097 | validation: 0.3770562026813141]
	TIME [epoch: 10.2 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3954961200562149		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 0.3954961200562149 | validation: 0.3875915470350432]
	TIME [epoch: 10.2 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37763555346484895		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 0.37763555346484895 | validation: 0.3896491504761074]
	TIME [epoch: 10.2 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3838482682596652		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 0.3838482682596652 | validation: 0.3825161952528208]
	TIME [epoch: 10.2 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3686178665892929		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 0.3686178665892929 | validation: 0.41195746897631524]
	TIME [epoch: 10.2 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3828939759511988		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 0.3828939759511988 | validation: 0.35950978491778246]
	TIME [epoch: 10.2 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3564312912703076		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 0.3564312912703076 | validation: 0.3878170170487421]
	TIME [epoch: 10.2 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3856759991044293		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 0.3856759991044293 | validation: 0.4127837927842377]
	TIME [epoch: 10.2 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38090207942705046		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 0.38090207942705046 | validation: 0.3976830823218118]
	TIME [epoch: 10.2 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37531882506026243		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 0.37531882506026243 | validation: 0.43352409131110237]
	TIME [epoch: 10.2 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3828050252649583		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 0.3828050252649583 | validation: 0.3810483420740532]
	TIME [epoch: 10.2 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3826515612759118		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 0.3826515612759118 | validation: 0.3610041259602971]
	TIME [epoch: 10.2 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3750327537787349		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 0.3750327537787349 | validation: 0.36624479334954524]
	TIME [epoch: 10.2 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37598107396124986		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 0.37598107396124986 | validation: 0.4082170747366314]
	TIME [epoch: 10.2 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37252716568828637		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 0.37252716568828637 | validation: 0.39254752718687896]
	TIME [epoch: 10.2 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36775431740429393		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 0.36775431740429393 | validation: 0.3600044324463316]
	TIME [epoch: 10.2 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3866153100839623		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 0.3866153100839623 | validation: 0.40180315419456447]
	TIME [epoch: 10.2 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39235320917104033		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 0.39235320917104033 | validation: 0.3860891532053596]
	TIME [epoch: 10.2 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3775749521302994		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 0.3775749521302994 | validation: 0.3617384092291475]
	TIME [epoch: 10.2 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37813726336183917		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 0.37813726336183917 | validation: 0.3653773155061804]
	TIME [epoch: 10.2 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37653367912576513		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 0.37653367912576513 | validation: 0.3856078838818799]
	TIME [epoch: 10.2 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39930356650585563		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 0.39930356650585563 | validation: 0.3992040268453556]
	TIME [epoch: 10.2 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37513373660199284		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 0.37513373660199284 | validation: 0.3941188734907656]
	TIME [epoch: 10.2 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37142634291728116		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 0.37142634291728116 | validation: 0.36644901571798544]
	TIME [epoch: 10.2 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38409890347526177		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 0.38409890347526177 | validation: 0.399641124964398]
	TIME [epoch: 10.2 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3867694094332989		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 0.3867694094332989 | validation: 0.3745165780739063]
	TIME [epoch: 10.2 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37379814539184697		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 0.37379814539184697 | validation: 0.3662863974602148]
	TIME [epoch: 10.2 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3857891038334074		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 0.3857891038334074 | validation: 0.374602488583601]
	TIME [epoch: 10.2 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3775207835355828		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 0.3775207835355828 | validation: 0.36491796651244374]
	TIME [epoch: 10.2 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37742329030376387		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 0.37742329030376387 | validation: 0.3659925138568805]
	TIME [epoch: 10.2 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39162696361040417		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 0.39162696361040417 | validation: 0.3968436274277943]
	TIME [epoch: 10.2 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3695779890894316		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 0.3695779890894316 | validation: 0.3810909020601963]
	TIME [epoch: 10.2 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36324066277819667		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 0.36324066277819667 | validation: 0.360303722937261]
	TIME [epoch: 10.2 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3699036742739801		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 0.3699036742739801 | validation: 0.40163321751963926]
	TIME [epoch: 10.2 sec]
Finished training in 20582.320 seconds.
