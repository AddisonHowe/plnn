Args:
Namespace(name='model_tr_study205', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0', training_data='data/transition_rate_studies/tr_study205/tr_study205_training/r0', validation_data='data/transition_rate_studies/tr_study205/tr_study205_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 720659299

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.385917511496638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.385917511496638 | validation: 10.084462506514226]
	TIME [epoch: 78.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.86627695423005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.86627695423005 | validation: 9.675134624280442]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.127314466756204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.127314466756204 | validation: 9.956470628198636]
	TIME [epoch: 9.74 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.226851966917383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.226851966917383 | validation: 10.940410450764924]
	TIME [epoch: 9.76 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.260692076928521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.260692076928521 | validation: 7.428036185263607]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.226937569700613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.226937569700613 | validation: 8.79803855261949]
	TIME [epoch: 9.75 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.34245400280581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.34245400280581 | validation: 10.451720265302338]
	TIME [epoch: 9.77 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.47068669252074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.47068669252074 | validation: 6.9954561059969755]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.006829051862757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.006829051862757 | validation: 8.058551572864268]
	TIME [epoch: 9.73 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.520179560074385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.520179560074385 | validation: 7.355988263442607]
	TIME [epoch: 9.73 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.259263078055847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.259263078055847 | validation: 7.091765937047386]
	TIME [epoch: 9.77 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.105882012579065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.105882012579065 | validation: 6.961152484615841]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.067240870589016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.067240870589016 | validation: 7.194891828642662]
	TIME [epoch: 9.73 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.899446011395375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.899446011395375 | validation: 7.206271393282914]
	TIME [epoch: 9.72 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.834877140195635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.834877140195635 | validation: 7.020455593483436]
	TIME [epoch: 9.74 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.916266902946274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.916266902946274 | validation: 6.6735656594371795]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.696300792436175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.696300792436175 | validation: 6.836460005834231]
	TIME [epoch: 9.73 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.59419075855971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.59419075855971 | validation: 6.414668402844038]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.569355496284184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.569355496284184 | validation: 6.833476508177894]
	TIME [epoch: 9.73 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.493019045506019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.493019045506019 | validation: 6.545488298299782]
	TIME [epoch: 9.73 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.347418375313739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.347418375313739 | validation: 6.278994536192388]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.510308452090913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.510308452090913 | validation: 5.863487772538735]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.534068903984822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.534068903984822 | validation: 7.856608993224523]
	TIME [epoch: 9.74 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.550060939847768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.550060939847768 | validation: 6.5202022774854935]
	TIME [epoch: 9.74 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.331693889376346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.331693889376346 | validation: 7.743331909514944]
	TIME [epoch: 9.75 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.387896921994553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.387896921994553 | validation: 6.2361628300584355]
	TIME [epoch: 9.76 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.038337213718504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.038337213718504 | validation: 7.54309365452486]
	TIME [epoch: 9.73 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.281641938468195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.281641938468195 | validation: 5.907580141616789]
	TIME [epoch: 9.73 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.781283118584931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.781283118584931 | validation: 7.753258682663498]
	TIME [epoch: 9.75 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.025720465097156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.025720465097156 | validation: 6.131191235466524]
	TIME [epoch: 9.74 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.583346563132435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.583346563132435 | validation: 6.943588601529509]
	TIME [epoch: 9.74 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8509597081080305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8509597081080305 | validation: 6.516037893732282]
	TIME [epoch: 9.74 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8321075331103085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8321075331103085 | validation: 7.620665623082822]
	TIME [epoch: 9.76 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.993765090634069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.993765090634069 | validation: 5.997608873145951]
	TIME [epoch: 9.74 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6040754082283355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6040754082283355 | validation: 7.806028512924857]
	TIME [epoch: 9.74 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.080063253750683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.080063253750683 | validation: 5.939192911128641]
	TIME [epoch: 9.74 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.522029117072207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.522029117072207 | validation: 7.479657218127449]
	TIME [epoch: 9.76 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.855413150226558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.855413150226558 | validation: 5.78530188544284]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.395107740087807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.395107740087807 | validation: 7.387045167914534]
	TIME [epoch: 9.73 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4846753507116075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4846753507116075 | validation: 5.287292656717373]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.585986896241434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.585986896241434 | validation: 5.496362277032248]
	TIME [epoch: 9.73 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.54386233429949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.54386233429949 | validation: 5.317707879146244]
	TIME [epoch: 9.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.39974899394115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.39974899394115 | validation: 5.451642985604492]
	TIME [epoch: 9.73 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.322926144306041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.322926144306041 | validation: 5.537699152240471]
	TIME [epoch: 9.75 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.268168829625328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.268168829625328 | validation: 5.813524751282943]
	TIME [epoch: 9.72 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3774668359865965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3774668359865965 | validation: 5.6295709698044405]
	TIME [epoch: 9.73 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.341048542651173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.341048542651173 | validation: 5.26627599390371]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1259136080986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1259136080986 | validation: 5.116931934961248]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.10022779809038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.10022779809038 | validation: 5.3623778618375795]
	TIME [epoch: 9.73 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176704750717029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176704750717029 | validation: 5.35804117468286]
	TIME [epoch: 9.73 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.161636191729048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.161636191729048 | validation: 5.082978779941844]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.092494154678426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.092494154678426 | validation: 5.337130731030222]
	TIME [epoch: 9.74 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.123171437029605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.123171437029605 | validation: 5.1211007841251535]
	TIME [epoch: 9.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.233620695017381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.233620695017381 | validation: 5.074654479182877]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9605352392021294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9605352392021294 | validation: 5.619356156411774]
	TIME [epoch: 9.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.15175033994946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.15175033994946 | validation: 4.995278395131644]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9901561318198056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9901561318198056 | validation: 5.2472038413461375]
	TIME [epoch: 9.73 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.04919378360715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.04919378360715 | validation: 5.016704652686375]
	TIME [epoch: 9.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9664152264292873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9664152264292873 | validation: 4.96443805109451]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.976803316655998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.976803316655998 | validation: 4.884061880025463]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.902967576024966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.902967576024966 | validation: 5.064531213051733]
	TIME [epoch: 9.73 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.888376620754012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.888376620754012 | validation: 5.018373257997115]
	TIME [epoch: 9.74 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.913640203904092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.913640203904092 | validation: 4.965508504656851]
	TIME [epoch: 9.74 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769323163521766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.769323163521766 | validation: 5.203097962288232]
	TIME [epoch: 9.74 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832911460769503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.832911460769503 | validation: 4.929038335506658]
	TIME [epoch: 9.73 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8143020486523866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8143020486523866 | validation: 5.062265792269351]
	TIME [epoch: 9.75 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776672113237076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.776672113237076 | validation: 4.865745722803245]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.795979531853942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.795979531853942 | validation: 4.63894830335996]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814339772151962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.814339772151962 | validation: 5.205143287251127]
	TIME [epoch: 9.76 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8355944836302287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8355944836302287 | validation: 5.196966837233305]
	TIME [epoch: 9.75 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7181268918511434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7181268918511434 | validation: 5.255593184597766]
	TIME [epoch: 9.74 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7825087085711515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7825087085711515 | validation: 4.819148076499259]
	TIME [epoch: 9.75 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676621015308572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.676621015308572 | validation: 5.052813815230105]
	TIME [epoch: 9.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.758727519691824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.758727519691824 | validation: 4.857324867785517]
	TIME [epoch: 9.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6073419759737044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6073419759737044 | validation: 4.594245224658525]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8496019990612096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8496019990612096 | validation: 4.9257818222337875]
	TIME [epoch: 9.74 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638912448375468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.638912448375468 | validation: 5.676065009652202]
	TIME [epoch: 9.76 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8294691631590703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8294691631590703 | validation: 4.812983123309084]
	TIME [epoch: 9.73 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6331351474228164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6331351474228164 | validation: 4.681384602539899]
	TIME [epoch: 9.72 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7336748075587125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7336748075587125 | validation: 4.407774324572828]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5295509151413795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5295509151413795 | validation: 4.538688143512894]
	TIME [epoch: 9.76 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4676736118134626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4676736118134626 | validation: 4.579232326199277]
	TIME [epoch: 9.74 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5792866750225394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5792866750225394 | validation: 5.49502536998108]
	TIME [epoch: 9.74 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4244498790259166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4244498790259166 | validation: 4.740069337991027]
	TIME [epoch: 9.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6501660003250285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6501660003250285 | validation: 4.777117323214416]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629836753695745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.629836753695745 | validation: 4.98588484309122]
	TIME [epoch: 9.76 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823997839204696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.823997839204696 | validation: 4.092455578255451]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6417712085357126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6417712085357126 | validation: 4.810187558114724]
	TIME [epoch: 9.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6171680119645337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6171680119645337 | validation: 4.36807656021431]
	TIME [epoch: 9.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4806536131383394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4806536131383394 | validation: 4.464922757086758]
	TIME [epoch: 9.74 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6278010550578053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6278010550578053 | validation: 4.205488228961805]
	TIME [epoch: 9.73 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6543451368825295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6543451368825295 | validation: 4.649496909952087]
	TIME [epoch: 9.76 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4221376763895264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4221376763895264 | validation: 4.863266530048032]
	TIME [epoch: 9.74 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519285405752485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.519285405752485 | validation: 4.794301733467836]
	TIME [epoch: 9.74 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5373761299990925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5373761299990925 | validation: 4.452996030867261]
	TIME [epoch: 9.76 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.29877726301038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.29877726301038 | validation: 4.400173633777686]
	TIME [epoch: 9.74 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540205135550085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.540205135550085 | validation: 4.280159295595342]
	TIME [epoch: 9.73 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045349955466195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6045349955466195 | validation: 3.9758810131199107]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5072304802214815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5072304802214815 | validation: 3.9595919343433783]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501771314165867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.501771314165867 | validation: 4.017729404276739]
	TIME [epoch: 9.74 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.087356822992569		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.087356822992569 | validation: 5.162182389975972]
	TIME [epoch: 9.74 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9212509087204714		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.9212509087204714 | validation: 4.0432807709888205]
	TIME [epoch: 9.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6914813214634647		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.6914813214634647 | validation: 5.995440690366976]
	TIME [epoch: 9.75 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.064835286946366		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 4.064835286946366 | validation: 4.060277714051669]
	TIME [epoch: 9.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.410198079838577		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.410198079838577 | validation: 4.6973490233212765]
	TIME [epoch: 9.73 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3846871510991248		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.3846871510991248 | validation: 4.483835453659708]
	TIME [epoch: 9.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.332967505255227		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.332967505255227 | validation: 4.742819370868189]
	TIME [epoch: 9.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.359449255180794		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.359449255180794 | validation: 4.573516368948092]
	TIME [epoch: 9.74 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.231801666124791		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.231801666124791 | validation: 5.2409046196192834]
	TIME [epoch: 9.74 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5898080514232147		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.5898080514232147 | validation: 4.187510527388749]
	TIME [epoch: 9.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.330300246835791		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.330300246835791 | validation: 4.159466634026881]
	TIME [epoch: 9.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.244191691166015		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.244191691166015 | validation: 3.879779825287418]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4051685684819204		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.4051685684819204 | validation: 4.318808190088625]
	TIME [epoch: 9.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2215107176818067		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.2215107176818067 | validation: 4.420463894110336]
	TIME [epoch: 9.76 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2280641380575417		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.2280641380575417 | validation: 4.38508957747062]
	TIME [epoch: 9.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3157286577335383		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.3157286577335383 | validation: 3.930340546355036]
	TIME [epoch: 9.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.152257786388171		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.152257786388171 | validation: 4.308540890378286]
	TIME [epoch: 9.73 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2031536309977966		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.2031536309977966 | validation: 4.240076671758321]
	TIME [epoch: 9.75 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3427214935935545		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.3427214935935545 | validation: 3.854211207191938]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7177012609918236		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.7177012609918236 | validation: 4.937275724601683]
	TIME [epoch: 9.73 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5092739106942		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.5092739106942 | validation: 4.070776091738737]
	TIME [epoch: 9.75 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.309813056217357		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.309813056217357 | validation: 5.132133191028507]
	TIME [epoch: 9.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7103667399009765		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.7103667399009765 | validation: 3.839879965520897]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5584305806922814		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.5584305806922814 | validation: 4.104020965611255]
	TIME [epoch: 9.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.279458433651757		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.279458433651757 | validation: 4.566862677457414]
	TIME [epoch: 9.75 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1006771022785564		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.1006771022785564 | validation: 4.770508750308818]
	TIME [epoch: 9.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.23720342233866		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.23720342233866 | validation: 4.027960175728357]
	TIME [epoch: 9.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1313325664546534		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.1313325664546534 | validation: 4.1842906963324]
	TIME [epoch: 9.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1236355214818152		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.1236355214818152 | validation: 5.166057061047254]
	TIME [epoch: 9.74 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4420774694919194		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.4420774694919194 | validation: 4.179575771530165]
	TIME [epoch: 9.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1530543928923676		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.1530543928923676 | validation: 5.020639850325571]
	TIME [epoch: 9.73 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3921303450936575		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.3921303450936575 | validation: 4.170485999540484]
	TIME [epoch: 9.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.218484471056221		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.218484471056221 | validation: 4.450782526261471]
	TIME [epoch: 9.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1611479821622863		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.1611479821622863 | validation: 4.338169978036448]
	TIME [epoch: 9.72 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1776659343640548		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.1776659343640548 | validation: 4.254177382863579]
	TIME [epoch: 9.72 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.150120089270499		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.150120089270499 | validation: 5.414122544692829]
	TIME [epoch: 9.74 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5405480744774374		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.5405480744774374 | validation: 4.7303847034463935]
	TIME [epoch: 9.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.245229300993045		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.245229300993045 | validation: 4.335151737778062]
	TIME [epoch: 9.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.15426489299609		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.15426489299609 | validation: 4.318411157856233]
	TIME [epoch: 9.72 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.212863449295977		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.212863449295977 | validation: 3.956400964930746]
	TIME [epoch: 9.75 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102077898576986		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.102077898576986 | validation: 4.743970851840933]
	TIME [epoch: 9.72 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.136167863287732		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.136167863287732 | validation: 4.490604099567622]
	TIME [epoch: 9.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.109758283284062		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.109758283284062 | validation: 4.235477090468948]
	TIME [epoch: 9.74 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.089616056231162		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.089616056231162 | validation: 4.093732870683819]
	TIME [epoch: 9.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2164509911957153		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.2164509911957153 | validation: 4.691861140074342]
	TIME [epoch: 9.72 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1370983454680874		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.1370983454680874 | validation: 4.186929969471124]
	TIME [epoch: 9.71 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1870837297062677		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.1870837297062677 | validation: 3.93391989513231]
	TIME [epoch: 9.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1001814761694666		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.1001814761694666 | validation: 4.392777216767353]
	TIME [epoch: 9.72 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1024231296932707		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.1024231296932707 | validation: 4.009759202590821]
	TIME [epoch: 9.71 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1098032454044984		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.1098032454044984 | validation: 3.8658382722819646]
	TIME [epoch: 9.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.134761259472637		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.134761259472637 | validation: 4.056143498054119]
	TIME [epoch: 9.74 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.124155754607424		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.124155754607424 | validation: 5.336105629992892]
	TIME [epoch: 9.71 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4411541937135808		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.4411541937135808 | validation: 4.184727517954725]
	TIME [epoch: 9.72 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1834553159541037		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.1834553159541037 | validation: 4.810909276322481]
	TIME [epoch: 9.74 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.250791681175315		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.250791681175315 | validation: 4.02595972357402]
	TIME [epoch: 9.73 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.247478746839539		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.247478746839539 | validation: 4.173192086966031]
	TIME [epoch: 9.72 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0554012170404388		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.0554012170404388 | validation: 4.31104539227018]
	TIME [epoch: 9.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0824887596095105		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.0824887596095105 | validation: 3.905571967484599]
	TIME [epoch: 9.74 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995375957822005		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 2.995375957822005 | validation: 4.50546202027648]
	TIME [epoch: 9.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1576621249559107		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.1576621249559107 | validation: 3.8181712089574673]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.104831494252372		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.104831494252372 | validation: 4.256836134978354]
	TIME [epoch: 9.74 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0492879247981373		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.0492879247981373 | validation: 3.995133968626829]
	TIME [epoch: 9.75 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1597772666117696		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.1597772666117696 | validation: 4.082807596320058]
	TIME [epoch: 9.72 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0714126973563975		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.0714126973563975 | validation: 3.9662076818525005]
	TIME [epoch: 9.72 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0045099703458535		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 3.0045099703458535 | validation: 4.083979854225799]
	TIME [epoch: 9.71 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.903601278763287		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 2.903601278763287 | validation: 4.052642494421964]
	TIME [epoch: 9.73 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0354673247814037		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.0354673247814037 | validation: 3.854742934697053]
	TIME [epoch: 9.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.380366429712372		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.380366429712372 | validation: 4.311405973086226]
	TIME [epoch: 9.71 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1984874902360305		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 3.1984874902360305 | validation: 3.9670930469679404]
	TIME [epoch: 9.74 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961593676337789		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 2.961593676337789 | validation: 5.265241041860921]
	TIME [epoch: 9.72 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.378373562554477		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.378373562554477 | validation: 4.255778128867416]
	TIME [epoch: 9.73 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001483389068091		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.001483389068091 | validation: 5.498650231083361]
	TIME [epoch: 9.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4176269567830575		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 3.4176269567830575 | validation: 4.222412813900651]
	TIME [epoch: 9.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.913125924234445		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 2.913125924234445 | validation: 4.226361222290179]
	TIME [epoch: 9.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9694813257370485		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 2.9694813257370485 | validation: 3.871175723513717]
	TIME [epoch: 9.73 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020232652637635		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.020232652637635 | validation: 4.533419027102374]
	TIME [epoch: 9.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9969572771628696		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 2.9969572771628696 | validation: 5.225819115407279]
	TIME [epoch: 9.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.022481856349105		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.022481856349105 | validation: 3.9494641781803943]
	TIME [epoch: 9.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9927008128199		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 2.9927008128199 | validation: 4.207426845311729]
	TIME [epoch: 9.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.989617962469934		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 2.989617962469934 | validation: 4.298767323175907]
	TIME [epoch: 9.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9356226250861615		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 2.9356226250861615 | validation: 4.8984892181188044]
	TIME [epoch: 9.74 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541116725614182		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 3.541116725614182 | validation: 4.228562523895422]
	TIME [epoch: 9.74 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1374641686097275		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 3.1374641686097275 | validation: 4.212352388068411]
	TIME [epoch: 9.74 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8095959579458394		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 2.8095959579458394 | validation: 4.439765429165716]
	TIME [epoch: 9.75 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.060653310278564		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 3.060653310278564 | validation: 4.137431608877762]
	TIME [epoch: 9.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980815181839875		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 2.980815181839875 | validation: 3.8467538504068717]
	TIME [epoch: 9.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955414938558488		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 2.955414938558488 | validation: 3.764992479839283]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9918677056082297		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 2.9918677056082297 | validation: 3.9047153018176552]
	TIME [epoch: 9.76 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1783199710340955		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.1783199710340955 | validation: 3.9327865944767555]
	TIME [epoch: 9.74 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6220492353591736		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.6220492353591736 | validation: 5.631846954613409]
	TIME [epoch: 9.74 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.043419217989286		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 4.043419217989286 | validation: 4.471420965682224]
	TIME [epoch: 9.72 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6166402868637966		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.6166402868637966 | validation: 5.253111012288021]
	TIME [epoch: 9.74 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7495635376272176		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 3.7495635376272176 | validation: 3.9978820960221526]
	TIME [epoch: 9.73 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.338964369279153		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.338964369279153 | validation: 4.02461444679355]
	TIME [epoch: 9.72 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.88283226752233		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.88283226752233 | validation: 3.8093178206539675]
	TIME [epoch: 9.72 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570714723998512		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 2.9570714723998512 | validation: 4.275595122882415]
	TIME [epoch: 9.72 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.02252373518825		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.02252373518825 | validation: 3.8447467577283723]
	TIME [epoch: 9.71 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8580127030394884		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 2.8580127030394884 | validation: 3.8385172121249163]
	TIME [epoch: 9.71 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8693656786405795		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 2.8693656786405795 | validation: 4.080307843434169]
	TIME [epoch: 9.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950200001957101		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 2.950200001957101 | validation: 3.916646377453133]
	TIME [epoch: 9.72 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0732850142102537		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.0732850142102537 | validation: 5.361330945989621]
	TIME [epoch: 9.71 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532618630856019		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.532618630856019 | validation: 5.911170750643803]
	TIME [epoch: 9.71 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.029671186505018		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 4.029671186505018 | validation: 4.219824968681312]
	TIME [epoch: 9.74 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3188668435709743		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 3.3188668435709743 | validation: 5.629959883584196]
	TIME [epoch: 9.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749981788677216		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 3.7749981788677216 | validation: 3.95212555380162]
	TIME [epoch: 9.71 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4773075451356683		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.4773075451356683 | validation: 4.571252543729998]
	TIME [epoch: 9.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.370514812347153		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.370514812347153 | validation: 4.060741827446194]
	TIME [epoch: 9.72 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3738683406036634		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.3738683406036634 | validation: 5.063809797026302]
	TIME [epoch: 9.71 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5479743464972637		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.5479743464972637 | validation: 3.9421286242524762]
	TIME [epoch: 9.72 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0905502668867557		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.0905502668867557 | validation: 5.1927169517803]
	TIME [epoch: 9.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2260648556985188		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.2260648556985188 | validation: 3.8259307433452627]
	TIME [epoch: 9.72 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.866522450513047		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 2.866522450513047 | validation: 3.91642929080366]
	TIME [epoch: 9.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7353144992558622		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 2.7353144992558622 | validation: 4.743372429801814]
	TIME [epoch: 9.72 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949345085112851		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 2.949345085112851 | validation: 3.767252325618009]
	TIME [epoch: 9.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4181407779946995		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.4181407779946995 | validation: 5.0627329023378325]
	TIME [epoch: 9.72 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2474584371660127		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.2474584371660127 | validation: 3.830364229340612]
	TIME [epoch: 9.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.795901683902856		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 2.795901683902856 | validation: 4.064767905894195]
	TIME [epoch: 9.72 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9667208848120823		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 2.9667208848120823 | validation: 3.7334982017233838]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8718499118103056		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 2.8718499118103056 | validation: 4.789799298465458]
	TIME [epoch: 9.72 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2823657115844775		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.2823657115844775 | validation: 3.745148986027985]
	TIME [epoch: 9.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0507974064786714		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.0507974064786714 | validation: 3.828323348566928]
	TIME [epoch: 9.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1002275591124624		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.1002275591124624 | validation: 4.0989553200608455]
	TIME [epoch: 9.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8661928222001634		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 2.8661928222001634 | validation: 3.796585252993806]
	TIME [epoch: 9.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7351068522034336		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 2.7351068522034336 | validation: 4.176267311278237]
	TIME [epoch: 9.72 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9982367282080924		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 2.9982367282080924 | validation: 3.774199475027998]
	TIME [epoch: 9.75 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.26824895057578		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.26824895057578 | validation: 4.996021596802285]
	TIME [epoch: 9.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.104944799724574		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.104944799724574 | validation: 4.873471193317303]
	TIME [epoch: 9.73 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.840022309354777		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.840022309354777 | validation: 3.8124947607937667]
	TIME [epoch: 9.73 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4043271336714915		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.4043271336714915 | validation: 4.8164675403880475]
	TIME [epoch: 9.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3776329725657823		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.3776329725657823 | validation: 4.048050040274299]
	TIME [epoch: 9.73 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970770415657105		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 2.970770415657105 | validation: 4.854794600684431]
	TIME [epoch: 9.73 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9778406999232034		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.9778406999232034 | validation: 3.966278255924007]
	TIME [epoch: 9.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.789105455829225		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 2.789105455829225 | validation: 3.9086565111280507]
	TIME [epoch: 9.75 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.882666777662155		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 2.882666777662155 | validation: 4.664293289363443]
	TIME [epoch: 9.73 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9010042655729467		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 2.9010042655729467 | validation: 5.084508449109911]
	TIME [epoch: 9.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1874621528162894		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.1874621528162894 | validation: 3.6786808631851646]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011230982252511		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.011230982252511 | validation: 3.754448898974863]
	TIME [epoch: 9.75 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994010486505154		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 2.994010486505154 | validation: 3.9241207297946215]
	TIME [epoch: 9.75 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7623849914940517		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 2.7623849914940517 | validation: 4.525076465798413]
	TIME [epoch: 9.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.875268656503457		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 2.875268656503457 | validation: 3.9638534278759368]
	TIME [epoch: 9.77 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7776888694629895		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 2.7776888694629895 | validation: 3.8021119064003743]
	TIME [epoch: 9.75 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.222324496406929		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.222324496406929 | validation: 3.8832213578472103]
	TIME [epoch: 9.75 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.798253654675366		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 2.798253654675366 | validation: 3.844616999230392]
	TIME [epoch: 9.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7013397278967615		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 2.7013397278967615 | validation: 3.9652128388310848]
	TIME [epoch: 9.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.749042916743252		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 2.749042916743252 | validation: 3.932775409818095]
	TIME [epoch: 9.74 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6520332573112464		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 2.6520332573112464 | validation: 3.9599338994722797]
	TIME [epoch: 9.74 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.666855948528831		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 2.666855948528831 | validation: 4.063761747963239]
	TIME [epoch: 9.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.833200037666174		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 2.833200037666174 | validation: 3.751689019986309]
	TIME [epoch: 9.75 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7332257887290607		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 2.7332257887290607 | validation: 3.678057295519875]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.009979941172422		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.009979941172422 | validation: 4.111133022467375]
	TIME [epoch: 9.75 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.774208024069634		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 2.774208024069634 | validation: 3.6510941621008914]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8186167249762106		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 2.8186167249762106 | validation: 3.873633276645392]
	TIME [epoch: 9.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.682835691919102		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 2.682835691919102 | validation: 3.8756160695115676]
	TIME [epoch: 9.75 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6437322368654783		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 2.6437322368654783 | validation: 4.25872755779914]
	TIME [epoch: 9.75 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.790648642436035		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 2.790648642436035 | validation: 3.7242973604258225]
	TIME [epoch: 9.76 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.710052802023475		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 2.710052802023475 | validation: 3.6458742176960492]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6091544904678856		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 2.6091544904678856 | validation: 3.8852077434696652]
	TIME [epoch: 9.75 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6621420113506558		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 2.6621420113506558 | validation: 4.247499303709856]
	TIME [epoch: 9.77 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.002105228311631		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.002105228311631 | validation: 3.7788188277325108]
	TIME [epoch: 9.74 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.758956651541832		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 2.758956651541832 | validation: 3.8560776253620204]
	TIME [epoch: 9.74 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.676878468486273		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 2.676878468486273 | validation: 3.846253198534318]
	TIME [epoch: 9.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6384774965959417		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 2.6384774965959417 | validation: 3.869133743024031]
	TIME [epoch: 9.76 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7046207132991835		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 2.7046207132991835 | validation: 4.021524632200399]
	TIME [epoch: 9.74 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7087448354770265		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 2.7087448354770265 | validation: 3.6600555640427515]
	TIME [epoch: 9.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6674085376203487		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 2.6674085376203487 | validation: 4.000658866689679]
	TIME [epoch: 9.75 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.689333426349237		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 2.689333426349237 | validation: 4.3650657043077254]
	TIME [epoch: 9.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.730650296067699		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 2.730650296067699 | validation: 3.760874407756729]
	TIME [epoch: 9.74 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.898005929182572		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 2.898005929182572 | validation: 4.964240229886557]
	TIME [epoch: 9.75 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3742299743545785		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.3742299743545785 | validation: 3.725568163472991]
	TIME [epoch: 9.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0815464385660323		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.0815464385660323 | validation: 4.511473950247752]
	TIME [epoch: 9.75 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.829592733692126		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 2.829592733692126 | validation: 3.8842070667075412]
	TIME [epoch: 9.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.623012733360977		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 2.623012733360977 | validation: 3.8332410611964423]
	TIME [epoch: 9.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.691332190351222		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 2.691332190351222 | validation: 3.947228828386725]
	TIME [epoch: 9.76 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.805344106779015		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 2.805344106779015 | validation: 3.8569466671188835]
	TIME [epoch: 9.74 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.703924114934672		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 2.703924114934672 | validation: 3.616065070544771]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7392002274865073		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 2.7392002274865073 | validation: 3.6756513262885258]
	TIME [epoch: 9.74 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.578506196391088		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 2.578506196391088 | validation: 4.088689780959657]
	TIME [epoch: 9.76 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7178295821211336		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 2.7178295821211336 | validation: 3.661809425533494]
	TIME [epoch: 9.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5558685213868317		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 2.5558685213868317 | validation: 3.761647827495413]
	TIME [epoch: 9.73 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6033294424409728		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 2.6033294424409728 | validation: 3.849911542479464]
	TIME [epoch: 9.75 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7728682301357908		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 2.7728682301357908 | validation: 3.9334151469101006]
	TIME [epoch: 9.73 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6754928796858986		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 2.6754928796858986 | validation: 3.7727270253747793]
	TIME [epoch: 9.73 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6209975506379695		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 2.6209975506379695 | validation: 3.759983778410093]
	TIME [epoch: 9.73 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5364132666177843		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 2.5364132666177843 | validation: 4.317762904546397]
	TIME [epoch: 9.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7519125720497244		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 2.7519125720497244 | validation: 3.6266983042707825]
	TIME [epoch: 9.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5891873386341424		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 2.5891873386341424 | validation: 3.6975734167246106]
	TIME [epoch: 9.73 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5834455074480758		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 2.5834455074480758 | validation: 3.783021010937135]
	TIME [epoch: 9.73 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.62652504140198		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 2.62652504140198 | validation: 3.641456180189132]
	TIME [epoch: 9.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5789423114185968		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 2.5789423114185968 | validation: 3.7673949886379274]
	TIME [epoch: 9.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.571518249115928		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 2.571518249115928 | validation: 3.6608660730499345]
	TIME [epoch: 9.73 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.589420152955753		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 2.589420152955753 | validation: 3.768592364337163]
	TIME [epoch: 9.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.699372562568019		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 2.699372562568019 | validation: 4.156688341935171]
	TIME [epoch: 9.75 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.046751478074106		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.046751478074106 | validation: 4.61474717344269]
	TIME [epoch: 9.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0734217722055037		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.0734217722055037 | validation: 3.8525275648678985]
	TIME [epoch: 9.73 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8632772868320755		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 2.8632772868320755 | validation: 4.15425424925188]
	TIME [epoch: 9.75 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6703793291017996		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 2.6703793291017996 | validation: 3.654613753560267]
	TIME [epoch: 9.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8418095666253165		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 2.8418095666253165 | validation: 4.168111536644979]
	TIME [epoch: 9.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.768499477730581		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 2.768499477730581 | validation: 4.162368445211177]
	TIME [epoch: 9.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6007432497474		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 2.6007432497474 | validation: 3.801158485865127]
	TIME [epoch: 9.76 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.623298984246061		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 2.623298984246061 | validation: 4.51080523293467]
	TIME [epoch: 9.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.843017089662497		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 2.843017089662497 | validation: 3.8829676707936707]
	TIME [epoch: 9.73 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6075828714769576		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 2.6075828714769576 | validation: 3.674915944432699]
	TIME [epoch: 9.73 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542578300664985		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 2.542578300664985 | validation: 3.8486668049387687]
	TIME [epoch: 9.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5653686248246013		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 2.5653686248246013 | validation: 3.746609021318246]
	TIME [epoch: 9.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6024234438349834		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 2.6024234438349834 | validation: 3.637480675925881]
	TIME [epoch: 9.73 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9012236576370425		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 2.9012236576370425 | validation: 3.967062510588405]
	TIME [epoch: 9.74 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6613616713545216		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 2.6613616713545216 | validation: 3.6399042599613525]
	TIME [epoch: 9.74 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.735068279610192		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 2.735068279610192 | validation: 3.878043942665862]
	TIME [epoch: 9.72 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8359267406888864		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 2.8359267406888864 | validation: 4.667507019631065]
	TIME [epoch: 9.72 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944301433926167		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 2.944301433926167 | validation: 3.675794364361714]
	TIME [epoch: 9.74 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5561779523308843		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 2.5561779523308843 | validation: 3.6552458583370875]
	TIME [epoch: 9.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.500693635149768		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 2.500693635149768 | validation: 3.767197836107495]
	TIME [epoch: 9.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.734004980678447		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 2.734004980678447 | validation: 3.570207754350916]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8032154023880054		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 2.8032154023880054 | validation: 3.7024503677882805]
	TIME [epoch: 9.75 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4986241017094892		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 2.4986241017094892 | validation: 3.8869015187247267]
	TIME [epoch: 9.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6747290835336393		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 2.6747290835336393 | validation: 3.585990971592199]
	TIME [epoch: 9.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.526231288250445		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 2.526231288250445 | validation: 3.7721133813042167]
	TIME [epoch: 9.74 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6782984830144487		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 2.6782984830144487 | validation: 3.6109152891728025]
	TIME [epoch: 9.75 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6505652014180616		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 2.6505652014180616 | validation: 3.646329757075914]
	TIME [epoch: 9.74 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.636373017130036		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 2.636373017130036 | validation: 3.669249433903264]
	TIME [epoch: 9.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5774792709252354		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 2.5774792709252354 | validation: 3.8428671674669364]
	TIME [epoch: 9.75 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6193921726362164		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 2.6193921726362164 | validation: 4.785788009993395]
	TIME [epoch: 9.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.080712028865551		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.080712028865551 | validation: 3.65015422595675]
	TIME [epoch: 9.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7116006804103407		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 2.7116006804103407 | validation: 4.05556453761506]
	TIME [epoch: 9.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5996314889743695		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 2.5996314889743695 | validation: 3.7258681493995836]
	TIME [epoch: 9.75 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5192327678971003		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 2.5192327678971003 | validation: 3.6198336514524136]
	TIME [epoch: 9.73 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.561728329267557		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 2.561728329267557 | validation: 4.026109246939531]
	TIME [epoch: 9.73 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6801653425037197		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 2.6801653425037197 | validation: 3.6898788249178067]
	TIME [epoch: 9.74 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5708244278352006		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 2.5708244278352006 | validation: 3.6266105822223653]
	TIME [epoch: 9.75 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5016627962725706		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 2.5016627962725706 | validation: 3.603005226022625]
	TIME [epoch: 9.73 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.569975203996826		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 2.569975203996826 | validation: 3.6238337405787706]
	TIME [epoch: 9.73 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.553115394878772		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 2.553115394878772 | validation: 3.626750650039534]
	TIME [epoch: 9.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.52987708111623		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 2.52987708111623 | validation: 3.5765565043331287]
	TIME [epoch: 9.73 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5321335344645517		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 2.5321335344645517 | validation: 3.7548368794706595]
	TIME [epoch: 9.73 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5173853367001664		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 2.5173853367001664 | validation: 3.9818802347671]
	TIME [epoch: 9.73 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6084547499188298		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 2.6084547499188298 | validation: 3.6222154080654287]
	TIME [epoch: 9.76 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6102358229806386		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 2.6102358229806386 | validation: 3.743536763997511]
	TIME [epoch: 9.73 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5794896116266925		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 2.5794896116266925 | validation: 3.6235174501937806]
	TIME [epoch: 9.73 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.678514755735713		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 2.678514755735713 | validation: 3.765547796464289]
	TIME [epoch: 9.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.624358493624876		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 2.624358493624876 | validation: 3.7221212152295924]
	TIME [epoch: 9.75 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5367173876391806		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 2.5367173876391806 | validation: 3.559373575661682]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6962461073631094		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 2.6962461073631094 | validation: 3.7346343524711445]
	TIME [epoch: 9.73 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.598473091325058		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 2.598473091325058 | validation: 3.7271247792110977]
	TIME [epoch: 9.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5138472574041897		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 2.5138472574041897 | validation: 3.665146659843708]
	TIME [epoch: 9.75 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4601902266752003		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 2.4601902266752003 | validation: 4.011666993871975]
	TIME [epoch: 9.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.812279421982052		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 2.812279421982052 | validation: 3.7600575536675307]
	TIME [epoch: 9.73 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4916886215515683		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 2.4916886215515683 | validation: 3.5733289607917493]
	TIME [epoch: 9.76 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.472163043132804		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 2.472163043132804 | validation: 3.8524292649858785]
	TIME [epoch: 9.73 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5513433595810775		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 2.5513433595810775 | validation: 3.6161091724523784]
	TIME [epoch: 9.73 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4543427610642996		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 2.4543427610642996 | validation: 4.248985263705728]
	TIME [epoch: 9.73 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8387071067369196		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 2.8387071067369196 | validation: 3.7507064989967276]
	TIME [epoch: 9.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6404038987880307		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 2.6404038987880307 | validation: 3.9877354591562324]
	TIME [epoch: 9.72 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.669239253161017		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 2.669239253161017 | validation: 3.5607585214248734]
	TIME [epoch: 9.74 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.485430390751211		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 2.485430390751211 | validation: 3.675522190840826]
	TIME [epoch: 9.73 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6965505524968743		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 2.6965505524968743 | validation: 3.6298741541438004]
	TIME [epoch: 9.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.485454921482644		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 2.485454921482644 | validation: 3.553865104808423]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.651775206271231		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 2.651775206271231 | validation: 4.07582030193125]
	TIME [epoch: 9.73 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.69104817643651		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 2.69104817643651 | validation: 3.561476196587051]
	TIME [epoch: 9.75 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.459141955780088		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 2.459141955780088 | validation: 4.075805499190409]
	TIME [epoch: 9.73 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.514476785006848		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 2.514476785006848 | validation: 3.829377325252254]
	TIME [epoch: 9.72 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5827768510270888		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 2.5827768510270888 | validation: 3.648769467655669]
	TIME [epoch: 9.73 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.582028983886623		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 2.582028983886623 | validation: 4.1407479201034025]
	TIME [epoch: 9.76 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5512914984616635		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 2.5512914984616635 | validation: 3.647241907481739]
	TIME [epoch: 9.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4471947618747425		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 2.4471947618747425 | validation: 3.560535097594823]
	TIME [epoch: 9.74 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.566993294893974		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 2.566993294893974 | validation: 3.612980257728425]
	TIME [epoch: 9.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.573365344298877		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 2.573365344298877 | validation: 3.5696471804242855]
	TIME [epoch: 9.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7482553849209337		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 2.7482553849209337 | validation: 3.938976134845108]
	TIME [epoch: 9.72 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6229483495428516		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 2.6229483495428516 | validation: 3.73233728804711]
	TIME [epoch: 9.73 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.446418352915288		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 2.446418352915288 | validation: 3.631289598848961]
	TIME [epoch: 9.75 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5056675999163267		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 2.5056675999163267 | validation: 3.8637583028230735]
	TIME [epoch: 9.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5371674191716087		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 2.5371674191716087 | validation: 3.691143470688304]
	TIME [epoch: 9.72 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.575372283032926		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 2.575372283032926 | validation: 3.5808848280843484]
	TIME [epoch: 9.73 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.424499212215232		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 2.424499212215232 | validation: 3.6891766847637633]
	TIME [epoch: 9.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.580215659484212		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 2.580215659484212 | validation: 3.5891276932603025]
	TIME [epoch: 9.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.453097252556196		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 2.453097252556196 | validation: 3.824122886165963]
	TIME [epoch: 9.73 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5231258847354616		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 2.5231258847354616 | validation: 3.705928491286678]
	TIME [epoch: 9.72 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4684225684313543		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 2.4684225684313543 | validation: 3.8873864660959927]
	TIME [epoch: 9.74 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5455140169673767		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 2.5455140169673767 | validation: 3.550492842626669]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.407404493594684		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 2.407404493594684 | validation: 4.000599718698182]
	TIME [epoch: 9.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7192921464715902		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 2.7192921464715902 | validation: 3.6456431058103864]
	TIME [epoch: 9.76 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4923279330967674		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 2.4923279330967674 | validation: 3.552148083198074]
	TIME [epoch: 9.75 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.450751940580902		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 2.450751940580902 | validation: 3.734518547383195]
	TIME [epoch: 9.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6814488286922313		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 2.6814488286922313 | validation: 3.7661259289009297]
	TIME [epoch: 9.73 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.578439379060983		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 2.578439379060983 | validation: 3.535354691761497]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4876456563306775		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 2.4876456563306775 | validation: 3.69823254087319]
	TIME [epoch: 9.74 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.442035832219987		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 2.442035832219987 | validation: 3.7590938635708104]
	TIME [epoch: 9.72 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.589392157001496		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 2.589392157001496 | validation: 3.515157191147362]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4681946074975003		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 2.4681946074975003 | validation: 3.5627946914527513]
	TIME [epoch: 9.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.573602511456927		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 2.573602511456927 | validation: 3.531835248059497]
	TIME [epoch: 9.73 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4529583689156076		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 2.4529583689156076 | validation: 4.445737984065316]
	TIME [epoch: 9.73 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6617166722393764		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 2.6617166722393764 | validation: 3.720062791408433]
	TIME [epoch: 9.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4597801434425834		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 2.4597801434425834 | validation: 3.5596678588964585]
	TIME [epoch: 9.75 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4418065282487316		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 2.4418065282487316 | validation: 3.578419556842689]
	TIME [epoch: 9.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.458095863837209		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 2.458095863837209 | validation: 3.8048182693571277]
	TIME [epoch: 9.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6135191626041054		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 2.6135191626041054 | validation: 3.7790807274299154]
	TIME [epoch: 9.73 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5448379957740705		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 2.5448379957740705 | validation: 3.765096473219523]
	TIME [epoch: 9.72 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4741355864051457		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 2.4741355864051457 | validation: 3.866325538241267]
	TIME [epoch: 9.72 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5749466558171363		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 2.5749466558171363 | validation: 3.6705880445724928]
	TIME [epoch: 9.72 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.53924971382418		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 2.53924971382418 | validation: 3.617233051098149]
	TIME [epoch: 9.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.761581288147842		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 2.761581288147842 | validation: 3.551396260453297]
	TIME [epoch: 9.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.477583626863416		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 2.477583626863416 | validation: 3.5189160774458323]
	TIME [epoch: 9.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5147207642220737		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 2.5147207642220737 | validation: 3.6634087792861716]
	TIME [epoch: 9.73 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.49381405179686		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 2.49381405179686 | validation: 3.530924884175042]
	TIME [epoch: 9.74 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4208916151413655		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 2.4208916151413655 | validation: 3.859389916503409]
	TIME [epoch: 9.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5871614121846465		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 2.5871614121846465 | validation: 3.5804414675170584]
	TIME [epoch: 9.72 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.550261296263033		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 2.550261296263033 | validation: 3.565160342605353]
	TIME [epoch: 9.74 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.550626461638722		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 2.550626461638722 | validation: 3.53292555028632]
	TIME [epoch: 9.74 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.904398420541188		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 2.904398420541188 | validation: 3.5558850864406786]
	TIME [epoch: 9.73 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4299501644132304		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 2.4299501644132304 | validation: 3.6387461158465744]
	TIME [epoch: 9.72 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.467521250426399		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 2.467521250426399 | validation: 3.554735396934217]
	TIME [epoch: 9.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.49978258010333		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 2.49978258010333 | validation: 4.03779213219475]
	TIME [epoch: 9.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.519520335489357		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 2.519520335489357 | validation: 3.5613329174211934]
	TIME [epoch: 9.72 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4105038174295874		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 2.4105038174295874 | validation: 3.593451393423546]
	TIME [epoch: 9.71 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4545940512477262		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 2.4545940512477262 | validation: 3.5531571896260448]
	TIME [epoch: 9.74 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.425671958056336		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 2.425671958056336 | validation: 3.687030279488819]
	TIME [epoch: 9.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.51711217857446		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 2.51711217857446 | validation: 3.564239351960292]
	TIME [epoch: 9.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4439855134156465		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 2.4439855134156465 | validation: 3.6030337546311775]
	TIME [epoch: 9.73 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4816067190015367		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 2.4816067190015367 | validation: 3.5370590459923545]
	TIME [epoch: 9.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.403799977701393		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 2.403799977701393 | validation: 3.5629047073022697]
	TIME [epoch: 9.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9059626839802055		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 2.9059626839802055 | validation: 4.461206744304961]
	TIME [epoch: 9.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.57002991861843		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 2.57002991861843 | validation: 3.5675454076876334]
	TIME [epoch: 9.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.494585496196946		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 2.494585496196946 | validation: 3.576234095544232]
	TIME [epoch: 9.72 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.461557665050833		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 2.461557665050833 | validation: 3.529646748763582]
	TIME [epoch: 9.71 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.492105248743956		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 2.492105248743956 | validation: 3.538832670469279]
	TIME [epoch: 9.71 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.45274578739969		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 2.45274578739969 | validation: 3.667607288654399]
	TIME [epoch: 9.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4064062640033015		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 2.4064062640033015 | validation: 3.696537719744822]
	TIME [epoch: 9.72 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4975711450176954		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 2.4975711450176954 | validation: 3.5445366326337586]
	TIME [epoch: 9.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.448266523403387		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 2.448266523403387 | validation: 3.532815760412729]
	TIME [epoch: 9.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4567548892869144		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 2.4567548892869144 | validation: 3.5343390876586716]
	TIME [epoch: 9.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4440271242563645		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 2.4440271242563645 | validation: 3.6538230711748434]
	TIME [epoch: 9.73 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.458885949098696		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 2.458885949098696 | validation: 3.592455543397877]
	TIME [epoch: 9.73 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3913769416423465		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 2.3913769416423465 | validation: 3.61717944632201]
	TIME [epoch: 9.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6316573627949307		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 2.6316573627949307 | validation: 3.5341612669022546]
	TIME [epoch: 9.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.419496736606137		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 2.419496736606137 | validation: 3.570606068867645]
	TIME [epoch: 9.73 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4270845315783034		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 2.4270845315783034 | validation: 3.532891405369196]
	TIME [epoch: 9.73 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.502870697617408		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 2.502870697617408 | validation: 3.479901801918419]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.441960621565142		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 2.441960621565142 | validation: 3.518630172566284]
	TIME [epoch: 9.72 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5115197068487314		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 2.5115197068487314 | validation: 3.5264380197196807]
	TIME [epoch: 9.73 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4328946064205352		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 2.4328946064205352 | validation: 3.5364517251729417]
	TIME [epoch: 9.71 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.369335894567464		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 2.369335894567464 | validation: 3.6323217549958575]
	TIME [epoch: 9.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.541272768941272		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 2.541272768941272 | validation: 3.7916296498357718]
	TIME [epoch: 9.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.513752418834019		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 2.513752418834019 | validation: 3.648983769204266]
	TIME [epoch: 9.71 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.419059906360835		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 2.419059906360835 | validation: 3.6194503820368222]
	TIME [epoch: 9.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.514976249298011		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 2.514976249298011 | validation: 3.671429176393721]
	TIME [epoch: 9.72 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542372918277644		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 2.542372918277644 | validation: 3.6509472822627322]
	TIME [epoch: 9.72 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.397734632455835		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 2.397734632455835 | validation: 3.593958339835084]
	TIME [epoch: 9.72 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.473990378381361		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 2.473990378381361 | validation: 3.484272846691835]
	TIME [epoch: 9.75 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5216150435544367		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 2.5216150435544367 | validation: 3.692755994249252]
	TIME [epoch: 9.72 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5277769032496367		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 2.5277769032496367 | validation: 3.497728502281014]
	TIME [epoch: 9.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3847505951028545		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 2.3847505951028545 | validation: 3.6183514841950375]
	TIME [epoch: 9.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.391101989675465		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 2.391101989675465 | validation: 3.8948398520810135]
	TIME [epoch: 9.75 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4998475202400416		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 2.4998475202400416 | validation: 3.540680929335678]
	TIME [epoch: 9.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.398758400028203		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 2.398758400028203 | validation: 3.499986717385766]
	TIME [epoch: 9.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3671835695282857		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 2.3671835695282857 | validation: 3.596083457731057]
	TIME [epoch: 9.73 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.428531610625767		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 2.428531610625767 | validation: 3.807578156097416]
	TIME [epoch: 9.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4444484607795762		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 2.4444484607795762 | validation: 3.5741414568995924]
	TIME [epoch: 9.73 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.475504776859171		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 2.475504776859171 | validation: 3.6539988908459033]
	TIME [epoch: 9.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.588794728211064		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 2.588794728211064 | validation: 3.509456220943585]
	TIME [epoch: 9.72 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3950789459918873		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 2.3950789459918873 | validation: 3.5194476991787056]
	TIME [epoch: 9.71 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.429418819581477		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 2.429418819581477 | validation: 3.5120391864900484]
	TIME [epoch: 9.69 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.384564540546791		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 2.384564540546791 | validation: 3.5446306580428337]
	TIME [epoch: 9.71 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4150673305789985		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 2.4150673305789985 | validation: 3.503576609327805]
	TIME [epoch: 9.73 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4325946568062773		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 2.4325946568062773 | validation: 3.4851082073037403]
	TIME [epoch: 9.72 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7185831444431914		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 2.7185831444431914 | validation: 3.7857497942188054]
	TIME [epoch: 9.72 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6115914456715403		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 2.6115914456715403 | validation: 3.5524690407993247]
	TIME [epoch: 9.72 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.421458843706975		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 2.421458843706975 | validation: 3.5629679386416604]
	TIME [epoch: 9.73 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4139298592704037		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 2.4139298592704037 | validation: 3.6759849797649453]
	TIME [epoch: 9.72 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.439744646327326		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 2.439744646327326 | validation: 3.5466579758852244]
	TIME [epoch: 9.71 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.562767389457149		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 2.562767389457149 | validation: 3.6117345879250826]
	TIME [epoch: 9.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.508816321317357		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 2.508816321317357 | validation: 3.555362288763362]
	TIME [epoch: 9.73 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3851031597426378		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 2.3851031597426378 | validation: 3.5295141284997964]
	TIME [epoch: 9.72 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4024089234671613		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 2.4024089234671613 | validation: 3.5443900063652234]
	TIME [epoch: 9.71 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4225118513490584		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 2.4225118513490584 | validation: 3.5661984929884873]
	TIME [epoch: 9.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.460857453419494		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 2.460857453419494 | validation: 3.5363023165297798]
	TIME [epoch: 9.71 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.456781317282004		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 2.456781317282004 | validation: 3.559928863983505]
	TIME [epoch: 9.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3844312954089846		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 2.3844312954089846 | validation: 3.537951302288621]
	TIME [epoch: 9.71 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3635659524384014		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 2.3635659524384014 | validation: 3.754911896816224]
	TIME [epoch: 9.74 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4676409553434384		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 2.4676409553434384 | validation: 3.5415098763517996]
	TIME [epoch: 9.72 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3928939310549913		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 2.3928939310549913 | validation: 3.6216346724474624]
	TIME [epoch: 9.71 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.39260882775229		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 2.39260882775229 | validation: 3.603818116063106]
	TIME [epoch: 9.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3963792597563294		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 2.3963792597563294 | validation: 3.6433644030866703]
	TIME [epoch: 9.72 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.438709004809982		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 2.438709004809982 | validation: 3.716044937881983]
	TIME [epoch: 9.72 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.409192893040542		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 2.409192893040542 | validation: 3.516473534921646]
	TIME [epoch: 9.72 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.389228258195173		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 2.389228258195173 | validation: 3.4998949740113527]
	TIME [epoch: 9.74 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.393282711640527		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 2.393282711640527 | validation: 3.5933113440501256]
	TIME [epoch: 9.72 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3874983548249307		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 2.3874983548249307 | validation: 3.6037836264869583]
	TIME [epoch: 9.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4528079301903682		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 2.4528079301903682 | validation: 3.579359542290051]
	TIME [epoch: 9.71 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.458930775801476		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 2.458930775801476 | validation: 3.6495043758580845]
	TIME [epoch: 9.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4830447675027862		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 2.4830447675027862 | validation: 3.6638258899981593]
	TIME [epoch: 9.79 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4082991168163295		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 2.4082991168163295 | validation: 3.630759916812561]
	TIME [epoch: 9.71 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.402758282774095		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 2.402758282774095 | validation: 3.559826698730049]
	TIME [epoch: 9.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4382000344200243		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 2.4382000344200243 | validation: 3.52824738477608]
	TIME [epoch: 9.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.405760768977074		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 2.405760768977074 | validation: 3.539059186407511]
	TIME [epoch: 9.7 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4454609636453633		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 2.4454609636453633 | validation: 3.512421658563697]
	TIME [epoch: 9.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4354329506666317		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 2.4354329506666317 | validation: 3.503908133596504]
	TIME [epoch: 9.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3975495149386257		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 2.3975495149386257 | validation: 3.5340827257929877]
	TIME [epoch: 9.74 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3801614553619403		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 2.3801614553619403 | validation: 3.619014308788318]
	TIME [epoch: 9.71 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.405145424613342		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 2.405145424613342 | validation: 3.5399039887221218]
	TIME [epoch: 9.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.506617418409732		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 2.506617418409732 | validation: 3.5165378180313445]
	TIME [epoch: 9.72 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3817123572191976		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 2.3817123572191976 | validation: 3.5501392201445587]
	TIME [epoch: 9.72 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.392153015208964		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 2.392153015208964 | validation: 3.5091864078735533]
	TIME [epoch: 9.72 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.355272485892418		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 2.355272485892418 | validation: 3.595957878108164]
	TIME [epoch: 9.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.461960565717807		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 2.461960565717807 | validation: 3.593969934339117]
	TIME [epoch: 9.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3848737310525623		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 2.3848737310525623 | validation: 3.511072144373397]
	TIME [epoch: 9.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.387098881700257		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 2.387098881700257 | validation: 3.5275871080888637]
	TIME [epoch: 9.72 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4636520851537473		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 2.4636520851537473 | validation: 3.7114846169790487]
	TIME [epoch: 9.73 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.420410282945948		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 2.420410282945948 | validation: 3.7063887467694645]
	TIME [epoch: 9.73 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.540629894251951		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 2.540629894251951 | validation: 3.839877791990215]
	TIME [epoch: 9.72 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.592717198495334		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 2.592717198495334 | validation: 3.522151606016214]
	TIME [epoch: 9.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4097616393350805		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 2.4097616393350805 | validation: 3.564190989078028]
	TIME [epoch: 9.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3929763644273274		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 2.3929763644273274 | validation: 3.535896015024432]
	TIME [epoch: 9.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.416874859644607		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 2.416874859644607 | validation: 3.6984124541578693]
	TIME [epoch: 9.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4075593034955864		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 2.4075593034955864 | validation: 3.5722387689677184]
	TIME [epoch: 9.71 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4962797400376564		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 2.4962797400376564 | validation: 3.620306839281495]
	TIME [epoch: 9.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.429727309614622		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 2.429727309614622 | validation: 3.5168576813264965]
	TIME [epoch: 9.71 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.395868469548022		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 2.395868469548022 | validation: 3.531069828264043]
	TIME [epoch: 9.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.371786949064777		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 2.371786949064777 | validation: 3.738247299832866]
	TIME [epoch: 9.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3974091017403953		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 2.3974091017403953 | validation: 3.535385843612393]
	TIME [epoch: 9.73 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.404894799049946		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 2.404894799049946 | validation: 3.561545924823209]
	TIME [epoch: 9.71 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4357745297184836		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 2.4357745297184836 | validation: 3.6082015701511283]
	TIME [epoch: 9.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3947151453645024		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 2.3947151453645024 | validation: 3.6371425722519217]
	TIME [epoch: 9.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.462261888373424		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 2.462261888373424 | validation: 3.5290244259295687]
	TIME [epoch: 9.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4452481900051635		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 2.4452481900051635 | validation: 3.54280986434527]
	TIME [epoch: 9.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4744212587476055		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 2.4744212587476055 | validation: 3.621117404817884]
	TIME [epoch: 9.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5274464766015745		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 2.5274464766015745 | validation: 3.5755394527330715]
	TIME [epoch: 9.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.375409758651929		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 2.375409758651929 | validation: 3.6159665001401864]
	TIME [epoch: 9.72 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.422954225508562		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 2.422954225508562 | validation: 3.535759773508404]
	TIME [epoch: 9.72 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3975912716709966		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 2.3975912716709966 | validation: 3.556548427490501]
	TIME [epoch: 9.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.379852670765229		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 2.379852670765229 | validation: 3.578351068506223]
	TIME [epoch: 9.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4653322968744766		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 2.4653322968744766 | validation: 3.618805203102355]
	TIME [epoch: 9.71 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.408381054026676		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 2.408381054026676 | validation: 3.5583465907537652]
	TIME [epoch: 9.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4347035821077343		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 2.4347035821077343 | validation: 3.524621307272574]
	TIME [epoch: 9.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3801576895683487		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 2.3801576895683487 | validation: 3.5019789369343552]
	TIME [epoch: 9.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4148480705471473		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 2.4148480705471473 | validation: 3.585833576905927]
	TIME [epoch: 9.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.420477649522041		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 2.420477649522041 | validation: 3.552137052069734]
	TIME [epoch: 9.72 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3861026133001197		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 2.3861026133001197 | validation: 3.5371187520753096]
	TIME [epoch: 9.74 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3596727656704957		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 2.3596727656704957 | validation: 4.061725597694393]
	TIME [epoch: 9.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5772316438843372		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 2.5772316438843372 | validation: 3.6128113327953715]
	TIME [epoch: 9.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3777819457645775		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 2.3777819457645775 | validation: 3.5957132133229552]
	TIME [epoch: 9.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.451947862372875		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 2.451947862372875 | validation: 3.5301097687495115]
	TIME [epoch: 9.74 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4085375731545344		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 2.4085375731545344 | validation: 3.580102373728555]
	TIME [epoch: 9.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.378410032699458		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 2.378410032699458 | validation: 3.5484284903916024]
	TIME [epoch: 9.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.373693662749265		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 2.373693662749265 | validation: 3.540562622219563]
	TIME [epoch: 9.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.423843397162324		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 2.423843397162324 | validation: 3.4801557405164987]
	TIME [epoch: 9.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3914414786789613		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 2.3914414786789613 | validation: 3.5264853481906178]
	TIME [epoch: 9.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.37622177775721		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 2.37622177775721 | validation: 3.570461341204595]
	TIME [epoch: 9.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.427007852704933		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 2.427007852704933 | validation: 3.528960948706897]
	TIME [epoch: 9.73 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.53150314978173		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 2.53150314978173 | validation: 3.590153976213249]
	TIME [epoch: 9.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3394987862677463		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 2.3394987862677463 | validation: 3.6808016111141986]
	TIME [epoch: 9.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4363300737140685		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 2.4363300737140685 | validation: 3.518131132140331]
	TIME [epoch: 9.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3770960988284497		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 2.3770960988284497 | validation: 3.5592644865982757]
	TIME [epoch: 9.74 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.392187933749624		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 2.392187933749624 | validation: 3.64153268406204]
	TIME [epoch: 9.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3919208153416847		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 2.3919208153416847 | validation: 3.525380895154997]
	TIME [epoch: 9.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.518536900207361		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 2.518536900207361 | validation: 3.659110833973954]
	TIME [epoch: 9.71 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3714533599723486		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 2.3714533599723486 | validation: 3.5072811972532225]
	TIME [epoch: 9.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3936445844465615		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 2.3936445844465615 | validation: 3.483086902432366]
	TIME [epoch: 9.72 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.41941855135201		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 2.41941855135201 | validation: 3.742831122039197]
	TIME [epoch: 9.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.429612326758781		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 2.429612326758781 | validation: 3.496430084344276]
	TIME [epoch: 9.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4556259914397796		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 2.4556259914397796 | validation: 3.493807856100095]
	TIME [epoch: 9.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3565028971869197		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 2.3565028971869197 | validation: 3.8960578801385997]
	TIME [epoch: 9.71 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.553724354214805		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 2.553724354214805 | validation: 3.627815268897251]
	TIME [epoch: 9.71 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.429511079357521		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 2.429511079357521 | validation: 3.526650280583256]
	TIME [epoch: 9.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3854963660115964		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 2.3854963660115964 | validation: 3.55611238213889]
	TIME [epoch: 9.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4436203090515463		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 2.4436203090515463 | validation: 3.527943569104922]
	TIME [epoch: 9.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3811823509061645		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 2.3811823509061645 | validation: 3.5763270506430986]
	TIME [epoch: 9.73 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3637610537976754		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 2.3637610537976754 | validation: 3.4986729250017676]
	TIME [epoch: 9.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3475633377988094		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 2.3475633377988094 | validation: 3.5583795738057193]
	TIME [epoch: 9.71 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4150746855701		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 2.4150746855701 | validation: 3.543634685624442]
	TIME [epoch: 9.71 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.358142776158137		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 2.358142776158137 | validation: 3.488429442027599]
	TIME [epoch: 9.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.350340207534424		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 2.350340207534424 | validation: 3.625603123516263]
	TIME [epoch: 9.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3939537497892855		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 2.3939537497892855 | validation: 3.6879820707774247]
	TIME [epoch: 9.71 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4525242274287087		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 2.4525242274287087 | validation: 3.5195744393238995]
	TIME [epoch: 9.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3743703219930192		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 2.3743703219930192 | validation: 3.534524717521955]
	TIME [epoch: 9.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4223373451102472		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 2.4223373451102472 | validation: 3.607341507336524]
	TIME [epoch: 9.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.372236944954165		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 2.372236944954165 | validation: 3.63428151003289]
	TIME [epoch: 9.72 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.353229080974485		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 2.353229080974485 | validation: 3.6149173477022165]
	TIME [epoch: 9.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4198835994618904		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 2.4198835994618904 | validation: 3.5444762745244818]
	TIME [epoch: 9.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3544603370277044		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 2.3544603370277044 | validation: 3.558816624249768]
	TIME [epoch: 9.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4194424803944985		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 2.4194424803944985 | validation: 3.6530824936242325]
	TIME [epoch: 9.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.509222418443337		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 2.509222418443337 | validation: 3.670987973280926]
	TIME [epoch: 9.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.396887854227807		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 2.396887854227807 | validation: 3.6378084449068626]
	TIME [epoch: 9.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.408701426875944		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 2.408701426875944 | validation: 3.6102667355882194]
	TIME [epoch: 9.71 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.395718616358775		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 2.395718616358775 | validation: 3.535351752204832]
	TIME [epoch: 9.71 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.371811533635511		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 2.371811533635511 | validation: 3.6537121712058553]
	TIME [epoch: 9.71 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.382863900863497		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 2.382863900863497 | validation: 3.518332343653642]
	TIME [epoch: 9.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.370775832497157		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 2.370775832497157 | validation: 3.547627425463956]
	TIME [epoch: 9.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.437510206954736		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 2.437510206954736 | validation: 3.603710904531298]
	TIME [epoch: 9.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.387375829519793		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 2.387375829519793 | validation: 3.5542854524541805]
	TIME [epoch: 9.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.347279125373626		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 2.347279125373626 | validation: 3.6088387962878588]
	TIME [epoch: 9.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3761508156450923		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 2.3761508156450923 | validation: 3.5001112574769877]
	TIME [epoch: 9.71 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.397607036827578		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 2.397607036827578 | validation: 3.5285348132671106]
	TIME [epoch: 9.71 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.372792887817801		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 2.372792887817801 | validation: 3.5538119980413776]
	TIME [epoch: 9.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3352808384720207		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 2.3352808384720207 | validation: 3.5127625423791073]
	TIME [epoch: 9.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359559878283524		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 2.359559878283524 | validation: 3.556458091655873]
	TIME [epoch: 9.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.391972684965583		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 2.391972684965583 | validation: 3.6014755499041904]
	TIME [epoch: 9.71 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4106238570331326		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 2.4106238570331326 | validation: 3.676916628237834]
	TIME [epoch: 9.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.426408527684379		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 2.426408527684379 | validation: 3.575100195777223]
	TIME [epoch: 9.71 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.383200666525141		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 2.383200666525141 | validation: 3.521225886984597]
	TIME [epoch: 9.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3528145800775206		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 2.3528145800775206 | validation: 3.4934922257500456]
	TIME [epoch: 9.73 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.329797408461449		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 2.329797408461449 | validation: 3.48908533569385]
	TIME [epoch: 9.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3290613418574284		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 2.3290613418574284 | validation: 3.6544249740015657]
	TIME [epoch: 9.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5069800050881406		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 2.5069800050881406 | validation: 3.5231829505399355]
	TIME [epoch: 9.71 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3978533352164813		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 2.3978533352164813 | validation: 3.483960502397516]
	TIME [epoch: 9.74 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3832361375669975		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 2.3832361375669975 | validation: 3.5100190636205104]
	TIME [epoch: 9.73 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3719664964405296		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 2.3719664964405296 | validation: 3.5152902196931786]
	TIME [epoch: 9.71 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3338292096202937		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 2.3338292096202937 | validation: 3.5045538463836228]
	TIME [epoch: 9.72 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.35268231958839		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 2.35268231958839 | validation: 3.6869019563635086]
	TIME [epoch: 9.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.446752203229807		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 2.446752203229807 | validation: 3.6046966636781117]
	TIME [epoch: 9.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.496717790633599		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 2.496717790633599 | validation: 3.5379107374208423]
	TIME [epoch: 9.72 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.400799526528016		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 2.400799526528016 | validation: 3.5876088145337195]
	TIME [epoch: 9.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3735650506213104		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 2.3735650506213104 | validation: 3.498834288602944]
	TIME [epoch: 9.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.371390166140949		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 2.371390166140949 | validation: 3.552459312478227]
	TIME [epoch: 9.71 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.343521819549531		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 2.343521819549531 | validation: 3.4850192221660223]
	TIME [epoch: 9.71 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326094271917081		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 2.326094271917081 | validation: 3.502977605786689]
	TIME [epoch: 9.74 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4488333778069205		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 2.4488333778069205 | validation: 3.562453382182588]
	TIME [epoch: 9.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3564570308327957		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 2.3564570308327957 | validation: 3.75231174524145]
	TIME [epoch: 9.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3946755962424073		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 2.3946755962424073 | validation: 3.7831019561250083]
	TIME [epoch: 9.71 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.446978552203285		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 2.446978552203285 | validation: 3.5475909603973084]
	TIME [epoch: 9.73 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.371561654692383		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 2.371561654692383 | validation: 3.6150244855360176]
	TIME [epoch: 9.72 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3809428224526643		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 2.3809428224526643 | validation: 3.498592479948409]
	TIME [epoch: 9.72 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3419224420165774		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 2.3419224420165774 | validation: 3.536470157094012]
	TIME [epoch: 9.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359402713344511		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 2.359402713344511 | validation: 3.544753849856923]
	TIME [epoch: 9.74 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337572452929975		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 2.337572452929975 | validation: 3.548760639689863]
	TIME [epoch: 9.72 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4045705974593754		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 2.4045705974593754 | validation: 3.6597489601961932]
	TIME [epoch: 9.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3875259343559896		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 2.3875259343559896 | validation: 3.516682861889624]
	TIME [epoch: 9.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.340518738762916		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 2.340518738762916 | validation: 3.5299298037363576]
	TIME [epoch: 9.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.347640738918471		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 2.347640738918471 | validation: 3.515024384532844]
	TIME [epoch: 9.71 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4354575189671306		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 2.4354575189671306 | validation: 3.7578189802027557]
	TIME [epoch: 9.74 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.460980779671709		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 2.460980779671709 | validation: 3.601230366829609]
	TIME [epoch: 9.74 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3511588205897764		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 2.3511588205897764 | validation: 3.5160921280721027]
	TIME [epoch: 9.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3390060426598573		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 2.3390060426598573 | validation: 3.5060519513747206]
	TIME [epoch: 9.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3551101893822497		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 2.3551101893822497 | validation: 3.609333516985032]
	TIME [epoch: 9.73 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3741162125025896		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 2.3741162125025896 | validation: 3.5982028494277767]
	TIME [epoch: 9.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.364378832543245		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 2.364378832543245 | validation: 3.554000109764178]
	TIME [epoch: 9.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.338127426503891		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 2.338127426503891 | validation: 3.6178467804054253]
	TIME [epoch: 9.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.369695427130572		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 2.369695427130572 | validation: 3.503818996029811]
	TIME [epoch: 9.71 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.341308797371759		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 2.341308797371759 | validation: 3.5666684521471064]
	TIME [epoch: 9.75 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331074818034197		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 2.331074818034197 | validation: 3.5839114746748555]
	TIME [epoch: 9.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4141410167996353		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 2.4141410167996353 | validation: 3.5421597606232598]
	TIME [epoch: 9.73 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.370089512292099		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 2.370089512292099 | validation: 3.487211179497067]
	TIME [epoch: 9.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3628254190303104		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 2.3628254190303104 | validation: 3.4948354679080844]
	TIME [epoch: 9.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.408810341770967		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 2.408810341770967 | validation: 3.551119868134447]
	TIME [epoch: 9.72 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3539320088078766		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 2.3539320088078766 | validation: 3.600324799999632]
	TIME [epoch: 9.73 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.352689336789364		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 2.352689336789364 | validation: 3.500206999886182]
	TIME [epoch: 9.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.356601377871562		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 2.356601377871562 | validation: 3.6196700754525444]
	TIME [epoch: 9.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3760031837425055		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 2.3760031837425055 | validation: 3.6355921834532867]
	TIME [epoch: 9.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3703512974235834		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 2.3703512974235834 | validation: 3.551179612344896]
	TIME [epoch: 9.72 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3415470174018593		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 2.3415470174018593 | validation: 3.476653471674648]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4437673335537524		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 2.4437673335537524 | validation: 3.5300323409296954]
	TIME [epoch: 9.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3609500696686885		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 2.3609500696686885 | validation: 3.6426623929977566]
	TIME [epoch: 9.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4385871783225284		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 2.4385871783225284 | validation: 3.50367506251863]
	TIME [epoch: 9.75 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3207992181601194		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 2.3207992181601194 | validation: 3.527689485800046]
	TIME [epoch: 9.76 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3285922553877416		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 2.3285922553877416 | validation: 3.516679150565618]
	TIME [epoch: 9.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.352629144999616		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 2.352629144999616 | validation: 3.547681894014787]
	TIME [epoch: 9.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3888846662763865		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 2.3888846662763865 | validation: 3.566937255464753]
	TIME [epoch: 9.76 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.356883567122849		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 2.356883567122849 | validation: 3.5250135925472468]
	TIME [epoch: 9.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4088131437251947		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 2.4088131437251947 | validation: 3.670539007355269]
	TIME [epoch: 9.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3676529445122716		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 2.3676529445122716 | validation: 3.5200476269723073]
	TIME [epoch: 9.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.324508735887998		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 2.324508735887998 | validation: 3.5742702764795466]
	TIME [epoch: 9.77 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3697442124103514		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 2.3697442124103514 | validation: 3.590288347382238]
	TIME [epoch: 9.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.376430600229717		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 2.376430600229717 | validation: 3.5333669314865763]
	TIME [epoch: 9.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.353766474700855		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 2.353766474700855 | validation: 3.5299906137677532]
	TIME [epoch: 9.75 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3716607865694406		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 2.3716607865694406 | validation: 3.536166557878066]
	TIME [epoch: 9.76 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.371349161441112		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 2.371349161441112 | validation: 3.5347694390520368]
	TIME [epoch: 9.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3828592585648716		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 2.3828592585648716 | validation: 3.5431411519795946]
	TIME [epoch: 9.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3327146048865037		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 2.3327146048865037 | validation: 3.684663110989538]
	TIME [epoch: 9.76 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.405926597010545		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 2.405926597010545 | validation: 3.527292845346606]
	TIME [epoch: 9.74 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3445621101574767		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 2.3445621101574767 | validation: 3.5324145487447574]
	TIME [epoch: 9.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3193905277916427		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 2.3193905277916427 | validation: 3.499945255868592]
	TIME [epoch: 9.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.343238263800025		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 2.343238263800025 | validation: 3.541469260027962]
	TIME [epoch: 9.76 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.332224620557442		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 2.332224620557442 | validation: 3.4995900595955893]
	TIME [epoch: 9.74 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3398798563173084		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 2.3398798563173084 | validation: 3.4860088965695524]
	TIME [epoch: 9.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.332389159153126		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 2.332389159153126 | validation: 3.4673252229092952]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3407945560583254		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 2.3407945560583254 | validation: 3.5322517042871913]
	TIME [epoch: 9.76 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.356059817659184		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 2.356059817659184 | validation: 3.518431793220438]
	TIME [epoch: 9.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3291632061562666		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 2.3291632061562666 | validation: 3.5082744012240403]
	TIME [epoch: 9.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3487758655915		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 2.3487758655915 | validation: 3.6103289362015905]
	TIME [epoch: 9.76 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.340740699590087		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 2.340740699590087 | validation: 3.4838890659136483]
	TIME [epoch: 9.74 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3346194255692287		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 2.3346194255692287 | validation: 3.528544719469112]
	TIME [epoch: 9.75 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3497103780699446		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 2.3497103780699446 | validation: 3.5229271988919812]
	TIME [epoch: 9.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.351948195682872		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 2.351948195682872 | validation: 3.504118977899336]
	TIME [epoch: 9.76 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3394572999550904		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 2.3394572999550904 | validation: 3.4890407463186945]
	TIME [epoch: 9.74 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3474860277161858		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 2.3474860277161858 | validation: 3.5166987570755635]
	TIME [epoch: 9.74 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331466934933355		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 2.331466934933355 | validation: 3.503513692039512]
	TIME [epoch: 9.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.338606085536021		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.338606085536021 | validation: 3.483602389422008]
	TIME [epoch: 9.76 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.355749724462264		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.355749724462264 | validation: 3.5870741334750504]
	TIME [epoch: 9.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.348168214774198		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.348168214774198 | validation: 3.5169071353014805]
	TIME [epoch: 9.74 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.356625262132654		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 2.356625262132654 | validation: 3.518615764571854]
	TIME [epoch: 9.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3989386236014374		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 2.3989386236014374 | validation: 3.49169156072605]
	TIME [epoch: 9.76 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.327424528322767		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 2.327424528322767 | validation: 3.4832125982704576]
	TIME [epoch: 9.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3602726624816466		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 2.3602726624816466 | validation: 3.498349813074191]
	TIME [epoch: 9.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.322758511463992		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 2.322758511463992 | validation: 3.4798905147151045]
	TIME [epoch: 9.75 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.346834611608607		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.346834611608607 | validation: 3.4769190662940335]
	TIME [epoch: 9.74 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3312112299412213		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 2.3312112299412213 | validation: 3.462049569958955]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.314871025218169		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 2.314871025218169 | validation: 3.4992491512982453]
	TIME [epoch: 9.74 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3504409122564076		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.3504409122564076 | validation: 3.4865114913515014]
	TIME [epoch: 9.75 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.32674218497148		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 2.32674218497148 | validation: 3.582598606170943]
	TIME [epoch: 9.74 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.353158321000708		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 2.353158321000708 | validation: 3.5070354322472292]
	TIME [epoch: 9.73 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3605249196284883		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.3605249196284883 | validation: 3.5721198190395156]
	TIME [epoch: 9.74 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4075073498478208		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 2.4075073498478208 | validation: 3.531313131679758]
	TIME [epoch: 9.76 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3430417241714485		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 2.3430417241714485 | validation: 3.5371382800387856]
	TIME [epoch: 9.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.327791195403512		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.327791195403512 | validation: 3.4734682121135836]
	TIME [epoch: 9.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3206927037518033		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 2.3206927037518033 | validation: 3.48665522608393]
	TIME [epoch: 9.75 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3415359947306933		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.3415359947306933 | validation: 3.531658331215429]
	TIME [epoch: 9.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3200678309393123		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 2.3200678309393123 | validation: 3.487977448956343]
	TIME [epoch: 9.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3458271151793006		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 2.3458271151793006 | validation: 3.568445472146263]
	TIME [epoch: 9.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3483804472310537		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 2.3483804472310537 | validation: 3.5336468998935993]
	TIME [epoch: 9.76 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337125358510119		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.337125358510119 | validation: 3.5187824352052686]
	TIME [epoch: 9.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3119221727272508		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 2.3119221727272508 | validation: 3.510757829829556]
	TIME [epoch: 9.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3534477039608		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 2.3534477039608 | validation: 3.4736858067294065]
	TIME [epoch: 9.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.320125703833456		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 2.320125703833456 | validation: 3.5125808952298305]
	TIME [epoch: 9.75 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.35586370167954		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 2.35586370167954 | validation: 3.495313387567056]
	TIME [epoch: 9.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4017041102434313		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 2.4017041102434313 | validation: 3.740439768897526]
	TIME [epoch: 9.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.374584276332367		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 2.374584276332367 | validation: 3.4983736896347213]
	TIME [epoch: 9.76 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3376510116948195		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.3376510116948195 | validation: 3.552656534288878]
	TIME [epoch: 9.75 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3180082224325074		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.3180082224325074 | validation: 3.54909026748255]
	TIME [epoch: 9.75 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.48112331849509		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 2.48112331849509 | validation: 3.544379692551452]
	TIME [epoch: 9.76 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.363546240795884		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.363546240795884 | validation: 3.466156100793954]
	TIME [epoch: 9.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.317562060673362		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 2.317562060673362 | validation: 3.4773617035066056]
	TIME [epoch: 9.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.312210309610243		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 2.312210309610243 | validation: 3.4733849609771257]
	TIME [epoch: 9.75 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.319880277669186		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 2.319880277669186 | validation: 3.507428913557203]
	TIME [epoch: 9.75 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3358979903149217		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 2.3358979903149217 | validation: 3.479040005260951]
	TIME [epoch: 9.78 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3950359392989964		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 2.3950359392989964 | validation: 3.5094863138790537]
	TIME [epoch: 9.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3562031828724077		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 2.3562031828724077 | validation: 3.49577917845342]
	TIME [epoch: 9.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.325135132574153		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 2.325135132574153 | validation: 3.4904586222847964]
	TIME [epoch: 9.76 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3272640251607153		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 2.3272640251607153 | validation: 3.518327160469983]
	TIME [epoch: 9.76 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.325934567537421		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 2.325934567537421 | validation: 3.469856067143619]
	TIME [epoch: 9.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316975522253397		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 2.316975522253397 | validation: 3.4811385256946212]
	TIME [epoch: 9.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3328006061111664		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 2.3328006061111664 | validation: 3.491930582563424]
	TIME [epoch: 9.77 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.327821991770635		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 2.327821991770635 | validation: 3.483453341424405]
	TIME [epoch: 9.75 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3191174253438636		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 2.3191174253438636 | validation: 3.4625671592945118]
	TIME [epoch: 9.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3116359831570685		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 2.3116359831570685 | validation: 3.49298712098063]
	TIME [epoch: 9.75 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3097651171870885		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 2.3097651171870885 | validation: 3.4586806969896235]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3215345277215547		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 2.3215345277215547 | validation: 3.5140573961378583]
	TIME [epoch: 9.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3280357581983155		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 2.3280357581983155 | validation: 3.473915444746867]
	TIME [epoch: 9.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3155722665960488		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 2.3155722665960488 | validation: 3.5169431504006448]
	TIME [epoch: 9.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.333424619324342		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 2.333424619324342 | validation: 3.5098862348258457]
	TIME [epoch: 9.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3126357991897173		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 2.3126357991897173 | validation: 3.5379616525744657]
	TIME [epoch: 9.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3274020550647205		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 2.3274020550647205 | validation: 3.485955555209463]
	TIME [epoch: 9.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3013143332799797		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 2.3013143332799797 | validation: 3.517477086051291]
	TIME [epoch: 9.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.330951787290899		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 2.330951787290899 | validation: 3.466494399500425]
	TIME [epoch: 9.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3074821986119707		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 2.3074821986119707 | validation: 3.5580948008506734]
	TIME [epoch: 9.71 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.321612828998665		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 2.321612828998665 | validation: 3.5339600678935]
	TIME [epoch: 9.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.33062214176683		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 2.33062214176683 | validation: 3.580685985223326]
	TIME [epoch: 9.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.360931077503871		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 2.360931077503871 | validation: 3.462277597079356]
	TIME [epoch: 9.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3033291033095766		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 2.3033291033095766 | validation: 3.474151747315815]
	TIME [epoch: 9.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3407046426498535		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 2.3407046426498535 | validation: 3.5229517437925417]
	TIME [epoch: 9.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3149606495265362		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 2.3149606495265362 | validation: 3.4850773194033993]
	TIME [epoch: 9.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3163465347049175		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 2.3163465347049175 | validation: 3.4976053158762626]
	TIME [epoch: 9.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3127531443165306		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 2.3127531443165306 | validation: 3.4680235050371784]
	TIME [epoch: 9.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3058792354883173		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 2.3058792354883173 | validation: 3.5046100514924787]
	TIME [epoch: 9.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337747629749131		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 2.337747629749131 | validation: 3.4854241637138457]
	TIME [epoch: 9.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3272446888063696		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 2.3272446888063696 | validation: 3.4850625736463208]
	TIME [epoch: 9.72 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316375943622707		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 2.316375943622707 | validation: 3.4632058680606486]
	TIME [epoch: 9.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2970614089618224		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 2.2970614089618224 | validation: 3.487211075010646]
	TIME [epoch: 9.74 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3841444645939265		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 2.3841444645939265 | validation: 3.6189289644713982]
	TIME [epoch: 9.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3707239804307796		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 2.3707239804307796 | validation: 3.5412621910368474]
	TIME [epoch: 9.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.336932759921838		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 2.336932759921838 | validation: 3.444624264435147]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3185369426379205		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 2.3185369426379205 | validation: 3.4737392655354644]
	TIME [epoch: 9.75 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4101515713273813		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 2.4101515713273813 | validation: 3.566095387842355]
	TIME [epoch: 9.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3303910132791055		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 2.3303910132791055 | validation: 3.4595941478511474]
	TIME [epoch: 9.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3042854292463786		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 2.3042854292463786 | validation: 3.4962107578494557]
	TIME [epoch: 9.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.309279215786175		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 2.309279215786175 | validation: 3.471513875189873]
	TIME [epoch: 9.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2999160346984406		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 2.2999160346984406 | validation: 3.4777604563513123]
	TIME [epoch: 9.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.301571976709341		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 2.301571976709341 | validation: 3.5046758001096756]
	TIME [epoch: 9.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3102369301218473		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 2.3102369301218473 | validation: 3.455987215777866]
	TIME [epoch: 9.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3412006024817025		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 2.3412006024817025 | validation: 3.4848339882861414]
	TIME [epoch: 9.73 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.323473681942108		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 2.323473681942108 | validation: 3.510505270238031]
	TIME [epoch: 9.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3139630758214382		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 2.3139630758214382 | validation: 3.4562372779920794]
	TIME [epoch: 9.73 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3473170581264204		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 2.3473170581264204 | validation: 3.643967714779455]
	TIME [epoch: 9.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3794372796664858		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 2.3794372796664858 | validation: 3.5183247252187266]
	TIME [epoch: 9.73 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3895488154851026		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 2.3895488154851026 | validation: 3.4822246615927486]
	TIME [epoch: 9.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.306674222827747		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 2.306674222827747 | validation: 3.5501878984257]
	TIME [epoch: 9.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.307495393844531		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 2.307495393844531 | validation: 3.546835619774807]
	TIME [epoch: 9.74 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4155759886982144		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 2.4155759886982144 | validation: 3.511583727269483]
	TIME [epoch: 9.73 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.30523147478651		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 2.30523147478651 | validation: 3.478974969230425]
	TIME [epoch: 9.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2926794987440386		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 2.2926794987440386 | validation: 3.5110843361614243]
	TIME [epoch: 9.75 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.294419748236007		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 2.294419748236007 | validation: 3.4759855393739434]
	TIME [epoch: 9.74 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3237193250088213		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 2.3237193250088213 | validation: 3.512303038474878]
	TIME [epoch: 9.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3060728132747		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 2.3060728132747 | validation: 3.4617863115895937]
	TIME [epoch: 9.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2892210934329453		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 2.2892210934329453 | validation: 3.45659524384655]
	TIME [epoch: 9.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.300924185554085		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 2.300924185554085 | validation: 3.482734387487095]
	TIME [epoch: 9.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.346505512678525		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 2.346505512678525 | validation: 3.4848185329892187]
	TIME [epoch: 9.74 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3060173189420397		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 2.3060173189420397 | validation: 3.4564530243053655]
	TIME [epoch: 9.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3101940549125137		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 2.3101940549125137 | validation: 3.480793152715494]
	TIME [epoch: 9.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284973218504014		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 2.284973218504014 | validation: 3.502162399518894]
	TIME [epoch: 9.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3255121491414226		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 2.3255121491414226 | validation: 3.504390532729531]
	TIME [epoch: 9.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.29856584583076		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 2.29856584583076 | validation: 3.4639545336487276]
	TIME [epoch: 9.74 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303167852448575		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 2.303167852448575 | validation: 3.512652928925836]
	TIME [epoch: 9.75 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3127272637168024		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 2.3127272637168024 | validation: 3.4600223397967618]
	TIME [epoch: 9.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3112530354741523		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 2.3112530354741523 | validation: 3.4958824901960868]
	TIME [epoch: 9.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3145080268378857		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 2.3145080268378857 | validation: 3.5622203126675864]
	TIME [epoch: 9.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326187696785854		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 2.326187696785854 | validation: 3.4523669594928617]
	TIME [epoch: 9.73 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.30174228016045		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 2.30174228016045 | validation: 3.4627727162757824]
	TIME [epoch: 9.73 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3160950330519467		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 2.3160950330519467 | validation: 3.45670131794348]
	TIME [epoch: 9.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.307179894957724		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 2.307179894957724 | validation: 3.4560126802899163]
	TIME [epoch: 9.74 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3011691532453846		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 2.3011691532453846 | validation: 3.448830753527503]
	TIME [epoch: 9.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3055661471883857		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 2.3055661471883857 | validation: 3.4840490661956576]
	TIME [epoch: 9.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315409167224575		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 2.315409167224575 | validation: 3.48397801706718]
	TIME [epoch: 9.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3278905621024166		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 2.3278905621024166 | validation: 3.4704403553061196]
	TIME [epoch: 9.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315155376718102		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 2.315155376718102 | validation: 3.4513272802946813]
	TIME [epoch: 9.72 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.300565061808856		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 2.300565061808856 | validation: 3.457256989639021]
	TIME [epoch: 9.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3013421297046697		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 2.3013421297046697 | validation: 3.4568643235156253]
	TIME [epoch: 9.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3071577223194897		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 2.3071577223194897 | validation: 3.454917029265221]
	TIME [epoch: 9.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.296636191247651		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 2.296636191247651 | validation: 3.4701161314763023]
	TIME [epoch: 9.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.304739861116102		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 2.304739861116102 | validation: 3.521840754075061]
	TIME [epoch: 9.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.317088920213968		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 2.317088920213968 | validation: 3.4879869155474332]
	TIME [epoch: 9.76 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3182736410698235		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 2.3182736410698235 | validation: 3.4716823679496147]
	TIME [epoch: 9.74 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298750712325627		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 2.298750712325627 | validation: 3.4812623029958343]
	TIME [epoch: 9.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2921328894761612		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 2.2921328894761612 | validation: 3.44704206442707]
	TIME [epoch: 9.74 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288534055202814		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 2.288534055202814 | validation: 3.448099347621923]
	TIME [epoch: 9.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3035745595358863		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 2.3035745595358863 | validation: 3.4771408033615865]
	TIME [epoch: 9.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2873841331563773		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 2.2873841331563773 | validation: 3.4654651512904264]
	TIME [epoch: 9.73 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.314839682960463		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 2.314839682960463 | validation: 3.50242485703812]
	TIME [epoch: 9.75 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4050368990093545		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 2.4050368990093545 | validation: 3.649969963234865]
	TIME [epoch: 9.74 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3306382118283677		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 2.3306382118283677 | validation: 3.4890264431147844]
	TIME [epoch: 9.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2861257616702204		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 2.2861257616702204 | validation: 3.460943584032733]
	TIME [epoch: 9.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.280016548243193		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 2.280016548243193 | validation: 3.44903667252841]
	TIME [epoch: 9.75 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.286303315405868		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 2.286303315405868 | validation: 3.4633752247159264]
	TIME [epoch: 9.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284896838540528		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 2.284896838540528 | validation: 3.4560012641813933]
	TIME [epoch: 9.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.295164408467496		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 2.295164408467496 | validation: 3.461319401364238]
	TIME [epoch: 9.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.297782534598107		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 2.297782534598107 | validation: 3.4606585852084017]
	TIME [epoch: 9.75 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3060786045067614		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 2.3060786045067614 | validation: 3.4788035624790634]
	TIME [epoch: 9.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315216351183248		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 2.315216351183248 | validation: 3.4489602411749103]
	TIME [epoch: 9.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3187589062360057		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 2.3187589062360057 | validation: 3.4868801155061853]
	TIME [epoch: 9.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2928086517952844		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 2.2928086517952844 | validation: 3.507571229529134]
	TIME [epoch: 9.74 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.300823694661273		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 2.300823694661273 | validation: 3.4813445434722614]
	TIME [epoch: 9.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3204746497195985		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 2.3204746497195985 | validation: 3.484814743963617]
	TIME [epoch: 9.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.291355061149645		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 2.291355061149645 | validation: 3.484136774985229]
	TIME [epoch: 9.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2901698476384773		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 2.2901698476384773 | validation: 3.4886125936232846]
	TIME [epoch: 9.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2862081242404826		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 2.2862081242404826 | validation: 3.453654496485588]
	TIME [epoch: 9.72 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.279678268770697		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 2.279678268770697 | validation: 3.473423183355734]
	TIME [epoch: 9.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3127971527246243		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 2.3127971527246243 | validation: 3.527060960970962]
	TIME [epoch: 9.74 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3152928257400007		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 2.3152928257400007 | validation: 3.4573952113731217]
	TIME [epoch: 9.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2965204961483368		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 2.2965204961483368 | validation: 3.538809305451954]
	TIME [epoch: 9.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3359188861923523		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 2.3359188861923523 | validation: 3.49607786762517]
	TIME [epoch: 9.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.306986483146484		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 2.306986483146484 | validation: 3.4856113177489165]
	TIME [epoch: 9.74 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.302914551751882		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 2.302914551751882 | validation: 3.4485666200808764]
	TIME [epoch: 9.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288489551117367		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 2.288489551117367 | validation: 3.457126522843534]
	TIME [epoch: 9.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3055670194282945		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 2.3055670194282945 | validation: 3.452964149018036]
	TIME [epoch: 9.74 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.28545477358651		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 2.28545477358651 | validation: 3.4446031696534742]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3138133130923557		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 2.3138133130923557 | validation: 3.448802612945177]
	TIME [epoch: 9.72 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2965977291875346		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 2.2965977291875346 | validation: 3.464837565390573]
	TIME [epoch: 9.72 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2871373186583837		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 2.2871373186583837 | validation: 3.4495787433051976]
	TIME [epoch: 9.74 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2702280762547797		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 2.2702280762547797 | validation: 3.4712380850171196]
	TIME [epoch: 9.73 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3106808486131745		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 2.3106808486131745 | validation: 3.4533028612101675]
	TIME [epoch: 9.73 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.296977262739357		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 2.296977262739357 | validation: 3.4797757024300964]
	TIME [epoch: 9.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.294280555172248		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 2.294280555172248 | validation: 3.4770676590933567]
	TIME [epoch: 9.75 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.28128718641889		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 2.28128718641889 | validation: 3.4323052063596635]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2875547124208113		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 2.2875547124208113 | validation: 3.4727659639396053]
	TIME [epoch: 9.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2877710117174788		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 2.2877710117174788 | validation: 3.4525364108923204]
	TIME [epoch: 9.73 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2873148771702363		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 2.2873148771702363 | validation: 3.450394621831317]
	TIME [epoch: 9.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2824156068718726		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 2.2824156068718726 | validation: 3.480497568125976]
	TIME [epoch: 9.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3143312364451147		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 2.3143312364451147 | validation: 3.478714364197789]
	TIME [epoch: 9.71 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.31194289519793		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 2.31194289519793 | validation: 3.4928662229873098]
	TIME [epoch: 9.74 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3250505985288767		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 2.3250505985288767 | validation: 3.465512613396113]
	TIME [epoch: 9.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2883813138794618		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 2.2883813138794618 | validation: 3.4955300244464484]
	TIME [epoch: 9.72 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2988634312900964		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 2.2988634312900964 | validation: 3.4597705463990067]
	TIME [epoch: 9.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288564992758622		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 2.288564992758622 | validation: 3.4372595249717364]
	TIME [epoch: 9.74 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.283439629758026		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 2.283439629758026 | validation: 3.483165574241033]
	TIME [epoch: 9.73 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2955245092491325		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 2.2955245092491325 | validation: 3.4807502465428297]
	TIME [epoch: 9.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2998479840006225		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 2.2998479840006225 | validation: 3.4905787044074934]
	TIME [epoch: 9.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.295609856148568		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 2.295609856148568 | validation: 3.4798415151405595]
	TIME [epoch: 9.73 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2975287917363323		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 2.2975287917363323 | validation: 3.4411121786825243]
	TIME [epoch: 9.72 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2930079482166006		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 2.2930079482166006 | validation: 3.468097498991442]
	TIME [epoch: 9.71 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2971379062139445		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 2.2971379062139445 | validation: 3.4499396015068364]
	TIME [epoch: 9.73 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.294595598619579		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 2.294595598619579 | validation: 3.5182265760077485]
	TIME [epoch: 9.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.299329242516126		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 2.299329242516126 | validation: 3.4548636970221605]
	TIME [epoch: 9.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289424086997112		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 2.289424086997112 | validation: 3.457172135476792]
	TIME [epoch: 9.71 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2820240651557855		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 2.2820240651557855 | validation: 3.472703903413809]
	TIME [epoch: 9.72 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2955707434730024		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 2.2955707434730024 | validation: 3.5295549384965716]
	TIME [epoch: 9.71 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3067582710190533		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 2.3067582710190533 | validation: 3.481343916087127]
	TIME [epoch: 9.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.297304815462385		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 2.297304815462385 | validation: 3.4429442678361823]
	TIME [epoch: 9.71 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.285589083636053		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 2.285589083636053 | validation: 3.454005617370928]
	TIME [epoch: 9.74 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3047569011065887		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 2.3047569011065887 | validation: 3.527409771403602]
	TIME [epoch: 9.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3124940651339494		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 2.3124940651339494 | validation: 3.4376098140662372]
	TIME [epoch: 9.72 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2894450001619853		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 2.2894450001619853 | validation: 3.499663524786307]
	TIME [epoch: 9.71 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.28498733180954		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 2.28498733180954 | validation: 3.4762886618075015]
	TIME [epoch: 9.72 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3121040476331		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 2.3121040476331 | validation: 3.4468275309997285]
	TIME [epoch: 9.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303544091500078		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 2.303544091500078 | validation: 3.515105147805637]
	TIME [epoch: 9.72 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.312838080348763		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 2.312838080348763 | validation: 3.4711411308462368]
	TIME [epoch: 9.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2922691176053136		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 2.2922691176053136 | validation: 3.6336593146352527]
	TIME [epoch: 9.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3246287366556198		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 2.3246287366556198 | validation: 3.447026701225276]
	TIME [epoch: 9.72 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.271109355690455		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 2.271109355690455 | validation: 3.5073238363651913]
	TIME [epoch: 9.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2896565485246256		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 2.2896565485246256 | validation: 3.442806660823684]
	TIME [epoch: 9.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2765084830518467		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 2.2765084830518467 | validation: 3.622470506118694]
	TIME [epoch: 9.71 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.374498585087179		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 2.374498585087179 | validation: 3.4473010959565626]
	TIME [epoch: 9.72 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2937752035627845		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 2.2937752035627845 | validation: 3.451389060840678]
	TIME [epoch: 9.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2736763078889646		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 2.2736763078889646 | validation: 3.4401306913621386]
	TIME [epoch: 9.73 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.280314855808893		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 2.280314855808893 | validation: 3.4592071113917164]
	TIME [epoch: 9.71 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2951162582775506		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 2.2951162582775506 | validation: 3.4771534441688825]
	TIME [epoch: 9.71 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2780563706199226		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 2.2780563706199226 | validation: 3.4461603683056468]
	TIME [epoch: 9.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2857236856271617		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 2.2857236856271617 | validation: 3.4501163539498316]
	TIME [epoch: 9.71 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284977412181491		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 2.284977412181491 | validation: 3.4568166093067543]
	TIME [epoch: 9.71 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2771506495007103		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 2.2771506495007103 | validation: 3.477729509084251]
	TIME [epoch: 9.71 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2862845288309632		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 2.2862845288309632 | validation: 3.4458623257166705]
	TIME [epoch: 9.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2979512709462515		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 2.2979512709462515 | validation: 3.449904032539529]
	TIME [epoch: 9.71 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315548846639379		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 2.315548846639379 | validation: 3.4836706464199536]
	TIME [epoch: 9.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315739358533022		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 2.315739358533022 | validation: 3.471640738383975]
	TIME [epoch: 9.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.306885805465827		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 2.306885805465827 | validation: 3.4415209176688304]
	TIME [epoch: 9.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2702574594308595		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 2.2702574594308595 | validation: 3.465934130379949]
	TIME [epoch: 9.71 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2776296025988403		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 2.2776296025988403 | validation: 3.4440764935468433]
	TIME [epoch: 9.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284666149442715		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 2.284666149442715 | validation: 3.4341309599917382]
	TIME [epoch: 9.73 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.280482756875172		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 2.280482756875172 | validation: 3.4392900151470363]
	TIME [epoch: 9.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.281216512001829		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 2.281216512001829 | validation: 3.463581692073]
	TIME [epoch: 9.71 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2736665301876		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 2.2736665301876 | validation: 3.4573864006528936]
	TIME [epoch: 9.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2789800847001773		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 2.2789800847001773 | validation: 3.44838062934235]
	TIME [epoch: 9.72 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2819966000200607		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 2.2819966000200607 | validation: 3.514233118149873]
	TIME [epoch: 9.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2759074263080796		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 2.2759074263080796 | validation: 3.452997079123834]
	TIME [epoch: 9.72 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2946048464953663		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 2.2946048464953663 | validation: 3.48056396934391]
	TIME [epoch: 9.71 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3303476000281544		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 2.3303476000281544 | validation: 3.4844629820448736]
	TIME [epoch: 9.74 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.29140506560428		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 2.29140506560428 | validation: 3.460952929376521]
	TIME [epoch: 9.72 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2994326553159086		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 2.2994326553159086 | validation: 3.4404024042325747]
	TIME [epoch: 9.72 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2944204441274985		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 2.2944204441274985 | validation: 3.4614432186284354]
	TIME [epoch: 9.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.296090292404851		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 2.296090292404851 | validation: 3.4346619783620596]
	TIME [epoch: 9.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.280081708867126		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 2.280081708867126 | validation: 3.5198113106479925]
	TIME [epoch: 9.72 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.302440757934893		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 2.302440757934893 | validation: 3.4451741364369317]
	TIME [epoch: 9.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3063016653901562		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 2.3063016653901562 | validation: 3.461172495666837]
	TIME [epoch: 9.72 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2933025543842707		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 2.2933025543842707 | validation: 3.4645908819575455]
	TIME [epoch: 9.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3087882121573102		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 2.3087882121573102 | validation: 3.499132675392151]
	TIME [epoch: 9.72 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2875529062207454		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 2.2875529062207454 | validation: 3.4705437861427355]
	TIME [epoch: 9.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273418249370695		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 2.273418249370695 | validation: 3.4392430259129583]
	TIME [epoch: 9.74 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284541657038541		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 2.284541657038541 | validation: 3.450995487988074]
	TIME [epoch: 9.71 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2729851771098026		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 2.2729851771098026 | validation: 3.4722341868623285]
	TIME [epoch: 9.71 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3166632791824564		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 2.3166632791824564 | validation: 3.4619788381365626]
	TIME [epoch: 9.72 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2830738839986155		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 2.2830738839986155 | validation: 3.469030981075405]
	TIME [epoch: 9.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2816937026803172		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 2.2816937026803172 | validation: 3.4982192892878015]
	TIME [epoch: 9.71 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.327239731280247		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 2.327239731280247 | validation: 3.4724143965665206]
	TIME [epoch: 9.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.282883470237024		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 2.282883470237024 | validation: 3.4412057257087922]
	TIME [epoch: 9.71 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303559814356597		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 2.303559814356597 | validation: 3.462478129843409]
	TIME [epoch: 9.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2844145669334353		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 2.2844145669334353 | validation: 3.4932508728932317]
	TIME [epoch: 9.72 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288054448319854		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 2.288054448319854 | validation: 3.436528186300182]
	TIME [epoch: 9.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2878431673581234		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 2.2878431673581234 | validation: 3.475022890192928]
	TIME [epoch: 9.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278108643983941		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 2.278108643983941 | validation: 3.445420971863455]
	TIME [epoch: 9.72 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2788851975529325		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 2.2788851975529325 | validation: 3.4334347527017894]
	TIME [epoch: 9.71 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.302603734154081		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 2.302603734154081 | validation: 3.448404367049656]
	TIME [epoch: 9.72 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269619717386637		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 2.269619717386637 | validation: 3.481080374945951]
	TIME [epoch: 9.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2795344777276414		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 2.2795344777276414 | validation: 3.466254749120447]
	TIME [epoch: 9.71 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298935987996186		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 2.298935987996186 | validation: 3.4584015312989203]
	TIME [epoch: 9.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2724566074198065		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 2.2724566074198065 | validation: 3.46529271843715]
	TIME [epoch: 9.71 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284813927291217		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 2.284813927291217 | validation: 3.4406617536168564]
	TIME [epoch: 9.72 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284519551843533		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 2.284519551843533 | validation: 3.4439062870587294]
	TIME [epoch: 9.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266305806940883		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 2.266305806940883 | validation: 3.4626729251632957]
	TIME [epoch: 9.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284265216024836		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 2.284265216024836 | validation: 3.476756187626114]
	TIME [epoch: 9.71 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3064973024302566		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 2.3064973024302566 | validation: 3.48368543566746]
	TIME [epoch: 9.71 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2801197647631533		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 2.2801197647631533 | validation: 3.4463657321716887]
	TIME [epoch: 9.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2882281232843438		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 2.2882281232843438 | validation: 3.4714107624674733]
	TIME [epoch: 9.71 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2767004198899157		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 2.2767004198899157 | validation: 3.4630511754925157]
	TIME [epoch: 9.74 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3047051711465074		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 2.3047051711465074 | validation: 3.4858248924952218]
	TIME [epoch: 9.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2745397639894		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 2.2745397639894 | validation: 3.4540639374890016]
	TIME [epoch: 9.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2865512515373103		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 2.2865512515373103 | validation: 3.4717864759537127]
	TIME [epoch: 9.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.279746916420559		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 2.279746916420559 | validation: 3.4571351950296925]
	TIME [epoch: 9.72 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.285098346484476		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 2.285098346484476 | validation: 3.4817303497791476]
	TIME [epoch: 9.72 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.277513372917772		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 2.277513372917772 | validation: 3.451819046941426]
	TIME [epoch: 9.71 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2747993782091243		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 2.2747993782091243 | validation: 3.4474511613655965]
	TIME [epoch: 9.71 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3055418667376832		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 2.3055418667376832 | validation: 3.4638125193926466]
	TIME [epoch: 9.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2744748752463897		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 2.2744748752463897 | validation: 3.4412242427396893]
	TIME [epoch: 9.71 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.272499444240844		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 2.272499444240844 | validation: 3.440054284875093]
	TIME [epoch: 9.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2874617582891417		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 2.2874617582891417 | validation: 3.4446065255612712]
	TIME [epoch: 9.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2722054003306082		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 2.2722054003306082 | validation: 3.4960071035567357]
	TIME [epoch: 9.71 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2765627485397664		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 2.2765627485397664 | validation: 3.4520375638590863]
	TIME [epoch: 9.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2729433378024106		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 2.2729433378024106 | validation: 3.432844796147027]
	TIME [epoch: 9.71 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2699015795306066		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 2.2699015795306066 | validation: 3.4587422269276464]
	TIME [epoch: 9.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2833468001216533		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 2.2833468001216533 | validation: 3.448820852548156]
	TIME [epoch: 9.71 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.27450684199786		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 2.27450684199786 | validation: 3.464172018786511]
	TIME [epoch: 9.72 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.277445248921741		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 2.277445248921741 | validation: 3.4641039985277144]
	TIME [epoch: 9.71 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.274426557696274		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 2.274426557696274 | validation: 3.443714226988067]
	TIME [epoch: 9.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2921803927794953		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 2.2921803927794953 | validation: 3.4677280531141608]
	TIME [epoch: 9.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2884041468428364		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 2.2884041468428364 | validation: 3.4792292620157594]
	TIME [epoch: 9.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2685924622327795		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 2.2685924622327795 | validation: 3.468493799933682]
	TIME [epoch: 9.71 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289996371877336		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 2.289996371877336 | validation: 3.489570861731297]
	TIME [epoch: 9.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2851836975497806		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 2.2851836975497806 | validation: 3.4452347919450905]
	TIME [epoch: 9.71 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273919686160049		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 2.273919686160049 | validation: 3.4326776340023746]
	TIME [epoch: 9.71 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2765275189858167		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 2.2765275189858167 | validation: 3.4529124379085636]
	TIME [epoch: 9.71 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.299323290732571		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 2.299323290732571 | validation: 3.5493920222716606]
	TIME [epoch: 9.69 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3017739699275257		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 2.3017739699275257 | validation: 3.4332294258105707]
	TIME [epoch: 9.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2652058627625764		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 2.2652058627625764 | validation: 3.4598356342553487]
	TIME [epoch: 9.69 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278913837463417		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 2.278913837463417 | validation: 3.4527040977033767]
	TIME [epoch: 9.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2745685958128314		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 2.2745685958128314 | validation: 3.432573530942412]
	TIME [epoch: 9.71 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.26250341665835		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 2.26250341665835 | validation: 3.4896387839725835]
	TIME [epoch: 9.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.283262055237097		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 2.283262055237097 | validation: 3.469995938952385]
	TIME [epoch: 9.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2755384082241035		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 2.2755384082241035 | validation: 3.435889842698234]
	TIME [epoch: 9.72 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.264177934950862		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 2.264177934950862 | validation: 3.4405188846434602]
	TIME [epoch: 9.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263523651565032		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 2.263523651565032 | validation: 3.4692397542674054]
	TIME [epoch: 9.71 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3090102522455966		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 2.3090102522455966 | validation: 3.4433904047610886]
	TIME [epoch: 9.71 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270791282780332		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 2.270791282780332 | validation: 3.4469027822112652]
	TIME [epoch: 9.71 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.27247704879751		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 2.27247704879751 | validation: 3.425674368284391]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2722093364201634		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 2.2722093364201634 | validation: 3.5007069314691273]
	TIME [epoch: 9.72 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289225045177809		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 2.289225045177809 | validation: 3.425637474090057]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_989.pth
	Model improved!!!
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2623267764624257		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 2.2623267764624257 | validation: 3.460090322425033]
	TIME [epoch: 9.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.292145618993485		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 2.292145618993485 | validation: 3.4342565586893214]
	TIME [epoch: 9.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265870034829144		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 2.265870034829144 | validation: 3.431831655976152]
	TIME [epoch: 9.72 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3036855242673857		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 2.3036855242673857 | validation: 3.4683305331181966]
	TIME [epoch: 9.72 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266410718783736		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 2.266410718783736 | validation: 3.4306749286206686]
	TIME [epoch: 9.71 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.292790585852228		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 2.292790585852228 | validation: 3.4350153319146335]
	TIME [epoch: 9.71 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2670872212509683		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 2.2670872212509683 | validation: 3.4510549782736697]
	TIME [epoch: 9.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.271647451478001		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 2.271647451478001 | validation: 3.4269123450680694]
	TIME [epoch: 9.71 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2705625826549993		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 2.2705625826549993 | validation: 3.4367421109815575]
	TIME [epoch: 9.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2665137866861		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 2.2665137866861 | validation: 3.4710064073930527]
	TIME [epoch: 9.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270126291337275		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 2.270126291337275 | validation: 3.45886356016154]
	TIME [epoch: 9.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2794767843820587		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 2.2794767843820587 | validation: 3.4775351310849305]
	TIME [epoch: 9.71 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2803683086906723		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 2.2803683086906723 | validation: 3.4327431042220145]
	TIME [epoch: 9.71 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256154845381642		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 2.256154845381642 | validation: 3.4820923748788233]
	TIME [epoch: 9.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.274624058895349		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 2.274624058895349 | validation: 3.427631375199423]
	TIME [epoch: 9.72 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270119410970829		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 2.270119410970829 | validation: 3.436055041169883]
	TIME [epoch: 9.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2582798975481317		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 2.2582798975481317 | validation: 3.4364508443193906]
	TIME [epoch: 9.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2720690505724		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 2.2720690505724 | validation: 3.4630421365493387]
	TIME [epoch: 9.72 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.276644476438057		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 2.276644476438057 | validation: 3.461040050286148]
	TIME [epoch: 9.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2972915984190565		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 2.2972915984190565 | validation: 3.464221531659325]
	TIME [epoch: 9.72 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2729822369056203		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 2.2729822369056203 | validation: 3.419667236263617]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.29109614530633		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 2.29109614530633 | validation: 3.454031261157872]
	TIME [epoch: 9.72 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259453517624693		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 2.259453517624693 | validation: 3.4280376737227853]
	TIME [epoch: 9.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266700546065082		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 2.266700546065082 | validation: 3.4717183211390625]
	TIME [epoch: 9.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.271299918007535		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 2.271299918007535 | validation: 3.453169227198754]
	TIME [epoch: 9.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2727889012100007		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 2.2727889012100007 | validation: 3.4418867824946595]
	TIME [epoch: 9.72 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.309498388771664		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 2.309498388771664 | validation: 3.4671933128252626]
	TIME [epoch: 9.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289654354577793		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 2.289654354577793 | validation: 3.49847875794448]
	TIME [epoch: 9.72 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3065596336324865		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 2.3065596336324865 | validation: 3.459498224623599]
	TIME [epoch: 9.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.277119767989034		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 2.277119767989034 | validation: 3.4442191887553126]
	TIME [epoch: 9.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3155461996529105		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 2.3155461996529105 | validation: 3.440527234761794]
	TIME [epoch: 9.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2677993329387465		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 2.2677993329387465 | validation: 3.447345910356172]
	TIME [epoch: 9.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266963130917415		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 2.266963130917415 | validation: 3.461397377577638]
	TIME [epoch: 9.71 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266209472476798		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 2.266209472476798 | validation: 3.4246948521129923]
	TIME [epoch: 9.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.268012105186167		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 2.268012105186167 | validation: 3.4279484106212372]
	TIME [epoch: 9.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2837878435117394		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 2.2837878435117394 | validation: 3.4359508759379755]
	TIME [epoch: 9.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255951537558441		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 2.255951537558441 | validation: 3.4264617398935804]
	TIME [epoch: 9.71 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2798210377554273		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 2.2798210377554273 | validation: 3.4290700265321226]
	TIME [epoch: 9.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2714791909033973		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 2.2714791909033973 | validation: 3.4651207817262155]
	TIME [epoch: 9.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303613104827563		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 2.303613104827563 | validation: 3.45682276141152]
	TIME [epoch: 9.69 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2704280277552735		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 2.2704280277552735 | validation: 3.445157051443815]
	TIME [epoch: 9.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2663521345409743		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 2.2663521345409743 | validation: 3.4438916206477526]
	TIME [epoch: 9.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2720513820293045		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 2.2720513820293045 | validation: 3.4367296812041865]
	TIME [epoch: 9.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284643365824558		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 2.284643365824558 | validation: 3.4343470006593897]
	TIME [epoch: 9.72 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2584630397586123		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 2.2584630397586123 | validation: 3.4215877798375947]
	TIME [epoch: 9.72 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262901007466861		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 2.262901007466861 | validation: 3.4309620044933387]
	TIME [epoch: 9.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.291516522184477		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 2.291516522184477 | validation: 3.49570763581483]
	TIME [epoch: 9.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2800653206926307		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 2.2800653206926307 | validation: 3.4433336891506583]
	TIME [epoch: 9.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2684500182760807		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 2.2684500182760807 | validation: 3.4337960937466914]
	TIME [epoch: 9.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2687589076889387		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 2.2687589076889387 | validation: 3.425331346140152]
	TIME [epoch: 9.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2653140813128063		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 2.2653140813128063 | validation: 3.4566695100270706]
	TIME [epoch: 9.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2661392451180733		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 2.2661392451180733 | validation: 3.4477608572142304]
	TIME [epoch: 9.72 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2667959945180343		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 2.2667959945180343 | validation: 3.438909591380566]
	TIME [epoch: 9.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261016528214178		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 2.261016528214178 | validation: 3.443348931164146]
	TIME [epoch: 9.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2769988876658473		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 2.2769988876658473 | validation: 3.4314141434562986]
	TIME [epoch: 9.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2747206552434247		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 2.2747206552434247 | validation: 3.4361075417575773]
	TIME [epoch: 9.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2629453574492944		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 2.2629453574492944 | validation: 3.470235847894593]
	TIME [epoch: 9.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265844166649476		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 2.265844166649476 | validation: 3.4370750990941694]
	TIME [epoch: 9.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289040386948348		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 2.289040386948348 | validation: 3.4563183735454217]
	TIME [epoch: 9.73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2688891555185307		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 2.2688891555185307 | validation: 3.4607993769141006]
	TIME [epoch: 9.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269353038831021		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 2.269353038831021 | validation: 3.4286229026967825]
	TIME [epoch: 9.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2562356945643005		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 2.2562356945643005 | validation: 3.435658266691183]
	TIME [epoch: 9.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2637124943428786		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 2.2637124943428786 | validation: 3.4967917512024718]
	TIME [epoch: 9.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.305343528512554		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 2.305343528512554 | validation: 3.427681912965918]
	TIME [epoch: 9.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.26629532363621		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 2.26629532363621 | validation: 3.4411132201098367]
	TIME [epoch: 9.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2577607527208334		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 2.2577607527208334 | validation: 3.4411015429135343]
	TIME [epoch: 9.69 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2604030028519047		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 2.2604030028519047 | validation: 3.4522730707364935]
	TIME [epoch: 9.73 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.299327126400292		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 2.299327126400292 | validation: 3.4525840106736005]
	TIME [epoch: 9.72 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2705051996442527		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 2.2705051996442527 | validation: 3.4304521064009608]
	TIME [epoch: 9.71 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2652697053748727		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 2.2652697053748727 | validation: 3.430225683254206]
	TIME [epoch: 9.69 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2572828899991		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 2.2572828899991 | validation: 3.4353637419160212]
	TIME [epoch: 9.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2532322165512966		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 2.2532322165512966 | validation: 3.4323818204654404]
	TIME [epoch: 9.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2997988705551036		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 2.2997988705551036 | validation: 3.495108289280368]
	TIME [epoch: 9.72 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2762627880680593		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 2.2762627880680593 | validation: 3.413283489104855]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270169123401463		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 2.270169123401463 | validation: 3.4512753555051696]
	TIME [epoch: 9.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2693602006628124		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 2.2693602006628124 | validation: 3.422289760454287]
	TIME [epoch: 9.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2663292885320723		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 2.2663292885320723 | validation: 3.4346154443989154]
	TIME [epoch: 9.72 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2505868716972324		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 2.2505868716972324 | validation: 3.437916019738923]
	TIME [epoch: 9.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.275900127549804		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 2.275900127549804 | validation: 3.4999104251520543]
	TIME [epoch: 9.72 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2985257799594114		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 2.2985257799594114 | validation: 3.434425924850375]
	TIME [epoch: 9.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2599041528657646		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 2.2599041528657646 | validation: 3.4464393443030565]
	TIME [epoch: 9.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265408039340611		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 2.265408039340611 | validation: 3.4428785856970205]
	TIME [epoch: 9.72 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263555190302399		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 2.263555190302399 | validation: 3.413627384296715]
	TIME [epoch: 9.72 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258573999209235		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 2.258573999209235 | validation: 3.4268265188443467]
	TIME [epoch: 9.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2575250478822477		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 2.2575250478822477 | validation: 3.448917490022133]
	TIME [epoch: 9.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2610762261008013		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 2.2610762261008013 | validation: 3.413607042372538]
	TIME [epoch: 9.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2740040755997177		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 2.2740040755997177 | validation: 3.4291136498973764]
	TIME [epoch: 9.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25787606267707		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 2.25787606267707 | validation: 3.4408886773977736]
	TIME [epoch: 9.72 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2705137889554727		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 2.2705137889554727 | validation: 3.4698535584780963]
	TIME [epoch: 9.73 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2718134586994068		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 2.2718134586994068 | validation: 3.4343972940992313]
	TIME [epoch: 9.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261860946487698		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 2.261860946487698 | validation: 3.431905563733497]
	TIME [epoch: 9.72 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2733208274176127		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 2.2733208274176127 | validation: 3.435411253046491]
	TIME [epoch: 9.72 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256592769713177		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 2.256592769713177 | validation: 3.4305274505317436]
	TIME [epoch: 9.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265822110143618		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 2.265822110143618 | validation: 3.445048149493392]
	TIME [epoch: 9.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289319620125836		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 2.289319620125836 | validation: 3.437955638099186]
	TIME [epoch: 9.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2713481801762088		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 2.2713481801762088 | validation: 3.430081950621146]
	TIME [epoch: 9.74 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2630251844528497		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 2.2630251844528497 | validation: 3.4431508601282226]
	TIME [epoch: 9.73 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261597148948364		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 2.261597148948364 | validation: 3.4344569098606543]
	TIME [epoch: 9.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2629395138197825		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 2.2629395138197825 | validation: 3.4286966062242747]
	TIME [epoch: 9.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265526045912323		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 2.265526045912323 | validation: 3.4382831527712785]
	TIME [epoch: 9.74 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265353065656351		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 2.265353065656351 | validation: 3.4373652216507367]
	TIME [epoch: 9.73 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.264766419131918		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 2.264766419131918 | validation: 3.4634415593364203]
	TIME [epoch: 9.72 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263083509422771		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 2.263083509422771 | validation: 3.4187099817640116]
	TIME [epoch: 9.73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2614486816438566		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 2.2614486816438566 | validation: 3.433861207647443]
	TIME [epoch: 9.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263667577720707		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 2.263667577720707 | validation: 3.4325319510799703]
	TIME [epoch: 9.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2602347281170223		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 2.2602347281170223 | validation: 3.4218285649440316]
	TIME [epoch: 9.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263252776821062		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 2.263252776821062 | validation: 3.436633483055977]
	TIME [epoch: 9.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2663864374606697		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 2.2663864374606697 | validation: 3.442091914413261]
	TIME [epoch: 9.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2645804367495996		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 2.2645804367495996 | validation: 3.432786899478988]
	TIME [epoch: 9.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261625744778594		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 2.261625744778594 | validation: 3.450395965932643]
	TIME [epoch: 9.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2760970238307303		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 2.2760970238307303 | validation: 3.4324755969913503]
	TIME [epoch: 9.73 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.282038524568589		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 2.282038524568589 | validation: 3.4554729729895075]
	TIME [epoch: 9.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2802981410671945		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 2.2802981410671945 | validation: 3.437320146627833]
	TIME [epoch: 9.72 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2613190677749415		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 2.2613190677749415 | validation: 3.4301698451679523]
	TIME [epoch: 9.71 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.264550531581792		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 2.264550531581792 | validation: 3.4216712572369223]
	TIME [epoch: 9.74 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2625777169687185		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 2.2625777169687185 | validation: 3.442600972419458]
	TIME [epoch: 9.71 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2962237376364483		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 2.2962237376364483 | validation: 3.4804001874363575]
	TIME [epoch: 9.71 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2637591219293416		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 2.2637591219293416 | validation: 3.4178144551774796]
	TIME [epoch: 9.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2560787623879164		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 2.2560787623879164 | validation: 3.4592813067987795]
	TIME [epoch: 9.74 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2667265337112767		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 2.2667265337112767 | validation: 3.432989458467263]
	TIME [epoch: 9.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2691412199320733		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 2.2691412199320733 | validation: 3.4340352012971667]
	TIME [epoch: 9.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257610602766399		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 2.257610602766399 | validation: 3.4369921519293714]
	TIME [epoch: 9.73 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2529413589683154		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 2.2529413589683154 | validation: 3.445843086536631]
	TIME [epoch: 9.72 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.267517513908539		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 2.267517513908539 | validation: 3.428044486529466]
	TIME [epoch: 9.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25522935577161		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 2.25522935577161 | validation: 3.415814839753316]
	TIME [epoch: 9.72 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.281434923086521		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 2.281434923086521 | validation: 3.4329157921411264]
	TIME [epoch: 9.74 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2662291558148695		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 2.2662291558148695 | validation: 3.4248532748074063]
	TIME [epoch: 9.71 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255789685323847		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 2.255789685323847 | validation: 3.4301780480376713]
	TIME [epoch: 9.72 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2535537808722497		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 2.2535537808722497 | validation: 3.4161755185858063]
	TIME [epoch: 9.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2666163432371915		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 2.2666163432371915 | validation: 3.4403794325348898]
	TIME [epoch: 9.73 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2778581543149787		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 2.2778581543149787 | validation: 3.421565705295651]
	TIME [epoch: 9.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525727124134973		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 2.2525727124134973 | validation: 3.4329905421581]
	TIME [epoch: 9.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25434289640319		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 2.25434289640319 | validation: 3.4330035474677425]
	TIME [epoch: 9.72 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2571872855770296		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 2.2571872855770296 | validation: 3.433415269799391]
	TIME [epoch: 9.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262067775851282		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 2.262067775851282 | validation: 3.434617648716462]
	TIME [epoch: 9.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2728884505837117		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 2.2728884505837117 | validation: 3.478785009505425]
	TIME [epoch: 9.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259485421841961		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 2.259485421841961 | validation: 3.4255283958894793]
	TIME [epoch: 9.72 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2771314213497726		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 2.2771314213497726 | validation: 3.4464099188145867]
	TIME [epoch: 9.71 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2635660751793933		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 2.2635660751793933 | validation: 3.433069334460541]
	TIME [epoch: 9.71 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273043185512671		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 2.273043185512671 | validation: 3.4327644964261586]
	TIME [epoch: 9.72 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2615906793320035		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 2.2615906793320035 | validation: 3.493319698643337]
	TIME [epoch: 9.73 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3115250643010286		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 2.3115250643010286 | validation: 3.4589994997920592]
	TIME [epoch: 9.71 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.268750427837318		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 2.268750427837318 | validation: 3.4277683852982848]
	TIME [epoch: 9.72 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2602133793545307		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 2.2602133793545307 | validation: 3.4221653653450854]
	TIME [epoch: 9.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260333724856704		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 2.260333724856704 | validation: 3.4303260053673443]
	TIME [epoch: 9.73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261674373965522		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 2.261674373965522 | validation: 3.4339308053091253]
	TIME [epoch: 9.71 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254533431455132		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 2.254533431455132 | validation: 3.421500255983491]
	TIME [epoch: 9.71 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25168224884109		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 2.25168224884109 | validation: 3.432191413986112]
	TIME [epoch: 9.73 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2561577875748284		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 2.2561577875748284 | validation: 3.4393685575643076]
	TIME [epoch: 9.72 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.274336273551719		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 2.274336273551719 | validation: 3.427668071567018]
	TIME [epoch: 9.71 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2543289682464325		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 2.2543289682464325 | validation: 3.4357345504307597]
	TIME [epoch: 9.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2568402787701105		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 2.2568402787701105 | validation: 3.436806096836107]
	TIME [epoch: 9.73 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2557194895401618		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 2.2557194895401618 | validation: 3.4293417661666705]
	TIME [epoch: 9.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260226482747022		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 2.260226482747022 | validation: 3.4378893567969397]
	TIME [epoch: 9.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2527588769418037		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 2.2527588769418037 | validation: 3.44216639876455]
	TIME [epoch: 9.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2634214507721206		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 2.2634214507721206 | validation: 3.421359970208419]
	TIME [epoch: 9.73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2682537712215387		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 2.2682537712215387 | validation: 3.432998788775791]
	TIME [epoch: 9.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260977672157197		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 2.260977672157197 | validation: 3.428982937890986]
	TIME [epoch: 9.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2529943192003437		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 2.2529943192003437 | validation: 3.431353598694085]
	TIME [epoch: 9.73 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25455034381672		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 2.25455034381672 | validation: 3.4344474583759865]
	TIME [epoch: 9.73 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269965812313879		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 2.269965812313879 | validation: 3.430020303969892]
	TIME [epoch: 9.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252746353717574		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 2.252746353717574 | validation: 3.4162601585086003]
	TIME [epoch: 9.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2621431017310356		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 2.2621431017310356 | validation: 3.427115718888618]
	TIME [epoch: 9.73 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25831804758739		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 2.25831804758739 | validation: 3.4436921504469273]
	TIME [epoch: 9.71 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2547203058508076		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 2.2547203058508076 | validation: 3.4203225212634947]
	TIME [epoch: 9.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256342436567947		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 2.256342436567947 | validation: 3.4265371581688]
	TIME [epoch: 9.71 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2559140701894287		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 2.2559140701894287 | validation: 3.430802756866406]
	TIME [epoch: 9.73 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2644679983346463		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 2.2644679983346463 | validation: 3.453880270819043]
	TIME [epoch: 9.71 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3321497542868284		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 2.3321497542868284 | validation: 3.430386875519348]
	TIME [epoch: 9.72 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258251348817972		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 2.258251348817972 | validation: 3.4264228442409475]
	TIME [epoch: 9.72 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249292673984261		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 2.249292673984261 | validation: 3.4401781192062675]
	TIME [epoch: 9.74 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258984617511367		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 2.258984617511367 | validation: 3.4610569716505064]
	TIME [epoch: 9.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2545123011872232		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 2.2545123011872232 | validation: 3.4252921191510994]
	TIME [epoch: 9.72 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2593118639121457		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 2.2593118639121457 | validation: 3.4238084238002724]
	TIME [epoch: 9.73 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2609974626966087		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 2.2609974626966087 | validation: 3.4249991427243516]
	TIME [epoch: 9.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2612458944847056		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 2.2612458944847056 | validation: 3.4113734327665566]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1165.pth
	Model improved!!!
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254557729843458		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 2.254557729843458 | validation: 3.4322438972291565]
	TIME [epoch: 9.72 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2575886294892706		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 2.2575886294892706 | validation: 3.4348967665106875]
	TIME [epoch: 9.73 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257726834627493		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 2.257726834627493 | validation: 3.464439615879151]
	TIME [epoch: 9.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2643171886361637		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 2.2643171886361637 | validation: 3.453821250427085]
	TIME [epoch: 9.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263026263975719		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 2.263026263975719 | validation: 3.4264475855914136]
	TIME [epoch: 9.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2649536407700666		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 2.2649536407700666 | validation: 3.428608816049687]
	TIME [epoch: 9.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2729602584246433		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 2.2729602584246433 | validation: 3.429818246957386]
	TIME [epoch: 9.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256671308751742		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 2.256671308751742 | validation: 3.427159209682234]
	TIME [epoch: 9.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256480150420824		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 2.256480150420824 | validation: 3.4245100105667006]
	TIME [epoch: 9.71 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261667204956794		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 2.261667204956794 | validation: 3.4247834473947596]
	TIME [epoch: 9.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2544129827046504		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 2.2544129827046504 | validation: 3.437268244215994]
	TIME [epoch: 9.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2635622029810594		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 2.2635622029810594 | validation: 3.428455797149218]
	TIME [epoch: 9.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260370274974468		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 2.260370274974468 | validation: 3.419799421804269]
	TIME [epoch: 9.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2629512292483924		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 2.2629512292483924 | validation: 3.4267459487673775]
	TIME [epoch: 9.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25689535762753		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 2.25689535762753 | validation: 3.4337356678555753]
	TIME [epoch: 9.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2770874647558657		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 2.2770874647558657 | validation: 3.4357388752231985]
	TIME [epoch: 9.71 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257656404358476		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 2.257656404358476 | validation: 3.453831369818079]
	TIME [epoch: 9.72 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2590253768324056		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 2.2590253768324056 | validation: 3.4263674134632014]
	TIME [epoch: 9.72 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.267228287176846		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 2.267228287176846 | validation: 3.429501649435713]
	TIME [epoch: 9.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.272130642583863		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 2.272130642583863 | validation: 3.4088663123861758]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1185.pth
	Model improved!!!
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260685119078596		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 2.260685119078596 | validation: 3.438406622820074]
	TIME [epoch: 9.74 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251568356213459		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 2.251568356213459 | validation: 3.4296272008031763]
	TIME [epoch: 9.71 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520367399897996		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 2.2520367399897996 | validation: 3.437562888510751]
	TIME [epoch: 9.72 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248667538203056		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 2.248667538203056 | validation: 3.438717295666115]
	TIME [epoch: 9.73 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2561425185725534		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 2.2561425185725534 | validation: 3.4366862384417702]
	TIME [epoch: 9.72 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2562680322444213		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 2.2562680322444213 | validation: 3.4308623733250716]
	TIME [epoch: 9.71 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2682979889549273		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 2.2682979889549273 | validation: 3.442146705530693]
	TIME [epoch: 9.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2576213709493604		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 2.2576213709493604 | validation: 3.4379124792179083]
	TIME [epoch: 9.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256622390410963		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 2.256622390410963 | validation: 3.4633129767202555]
	TIME [epoch: 9.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261247489692301		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 2.261247489692301 | validation: 3.4386950487228307]
	TIME [epoch: 9.71 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.27631733852047		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 2.27631733852047 | validation: 3.4569741159251954]
	TIME [epoch: 9.72 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273320939248341		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 2.273320939248341 | validation: 3.4303400486326403]
	TIME [epoch: 9.75 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2623454400499994		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 2.2623454400499994 | validation: 3.456675054802561]
	TIME [epoch: 9.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2621490234237718		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 2.2621490234237718 | validation: 3.424511866393707]
	TIME [epoch: 9.72 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2595221821373626		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 2.2595221821373626 | validation: 3.4288015257987614]
	TIME [epoch: 9.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260560176316258		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 2.260560176316258 | validation: 3.4287118689476035]
	TIME [epoch: 9.73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265144562837609		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 2.265144562837609 | validation: 3.4414032681965194]
	TIME [epoch: 9.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261201747732353		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 2.261201747732353 | validation: 3.4305392717593475]
	TIME [epoch: 9.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258570716566742		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 2.258570716566742 | validation: 3.4343445410524613]
	TIME [epoch: 9.74 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2467865298673844		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 2.2467865298673844 | validation: 3.4222377224793794]
	TIME [epoch: 9.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252881705181079		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 2.252881705181079 | validation: 3.441095262730361]
	TIME [epoch: 9.72 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606932354889784		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 2.2606932354889784 | validation: 3.4269539114341683]
	TIME [epoch: 9.72 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2550814505784325		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 2.2550814505784325 | validation: 3.436424874525195]
	TIME [epoch: 9.75 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256042069567401		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 2.256042069567401 | validation: 3.445179216754074]
	TIME [epoch: 9.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2575697996358284		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 2.2575697996358284 | validation: 3.4384387092794713]
	TIME [epoch: 9.72 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2590580526487374		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 2.2590580526487374 | validation: 3.4459331550639507]
	TIME [epoch: 9.73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516177592759306		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 2.2516177592759306 | validation: 3.438130572957302]
	TIME [epoch: 9.74 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257823076514728		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 2.257823076514728 | validation: 3.4465687392407993]
	TIME [epoch: 9.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254637990256337		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 2.254637990256337 | validation: 3.429983006211096]
	TIME [epoch: 9.72 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2503495454698266		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 2.2503495454698266 | validation: 3.427892417194867]
	TIME [epoch: 9.74 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2621944931470943		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 2.2621944931470943 | validation: 3.415346005015251]
	TIME [epoch: 9.73 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252164791224243		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 2.252164791224243 | validation: 3.450829875097592]
	TIME [epoch: 9.72 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2594489043117063		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 2.2594489043117063 | validation: 3.4486420737921146]
	TIME [epoch: 9.71 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25758113377277		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 2.25758113377277 | validation: 3.435746911805]
	TIME [epoch: 9.74 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248113272375784		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 2.248113272375784 | validation: 3.4292078906322856]
	TIME [epoch: 9.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2502977025647417		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 2.2502977025647417 | validation: 3.4326511761872487]
	TIME [epoch: 9.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2472110751645222		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 2.2472110751645222 | validation: 3.4322900958709956]
	TIME [epoch: 9.71 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249007224084587		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 2.249007224084587 | validation: 3.4228809952107255]
	TIME [epoch: 9.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2551341670401337		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 2.2551341670401337 | validation: 3.4245090450528997]
	TIME [epoch: 9.71 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263287098665376		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 2.263287098665376 | validation: 3.4268742595311203]
	TIME [epoch: 9.72 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2638506764175226		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 2.2638506764175226 | validation: 3.430355969301203]
	TIME [epoch: 9.73 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2527931404623347		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 2.2527931404623347 | validation: 3.4339058774908544]
	TIME [epoch: 9.72 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256720193946497		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 2.256720193946497 | validation: 3.41720598734138]
	TIME [epoch: 9.72 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252141642796701		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 2.252141642796701 | validation: 3.438274697967285]
	TIME [epoch: 9.71 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2596349960476054		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 2.2596349960476054 | validation: 3.430382360463069]
	TIME [epoch: 9.74 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2690837939567214		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 2.2690837939567214 | validation: 3.4350125679070844]
	TIME [epoch: 9.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2571203019120056		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 2.2571203019120056 | validation: 3.4306818286950613]
	TIME [epoch: 9.71 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252958475420702		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 2.252958475420702 | validation: 3.4335139984068292]
	TIME [epoch: 9.72 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2592883775159427		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 2.2592883775159427 | validation: 3.4348349010329473]
	TIME [epoch: 9.74 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512541888636983		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 2.2512541888636983 | validation: 3.415832829863776]
	TIME [epoch: 9.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2542363901510605		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 2.2542363901510605 | validation: 3.416377139704961]
	TIME [epoch: 9.71 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248158931996243		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 2.248158931996243 | validation: 3.442866397022692]
	TIME [epoch: 9.72 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2569816225919452		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 2.2569816225919452 | validation: 3.424099309655766]
	TIME [epoch: 9.73 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250363708993434		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 2.250363708993434 | validation: 3.408628746751059]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1239.pth
	Model improved!!!
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246485448734835		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 2.246485448734835 | validation: 3.4182616646852195]
	TIME [epoch: 9.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2550126014733323		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 2.2550126014733323 | validation: 3.450114132441238]
	TIME [epoch: 9.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256862518635478		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 2.256862518635478 | validation: 3.4221732645110525]
	TIME [epoch: 9.72 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256625639074575		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 2.256625639074575 | validation: 3.4403241664105373]
	TIME [epoch: 9.71 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270635815082703		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 2.270635815082703 | validation: 3.4330432827968367]
	TIME [epoch: 9.71 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2573348493840464		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 2.2573348493840464 | validation: 3.426155914772676]
	TIME [epoch: 9.73 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257753117333306		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 2.257753117333306 | validation: 3.4470998505473354]
	TIME [epoch: 9.72 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2648746702787728		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 2.2648746702787728 | validation: 3.4498539649715076]
	TIME [epoch: 9.71 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263733493591064		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 2.263733493591064 | validation: 3.4384889272424024]
	TIME [epoch: 9.73 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2501126973496577		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 2.2501126973496577 | validation: 3.422105967435075]
	TIME [epoch: 9.73 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246979455737383		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 2.246979455737383 | validation: 3.442160607024501]
	TIME [epoch: 9.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2622871564642217		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 2.2622871564642217 | validation: 3.4398992117241436]
	TIME [epoch: 9.72 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249558196201155		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 2.249558196201155 | validation: 3.444340850541405]
	TIME [epoch: 9.73 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261475257511191		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 2.261475257511191 | validation: 3.4397717796986003]
	TIME [epoch: 9.72 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2507006865974493		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 2.2507006865974493 | validation: 3.4261607080384215]
	TIME [epoch: 9.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252348432051681		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 2.252348432051681 | validation: 3.4355374146131874]
	TIME [epoch: 9.72 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261775238699956		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 2.261775238699956 | validation: 3.439229403524413]
	TIME [epoch: 9.73 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252889675606291		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 2.252889675606291 | validation: 3.423147847751023]
	TIME [epoch: 9.72 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254893804723868		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 2.254893804723868 | validation: 3.4207935173995416]
	TIME [epoch: 9.71 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2527904711470823		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 2.2527904711470823 | validation: 3.4181990244305274]
	TIME [epoch: 9.71 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490739822219132		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 2.2490739822219132 | validation: 3.419059896712046]
	TIME [epoch: 9.73 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2496137008695754		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 2.2496137008695754 | validation: 3.440625019400102]
	TIME [epoch: 9.72 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2491564186189743		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 2.2491564186189743 | validation: 3.4191980496944328]
	TIME [epoch: 9.72 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2612983002708567		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 2.2612983002708567 | validation: 3.447156008220029]
	TIME [epoch: 9.72 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2579073442687783		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 2.2579073442687783 | validation: 3.4228089044484076]
	TIME [epoch: 9.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260867174223678		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 2.260867174223678 | validation: 3.4342969900705]
	TIME [epoch: 9.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254806320202209		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 2.254806320202209 | validation: 3.4339784876590613]
	TIME [epoch: 9.71 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25845477324005		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 2.25845477324005 | validation: 3.434985622357544]
	TIME [epoch: 9.73 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.276822705240903		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 2.276822705240903 | validation: 3.4371276421086896]
	TIME [epoch: 9.73 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253508685521738		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 2.253508685521738 | validation: 3.437096733009984]
	TIME [epoch: 9.71 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2547452540074175		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 2.2547452540074175 | validation: 3.4292100154742187]
	TIME [epoch: 9.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252896532115968		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 2.252896532115968 | validation: 3.4463329431751313]
	TIME [epoch: 9.73 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258189587506057		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 2.258189587506057 | validation: 3.417509059040977]
	TIME [epoch: 9.71 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2523467247318747		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 2.2523467247318747 | validation: 3.4428762268111894]
	TIME [epoch: 9.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2653426416148377		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 2.2653426416148377 | validation: 3.433881738943431]
	TIME [epoch: 9.73 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2566966192817453		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 2.2566966192817453 | validation: 3.4273094353674187]
	TIME [epoch: 9.73 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2499053146111976		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 2.2499053146111976 | validation: 3.436827260772668]
	TIME [epoch: 9.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250022086309779		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 2.250022086309779 | validation: 3.43010079393818]
	TIME [epoch: 9.72 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25469438122307		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 2.25469438122307 | validation: 3.4196113724781982]
	TIME [epoch: 9.74 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2532123843038483		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 2.2532123843038483 | validation: 3.446135760620959]
	TIME [epoch: 9.72 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257517132043561		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 2.257517132043561 | validation: 3.4402810058099598]
	TIME [epoch: 9.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251366225703823		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 2.251366225703823 | validation: 3.4277505303552864]
	TIME [epoch: 9.71 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25450422536552		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 2.25450422536552 | validation: 3.426021836102307]
	TIME [epoch: 9.74 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25428568415103		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 2.25428568415103 | validation: 3.4273551830456466]
	TIME [epoch: 9.71 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2523294385084593		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 2.2523294385084593 | validation: 3.427598885050104]
	TIME [epoch: 9.72 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2545708467085577		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 2.2545708467085577 | validation: 3.424345965932649]
	TIME [epoch: 9.72 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251800267426104		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 2.251800267426104 | validation: 3.4247545308468883]
	TIME [epoch: 9.74 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2543954112636584		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 2.2543954112636584 | validation: 3.420746416959286]
	TIME [epoch: 9.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257244548813043		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 2.257244548813043 | validation: 3.4334624904379196]
	TIME [epoch: 9.72 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2486869899783706		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 2.2486869899783706 | validation: 3.439484767569403]
	TIME [epoch: 9.72 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2541738670032707		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 2.2541738670032707 | validation: 3.4383767592179275]
	TIME [epoch: 9.72 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525942152392986		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 2.2525942152392986 | validation: 3.4209735436721105]
	TIME [epoch: 9.71 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2463982396323976		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 2.2463982396323976 | validation: 3.419981905124828]
	TIME [epoch: 9.71 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24892187436886		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 2.24892187436886 | validation: 3.4229799203009237]
	TIME [epoch: 9.73 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2521131843684197		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 2.2521131843684197 | validation: 3.42344479597625]
	TIME [epoch: 9.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247682547669583		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 2.247682547669583 | validation: 3.436584975519544]
	TIME [epoch: 9.71 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250606259794334		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 2.250606259794334 | validation: 3.442759920905811]
	TIME [epoch: 9.72 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263618055352635		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 2.263618055352635 | validation: 3.4159141826025694]
	TIME [epoch: 9.73 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2469226746277458		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 2.2469226746277458 | validation: 3.4193648851802574]
	TIME [epoch: 9.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257364688462461		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 2.257364688462461 | validation: 3.4211836679493968]
	TIME [epoch: 9.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2610955690138113		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 2.2610955690138113 | validation: 3.4158038922022413]
	TIME [epoch: 9.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2537531812126064		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 2.2537531812126064 | validation: 3.4895966093338373]
	TIME [epoch: 9.73 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.276433516321986		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 2.276433516321986 | validation: 3.4448723082402677]
	TIME [epoch: 9.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25940386924488		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 2.25940386924488 | validation: 3.445231715969876]
	TIME [epoch: 9.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248209328397912		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 2.248209328397912 | validation: 3.429106617908849]
	TIME [epoch: 9.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2461227095436125		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 2.2461227095436125 | validation: 3.4271591046277106]
	TIME [epoch: 9.71 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2468241125549686		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 2.2468241125549686 | validation: 3.428014397098163]
	TIME [epoch: 9.71 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2561126457508314		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 2.2561126457508314 | validation: 3.434360576686198]
	TIME [epoch: 9.75 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257774478534342		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 2.257774478534342 | validation: 3.428429591917606]
	TIME [epoch: 9.74 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520216818242984		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 2.2520216818242984 | validation: 3.435584787594513]
	TIME [epoch: 9.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2653553179456694		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 2.2653553179456694 | validation: 3.4218297080687905]
	TIME [epoch: 9.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525603052063117		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 2.2525603052063117 | validation: 3.4320491023792346]
	TIME [epoch: 9.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2510485525785526		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 2.2510485525785526 | validation: 3.435325512059202]
	TIME [epoch: 9.73 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25730545802992		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 2.25730545802992 | validation: 3.422016919642347]
	TIME [epoch: 9.71 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2510301604082756		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 2.2510301604082756 | validation: 3.4168485308591743]
	TIME [epoch: 9.71 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248163308443112		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 2.248163308443112 | validation: 3.4323172850039603]
	TIME [epoch: 9.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2497984115651115		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 2.2497984115651115 | validation: 3.4139342444321246]
	TIME [epoch: 9.71 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247749379162291		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 2.247749379162291 | validation: 3.42703629137299]
	TIME [epoch: 9.71 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250268711618799		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 2.250268711618799 | validation: 3.4184082530040882]
	TIME [epoch: 9.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249421661867521		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 2.249421661867521 | validation: 3.4307907498914143]
	TIME [epoch: 9.73 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2502818559655124		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 2.2502818559655124 | validation: 3.4208373642561867]
	TIME [epoch: 9.72 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2548462341576525		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 2.2548462341576525 | validation: 3.4275875629205683]
	TIME [epoch: 9.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483296845366545		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 2.2483296845366545 | validation: 3.4236800814733663]
	TIME [epoch: 9.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254587422561517		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 2.254587422561517 | validation: 3.4245683095074013]
	TIME [epoch: 9.74 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244307583764983		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 2.244307583764983 | validation: 3.440619571516138]
	TIME [epoch: 9.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253581128398556		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 2.253581128398556 | validation: 3.4205274212401595]
	TIME [epoch: 9.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248227893576815		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 2.248227893576815 | validation: 3.443425533237994]
	TIME [epoch: 9.71 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252293332200323		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 2.252293332200323 | validation: 3.422593308787197]
	TIME [epoch: 9.73 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246275726431907		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 2.246275726431907 | validation: 3.4204742796735275]
	TIME [epoch: 9.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2588837279464835		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 2.2588837279464835 | validation: 3.4186052659245694]
	TIME [epoch: 9.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2603912197300424		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 2.2603912197300424 | validation: 3.4318148960761508]
	TIME [epoch: 9.73 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259377002784862		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 2.259377002784862 | validation: 3.428014966894383]
	TIME [epoch: 9.71 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433700150238427		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 2.2433700150238427 | validation: 3.431428514019013]
	TIME [epoch: 9.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250981972662423		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 2.250981972662423 | validation: 3.4280345277034248]
	TIME [epoch: 9.71 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2528154541567273		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 2.2528154541567273 | validation: 3.445428506326669]
	TIME [epoch: 9.74 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2540855023129858		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 2.2540855023129858 | validation: 3.4167372848867172]
	TIME [epoch: 9.71 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2470402653459542		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 2.2470402653459542 | validation: 3.419652890704337]
	TIME [epoch: 9.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252967169314501		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 2.252967169314501 | validation: 3.4086538811423894]
	TIME [epoch: 9.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246813798211786		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 2.246813798211786 | validation: 3.4234570491900382]
	TIME [epoch: 9.73 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2517288580505324		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 2.2517288580505324 | validation: 3.4195575554550146]
	TIME [epoch: 9.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250610379254418		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 2.250610379254418 | validation: 3.4265366586268655]
	TIME [epoch: 9.71 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2510348062810506		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 2.2510348062810506 | validation: 3.4293711893764685]
	TIME [epoch: 9.72 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250184942392754		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 2.250184942392754 | validation: 3.4170478175696912]
	TIME [epoch: 9.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520413302487077		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 2.2520413302487077 | validation: 3.421811889095965]
	TIME [epoch: 9.71 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2528671299309706		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 2.2528671299309706 | validation: 3.434893413253629]
	TIME [epoch: 9.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257923743343036		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 2.257923743343036 | validation: 3.4477017929813396]
	TIME [epoch: 9.73 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260456905406996		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 2.260456905406996 | validation: 3.4205991026638323]
	TIME [epoch: 9.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253683416053666		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 2.253683416053666 | validation: 3.4253292386431484]
	TIME [epoch: 9.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2601561399922194		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 2.2601561399922194 | validation: 3.4365910228973338]
	TIME [epoch: 9.71 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2585313912591434		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 2.2585313912591434 | validation: 3.43258160155891]
	TIME [epoch: 9.73 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2570023864206052		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 2.2570023864206052 | validation: 3.4286695984850803]
	TIME [epoch: 9.71 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606696282830825		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 2.2606696282830825 | validation: 3.4183906817985794]
	TIME [epoch: 9.72 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246272740559731		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 2.246272740559731 | validation: 3.433502977969147]
	TIME [epoch: 9.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25491532493636		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 2.25491532493636 | validation: 3.420919191531616]
	TIME [epoch: 9.72 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434992105624993		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 2.2434992105624993 | validation: 3.4301147194569133]
	TIME [epoch: 9.71 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2547760281946507		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 2.2547760281946507 | validation: 3.44525783666548]
	TIME [epoch: 9.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2527313109079894		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 2.2527313109079894 | validation: 3.42825674834469]
	TIME [epoch: 9.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2541796785523		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 2.2541796785523 | validation: 3.408986914348796]
	TIME [epoch: 9.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2546754610588735		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 2.2546754610588735 | validation: 3.4212774298913486]
	TIME [epoch: 9.72 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250259859981493		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 2.250259859981493 | validation: 3.409000268276289]
	TIME [epoch: 9.72 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246591364743451		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 2.246591364743451 | validation: 3.4252784803208853]
	TIME [epoch: 9.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456834041838327		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 2.2456834041838327 | validation: 3.437496289932226]
	TIME [epoch: 9.71 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2507640873165102		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 2.2507640873165102 | validation: 3.4260289499270984]
	TIME [epoch: 9.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252561699407008		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 2.252561699407008 | validation: 3.448288243901055]
	TIME [epoch: 9.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263276850445778		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 2.263276850445778 | validation: 3.4484152738608245]
	TIME [epoch: 9.73 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262780831291651		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 2.262780831291651 | validation: 3.4186860677276307]
	TIME [epoch: 9.71 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520917109364427		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 2.2520917109364427 | validation: 3.4224806865520714]
	TIME [epoch: 9.71 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249951251867153		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 2.249951251867153 | validation: 3.417789001316306]
	TIME [epoch: 9.72 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253616923802943		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 2.253616923802943 | validation: 3.4527442149956435]
	TIME [epoch: 9.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270251267979417		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 2.270251267979417 | validation: 3.44338688815667]
	TIME [epoch: 9.7 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2500862702558893		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 2.2500862702558893 | validation: 3.415680463738472]
	TIME [epoch: 9.72 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2458946724802202		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 2.2458946724802202 | validation: 3.4093678256168607]
	TIME [epoch: 9.73 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2553023460039383		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 2.2553023460039383 | validation: 3.429007796345642]
	TIME [epoch: 9.72 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2528551258533085		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 2.2528551258533085 | validation: 3.4282871545962776]
	TIME [epoch: 9.71 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249389916074977		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 2.249389916074977 | validation: 3.4184113725158354]
	TIME [epoch: 9.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2481204768311898		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 2.2481204768311898 | validation: 3.4254132374044968]
	TIME [epoch: 9.73 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245672963939991		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 2.245672963939991 | validation: 3.4414537679467982]
	TIME [epoch: 9.71 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25793413344629		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 2.25793413344629 | validation: 3.454125906792801]
	TIME [epoch: 9.71 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2659146949429805		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 2.2659146949429805 | validation: 3.443480319514957]
	TIME [epoch: 9.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2531495897081166		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 2.2531495897081166 | validation: 3.448628349858124]
	TIME [epoch: 9.73 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2637498620166943		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 2.2637498620166943 | validation: 3.4593893310884627]
	TIME [epoch: 9.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262710319126138		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 2.262710319126138 | validation: 3.4341629741904844]
	TIME [epoch: 9.71 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245659597706708		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 2.245659597706708 | validation: 3.4183594030469204]
	TIME [epoch: 9.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2576347653351623		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 2.2576347653351623 | validation: 3.4429970134296926]
	TIME [epoch: 9.71 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246119761062978		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 2.246119761062978 | validation: 3.4214362269165814]
	TIME [epoch: 9.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25157421362973		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 2.25157421362973 | validation: 3.4195554978354745]
	TIME [epoch: 9.71 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250094529471455		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 2.250094529471455 | validation: 3.424699544593846]
	TIME [epoch: 9.74 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248847850686885		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 2.248847850686885 | validation: 3.437148893730945]
	TIME [epoch: 9.71 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256341068961949		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 2.256341068961949 | validation: 3.417832566504602]
	TIME [epoch: 9.72 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2670570485928003		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 2.2670570485928003 | validation: 3.423006828789309]
	TIME [epoch: 9.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2670520603825675		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 2.2670520603825675 | validation: 3.427085936927441]
	TIME [epoch: 9.73 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261356879377481		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 2.261356879377481 | validation: 3.420327259730399]
	TIME [epoch: 9.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253990071831143		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 2.253990071831143 | validation: 3.4160904480078784]
	TIME [epoch: 9.71 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2543223725977874		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 2.2543223725977874 | validation: 3.429038621947764]
	TIME [epoch: 9.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247671956022799		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 2.247671956022799 | validation: 3.427689169961928]
	TIME [epoch: 9.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24454441324824		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 2.24454441324824 | validation: 3.4359287933395293]
	TIME [epoch: 9.71 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2469804314940918		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 2.2469804314940918 | validation: 3.4203761778220656]
	TIME [epoch: 9.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479097395734313		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 2.2479097395734313 | validation: 3.426312465777644]
	TIME [epoch: 9.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2564302167596084		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 2.2564302167596084 | validation: 3.452226292928839]
	TIME [epoch: 9.72 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254071455851558		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 2.254071455851558 | validation: 3.4469464902634424]
	TIME [epoch: 9.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2510865644807336		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 2.2510865644807336 | validation: 3.422971248973811]
	TIME [epoch: 9.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243863451344875		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 2.243863451344875 | validation: 3.430426321145039]
	TIME [epoch: 9.72 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2518462483795556		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 2.2518462483795556 | validation: 3.4234641944778264]
	TIME [epoch: 9.71 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498510279891133		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 2.2498510279891133 | validation: 3.4287863530035354]
	TIME [epoch: 9.72 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2493341324526006		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 2.2493341324526006 | validation: 3.421503039393012]
	TIME [epoch: 9.73 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24213072718356		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 2.24213072718356 | validation: 3.4333852244301886]
	TIME [epoch: 9.73 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2494994125661614		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 2.2494994125661614 | validation: 3.423325990149519]
	TIME [epoch: 9.72 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251371223412325		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 2.251371223412325 | validation: 3.4261893857085886]
	TIME [epoch: 9.71 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2491350494920477		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 2.2491350494920477 | validation: 3.435628427173459]
	TIME [epoch: 9.73 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247802533760316		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 2.247802533760316 | validation: 3.423154718936662]
	TIME [epoch: 9.72 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2426716472508605		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 2.2426716472508605 | validation: 3.414966131220505]
	TIME [epoch: 9.71 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251127801315815		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 2.251127801315815 | validation: 3.4233928260590267]
	TIME [epoch: 9.72 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476774044758714		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 2.2476774044758714 | validation: 3.4353355708214552]
	TIME [epoch: 9.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2504515477056075		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 2.2504515477056075 | validation: 3.4198768723327224]
	TIME [epoch: 9.71 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2508411745005574		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 2.2508411745005574 | validation: 3.4201858272964465]
	TIME [epoch: 9.71 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2492744307225343		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 2.2492744307225343 | validation: 3.427139920747211]
	TIME [epoch: 9.71 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448758665872526		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 2.2448758665872526 | validation: 3.418371795543495]
	TIME [epoch: 9.72 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2451983105459425		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 2.2451983105459425 | validation: 3.43824202876281]
	TIME [epoch: 9.72 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525691760599242		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 2.2525691760599242 | validation: 3.4347597650412216]
	TIME [epoch: 9.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251394641092682		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 2.251394641092682 | validation: 3.415822609472595]
	TIME [epoch: 9.73 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2521038393293993		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 2.2521038393293993 | validation: 3.424027870380109]
	TIME [epoch: 9.72 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2555619568255088		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 2.2555619568255088 | validation: 3.4065086997792653]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1421.pth
	Model improved!!!
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2499741551291956		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 2.2499741551291956 | validation: 3.4330989138968016]
	TIME [epoch: 9.72 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256602023386362		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 2.256602023386362 | validation: 3.4391033940528155]
	TIME [epoch: 9.74 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520900247904665		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 2.2520900247904665 | validation: 3.4325356349996103]
	TIME [epoch: 9.73 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2496728115020956		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 2.2496728115020956 | validation: 3.4171120477916985]
	TIME [epoch: 9.73 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2540791466566876		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 2.2540791466566876 | validation: 3.453238092212374]
	TIME [epoch: 9.73 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516494586572797		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 2.2516494586572797 | validation: 3.4258914795138145]
	TIME [epoch: 9.74 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2495705090208653		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 2.2495705090208653 | validation: 3.4159678668689244]
	TIME [epoch: 9.72 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256462099098484		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 2.256462099098484 | validation: 3.4187360136910447]
	TIME [epoch: 9.71 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2554445042674516		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 2.2554445042674516 | validation: 3.4141451288211404]
	TIME [epoch: 9.71 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2500716666291893		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 2.2500716666291893 | validation: 3.4296294485282206]
	TIME [epoch: 9.73 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244575040727015		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 2.244575040727015 | validation: 3.426645431583641]
	TIME [epoch: 9.72 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24902803493638		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 2.24902803493638 | validation: 3.4195936152873703]
	TIME [epoch: 9.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2517420751001356		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 2.2517420751001356 | validation: 3.4229252644285943]
	TIME [epoch: 9.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2520720675518833		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 2.2520720675518833 | validation: 3.421574588846602]
	TIME [epoch: 9.71 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2497081277164335		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 2.2497081277164335 | validation: 3.417404201084167]
	TIME [epoch: 9.72 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2477275413103976		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 2.2477275413103976 | validation: 3.436768829617433]
	TIME [epoch: 9.72 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2547239918211885		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 2.2547239918211885 | validation: 3.426418346904317]
	TIME [epoch: 9.74 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2500714712966214		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 2.2500714712966214 | validation: 3.4141885421611526]
	TIME [epoch: 9.72 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253210785886638		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 2.253210785886638 | validation: 3.4248416898511387]
	TIME [epoch: 9.73 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2502157168602848		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 2.2502157168602848 | validation: 3.440509784815933]
	TIME [epoch: 9.72 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2472338321388508		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 2.2472338321388508 | validation: 3.4338111832145914]
	TIME [epoch: 9.74 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2514488200979943		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 2.2514488200979943 | validation: 3.4328255538484123]
	TIME [epoch: 9.71 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2511816610089763		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 2.2511816610089763 | validation: 3.437808850315428]
	TIME [epoch: 9.72 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2472583229272054		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 2.2472583229272054 | validation: 3.4225897875285556]
	TIME [epoch: 9.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2474197436859717		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 2.2474197436859717 | validation: 3.418672814594655]
	TIME [epoch: 9.73 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248613457494357		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 2.248613457494357 | validation: 3.4181166317589793]
	TIME [epoch: 9.72 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2444562775995953		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 2.2444562775995953 | validation: 3.4399018714381064]
	TIME [epoch: 9.71 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252741409522961		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 2.252741409522961 | validation: 3.418079987862581]
	TIME [epoch: 9.73 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2546267152553097		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 2.2546267152553097 | validation: 3.445308856105477]
	TIME [epoch: 9.71 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2487957859698993		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 2.2487957859698993 | validation: 3.433036002992983]
	TIME [epoch: 9.71 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2537282208772518		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 2.2537282208772518 | validation: 3.43253258154582]
	TIME [epoch: 9.72 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2487629145973465		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 2.2487629145973465 | validation: 3.4137285130309354]
	TIME [epoch: 9.73 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25479776001037		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 2.25479776001037 | validation: 3.4282665381350252]
	TIME [epoch: 9.72 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2519937320461323		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 2.2519937320461323 | validation: 3.4134758341721763]
	TIME [epoch: 9.72 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2440320457732823		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 2.2440320457732823 | validation: 3.4408494555402074]
	TIME [epoch: 9.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246208121841396		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 2.246208121841396 | validation: 3.4340038076908774]
	TIME [epoch: 9.72 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476639298254266		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 2.2476639298254266 | validation: 3.4348226060603553]
	TIME [epoch: 9.71 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516572154036183		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 2.2516572154036183 | validation: 3.4392992803384064]
	TIME [epoch: 9.71 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246603947207315		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 2.246603947207315 | validation: 3.4162942388715476]
	TIME [epoch: 9.73 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2467471928377334		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 2.2467471928377334 | validation: 3.4406826922649136]
	TIME [epoch: 9.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606739010388646		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 2.2606739010388646 | validation: 3.460561489941795]
	TIME [epoch: 9.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2567879586070374		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 2.2567879586070374 | validation: 3.442568869152258]
	TIME [epoch: 9.72 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248428704065012		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 2.248428704065012 | validation: 3.427234932537542]
	TIME [epoch: 9.72 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2511276705369405		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 2.2511276705369405 | validation: 3.423365224288531]
	TIME [epoch: 9.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2515351919018647		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 2.2515351919018647 | validation: 3.4335064579112338]
	TIME [epoch: 9.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250347947384912		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 2.250347947384912 | validation: 3.43157265787307]
	TIME [epoch: 9.72 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2489544647512814		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 2.2489544647512814 | validation: 3.441145086211261]
	TIME [epoch: 9.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2491570205355904		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 2.2491570205355904 | validation: 3.435622951934704]
	TIME [epoch: 9.72 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24851051328754		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 2.24851051328754 | validation: 3.4296979432058357]
	TIME [epoch: 9.72 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2493342105668988		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 2.2493342105668988 | validation: 3.416211695723412]
	TIME [epoch: 9.73 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498860297356016		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 2.2498860297356016 | validation: 3.423053254085121]
	TIME [epoch: 9.71 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2481483043475956		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 2.2481483043475956 | validation: 3.4173202347675944]
	TIME [epoch: 9.71 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445065893652614		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 2.2445065893652614 | validation: 3.4256479745146073]
	TIME [epoch: 9.71 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2499810416238004		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 2.2499810416238004 | validation: 3.4281052704921446]
	TIME [epoch: 9.73 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247920267049878		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 2.247920267049878 | validation: 3.4442983286983093]
	TIME [epoch: 9.7 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250251685299273		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 2.250251685299273 | validation: 3.4237972647238086]
	TIME [epoch: 9.72 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24613932598155		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 2.24613932598155 | validation: 3.431120169687673]
	TIME [epoch: 9.7 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2484035291981153		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 2.2484035291981153 | validation: 3.4290358940614114]
	TIME [epoch: 9.73 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473084351940527		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 2.2473084351940527 | validation: 3.4249568663275625]
	TIME [epoch: 9.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445869860562024		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 2.2445869860562024 | validation: 3.437163087769135]
	TIME [epoch: 9.7 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247592456731708		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 2.247592456731708 | validation: 3.438379917151307]
	TIME [epoch: 9.72 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246107554925183		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 2.246107554925183 | validation: 3.4382366167906877]
	TIME [epoch: 9.71 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248980531091668		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 2.248980531091668 | validation: 3.425511370184911]
	TIME [epoch: 9.71 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2544191034048726		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 2.2544191034048726 | validation: 3.4339883352505134]
	TIME [epoch: 9.7 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2453029892776235		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 2.2453029892776235 | validation: 3.432187486945526]
	TIME [epoch: 9.72 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512652000816686		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 2.2512652000816686 | validation: 3.4070688837659002]
	TIME [epoch: 9.72 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250154381492167		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 2.250154381492167 | validation: 3.4189781565912676]
	TIME [epoch: 9.71 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244570267628304		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 2.244570267628304 | validation: 3.4239374803462277]
	TIME [epoch: 9.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498213808022336		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 2.2498213808022336 | validation: 3.4270964572680613]
	TIME [epoch: 9.73 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498421498805095		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 2.2498421498805095 | validation: 3.4374987702129443]
	TIME [epoch: 9.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2522920470296564		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 2.2522920470296564 | validation: 3.4106889442984203]
	TIME [epoch: 9.71 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247090305758335		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 2.247090305758335 | validation: 3.420735927518663]
	TIME [epoch: 9.71 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2446324502027415		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 2.2446324502027415 | validation: 3.425966781679781]
	TIME [epoch: 9.73 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2555825193938093		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 2.2555825193938093 | validation: 3.4368889406992857]
	TIME [epoch: 9.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2620161145354807		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 2.2620161145354807 | validation: 3.4298446484507155]
	TIME [epoch: 9.71 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2424970791716414		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 2.2424970791716414 | validation: 3.4309436454864612]
	TIME [epoch: 9.73 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2465511928766686		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 2.2465511928766686 | validation: 3.4274164174312]
	TIME [epoch: 9.72 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2477140726027307		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 2.2477140726027307 | validation: 3.4161740206978903]
	TIME [epoch: 9.71 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512364895540165		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 2.2512364895540165 | validation: 3.4192000448637807]
	TIME [epoch: 9.71 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249467884252306		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 2.249467884252306 | validation: 3.428635629940653]
	TIME [epoch: 9.72 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2522736296285455		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 2.2522736296285455 | validation: 3.4533271642295644]
	TIME [epoch: 9.71 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512885007851917		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 2.2512885007851917 | validation: 3.4356306792937983]
	TIME [epoch: 9.7 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24416671862813		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 2.24416671862813 | validation: 3.4357744259483547]
	TIME [epoch: 9.71 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2610512250206516		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 2.2610512250206516 | validation: 3.4303635279687503]
	TIME [epoch: 9.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247126947352307		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 2.247126947352307 | validation: 3.430639959603772]
	TIME [epoch: 9.72 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246832404953668		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 2.246832404953668 | validation: 3.444174476004007]
	TIME [epoch: 9.72 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241520095439441		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 2.241520095439441 | validation: 3.4276039808462624]
	TIME [epoch: 9.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243879242063712		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 2.243879242063712 | validation: 3.4315561950250153]
	TIME [epoch: 9.72 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448704235539645		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 2.2448704235539645 | validation: 3.4331305128920047]
	TIME [epoch: 9.72 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2418983035937563		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 2.2418983035937563 | validation: 3.4258949772941714]
	TIME [epoch: 9.72 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246317547486364		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 2.246317547486364 | validation: 3.4219801299695325]
	TIME [epoch: 9.72 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241099965812982		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 2.241099965812982 | validation: 3.4103998852270667]
	TIME [epoch: 9.71 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473010939195137		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 2.2473010939195137 | validation: 3.419889280647716]
	TIME [epoch: 9.7 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516948484480976		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 2.2516948484480976 | validation: 3.4252791486860383]
	TIME [epoch: 9.71 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2507149756487737		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 2.2507149756487737 | validation: 3.4367813310134863]
	TIME [epoch: 9.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248535750518511		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 2.248535750518511 | validation: 3.4337723760044674]
	TIME [epoch: 9.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2539774021431365		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 2.2539774021431365 | validation: 3.4207070166092457]
	TIME [epoch: 9.71 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2562410050591213		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 2.2562410050591213 | validation: 3.4244498569622794]
	TIME [epoch: 9.71 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2529960600359997		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 2.2529960600359997 | validation: 3.4239210093145283]
	TIME [epoch: 9.73 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24945807459781		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 2.24945807459781 | validation: 3.4064962663387646]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1521.pth
	Model improved!!!
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2529891209725563		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 2.2529891209725563 | validation: 3.429626050044891]
	TIME [epoch: 9.7 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2567681623744833		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 2.2567681623744833 | validation: 3.435342311124746]
	TIME [epoch: 9.7 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244116572386882		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 2.244116572386882 | validation: 3.4328606425551147]
	TIME [epoch: 9.71 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2501546471528093		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 2.2501546471528093 | validation: 3.4312989426519334]
	TIME [epoch: 9.7 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252864423401002		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 2.252864423401002 | validation: 3.4121212772235823]
	TIME [epoch: 9.69 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255342246826678		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 2.255342246826678 | validation: 3.433197511381449]
	TIME [epoch: 9.72 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243869993700732		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 2.243869993700732 | validation: 3.424394637927788]
	TIME [epoch: 9.7 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478606317037952		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 2.2478606317037952 | validation: 3.433445465646375]
	TIME [epoch: 9.69 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248312133287578		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 2.248312133287578 | validation: 3.440846638153]
	TIME [epoch: 9.72 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2466897633466605		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 2.2466897633466605 | validation: 3.4432363037837663]
	TIME [epoch: 9.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2455968913514828		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 2.2455968913514828 | validation: 3.438166560873483]
	TIME [epoch: 9.71 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2503987912778776		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 2.2503987912778776 | validation: 3.429277997184753]
	TIME [epoch: 9.69 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249787084746446		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 2.249787084746446 | validation: 3.4237071355801594]
	TIME [epoch: 9.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525253904996485		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 2.2525253904996485 | validation: 3.4365932218385713]
	TIME [epoch: 9.71 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251486752023914		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 2.251486752023914 | validation: 3.42801424114647]
	TIME [epoch: 9.69 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244393562753832		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 2.244393562753832 | validation: 3.43018623326067]
	TIME [epoch: 9.7 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2416400301133375		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 2.2416400301133375 | validation: 3.4054011508242583]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1538.pth
	Model improved!!!
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240304371373438		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 2.240304371373438 | validation: 3.4208112633093]
	TIME [epoch: 9.71 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490188729700655		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 2.2490188729700655 | validation: 3.4255808053565455]
	TIME [epoch: 9.7 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250567830638464		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 2.250567830638464 | validation: 3.417693576643859]
	TIME [epoch: 9.7 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2477170690986723		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 2.2477170690986723 | validation: 3.417818947340096]
	TIME [epoch: 9.73 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2519943604253116		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 2.2519943604253116 | validation: 3.4193492357617346]
	TIME [epoch: 9.7 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251588407574225		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 2.251588407574225 | validation: 3.4261119922548118]
	TIME [epoch: 9.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247750116550278		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 2.247750116550278 | validation: 3.433757735097398]
	TIME [epoch: 9.72 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24120827568341		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 2.24120827568341 | validation: 3.432824628397296]
	TIME [epoch: 9.71 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2546380502221117		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 2.2546380502221117 | validation: 3.424811205168036]
	TIME [epoch: 9.71 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2486847359496624		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 2.2486847359496624 | validation: 3.4135918324069747]
	TIME [epoch: 9.71 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2471184413739693		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 2.2471184413739693 | validation: 3.4288560442109906]
	TIME [epoch: 9.73 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244171743768009		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 2.244171743768009 | validation: 3.445072798002794]
	TIME [epoch: 9.71 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2459620330664776		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 2.2459620330664776 | validation: 3.420484337659765]
	TIME [epoch: 9.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2457389849950604		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 2.2457389849950604 | validation: 3.432894529736425]
	TIME [epoch: 9.69 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251075799027881		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 2.251075799027881 | validation: 3.430143291551648]
	TIME [epoch: 9.72 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2536772777196576		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 2.2536772777196576 | validation: 3.4236174482119313]
	TIME [epoch: 9.71 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2484061989087034		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 2.2484061989087034 | validation: 3.422606835373713]
	TIME [epoch: 9.7 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247442679192387		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 2.247442679192387 | validation: 3.409205396872482]
	TIME [epoch: 9.71 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2541038661758086		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 2.2541038661758086 | validation: 3.4241299420224913]
	TIME [epoch: 9.72 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2492492056256452		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 2.2492492056256452 | validation: 3.4253518074384073]
	TIME [epoch: 9.71 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2454361686187614		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 2.2454361686187614 | validation: 3.4287814044457763]
	TIME [epoch: 9.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445371212164513		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 2.2445371212164513 | validation: 3.414698821977057]
	TIME [epoch: 9.72 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2443364608461662		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 2.2443364608461662 | validation: 3.419423982034756]
	TIME [epoch: 9.71 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476652577309886		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 2.2476652577309886 | validation: 3.421202691685053]
	TIME [epoch: 9.71 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240715061829749		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 2.240715061829749 | validation: 3.4170465058418062]
	TIME [epoch: 9.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448623895671895		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 2.2448623895671895 | validation: 3.429646057071928]
	TIME [epoch: 9.74 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240891514482386		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 2.240891514482386 | validation: 3.432448849034813]
	TIME [epoch: 9.71 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245993936432316		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 2.245993936432316 | validation: 3.4200699237347103]
	TIME [epoch: 9.76 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2462153045213364		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 2.2462153045213364 | validation: 3.428240143480167]
	TIME [epoch: 9.7 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2494949490897667		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 2.2494949490897667 | validation: 3.41733390367288]
	TIME [epoch: 9.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2461412125830367		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 2.2461412125830367 | validation: 3.4164890073229994]
	TIME [epoch: 9.7 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251656407994296		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 2.251656407994296 | validation: 3.4270174119749655]
	TIME [epoch: 9.71 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25036958458925		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 2.25036958458925 | validation: 3.413244893980377]
	TIME [epoch: 9.72 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2460115258287128		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 2.2460115258287128 | validation: 3.4263274103424557]
	TIME [epoch: 9.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483360165691315		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 2.2483360165691315 | validation: 3.428375619541331]
	TIME [epoch: 9.71 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2459043910838608		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 2.2459043910838608 | validation: 3.429436715209768]
	TIME [epoch: 9.71 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2488538150141864		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 2.2488538150141864 | validation: 3.4182667841994068]
	TIME [epoch: 9.72 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2503069846855426		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 2.2503069846855426 | validation: 3.414494183384179]
	TIME [epoch: 9.72 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2422857660925084		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 2.2422857660925084 | validation: 3.432364159539082]
	TIME [epoch: 9.71 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244347318819863		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 2.244347318819863 | validation: 3.4241720815111663]
	TIME [epoch: 9.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448489780317553		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 2.2448489780317553 | validation: 3.418312107295147]
	TIME [epoch: 9.73 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2440342124223998		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 2.2440342124223998 | validation: 3.447499229313401]
	TIME [epoch: 9.71 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2474519561192627		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 2.2474519561192627 | validation: 3.4317714255019265]
	TIME [epoch: 9.71 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2514057460097		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 2.2514057460097 | validation: 3.4421299250477637]
	TIME [epoch: 9.71 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249591869803032		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 2.249591869803032 | validation: 3.4187199082634296]
	TIME [epoch: 9.72 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2555182697244534		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 2.2555182697244534 | validation: 3.4304192221147223]
	TIME [epoch: 9.71 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2492966527732303		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 2.2492966527732303 | validation: 3.4222640587436413]
	TIME [epoch: 9.71 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247469867322126		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 2.247469867322126 | validation: 3.421593473529573]
	TIME [epoch: 9.73 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2443242843825297		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 2.2443242843825297 | validation: 3.4298109630709313]
	TIME [epoch: 9.71 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2467399307803078		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 2.2467399307803078 | validation: 3.427963277501953]
	TIME [epoch: 9.72 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2507859478719534		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 2.2507859478719534 | validation: 3.4209880941887594]
	TIME [epoch: 9.7 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245462280269889		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 2.245462280269889 | validation: 3.424174574141653]
	TIME [epoch: 9.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2450074085597214		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 2.2450074085597214 | validation: 3.4259509619720916]
	TIME [epoch: 9.71 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2539887547584945		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 2.2539887547584945 | validation: 3.4187521211439424]
	TIME [epoch: 9.7 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2435832756991894		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 2.2435832756991894 | validation: 3.4299191852889543]
	TIME [epoch: 9.7 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2428288968563552		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 2.2428288968563552 | validation: 3.421008389651908]
	TIME [epoch: 9.73 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250573811722723		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 2.250573811722723 | validation: 3.4204861045840667]
	TIME [epoch: 9.72 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244802211354314		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 2.244802211354314 | validation: 3.4240015895143676]
	TIME [epoch: 9.69 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2462079871431393		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 2.2462079871431393 | validation: 3.431328823574699]
	TIME [epoch: 9.72 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24281379966339		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 2.24281379966339 | validation: 3.4281806677408757]
	TIME [epoch: 9.71 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476597496353383		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 2.2476597496353383 | validation: 3.4170686816974105]
	TIME [epoch: 9.7 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2464278427224866		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 2.2464278427224866 | validation: 3.433237118503407]
	TIME [epoch: 9.71 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241022451136532		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 2.241022451136532 | validation: 3.4236469243958756]
	TIME [epoch: 9.73 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245530974980627		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 2.245530974980627 | validation: 3.417144960139891]
	TIME [epoch: 9.72 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2451541441353617		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 2.2451541441353617 | validation: 3.433178009649529]
	TIME [epoch: 9.71 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242792724887798		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 2.242792724887798 | validation: 3.4331724945355604]
	TIME [epoch: 9.71 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248108998567461		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 2.248108998567461 | validation: 3.4319135383517314]
	TIME [epoch: 9.74 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2506798376872994		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 2.2506798376872994 | validation: 3.430912909251532]
	TIME [epoch: 9.7 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2454514073358722		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 2.2454514073358722 | validation: 3.427830922338576]
	TIME [epoch: 9.71 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2460209834642026		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 2.2460209834642026 | validation: 3.4229823544977456]
	TIME [epoch: 9.71 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252848329896498		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 2.252848329896498 | validation: 3.4193129866650906]
	TIME [epoch: 9.72 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2452541304399665		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 2.2452541304399665 | validation: 3.4261015849446994]
	TIME [epoch: 9.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253712591489356		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 2.253712591489356 | validation: 3.4274394994032504]
	TIME [epoch: 9.71 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456357897533072		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 2.2456357897533072 | validation: 3.4261087311679836]
	TIME [epoch: 9.73 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2437717644241433		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 2.2437717644241433 | validation: 3.4211515649514195]
	TIME [epoch: 9.72 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434509899108823		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 2.2434509899108823 | validation: 3.4250350029520167]
	TIME [epoch: 9.71 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2412272686735397		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 2.2412272686735397 | validation: 3.4265875884626475]
	TIME [epoch: 9.7 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241287028694779		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 2.241287028694779 | validation: 3.4281523037766353]
	TIME [epoch: 9.72 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247214376275808		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 2.247214376275808 | validation: 3.423836662454746]
	TIME [epoch: 9.69 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2397391645078804		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 2.2397391645078804 | validation: 3.425279300526897]
	TIME [epoch: 9.71 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244197717619382		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 2.244197717619382 | validation: 3.4415061518787486]
	TIME [epoch: 9.7 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2401344326433366		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 2.2401344326433366 | validation: 3.4321136282593727]
	TIME [epoch: 9.74 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2529785142989134		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 2.2529785142989134 | validation: 3.4212097671439734]
	TIME [epoch: 9.7 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243591845020139		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 2.243591845020139 | validation: 3.421843179096695]
	TIME [epoch: 9.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247292314744177		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 2.247292314744177 | validation: 3.4333152592300644]
	TIME [epoch: 9.73 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256941992530145		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 2.256941992530145 | validation: 3.4260000761489504]
	TIME [epoch: 9.7 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2487557174838493		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 2.2487557174838493 | validation: 3.436851009494887]
	TIME [epoch: 9.69 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2470553816197656		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 2.2470553816197656 | validation: 3.4332995387339666]
	TIME [epoch: 9.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255438304675111		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 2.255438304675111 | validation: 3.429124078620381]
	TIME [epoch: 9.72 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2503499596588945		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 2.2503499596588945 | validation: 3.4267834850901693]
	TIME [epoch: 9.71 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2484817182746903		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 2.2484817182746903 | validation: 3.433773335147344]
	TIME [epoch: 9.71 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244510947850614		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 2.244510947850614 | validation: 3.422016362387924]
	TIME [epoch: 9.7 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2446169067474067		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 2.2446169067474067 | validation: 3.4200703318919365]
	TIME [epoch: 9.72 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2464636462203176		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 2.2464636462203176 | validation: 3.430034832196418]
	TIME [epoch: 9.71 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2524582814229688		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 2.2524582814229688 | validation: 3.4269392146617963]
	TIME [epoch: 9.72 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2471315154893223		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 2.2471315154893223 | validation: 3.42651140788485]
	TIME [epoch: 9.71 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2391189118863646		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 2.2391189118863646 | validation: 3.4288628756470416]
	TIME [epoch: 9.72 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240047236312807		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 2.240047236312807 | validation: 3.4257902252240138]
	TIME [epoch: 9.71 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246751905210254		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 2.246751905210254 | validation: 3.434690929063564]
	TIME [epoch: 9.71 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251869164855752		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 2.251869164855752 | validation: 3.422266780322352]
	TIME [epoch: 9.71 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245903932466331		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 2.245903932466331 | validation: 3.427353710680034]
	TIME [epoch: 9.7 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2440371122683835		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 2.2440371122683835 | validation: 3.4200646038374822]
	TIME [epoch: 9.71 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2468126483759865		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 2.2468126483759865 | validation: 3.4231303773420483]
	TIME [epoch: 9.7 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247084663420387		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 2.247084663420387 | validation: 3.4380029886232126]
	TIME [epoch: 9.72 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243563995837984		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 2.243563995837984 | validation: 3.443592982266112]
	TIME [epoch: 9.7 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2492665863297225		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 2.2492665863297225 | validation: 3.448329428706531]
	TIME [epoch: 9.72 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2555152230665363		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 2.2555152230665363 | validation: 3.4296147462700413]
	TIME [epoch: 9.71 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516723508866705		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 2.2516723508866705 | validation: 3.4414643027429825]
	TIME [epoch: 9.75 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250014933421399		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 2.250014933421399 | validation: 3.4357647302574703]
	TIME [epoch: 9.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.239902314726198		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 2.239902314726198 | validation: 3.42972240816515]
	TIME [epoch: 9.72 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251712810682477		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 2.251712810682477 | validation: 3.4318351531054088]
	TIME [epoch: 9.72 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24925619599938		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 2.24925619599938 | validation: 3.420194389697024]
	TIME [epoch: 9.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2455637485935243		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 2.2455637485935243 | validation: 3.4158119196011287]
	TIME [epoch: 9.7 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476351879244616		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 2.2476351879244616 | validation: 3.4327707522274764]
	TIME [epoch: 9.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2472585190197227		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 2.2472585190197227 | validation: 3.431797428275436]
	TIME [epoch: 9.74 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498329540179456		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 2.2498329540179456 | validation: 3.4359383785865534]
	TIME [epoch: 9.72 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243960707165063		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 2.243960707165063 | validation: 3.4246467386194808]
	TIME [epoch: 9.71 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242997013903647		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 2.242997013903647 | validation: 3.428612415746161]
	TIME [epoch: 9.72 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248770636001678		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 2.248770636001678 | validation: 3.4167125257706097]
	TIME [epoch: 9.73 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241694487690588		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 2.241694487690588 | validation: 3.4328929606153498]
	TIME [epoch: 9.7 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252476933942091		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 2.252476933942091 | validation: 3.410415943208386]
	TIME [epoch: 9.7 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2566839127973575		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 2.2566839127973575 | validation: 3.423815069774999]
	TIME [epoch: 9.72 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251210439177729		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 2.251210439177729 | validation: 3.425414059186422]
	TIME [epoch: 9.73 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2438973744055586		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 2.2438973744055586 | validation: 3.4152756326316136]
	TIME [epoch: 9.71 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245622672311521		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 2.245622672311521 | validation: 3.4273983747933223]
	TIME [epoch: 9.7 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244293946396928		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 2.244293946396928 | validation: 3.4315895330680695]
	TIME [epoch: 9.73 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252351802914103		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 2.252351802914103 | validation: 3.4215646528520107]
	TIME [epoch: 9.71 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242632524160667		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 2.242632524160667 | validation: 3.4305265101277347]
	TIME [epoch: 9.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2472565504679904		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 2.2472565504679904 | validation: 3.42245103733874]
	TIME [epoch: 9.7 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478174809585965		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 2.2478174809585965 | validation: 3.4102455319904768]
	TIME [epoch: 9.74 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433312748043917		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 2.2433312748043917 | validation: 3.4303299550107]
	TIME [epoch: 9.7 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247710209327397		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 2.247710209327397 | validation: 3.4283899792764054]
	TIME [epoch: 9.72 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2488299095380997		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 2.2488299095380997 | validation: 3.4273297218096896]
	TIME [epoch: 9.69 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2528755590347758		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 2.2528755590347758 | validation: 3.442225228704198]
	TIME [epoch: 9.73 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247600933029793		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 2.247600933029793 | validation: 3.423901218240058]
	TIME [epoch: 9.7 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242880349476189		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 2.242880349476189 | validation: 3.4257436401165138]
	TIME [epoch: 9.7 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2457224812630345		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 2.2457224812630345 | validation: 3.424653719412494]
	TIME [epoch: 9.72 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249524420950511		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 2.249524420950511 | validation: 3.4089441943113528]
	TIME [epoch: 9.72 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240633330916427		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 2.240633330916427 | validation: 3.415420414214346]
	TIME [epoch: 9.72 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473201645327325		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 2.2473201645327325 | validation: 3.4226063956285038]
	TIME [epoch: 9.72 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2440749974949075		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 2.2440749974949075 | validation: 3.425101315997582]
	TIME [epoch: 9.72 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2538828398200894		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 2.2538828398200894 | validation: 3.435028519309423]
	TIME [epoch: 9.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2487031785742064		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 2.2487031785742064 | validation: 3.422254872363301]
	TIME [epoch: 9.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249993322920102		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 2.249993322920102 | validation: 3.4153795410653847]
	TIME [epoch: 9.72 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479826728111143		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 2.2479826728111143 | validation: 3.4212195695003698]
	TIME [epoch: 9.74 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24249037613938		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 2.24249037613938 | validation: 3.4277529970882252]
	TIME [epoch: 9.72 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2442184273035877		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 2.2442184273035877 | validation: 3.415612752781558]
	TIME [epoch: 9.72 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2442060779214104		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 2.2442060779214104 | validation: 3.4307776680709345]
	TIME [epoch: 9.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2438067595282485		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 2.2438067595282485 | validation: 3.4136413620491624]
	TIME [epoch: 9.73 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2497234027455173		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 2.2497234027455173 | validation: 3.430923148229474]
	TIME [epoch: 9.73 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2491157666711556		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 2.2491157666711556 | validation: 3.434670661683867]
	TIME [epoch: 9.72 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2394660278261442		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 2.2394660278261442 | validation: 3.4164131533992657]
	TIME [epoch: 9.74 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2455447657038574		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 2.2455447657038574 | validation: 3.417989883723125]
	TIME [epoch: 9.72 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247129832022427		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 2.247129832022427 | validation: 3.4266863648038197]
	TIME [epoch: 9.72 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478292388590635		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 2.2478292388590635 | validation: 3.4250706387570085]
	TIME [epoch: 9.72 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244606255801135		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 2.244606255801135 | validation: 3.43405855448827]
	TIME [epoch: 9.74 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2385232458546387		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 2.2385232458546387 | validation: 3.4136573131416266]
	TIME [epoch: 9.72 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490938774444205		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 2.2490938774444205 | validation: 3.4319032612142415]
	TIME [epoch: 9.72 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2468671473268405		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 2.2468671473268405 | validation: 3.4313330521730485]
	TIME [epoch: 9.72 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249550490819628		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 2.249550490819628 | validation: 3.4251791363134254]
	TIME [epoch: 9.74 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2470798996951435		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 2.2470798996951435 | validation: 3.4191682506118872]
	TIME [epoch: 9.71 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246232442075486		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 2.246232442075486 | validation: 3.422188816389326]
	TIME [epoch: 9.72 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2429878159029726		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 2.2429878159029726 | validation: 3.4173728235702105]
	TIME [epoch: 9.73 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2455298422705416		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 2.2455298422705416 | validation: 3.415450514329923]
	TIME [epoch: 9.71 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2431106369955827		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 2.2431106369955827 | validation: 3.430935800498422]
	TIME [epoch: 9.71 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249987835493692		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 2.249987835493692 | validation: 3.422868833366174]
	TIME [epoch: 9.72 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245644698651509		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 2.245644698651509 | validation: 3.4230501969438745]
	TIME [epoch: 9.73 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2496984633447146		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 2.2496984633447146 | validation: 3.419887623928767]
	TIME [epoch: 9.71 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2543728196420467		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 2.2543728196420467 | validation: 3.4206675302502276]
	TIME [epoch: 9.72 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250085141837457		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 2.250085141837457 | validation: 3.411642175550167]
	TIME [epoch: 9.71 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512712399316697		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 2.2512712399316697 | validation: 3.4225838433370983]
	TIME [epoch: 9.73 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445077771146837		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 2.2445077771146837 | validation: 3.42183180297202]
	TIME [epoch: 9.71 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2517048088063376		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 2.2517048088063376 | validation: 3.423645221778493]
	TIME [epoch: 9.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241031170103827		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 2.241031170103827 | validation: 3.423639143887525]
	TIME [epoch: 9.73 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2458999908864175		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 2.2458999908864175 | validation: 3.421382372876492]
	TIME [epoch: 9.73 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248847033318448		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 2.248847033318448 | validation: 3.433164445068742]
	TIME [epoch: 9.72 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2466528144143405		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 2.2466528144143405 | validation: 3.423304732339823]
	TIME [epoch: 9.72 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2441244704210366		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 2.2441244704210366 | validation: 3.426666149512946]
	TIME [epoch: 9.74 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245591171213966		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 2.245591171213966 | validation: 3.4274466089430144]
	TIME [epoch: 9.71 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24501703974033		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 2.24501703974033 | validation: 3.4413768781857335]
	TIME [epoch: 9.72 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483666372291475		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 2.2483666372291475 | validation: 3.4277381827042364]
	TIME [epoch: 9.7 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245414813416931		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 2.245414813416931 | validation: 3.418933442728086]
	TIME [epoch: 9.72 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2532730392017273		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 2.2532730392017273 | validation: 3.417256475596405]
	TIME [epoch: 9.71 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2461556588170772		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 2.2461556588170772 | validation: 3.422392753543993]
	TIME [epoch: 9.71 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2444868864931915		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 2.2444868864931915 | validation: 3.4215750650414747]
	TIME [epoch: 9.72 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483541214138185		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 2.2483541214138185 | validation: 3.4351644966756414]
	TIME [epoch: 9.72 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240148797203238		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 2.240148797203238 | validation: 3.430407322020168]
	TIME [epoch: 9.71 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247475955223451		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 2.247475955223451 | validation: 3.425305735791344]
	TIME [epoch: 9.71 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483171608452706		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 2.2483171608452706 | validation: 3.428993282709429]
	TIME [epoch: 9.72 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473612378425516		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 2.2473612378425516 | validation: 3.421143777655457]
	TIME [epoch: 9.72 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2505740099460736		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 2.2505740099460736 | validation: 3.4275484192970653]
	TIME [epoch: 9.72 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2419397760021034		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 2.2419397760021034 | validation: 3.427567707315393]
	TIME [epoch: 9.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240618744824124		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 2.240618744824124 | validation: 3.4403883229242527]
	TIME [epoch: 9.75 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2431189259656756		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 2.2431189259656756 | validation: 3.4277126600429653]
	TIME [epoch: 9.71 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479288411337057		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 2.2479288411337057 | validation: 3.4096402061494326]
	TIME [epoch: 9.71 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250005359366619		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 2.250005359366619 | validation: 3.4208704618884487]
	TIME [epoch: 9.72 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246233518655741		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 2.246233518655741 | validation: 3.421780601341644]
	TIME [epoch: 9.74 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2482710521696894		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 2.2482710521696894 | validation: 3.439852215431614]
	TIME [epoch: 9.71 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512907959843846		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 2.2512907959843846 | validation: 3.431347107325447]
	TIME [epoch: 9.72 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244177143214669		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 2.244177143214669 | validation: 3.4120103544234683]
	TIME [epoch: 9.73 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246988111768675		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 2.246988111768675 | validation: 3.4340314523495614]
	TIME [epoch: 9.72 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242743595175324		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 2.242743595175324 | validation: 3.422958363219218]
	TIME [epoch: 9.72 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483122843876266		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 2.2483122843876266 | validation: 3.4239722418200094]
	TIME [epoch: 9.7 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252774571061104		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 2.252774571061104 | validation: 3.4153077514082355]
	TIME [epoch: 9.73 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241390377183921		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 2.241390377183921 | validation: 3.4246228771247225]
	TIME [epoch: 9.71 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2495139777544284		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 2.2495139777544284 | validation: 3.4305415040701144]
	TIME [epoch: 9.72 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243187190893208		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 2.243187190893208 | validation: 3.4222502477546834]
	TIME [epoch: 9.72 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2453562498863326		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 2.2453562498863326 | validation: 3.4267766313823955]
	TIME [epoch: 9.74 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.239959213011726		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 2.239959213011726 | validation: 3.430538730332888]
	TIME [epoch: 9.71 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241631850825537		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 2.241631850825537 | validation: 3.4271650682142325]
	TIME [epoch: 9.72 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2504881101021		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 2.2504881101021 | validation: 3.425914643077897]
	TIME [epoch: 9.73 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247752202570758		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 2.247752202570758 | validation: 3.4274875016621213]
	TIME [epoch: 9.72 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244897212216652		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 2.244897212216652 | validation: 3.4309068135565326]
	TIME [epoch: 9.7 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2484419610714297		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 2.2484419610714297 | validation: 3.423368807893197]
	TIME [epoch: 9.71 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2504444055246537		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 2.2504444055246537 | validation: 3.4274221896785693]
	TIME [epoch: 9.73 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2453083575157047		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 2.2453083575157047 | validation: 3.413011778987794]
	TIME [epoch: 9.72 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2435663982214247		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 2.2435663982214247 | validation: 3.4164112410080403]
	TIME [epoch: 9.71 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244718567711169		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 2.244718567711169 | validation: 3.4185666495297893]
	TIME [epoch: 9.71 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244954858898175		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 2.244954858898175 | validation: 3.434975672217462]
	TIME [epoch: 9.72 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2499361447492108		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 2.2499361447492108 | validation: 3.4232206018320994]
	TIME [epoch: 9.72 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2393872379690047		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 2.2393872379690047 | validation: 3.4248196378045144]
	TIME [epoch: 9.72 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242008316099996		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 2.242008316099996 | validation: 3.4153103501482422]
	TIME [epoch: 9.72 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241742261465384		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 2.241742261465384 | validation: 3.4207099034653363]
	TIME [epoch: 9.73 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2428598702927096		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 2.2428598702927096 | validation: 3.4130380748547977]
	TIME [epoch: 9.7 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2499791259105977		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 2.2499791259105977 | validation: 3.4190832792571038]
	TIME [epoch: 9.7 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2424472396297555		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 2.2424472396297555 | validation: 3.4152093374138692]
	TIME [epoch: 9.73 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246604785880515		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 2.246604785880515 | validation: 3.4355050445447795]
	TIME [epoch: 9.72 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2441872313176345		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 2.2441872313176345 | validation: 3.422963309689903]
	TIME [epoch: 9.72 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2449872558142228		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 2.2449872558142228 | validation: 3.4224896157108375]
	TIME [epoch: 9.72 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2436656190092625		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 2.2436656190092625 | validation: 3.4228217677343333]
	TIME [epoch: 9.73 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2521572816404216		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 2.2521572816404216 | validation: 3.4183976374287477]
	TIME [epoch: 9.71 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483724547691404		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 2.2483724547691404 | validation: 3.418602884991766]
	TIME [epoch: 9.72 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2458814647575753		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 2.2458814647575753 | validation: 3.417390365705899]
	TIME [epoch: 9.71 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248635577490455		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 2.248635577490455 | validation: 3.423495914901847]
	TIME [epoch: 9.73 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2450412986028887		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 2.2450412986028887 | validation: 3.430637109318006]
	TIME [epoch: 9.7 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251992362230051		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 2.251992362230051 | validation: 3.42468796030877]
	TIME [epoch: 9.73 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246228220389561		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 2.246228220389561 | validation: 3.4302410819817086]
	TIME [epoch: 9.72 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2452691082170135		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 2.2452691082170135 | validation: 3.428129152858101]
	TIME [epoch: 9.72 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473218807004844		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 2.2473218807004844 | validation: 3.4187749137159678]
	TIME [epoch: 9.71 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245664028348302		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 2.245664028348302 | validation: 3.4145089185180577]
	TIME [epoch: 9.72 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2471513856506014		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 2.2471513856506014 | validation: 3.4310482247421636]
	TIME [epoch: 9.73 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248055604318755		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 2.248055604318755 | validation: 3.432365590094571]
	TIME [epoch: 9.72 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2475186869890065		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 2.2475186869890065 | validation: 3.433255181570712]
	TIME [epoch: 9.71 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248930118498827		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 2.248930118498827 | validation: 3.4253179025258373]
	TIME [epoch: 9.71 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243899562709737		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 2.243899562709737 | validation: 3.4255700551797426]
	TIME [epoch: 9.73 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483153819947264		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 2.2483153819947264 | validation: 3.434458468009267]
	TIME [epoch: 9.71 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2402465622683336		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 2.2402465622683336 | validation: 3.4336864990717464]
	TIME [epoch: 9.71 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479636820495714		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 2.2479636820495714 | validation: 3.42185935566827]
	TIME [epoch: 9.71 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248096336147104		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 2.248096336147104 | validation: 3.4202806581910683]
	TIME [epoch: 9.73 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254146484597302		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 2.254146484597302 | validation: 3.426142061358725]
	TIME [epoch: 9.72 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2465116421429645		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 2.2465116421429645 | validation: 3.4317502654095065]
	TIME [epoch: 9.71 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245119712756344		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 2.245119712756344 | validation: 3.4309593113475443]
	TIME [epoch: 9.74 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2399230625189506		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 2.2399230625189506 | validation: 3.431664969668077]
	TIME [epoch: 9.71 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2409805142377186		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 2.2409805142377186 | validation: 3.4334108721035923]
	TIME [epoch: 9.72 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2552139401035127		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 2.2552139401035127 | validation: 3.438870623872109]
	TIME [epoch: 9.71 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2438980346636117		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 2.2438980346636117 | validation: 3.407195319104021]
	TIME [epoch: 9.74 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2447100911763824		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 2.2447100911763824 | validation: 3.4102608322365278]
	TIME [epoch: 9.72 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246469319110404		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 2.246469319110404 | validation: 3.4259882034847084]
	TIME [epoch: 9.71 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2503289867145027		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 2.2503289867145027 | validation: 3.416914899122129]
	TIME [epoch: 9.72 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247379720224579		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 2.247379720224579 | validation: 3.4155850986239757]
	TIME [epoch: 9.73 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2439434744524194		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 2.2439434744524194 | validation: 3.4208725272182834]
	TIME [epoch: 9.71 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2458417296260746		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 2.2458417296260746 | validation: 3.421171743289972]
	TIME [epoch: 9.71 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248984717736079		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 2.248984717736079 | validation: 3.4268930671165947]
	TIME [epoch: 9.71 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2422563411402665		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 2.2422563411402665 | validation: 3.4270317944384203]
	TIME [epoch: 9.73 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2484599802463476		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 2.2484599802463476 | validation: 3.4310306612358907]
	TIME [epoch: 9.71 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247837561283351		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 2.247837561283351 | validation: 3.419071875416884]
	TIME [epoch: 9.72 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433560804103925		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 2.2433560804103925 | validation: 3.4291170635795307]
	TIME [epoch: 9.73 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2439535443314567		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 2.2439535443314567 | validation: 3.431292034793555]
	TIME [epoch: 9.72 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2468449099286096		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 2.2468449099286096 | validation: 3.423849124701768]
	TIME [epoch: 9.72 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2393753540983847		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 2.2393753540983847 | validation: 3.4125876211179365]
	TIME [epoch: 9.7 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2537576733945		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 2.2537576733945 | validation: 3.418038818691544]
	TIME [epoch: 9.74 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2537622844201364		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 2.2537622844201364 | validation: 3.4320065153641472]
	TIME [epoch: 9.72 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252246130169038		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 2.252246130169038 | validation: 3.4264050075533192]
	TIME [epoch: 9.71 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2447546924013446		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 2.2447546924013446 | validation: 3.4162934195396044]
	TIME [epoch: 9.72 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2435478931931527		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 2.2435478931931527 | validation: 3.427382645553687]
	TIME [epoch: 9.73 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2416697814560473		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 2.2416697814560473 | validation: 3.427721412219357]
	TIME [epoch: 9.72 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.239333936436127		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 2.239333936436127 | validation: 3.4350658317302156]
	TIME [epoch: 9.72 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250944435299929		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 2.250944435299929 | validation: 3.410900160565568]
	TIME [epoch: 9.74 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2423911326920978		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 2.2423911326920978 | validation: 3.409689843843029]
	TIME [epoch: 9.72 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2452270703108272		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 2.2452270703108272 | validation: 3.419899614178754]
	TIME [epoch: 9.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445600907681276		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 2.2445600907681276 | validation: 3.4274114275665184]
	TIME [epoch: 9.71 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2485163798000856		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 2.2485163798000856 | validation: 3.4237116432748564]
	TIME [epoch: 9.74 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244960529683616		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 2.244960529683616 | validation: 3.4356016571812473]
	TIME [epoch: 9.71 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2465520136529884		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 2.2465520136529884 | validation: 3.4300365060031726]
	TIME [epoch: 9.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245188280821422		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 2.245188280821422 | validation: 3.4220355791467183]
	TIME [epoch: 9.72 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241172164212016		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 2.241172164212016 | validation: 3.4225058893338747]
	TIME [epoch: 9.74 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2406590002152043		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 2.2406590002152043 | validation: 3.4153355842881674]
	TIME [epoch: 9.72 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2455357941507055		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 2.2455357941507055 | validation: 3.427472184871541]
	TIME [epoch: 9.72 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251640452339264		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 2.251640452339264 | validation: 3.4298459569985527]
	TIME [epoch: 9.73 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2466553318725735		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 2.2466553318725735 | validation: 3.4250261628029377]
	TIME [epoch: 9.72 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242350636279853		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 2.242350636279853 | validation: 3.427449269957724]
	TIME [epoch: 9.71 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249005937040921		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 2.249005937040921 | validation: 3.432721763043105]
	TIME [epoch: 9.71 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2465616799650117		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 2.2465616799650117 | validation: 3.4099839887559065]
	TIME [epoch: 9.73 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2451744915776164		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 2.2451744915776164 | validation: 3.4143047515160267]
	TIME [epoch: 9.72 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2425862202560483		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 2.2425862202560483 | validation: 3.419408654491878]
	TIME [epoch: 9.72 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248218756075397		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 2.248218756075397 | validation: 3.4230054945828345]
	TIME [epoch: 9.72 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244918145752128		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 2.244918145752128 | validation: 3.4185842521125926]
	TIME [epoch: 9.73 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244500594215702		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 2.244500594215702 | validation: 3.419205253276026]
	TIME [epoch: 9.71 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244386294775061		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 2.244386294775061 | validation: 3.4145335632872817]
	TIME [epoch: 9.71 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2421057018543005		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 2.2421057018543005 | validation: 3.43208765250435]
	TIME [epoch: 9.72 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24381161354667		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 2.24381161354667 | validation: 3.42812818780027]
	TIME [epoch: 9.7 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2398479076870603		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 2.2398479076870603 | validation: 3.4313136173828505]
	TIME [epoch: 9.72 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434074992280917		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 2.2434074992280917 | validation: 3.4209635925436594]
	TIME [epoch: 9.7 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433507128404084		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 2.2433507128404084 | validation: 3.426309630445993]
	TIME [epoch: 9.73 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2450261285366655		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 2.2450261285366655 | validation: 3.420433649154995]
	TIME [epoch: 9.71 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245243937359508		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 2.245243937359508 | validation: 3.422342531687332]
	TIME [epoch: 9.71 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.236445574749104		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 2.236445574749104 | validation: 3.4262824120684034]
	TIME [epoch: 9.71 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2413615721600864		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 2.2413615721600864 | validation: 3.4069553843053337]
	TIME [epoch: 9.73 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247422150022378		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 2.247422150022378 | validation: 3.4295729575129847]
	TIME [epoch: 9.71 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243144310075371		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 2.243144310075371 | validation: 3.414073628858724]
	TIME [epoch: 9.71 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247575937129569		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 2.247575937129569 | validation: 3.428556550340942]
	TIME [epoch: 9.72 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246023123438631		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 2.246023123438631 | validation: 3.4148558388945514]
	TIME [epoch: 9.72 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243945914754406		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 2.243945914754406 | validation: 3.41895274164174]
	TIME [epoch: 9.77 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2500570077019435		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 2.2500570077019435 | validation: 3.423619949013946]
	TIME [epoch: 9.71 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248705006466756		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 2.248705006466756 | validation: 3.4238176640586504]
	TIME [epoch: 9.72 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479944700136847		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 2.2479944700136847 | validation: 3.439244843453214]
	TIME [epoch: 9.72 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2477146013755243		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 2.2477146013755243 | validation: 3.4271679764452143]
	TIME [epoch: 9.71 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247077283891491		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 2.247077283891491 | validation: 3.4253957445821896]
	TIME [epoch: 9.71 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247043014563735		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 2.247043014563735 | validation: 3.4335859918921607]
	TIME [epoch: 9.72 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246351123056991		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 2.246351123056991 | validation: 3.43478757020147]
	TIME [epoch: 9.71 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24663379693072		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 2.24663379693072 | validation: 3.433603332463962]
	TIME [epoch: 9.71 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2443896427020404		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 2.2443896427020404 | validation: 3.42238149037471]
	TIME [epoch: 9.72 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2403677708584544		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 2.2403677708584544 | validation: 3.417057222613205]
	TIME [epoch: 9.73 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2430021084734904		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 2.2430021084734904 | validation: 3.4319514201226453]
	TIME [epoch: 9.72 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2466278149359		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 2.2466278149359 | validation: 3.4176202915029372]
	TIME [epoch: 9.7 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434352354930995		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 2.2434352354930995 | validation: 3.42417139396687]
	TIME [epoch: 9.71 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2489192849501958		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 2.2489192849501958 | validation: 3.4382377634912906]
	TIME [epoch: 9.71 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242635011600546		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 2.242635011600546 | validation: 3.4208807684244484]
	TIME [epoch: 9.71 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2480652410457527		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 2.2480652410457527 | validation: 3.4215820911752424]
	TIME [epoch: 9.71 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244325238018139		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 2.244325238018139 | validation: 3.435000112397517]
	TIME [epoch: 9.74 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2525288166184247		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 2.2525288166184247 | validation: 3.413154406353483]
	TIME [epoch: 9.7 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2505783034607254		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 2.2505783034607254 | validation: 3.42357466361965]
	TIME [epoch: 9.7 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2438485585857775		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 2.2438485585857775 | validation: 3.433689971183355]
	TIME [epoch: 9.71 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.237905602861178		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 2.237905602861178 | validation: 3.418058270572654]
	TIME [epoch: 9.73 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479955156925087		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 2.2479955156925087 | validation: 3.426389764960817]
	TIME [epoch: 9.71 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2414712934152234		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 2.2414712934152234 | validation: 3.432239199616625]
	TIME [epoch: 9.72 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2528300749697383		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 2.2528300749697383 | validation: 3.42123774812159]
	TIME [epoch: 9.71 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248996100032914		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 2.248996100032914 | validation: 3.427677078268297]
	TIME [epoch: 9.73 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244627458154759		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 2.244627458154759 | validation: 3.429276547948623]
	TIME [epoch: 9.71 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246688311184922		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 2.246688311184922 | validation: 3.419943980981944]
	TIME [epoch: 9.71 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247631354099129		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 2.247631354099129 | validation: 3.424232466385404]
	TIME [epoch: 9.72 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2443790046859577		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 2.2443790046859577 | validation: 3.417181845925514]
	TIME [epoch: 9.72 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.238644739499586		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 2.238644739499586 | validation: 3.4275770518963338]
	TIME [epoch: 9.72 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2480446568686654		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 2.2480446568686654 | validation: 3.4090152233232454]
	TIME [epoch: 9.71 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2498685826091864		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 2.2498685826091864 | validation: 3.429521984651614]
	TIME [epoch: 9.73 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2459359199053432		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 2.2459359199053432 | validation: 3.4148934201958867]
	TIME [epoch: 9.72 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2442544642339763		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 2.2442544642339763 | validation: 3.4169451485073865]
	TIME [epoch: 9.71 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245990102325088		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 2.245990102325088 | validation: 3.4322536960146857]
	TIME [epoch: 9.72 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245536452264669		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 2.245536452264669 | validation: 3.422979848262299]
	TIME [epoch: 9.73 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2454590571896835		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 2.2454590571896835 | validation: 3.414578222505562]
	TIME [epoch: 9.71 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242821963107162		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 2.242821963107162 | validation: 3.4332378418879284]
	TIME [epoch: 9.71 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2452263285395815		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 2.2452263285395815 | validation: 3.4202390655788704]
	TIME [epoch: 9.71 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2405011973065405		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 2.2405011973065405 | validation: 3.427120274924375]
	TIME [epoch: 9.72 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2468872999967546		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 2.2468872999967546 | validation: 3.4219835023645944]
	TIME [epoch: 9.72 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2425330468723526		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 2.2425330468723526 | validation: 3.414741868337938]
	TIME [epoch: 9.71 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240849158556133		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 2.240849158556133 | validation: 3.408175835143245]
	TIME [epoch: 9.72 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241887799955502		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 2.241887799955502 | validation: 3.4164493593434457]
	TIME [epoch: 9.71 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2475272657503296		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 2.2475272657503296 | validation: 3.411349941838793]
	TIME [epoch: 9.72 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2437939570535517		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 2.2437939570535517 | validation: 3.4067044645264533]
	TIME [epoch: 9.72 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251758902267558		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 2.251758902267558 | validation: 3.4230298431527633]
	TIME [epoch: 9.74 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241177411322817		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 2.241177411322817 | validation: 3.413911352511552]
	TIME [epoch: 9.7 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2441158521734805		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 2.2441158521734805 | validation: 3.414394233913624]
	TIME [epoch: 9.71 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23888832879344		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 2.23888832879344 | validation: 3.409314530164499]
	TIME [epoch: 9.72 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2486728330666024		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 2.2486728330666024 | validation: 3.4154313164403094]
	TIME [epoch: 9.72 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2450271967035746		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 2.2450271967035746 | validation: 3.436053426007459]
	TIME [epoch: 9.71 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2429828877758005		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 2.2429828877758005 | validation: 3.4155274879601327]
	TIME [epoch: 9.71 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247518216263663		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 2.247518216263663 | validation: 3.4173929448764047]
	TIME [epoch: 9.72 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2512937219113303		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 2.2512937219113303 | validation: 3.4109987817335914]
	TIME [epoch: 9.72 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247608560059892		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 2.247608560059892 | validation: 3.4071726343801583]
	TIME [epoch: 9.7 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242475018467495		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 2.242475018467495 | validation: 3.4260192649387933]
	TIME [epoch: 9.69 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243461217151048		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 2.243461217151048 | validation: 3.421542569303002]
	TIME [epoch: 9.72 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246824595912399		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 2.246824595912399 | validation: 3.4213380955278776]
	TIME [epoch: 9.71 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.238853403078009		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 2.238853403078009 | validation: 3.420825029817832]
	TIME [epoch: 9.7 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483306310536433		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 2.2483306310536433 | validation: 3.424208843187263]
	TIME [epoch: 9.7 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241304812909261		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 2.241304812909261 | validation: 3.424803647554181]
	TIME [epoch: 9.72 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245421131360405		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 2.245421131360405 | validation: 3.418839968735003]
	TIME [epoch: 9.71 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249210952706224		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 2.249210952706224 | validation: 3.4412059545851426]
	TIME [epoch: 9.71 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2481298902118683		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 2.2481298902118683 | validation: 3.41110936212615]
	TIME [epoch: 9.71 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240364608976843		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 2.240364608976843 | validation: 3.438821685987772]
	TIME [epoch: 9.71 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247122198837573		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 2.247122198837573 | validation: 3.4252523503186763]
	TIME [epoch: 9.71 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247618516418286		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 2.247618516418286 | validation: 3.422359759430637]
	TIME [epoch: 9.7 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245869465441774		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 2.245869465441774 | validation: 3.42291808339573]
	TIME [epoch: 9.72 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250661211428601		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 2.250661211428601 | validation: 3.419192695614738]
	TIME [epoch: 9.69 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249978411366032		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 2.249978411366032 | validation: 3.4121456830325254]
	TIME [epoch: 9.71 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2457806528106348		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 2.2457806528106348 | validation: 3.4116010904063945]
	TIME [epoch: 9.7 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2473936414573683		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 2.2473936414573683 | validation: 3.4183037930781883]
	TIME [epoch: 9.72 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243422514485269		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 2.243422514485269 | validation: 3.4249224220987355]
	TIME [epoch: 9.7 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244882592272181		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 2.244882592272181 | validation: 3.422867412300425]
	TIME [epoch: 9.71 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2451518544328612		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 2.2451518544328612 | validation: 3.4216138199759123]
	TIME [epoch: 9.71 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242398211658652		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 2.242398211658652 | validation: 3.4294916801565867]
	TIME [epoch: 9.72 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2495150232658703		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 2.2495150232658703 | validation: 3.4219994293554343]
	TIME [epoch: 9.69 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2398954519863534		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 2.2398954519863534 | validation: 3.4149103867099657]
	TIME [epoch: 9.71 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456430473608617		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 2.2456430473608617 | validation: 3.406333108876536]
	TIME [epoch: 9.72 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479000404199754		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 2.2479000404199754 | validation: 3.4264097820673634]
	TIME [epoch: 9.71 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243296847226259		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 2.243296847226259 | validation: 3.4188270701732755]
	TIME [epoch: 9.7 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2461922790139286		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 2.2461922790139286 | validation: 3.4254458338800764]
	TIME [epoch: 9.7 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2417762028397954		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 2.2417762028397954 | validation: 3.41451858419799]
	TIME [epoch: 9.7 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242680135694411		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 2.242680135694411 | validation: 3.4219983543351806]
	TIME [epoch: 9.7 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478592959268675		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 2.2478592959268675 | validation: 3.4266851941788956]
	TIME [epoch: 9.71 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2428846072593234		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 2.2428846072593234 | validation: 3.4352836068175625]
	TIME [epoch: 9.71 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241504603686928		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 2.241504603686928 | validation: 3.4302252213338322]
	TIME [epoch: 9.72 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24282607221262		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 2.24282607221262 | validation: 3.4398749959436703]
	TIME [epoch: 9.71 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252071825504104		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 2.252071825504104 | validation: 3.4383685053214057]
	TIME [epoch: 9.7 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2419706849631473		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 2.2419706849631473 | validation: 3.416350816260282]
	TIME [epoch: 9.71 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246014496123933		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 2.246014496123933 | validation: 3.4326156998430486]
	TIME [epoch: 9.71 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242464066623907		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 2.242464066623907 | validation: 3.417548766359031]
	TIME [epoch: 9.71 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.235645546449266		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 2.235645546449266 | validation: 3.422497283189242]
	TIME [epoch: 9.7 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478305855689262		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 2.2478305855689262 | validation: 3.4213431589380607]
	TIME [epoch: 9.73 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478839097532473		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 2.2478839097532473 | validation: 3.4196268605392652]
	TIME [epoch: 9.71 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2446628095553307		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 2.2446628095553307 | validation: 3.422509137234996]
	TIME [epoch: 9.7 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2462421131019257		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 2.2462421131019257 | validation: 3.4211564539069617]
	TIME [epoch: 9.7 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244135462492569		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 2.244135462492569 | validation: 3.4232329530289682]
	TIME [epoch: 9.71 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2449669067815905		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 2.2449669067815905 | validation: 3.4228280651236718]
	TIME [epoch: 9.7 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243906952788452		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 2.243906952788452 | validation: 3.41858060933746]
	TIME [epoch: 9.7 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2446533657543535		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 2.2446533657543535 | validation: 3.4271597452147637]
	TIME [epoch: 9.71 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2480791694362194		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 2.2480791694362194 | validation: 3.406984665867441]
	TIME [epoch: 9.72 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2445844201096423		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 2.2445844201096423 | validation: 3.4120813345091072]
	TIME [epoch: 9.71 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241364997207007		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 2.241364997207007 | validation: 3.429812688578695]
	TIME [epoch: 9.7 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2410369547162743		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 2.2410369547162743 | validation: 3.4165773254925353]
	TIME [epoch: 9.72 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2421729058330087		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 2.2421729058330087 | validation: 3.4219943566864393]
	TIME [epoch: 9.69 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244086292053516		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 2.244086292053516 | validation: 3.4113933433692343]
	TIME [epoch: 9.7 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2452576033963436		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 2.2452576033963436 | validation: 3.4246597403705326]
	TIME [epoch: 9.7 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448241225818295		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 2.2448241225818295 | validation: 3.4186374529122254]
	TIME [epoch: 9.72 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2482595753912116		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 2.2482595753912116 | validation: 3.435531190083566]
	TIME [epoch: 9.69 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2439834770200284		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 2.2439834770200284 | validation: 3.4164207839754157]
	TIME [epoch: 9.7 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24255751718586		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 2.24255751718586 | validation: 3.428609654478785]
	TIME [epoch: 9.71 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2405361642636596		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 2.2405361642636596 | validation: 3.413629183231618]
	TIME [epoch: 9.72 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2387289027201533		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 2.2387289027201533 | validation: 3.431140913137682]
	TIME [epoch: 9.7 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456013177187817		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 2.2456013177187817 | validation: 3.415854532927757]
	TIME [epoch: 9.7 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2401417432917157		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 2.2401417432917157 | validation: 3.423365406084128]
	TIME [epoch: 9.71 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244753892071356		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 2.244753892071356 | validation: 3.4350309650020576]
	TIME [epoch: 9.7 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456805398051913		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 2.2456805398051913 | validation: 3.4244709761033234]
	TIME [epoch: 9.71 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245874951434742		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 2.245874951434742 | validation: 3.4283154761976355]
	TIME [epoch: 9.69 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2449351481976203		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 2.2449351481976203 | validation: 3.4196851687316987]
	TIME [epoch: 9.72 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483561948240065		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 2.2483561948240065 | validation: 3.424223761097569]
	TIME [epoch: 9.71 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434635749076413		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 2.2434635749076413 | validation: 3.421499198974629]
	TIME [epoch: 9.7 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490233952714247		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 2.2490233952714247 | validation: 3.4361975881563196]
	TIME [epoch: 9.7 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2435596706076213		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 2.2435596706076213 | validation: 3.4134775440054366]
	TIME [epoch: 9.73 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245583134524576		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 2.245583134524576 | validation: 3.405383595003564]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1977.pth
	Model improved!!!
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2447617274227296		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 2.2447617274227296 | validation: 3.431294653639049]
	TIME [epoch: 9.71 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242262964651926		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 2.242262964651926 | validation: 3.4290940295905457]
	TIME [epoch: 9.75 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241119099395965		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 2.241119099395965 | validation: 3.4195705211507494]
	TIME [epoch: 9.73 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248022929095415		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 2.248022929095415 | validation: 3.4191289372940243]
	TIME [epoch: 9.71 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250677493102984		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 2.250677493102984 | validation: 3.422150857484352]
	TIME [epoch: 9.72 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245377490538234		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 2.245377490538234 | validation: 3.4308314764492667]
	TIME [epoch: 9.73 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2464668952230116		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 2.2464668952230116 | validation: 3.4279118350390694]
	TIME [epoch: 9.72 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2509521994165924		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 2.2509521994165924 | validation: 3.4251817454663613]
	TIME [epoch: 9.71 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24701301379323		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 2.24701301379323 | validation: 3.4252202350837515]
	TIME [epoch: 9.72 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2377406666348123		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 2.2377406666348123 | validation: 3.4213133540964806]
	TIME [epoch: 9.73 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2508716508178788		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 2.2508716508178788 | validation: 3.417970282786985]
	TIME [epoch: 9.72 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2476848045209152		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 2.2476848045209152 | validation: 3.4198573301630235]
	TIME [epoch: 9.71 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242833729419159		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 2.242833729419159 | validation: 3.413627621205185]
	TIME [epoch: 9.71 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24256823753154		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 2.24256823753154 | validation: 3.4277704302315124]
	TIME [epoch: 9.73 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2404720426753344		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 2.2404720426753344 | validation: 3.4205674928519407]
	TIME [epoch: 9.72 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243371061168092		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 2.243371061168092 | validation: 3.418390626739907]
	TIME [epoch: 9.71 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2399057422616524		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 2.2399057422616524 | validation: 3.4223682883750137]
	TIME [epoch: 9.72 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2415239012735997		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 2.2415239012735997 | validation: 3.4202843316082836]
	TIME [epoch: 9.71 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433175090334894		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 2.2433175090334894 | validation: 3.4202047703313965]
	TIME [epoch: 9.71 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2421365481754387		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 2.2421365481754387 | validation: 3.4355603748643375]
	TIME [epoch: 9.71 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2462922070048883		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 2.2462922070048883 | validation: 3.4262363798080746]
	TIME [epoch: 9.72 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2483732105454153		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 2.2483732105454153 | validation: 3.4044319818135205]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study205/model_tr_study205_r0_20240219_235113/states/model_tr_study205_1999.pth
	Model improved!!!
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245094629252134		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 2.245094629252134 | validation: 3.407143423986969]
	TIME [epoch: 9.72 sec]
Finished training in 19598.698 seconds.
