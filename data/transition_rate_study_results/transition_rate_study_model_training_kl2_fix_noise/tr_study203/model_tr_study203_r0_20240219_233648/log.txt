Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r0', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1689095351

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.990041487737026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.990041487737026 | validation: 10.962831027942393]
	TIME [epoch: 79.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.34201082884307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.34201082884307 | validation: 10.017361081820539]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.051722155253488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.051722155253488 | validation: 10.482072336909361]
	TIME [epoch: 8.35 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.521427907383824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.521427907383824 | validation: 8.949701080412424]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.631584633236043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.631584633236043 | validation: 8.958785894768848]
	TIME [epoch: 8.34 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.227558362272644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.227558362272644 | validation: 6.8204611365552115]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.839361768797024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.839361768797024 | validation: 6.354684720778751]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.902421466386743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.902421466386743 | validation: 7.74034878532859]
	TIME [epoch: 8.36 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.536183284561072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.536183284561072 | validation: 4.876860546168153]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.088108613272889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.088108613272889 | validation: 7.752576905839609]
	TIME [epoch: 8.32 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.256268475069542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.256268475069542 | validation: 4.367113831113171]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.51816722458155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.51816722458155 | validation: 4.790339512470126]
	TIME [epoch: 8.35 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.514128141190022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.514128141190022 | validation: 4.560814586837548]
	TIME [epoch: 8.32 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.264643668572793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.264643668572793 | validation: 4.6127885000426305]
	TIME [epoch: 8.32 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.183167255083793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.183167255083793 | validation: 4.136593080267614]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.12985824455275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.12985824455275 | validation: 4.326848409742855]
	TIME [epoch: 8.33 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.310352135061295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.310352135061295 | validation: 4.069717209680029]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.048417130448902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.048417130448902 | validation: 3.993195155597065]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.15477623908077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.15477623908077 | validation: 4.929479785976536]
	TIME [epoch: 8.36 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.149922365578105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.149922365578105 | validation: 4.317087149173046]
	TIME [epoch: 8.33 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.181443925279732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.181443925279732 | validation: 4.013179564130803]
	TIME [epoch: 8.33 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.025177363104936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.025177363104936 | validation: 3.9634251352574843]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.788396995838532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.788396995838532 | validation: 4.291202386198]
	TIME [epoch: 8.34 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.0602724821537945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0602724821537945 | validation: 4.765835614401735]
	TIME [epoch: 8.33 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.0779853596552496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0779853596552496 | validation: 3.597744396053086]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.800931080327534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.800931080327534 | validation: 4.169008425865439]
	TIME [epoch: 8.36 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.792217891128309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792217891128309 | validation: 3.8045629055227375]
	TIME [epoch: 8.33 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.633789315928174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.633789315928174 | validation: 3.549307768291312]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.506998692991234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.506998692991234 | validation: 3.681834751054655]
	TIME [epoch: 8.36 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.715579897265364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.715579897265364 | validation: 4.4381331442551355]
	TIME [epoch: 8.34 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4877219250230285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4877219250230285 | validation: 4.248040277965498]
	TIME [epoch: 8.33 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.438927867463623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.438927867463623 | validation: 3.580481123133308]
	TIME [epoch: 8.33 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.445965097037209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.445965097037209 | validation: 3.089959791529982]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.506517842893766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.506517842893766 | validation: 3.199060155902946]
	TIME [epoch: 8.34 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.233837486932373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.233837486932373 | validation: 3.057234405831698]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.105860024813158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.105860024813158 | validation: 4.052387675572538]
	TIME [epoch: 8.34 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.128455460357257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.128455460357257 | validation: 2.978432248875898]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.102398997513658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.102398997513658 | validation: 3.5757378322474676]
	TIME [epoch: 8.33 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0995547599406335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0995547599406335 | validation: 3.080460572969923]
	TIME [epoch: 8.33 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.162743086050519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.162743086050519 | validation: 4.82113411365983]
	TIME [epoch: 8.35 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.233623432775637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.233623432775637 | validation: 2.840631898402064]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.040379975122811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040379975122811 | validation: 2.7470972462923826]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9582243494463163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9582243494463163 | validation: 3.0612287615059364]
	TIME [epoch: 8.34 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8330004012796905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8330004012796905 | validation: 2.5625426422882187]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.867004582230222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.867004582230222 | validation: 3.0466590671149545]
	TIME [epoch: 8.32 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.837935093591463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.837935093591463 | validation: 2.5433679788009207]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6738230400526986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6738230400526986 | validation: 2.5126452246098956]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.571939823438137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.571939823438137 | validation: 2.5731934309631153]
	TIME [epoch: 8.33 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.580344125027532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580344125027532 | validation: 2.4037132876045213]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.596082485360495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.596082485360495 | validation: 2.8328413317725962]
	TIME [epoch: 8.35 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.596392503313963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.596392503313963 | validation: 2.5542177741035648]
	TIME [epoch: 8.37 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4870365338428284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4870365338428284 | validation: 2.162742591752609]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2785237053693828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2785237053693828 | validation: 2.687408124481595]
	TIME [epoch: 8.34 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.551377064153976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.551377064153976 | validation: 2.609326148062765]
	TIME [epoch: 8.35 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.353110384756783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.353110384756783 | validation: 2.1977907743860436]
	TIME [epoch: 8.35 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4610427018720182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4610427018720182 | validation: 2.4977464849455364]
	TIME [epoch: 8.33 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.497245584659106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497245584659106 | validation: 2.414551902453452]
	TIME [epoch: 8.34 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.531772687701725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.531772687701725 | validation: 2.520853228549261]
	TIME [epoch: 8.36 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.284244314104199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.284244314104199 | validation: 2.550119681457383]
	TIME [epoch: 8.34 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3034371938131444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3034371938131444 | validation: 2.20269363707214]
	TIME [epoch: 8.33 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.387691487520468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.387691487520468 | validation: 2.143218419432432]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2946053234258366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2946053234258366 | validation: 2.1279765848919556]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1772210091478157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1772210091478157 | validation: 2.1129884061497055]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2448693830178383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2448693830178383 | validation: 2.375666987773171]
	TIME [epoch: 8.33 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.325066476724346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.325066476724346 | validation: 2.181923416428677]
	TIME [epoch: 8.35 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2706402896722273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2706402896722273 | validation: 2.192371103564043]
	TIME [epoch: 8.34 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2500199848117637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2500199848117637 | validation: 2.3329033171880145]
	TIME [epoch: 8.32 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2576239500521744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2576239500521744 | validation: 2.1937075185453825]
	TIME [epoch: 8.33 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.532059791420644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.532059791420644 | validation: 2.369018184631347]
	TIME [epoch: 8.36 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.353446171273474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.353446171273474 | validation: 2.3158507795859347]
	TIME [epoch: 8.33 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1693807862679004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1693807862679004 | validation: 2.088159468606618]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.303026742417141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.303026742417141 | validation: 2.2328393876125663]
	TIME [epoch: 8.35 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0535316396470025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0535316396470025 | validation: 1.8400471254902078]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5165263369266753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5165263369266753 | validation: 1.6945509269802905]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.451341544798265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.451341544798265 | validation: 1.8854099911554358]
	TIME [epoch: 8.32 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2881408522237896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2881408522237896 | validation: 1.7535785539459525]
	TIME [epoch: 8.35 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.445623417620859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.445623417620859 | validation: 1.8073465270635678]
	TIME [epoch: 8.32 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.21264750334286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.21264750334286 | validation: 1.7063826191175608]
	TIME [epoch: 8.32 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.254791094411167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.254791094411167 | validation: 1.5460312160938237]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.049239179662418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.049239179662418 | validation: 1.652820231394882]
	TIME [epoch: 8.34 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.108728407043975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.108728407043975 | validation: 1.2448008526006464]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8676447781411873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8676447781411873 | validation: 1.3746949580643735]
	TIME [epoch: 8.33 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9173756593275357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9173756593275357 | validation: 1.277823307254181]
	TIME [epoch: 8.34 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0880859694506344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0880859694506344 | validation: 2.086921085181328]
	TIME [epoch: 8.32 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9981552584034028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9981552584034028 | validation: 1.1923172937977387]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9664651421675152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9664651421675152 | validation: 1.3021003182536972]
	TIME [epoch: 8.34 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8500534277693674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8500534277693674 | validation: 1.7114593693928517]
	TIME [epoch: 8.33 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8345850874571568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8345850874571568 | validation: 1.5901448951114185]
	TIME [epoch: 8.32 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9129305346068677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9129305346068677 | validation: 1.515890336968197]
	TIME [epoch: 8.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8914508512950117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8914508512950117 | validation: 1.4703029436691692]
	TIME [epoch: 8.34 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9821031080709006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9821031080709006 | validation: 1.2543109136533963]
	TIME [epoch: 8.32 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0686048306881717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0686048306881717 | validation: 1.2420212308267498]
	TIME [epoch: 8.32 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.722901630332041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.722901630332041 | validation: 1.6703243075623546]
	TIME [epoch: 8.32 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8015720353267912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8015720353267912 | validation: 1.7838882593408618]
	TIME [epoch: 8.34 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4039858564181955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4039858564181955 | validation: 2.0210834603409604]
	TIME [epoch: 8.32 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7680413847975167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7680413847975167 | validation: 1.3281803469250506]
	TIME [epoch: 8.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8473691736098192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8473691736098192 | validation: 1.2021574686386773]
	TIME [epoch: 8.32 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0704742178678046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0704742178678046 | validation: 1.469037871459829]
	TIME [epoch: 8.33 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6505494011027988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6505494011027988 | validation: 1.9018480423880246]
	TIME [epoch: 8.31 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5738923091622787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5738923091622787 | validation: 1.6778049012367229]
	TIME [epoch: 8.31 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1600941105736364		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 2.1600941105736364 | validation: 1.3821145117897864]
	TIME [epoch: 8.34 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5791267143691852		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 1.5791267143691852 | validation: 1.3802414569933847]
	TIME [epoch: 8.32 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.654013982224471		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 1.654013982224471 | validation: 2.2655997727873673]
	TIME [epoch: 8.31 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0236636670494894		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 2.0236636670494894 | validation: 1.3186551287702808]
	TIME [epoch: 8.31 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7205751072562658		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 1.7205751072562658 | validation: 2.1508488695934247]
	TIME [epoch: 8.34 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7765362645919232		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 1.7765362645919232 | validation: 1.6896086757001019]
	TIME [epoch: 8.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7778489731922615		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 1.7778489731922615 | validation: 3.2788892594778307]
	TIME [epoch: 8.32 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8677365804069637		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 1.8677365804069637 | validation: 1.0822301576154778]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5611255469363952		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 1.5611255469363952 | validation: 1.711868784107564]
	TIME [epoch: 8.33 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.862417537306327		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 1.862417537306327 | validation: 2.542241103211878]
	TIME [epoch: 8.31 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6025338697009903		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 1.6025338697009903 | validation: 1.4102473066080088]
	TIME [epoch: 8.32 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4413145904158773		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 1.4413145904158773 | validation: 1.3587100710964668]
	TIME [epoch: 8.34 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.774753744499962		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 2.774753744499962 | validation: 1.24049776572981]
	TIME [epoch: 8.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5205258274770774		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 1.5205258274770774 | validation: 1.2630599135535023]
	TIME [epoch: 8.31 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6836809324040716		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 1.6836809324040716 | validation: 1.5268759271199004]
	TIME [epoch: 8.31 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.066399089042241		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 2.066399089042241 | validation: 1.761723207815572]
	TIME [epoch: 8.34 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7006435937574487		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 1.7006435937574487 | validation: 1.7067451987452187]
	TIME [epoch: 8.31 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5282328628378865		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 1.5282328628378865 | validation: 1.3186917923289112]
	TIME [epoch: 8.31 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6755345184035673		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 1.6755345184035673 | validation: 1.7252340614301185]
	TIME [epoch: 8.33 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7221536198701588		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 1.7221536198701588 | validation: 3.3836100245228424]
	TIME [epoch: 8.32 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.071224310028025		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 2.071224310028025 | validation: 2.9672457181585923]
	TIME [epoch: 8.31 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.956176230915751		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 1.956176230915751 | validation: 1.0918625864879008]
	TIME [epoch: 8.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.759416861200546		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 1.759416861200546 | validation: 1.1969971593650406]
	TIME [epoch: 8.34 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6060812035084253		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 1.6060812035084253 | validation: 5.021171738219576]
	TIME [epoch: 8.32 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9132028521677455		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 1.9132028521677455 | validation: 2.564291170444878]
	TIME [epoch: 8.31 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8837170522015754		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 1.8837170522015754 | validation: 0.9767767753079122]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.402263689914326		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 1.402263689914326 | validation: 1.0772382047868878]
	TIME [epoch: 8.36 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3239888745755017		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 1.3239888745755017 | validation: 1.2776480139008237]
	TIME [epoch: 8.35 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3823591876576		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 1.3823591876576 | validation: 1.2567687674592425]
	TIME [epoch: 8.34 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4463036195683077		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 1.4463036195683077 | validation: 1.6536620396453505]
	TIME [epoch: 8.36 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4843318128948533		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 1.4843318128948533 | validation: 2.306614924017782]
	TIME [epoch: 8.36 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.476625550194324		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 1.476625550194324 | validation: 1.0373664762583024]
	TIME [epoch: 8.34 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4985685250011733		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 1.4985685250011733 | validation: 1.5364238262195105]
	TIME [epoch: 8.34 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6859833876899089		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 1.6859833876899089 | validation: 0.9866356683228126]
	TIME [epoch: 8.38 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3872665353864724		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 1.3872665353864724 | validation: 0.9663192544582293]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6262922178852925		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 1.6262922178852925 | validation: 0.8829215362081451]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.284810100027883		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 1.284810100027883 | validation: 1.163795488148156]
	TIME [epoch: 8.35 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5254596359714971		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 1.5254596359714971 | validation: 1.2181848483593185]
	TIME [epoch: 8.36 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.326208811698765		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 1.326208811698765 | validation: 1.048613619551718]
	TIME [epoch: 8.34 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.478041486863342		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 1.478041486863342 | validation: 0.9156402757966149]
	TIME [epoch: 8.34 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3007307839270092		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 1.3007307839270092 | validation: 1.2914663317101116]
	TIME [epoch: 8.36 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2966902459808423		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 1.2966902459808423 | validation: 1.4071135778263222]
	TIME [epoch: 8.34 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.321786062516542		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 1.321786062516542 | validation: 0.7624068782149102]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7737349286055217		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 1.7737349286055217 | validation: 1.4600606556951283]
	TIME [epoch: 8.34 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.31259837839692		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 1.31259837839692 | validation: 1.0417526288557588]
	TIME [epoch: 8.37 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2698865269480546		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 1.2698865269480546 | validation: 0.6844768809592352]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.115657225263333		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 1.115657225263333 | validation: 1.2301155533877113]
	TIME [epoch: 8.33 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.213246258765127		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 1.213246258765127 | validation: 0.82855304283146]
	TIME [epoch: 8.35 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2222581083279453		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 1.2222581083279453 | validation: 1.3388891315234335]
	TIME [epoch: 8.34 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3153230649587315		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 1.3153230649587315 | validation: 0.6952592723603899]
	TIME [epoch: 8.33 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2395011108269258		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 1.2395011108269258 | validation: 0.9169580039221008]
	TIME [epoch: 8.33 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.247596519865086		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 1.247596519865086 | validation: 1.02057533432638]
	TIME [epoch: 8.36 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2537930718341899		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 1.2537930718341899 | validation: 1.1650519168361488]
	TIME [epoch: 8.33 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1498256243391407		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 1.1498256243391407 | validation: 1.1323076336649758]
	TIME [epoch: 8.33 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.219648768572444		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 1.219648768572444 | validation: 1.9007924127060964]
	TIME [epoch: 8.33 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.233358819636939		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 1.233358819636939 | validation: 1.7040411142887977]
	TIME [epoch: 8.35 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.524418683488936		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 1.524418683488936 | validation: 0.7129193453635951]
	TIME [epoch: 8.33 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.158655515063386		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 1.158655515063386 | validation: 0.861827250612284]
	TIME [epoch: 8.32 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0715032569798935		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 1.0715032569798935 | validation: 1.006669608061745]
	TIME [epoch: 8.35 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.229031870902189		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 1.229031870902189 | validation: 0.6923972126027059]
	TIME [epoch: 8.34 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1805675781228375		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 1.1805675781228375 | validation: 0.8970040500714491]
	TIME [epoch: 8.32 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1093414635325705		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 1.1093414635325705 | validation: 0.7499982447522159]
	TIME [epoch: 8.33 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.267905778098508		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 1.267905778098508 | validation: 0.8317295925710888]
	TIME [epoch: 8.36 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3440683369373239		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 1.3440683369373239 | validation: 0.725069859955966]
	TIME [epoch: 8.33 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2165290119700427		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 1.2165290119700427 | validation: 0.8493452080592805]
	TIME [epoch: 8.33 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3939429788143052		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 1.3939429788143052 | validation: 0.6224492552489207]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4465051964341493		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 1.4465051964341493 | validation: 0.9878938344947624]
	TIME [epoch: 8.34 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1616644190105314		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 1.1616644190105314 | validation: 0.862017913230833]
	TIME [epoch: 8.32 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2001092453422992		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 1.2001092453422992 | validation: 1.2817579023336054]
	TIME [epoch: 8.32 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0913093450216191		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 1.0913093450216191 | validation: 0.9916020071720085]
	TIME [epoch: 8.35 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1078616260947		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 1.1078616260947 | validation: 0.7309186696140142]
	TIME [epoch: 8.33 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2054929597584798		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 1.2054929597584798 | validation: 0.8436684377101774]
	TIME [epoch: 8.33 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0025956271547214		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 1.0025956271547214 | validation: 1.0013953897963033]
	TIME [epoch: 8.33 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0437645706652303		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 1.0437645706652303 | validation: 0.6780585873705138]
	TIME [epoch: 8.36 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9524617907462151		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 0.9524617907462151 | validation: 1.06252370890701]
	TIME [epoch: 8.33 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2463527513999784		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 1.2463527513999784 | validation: 0.6363847106421356]
	TIME [epoch: 8.33 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.022115378962723		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 1.022115378962723 | validation: 0.8889837083228358]
	TIME [epoch: 8.35 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4815978439291815		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 1.4815978439291815 | validation: 1.6190952593777064]
	TIME [epoch: 8.33 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2750168554484604		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 1.2750168554484604 | validation: 1.0119480039606694]
	TIME [epoch: 8.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.102192941887054		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 1.102192941887054 | validation: 0.6677820098159146]
	TIME [epoch: 8.33 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.109597707579777		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 1.109597707579777 | validation: 0.7515833779404253]
	TIME [epoch: 8.36 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9214385607738859		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 0.9214385607738859 | validation: 1.3411821331480245]
	TIME [epoch: 8.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4451469567356505		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 1.4451469567356505 | validation: 0.8053103183752133]
	TIME [epoch: 8.33 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2258613877583322		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 1.2258613877583322 | validation: 0.6367990524170464]
	TIME [epoch: 8.34 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9499719175191949		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 0.9499719175191949 | validation: 0.9786015155003993]
	TIME [epoch: 8.35 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0155304068049542		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 1.0155304068049542 | validation: 1.1414878944256561]
	TIME [epoch: 8.32 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.098950425439995		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 1.098950425439995 | validation: 0.574768486427082]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0763934836173121		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 1.0763934836173121 | validation: 1.0844616781008367]
	TIME [epoch: 8.36 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.45665393719716		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 1.45665393719716 | validation: 1.1808567937002048]
	TIME [epoch: 8.33 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9952665744030892		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 0.9952665744030892 | validation: 0.5428882734787581]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1619691460403008		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 1.1619691460403008 | validation: 0.6440665627910517]
	TIME [epoch: 8.33 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0316848581599232		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 1.0316848581599232 | validation: 0.7044090570154384]
	TIME [epoch: 8.35 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9703611712829557		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 0.9703611712829557 | validation: 0.6222232755551458]
	TIME [epoch: 8.32 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.02329149307049		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 1.02329149307049 | validation: 0.6136352931358999]
	TIME [epoch: 8.32 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0596304849743963		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 1.0596304849743963 | validation: 1.0948507653313224]
	TIME [epoch: 8.34 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8568476072434891		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 0.8568476072434891 | validation: 2.335043765055558]
	TIME [epoch: 8.33 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0886819800920793		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 1.0886819800920793 | validation: 0.5787825008737352]
	TIME [epoch: 8.33 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9334289474466761		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 0.9334289474466761 | validation: 0.5266018326513894]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1347310806804258		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 1.1347310806804258 | validation: 2.5158636920079296]
	TIME [epoch: 8.36 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2421733690749952		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 1.2421733690749952 | validation: 0.5332700951198754]
	TIME [epoch: 8.33 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3970403383771912		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 1.3970403383771912 | validation: 0.9722957900873075]
	TIME [epoch: 8.32 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.195821763708521		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 1.195821763708521 | validation: 0.9675849242177759]
	TIME [epoch: 8.34 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0336129769414442		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 1.0336129769414442 | validation: 0.6552296332065773]
	TIME [epoch: 8.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0308033518659658		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 1.0308033518659658 | validation: 0.7790444254687456]
	TIME [epoch: 8.32 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9345399542457627		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 0.9345399542457627 | validation: 0.460192124292244]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1313813494918095		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 1.1313813494918095 | validation: 2.021381369786649]
	TIME [epoch: 8.34 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2030215129947177		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 1.2030215129947177 | validation: 1.3558307799096359]
	TIME [epoch: 8.32 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0353433772645015		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 1.0353433772645015 | validation: 0.7525911328631101]
	TIME [epoch: 8.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0798746131059893		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 1.0798746131059893 | validation: 0.6740576181504738]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0200384830694795		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 1.0200384830694795 | validation: 0.6608366874885758]
	TIME [epoch: 8.34 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1778424854203675		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 1.1778424854203675 | validation: 0.4896709472679097]
	TIME [epoch: 8.31 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7920345786342144		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 0.7920345786342144 | validation: 0.8001305406227909]
	TIME [epoch: 8.31 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7948471200731626		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 0.7948471200731626 | validation: 0.644401875519279]
	TIME [epoch: 8.33 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8755746696426632		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 0.8755746696426632 | validation: 0.45659705966287856]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.855011426159687		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 0.855011426159687 | validation: 0.6396200829780437]
	TIME [epoch: 8.33 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7844838247878338		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 0.7844838247878338 | validation: 0.7056962376647824]
	TIME [epoch: 8.32 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1730825966806937		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 1.1730825966806937 | validation: 0.6195107395472386]
	TIME [epoch: 8.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8360738130965848		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 0.8360738130965848 | validation: 0.9922273854539898]
	TIME [epoch: 8.32 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.889701773281201		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 0.889701773281201 | validation: 0.5184235881135976]
	TIME [epoch: 8.31 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7885982190009201		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 0.7885982190009201 | validation: 1.0250658257977432]
	TIME [epoch: 8.32 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8966210786590889		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 0.8966210786590889 | validation: 0.7541983152337289]
	TIME [epoch: 8.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8488921884251657		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 0.8488921884251657 | validation: 0.5553472815024872]
	TIME [epoch: 8.32 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8461848580195934		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 0.8461848580195934 | validation: 0.6241380305592029]
	TIME [epoch: 8.32 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9425316931036759		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 0.9425316931036759 | validation: 1.15999707973601]
	TIME [epoch: 8.34 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8937747067871944		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 0.8937747067871944 | validation: 0.41586970863909567]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8651425343097369		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 0.8651425343097369 | validation: 0.4395439670032383]
	TIME [epoch: 8.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7949923250348518		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 0.7949923250348518 | validation: 0.4209225060378907]
	TIME [epoch: 8.33 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8951333199964262		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 0.8951333199964262 | validation: 0.6951876953836396]
	TIME [epoch: 8.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.053533422586504		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 1.053533422586504 | validation: 0.6245290605194762]
	TIME [epoch: 8.33 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9745469894004293		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 0.9745469894004293 | validation: 0.6334957815082815]
	TIME [epoch: 8.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8421378334060963		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 0.8421378334060963 | validation: 0.3594654197702728]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8362390488251197		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 0.8362390488251197 | validation: 0.6063723628143973]
	TIME [epoch: 8.34 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8071201426310136		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 0.8071201426310136 | validation: 0.7189489949234235]
	TIME [epoch: 8.34 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9743846632289623		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 0.9743846632289623 | validation: 0.5472624890090465]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7702802724893223		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 0.7702802724893223 | validation: 0.42281194498054886]
	TIME [epoch: 8.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8750565841962092		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 0.8750565841962092 | validation: 0.5368476620100407]
	TIME [epoch: 8.34 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.888929737155858		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 0.888929737155858 | validation: 1.2799859887333949]
	TIME [epoch: 8.34 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9509635298166685		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 0.9509635298166685 | validation: 0.5396588399388907]
	TIME [epoch: 8.34 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9027641993219732		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 0.9027641993219732 | validation: 0.38819190567928497]
	TIME [epoch: 8.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.745532078851526		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 0.745532078851526 | validation: 0.6489556115077746]
	TIME [epoch: 8.33 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8464077654449668		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 0.8464077654449668 | validation: 1.1544870128922242]
	TIME [epoch: 8.34 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4186805763284425		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 1.4186805763284425 | validation: 0.6333482610507426]
	TIME [epoch: 8.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6580741099776996		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 0.6580741099776996 | validation: 0.6597324656458265]
	TIME [epoch: 8.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8831816609962846		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 0.8831816609962846 | validation: 0.623258491300462]
	TIME [epoch: 8.34 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9723263029070189		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 0.9723263029070189 | validation: 0.4212643517370073]
	TIME [epoch: 8.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8551185008672849		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 0.8551185008672849 | validation: 0.5711959023917429]
	TIME [epoch: 8.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7662789592606084		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 0.7662789592606084 | validation: 0.5809634242479976]
	TIME [epoch: 8.34 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8429530201868074		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 0.8429530201868074 | validation: 0.7506789633180995]
	TIME [epoch: 8.34 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8773061951432849		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 0.8773061951432849 | validation: 0.5169559884095053]
	TIME [epoch: 8.34 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6893710582212799		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 0.6893710582212799 | validation: 1.6671459768846364]
	TIME [epoch: 8.37 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8957228522406817		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 0.8957228522406817 | validation: 1.04753217801772]
	TIME [epoch: 8.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8099102464214093		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 0.8099102464214093 | validation: 0.5847198935762878]
	TIME [epoch: 8.34 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8887048947266388		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 0.8887048947266388 | validation: 0.439881740189918]
	TIME [epoch: 8.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0038984330075131		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 1.0038984330075131 | validation: 0.46518597006798335]
	TIME [epoch: 8.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6518722532325312		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 0.6518722532325312 | validation: 0.7050287047755563]
	TIME [epoch: 8.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8374614925694068		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 0.8374614925694068 | validation: 0.822705768944802]
	TIME [epoch: 8.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8592460047659142		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 0.8592460047659142 | validation: 0.4354091467647664]
	TIME [epoch: 8.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8495347729955182		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 0.8495347729955182 | validation: 0.47207398936989886]
	TIME [epoch: 8.34 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9756883952864788		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 0.9756883952864788 | validation: 0.5953728870340576]
	TIME [epoch: 8.34 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8763384697323943		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 0.8763384697323943 | validation: 0.44081074372418916]
	TIME [epoch: 8.34 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.810166802130785		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 0.810166802130785 | validation: 0.5723253956288435]
	TIME [epoch: 8.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.853947175098584		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 0.853947175098584 | validation: 0.6797939764214599]
	TIME [epoch: 8.34 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7366923566707786		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 0.7366923566707786 | validation: 0.35862385650344164]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.774800568550934		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 0.774800568550934 | validation: 0.48896215014986977]
	TIME [epoch: 8.37 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8065364861773257		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 0.8065364861773257 | validation: 0.3790940647791699]
	TIME [epoch: 8.34 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6811160292306375		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 0.6811160292306375 | validation: 0.37316264272829325]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7351095053564731		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 0.7351095053564731 | validation: 0.8099661604220871]
	TIME [epoch: 8.34 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7319629620373415		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 0.7319629620373415 | validation: 0.4096002890811684]
	TIME [epoch: 8.37 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7931427537976526		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 0.7931427537976526 | validation: 0.7113257938610958]
	TIME [epoch: 8.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1355057540079023		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 1.1355057540079023 | validation: 2.095036864365628]
	TIME [epoch: 8.33 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8525075489740261		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 0.8525075489740261 | validation: 0.41994518042381473]
	TIME [epoch: 8.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7308095701623889		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 0.7308095701623889 | validation: 0.3792771041038866]
	TIME [epoch: 8.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6942397646434849		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 0.6942397646434849 | validation: 0.4263953981386336]
	TIME [epoch: 8.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6084047554779624		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 0.6084047554779624 | validation: 0.43002771990054245]
	TIME [epoch: 8.33 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8967583898483668		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 0.8967583898483668 | validation: 0.518649988633212]
	TIME [epoch: 8.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8827645019004334		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 0.8827645019004334 | validation: 0.40545763332813217]
	TIME [epoch: 8.34 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7982273730344895		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 0.7982273730344895 | validation: 0.49676620441117497]
	TIME [epoch: 8.33 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8507707133023912		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 0.8507707133023912 | validation: 0.6003277911770228]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7239787162169223		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 0.7239787162169223 | validation: 0.4160638307275988]
	TIME [epoch: 8.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7243189850601144		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 0.7243189850601144 | validation: 0.8439412539126563]
	TIME [epoch: 8.34 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8135232340263137		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 0.8135232340263137 | validation: 0.675384265063621]
	TIME [epoch: 8.33 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9674977370056579		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 0.9674977370056579 | validation: 0.8748182723845976]
	TIME [epoch: 8.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3224134270955363		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 2.3224134270955363 | validation: 0.41039417019297014]
	TIME [epoch: 8.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7481704694990768		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 0.7481704694990768 | validation: 0.461015806135475]
	TIME [epoch: 8.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7016209829277604		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 0.7016209829277604 | validation: 0.34649929523734363]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8675067628415126		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 0.8675067628415126 | validation: 0.5515985967609769]
	TIME [epoch: 8.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9589233468023594		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 0.9589233468023594 | validation: 0.44934462096061034]
	TIME [epoch: 8.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6796154614578586		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 0.6796154614578586 | validation: 0.37680801012344434]
	TIME [epoch: 8.33 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8807386501039256		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 0.8807386501039256 | validation: 0.818185678585664]
	TIME [epoch: 8.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6867839497373843		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 0.6867839497373843 | validation: 0.8530203377284007]
	TIME [epoch: 8.33 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7237230099541304		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 0.7237230099541304 | validation: 0.5554515686532631]
	TIME [epoch: 8.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6835484326986613		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 0.6835484326986613 | validation: 1.77566358711337]
	TIME [epoch: 8.33 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9061831403589649		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 0.9061831403589649 | validation: 0.4984897353614038]
	TIME [epoch: 8.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9047462309573706		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 0.9047462309573706 | validation: 0.7598484617203849]
	TIME [epoch: 8.33 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8405181893843799		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 0.8405181893843799 | validation: 0.6109962020115689]
	TIME [epoch: 8.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6952832472019608		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 0.6952832472019608 | validation: 0.550864664272924]
	TIME [epoch: 8.33 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8962757131895598		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 0.8962757131895598 | validation: 0.5485287051158586]
	TIME [epoch: 8.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7120939880602213		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 0.7120939880602213 | validation: 0.4962024916356257]
	TIME [epoch: 8.33 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.116816735252878		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 1.116816735252878 | validation: 0.3421997683328076]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7585967680475998		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 0.7585967680475998 | validation: 0.6712615130195752]
	TIME [epoch: 8.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8101700033460905		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 0.8101700033460905 | validation: 0.4172822288771475]
	TIME [epoch: 8.33 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6105403332236432		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 0.6105403332236432 | validation: 0.36049715194648346]
	TIME [epoch: 8.33 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7443257463915751		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 0.7443257463915751 | validation: 1.1023520282311652]
	TIME [epoch: 8.33 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7482365557969668		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 0.7482365557969668 | validation: 0.4778676552370379]
	TIME [epoch: 8.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6840026663638239		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 0.6840026663638239 | validation: 0.4970386096001828]
	TIME [epoch: 8.32 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6707777333193451		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 0.6707777333193451 | validation: 0.4613632909941645]
	TIME [epoch: 8.33 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7547814619545471		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 0.7547814619545471 | validation: 0.44519300344928947]
	TIME [epoch: 8.33 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.588713842316843		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 0.588713842316843 | validation: 0.4847302662803963]
	TIME [epoch: 8.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6307276997828442		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 0.6307276997828442 | validation: 0.3812952359377931]
	TIME [epoch: 8.32 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7458129670403142		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 0.7458129670403142 | validation: 0.44053373776383264]
	TIME [epoch: 8.32 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6997197016646748		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 0.6997197016646748 | validation: 0.3625260276054439]
	TIME [epoch: 8.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6937774255459945		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 0.6937774255459945 | validation: 0.6796637284723279]
	TIME [epoch: 8.33 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7560427897766678		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 0.7560427897766678 | validation: 0.628495666117557]
	TIME [epoch: 8.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6700786460029362		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 0.6700786460029362 | validation: 0.33777182092109004]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5918546228280557		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 0.5918546228280557 | validation: 0.3989365068159035]
	TIME [epoch: 8.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5457530049127494		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 0.5457530049127494 | validation: 0.453408546694837]
	TIME [epoch: 8.32 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6518580819370469		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 0.6518580819370469 | validation: 0.2775067657370806]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.532743161320561		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 0.532743161320561 | validation: 0.2849868478530544]
	TIME [epoch: 8.34 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8404344456264227		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 0.8404344456264227 | validation: 0.7961744428593296]
	TIME [epoch: 8.33 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7284344160952674		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 0.7284344160952674 | validation: 0.46813993585236924]
	TIME [epoch: 8.32 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5282945168469753		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 0.5282945168469753 | validation: 0.3133492099693339]
	TIME [epoch: 8.32 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6243484357960776		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 0.6243484357960776 | validation: 0.3660119366082696]
	TIME [epoch: 8.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6251791665211087		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 0.6251791665211087 | validation: 0.5423808726279149]
	TIME [epoch: 8.32 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6554511325028536		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 0.6554511325028536 | validation: 0.42272049693848646]
	TIME [epoch: 8.32 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6293938673559961		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 0.6293938673559961 | validation: 0.354262394201481]
	TIME [epoch: 8.33 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.616026409414925		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 0.616026409414925 | validation: 0.47576211895060305]
	TIME [epoch: 8.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.580159352524238		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 0.580159352524238 | validation: 0.3671458652526144]
	TIME [epoch: 8.32 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7589753278915563		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 0.7589753278915563 | validation: 0.4541847572566292]
	TIME [epoch: 8.32 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6202570387753866		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 0.6202570387753866 | validation: 0.4163155224976386]
	TIME [epoch: 8.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6460923329046404		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 0.6460923329046404 | validation: 0.3872337683042454]
	TIME [epoch: 8.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6077154588345534		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 0.6077154588345534 | validation: 1.0735145645159268]
	TIME [epoch: 8.32 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6556179544662684		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 0.6556179544662684 | validation: 0.5455120162966387]
	TIME [epoch: 8.32 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.58935955190343		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 0.58935955190343 | validation: 0.5862770959917967]
	TIME [epoch: 8.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7174708791378649		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 0.7174708791378649 | validation: 0.40096168896396933]
	TIME [epoch: 8.32 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.712921275428669		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 0.712921275428669 | validation: 0.6796529893361637]
	TIME [epoch: 8.32 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7574488674848647		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 0.7574488674848647 | validation: 0.5112756010640113]
	TIME [epoch: 8.34 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6006249222978706		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 0.6006249222978706 | validation: 0.6258316402902946]
	TIME [epoch: 8.33 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6278185197976205		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 0.6278185197976205 | validation: 0.3752007219623462]
	TIME [epoch: 8.33 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6298872192957214		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 0.6298872192957214 | validation: 0.5926396934747963]
	TIME [epoch: 8.32 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6136783742809637		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 0.6136783742809637 | validation: 0.33772412358910353]
	TIME [epoch: 8.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5936022012895847		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 0.5936022012895847 | validation: 0.3537461159446618]
	TIME [epoch: 8.33 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6344610542875909		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 0.6344610542875909 | validation: 0.46233159259690615]
	TIME [epoch: 8.32 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6495619225092841		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 0.6495619225092841 | validation: 0.4240127619776489]
	TIME [epoch: 8.32 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5705782912794664		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 0.5705782912794664 | validation: 0.30975279855335835]
	TIME [epoch: 8.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5994436732009467		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 0.5994436732009467 | validation: 0.3966006506210228]
	TIME [epoch: 8.32 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7056872225235228		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 0.7056872225235228 | validation: 0.36974236542688155]
	TIME [epoch: 8.32 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.674013189476053		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 0.674013189476053 | validation: 0.47064018373161226]
	TIME [epoch: 8.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5638765107135624		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 0.5638765107135624 | validation: 0.46067012534138985]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.540638697643327		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 0.540638697643327 | validation: 0.5952898085284529]
	TIME [epoch: 8.32 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5798504472234773		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 0.5798504472234773 | validation: 0.6259838401818041]
	TIME [epoch: 8.33 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6697446495243474		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 0.6697446495243474 | validation: 0.4065966516018104]
	TIME [epoch: 8.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5504204665940744		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 0.5504204665940744 | validation: 0.4108384368086794]
	TIME [epoch: 8.33 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5385536140684892		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 0.5385536140684892 | validation: 0.36326435753892244]
	TIME [epoch: 8.32 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6198694333816258		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 0.6198694333816258 | validation: 0.41572082363857976]
	TIME [epoch: 8.33 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5083223963248387		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 0.5083223963248387 | validation: 0.3045455076613808]
	TIME [epoch: 8.34 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.640246042395197		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 0.640246042395197 | validation: 0.24564965854107823]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5309470830574363		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 0.5309470830574363 | validation: 0.38529549172673866]
	TIME [epoch: 8.32 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7558877390769301		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 0.7558877390769301 | validation: 0.6576991887311447]
	TIME [epoch: 8.33 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.490144795080439		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 0.490144795080439 | validation: 0.2882631727231929]
	TIME [epoch: 8.32 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4895220537133166		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 0.4895220537133166 | validation: 0.5525066620523826]
	TIME [epoch: 8.31 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6474488882903879		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 0.6474488882903879 | validation: 0.45743363044246077]
	TIME [epoch: 8.31 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6671277695384721		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 0.6671277695384721 | validation: 0.40403659777067324]
	TIME [epoch: 8.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5557145341340645		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 0.5557145341340645 | validation: 0.32630033822013405]
	TIME [epoch: 8.31 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.423981961042304		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 0.423981961042304 | validation: 0.5993786121476175]
	TIME [epoch: 8.32 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5145497898430869		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 0.5145497898430869 | validation: 0.7115882391726003]
	TIME [epoch: 8.33 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.726734620801577		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 0.726734620801577 | validation: 0.4618061731677512]
	TIME [epoch: 8.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5307354418840389		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 0.5307354418840389 | validation: 0.3964339357195594]
	TIME [epoch: 8.31 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6262633098281969		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 0.6262633098281969 | validation: 1.258860110834809]
	TIME [epoch: 8.31 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6676207360940543		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 0.6676207360940543 | validation: 0.2708285564931056]
	TIME [epoch: 8.34 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44972522145065713		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 0.44972522145065713 | validation: 0.4359848056789142]
	TIME [epoch: 8.32 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5791938723311725		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 0.5791938723311725 | validation: 0.43038356276738926]
	TIME [epoch: 8.31 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5400885515777883		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 0.5400885515777883 | validation: 2.2080175085749074]
	TIME [epoch: 8.32 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8315694216102386		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 0.8315694216102386 | validation: 0.28667484853950476]
	TIME [epoch: 8.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4957260618841065		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 0.4957260618841065 | validation: 0.690331827286605]
	TIME [epoch: 8.31 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5549009135848056		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 0.5549009135848056 | validation: 0.3164777471317768]
	TIME [epoch: 8.31 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5246755192283351		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 0.5246755192283351 | validation: 0.46354052660605627]
	TIME [epoch: 8.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46506965196951755		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 0.46506965196951755 | validation: 0.30202131090662415]
	TIME [epoch: 8.32 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5194940608105859		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 0.5194940608105859 | validation: 0.530080348817089]
	TIME [epoch: 8.31 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5555230543475763		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 0.5555230543475763 | validation: 0.2681415928583]
	TIME [epoch: 8.31 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4864111821438146		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 0.4864111821438146 | validation: 0.32260881375655104]
	TIME [epoch: 8.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5060408251117491		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 0.5060408251117491 | validation: 0.2661596691603359]
	TIME [epoch: 8.31 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5949398472148751		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 0.5949398472148751 | validation: 0.5758454695989437]
	TIME [epoch: 8.31 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6064759631023753		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 0.6064759631023753 | validation: 0.49520105444692286]
	TIME [epoch: 8.32 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5116573111787928		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 0.5116573111787928 | validation: 0.4224813067960005]
	TIME [epoch: 8.33 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7308299039908184		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 0.7308299039908184 | validation: 0.3752193470775019]
	TIME [epoch: 8.31 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5499139137550364		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 0.5499139137550364 | validation: 0.38578052806894303]
	TIME [epoch: 8.31 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6299194608182899		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 0.6299194608182899 | validation: 0.3063080026170747]
	TIME [epoch: 8.34 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5644860045575615		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 0.5644860045575615 | validation: 0.26279034262485385]
	TIME [epoch: 8.31 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.518597127793433		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 0.518597127793433 | validation: 0.3931704070626274]
	TIME [epoch: 8.31 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5209931835751128		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 0.5209931835751128 | validation: 0.36935500634463986]
	TIME [epoch: 8.31 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4510318915688519		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 0.4510318915688519 | validation: 0.2701611898404886]
	TIME [epoch: 8.34 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4724513367810369		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 0.4724513367810369 | validation: 0.8801669309847315]
	TIME [epoch: 8.32 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6041198347253547		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 0.6041198347253547 | validation: 0.4496965130292818]
	TIME [epoch: 8.31 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5117723233263021		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 0.5117723233263021 | validation: 0.3784853915227461]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5798618560697061		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 0.5798618560697061 | validation: 0.31503885918145125]
	TIME [epoch: 8.32 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44901275803242624		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 0.44901275803242624 | validation: 0.5969630887408142]
	TIME [epoch: 8.31 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4647351989241014		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 0.4647351989241014 | validation: 0.3837438183158972]
	TIME [epoch: 8.31 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49221788008778855		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 0.49221788008778855 | validation: 0.2993710850359989]
	TIME [epoch: 8.34 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4239338715221696		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 0.4239338715221696 | validation: 0.4463986396002681]
	TIME [epoch: 8.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4670231142187884		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 0.4670231142187884 | validation: 0.3141326878361438]
	TIME [epoch: 8.31 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5091876045786861		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 0.5091876045786861 | validation: 0.4744310303856123]
	TIME [epoch: 8.32 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4672773120396174		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 0.4672773120396174 | validation: 0.22021761971975473]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45919062196579646		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 0.45919062196579646 | validation: 0.3791076966943086]
	TIME [epoch: 8.31 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47163404411339743		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 0.47163404411339743 | validation: 0.271533998800804]
	TIME [epoch: 8.31 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0409243957687013		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 1.0409243957687013 | validation: 0.19994744160309158]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4334077989177211		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 0.4334077989177211 | validation: 0.21858193508440374]
	TIME [epoch: 8.32 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42589590358759005		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 0.42589590358759005 | validation: 0.44331833929659115]
	TIME [epoch: 8.31 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4532686102126191		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 0.4532686102126191 | validation: 0.3213690897631141]
	TIME [epoch: 8.31 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4462726576879451		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 0.4462726576879451 | validation: 0.2591322943161199]
	TIME [epoch: 8.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4584089152027463		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 0.4584089152027463 | validation: 0.3813975663392303]
	TIME [epoch: 8.31 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45642811364572256		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 0.45642811364572256 | validation: 0.24618827044893357]
	TIME [epoch: 8.31 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49333478788711355		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 0.49333478788711355 | validation: 0.7397838308025564]
	TIME [epoch: 8.32 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45617333911575486		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 0.45617333911575486 | validation: 0.25569358295407524]
	TIME [epoch: 8.33 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4210244271847712		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 0.4210244271847712 | validation: 0.32887335591900324]
	TIME [epoch: 8.31 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3704398755784174		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 0.3704398755784174 | validation: 0.2866959208480517]
	TIME [epoch: 8.31 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4907394420846751		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 0.4907394420846751 | validation: 0.24875192253379777]
	TIME [epoch: 8.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3720088097723593		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 0.3720088097723593 | validation: 0.38560127944528205]
	TIME [epoch: 8.32 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49830157455908514		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 0.49830157455908514 | validation: 0.3977031659860136]
	TIME [epoch: 8.31 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4591190821855394		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 0.4591190821855394 | validation: 0.2720254957191735]
	TIME [epoch: 8.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3734687567790601		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 0.3734687567790601 | validation: 0.467872303899835]
	TIME [epoch: 8.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49040122034051387		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 0.49040122034051387 | validation: 0.3997580820111015]
	TIME [epoch: 8.31 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4381135259650672		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 0.4381135259650672 | validation: 0.24629854597231804]
	TIME [epoch: 8.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4239505817112863		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 0.4239505817112863 | validation: 0.22703030504014898]
	TIME [epoch: 8.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45744685200264074		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 0.45744685200264074 | validation: 0.4609828062914645]
	TIME [epoch: 8.32 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4518077586965633		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 0.4518077586965633 | validation: 0.28326662459952484]
	TIME [epoch: 8.31 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48583648921798994		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 0.48583648921798994 | validation: 0.533546872716166]
	TIME [epoch: 8.31 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.540187039992011		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 0.540187039992011 | validation: 0.27423699489751735]
	TIME [epoch: 8.34 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4034536997214008		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 0.4034536997214008 | validation: 0.3147858623815787]
	TIME [epoch: 8.31 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48208265765270897		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 0.48208265765270897 | validation: 0.21233019276103277]
	TIME [epoch: 8.31 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4912067001029972		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 0.4912067001029972 | validation: 0.2692130185508037]
	TIME [epoch: 8.32 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3828362087932225		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 0.3828362087932225 | validation: 0.2433139114217846]
	TIME [epoch: 8.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4417052402226225		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 0.4417052402226225 | validation: 0.5221777577471441]
	TIME [epoch: 8.31 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5705980983284954		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 0.5705980983284954 | validation: 0.3509277654226048]
	TIME [epoch: 8.31 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6426477602558004		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 0.6426477602558004 | validation: 0.24304850449293214]
	TIME [epoch: 8.33 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.334499294896999		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 0.334499294896999 | validation: 0.3774665391309806]
	TIME [epoch: 8.33 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39846380745046084		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 0.39846380745046084 | validation: 0.384015626631277]
	TIME [epoch: 8.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3879778922205857		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 0.3879778922205857 | validation: 0.6122667888171454]
	TIME [epoch: 8.32 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46276353901450457		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 0.46276353901450457 | validation: 0.4990721905880135]
	TIME [epoch: 8.33 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4296107674657531		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 0.4296107674657531 | validation: 0.23188111905888686]
	TIME [epoch: 8.31 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4130487662650113		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 0.4130487662650113 | validation: 0.3279892352731509]
	TIME [epoch: 8.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45723752430122555		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 0.45723752430122555 | validation: 0.3886277434904818]
	TIME [epoch: 8.32 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3808988730071015		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 0.3808988730071015 | validation: 0.3172106621653384]
	TIME [epoch: 8.33 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6477026872316252		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 0.6477026872316252 | validation: 0.3118913830374285]
	TIME [epoch: 8.31 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3735485387938192		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 0.3735485387938192 | validation: 0.20817139734480822]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4235716992978742		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 0.4235716992978742 | validation: 0.3122273564318115]
	TIME [epoch: 8.33 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4544774613784887		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 0.4544774613784887 | validation: 0.28290612473306453]
	TIME [epoch: 8.31 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4799771867095857		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 0.4799771867095857 | validation: 0.29041826326232134]
	TIME [epoch: 8.31 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3979315965064689		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 0.3979315965064689 | validation: 0.2244842306990629]
	TIME [epoch: 8.31 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3537161893334998		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 0.3537161893334998 | validation: 0.2646325854757394]
	TIME [epoch: 8.34 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.389735232325736		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 0.389735232325736 | validation: 0.3924839042685059]
	TIME [epoch: 8.31 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37753070579427933		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 0.37753070579427933 | validation: 0.2736756214996128]
	TIME [epoch: 8.31 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34705209830228445		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 0.34705209830228445 | validation: 0.39456978258610775]
	TIME [epoch: 8.32 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4089531304736346		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 0.4089531304736346 | validation: 0.2998174549419991]
	TIME [epoch: 8.33 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35396775662626795		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 0.35396775662626795 | validation: 0.24427899882046913]
	TIME [epoch: 8.31 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42347411158910536		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 0.42347411158910536 | validation: 0.2207616146325505]
	TIME [epoch: 8.31 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30891732276613015		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 0.30891732276613015 | validation: 1.2933055043364003]
	TIME [epoch: 8.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4766853491953861		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 0.4766853491953861 | validation: 0.23575239733835657]
	TIME [epoch: 8.31 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3810066561466012		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 0.3810066561466012 | validation: 0.3147151113613631]
	TIME [epoch: 8.32 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3741004217814317		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 0.3741004217814317 | validation: 0.23664830546113103]
	TIME [epoch: 8.32 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38879415844767984		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 0.38879415844767984 | validation: 0.32317023700336084]
	TIME [epoch: 8.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37853113884434636		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 0.37853113884434636 | validation: 0.1953108516403045]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36366304758528306		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 0.36366304758528306 | validation: 0.20935604017576187]
	TIME [epoch: 8.32 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38800375778467383		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 0.38800375778467383 | validation: 0.31028741917012786]
	TIME [epoch: 8.33 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36002543661803454		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 0.36002543661803454 | validation: 0.1763849131507606]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43055382433742845		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 0.43055382433742845 | validation: 0.3065404737739917]
	TIME [epoch: 8.33 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38127132396202856		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 0.38127132396202856 | validation: 0.2390255021094536]
	TIME [epoch: 8.32 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.344357102897911		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 0.344357102897911 | validation: 0.22934980532462623]
	TIME [epoch: 8.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5976425903673092		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 0.5976425903673092 | validation: 0.39361582279172624]
	TIME [epoch: 8.32 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42988640301389996		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 0.42988640301389996 | validation: 0.1990289907665747]
	TIME [epoch: 8.32 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36779258787439584		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 0.36779258787439584 | validation: 0.2544026834280232]
	TIME [epoch: 8.33 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37314701986403503		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 0.37314701986403503 | validation: 0.3706336040357119]
	TIME [epoch: 8.34 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3586807155202921		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 0.3586807155202921 | validation: 0.21943857726873955]
	TIME [epoch: 8.32 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34504366723091956		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 0.34504366723091956 | validation: 0.24940032985411115]
	TIME [epoch: 8.32 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38508737700363466		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 0.38508737700363466 | validation: 0.3267680034786331]
	TIME [epoch: 8.34 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32285476529608564		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 0.32285476529608564 | validation: 0.4754665406239817]
	TIME [epoch: 8.33 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3832722867056169		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 0.3832722867056169 | validation: 0.25780638184674065]
	TIME [epoch: 8.32 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39526145625922704		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 0.39526145625922704 | validation: 0.26158324024962687]
	TIME [epoch: 8.32 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34854101992130915		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 0.34854101992130915 | validation: 0.211757568817091]
	TIME [epoch: 8.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3343569875315931		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.3343569875315931 | validation: 0.4413761362965311]
	TIME [epoch: 8.32 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3357931366550727		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.3357931366550727 | validation: 0.30139871030882326]
	TIME [epoch: 8.32 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4003067276128317		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.4003067276128317 | validation: 0.20743597221058102]
	TIME [epoch: 8.33 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3535920655306636		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.3535920655306636 | validation: 0.24449923130372214]
	TIME [epoch: 8.33 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4211847573168711		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.4211847573168711 | validation: 0.2976389143978811]
	TIME [epoch: 8.31 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6239477366924349		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.6239477366924349 | validation: 1.342200753569546]
	TIME [epoch: 8.31 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5787694069941384		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.5787694069941384 | validation: 0.30415222259037394]
	TIME [epoch: 8.34 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3203037352307624		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.3203037352307624 | validation: 0.319612165799602]
	TIME [epoch: 8.32 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3030480788798363		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.3030480788798363 | validation: 0.19381553049298572]
	TIME [epoch: 8.31 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3247103550526368		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.3247103550526368 | validation: 0.2663703033167855]
	TIME [epoch: 8.32 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3275754235161305		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.3275754235161305 | validation: 0.2410559925367311]
	TIME [epoch: 8.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3259313876680602		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.3259313876680602 | validation: 0.19889709872211003]
	TIME [epoch: 8.32 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3169851227741075		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.3169851227741075 | validation: 0.27294598145999394]
	TIME [epoch: 8.32 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3412615112377317		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.3412615112377317 | validation: 0.35924716923971667]
	TIME [epoch: 8.34 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5287599145673467		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.5287599145673467 | validation: 0.5166856538581028]
	TIME [epoch: 8.34 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3776836475334379		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.3776836475334379 | validation: 0.23618208256625398]
	TIME [epoch: 8.33 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40651694126737337		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.40651694126737337 | validation: 0.2939839079895816]
	TIME [epoch: 8.33 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5514015080151248		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.5514015080151248 | validation: 0.2572374587885303]
	TIME [epoch: 8.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41712422494112367		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.41712422494112367 | validation: 0.3192056219069354]
	TIME [epoch: 8.33 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34397640728287543		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.34397640728287543 | validation: 0.2469493767226801]
	TIME [epoch: 8.33 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3041385373079877		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.3041385373079877 | validation: 0.23113458899119685]
	TIME [epoch: 8.33 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29680758096794607		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.29680758096794607 | validation: 0.20731649372676594]
	TIME [epoch: 8.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3103186055776782		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.3103186055776782 | validation: 0.2818609548344052]
	TIME [epoch: 8.33 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3299088602010811		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.3299088602010811 | validation: 0.1797211030451555]
	TIME [epoch: 8.32 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3522255856446842		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.3522255856446842 | validation: 0.18356711286595961]
	TIME [epoch: 8.35 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34885495441190895		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.34885495441190895 | validation: 0.21451105006405713]
	TIME [epoch: 8.32 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2820867665035025		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.2820867665035025 | validation: 0.190843430160606]
	TIME [epoch: 8.32 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27052175912248033		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.27052175912248033 | validation: 0.21944809938636106]
	TIME [epoch: 8.32 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29190023297020434		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.29190023297020434 | validation: 0.2009177490431997]
	TIME [epoch: 8.34 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48749084310352114		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.48749084310352114 | validation: 1.3056124603447044]
	TIME [epoch: 8.32 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43488471603981516		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.43488471603981516 | validation: 0.4480566197128897]
	TIME [epoch: 8.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36094996184929823		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.36094996184929823 | validation: 0.1865768488458507]
	TIME [epoch: 8.34 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3057198469053902		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.3057198469053902 | validation: 0.18504554780412008]
	TIME [epoch: 8.32 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3077554900557174		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.3077554900557174 | validation: 0.20555287625295507]
	TIME [epoch: 8.32 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3331051135136546		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.3331051135136546 | validation: 0.21151338615946497]
	TIME [epoch: 8.31 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2676047549938447		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.2676047549938447 | validation: 0.18758708001987556]
	TIME [epoch: 8.35 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39070391013935996		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.39070391013935996 | validation: 0.26505742289262846]
	TIME [epoch: 8.33 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28140140087765453		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.28140140087765453 | validation: 0.4013930474309732]
	TIME [epoch: 8.33 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28341182972806345		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.28341182972806345 | validation: 0.20569932556319553]
	TIME [epoch: 8.32 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41663653036947534		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.41663653036947534 | validation: 0.21232066045254516]
	TIME [epoch: 8.35 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4410232329944189		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.4410232329944189 | validation: 0.20288458692795372]
	TIME [epoch: 8.32 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.271721293247714		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.271721293247714 | validation: 0.17580111181723596]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2852214688231574		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.2852214688231574 | validation: 0.33596892354081537]
	TIME [epoch: 8.34 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33593692087593113		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.33593692087593113 | validation: 0.20653493021447783]
	TIME [epoch: 8.33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2727386100545335		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.2727386100545335 | validation: 0.1745767525508874]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3152474258490951		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.3152474258490951 | validation: 0.21586532444819012]
	TIME [epoch: 8.34 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27046390494917827		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.27046390494917827 | validation: 0.4045980212677647]
	TIME [epoch: 8.36 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3323063714545036		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.3323063714545036 | validation: 0.1767098587924314]
	TIME [epoch: 8.34 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28558315647353105		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.28558315647353105 | validation: 0.21477250051688546]
	TIME [epoch: 8.34 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25629321686707607		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.25629321686707607 | validation: 0.15466091120913344]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3063633274560715		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.3063633274560715 | validation: 0.20729500382716176]
	TIME [epoch: 8.37 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32868303233448026		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.32868303233448026 | validation: 0.29129383707471135]
	TIME [epoch: 8.33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.341454018759509		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.341454018759509 | validation: 0.23816378946899885]
	TIME [epoch: 8.34 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3099384823202046		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.3099384823202046 | validation: 0.21379225117964143]
	TIME [epoch: 8.35 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2859990296596403		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.2859990296596403 | validation: 0.2302549049382767]
	TIME [epoch: 8.35 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2866368968272653		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.2866368968272653 | validation: 0.3534893221944506]
	TIME [epoch: 8.33 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2698265913669443		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.2698265913669443 | validation: 0.24839819754779963]
	TIME [epoch: 8.34 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32275461027969427		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 0.32275461027969427 | validation: 0.42065204749933294]
	TIME [epoch: 8.36 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31217942323028425		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.31217942323028425 | validation: 0.24472264649826603]
	TIME [epoch: 8.34 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2783004652462797		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.2783004652462797 | validation: 0.168175140189964]
	TIME [epoch: 8.33 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3059384519963002		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.3059384519963002 | validation: 0.27823225982343264]
	TIME [epoch: 8.35 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4482644146009155		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.4482644146009155 | validation: 0.467066291108509]
	TIME [epoch: 8.36 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37310183998709257		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.37310183998709257 | validation: 0.22251441084253454]
	TIME [epoch: 8.33 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2880695643253484		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.2880695643253484 | validation: 0.2471350597637038]
	TIME [epoch: 8.33 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3581575454245474		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.3581575454245474 | validation: 0.2302230614885577]
	TIME [epoch: 8.36 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36177954669967566		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.36177954669967566 | validation: 0.1664357387120222]
	TIME [epoch: 8.34 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2828861716929322		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.2828861716929322 | validation: 0.15174156092326674]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45076878760082195		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.45076878760082195 | validation: 0.2969429344923942]
	TIME [epoch: 8.34 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33950570841209105		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.33950570841209105 | validation: 0.27728938512146417]
	TIME [epoch: 8.37 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2974162522227652		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.2974162522227652 | validation: 0.2346796048095105]
	TIME [epoch: 8.34 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29123013249562313		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.29123013249562313 | validation: 0.20749858860037879]
	TIME [epoch: 8.33 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3074916289719567		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.3074916289719567 | validation: 0.20723563255910643]
	TIME [epoch: 8.35 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2322617977958675		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.2322617977958675 | validation: 0.2322803856961405]
	TIME [epoch: 8.34 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31244816741956494		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.31244816741956494 | validation: 0.706769499710822]
	TIME [epoch: 8.34 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32824282500849966		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.32824282500849966 | validation: 0.21464264057168897]
	TIME [epoch: 8.33 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27631743189544666		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.27631743189544666 | validation: 0.259543955662179]
	TIME [epoch: 8.36 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2561739964041154		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.2561739964041154 | validation: 0.7393765020469425]
	TIME [epoch: 8.34 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3771739981025336		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.3771739981025336 | validation: 0.17724179721442276]
	TIME [epoch: 8.33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29599219599263843		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.29599219599263843 | validation: 0.17437408719141018]
	TIME [epoch: 8.34 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2408650387260179		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.2408650387260179 | validation: 0.38293921900222627]
	TIME [epoch: 8.36 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2915576064102603		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.2915576064102603 | validation: 0.18115320455700568]
	TIME [epoch: 8.33 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24832912356631165		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.24832912356631165 | validation: 0.15989904310279607]
	TIME [epoch: 8.33 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26892040982426096		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.26892040982426096 | validation: 0.16610577757191708]
	TIME [epoch: 8.35 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24527500243635747		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.24527500243635747 | validation: 0.46590839563844777]
	TIME [epoch: 8.34 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2778196116009707		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.2778196116009707 | validation: 0.3919561589024354]
	TIME [epoch: 8.33 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26993183489504646		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.26993183489504646 | validation: 0.309624739286149]
	TIME [epoch: 8.33 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2757720106890783		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.2757720106890783 | validation: 0.22491986626842075]
	TIME [epoch: 8.37 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3410204502511568		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.3410204502511568 | validation: 0.5657185172379262]
	TIME [epoch: 8.34 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3313802509706697		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.3313802509706697 | validation: 0.1613905484593242]
	TIME [epoch: 8.34 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21475906751641732		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.21475906751641732 | validation: 1.3596012947433922]
	TIME [epoch: 8.35 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4647593567142663		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.4647593567142663 | validation: 0.20764423604332088]
	TIME [epoch: 8.36 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23226334105043955		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.23226334105043955 | validation: 0.30264296907465593]
	TIME [epoch: 8.34 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27468680261871		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.27468680261871 | validation: 0.2724224131754662]
	TIME [epoch: 8.35 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2801582831914102		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.2801582831914102 | validation: 0.1414931866011201]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2616190628080499		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.2616190628080499 | validation: 0.19097780377732398]
	TIME [epoch: 8.36 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3171114644024222		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.3171114644024222 | validation: 0.3294683391389199]
	TIME [epoch: 8.33 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3358643954322068		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.3358643954322068 | validation: 0.33278682723697306]
	TIME [epoch: 8.33 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27328668893647906		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.27328668893647906 | validation: 0.24242129302236232]
	TIME [epoch: 8.35 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2440476249157047		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.2440476249157047 | validation: 0.15553884415359376]
	TIME [epoch: 8.33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5691181087673781		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.5691181087673781 | validation: 0.641066404848339]
	TIME [epoch: 8.33 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3557197340348207		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.3557197340348207 | validation: 0.26265940737090543]
	TIME [epoch: 8.34 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25606066721522575		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.25606066721522575 | validation: 0.1938831445760988]
	TIME [epoch: 8.33 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2664703684119313		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.2664703684119313 | validation: 0.2688366533456509]
	TIME [epoch: 8.32 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37155292610987783		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.37155292610987783 | validation: 0.37876065789822144]
	TIME [epoch: 8.32 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2650714363395269		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.2650714363395269 | validation: 0.228556612369706]
	TIME [epoch: 8.35 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3772606787574699		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.3772606787574699 | validation: 0.2584662261713224]
	TIME [epoch: 8.33 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23299570295659233		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.23299570295659233 | validation: 0.17237293233875356]
	TIME [epoch: 8.32 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28427368359732613		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.28427368359732613 | validation: 0.8833003627898552]
	TIME [epoch: 8.34 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4182232531154589		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.4182232531154589 | validation: 0.19612099773384806]
	TIME [epoch: 8.35 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23175455577403992		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.23175455577403992 | validation: 0.19305701386711394]
	TIME [epoch: 8.33 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22897496644917664		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.22897496644917664 | validation: 0.25013971382788786]
	TIME [epoch: 8.32 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27222702414623545		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.27222702414623545 | validation: 0.24597350829428835]
	TIME [epoch: 8.35 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33395778097927664		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.33395778097927664 | validation: 0.20432247387205527]
	TIME [epoch: 8.33 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27479757183996356		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.27479757183996356 | validation: 0.17418388579247535]
	TIME [epoch: 8.32 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24227748473021044		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.24227748473021044 | validation: 0.21655194624415375]
	TIME [epoch: 8.33 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2443683557680636		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.2443683557680636 | validation: 0.1850574296889687]
	TIME [epoch: 8.35 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37898507165031103		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.37898507165031103 | validation: 0.3976318210021276]
	TIME [epoch: 8.32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37580809789711245		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 0.37580809789711245 | validation: 0.18401924744680498]
	TIME [epoch: 8.33 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3307914597820402		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.3307914597820402 | validation: 0.5915245129408314]
	TIME [epoch: 8.34 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28107487935305425		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.28107487935305425 | validation: 0.3385504025957137]
	TIME [epoch: 8.33 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24196027500510514		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.24196027500510514 | validation: 0.15843407365491932]
	TIME [epoch: 8.33 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25675250433918745		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.25675250433918745 | validation: 0.17672146730069213]
	TIME [epoch: 8.33 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24935668790013157		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.24935668790013157 | validation: 0.16397180394345234]
	TIME [epoch: 8.35 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23589097849530466		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.23589097849530466 | validation: 0.1955004044088816]
	TIME [epoch: 8.33 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24197868405480366		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.24197868405480366 | validation: 0.1817200670565876]
	TIME [epoch: 8.32 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2603209906072651		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.2603209906072651 | validation: 0.2534994012281205]
	TIME [epoch: 8.32 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2521109484798715		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.2521109484798715 | validation: 0.1586879304193212]
	TIME [epoch: 8.35 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2800566286279265		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.2800566286279265 | validation: 0.3168181059435018]
	TIME [epoch: 8.32 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2575036432644983		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.2575036432644983 | validation: 0.1733411016126336]
	TIME [epoch: 8.33 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2800422051201753		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.2800422051201753 | validation: 0.16684891422846826]
	TIME [epoch: 8.34 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.256549119998603		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.256549119998603 | validation: 0.21467708502701754]
	TIME [epoch: 8.33 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25265010845507535		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.25265010845507535 | validation: 0.18331047103532228]
	TIME [epoch: 8.32 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26675027964366416		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.26675027964366416 | validation: 0.13776467131259656]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28700481058896377		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.28700481058896377 | validation: 0.20931401772689823]
	TIME [epoch: 8.34 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27160827248708125		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.27160827248708125 | validation: 0.17527525782800035]
	TIME [epoch: 8.32 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2505784028934951		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.2505784028934951 | validation: 0.1244443199061501]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23289917869708154		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.23289917869708154 | validation: 0.18104826165108662]
	TIME [epoch: 8.33 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2754291343169962		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.2754291343169962 | validation: 0.16749154574020084]
	TIME [epoch: 8.32 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19795078073748804		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.19795078073748804 | validation: 0.17310686140228915]
	TIME [epoch: 8.32 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2250672356643594		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.2250672356643594 | validation: 0.15953366256416115]
	TIME [epoch: 8.32 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23866306570865206		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.23866306570865206 | validation: 0.1573795679194313]
	TIME [epoch: 8.34 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2909784652510207		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.2909784652510207 | validation: 0.1555577942808815]
	TIME [epoch: 8.32 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2397348268584194		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.2397348268584194 | validation: 0.23024225422051525]
	TIME [epoch: 8.31 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23351262635108766		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.23351262635108766 | validation: 0.17418040065391577]
	TIME [epoch: 8.32 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25072125852345045		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.25072125852345045 | validation: 0.1293029198655857]
	TIME [epoch: 8.34 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22296212250700936		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.22296212250700936 | validation: 0.20535792504135075]
	TIME [epoch: 8.32 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38368533165201774		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.38368533165201774 | validation: 0.25788556444507543]
	TIME [epoch: 8.32 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3468145900110738		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.3468145900110738 | validation: 0.20162903407430297]
	TIME [epoch: 8.34 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27957851927390415		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.27957851927390415 | validation: 0.202504696535121]
	TIME [epoch: 8.33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2940056220492155		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.2940056220492155 | validation: 0.19045371396807576]
	TIME [epoch: 8.32 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23606020690438498		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.23606020690438498 | validation: 0.1793079198365371]
	TIME [epoch: 8.32 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2136376419677286		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.2136376419677286 | validation: 0.2761143355453036]
	TIME [epoch: 8.34 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2101708808964548		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.2101708808964548 | validation: 0.21738441298185252]
	TIME [epoch: 8.32 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2678587950925693		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.2678587950925693 | validation: 0.18272968613449792]
	TIME [epoch: 8.35 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20225156944741568		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.20225156944741568 | validation: 0.1398793831519593]
	TIME [epoch: 8.32 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2122873731963267		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.2122873731963267 | validation: 0.24680441120776217]
	TIME [epoch: 8.34 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24690140187812912		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.24690140187812912 | validation: 0.1364526291003225]
	TIME [epoch: 8.32 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18944707017004553		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.18944707017004553 | validation: 0.11576951030565034]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23941386405286394		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.23941386405286394 | validation: 0.13156668984558062]
	TIME [epoch: 8.34 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2181354433130338		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.2181354433130338 | validation: 0.17962249625328597]
	TIME [epoch: 8.32 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20121570356144006		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.20121570356144006 | validation: 0.15626414449867757]
	TIME [epoch: 8.32 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26881388185454297		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.26881388185454297 | validation: 0.3231533702801005]
	TIME [epoch: 8.32 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5643217207205768		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.5643217207205768 | validation: 0.12272426542116814]
	TIME [epoch: 8.34 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3477759134261372		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.3477759134261372 | validation: 0.12303765178187107]
	TIME [epoch: 8.32 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2172127106182201		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.2172127106182201 | validation: 0.1659531668969555]
	TIME [epoch: 8.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22574567972464155		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.22574567972464155 | validation: 0.2517572805969309]
	TIME [epoch: 8.33 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2113402082337914		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.2113402082337914 | validation: 0.12999545979040206]
	TIME [epoch: 8.32 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19157515903247124		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.19157515903247124 | validation: 0.19546153458377336]
	TIME [epoch: 8.31 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21578247749486462		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.21578247749486462 | validation: 0.10749422622636275]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2476902911108477		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.2476902911108477 | validation: 0.11164720954345411]
	TIME [epoch: 8.35 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17628841618509983		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.17628841618509983 | validation: 0.1344858149461794]
	TIME [epoch: 8.32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2336753419185686		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.2336753419185686 | validation: 0.1800632111572752]
	TIME [epoch: 8.32 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.305594809913764		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.305594809913764 | validation: 0.2559921370124187]
	TIME [epoch: 8.32 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20323659823682455		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.20323659823682455 | validation: 0.1839269309196932]
	TIME [epoch: 8.35 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21648735923612347		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.21648735923612347 | validation: 0.12051681479306275]
	TIME [epoch: 8.33 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28073848191537676		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.28073848191537676 | validation: 0.15520998151760085]
	TIME [epoch: 8.32 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1813553794333829		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.1813553794333829 | validation: 0.1200061653980983]
	TIME [epoch: 8.34 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17559028743234129		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.17559028743234129 | validation: 0.15514123631748672]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43060531960429255		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.43060531960429255 | validation: 0.2302335821596504]
	TIME [epoch: 8.32 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22300707244740697		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.22300707244740697 | validation: 0.17009234859991415]
	TIME [epoch: 8.32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18451612987796934		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.18451612987796934 | validation: 0.12012339749491321]
	TIME [epoch: 8.35 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19402330339337576		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.19402330339337576 | validation: 0.16682119736887394]
	TIME [epoch: 8.33 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20177264081397936		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.20177264081397936 | validation: 0.12322335896797544]
	TIME [epoch: 8.33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2314139714509836		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.2314139714509836 | validation: 0.15726086194854605]
	TIME [epoch: 8.33 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21298324273464578		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.21298324273464578 | validation: 0.11958645258967907]
	TIME [epoch: 8.34 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38416324996781015		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.38416324996781015 | validation: 0.23730199825916837]
	TIME [epoch: 8.32 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26438890062552567		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.26438890062552567 | validation: 0.2810667570965325]
	TIME [epoch: 8.33 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40557627995659107		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.40557627995659107 | validation: 0.2794000447427416]
	TIME [epoch: 8.34 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25221547414366463		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.25221547414366463 | validation: 0.22337580756354536]
	TIME [epoch: 8.33 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2130205791796193		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.2130205791796193 | validation: 0.22489388451056394]
	TIME [epoch: 8.32 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4289163300798591		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.4289163300798591 | validation: 0.17588948131956528]
	TIME [epoch: 8.32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3510516101079		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.3510516101079 | validation: 0.13169038915452105]
	TIME [epoch: 8.34 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2513159194435949		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.2513159194435949 | validation: 0.22794582666059945]
	TIME [epoch: 8.31 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23371126571898113		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.23371126571898113 | validation: 0.2539112912884128]
	TIME [epoch: 8.32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24854469933701578		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.24854469933701578 | validation: 0.11658708089159026]
	TIME [epoch: 8.34 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24791808501899842		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.24791808501899842 | validation: 0.18641774085737978]
	TIME [epoch: 8.33 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24953287043860564		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.24953287043860564 | validation: 0.2829247670546467]
	TIME [epoch: 8.32 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2068847047495171		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.2068847047495171 | validation: 0.1296677821811898]
	TIME [epoch: 8.31 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5221715895962424		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.5221715895962424 | validation: 0.43335793668298767]
	TIME [epoch: 8.34 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2369980523893677		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.2369980523893677 | validation: 0.11602638055438254]
	TIME [epoch: 8.32 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22527167121250663		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.22527167121250663 | validation: 0.14040261685789313]
	TIME [epoch: 8.32 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22027252564851377		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.22027252564851377 | validation: 0.260097978139484]
	TIME [epoch: 8.32 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18233354519109934		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.18233354519109934 | validation: 0.13919974301945529]
	TIME [epoch: 8.34 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21820446431198598		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.21820446431198598 | validation: 0.1732069197148087]
	TIME [epoch: 8.32 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21157924130306333		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.21157924130306333 | validation: 0.12849581954637024]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16897270716638318		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.16897270716638318 | validation: 0.14469011226281897]
	TIME [epoch: 8.34 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21246126238692842		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.21246126238692842 | validation: 0.29187595754282314]
	TIME [epoch: 8.33 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2033993598855472		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.2033993598855472 | validation: 0.2090864669728294]
	TIME [epoch: 8.32 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2236621994730114		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.2236621994730114 | validation: 0.14605654525688075]
	TIME [epoch: 8.32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18127858928723473		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.18127858928723473 | validation: 0.2153176212351537]
	TIME [epoch: 8.34 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18543645978352083		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.18543645978352083 | validation: 0.13647125303231314]
	TIME [epoch: 8.32 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19263204975417253		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.19263204975417253 | validation: 0.13270146447713332]
	TIME [epoch: 8.32 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18181131115075844		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.18181131115075844 | validation: 0.12431527644348878]
	TIME [epoch: 8.33 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26320988692609193		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.26320988692609193 | validation: 0.18062169989125637]
	TIME [epoch: 8.33 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20083713219483804		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.20083713219483804 | validation: 0.17397310733315424]
	TIME [epoch: 8.32 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26040196072363686		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.26040196072363686 | validation: 0.24123216048230967]
	TIME [epoch: 8.32 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23940626423308506		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.23940626423308506 | validation: 0.14217241189708127]
	TIME [epoch: 8.34 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20318095292300214		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.20318095292300214 | validation: 0.1600650210133605]
	TIME [epoch: 8.32 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20064274728870535		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.20064274728870535 | validation: 0.4120779897520196]
	TIME [epoch: 8.32 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2514748426312208		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.2514748426312208 | validation: 0.09952690680157034]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1938028568765735		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.1938028568765735 | validation: 0.12498535267422023]
	TIME [epoch: 8.34 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17138949121445832		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.17138949121445832 | validation: 0.17600051837213726]
	TIME [epoch: 8.32 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22291318854238956		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.22291318854238956 | validation: 0.09524262829545321]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1683275704751207		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.1683275704751207 | validation: 0.09545210221507566]
	TIME [epoch: 8.34 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17079900689128655		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.17079900689128655 | validation: 0.1282009738280092]
	TIME [epoch: 8.33 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2046633234674256		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.2046633234674256 | validation: 0.16395644554449026]
	TIME [epoch: 8.32 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25620891938152407		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.25620891938152407 | validation: 0.4249325808533503]
	TIME [epoch: 8.32 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24695430657559717		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.24695430657559717 | validation: 0.10592651386547676]
	TIME [epoch: 8.34 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1781512591301848		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.1781512591301848 | validation: 0.16743061378601676]
	TIME [epoch: 8.32 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17374352607902602		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.17374352607902602 | validation: 0.17205843853311803]
	TIME [epoch: 8.32 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3032592392382332		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.3032592392382332 | validation: 0.43314722092964797]
	TIME [epoch: 8.33 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29728322071734514		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.29728322071734514 | validation: 0.2121012478876697]
	TIME [epoch: 8.34 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3119316353439407		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.3119316353439407 | validation: 0.19778466583659526]
	TIME [epoch: 8.32 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20079331407545387		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.20079331407545387 | validation: 0.10578043472366062]
	TIME [epoch: 8.32 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1558044726650833		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.1558044726650833 | validation: 0.09786051380557496]
	TIME [epoch: 8.34 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16958629165843234		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.16958629165843234 | validation: 0.11700377356856313]
	TIME [epoch: 8.32 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24382182207826544		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.24382182207826544 | validation: 0.2824108501815761]
	TIME [epoch: 8.31 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22039995739569576		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.22039995739569576 | validation: 0.13336001396467134]
	TIME [epoch: 8.32 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1681870810195936		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.1681870810195936 | validation: 0.10794752089792821]
	TIME [epoch: 8.34 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2952106182458026		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.2952106182458026 | validation: 0.21113728778878083]
	TIME [epoch: 8.32 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20507377581771977		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.20507377581771977 | validation: 0.12784365714715185]
	TIME [epoch: 8.32 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19173967479356963		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.19173967479356963 | validation: 0.2258390827808662]
	TIME [epoch: 8.34 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2675153702137934		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.2675153702137934 | validation: 0.480258104124883]
	TIME [epoch: 8.33 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2682064948585857		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.2682064948585857 | validation: 0.494317788407247]
	TIME [epoch: 8.32 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30206157132261086		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.30206157132261086 | validation: 0.2502319945875627]
	TIME [epoch: 8.32 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1917019681218522		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.1917019681218522 | validation: 0.14212033688193246]
	TIME [epoch: 8.34 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19417842461614881		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.19417842461614881 | validation: 0.1322261095997526]
	TIME [epoch: 8.32 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3721424473104894		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.3721424473104894 | validation: 1.398150189171697]
	TIME [epoch: 8.32 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44630096752049314		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.44630096752049314 | validation: 0.1080508902456081]
	TIME [epoch: 8.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14045351967891967		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.14045351967891967 | validation: 0.09849815832433478]
	TIME [epoch: 8.34 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1777110683196537		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.1777110683196537 | validation: 0.13557335653071023]
	TIME [epoch: 8.32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19247669547713428		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.19247669547713428 | validation: 0.14455144368506176]
	TIME [epoch: 8.32 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2215832180832477		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.2215832180832477 | validation: 0.16734856087225927]
	TIME [epoch: 8.33 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17756872135557758		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.17756872135557758 | validation: 0.12554317639939439]
	TIME [epoch: 8.33 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18954275952498825		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.18954275952498825 | validation: 0.1466825213365211]
	TIME [epoch: 8.32 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20862981717088197		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.20862981717088197 | validation: 0.15209179013377933]
	TIME [epoch: 8.32 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16444489108694738		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.16444489108694738 | validation: 0.13779589358212202]
	TIME [epoch: 8.34 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16589099454498743		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.16589099454498743 | validation: 0.10478513954992315]
	TIME [epoch: 8.33 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30470334351342315		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.30470334351342315 | validation: 0.11409025879639186]
	TIME [epoch: 8.32 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1678972951167123		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.1678972951167123 | validation: 0.18288599024283342]
	TIME [epoch: 8.33 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18075397317353764		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.18075397317353764 | validation: 0.12081149267885047]
	TIME [epoch: 8.34 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16872341803861976		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.16872341803861976 | validation: 0.15877845514497374]
	TIME [epoch: 8.32 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2026558851821279		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.2026558851821279 | validation: 0.16982311717907933]
	TIME [epoch: 8.32 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17940084217128846		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.17940084217128846 | validation: 0.1795349637522554]
	TIME [epoch: 8.35 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18291634929114692		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.18291634929114692 | validation: 0.14149401024603722]
	TIME [epoch: 8.32 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1624361325058678		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.1624361325058678 | validation: 0.13908799303355995]
	TIME [epoch: 8.32 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21733270461944842		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.21733270461944842 | validation: 0.11502113213147272]
	TIME [epoch: 8.32 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17641112855961155		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.17641112855961155 | validation: 0.1117792653819632]
	TIME [epoch: 8.35 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1682125129588353		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.1682125129588353 | validation: 0.13327443807070133]
	TIME [epoch: 8.33 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1599168903863359		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.1599168903863359 | validation: 0.1806098637317794]
	TIME [epoch: 8.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18980012076717423		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.18980012076717423 | validation: 0.1267003040292896]
	TIME [epoch: 8.34 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2103217187566043		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.2103217187566043 | validation: 0.19599118175994024]
	TIME [epoch: 8.33 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2758651425604317		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.2758651425604317 | validation: 0.2644694891158652]
	TIME [epoch: 8.32 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23950656285983207		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.23950656285983207 | validation: 0.14415692334843455]
	TIME [epoch: 8.32 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17179337668181452		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.17179337668181452 | validation: 0.10206063636885981]
	TIME [epoch: 8.35 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1560078587073082		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.1560078587073082 | validation: 0.10532327290522053]
	TIME [epoch: 8.33 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17939632925325383		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.17939632925325383 | validation: 0.10206245335060263]
	TIME [epoch: 8.32 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1983895394482566		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.1983895394482566 | validation: 0.2110329714752474]
	TIME [epoch: 8.32 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19222594079703773		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.19222594079703773 | validation: 0.6829986203144307]
	TIME [epoch: 8.35 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3070573721883122		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.3070573721883122 | validation: 0.25441802890948045]
	TIME [epoch: 8.33 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17956384001965564		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.17956384001965564 | validation: 0.14286144219922486]
	TIME [epoch: 8.32 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15479504780621675		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.15479504780621675 | validation: 0.28136152968617156]
	TIME [epoch: 8.34 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2478817081081987		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.2478817081081987 | validation: 0.11279604063666018]
	TIME [epoch: 8.33 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15726282327647978		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.15726282327647978 | validation: 0.0887538097514271]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17676579503978546		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.17676579503978546 | validation: 0.1777899838939134]
	TIME [epoch: 8.32 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17839727803328614		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.17839727803328614 | validation: 0.11413657309882254]
	TIME [epoch: 8.34 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16716995795125505		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.16716995795125505 | validation: 0.109991486438831]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16610082374603674		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.16610082374603674 | validation: 0.12163808034251683]
	TIME [epoch: 8.32 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19917700329967114		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.19917700329967114 | validation: 0.13938786664818453]
	TIME [epoch: 8.32 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2021053562680731		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.2021053562680731 | validation: 0.1719826007566172]
	TIME [epoch: 8.34 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1621194181600557		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.1621194181600557 | validation: 0.12216339386783931]
	TIME [epoch: 8.32 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2547332178723217		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.2547332178723217 | validation: 0.6550475404727637]
	TIME [epoch: 8.32 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2452746800467039		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.2452746800467039 | validation: 0.14208974511856887]
	TIME [epoch: 8.34 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16183076942650582		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.16183076942650582 | validation: 0.145995757129727]
	TIME [epoch: 8.33 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14861538498701563		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.14861538498701563 | validation: 0.10837816698938227]
	TIME [epoch: 8.32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16955909689277368		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.16955909689277368 | validation: 0.21919411300891584]
	TIME [epoch: 8.31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19792085268102066		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.19792085268102066 | validation: 0.277496548143188]
	TIME [epoch: 8.35 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20820305388251792		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.20820305388251792 | validation: 0.156307137731169]
	TIME [epoch: 8.32 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18525309169164578		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.18525309169164578 | validation: 0.16984391275671176]
	TIME [epoch: 8.31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1529549081554784		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.1529549081554784 | validation: 0.09446154832248846]
	TIME [epoch: 8.33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1868922941600779		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.1868922941600779 | validation: 0.1055143226906747]
	TIME [epoch: 8.33 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17532193269060764		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.17532193269060764 | validation: 0.1288943804419227]
	TIME [epoch: 8.32 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1941305347085		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.1941305347085 | validation: 0.0858773865736713]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16656286565472567		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.16656286565472567 | validation: 0.13647415010061037]
	TIME [epoch: 8.34 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1766242555933091		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.1766242555933091 | validation: 0.12283362242495977]
	TIME [epoch: 8.31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19512455865176798		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.19512455865176798 | validation: 0.15106491788952303]
	TIME [epoch: 8.31 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17321422585951043		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.17321422585951043 | validation: 0.15635848889732823]
	TIME [epoch: 8.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14009302593544737		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.14009302593544737 | validation: 0.1429467973746371]
	TIME [epoch: 8.32 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16059331190228		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.16059331190228 | validation: 0.12419008322778441]
	TIME [epoch: 8.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17086854583342437		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.17086854583342437 | validation: 0.08663393652475633]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17091543434577228		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.17091543434577228 | validation: 0.24361277175882995]
	TIME [epoch: 8.31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19401637335649624		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.19401637335649624 | validation: 0.11511022022575691]
	TIME [epoch: 8.31 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17722338690463585		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.17722338690463585 | validation: 0.09901680790132816]
	TIME [epoch: 8.31 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1594182704110963		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.1594182704110963 | validation: 0.11384378184194041]
	TIME [epoch: 8.31 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.180440640137199		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.180440640137199 | validation: 0.1193004633534476]
	TIME [epoch: 8.33 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1739854563013879		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.1739854563013879 | validation: 0.1082683447410373]
	TIME [epoch: 8.31 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17454201651512358		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.17454201651512358 | validation: 0.12882353130805463]
	TIME [epoch: 8.31 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17393186589500007		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.17393186589500007 | validation: 0.3677683510550316]
	TIME [epoch: 8.31 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17546610067023627		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.17546610067023627 | validation: 0.08776634007051337]
	TIME [epoch: 8.33 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1635120415417586		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.1635120415417586 | validation: 0.12479321703715629]
	TIME [epoch: 8.31 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1748236746200306		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.1748236746200306 | validation: 0.14007193394599185]
	TIME [epoch: 8.31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18174712233394325		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.18174712233394325 | validation: 0.15556803319383716]
	TIME [epoch: 8.32 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18004358235420442		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.18004358235420442 | validation: 0.20303612724449105]
	TIME [epoch: 8.32 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15970278390585313		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.15970278390585313 | validation: 0.1008143016147529]
	TIME [epoch: 8.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16173834464160367		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.16173834464160367 | validation: 0.21644063953911147]
	TIME [epoch: 8.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23987513355329443		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.23987513355329443 | validation: 0.11972093707490326]
	TIME [epoch: 8.33 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18228606777432338		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.18228606777432338 | validation: 0.18146748229059867]
	TIME [epoch: 8.31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13704351567947798		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.13704351567947798 | validation: 0.12379612352276426]
	TIME [epoch: 8.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1681611295251682		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.1681611295251682 | validation: 0.09182230675182888]
	TIME [epoch: 8.32 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15742310230587295		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.15742310230587295 | validation: 0.10443294317719554]
	TIME [epoch: 8.31 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14014277970204567		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.14014277970204567 | validation: 0.15891605410498752]
	TIME [epoch: 8.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1788619564671213		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.1788619564671213 | validation: 0.08093139800206987]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3942892671728287		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.3942892671728287 | validation: 0.7486175400060037]
	TIME [epoch: 8.35 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2259249399882267		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.2259249399882267 | validation: 0.12196548800155863]
	TIME [epoch: 8.34 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16065174902634918		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.16065174902634918 | validation: 0.14031788107882961]
	TIME [epoch: 8.33 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17529705946115798		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.17529705946115798 | validation: 0.261967262151124]
	TIME [epoch: 8.33 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16813231399774728		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.16813231399774728 | validation: 0.09869477865789131]
	TIME [epoch: 8.35 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14909828269004854		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.14909828269004854 | validation: 0.1968408180270823]
	TIME [epoch: 8.33 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.173008050568276		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.173008050568276 | validation: 0.15182648905806373]
	TIME [epoch: 8.33 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.164887317024135		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.164887317024135 | validation: 0.1298690852577968]
	TIME [epoch: 8.34 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15976982316189042		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.15976982316189042 | validation: 0.10029787954639595]
	TIME [epoch: 8.35 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15730180318491865		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.15730180318491865 | validation: 0.15140719259550928]
	TIME [epoch: 8.33 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16451058977079885		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.16451058977079885 | validation: 0.1310273874976224]
	TIME [epoch: 8.33 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1715479233143084		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.1715479233143084 | validation: 0.1968265223876965]
	TIME [epoch: 8.35 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15934419459218466		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.15934419459218466 | validation: 0.10996639716101198]
	TIME [epoch: 8.33 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1523929047397796		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.1523929047397796 | validation: 0.10645781805152957]
	TIME [epoch: 8.33 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15963826277117218		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.15963826277117218 | validation: 0.13988580325793046]
	TIME [epoch: 8.33 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1742817010848655		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.1742817010848655 | validation: 0.11532736579102558]
	TIME [epoch: 8.36 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16375521488408462		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.16375521488408462 | validation: 0.11938960854341651]
	TIME [epoch: 8.33 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14841591712985117		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.14841591712985117 | validation: 0.09706491515229859]
	TIME [epoch: 8.33 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17111148876848897		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.17111148876848897 | validation: 0.10143064044076934]
	TIME [epoch: 8.35 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14402024208214398		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.14402024208214398 | validation: 0.11204928303470363]
	TIME [epoch: 8.34 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1864208792927214		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.1864208792927214 | validation: 0.1370235287651535]
	TIME [epoch: 8.33 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1565582584812491		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.1565582584812491 | validation: 0.11805899079100914]
	TIME [epoch: 8.33 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14221182874191168		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.14221182874191168 | validation: 0.1072139340166594]
	TIME [epoch: 8.35 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13853910368474917		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.13853910368474917 | validation: 0.1943981194548679]
	TIME [epoch: 8.33 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14404458611090082		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.14404458611090082 | validation: 0.10485219542014101]
	TIME [epoch: 8.33 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13044272337115198		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.13044272337115198 | validation: 0.09614186162245163]
	TIME [epoch: 8.33 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14355875796473644		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.14355875796473644 | validation: 0.12811925602245358]
	TIME [epoch: 8.36 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15973915470573027		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.15973915470573027 | validation: 0.10133455247837164]
	TIME [epoch: 8.33 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1499193735943754		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.1499193735943754 | validation: 0.19796781705674144]
	TIME [epoch: 8.33 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2021432948195876		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.2021432948195876 | validation: 0.11225928503469698]
	TIME [epoch: 8.35 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1588727465132933		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.1588727465132933 | validation: 0.15666536541918047]
	TIME [epoch: 8.34 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1536671195238481		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.1536671195238481 | validation: 0.1113714271772806]
	TIME [epoch: 8.33 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18370194140888235		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.18370194140888235 | validation: 0.1363992508752798]
	TIME [epoch: 8.33 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22854131781571724		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.22854131781571724 | validation: 0.19911358827995213]
	TIME [epoch: 8.35 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15654793302189113		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.15654793302189113 | validation: 0.10633287447013265]
	TIME [epoch: 8.34 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13116831347234242		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.13116831347234242 | validation: 0.08660990195304197]
	TIME [epoch: 8.33 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1407134967014985		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.1407134967014985 | validation: 0.10486353250538241]
	TIME [epoch: 8.34 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11791979942125401		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.11791979942125401 | validation: 0.14948625357991746]
	TIME [epoch: 8.36 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14214683154976457		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.14214683154976457 | validation: 0.10939641136160638]
	TIME [epoch: 8.33 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2200903897095688		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.2200903897095688 | validation: 0.14071479135045423]
	TIME [epoch: 8.34 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1367325493649656		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.1367325493649656 | validation: 0.18323502873999226]
	TIME [epoch: 8.36 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1360379132150405		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.1360379132150405 | validation: 0.09922298088068596]
	TIME [epoch: 8.34 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13597227078234345		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.13597227078234345 | validation: 0.09776705282474954]
	TIME [epoch: 8.32 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14417604175591606		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.14417604175591606 | validation: 0.14726226741196463]
	TIME [epoch: 8.33 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14075744544713625		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.14075744544713625 | validation: 0.11134030403461284]
	TIME [epoch: 8.35 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14642286067036778		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.14642286067036778 | validation: 0.09748857231355745]
	TIME [epoch: 8.33 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1401372069870403		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.1401372069870403 | validation: 0.10719344477485337]
	TIME [epoch: 8.33 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1539503523741937		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.1539503523741937 | validation: 0.11861955781391875]
	TIME [epoch: 8.35 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15750833701050035		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.15750833701050035 | validation: 0.43256907011683443]
	TIME [epoch: 8.34 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24111051929198607		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.24111051929198607 | validation: 0.10635808730451705]
	TIME [epoch: 8.33 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14737158062082453		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.14737158062082453 | validation: 0.09502282050372213]
	TIME [epoch: 8.33 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15633154060029833		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.15633154060029833 | validation: 0.11628512854735229]
	TIME [epoch: 8.38 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15769270254897802		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.15769270254897802 | validation: 0.13564973678847153]
	TIME [epoch: 8.33 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14311389277374265		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.14311389277374265 | validation: 0.10790956973694565]
	TIME [epoch: 8.33 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1645005493316753		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.1645005493316753 | validation: 0.12475888048005962]
	TIME [epoch: 8.33 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1825687639426246		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.1825687639426246 | validation: 0.12459320654875143]
	TIME [epoch: 8.36 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14489742927023092		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.14489742927023092 | validation: 0.127706646279097]
	TIME [epoch: 8.33 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1918110564581131		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.1918110564581131 | validation: 0.5958319977395458]
	TIME [epoch: 8.33 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21326621541795682		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.21326621541795682 | validation: 0.21492923551202442]
	TIME [epoch: 8.35 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15460275816207253		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.15460275816207253 | validation: 0.09055636601742746]
	TIME [epoch: 8.34 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15603323762567983		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.15603323762567983 | validation: 0.09592966466061315]
	TIME [epoch: 8.33 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17181054307633617		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.17181054307633617 | validation: 0.27887681265206266]
	TIME [epoch: 8.33 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19688522522877128		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.19688522522877128 | validation: 0.09578004768648875]
	TIME [epoch: 8.36 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1258962749273372		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.1258962749273372 | validation: 0.12381750541965782]
	TIME [epoch: 8.34 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18603592795768503		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.18603592795768503 | validation: 0.1415208669122232]
	TIME [epoch: 8.33 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13151759392995774		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.13151759392995774 | validation: 0.12327477027319314]
	TIME [epoch: 8.34 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19223777538583242		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.19223777538583242 | validation: 0.1567069039271608]
	TIME [epoch: 8.34 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14185440596765134		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.14185440596765134 | validation: 0.13785579738268822]
	TIME [epoch: 8.32 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13067879018603604		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.13067879018603604 | validation: 0.08320296810305666]
	TIME [epoch: 8.33 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11648052732904964		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.11648052732904964 | validation: 0.09424312543721419]
	TIME [epoch: 8.35 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13416715327720807		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.13416715327720807 | validation: 0.16641238022174876]
	TIME [epoch: 8.34 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29959614758075814		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.29959614758075814 | validation: 0.48372456244318934]
	TIME [epoch: 8.33 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18376263015750674		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.18376263015750674 | validation: 0.11290057926988836]
	TIME [epoch: 8.34 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23773347369687153		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.23773347369687153 | validation: 0.6967711282426092]
	TIME [epoch: 8.37 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2149473195544162		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.2149473195544162 | validation: 0.5020843904030879]
	TIME [epoch: 8.33 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25802478341680946		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.25802478341680946 | validation: 0.12411064896861909]
	TIME [epoch: 8.33 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14868371981234327		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.14868371981234327 | validation: 0.1242918541254809]
	TIME [epoch: 8.35 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15170910908743207		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.15170910908743207 | validation: 0.245845902636992]
	TIME [epoch: 8.34 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3291659037665454		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.3291659037665454 | validation: 0.1750327954375473]
	TIME [epoch: 8.34 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1372157461867629		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.1372157461867629 | validation: 0.10743065052915035]
	TIME [epoch: 8.34 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12873426654545014		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.12873426654545014 | validation: 0.10084262412914098]
	TIME [epoch: 8.36 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13000832929625536		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.13000832929625536 | validation: 0.09304485508425149]
	TIME [epoch: 8.33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1336593047799387		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.1336593047799387 | validation: 0.09319907730673879]
	TIME [epoch: 8.34 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14033116346295188		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.14033116346295188 | validation: 0.11009509397446815]
	TIME [epoch: 8.34 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13811162776205563		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.13811162776205563 | validation: 0.15929496016833505]
	TIME [epoch: 8.35 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1759302673464072		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.1759302673464072 | validation: 0.08519519078413118]
	TIME [epoch: 8.34 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16180716074871315		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.16180716074871315 | validation: 0.12689259827923122]
	TIME [epoch: 8.34 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.138913937945187		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.138913937945187 | validation: 0.10224033131427102]
	TIME [epoch: 8.36 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.134817771483774		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.134817771483774 | validation: 0.08887521537062251]
	TIME [epoch: 8.34 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1608141670879296		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.1608141670879296 | validation: 0.10899087707035457]
	TIME [epoch: 8.33 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13687500643405132		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.13687500643405132 | validation: 0.18681952874041804]
	TIME [epoch: 8.34 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13653410207311806		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.13653410207311806 | validation: 0.10796420320306817]
	TIME [epoch: 8.36 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15175062440069928		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.15175062440069928 | validation: 0.14498942150889102]
	TIME [epoch: 8.33 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1540612551610861		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.1540612551610861 | validation: 0.0842112899349507]
	TIME [epoch: 8.33 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14037625473811344		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.14037625473811344 | validation: 0.1352813788034103]
	TIME [epoch: 8.36 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13956657398694397		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.13956657398694397 | validation: 0.12017311227186217]
	TIME [epoch: 8.35 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16137771394700515		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.16137771394700515 | validation: 0.10198218765955741]
	TIME [epoch: 8.33 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11193499810769159		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.11193499810769159 | validation: 0.10033753349898314]
	TIME [epoch: 8.34 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12727432078714473		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.12727432078714473 | validation: 0.1214675316159812]
	TIME [epoch: 8.35 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13268138133130428		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.13268138133130428 | validation: 0.20287982120847756]
	TIME [epoch: 8.33 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1434404462160505		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.1434404462160505 | validation: 0.10099271038976604]
	TIME [epoch: 8.33 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14535679192376832		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.14535679192376832 | validation: 0.16674123024867082]
	TIME [epoch: 8.33 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14208232550186248		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.14208232550186248 | validation: 0.10725438993337887]
	TIME [epoch: 8.35 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4760681969284132		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.4760681969284132 | validation: 0.34519816820664573]
	TIME [epoch: 8.34 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1944228564054867		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.1944228564054867 | validation: 0.10303288672652697]
	TIME [epoch: 8.33 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1556392605677109		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.1556392605677109 | validation: 0.12332933977982563]
	TIME [epoch: 8.35 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14178155994595365		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.14178155994595365 | validation: 0.09624706318931249]
	TIME [epoch: 8.34 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15939967457419274		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.15939967457419274 | validation: 0.19728250309500667]
	TIME [epoch: 8.33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15141189443445302		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.15141189443445302 | validation: 0.11053765547622707]
	TIME [epoch: 8.33 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1376929358232603		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.1376929358232603 | validation: 0.08480567052652903]
	TIME [epoch: 8.35 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14438214646559		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.14438214646559 | validation: 0.11667217330226895]
	TIME [epoch: 8.33 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11097046035313012		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.11097046035313012 | validation: 0.11589398442034457]
	TIME [epoch: 8.33 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.173004822245671		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.173004822245671 | validation: 0.16192437047857278]
	TIME [epoch: 8.34 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16520755020549482		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.16520755020549482 | validation: 0.09815505609890984]
	TIME [epoch: 8.34 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14038213297675514		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.14038213297675514 | validation: 0.12868603905246873]
	TIME [epoch: 8.33 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11754322038020182		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.11754322038020182 | validation: 0.12402894490709959]
	TIME [epoch: 8.33 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14586199251448775		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.14586199251448775 | validation: 0.22637829410046234]
	TIME [epoch: 8.35 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23449591511607343		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.23449591511607343 | validation: 0.13011970833141595]
	TIME [epoch: 8.33 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.175276687897514		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.175276687897514 | validation: 0.14135320882452648]
	TIME [epoch: 8.33 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1417849075090325		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.1417849075090325 | validation: 0.1076860396011183]
	TIME [epoch: 8.34 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14008529085024202		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.14008529085024202 | validation: 0.1019787221206903]
	TIME [epoch: 8.35 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16894659986844487		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.16894659986844487 | validation: 0.2008186368801791]
	TIME [epoch: 8.32 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20819795288242218		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.20819795288242218 | validation: 0.10780155426612967]
	TIME [epoch: 8.33 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1405758811974596		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.1405758811974596 | validation: 0.10436784834113104]
	TIME [epoch: 8.34 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1266262081225364		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.1266262081225364 | validation: 0.09576197817016885]
	TIME [epoch: 8.34 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1294905918424138		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.1294905918424138 | validation: 0.1298832269791264]
	TIME [epoch: 8.32 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15101026592207706		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.15101026592207706 | validation: 0.11350654047223316]
	TIME [epoch: 8.33 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16614580012988628		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.16614580012988628 | validation: 0.11558967670976383]
	TIME [epoch: 8.35 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16815974136821993		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.16815974136821993 | validation: 0.10880245873058947]
	TIME [epoch: 8.33 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14043992725161217		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.14043992725161217 | validation: 0.095230578701919]
	TIME [epoch: 8.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13307569602334485		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.13307569602334485 | validation: 0.12259011414472575]
	TIME [epoch: 8.32 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28072431791682845		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.28072431791682845 | validation: 0.09708916890722276]
	TIME [epoch: 8.35 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13345630111511922		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.13345630111511922 | validation: 0.11269231301055341]
	TIME [epoch: 8.32 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14812653121102465		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.14812653121102465 | validation: 0.0845239326011892]
	TIME [epoch: 8.33 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14384750774072033		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.14384750774072033 | validation: 0.13261673930920218]
	TIME [epoch: 8.34 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15546749826187733		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.15546749826187733 | validation: 0.12011545011543298]
	TIME [epoch: 8.34 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15829077790584764		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.15829077790584764 | validation: 0.09318037521846516]
	TIME [epoch: 8.32 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12878718283967536		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.12878718283967536 | validation: 0.08987654602824816]
	TIME [epoch: 8.32 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1436401863330133		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.1436401863330133 | validation: 0.18243493276089723]
	TIME [epoch: 8.35 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1840865944919798		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.1840865944919798 | validation: 0.12897752584309552]
	TIME [epoch: 8.32 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12456030094697264		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.12456030094697264 | validation: 0.09236010405972239]
	TIME [epoch: 8.32 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12050714237024825		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.12050714237024825 | validation: 0.10588259955896678]
	TIME [epoch: 8.32 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13345202379833315		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.13345202379833315 | validation: 0.13212697732097137]
	TIME [epoch: 8.34 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23942747627932456		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.23942747627932456 | validation: 0.22109687280506424]
	TIME [epoch: 8.32 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20191314560527576		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.20191314560527576 | validation: 0.14428018703717094]
	TIME [epoch: 8.32 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15572216509845987		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.15572216509845987 | validation: 0.11483345080190918]
	TIME [epoch: 8.34 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13743022661964846		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.13743022661964846 | validation: 0.1298707213252881]
	TIME [epoch: 8.32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12427066415582624		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.12427066415582624 | validation: 0.10825580959465662]
	TIME [epoch: 8.32 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11872847284290429		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.11872847284290429 | validation: 0.14185990472330517]
	TIME [epoch: 8.32 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14444662567821442		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.14444662567821442 | validation: 0.11089236375484246]
	TIME [epoch: 8.34 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12859008996125032		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.12859008996125032 | validation: 0.13978157782513706]
	TIME [epoch: 8.32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14945461897598247		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.14945461897598247 | validation: 0.12062839379221788]
	TIME [epoch: 8.31 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14451114379882413		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.14451114379882413 | validation: 0.11739492356044569]
	TIME [epoch: 8.33 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13753441917826298		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.13753441917826298 | validation: 0.1334872045671232]
	TIME [epoch: 8.33 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13594720099571272		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.13594720099571272 | validation: 0.1283799520075832]
	TIME [epoch: 8.31 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1336588411205767		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.1336588411205767 | validation: 0.08518605237877416]
	TIME [epoch: 8.32 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1298258712759786		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.1298258712759786 | validation: 0.09606619102716851]
	TIME [epoch: 8.35 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12456618317549562		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.12456618317549562 | validation: 0.11992521163830523]
	TIME [epoch: 8.32 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13619242272756968		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.13619242272756968 | validation: 0.09553859644057097]
	TIME [epoch: 8.32 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1424954043713847		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.1424954043713847 | validation: 0.21877802399949203]
	TIME [epoch: 8.32 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32705723200730713		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.32705723200730713 | validation: 0.33859353506568796]
	TIME [epoch: 8.35 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24017702566737942		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.24017702566737942 | validation: 0.12290090126435048]
	TIME [epoch: 8.32 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2192550346245934		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.2192550346245934 | validation: 0.27675230239098053]
	TIME [epoch: 8.32 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24260367868163013		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.24260367868163013 | validation: 0.2021644541506501]
	TIME [epoch: 8.34 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2601961574330719		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.2601961574330719 | validation: 0.21485045058934596]
	TIME [epoch: 8.34 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19177099597420988		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.19177099597420988 | validation: 0.16506146090238774]
	TIME [epoch: 8.33 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35942673073739495		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.35942673073739495 | validation: 0.40065616773214907]
	TIME [epoch: 8.32 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2174699359263112		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.2174699359263112 | validation: 0.25491381662571905]
	TIME [epoch: 8.35 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15550302761814302		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.15550302761814302 | validation: 0.11008374689869219]
	TIME [epoch: 8.32 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2516137628385861		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.2516137628385861 | validation: 0.673029011649712]
	TIME [epoch: 8.32 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28393759143547276		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.28393759143547276 | validation: 0.1354085631315535]
	TIME [epoch: 8.33 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17522155498036965		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.17522155498036965 | validation: 0.1593565117360633]
	TIME [epoch: 8.34 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12334681158080811		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.12334681158080811 | validation: 0.08968988796198807]
	TIME [epoch: 8.32 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14765742023045567		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.14765742023045567 | validation: 0.23669364588597594]
	TIME [epoch: 8.32 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15465105610226132		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.15465105610226132 | validation: 0.14906206046463588]
	TIME [epoch: 8.34 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19794616250779135		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.19794616250779135 | validation: 0.16147206219206894]
	TIME [epoch: 8.33 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21008977410752427		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.21008977410752427 | validation: 0.1461863228192174]
	TIME [epoch: 8.32 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.147133900675928		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.147133900675928 | validation: 0.104038664168461]
	TIME [epoch: 8.33 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11459614071171753		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.11459614071171753 | validation: 0.11058989560820442]
	TIME [epoch: 8.35 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18211368814347298		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.18211368814347298 | validation: 0.16072894694027018]
	TIME [epoch: 8.32 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15089778300697834		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.15089778300697834 | validation: 0.10435949480508847]
	TIME [epoch: 8.32 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1413833080493988		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.1413833080493988 | validation: 0.09624000028116847]
	TIME [epoch: 8.34 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1509387734266891		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.1509387734266891 | validation: 0.09648416349794567]
	TIME [epoch: 8.33 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13606290010290117		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.13606290010290117 | validation: 0.09829435988666657]
	TIME [epoch: 8.32 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1303261974967754		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.1303261974967754 | validation: 0.1054535610487835]
	TIME [epoch: 8.33 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13399404273368745		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.13399404273368745 | validation: 0.09126312048301843]
	TIME [epoch: 8.35 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16431890297436696		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.16431890297436696 | validation: 0.15648255097645458]
	TIME [epoch: 8.33 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12426147305106036		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.12426147305106036 | validation: 0.1142139231289935]
	TIME [epoch: 8.32 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1275401544597773		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.1275401544597773 | validation: 0.12721017572060714]
	TIME [epoch: 8.32 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12208506699705582		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.12208506699705582 | validation: 0.09917934563185989]
	TIME [epoch: 8.34 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1256226159029659		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.1256226159029659 | validation: 0.09571475762289519]
	TIME [epoch: 8.32 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1358906380311433		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.1358906380311433 | validation: 0.09669260555145814]
	TIME [epoch: 8.32 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13411587876895525		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.13411587876895525 | validation: 0.10492710074182295]
	TIME [epoch: 8.34 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14664091032420207		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.14664091032420207 | validation: 0.11141920369703566]
	TIME [epoch: 8.33 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15027654246117722		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.15027654246117722 | validation: 0.12921895144646076]
	TIME [epoch: 8.32 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1524116093060831		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.1524116093060831 | validation: 0.0912669102448358]
	TIME [epoch: 8.32 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13338851983057204		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.13338851983057204 | validation: 0.08978347460672687]
	TIME [epoch: 8.34 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12714710955119707		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.12714710955119707 | validation: 0.11018195459459332]
	TIME [epoch: 8.33 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2476664660912633		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.2476664660912633 | validation: 0.13913999582239187]
	TIME [epoch: 8.32 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15756659619585262		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.15756659619585262 | validation: 0.1421732724880164]
	TIME [epoch: 8.34 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13386373348728378		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.13386373348728378 | validation: 0.08942031932704134]
	TIME [epoch: 8.34 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12381376233224022		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.12381376233224022 | validation: 0.09579712685868882]
	TIME [epoch: 8.33 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15063193920338078		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.15063193920338078 | validation: 0.25650582039876424]
	TIME [epoch: 8.33 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16272472129453455		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.16272472129453455 | validation: 0.26146741655333267]
	TIME [epoch: 8.35 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1425430859481999		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.1425430859481999 | validation: 0.23368772504854593]
	TIME [epoch: 8.32 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15265676385124094		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.15265676385124094 | validation: 0.09172865195864047]
	TIME [epoch: 8.31 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11909025205321433		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.11909025205321433 | validation: 0.1047015609122363]
	TIME [epoch: 8.32 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11271910661892728		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.11271910661892728 | validation: 0.22896268536692793]
	TIME [epoch: 8.34 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14286553581994862		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.14286553581994862 | validation: 0.2111051442438875]
	TIME [epoch: 8.32 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15171347733328264		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.15171347733328264 | validation: 0.24938899507133183]
	TIME [epoch: 8.32 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1557984410293275		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.1557984410293275 | validation: 0.1806019853974247]
	TIME [epoch: 8.34 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1291529364796839		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.1291529364796839 | validation: 0.10140121437941765]
	TIME [epoch: 8.33 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.121291746883712		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.121291746883712 | validation: 0.11803839911425662]
	TIME [epoch: 8.32 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12386531488411193		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.12386531488411193 | validation: 0.11435842516650997]
	TIME [epoch: 8.32 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18644059181873412		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.18644059181873412 | validation: 0.2951226244612104]
	TIME [epoch: 8.34 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13860962927821038		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.13860962927821038 | validation: 0.10497251084206329]
	TIME [epoch: 8.32 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1356348299611062		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.1356348299611062 | validation: 0.17985897437837906]
	TIME [epoch: 8.32 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.128040414526004		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.128040414526004 | validation: 0.11857116720959843]
	TIME [epoch: 8.32 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12974640818478939		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.12974640818478939 | validation: 0.0925703546886859]
	TIME [epoch: 8.34 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12792772787576695		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.12792772787576695 | validation: 0.0928268878685399]
	TIME [epoch: 8.32 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10753230179929418		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.10753230179929418 | validation: 0.10223609135049479]
	TIME [epoch: 8.32 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12050197973167003		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.12050197973167003 | validation: 0.09436007389565577]
	TIME [epoch: 8.33 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12180642592849263		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.12180642592849263 | validation: 0.09635368293012958]
	TIME [epoch: 8.33 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12239275186065919		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.12239275186065919 | validation: 0.1136685600816449]
	TIME [epoch: 8.32 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11214458337306513		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.11214458337306513 | validation: 0.09337134123745768]
	TIME [epoch: 8.32 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1303373492242421		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.1303373492242421 | validation: 0.12418612097765719]
	TIME [epoch: 8.34 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13249569537143557		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.13249569537143557 | validation: 0.11893222904792253]
	TIME [epoch: 8.32 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12683364453875567		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.12683364453875567 | validation: 0.09877928121931201]
	TIME [epoch: 8.32 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14213373314192207		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.14213373314192207 | validation: 0.10218242684246258]
	TIME [epoch: 8.33 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1295411652006655		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.1295411652006655 | validation: 0.08819952068299022]
	TIME [epoch: 8.34 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13450045638951452		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.13450045638951452 | validation: 0.1268655117743079]
	TIME [epoch: 8.32 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13364686635074507		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.13364686635074507 | validation: 0.10298818711524488]
	TIME [epoch: 8.32 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12180379276592232		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.12180379276592232 | validation: 0.0967478113264173]
	TIME [epoch: 8.34 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12786389615990654		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.12786389615990654 | validation: 0.09931477705443195]
	TIME [epoch: 8.32 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13133066544877528		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.13133066544877528 | validation: 0.0948604810728609]
	TIME [epoch: 8.32 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10723797596381417		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.10723797596381417 | validation: 0.08514813870873886]
	TIME [epoch: 8.32 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13081591128522907		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.13081591128522907 | validation: 0.10224055471321242]
	TIME [epoch: 8.34 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14042328600881537		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.14042328600881537 | validation: 0.11457882026877947]
	TIME [epoch: 8.32 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11580108580892186		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.11580108580892186 | validation: 0.10063357386996744]
	TIME [epoch: 8.32 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10867516028725185		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.10867516028725185 | validation: 0.07880834301903478]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1050.pth
	Model improved!!!
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11422902497181839		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.11422902497181839 | validation: 0.11594982327337668]
	TIME [epoch: 8.32 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11187455220592668		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.11187455220592668 | validation: 0.10199045560918224]
	TIME [epoch: 8.31 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11911223595515272		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.11911223595515272 | validation: 0.1047081626288818]
	TIME [epoch: 8.31 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1315302833038819		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.1315302833038819 | validation: 0.18816547916771842]
	TIME [epoch: 8.33 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13341549291323507		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.13341549291323507 | validation: 0.11025040934297661]
	TIME [epoch: 8.31 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11725534769618902		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.11725534769618902 | validation: 0.1097272881775827]
	TIME [epoch: 8.31 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12058059717162342		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.12058059717162342 | validation: 0.09735183554864726]
	TIME [epoch: 8.31 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16865985873702444		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.16865985873702444 | validation: 0.0945090237244608]
	TIME [epoch: 8.33 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10940111170402553		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.10940111170402553 | validation: 0.08024731967962231]
	TIME [epoch: 8.31 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11101055148838186		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.11101055148838186 | validation: 0.17581647227717126]
	TIME [epoch: 8.31 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11832396244017114		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.11832396244017114 | validation: 0.12494692821602973]
	TIME [epoch: 8.33 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11946443825635997		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.11946443825635997 | validation: 0.09020729109983985]
	TIME [epoch: 8.31 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11355747632617826		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.11355747632617826 | validation: 0.10430162075078928]
	TIME [epoch: 8.31 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10566429695581325		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.10566429695581325 | validation: 0.08569762334838228]
	TIME [epoch: 8.31 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10465396782904382		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.10465396782904382 | validation: 0.08531208466253248]
	TIME [epoch: 8.33 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1296255710205421		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.1296255710205421 | validation: 0.13846228731118657]
	TIME [epoch: 8.31 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1376814972404232		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.1376814972404232 | validation: 0.12053875260068586]
	TIME [epoch: 8.31 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12944172154753328		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.12944172154753328 | validation: 0.1206556880610696]
	TIME [epoch: 8.32 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12677895220206578		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.12677895220206578 | validation: 0.09376445674667856]
	TIME [epoch: 8.32 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12767144010060888		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.12767144010060888 | validation: 0.116057161519242]
	TIME [epoch: 8.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12702822336926234		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.12702822336926234 | validation: 0.10306235130440575]
	TIME [epoch: 8.31 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348974778055401		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.1348974778055401 | validation: 0.09862896507161326]
	TIME [epoch: 8.33 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13358796832701167		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.13358796832701167 | validation: 0.11380815215318418]
	TIME [epoch: 8.31 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11584660462046381		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.11584660462046381 | validation: 0.10992249822607088]
	TIME [epoch: 8.31 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10831476221064448		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.10831476221064448 | validation: 0.08840066588740417]
	TIME [epoch: 8.32 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10094375305727428		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.10094375305727428 | validation: 0.09431765220371098]
	TIME [epoch: 8.34 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11858622911839703		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.11858622911839703 | validation: 0.09822851325187384]
	TIME [epoch: 8.32 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11411357747894997		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.11411357747894997 | validation: 0.09540002876637821]
	TIME [epoch: 8.31 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12289894404702655		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.12289894404702655 | validation: 0.08366785644181747]
	TIME [epoch: 8.33 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10637091509903826		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.10637091509903826 | validation: 0.0791436641835936]
	TIME [epoch: 8.32 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11061927218548602		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.11061927218548602 | validation: 0.10048116935588695]
	TIME [epoch: 8.32 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14074582317022655		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.14074582317022655 | validation: 0.1617434025168602]
	TIME [epoch: 8.31 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12871706748348316		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.12871706748348316 | validation: 0.09612813435065332]
	TIME [epoch: 8.34 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12339889674371926		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.12339889674371926 | validation: 0.18802697466409143]
	TIME [epoch: 8.32 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34114311073248815		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.34114311073248815 | validation: 0.1665713076315068]
	TIME [epoch: 8.31 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12773139616177703		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.12773139616177703 | validation: 0.16141525591240102]
	TIME [epoch: 8.32 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13428501939026044		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.13428501939026044 | validation: 0.08847309814431471]
	TIME [epoch: 8.34 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10689027483042587		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.10689027483042587 | validation: 0.08534914760928528]
	TIME [epoch: 8.32 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12788617812920372		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.12788617812920372 | validation: 0.16645460208771617]
	TIME [epoch: 8.32 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12295234439407857		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.12295234439407857 | validation: 0.08164517939612131]
	TIME [epoch: 8.34 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12488006534696985		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.12488006534696985 | validation: 0.2673566344375628]
	TIME [epoch: 8.32 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15115752561353438		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.15115752561353438 | validation: 0.14096296133141206]
	TIME [epoch: 8.32 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19711894914307093		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.19711894914307093 | validation: 0.16385230457682762]
	TIME [epoch: 8.32 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16474530776590507		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.16474530776590507 | validation: 0.21175281729014847]
	TIME [epoch: 8.34 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15113091891320724		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.15113091891320724 | validation: 0.11963563738386865]
	TIME [epoch: 8.32 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14993087214024353		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.14993087214024353 | validation: 0.10129287762710033]
	TIME [epoch: 8.32 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16615009211492163		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.16615009211492163 | validation: 0.20080829946936526]
	TIME [epoch: 8.33 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13107132715116088		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.13107132715116088 | validation: 0.11640429934265861]
	TIME [epoch: 8.33 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13030348571476827		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.13030348571476827 | validation: 0.08209864859371481]
	TIME [epoch: 8.32 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12004966159397253		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.12004966159397253 | validation: 0.09995080337411158]
	TIME [epoch: 8.32 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10658959089884698		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.10658959089884698 | validation: 0.0954921054749544]
	TIME [epoch: 8.34 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11582356443061173		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.11582356443061173 | validation: 0.10062728926327846]
	TIME [epoch: 8.32 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1267536427217686		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.1267536427217686 | validation: 0.11389540123693923]
	TIME [epoch: 8.31 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11562401627012801		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.11562401627012801 | validation: 0.09005804819207175]
	TIME [epoch: 8.31 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10657996834379597		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.10657996834379597 | validation: 0.07445063784029635]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1105.pth
	Model improved!!!
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1132987483235082		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.1132987483235082 | validation: 0.08055104478629281]
	TIME [epoch: 8.32 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10493344444783866		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.10493344444783866 | validation: 0.09807058172684985]
	TIME [epoch: 8.32 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11001775640673594		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.11001775640673594 | validation: 0.08606126516481907]
	TIME [epoch: 8.33 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09756081577389442		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.09756081577389442 | validation: 0.0793787114162372]
	TIME [epoch: 8.33 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1225588825884176		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.1225588825884176 | validation: 0.10315627209923178]
	TIME [epoch: 8.32 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12375354061914097		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.12375354061914097 | validation: 0.08988668618018009]
	TIME [epoch: 8.32 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11677372523435996		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.11677372523435996 | validation: 0.11400088751926127]
	TIME [epoch: 8.34 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11443921757068361		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.11443921757068361 | validation: 0.11829917774637448]
	TIME [epoch: 8.32 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.126465165178832		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.126465165178832 | validation: 0.09907906855731702]
	TIME [epoch: 8.32 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11501358699004974		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.11501358699004974 | validation: 0.0820869503788138]
	TIME [epoch: 8.32 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13646487061802576		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.13646487061802576 | validation: 0.1296588532010592]
	TIME [epoch: 8.33 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12878125438434102		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.12878125438434102 | validation: 0.1082656507767715]
	TIME [epoch: 8.32 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11140785629388845		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.11140785629388845 | validation: 0.08560884707134461]
	TIME [epoch: 8.32 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11384272127874888		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.11384272127874888 | validation: 0.08605255972015294]
	TIME [epoch: 8.34 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10710055284990412		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.10710055284990412 | validation: 0.11352300432483198]
	TIME [epoch: 8.32 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15534773376127448		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.15534773376127448 | validation: 0.12284939309305723]
	TIME [epoch: 8.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14399477595375493		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.14399477595375493 | validation: 0.11077282404413893]
	TIME [epoch: 8.32 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15654344002136078		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.15654344002136078 | validation: 0.11639032905479647]
	TIME [epoch: 8.34 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348758492511984		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.1348758492511984 | validation: 0.10445622090508161]
	TIME [epoch: 8.31 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12620226978278742		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.12620226978278742 | validation: 0.10083276994565415]
	TIME [epoch: 8.31 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12763834121326573		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.12763834121326573 | validation: 0.09072771767291037]
	TIME [epoch: 8.33 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11664566467026702		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.11664566467026702 | validation: 0.09434707510795012]
	TIME [epoch: 8.33 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1485212454855603		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.1485212454855603 | validation: 0.1444358898296654]
	TIME [epoch: 8.32 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12764806316960042		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.12764806316960042 | validation: 0.09382578596408905]
	TIME [epoch: 8.31 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1129515637694217		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.1129515637694217 | validation: 0.09227413855740374]
	TIME [epoch: 8.34 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10908643757391916		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.10908643757391916 | validation: 0.17074060006596642]
	TIME [epoch: 8.32 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12985808924827524		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.12985808924827524 | validation: 0.08136546978555906]
	TIME [epoch: 8.32 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11816580749978026		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.11816580749978026 | validation: 0.09041451151865529]
	TIME [epoch: 8.32 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11101586661802747		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.11101586661802747 | validation: 0.23585302191242713]
	TIME [epoch: 8.34 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1741471455862066		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.1741471455862066 | validation: 0.09502810726541705]
	TIME [epoch: 8.31 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10340245213926336		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.10340245213926336 | validation: 0.0938823659127124]
	TIME [epoch: 8.32 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09998421423663975		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.09998421423663975 | validation: 0.08618624596571384]
	TIME [epoch: 8.33 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10864990997322226		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.10864990997322226 | validation: 0.10109299259152622]
	TIME [epoch: 8.33 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12712886913100724		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.12712886913100724 | validation: 0.08656794992415644]
	TIME [epoch: 8.32 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10398485163860616		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.10398485163860616 | validation: 0.10754242438699557]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10199193743743243		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.10199193743743243 | validation: 0.10506130590097817]
	TIME [epoch: 8.34 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1246454959550399		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.1246454959550399 | validation: 0.09792163929262228]
	TIME [epoch: 8.32 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12638715813299153		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.12638715813299153 | validation: 0.1396719938079217]
	TIME [epoch: 8.32 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11359067082155141		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.11359067082155141 | validation: 0.1042068558411326]
	TIME [epoch: 8.32 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13176506513186054		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.13176506513186054 | validation: 0.14554483751513225]
	TIME [epoch: 8.33 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12396658002564538		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.12396658002564538 | validation: 0.06658192066478938]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1146.pth
	Model improved!!!
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10682379948631748		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.10682379948631748 | validation: 0.14540798570179708]
	TIME [epoch: 8.32 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1398319450391582		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.1398319450391582 | validation: 0.14558804096160785]
	TIME [epoch: 8.34 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1200973982902828		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.1200973982902828 | validation: 0.08678146716073663]
	TIME [epoch: 8.32 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10962830186406444		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.10962830186406444 | validation: 0.09694481463869409]
	TIME [epoch: 8.32 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10069690013568473		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.10069690013568473 | validation: 0.08134443788595416]
	TIME [epoch: 8.31 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1026187988787333		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.1026187988787333 | validation: 0.09528421520193661]
	TIME [epoch: 8.34 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11878014516682231		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.11878014516682231 | validation: 0.09759945336376978]
	TIME [epoch: 8.32 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10156131910463984		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.10156131910463984 | validation: 0.08248909615696598]
	TIME [epoch: 8.31 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10064203162062202		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.10064203162062202 | validation: 0.1159523472988853]
	TIME [epoch: 8.33 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1019706650022572		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.1019706650022572 | validation: 0.07895164679468897]
	TIME [epoch: 8.32 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11373410412493586		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.11373410412493586 | validation: 0.09658754422542803]
	TIME [epoch: 8.31 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11724123259155048		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.11724123259155048 | validation: 0.09960885026789973]
	TIME [epoch: 8.31 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11282123606421395		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.11282123606421395 | validation: 0.082272119823586]
	TIME [epoch: 8.33 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10827319046716635		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.10827319046716635 | validation: 0.0997392413366212]
	TIME [epoch: 8.31 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11764714400069501		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.11764714400069501 | validation: 0.09351578159055501]
	TIME [epoch: 8.31 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10645639874085386		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.10645639874085386 | validation: 0.09100052871732252]
	TIME [epoch: 8.31 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09864010099033434		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.09864010099033434 | validation: 0.08850214066344947]
	TIME [epoch: 8.33 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1080893143794259		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.1080893143794259 | validation: 0.09929074406603691]
	TIME [epoch: 8.31 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11571614492748519		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.11571614492748519 | validation: 0.07919931733077987]
	TIME [epoch: 8.31 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13734064122632986		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.13734064122632986 | validation: 0.07776148661719079]
	TIME [epoch: 8.33 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10884456526198064		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.10884456526198064 | validation: 0.0846019145343544]
	TIME [epoch: 8.32 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1132347160499875		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.1132347160499875 | validation: 0.0832252335393863]
	TIME [epoch: 8.31 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11835648114260319		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.11835648114260319 | validation: 0.09849288382810567]
	TIME [epoch: 8.31 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11487904212735978		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.11487904212735978 | validation: 0.10362971581729052]
	TIME [epoch: 8.33 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12371916781162277		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.12371916781162277 | validation: 0.08924261172226458]
	TIME [epoch: 8.31 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1141388106055889		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.1141388106055889 | validation: 0.08716280659966061]
	TIME [epoch: 8.31 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10574600640373419		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.10574600640373419 | validation: 0.0831356099795748]
	TIME [epoch: 8.32 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12114153880651853		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.12114153880651853 | validation: 0.1598202075242411]
	TIME [epoch: 8.33 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13592420709232406		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.13592420709232406 | validation: 0.10952547172057867]
	TIME [epoch: 8.31 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10610590654810799		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.10610590654810799 | validation: 0.0794279389481288]
	TIME [epoch: 8.31 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10301984966763107		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.10301984966763107 | validation: 0.12473065750507778]
	TIME [epoch: 8.33 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12444536051107705		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.12444536051107705 | validation: 0.09364071227744677]
	TIME [epoch: 8.32 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10884270231114143		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.10884270231114143 | validation: 0.08493516596574074]
	TIME [epoch: 8.31 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.108314314168427		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.108314314168427 | validation: 0.08531773318152841]
	TIME [epoch: 8.31 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10478334031380947		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.10478334031380947 | validation: 0.0771995345210243]
	TIME [epoch: 8.33 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1382902748530205		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.1382902748530205 | validation: 0.0995066644981375]
	TIME [epoch: 8.32 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13109183470633395		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.13109183470633395 | validation: 0.09304443851304631]
	TIME [epoch: 8.32 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11398036860244025		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.11398036860244025 | validation: 0.09368986560775594]
	TIME [epoch: 8.33 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10381092331710477		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.10381092331710477 | validation: 0.09334293711968214]
	TIME [epoch: 8.32 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11224757557409056		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.11224757557409056 | validation: 0.08549629087054972]
	TIME [epoch: 8.31 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10144203920770949		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.10144203920770949 | validation: 0.13745813967902434]
	TIME [epoch: 8.31 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12440477438636		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.12440477438636 | validation: 0.08068714522621954]
	TIME [epoch: 8.34 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10998006418054426		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.10998006418054426 | validation: 0.09381812597776412]
	TIME [epoch: 8.32 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13179306690941547		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.13179306690941547 | validation: 0.1029818116715224]
	TIME [epoch: 8.31 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11995425858400063		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.11995425858400063 | validation: 0.10294940565531019]
	TIME [epoch: 8.31 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.121731131625643		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.121731131625643 | validation: 0.08676440256399162]
	TIME [epoch: 8.33 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11736317847234701		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.11736317847234701 | validation: 0.10435000529516754]
	TIME [epoch: 8.31 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11094892104427427		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.11094892104427427 | validation: 0.09224941298012547]
	TIME [epoch: 8.31 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12355195610580341		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.12355195610580341 | validation: 0.10359029019995022]
	TIME [epoch: 8.33 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11136122222893423		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.11136122222893423 | validation: 0.08748074911473983]
	TIME [epoch: 8.32 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12032976684408694		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.12032976684408694 | validation: 0.09513004139342345]
	TIME [epoch: 8.32 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11590815329184137		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.11590815329184137 | validation: 0.0983269370678081]
	TIME [epoch: 8.32 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11904645955576472		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.11904645955576472 | validation: 0.11065629492797584]
	TIME [epoch: 8.33 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1237624145507819		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.1237624145507819 | validation: 0.11027694507927913]
	TIME [epoch: 8.31 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11275806023865778		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.11275806023865778 | validation: 0.12816378815682558]
	TIME [epoch: 8.31 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10626246008384796		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.10626246008384796 | validation: 0.06591245294713333]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1202.pth
	Model improved!!!
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09904996559268438		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.09904996559268438 | validation: 0.07762446409387555]
	TIME [epoch: 8.33 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10332103605329102		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.10332103605329102 | validation: 0.08614718026393682]
	TIME [epoch: 8.31 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09903527199070927		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.09903527199070927 | validation: 0.09382817929579346]
	TIME [epoch: 8.31 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10583008459106454		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.10583008459106454 | validation: 0.08825612481814071]
	TIME [epoch: 8.33 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1113009738283389		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.1113009738283389 | validation: 0.0804138234045322]
	TIME [epoch: 8.31 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11471934276137521		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.11471934276137521 | validation: 0.09333895425607835]
	TIME [epoch: 8.31 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11466231377164951		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.11466231377164951 | validation: 0.10372616332707613]
	TIME [epoch: 8.31 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11317829894054347		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.11317829894054347 | validation: 0.09395161551347311]
	TIME [epoch: 8.34 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12674602170652788		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.12674602170652788 | validation: 0.11551058982952969]
	TIME [epoch: 8.31 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11732258393617498		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.11732258393617498 | validation: 0.11201996340645679]
	TIME [epoch: 8.31 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15011002200115034		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.15011002200115034 | validation: 0.1133476423309823]
	TIME [epoch: 8.33 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10759407941381967		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.10759407941381967 | validation: 0.08663530070911968]
	TIME [epoch: 8.32 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11097726878977146		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.11097726878977146 | validation: 0.08775278107668194]
	TIME [epoch: 8.31 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10088036888950278		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.10088036888950278 | validation: 0.09038594577767747]
	TIME [epoch: 8.31 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11450344989557375		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.11450344989557375 | validation: 0.10036987516335233]
	TIME [epoch: 8.33 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10642358973248059		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.10642358973248059 | validation: 0.07949691296893807]
	TIME [epoch: 8.31 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09625436780619986		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.09625436780619986 | validation: 0.10815517851145898]
	TIME [epoch: 8.31 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10868382224581512		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.10868382224581512 | validation: 0.12944830238840058]
	TIME [epoch: 8.32 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1072382816006459		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.1072382816006459 | validation: 0.10453647003501912]
	TIME [epoch: 8.33 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15595320017424422		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.15595320017424422 | validation: 0.18165976628600566]
	TIME [epoch: 8.31 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12933165883711734		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.12933165883711734 | validation: 0.09834683731319466]
	TIME [epoch: 8.31 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09927350453480774		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.09927350453480774 | validation: 0.09526864493511089]
	TIME [epoch: 8.33 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10505220788206251		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.10505220788206251 | validation: 0.08752833532489765]
	TIME [epoch: 8.31 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0941834643587632		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.0941834643587632 | validation: 0.10159293102025137]
	TIME [epoch: 8.31 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09696760840507872		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.09696760840507872 | validation: 0.0899482627397523]
	TIME [epoch: 8.31 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10472480777170512		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.10472480777170512 | validation: 0.10878252721075575]
	TIME [epoch: 8.33 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10740326752528184		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.10740326752528184 | validation: 0.09785253207200854]
	TIME [epoch: 8.31 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10377802034582204		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.10377802034582204 | validation: 0.13019926597524756]
	TIME [epoch: 8.31 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10549926454797594		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.10549926454797594 | validation: 0.11088962967100253]
	TIME [epoch: 8.33 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1264052659063636		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.1264052659063636 | validation: 0.09844512475879423]
	TIME [epoch: 8.32 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0951809393521568		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.0951809393521568 | validation: 0.08940043122673874]
	TIME [epoch: 8.31 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10188932058168425		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.10188932058168425 | validation: 0.08019254275475599]
	TIME [epoch: 8.31 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10138789878229519		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.10138789878229519 | validation: 0.07733634701174687]
	TIME [epoch: 8.34 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10601709136276875		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.10601709136276875 | validation: 0.08757816694974924]
	TIME [epoch: 8.31 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13486073186530234		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.13486073186530234 | validation: 0.12863964089874086]
	TIME [epoch: 8.31 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1427714048112974		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.1427714048112974 | validation: 0.11015778860647318]
	TIME [epoch: 8.31 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1191227287829932		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.1191227287829932 | validation: 0.10480460459903967]
	TIME [epoch: 8.33 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10292025007846717		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.10292025007846717 | validation: 0.08745855309357806]
	TIME [epoch: 8.31 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10185649841230508		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.10185649841230508 | validation: 0.08233705965104304]
	TIME [epoch: 8.31 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10077508318818444		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.10077508318818444 | validation: 0.09953224231765745]
	TIME [epoch: 8.33 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09783441456607636		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.09783441456607636 | validation: 0.08743908432705642]
	TIME [epoch: 8.32 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1034497686549282		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.1034497686549282 | validation: 0.11607617998897013]
	TIME [epoch: 8.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10245829698874409		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.10245829698874409 | validation: 0.08883729591132364]
	TIME [epoch: 8.31 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09867514396378049		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.09867514396378049 | validation: 0.08073320647532695]
	TIME [epoch: 8.33 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09256945175904943		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.09256945175904943 | validation: 0.07397340613489292]
	TIME [epoch: 8.31 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09282896766860803		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.09282896766860803 | validation: 0.08543780428331121]
	TIME [epoch: 8.31 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10352407842826561		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.10352407842826561 | validation: 0.0946701026056661]
	TIME [epoch: 8.31 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09731606153832248		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.09731606153832248 | validation: 0.08500779972346836]
	TIME [epoch: 8.33 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09561319423395861		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.09561319423395861 | validation: 0.08236369005835964]
	TIME [epoch: 8.31 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09617624308427988		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.09617624308427988 | validation: 0.08197500174605246]
	TIME [epoch: 8.31 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09755345317831078		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.09755345317831078 | validation: 0.08708200570486872]
	TIME [epoch: 8.33 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10656676694102077		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.10656676694102077 | validation: 0.09712303796834074]
	TIME [epoch: 8.31 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1107348714554049		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.1107348714554049 | validation: 0.08599798027327224]
	TIME [epoch: 8.31 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1085028462093706		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.1085028462093706 | validation: 0.1087355600017679]
	TIME [epoch: 8.31 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10000373337030047		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.10000373337030047 | validation: 0.07457735351581632]
	TIME [epoch: 8.33 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09436125401430887		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.09436125401430887 | validation: 0.07249639237867837]
	TIME [epoch: 8.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09975347183926617		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.09975347183926617 | validation: 0.0809687411783768]
	TIME [epoch: 8.31 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10022822473503028		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.10022822473503028 | validation: 0.08770792371770969]
	TIME [epoch: 8.32 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10068989703042575		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.10068989703042575 | validation: 0.10292914418547831]
	TIME [epoch: 8.32 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1166461285501484		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.1166461285501484 | validation: 0.08865220241902247]
	TIME [epoch: 8.31 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1182777690595564		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.1182777690595564 | validation: 0.08477315874757295]
	TIME [epoch: 8.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10678541650211859		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.10678541650211859 | validation: 0.08577276410424789]
	TIME [epoch: 8.33 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11271926968456995		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.11271926968456995 | validation: 0.09063165210874774]
	TIME [epoch: 8.31 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10604180323336831		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.10604180323336831 | validation: 0.08231343191032373]
	TIME [epoch: 8.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09695316312987083		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.09695316312987083 | validation: 0.09456259774892811]
	TIME [epoch: 8.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10253011139785959		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.10253011139785959 | validation: 0.08300745877562793]
	TIME [epoch: 8.33 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10670381805511224		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.10670381805511224 | validation: 0.11076099274101203]
	TIME [epoch: 8.31 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12041262493719684		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.12041262493719684 | validation: 0.08534889267900882]
	TIME [epoch: 8.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10340773159552608		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.10340773159552608 | validation: 0.08227998015610416]
	TIME [epoch: 8.32 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10042374333585746		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.10042374333585746 | validation: 0.0977472775943066]
	TIME [epoch: 8.32 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12014005524338098		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.12014005524338098 | validation: 0.08063003127089925]
	TIME [epoch: 8.31 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10023729443772574		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.10023729443772574 | validation: 0.08461880503851765]
	TIME [epoch: 8.31 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09661373829259592		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.09661373829259592 | validation: 0.09072772368799648]
	TIME [epoch: 8.33 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10465549790476458		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.10465549790476458 | validation: 0.07756969442550815]
	TIME [epoch: 8.31 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10866472808957087		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.10866472808957087 | validation: 0.11219894049367163]
	TIME [epoch: 8.31 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10958616258787043		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.10958616258787043 | validation: 0.09904879388086746]
	TIME [epoch: 8.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09566427968217465		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.09566427968217465 | validation: 0.08386392154967401]
	TIME [epoch: 8.32 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09182478073232726		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.09182478073232726 | validation: 0.07502832790984253]
	TIME [epoch: 8.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09519271378038324		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.09519271378038324 | validation: 0.0855242502769004]
	TIME [epoch: 8.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09983391776902774		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.09983391776902774 | validation: 0.10889268291081741]
	TIME [epoch: 8.32 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1447143309928866		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.1447143309928866 | validation: 0.18120804530677498]
	TIME [epoch: 8.31 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1719361997461845		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.1719361997461845 | validation: 0.13097771965105326]
	TIME [epoch: 8.31 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10859988724991347		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.10859988724991347 | validation: 0.10487731712419424]
	TIME [epoch: 8.31 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12283085691470359		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.12283085691470359 | validation: 0.09864407312724008]
	TIME [epoch: 8.33 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09152780162076421		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.09152780162076421 | validation: 0.08494037790798813]
	TIME [epoch: 8.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08854259699838311		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.08854259699838311 | validation: 0.07721015870738154]
	TIME [epoch: 8.31 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10344943725664812		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.10344943725664812 | validation: 0.11546882774607427]
	TIME [epoch: 8.32 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10078592581584597		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.10078592581584597 | validation: 0.12672779441466267]
	TIME [epoch: 8.33 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11423165091254808		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.11423165091254808 | validation: 0.10435495589511247]
	TIME [epoch: 8.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10356894282643389		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.10356894282643389 | validation: 0.08308335236412306]
	TIME [epoch: 8.31 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0992831881904048		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.0992831881904048 | validation: 0.08610383915179695]
	TIME [epoch: 8.33 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09609799288753011		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.09609799288753011 | validation: 0.08759601545403813]
	TIME [epoch: 8.32 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09590026642023099		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.09590026642023099 | validation: 0.07560071256535356]
	TIME [epoch: 8.31 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09331951872082037		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.09331951872082037 | validation: 0.08267621703282577]
	TIME [epoch: 8.31 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09984443521406312		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.09984443521406312 | validation: 0.07870286572374816]
	TIME [epoch: 8.33 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09891295571168479		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.09891295571168479 | validation: 0.08326936023420048]
	TIME [epoch: 8.31 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09618304439249055		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.09618304439249055 | validation: 0.07339750872421547]
	TIME [epoch: 8.31 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1008759967678621		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.1008759967678621 | validation: 0.09416870966325634]
	TIME [epoch: 8.32 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10394354423863872		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.10394354423863872 | validation: 0.09145338376306295]
	TIME [epoch: 8.32 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10396266944266294		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.10396266944266294 | validation: 0.14760477337331684]
	TIME [epoch: 8.31 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1454163456043136		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.1454163456043136 | validation: 0.1072091923045369]
	TIME [epoch: 8.31 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11935349334330721		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.11935349334330721 | validation: 0.09645215675140396]
	TIME [epoch: 8.33 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11142287435638179		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.11142287435638179 | validation: 0.1370564999081736]
	TIME [epoch: 8.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10116785605810732		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.10116785605810732 | validation: 0.08342730149658371]
	TIME [epoch: 8.31 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0913294705533752		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.0913294705533752 | validation: 0.0763992825618554]
	TIME [epoch: 8.32 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09821620625543975		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.09821620625543975 | validation: 0.09516608686419067]
	TIME [epoch: 8.33 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09469331751691726		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.09469331751691726 | validation: 0.08237562294352851]
	TIME [epoch: 8.31 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10201152076672879		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.10201152076672879 | validation: 0.12116226684999815]
	TIME [epoch: 8.31 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15650085721030432		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.15650085721030432 | validation: 0.1647698272390765]
	TIME [epoch: 8.33 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16225266392415177		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.16225266392415177 | validation: 0.12813589835046044]
	TIME [epoch: 8.31 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10755595604514619		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.10755595604514619 | validation: 0.10023772369106915]
	TIME [epoch: 8.31 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09658672572877681		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.09658672572877681 | validation: 0.08778289635153391]
	TIME [epoch: 8.31 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09835761017095228		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.09835761017095228 | validation: 0.089185565596694]
	TIME [epoch: 8.33 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09859257881637531		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.09859257881637531 | validation: 0.08664209701180668]
	TIME [epoch: 8.31 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09384651765670313		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.09384651765670313 | validation: 0.0767726181408172]
	TIME [epoch: 8.31 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09846489239451603		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.09846489239451603 | validation: 0.07659669176406736]
	TIME [epoch: 8.31 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09491467422230915		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.09491467422230915 | validation: 0.07862469540768005]
	TIME [epoch: 8.32 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0973249219326258		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.0973249219326258 | validation: 0.07631192756240862]
	TIME [epoch: 8.31 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0877376698555072		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.0877376698555072 | validation: 0.07179555630892279]
	TIME [epoch: 8.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09192981599050407		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.09192981599050407 | validation: 0.0774434949392043]
	TIME [epoch: 8.33 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09970194429922738		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.09970194429922738 | validation: 0.07724524303548873]
	TIME [epoch: 8.31 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09660072891836863		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.09660072891836863 | validation: 0.08359689902200919]
	TIME [epoch: 8.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09478852997406992		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.09478852997406992 | validation: 0.07859875249211817]
	TIME [epoch: 8.31 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09548270613417707		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.09548270613417707 | validation: 0.08198081425025798]
	TIME [epoch: 8.33 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10666975575567128		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.10666975575567128 | validation: 0.09607176598846798]
	TIME [epoch: 8.31 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10155873351770069		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.10155873351770069 | validation: 0.08010614589842469]
	TIME [epoch: 8.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09585799581081053		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.09585799581081053 | validation: 0.08529419591110918]
	TIME [epoch: 8.32 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0989653019320628		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.0989653019320628 | validation: 0.0767500402652963]
	TIME [epoch: 8.32 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10211212347096525		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.10211212347096525 | validation: 0.09007845619230928]
	TIME [epoch: 8.31 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10562983646845787		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.10562983646845787 | validation: 0.0867932276476359]
	TIME [epoch: 8.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10374267446641734		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.10374267446641734 | validation: 0.08055851997355774]
	TIME [epoch: 8.33 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0967763360324664		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.0967763360324664 | validation: 0.083741808982185]
	TIME [epoch: 8.31 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11053680443785836		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.11053680443785836 | validation: 0.08009468769045824]
	TIME [epoch: 8.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08989984377169714		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.08989984377169714 | validation: 0.08593752037825725]
	TIME [epoch: 8.31 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09959415946139044		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.09959415946139044 | validation: 0.0772100387855545]
	TIME [epoch: 8.33 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0909660709071287		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.0909660709071287 | validation: 0.08304663492883105]
	TIME [epoch: 8.3 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09722851201521467		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.09722851201521467 | validation: 0.0911281096504846]
	TIME [epoch: 8.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09893615794491077		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.09893615794491077 | validation: 0.0794740378095575]
	TIME [epoch: 8.32 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09470638301685734		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.09470638301685734 | validation: 0.08516044363246973]
	TIME [epoch: 8.32 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09320830041780458		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.09320830041780458 | validation: 0.08528982854520997]
	TIME [epoch: 8.31 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08921801226032636		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.08921801226032636 | validation: 0.08876253558515546]
	TIME [epoch: 8.32 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09089039304857335		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.09089039304857335 | validation: 0.09041573781783684]
	TIME [epoch: 8.32 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09711646361849278		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.09711646361849278 | validation: 0.08789764867243778]
	TIME [epoch: 8.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0905761594744271		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.0905761594744271 | validation: 0.07367755375380923]
	TIME [epoch: 8.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09135039323853596		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.09135039323853596 | validation: 0.098653486404833]
	TIME [epoch: 8.31 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0944567004316764		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.0944567004316764 | validation: 0.08078706130013014]
	TIME [epoch: 8.33 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09385518050446635		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.09385518050446635 | validation: 0.08400109314396195]
	TIME [epoch: 8.31 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08843037766340982		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.08843037766340982 | validation: 0.07898591541914696]
	TIME [epoch: 8.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09673602427427616		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.09673602427427616 | validation: 0.08782168457701323]
	TIME [epoch: 8.32 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.096678255758192		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.096678255758192 | validation: 0.09750535393987197]
	TIME [epoch: 8.31 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09670949359286787		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.09670949359286787 | validation: 0.07634660334173554]
	TIME [epoch: 8.31 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10919621574435434		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.10919621574435434 | validation: 0.119706474527556]
	TIME [epoch: 8.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09541887577619268		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.09541887577619268 | validation: 0.08124670589138433]
	TIME [epoch: 8.33 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0972090661375047		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.0972090661375047 | validation: 0.08938965097697167]
	TIME [epoch: 8.31 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0926414203883569		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.0926414203883569 | validation: 0.0836193958239781]
	TIME [epoch: 8.31 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09382039583049293		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.09382039583049293 | validation: 0.08401266817611128]
	TIME [epoch: 8.32 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08719181137820986		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.08719181137820986 | validation: 0.08674244615657448]
	TIME [epoch: 8.33 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09386421822692965		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.09386421822692965 | validation: 0.0960713915060086]
	TIME [epoch: 8.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09589501713582099		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.09589501713582099 | validation: 0.08858070590725876]
	TIME [epoch: 8.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09207767811964661		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.09207767811964661 | validation: 0.08263217987010339]
	TIME [epoch: 8.33 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09704674233168864		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.09704674233168864 | validation: 0.074653257741189]
	TIME [epoch: 8.31 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09780781137129604		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.09780781137129604 | validation: 0.07910753224738482]
	TIME [epoch: 8.31 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09941179691026791		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.09941179691026791 | validation: 0.07842667342170916]
	TIME [epoch: 8.31 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10323906695316447		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.10323906695316447 | validation: 0.0816482463055042]
	TIME [epoch: 8.33 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10115594312758662		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.10115594312758662 | validation: 0.07776087456791703]
	TIME [epoch: 8.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09489593250283938		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.09489593250283938 | validation: 0.07143929670423853]
	TIME [epoch: 8.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09206872792424761		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.09206872792424761 | validation: 0.08244186518084606]
	TIME [epoch: 8.32 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09382914702413288		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.09382914702413288 | validation: 0.07996275662188902]
	TIME [epoch: 8.32 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09630774248904074		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.09630774248904074 | validation: 0.07451017526004414]
	TIME [epoch: 8.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09533656107368244		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.09533656107368244 | validation: 0.08120432925583941]
	TIME [epoch: 8.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09074916548128431		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.09074916548128431 | validation: 0.07835681512775006]
	TIME [epoch: 8.33 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08975211906981109		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.08975211906981109 | validation: 0.07536706847514035]
	TIME [epoch: 8.31 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09270442344272112		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.09270442344272112 | validation: 0.08052410063567259]
	TIME [epoch: 8.31 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09295631093699501		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.09295631093699501 | validation: 0.08074735399483246]
	TIME [epoch: 8.32 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09832086827317774		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.09832086827317774 | validation: 0.0814015520831531]
	TIME [epoch: 8.33 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09444530796959497		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.09444530796959497 | validation: 0.0914966993723793]
	TIME [epoch: 8.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10557494903016237		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.10557494903016237 | validation: 0.08922078856915161]
	TIME [epoch: 8.31 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10176557478471508		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.10176557478471508 | validation: 0.07895283816789789]
	TIME [epoch: 8.32 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08864221573950155		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.08864221573950155 | validation: 0.08549318361733185]
	TIME [epoch: 8.31 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09901211584413885		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.09901211584413885 | validation: 0.08430790217012488]
	TIME [epoch: 8.31 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09640417792627567		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.09640417792627567 | validation: 0.07856590501362853]
	TIME [epoch: 8.31 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08639984317157999		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.08639984317157999 | validation: 0.0800777245855452]
	TIME [epoch: 8.33 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08958384088832172		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.08958384088832172 | validation: 0.08796158869237201]
	TIME [epoch: 8.31 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08897346189070256		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.08897346189070256 | validation: 0.07778919053380834]
	TIME [epoch: 8.31 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08794146484623282		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.08794146484623282 | validation: 0.09217968599727658]
	TIME [epoch: 8.33 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10026019857273265		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.10026019857273265 | validation: 0.10483579974569207]
	TIME [epoch: 8.32 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09422076008624775		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.09422076008624775 | validation: 0.06607333824647554]
	TIME [epoch: 8.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10028026813618703		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.10028026813618703 | validation: 0.12399739726664746]
	TIME [epoch: 8.3 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11002012862571224		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.11002012862571224 | validation: 0.10883423189500799]
	TIME [epoch: 8.33 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0953784758371915		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.0953784758371915 | validation: 0.07878170792084391]
	TIME [epoch: 8.31 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0950760035535589		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.0950760035535589 | validation: 0.07912633622336762]
	TIME [epoch: 8.3 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08855758327856814		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.08855758327856814 | validation: 0.07890239363904866]
	TIME [epoch: 8.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08965625714247238		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.08965625714247238 | validation: 0.06751783915030565]
	TIME [epoch: 8.33 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0937098110525692		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.0937098110525692 | validation: 0.07738410132690776]
	TIME [epoch: 8.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0912626694946361		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.0912626694946361 | validation: 0.0932167750858332]
	TIME [epoch: 8.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09841517837652138		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.09841517837652138 | validation: 0.09854382489716698]
	TIME [epoch: 8.32 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1035156868522201		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.1035156868522201 | validation: 0.10560661367909385]
	TIME [epoch: 8.31 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10578082965169266		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.10578082965169266 | validation: 0.08568536566659282]
	TIME [epoch: 8.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09975930526576818		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.09975930526576818 | validation: 0.08082980748120216]
	TIME [epoch: 8.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09093112041881715		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.09093112041881715 | validation: 0.07405680844624037]
	TIME [epoch: 8.33 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09451671037132728		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.09451671037132728 | validation: 0.07876271444010491]
	TIME [epoch: 8.3 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09187371418215537		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.09187371418215537 | validation: 0.08443579988116347]
	TIME [epoch: 8.3 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08988222053549995		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.08988222053549995 | validation: 0.07358386027804255]
	TIME [epoch: 8.31 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09489941247815518		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.09489941247815518 | validation: 0.08543677246850234]
	TIME [epoch: 8.33 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09208364687289548		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.09208364687289548 | validation: 0.09035064961208203]
	TIME [epoch: 8.31 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09121331807979821		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.09121331807979821 | validation: 0.09117593817649335]
	TIME [epoch: 8.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09208132514754461		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.09208132514754461 | validation: 0.0818035051228744]
	TIME [epoch: 8.32 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09113825118720327		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.09113825118720327 | validation: 0.06865283782775873]
	TIME [epoch: 8.31 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0899264737592806		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.0899264737592806 | validation: 0.08057129670149246]
	TIME [epoch: 8.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08748792262916585		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.08748792262916585 | validation: 0.07211856406755643]
	TIME [epoch: 8.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09223514329569998		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.09223514329569998 | validation: 0.08833993074185562]
	TIME [epoch: 8.33 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09366361188761178		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.09366361188761178 | validation: 0.08021996171978443]
	TIME [epoch: 8.31 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08907669436719265		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.08907669436719265 | validation: 0.1001505730455014]
	TIME [epoch: 8.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09779567297753797		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.09779567297753797 | validation: 0.09948887307911143]
	TIME [epoch: 8.32 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10959377819841046		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.10959377819841046 | validation: 0.08531988958793077]
	TIME [epoch: 8.32 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10083019758496023		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.10083019758496023 | validation: 0.07571330133576248]
	TIME [epoch: 8.31 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08980245293084807		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.08980245293084807 | validation: 0.08568066430006126]
	TIME [epoch: 8.31 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09576637224537424		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.09576637224537424 | validation: 0.08284912337151067]
	TIME [epoch: 8.32 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09621080331246516		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.09621080331246516 | validation: 0.0851089383624014]
	TIME [epoch: 8.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09415471180053273		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.09415471180053273 | validation: 0.07464767928988894]
	TIME [epoch: 8.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08906303708462862		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.08906303708462862 | validation: 0.07690514772787727]
	TIME [epoch: 8.31 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09078847795417468		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.09078847795417468 | validation: 0.08016635227953477]
	TIME [epoch: 8.33 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09122320850061669		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.09122320850061669 | validation: 0.08303954091074833]
	TIME [epoch: 8.31 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1031753667152484		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.1031753667152484 | validation: 0.08131296023908802]
	TIME [epoch: 8.31 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09910440708942737		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.09910440708942737 | validation: 0.07261432878910401]
	TIME [epoch: 8.33 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0914578097055089		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.0914578097055089 | validation: 0.09280086443442773]
	TIME [epoch: 8.32 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10650783300219942		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.10650783300219942 | validation: 0.07924003081381079]
	TIME [epoch: 8.31 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09277782599054482		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.09277782599054482 | validation: 0.0695777500909735]
	TIME [epoch: 8.31 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0938364818162948		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.0938364818162948 | validation: 0.08243830265931244]
	TIME [epoch: 8.34 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09108203487642874		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.09108203487642874 | validation: 0.0787567161800491]
	TIME [epoch: 8.32 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08992063536391075		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.08992063536391075 | validation: 0.06969750340540093]
	TIME [epoch: 8.31 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09295887250899808		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.09295887250899808 | validation: 0.0775770919498993]
	TIME [epoch: 8.31 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09224410660573436		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.09224410660573436 | validation: 0.06939889780268435]
	TIME [epoch: 8.34 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0901150423797844		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.0901150423797844 | validation: 0.07868584798023517]
	TIME [epoch: 8.31 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09237157073301354		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.09237157073301354 | validation: 0.07028071966515079]
	TIME [epoch: 8.31 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09228698845630648		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.09228698845630648 | validation: 0.07837661783900701]
	TIME [epoch: 8.33 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10009824180736315		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.10009824180736315 | validation: 0.07755818158760724]
	TIME [epoch: 8.32 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0924651704119023		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.0924651704119023 | validation: 0.06844404598385331]
	TIME [epoch: 8.31 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09060597297114184		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.09060597297114184 | validation: 0.07045875092284856]
	TIME [epoch: 8.32 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09290146746028406		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.09290146746028406 | validation: 0.08072147329562371]
	TIME [epoch: 8.33 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09158534505970332		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.09158534505970332 | validation: 0.08191166035293684]
	TIME [epoch: 8.32 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0910617295656993		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.0910617295656993 | validation: 0.07327665751830417]
	TIME [epoch: 8.32 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08988704417693308		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.08988704417693308 | validation: 0.07532916050266225]
	TIME [epoch: 8.33 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09145553922360777		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.09145553922360777 | validation: 0.08064776120319464]
	TIME [epoch: 8.33 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1043017731214223		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.1043017731214223 | validation: 0.0911862431375478]
	TIME [epoch: 8.31 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0918226725949348		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.0918226725949348 | validation: 0.08677747219829894]
	TIME [epoch: 8.31 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09518571117355787		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.09518571117355787 | validation: 0.09591612546164971]
	TIME [epoch: 8.35 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09841056997862106		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.09841056997862106 | validation: 0.0868959143963689]
	TIME [epoch: 8.32 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09113964948410876		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.09113964948410876 | validation: 0.08016089584466976]
	TIME [epoch: 8.31 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08662838076852467		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.08662838076852467 | validation: 0.08095120991519965]
	TIME [epoch: 8.32 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08658775562333505		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.08658775562333505 | validation: 0.07987758896769243]
	TIME [epoch: 8.34 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08295402217656836		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.08295402217656836 | validation: 0.07900896788511318]
	TIME [epoch: 8.32 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09354606798942859		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.09354606798942859 | validation: 0.08205358615655348]
	TIME [epoch: 8.32 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09492232761796068		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.09492232761796068 | validation: 0.08009219797126646]
	TIME [epoch: 8.33 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08946695202185359		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.08946695202185359 | validation: 0.08386114354169295]
	TIME [epoch: 8.33 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09552302830670933		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.09552302830670933 | validation: 0.0826614047621402]
	TIME [epoch: 8.31 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0964005175752323		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.0964005175752323 | validation: 0.08039147370741828]
	TIME [epoch: 8.32 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0879601405191265		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.0879601405191265 | validation: 0.0809838507878542]
	TIME [epoch: 8.34 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0956494007061888		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.0956494007061888 | validation: 0.08495113084269937]
	TIME [epoch: 8.32 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09391668517838006		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.09391668517838006 | validation: 0.08451466112379737]
	TIME [epoch: 8.31 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09061332797961325		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.09061332797961325 | validation: 0.08026086907332214]
	TIME [epoch: 8.31 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09345976143721653		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.09345976143721653 | validation: 0.11165493457950565]
	TIME [epoch: 8.33 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09314953716728276		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.09314953716728276 | validation: 0.0859377057148348]
	TIME [epoch: 8.31 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08682754366344617		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.08682754366344617 | validation: 0.08353559006174856]
	TIME [epoch: 8.3 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09363160391651815		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.09363160391651815 | validation: 0.08352268345338167]
	TIME [epoch: 8.32 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09631111383924004		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.09631111383924004 | validation: 0.07808513053225498]
	TIME [epoch: 8.32 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10139642262728854		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.10139642262728854 | validation: 0.08338561326777558]
	TIME [epoch: 8.31 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09591285436628696		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.09591285436628696 | validation: 0.08370555269468774]
	TIME [epoch: 8.31 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08790232328003081		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.08790232328003081 | validation: 0.08361202593887858]
	TIME [epoch: 8.34 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09225565218993462		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.09225565218993462 | validation: 0.08604942842562271]
	TIME [epoch: 8.31 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09884225712244178		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.09884225712244178 | validation: 0.08333180103846967]
	TIME [epoch: 8.31 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09492886765218518		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.09492886765218518 | validation: 0.10306491217032715]
	TIME [epoch: 8.31 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11470319739340613		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.11470319739340613 | validation: 0.12075998263662593]
	TIME [epoch: 8.32 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09665549218711891		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.09665549218711891 | validation: 0.08670365809784474]
	TIME [epoch: 8.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08933642564997943		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.08933642564997943 | validation: 0.08373706935303303]
	TIME [epoch: 8.31 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09064235306159593		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.09064235306159593 | validation: 0.08070428524673912]
	TIME [epoch: 8.33 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08915135680855285		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.08915135680855285 | validation: 0.08176786348161436]
	TIME [epoch: 8.32 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09028352613279611		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.09028352613279611 | validation: 0.08227009269871288]
	TIME [epoch: 8.31 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08900677798409883		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.08900677798409883 | validation: 0.07430457650046468]
	TIME [epoch: 8.31 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0918479300158185		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.0918479300158185 | validation: 0.075849417735284]
	TIME [epoch: 8.33 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09536902988649668		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.09536902988649668 | validation: 0.13164869955712433]
	TIME [epoch: 8.31 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12323734321419877		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.12323734321419877 | validation: 0.13816697748971418]
	TIME [epoch: 8.31 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11277703888163666		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.11277703888163666 | validation: 0.10325127660845496]
	TIME [epoch: 8.32 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09128920807015316		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.09128920807015316 | validation: 0.08868943847468637]
	TIME [epoch: 8.32 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08663812626953084		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.08663812626953084 | validation: 0.0894433016313482]
	TIME [epoch: 8.31 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09319471523455673		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.09319471523455673 | validation: 0.08172051013300556]
	TIME [epoch: 8.32 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09964987530198087		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.09964987530198087 | validation: 0.1027874685430282]
	TIME [epoch: 8.33 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10020617372568356		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.10020617372568356 | validation: 0.09576131529197918]
	TIME [epoch: 8.31 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0979979321675931		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.0979979321675931 | validation: 0.0973444588016815]
	TIME [epoch: 8.31 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10708282544237116		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.10708282544237116 | validation: 0.0939305299831997]
	TIME [epoch: 8.31 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09441389095623891		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.09441389095623891 | validation: 0.08915252921779135]
	TIME [epoch: 8.34 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09438945417573143		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.09438945417573143 | validation: 0.09263425174164563]
	TIME [epoch: 8.31 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09428754767757672		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.09428754767757672 | validation: 0.09943194532353997]
	TIME [epoch: 8.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09448068853749995		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.09448068853749995 | validation: 0.07875387487589597]
	TIME [epoch: 8.32 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10014948978282526		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.10014948978282526 | validation: 0.10203166092202925]
	TIME [epoch: 8.32 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11026995977365825		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.11026995977365825 | validation: 0.11316658418642397]
	TIME [epoch: 8.3 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11312241241203243		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.11312241241203243 | validation: 0.13608663993477793]
	TIME [epoch: 8.31 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1137528797360325		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.1137528797360325 | validation: 0.11087103754750147]
	TIME [epoch: 8.34 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10652148991463413		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.10652148991463413 | validation: 0.11030317151676161]
	TIME [epoch: 8.32 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10992096229287278		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.10992096229287278 | validation: 0.12232083142247846]
	TIME [epoch: 8.31 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10521734040632684		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.10521734040632684 | validation: 0.10347431652895873]
	TIME [epoch: 8.32 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10174714267419019		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.10174714267419019 | validation: 0.10820893718150137]
	TIME [epoch: 8.32 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11364702431679019		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.11364702431679019 | validation: 0.10781191629081714]
	TIME [epoch: 8.32 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09960047818378018		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.09960047818378018 | validation: 0.10357046632769253]
	TIME [epoch: 8.32 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10146134803236932		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.10146134803236932 | validation: 0.11766754626921108]
	TIME [epoch: 8.34 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10333078251552749		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.10333078251552749 | validation: 0.11071466878067662]
	TIME [epoch: 8.31 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09781549718632923		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.09781549718632923 | validation: 0.08987666052600972]
	TIME [epoch: 8.31 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09518496540517773		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.09518496540517773 | validation: 0.08960242805279998]
	TIME [epoch: 8.32 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0973993835211527		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.0973993835211527 | validation: 0.10288490140712708]
	TIME [epoch: 8.33 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09502268250989175		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.09502268250989175 | validation: 0.08822012059683657]
	TIME [epoch: 8.32 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0906428432163465		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.0906428432163465 | validation: 0.09978504326989528]
	TIME [epoch: 8.31 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09943359376245417		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.09943359376245417 | validation: 0.0957203262417858]
	TIME [epoch: 8.33 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08941145254176525		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.08941145254176525 | validation: 0.09332601646123148]
	TIME [epoch: 8.32 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09859035798042731		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.09859035798042731 | validation: 0.09063415973920227]
	TIME [epoch: 8.31 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09783799999371584		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.09783799999371584 | validation: 0.10548006801177393]
	TIME [epoch: 8.31 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09634208288094291		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.09634208288094291 | validation: 0.0946632374447249]
	TIME [epoch: 8.33 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08768797966966083		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.08768797966966083 | validation: 0.09860421816301287]
	TIME [epoch: 8.31 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09879820371193247		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.09879820371193247 | validation: 0.08738731430101432]
	TIME [epoch: 8.31 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09213178245991606		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.09213178245991606 | validation: 0.08994703803645539]
	TIME [epoch: 8.31 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09385946541767981		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.09385946541767981 | validation: 0.10345634782744528]
	TIME [epoch: 8.33 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09467489649344439		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.09467489649344439 | validation: 0.102956814468081]
	TIME [epoch: 8.31 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0996898286837544		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.0996898286837544 | validation: 0.09127475432445467]
	TIME [epoch: 8.31 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0898507060861		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.0898507060861 | validation: 0.08638243196385961]
	TIME [epoch: 8.33 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10769626145916922		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.10769626145916922 | validation: 0.12502420488814034]
	TIME [epoch: 8.31 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10270291765900019		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.10270291765900019 | validation: 0.11209951865409325]
	TIME [epoch: 8.31 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12497937695517525		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.12497937695517525 | validation: 0.15553612049567606]
	TIME [epoch: 8.31 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11781338006803269		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.11781338006803269 | validation: 0.1070927569341786]
	TIME [epoch: 8.34 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09914603130881469		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.09914603130881469 | validation: 0.09752916722454211]
	TIME [epoch: 8.31 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09946955698762643		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.09946955698762643 | validation: 0.09742020316747461]
	TIME [epoch: 8.31 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10672768598975106		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.10672768598975106 | validation: 0.11548426552930094]
	TIME [epoch: 8.32 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10209576527174691		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.10209576527174691 | validation: 0.08739017177251684]
	TIME [epoch: 8.33 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09281238234311129		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.09281238234311129 | validation: 0.08535280331749173]
	TIME [epoch: 8.31 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09234639913110762		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.09234639913110762 | validation: 0.08535290273168672]
	TIME [epoch: 8.32 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09140579489004486		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.09140579489004486 | validation: 0.08583586382589317]
	TIME [epoch: 8.33 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09136030972021195		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.09136030972021195 | validation: 0.08832863199839112]
	TIME [epoch: 8.32 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08917916502173552		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.08917916502173552 | validation: 0.08429751229261596]
	TIME [epoch: 8.31 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08923180312073063		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.08923180312073063 | validation: 0.08359699520804395]
	TIME [epoch: 8.31 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09202216472350938		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.09202216472350938 | validation: 0.08497228016794134]
	TIME [epoch: 8.33 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0923109310598079		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.0923109310598079 | validation: 0.07152997817514659]
	TIME [epoch: 8.31 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0982736029037683		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.0982736029037683 | validation: 0.08280519247540866]
	TIME [epoch: 8.31 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08730669030443002		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.08730669030443002 | validation: 0.07398800287777463]
	TIME [epoch: 8.33 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08670732464339515		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.08670732464339515 | validation: 0.07051772073119807]
	TIME [epoch: 8.31 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08599016356465441		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.08599016356465441 | validation: 0.07706723783688896]
	TIME [epoch: 8.31 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0885256538529646		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.0885256538529646 | validation: 0.07627923253070365]
	TIME [epoch: 8.31 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08427951063262268		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.08427951063262268 | validation: 0.08404512441182396]
	TIME [epoch: 8.33 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08838281989854563		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.08838281989854563 | validation: 0.07791342065679784]
	TIME [epoch: 8.3 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08610231184749545		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.08610231184749545 | validation: 0.07452053410193857]
	TIME [epoch: 8.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08363147942143846		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.08363147942143846 | validation: 0.07979707407220152]
	TIME [epoch: 8.31 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08716859652373452		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.08716859652373452 | validation: 0.07629241494672118]
	TIME [epoch: 8.33 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08779953100718982		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.08779953100718982 | validation: 0.08005948804043905]
	TIME [epoch: 8.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08390619427323516		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.08390619427323516 | validation: 0.08375501287439169]
	TIME [epoch: 8.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08647577684651127		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.08647577684651127 | validation: 0.08425104433045874]
	TIME [epoch: 8.33 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08978550234593882		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.08978550234593882 | validation: 0.07591165816044473]
	TIME [epoch: 8.32 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08640627485748989		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.08640627485748989 | validation: 0.07678325865637654]
	TIME [epoch: 8.31 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09218167115691418		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.09218167115691418 | validation: 0.07838167241365399]
	TIME [epoch: 8.31 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08658403756068782		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.08658403756068782 | validation: 0.06958472539798931]
	TIME [epoch: 8.34 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08714191269448224		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.08714191269448224 | validation: 0.0782326552169273]
	TIME [epoch: 8.31 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09054096950576071		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.09054096950576071 | validation: 0.07807628710143746]
	TIME [epoch: 8.31 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0880441108008474		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.0880441108008474 | validation: 0.07657582604074903]
	TIME [epoch: 8.32 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0870131316082863		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.0870131316082863 | validation: 0.07348183923762312]
	TIME [epoch: 8.33 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08734255306550556		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.08734255306550556 | validation: 0.07265219265961317]
	TIME [epoch: 8.31 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08782951664601911		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.08782951664601911 | validation: 0.0719915123906157]
	TIME [epoch: 8.31 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0855414635164239		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.0855414635164239 | validation: 0.07630138947500009]
	TIME [epoch: 8.33 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0904979932481518		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.0904979932481518 | validation: 0.07734999432083497]
	TIME [epoch: 8.31 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09312674572824023		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.09312674572824023 | validation: 0.07628249367134118]
	TIME [epoch: 8.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08764896322902463		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.08764896322902463 | validation: 0.06741297113625809]
	TIME [epoch: 8.31 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08586195075940808		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.08586195075940808 | validation: 0.0747901035726662]
	TIME [epoch: 8.33 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08493232995144333		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.08493232995144333 | validation: 0.07660402926519452]
	TIME [epoch: 8.31 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08763931262194782		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.08763931262194782 | validation: 0.07557996909965022]
	TIME [epoch: 8.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09294641997706408		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.09294641997706408 | validation: 0.06639401157321251]
	TIME [epoch: 8.33 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08821239416621461		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.08821239416621461 | validation: 0.08045093994686808]
	TIME [epoch: 8.32 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08796514061892266		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.08796514061892266 | validation: 0.0721745394546478]
	TIME [epoch: 8.31 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08707703858107112		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.08707703858107112 | validation: 0.0708735553096844]
	TIME [epoch: 8.31 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08253195639544668		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.08253195639544668 | validation: 0.07758232806104737]
	TIME [epoch: 8.33 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08234957956830509		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.08234957956830509 | validation: 0.08066014752794072]
	TIME [epoch: 8.3 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09110413599919795		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.09110413599919795 | validation: 0.07700193134982983]
	TIME [epoch: 8.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08837877733974603		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.08837877733974603 | validation: 0.07579315547344662]
	TIME [epoch: 8.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08490716975062208		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.08490716975062208 | validation: 0.06791401716283754]
	TIME [epoch: 8.32 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08769964838776992		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.08769964838776992 | validation: 0.07311404473904703]
	TIME [epoch: 8.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08954678565106601		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.08954678565106601 | validation: 0.07840812140189399]
	TIME [epoch: 8.31 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09493443739480721		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.09493443739480721 | validation: 0.08065303243375398]
	TIME [epoch: 8.32 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09543192694373967		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.09543192694373967 | validation: 0.11306477711502175]
	TIME [epoch: 8.32 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10920720932377315		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.10920720932377315 | validation: 0.11699138619669688]
	TIME [epoch: 8.31 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11724585792461648		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.11724585792461648 | validation: 0.08828127985464951]
	TIME [epoch: 8.31 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09865416390387684		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.09865416390387684 | validation: 0.09510826830962829]
	TIME [epoch: 8.33 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0982376694214139		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.0982376694214139 | validation: 0.0869991326509966]
	TIME [epoch: 8.31 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0886602769899909		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.0886602769899909 | validation: 0.07586863392663484]
	TIME [epoch: 8.31 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08585358038395874		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.08585358038395874 | validation: 0.07150856559797739]
	TIME [epoch: 8.31 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08889134796694372		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.08889134796694372 | validation: 0.07356880485359436]
	TIME [epoch: 8.32 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08262586338442043		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.08262586338442043 | validation: 0.07556216173914873]
	TIME [epoch: 8.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08937136692789951		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.08937136692789951 | validation: 0.082395550734631]
	TIME [epoch: 8.31 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08865219365174885		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.08865219365174885 | validation: 0.07102553783099456]
	TIME [epoch: 8.33 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09026629368177455		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.09026629368177455 | validation: 0.07565380461669015]
	TIME [epoch: 8.32 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08496479759630539		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.08496479759630539 | validation: 0.07325972907999263]
	TIME [epoch: 8.31 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09031740556352218		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.09031740556352218 | validation: 0.07779003727345213]
	TIME [epoch: 8.31 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08466326415247341		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.08466326415247341 | validation: 0.07634931474454457]
	TIME [epoch: 8.33 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08444188780371598		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.08444188780371598 | validation: 0.08806325197010074]
	TIME [epoch: 8.31 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09551204676469552		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.09551204676469552 | validation: 0.08339243998101996]
	TIME [epoch: 8.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.092513431647919		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.092513431647919 | validation: 0.09341967821252772]
	TIME [epoch: 8.32 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0942185700745033		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.0942185700745033 | validation: 0.0867025165146541]
	TIME [epoch: 8.31 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09647247349411263		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.09647247349411263 | validation: 0.0889202809548903]
	TIME [epoch: 8.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09193788754593882		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.09193788754593882 | validation: 0.10597830557073543]
	TIME [epoch: 8.31 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09514092310218362		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.09514092310218362 | validation: 0.0901239154160031]
	TIME [epoch: 8.34 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08871563524007356		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.08871563524007356 | validation: 0.08618504593520902]
	TIME [epoch: 8.31 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08968380296526793		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.08968380296526793 | validation: 0.08361019574620454]
	TIME [epoch: 8.31 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08289113318350166		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.08289113318350166 | validation: 0.08272793683480784]
	TIME [epoch: 8.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08809976575869652		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.08809976575869652 | validation: 0.07314020894500331]
	TIME [epoch: 8.33 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09188259892837762		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.09188259892837762 | validation: 0.08313748797493342]
	TIME [epoch: 8.31 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08728365113853534		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.08728365113853534 | validation: 0.08437081984822696]
	TIME [epoch: 8.31 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09296929345739424		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.09296929345739424 | validation: 0.0863647808411262]
	TIME [epoch: 8.32 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08524464515516995		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.08524464515516995 | validation: 0.07851799946621144]
	TIME [epoch: 8.32 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08649487007239048		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.08649487007239048 | validation: 0.07814870955804967]
	TIME [epoch: 8.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09282120526774501		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.09282120526774501 | validation: 0.07593119686824351]
	TIME [epoch: 8.31 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08551236416608866		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.08551236416608866 | validation: 0.08242357785480232]
	TIME [epoch: 8.33 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08890195285361457		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.08890195285361457 | validation: 0.08166549435224922]
	TIME [epoch: 8.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09182060996398862		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.09182060996398862 | validation: 0.07506196698958162]
	TIME [epoch: 8.31 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08107212307023041		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.08107212307023041 | validation: 0.06643724376182947]
	TIME [epoch: 8.31 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0857813530732586		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.0857813530732586 | validation: 0.07329710731070928]
	TIME [epoch: 8.32 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0855914203720217		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.0855914203720217 | validation: 0.07217597680564775]
	TIME [epoch: 8.3 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0848660939604348		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.0848660939604348 | validation: 0.07277853424372584]
	TIME [epoch: 8.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08234020971001015		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.08234020971001015 | validation: 0.06946254369057175]
	TIME [epoch: 8.32 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08386064442132386		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.08386064442132386 | validation: 0.0769673972808059]
	TIME [epoch: 8.31 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08502754296263973		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.08502754296263973 | validation: 0.078012586077843]
	TIME [epoch: 8.33 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0897401013281126		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.0897401013281126 | validation: 0.07600211911482108]
	TIME [epoch: 8.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08624409849619327		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.08624409849619327 | validation: 0.0724251531038436]
	TIME [epoch: 8.32 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09301663655885499		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.09301663655885499 | validation: 0.08045878648672297]
	TIME [epoch: 8.31 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09056652692045607		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.09056652692045607 | validation: 0.07079538932393407]
	TIME [epoch: 8.3 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09514007265780862		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.09514007265780862 | validation: 0.08343456127872803]
	TIME [epoch: 8.32 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08739615387151313		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.08739615387151313 | validation: 0.07250427565994509]
	TIME [epoch: 8.32 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08668470702614384		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.08668470702614384 | validation: 0.07925614783300862]
	TIME [epoch: 8.31 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0907098586790126		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.0907098586790126 | validation: 0.07119498330246551]
	TIME [epoch: 8.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08389247037165347		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.08389247037165347 | validation: 0.08249221064171686]
	TIME [epoch: 8.33 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08666262208699543		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.08666262208699543 | validation: 0.06493607181932311]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1635.pth
	Model improved!!!
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09057279611365493		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.09057279611365493 | validation: 0.0746653784427891]
	TIME [epoch: 8.31 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08457515580701928		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.08457515580701928 | validation: 0.08212377376684268]
	TIME [epoch: 8.31 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08738999369604904		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.08738999369604904 | validation: 0.09694736398828671]
	TIME [epoch: 8.32 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0882799954756848		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.0882799954756848 | validation: 0.07628522869410892]
	TIME [epoch: 8.3 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08305665050254094		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.08305665050254094 | validation: 0.07780810073010166]
	TIME [epoch: 8.31 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0869989578211797		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.0869989578211797 | validation: 0.07813895559437453]
	TIME [epoch: 8.32 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09432862999756386		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.09432862999756386 | validation: 0.07897942067820772]
	TIME [epoch: 8.31 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08550270461460563		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.08550270461460563 | validation: 0.07757828559520905]
	TIME [epoch: 8.31 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10089132031403925		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.10089132031403925 | validation: 0.08351430513832023]
	TIME [epoch: 8.31 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08738898275100011		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.08738898275100011 | validation: 0.07520517055506508]
	TIME [epoch: 8.32 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09262942619034703		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.09262942619034703 | validation: 0.08207347133790457]
	TIME [epoch: 8.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08841795253958414		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.08841795253958414 | validation: 0.07834081493836569]
	TIME [epoch: 8.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0898136076189287		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.0898136076189287 | validation: 0.08083875368098672]
	TIME [epoch: 8.32 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08769497296277338		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.08769497296277338 | validation: 0.0812144201317865]
	TIME [epoch: 8.32 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09388712577065547		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.09388712577065547 | validation: 0.08543868221799678]
	TIME [epoch: 8.3 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09260134377090276		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.09260134377090276 | validation: 0.08844433788021397]
	TIME [epoch: 8.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09677445104948004		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.09677445104948004 | validation: 0.07080897590591229]
	TIME [epoch: 8.33 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08863743675873283		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.08863743675873283 | validation: 0.07986779013495855]
	TIME [epoch: 8.31 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09046599422537507		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.09046599422537507 | validation: 0.07972119892693638]
	TIME [epoch: 8.3 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09398443499724508		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.09398443499724508 | validation: 0.07265736665813109]
	TIME [epoch: 8.31 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09468010247173611		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.09468010247173611 | validation: 0.07873675901412389]
	TIME [epoch: 8.34 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09430579916152818		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.09430579916152818 | validation: 0.07680034238903674]
	TIME [epoch: 8.31 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0978275745345579		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.0978275745345579 | validation: 0.0880579058163106]
	TIME [epoch: 8.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09998230152992968		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.09998230152992968 | validation: 0.08261028456928789]
	TIME [epoch: 8.33 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09226722595432206		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.09226722595432206 | validation: 0.075377254364491]
	TIME [epoch: 8.32 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09420719859914073		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.09420719859914073 | validation: 0.07073525989514401]
	TIME [epoch: 8.31 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09339916058259866		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.09339916058259866 | validation: 0.07594874100368335]
	TIME [epoch: 8.31 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08685452593627829		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.08685452593627829 | validation: 0.06685096870494037]
	TIME [epoch: 8.34 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08700788245058783		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.08700788245058783 | validation: 0.07789948833426971]
	TIME [epoch: 8.31 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09272604647758623		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.09272604647758623 | validation: 0.07802070700656843]
	TIME [epoch: 8.3 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08991255315365936		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.08991255315365936 | validation: 0.07314282931084486]
	TIME [epoch: 8.32 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08940362579847551		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.08940362579847551 | validation: 0.07366125206181331]
	TIME [epoch: 8.33 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.093650015100183		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.093650015100183 | validation: 0.07139650313136028]
	TIME [epoch: 8.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09623771240045527		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.09623771240045527 | validation: 0.07845211518921252]
	TIME [epoch: 8.31 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09112751222313578		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.09112751222313578 | validation: 0.08150668335451826]
	TIME [epoch: 8.32 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09241193901551854		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.09241193901551854 | validation: 0.06882058143709924]
	TIME [epoch: 8.31 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09084306125816424		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.09084306125816424 | validation: 0.0809280694676265]
	TIME [epoch: 8.31 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09658138504212249		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.09658138504212249 | validation: 0.08014744164931936]
	TIME [epoch: 8.31 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09272698445229219		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.09272698445229219 | validation: 0.0733010284212054]
	TIME [epoch: 8.33 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09051351279801727		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.09051351279801727 | validation: 0.0806661757796646]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08809249973302288		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.08809249973302288 | validation: 0.07028473037992188]
	TIME [epoch: 8.31 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09228007371832599		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.09228007371832599 | validation: 0.07877492279757009]
	TIME [epoch: 8.33 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09133046530756903		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.09133046530756903 | validation: 0.07683102934195701]
	TIME [epoch: 8.32 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09046659172513534		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.09046659172513534 | validation: 0.07475232064083825]
	TIME [epoch: 8.3 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08492878544826124		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.08492878544826124 | validation: 0.07913157716464467]
	TIME [epoch: 8.31 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08731865281984859		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.08731865281984859 | validation: 0.06755345395794396]
	TIME [epoch: 8.33 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08881944045309494		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.08881944045309494 | validation: 0.07205401009066728]
	TIME [epoch: 8.32 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08628181868007753		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.08628181868007753 | validation: 0.07662999078878674]
	TIME [epoch: 8.31 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09088067706365191		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.09088067706365191 | validation: 0.07527358815591326]
	TIME [epoch: 8.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0861175071806591		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.0861175071806591 | validation: 0.07256378033543356]
	TIME [epoch: 8.33 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08753233553076137		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.08753233553076137 | validation: 0.08133690839233217]
	TIME [epoch: 8.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08832216048144301		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.08832216048144301 | validation: 0.06808291210692646]
	TIME [epoch: 8.3 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08992366032829784		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.08992366032829784 | validation: 0.07663916090125766]
	TIME [epoch: 8.32 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08748679360904668		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.08748679360904668 | validation: 0.08101741070032215]
	TIME [epoch: 8.31 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08798375956839866		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.08798375956839866 | validation: 0.07689184640507789]
	TIME [epoch: 8.31 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0879997553395844		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.0879997553395844 | validation: 0.07668290375279155]
	TIME [epoch: 8.31 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08770481875182015		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.08770481875182015 | validation: 0.08627186478252455]
	TIME [epoch: 8.34 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09047510672623277		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.09047510672623277 | validation: 0.08160185018151757]
	TIME [epoch: 8.31 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09184734034552999		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.09184734034552999 | validation: 0.08065195961561882]
	TIME [epoch: 8.31 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08601439687621917		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.08601439687621917 | validation: 0.08748594996511017]
	TIME [epoch: 8.32 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0860363877575717		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.0860363877575717 | validation: 0.08182623781463706]
	TIME [epoch: 8.32 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08834682377084582		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.08834682377084582 | validation: 0.08798960242295298]
	TIME [epoch: 8.31 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08726313976691205		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.08726313976691205 | validation: 0.0728842106265304]
	TIME [epoch: 8.31 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08682210695374219		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.08682210695374219 | validation: 0.0815952023147658]
	TIME [epoch: 8.33 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0859311364456985		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.0859311364456985 | validation: 0.0821519993970806]
	TIME [epoch: 8.31 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08391213823995629		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.08391213823995629 | validation: 0.07517079303179461]
	TIME [epoch: 8.31 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08466990134548076		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.08466990134548076 | validation: 0.06591161738587661]
	TIME [epoch: 8.31 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08419116865044986		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.08419116865044986 | validation: 0.07176542430428576]
	TIME [epoch: 8.33 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08547096262063505		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.08547096262063505 | validation: 0.07111976096188899]
	TIME [epoch: 8.31 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08841688427989149		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.08841688427989149 | validation: 0.07051679651412883]
	TIME [epoch: 8.3 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0846003111220782		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.0846003111220782 | validation: 0.07511437642777949]
	TIME [epoch: 8.32 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08896290613876681		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.08896290613876681 | validation: 0.06717244333990034]
	TIME [epoch: 8.31 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08643491828272856		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.08643491828272856 | validation: 0.07218091475535766]
	TIME [epoch: 8.31 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08210881351839969		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.08210881351839969 | validation: 0.07389588198953366]
	TIME [epoch: 8.31 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08091236373879765		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.08091236373879765 | validation: 0.06848077182334256]
	TIME [epoch: 8.33 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08579761622990731		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.08579761622990731 | validation: 0.0778860164951618]
	TIME [epoch: 8.31 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0880108216301541		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.0880108216301541 | validation: 0.07211397459975794]
	TIME [epoch: 8.31 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08950822542217811		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.08950822542217811 | validation: 0.07019780164303056]
	TIME [epoch: 8.3 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08373446148447347		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.08373446148447347 | validation: 0.06837163939575833]
	TIME [epoch: 8.33 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08376898193047908		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.08376898193047908 | validation: 0.07486053900763176]
	TIME [epoch: 8.31 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08415112298396657		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.08415112298396657 | validation: 0.07518908402860619]
	TIME [epoch: 8.3 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08813306116329177		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.08813306116329177 | validation: 0.06348169660961606]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1717.pth
	Model improved!!!
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08509727500210154		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.08509727500210154 | validation: 0.06517153763705939]
	TIME [epoch: 8.31 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08540073072654644		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.08540073072654644 | validation: 0.07244230052767515]
	TIME [epoch: 8.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08776076556659781		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.08776076556659781 | validation: 0.0749030132734356]
	TIME [epoch: 8.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08561002508451773		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.08561002508451773 | validation: 0.07738589249173995]
	TIME [epoch: 8.32 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08662248012592753		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.08662248012592753 | validation: 0.06085354794375323]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1722.pth
	Model improved!!!
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08634874193857862		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.08634874193857862 | validation: 0.06820454429349039]
	TIME [epoch: 8.31 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.086846790101911		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.086846790101911 | validation: 0.07162303181216523]
	TIME [epoch: 8.31 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08698081955734548		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.08698081955734548 | validation: 0.07629181258854675]
	TIME [epoch: 8.32 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08943570059561062		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.08943570059561062 | validation: 0.07350620984640313]
	TIME [epoch: 8.31 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08691137384275271		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.08691137384275271 | validation: 0.07128238262912445]
	TIME [epoch: 8.31 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0847319568878676		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.0847319568878676 | validation: 0.0746031294990546]
	TIME [epoch: 8.32 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08745999969851201		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.08745999969851201 | validation: 0.06690093074482945]
	TIME [epoch: 8.3 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0845204072155299		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.0845204072155299 | validation: 0.07604102265254195]
	TIME [epoch: 8.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08781685371152217		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.08781685371152217 | validation: 0.0789953550088068]
	TIME [epoch: 8.31 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08707591847298904		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.08707591847298904 | validation: 0.06914911127345215]
	TIME [epoch: 8.33 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08772202515360214		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.08772202515360214 | validation: 0.07228665185581017]
	TIME [epoch: 8.32 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08578708674495192		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.08578708674495192 | validation: 0.07839005903969609]
	TIME [epoch: 8.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08439937152029642		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.08439937152029642 | validation: 0.0805632351814071]
	TIME [epoch: 8.32 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0878123473348551		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.0878123473348551 | validation: 0.0633153955478302]
	TIME [epoch: 8.32 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08255689302124243		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.08255689302124243 | validation: 0.060366661949623104]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1737.pth
	Model improved!!!
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08546061470202415		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.08546061470202415 | validation: 0.07694983293664613]
	TIME [epoch: 8.31 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08886061184794868		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.08886061184794868 | validation: 0.06547983985843643]
	TIME [epoch: 8.33 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09035178154293462		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.09035178154293462 | validation: 0.07662292298399112]
	TIME [epoch: 8.31 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08992398729764285		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.08992398729764285 | validation: 0.07917312026155897]
	TIME [epoch: 8.31 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08729519866750565		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.08729519866750565 | validation: 0.07177344459487259]
	TIME [epoch: 8.31 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08017583294898518		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.08017583294898518 | validation: 0.08104309054598477]
	TIME [epoch: 8.32 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08533311385291664		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.08533311385291664 | validation: 0.07450092229515343]
	TIME [epoch: 8.3 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08689999801890305		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.08689999801890305 | validation: 0.07336939822036766]
	TIME [epoch: 8.31 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0896093264552083		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.0896093264552083 | validation: 0.06883791009072737]
	TIME [epoch: 8.33 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0880251256068875		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.0880251256068875 | validation: 0.07949369554154381]
	TIME [epoch: 8.31 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08565798247933096		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.08565798247933096 | validation: 0.0832770828747373]
	TIME [epoch: 8.31 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09366305929104353		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.09366305929104353 | validation: 0.09306028487639592]
	TIME [epoch: 8.3 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09267042585432164		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.09267042585432164 | validation: 0.090450870605636]
	TIME [epoch: 8.33 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09038575437784702		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.09038575437784702 | validation: 0.09280513013173901]
	TIME [epoch: 8.3 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09338371826362538		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.09338371826362538 | validation: 0.08471789063728766]
	TIME [epoch: 8.3 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08944036567511227		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.08944036567511227 | validation: 0.07876497907534594]
	TIME [epoch: 8.32 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0904809036211644		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.0904809036211644 | validation: 0.08382209968515213]
	TIME [epoch: 8.31 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08422971727589647		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.08422971727589647 | validation: 0.0828492203640301]
	TIME [epoch: 8.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08811652569154112		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.08811652569154112 | validation: 0.07319629327096197]
	TIME [epoch: 8.29 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08262461960894324		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.08262461960894324 | validation: 0.0795743509473611]
	TIME [epoch: 8.32 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08480083687587785		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.08480083687587785 | validation: 0.06912083628932576]
	TIME [epoch: 8.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08417624616029123		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.08417624616029123 | validation: 0.06350554111963551]
	TIME [epoch: 8.31 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08373352963212725		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.08373352963212725 | validation: 0.06789816795873505]
	TIME [epoch: 8.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08658266030439637		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.08658266030439637 | validation: 0.08249313653534851]
	TIME [epoch: 8.32 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08871730985439888		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.08871730985439888 | validation: 0.07756686538956847]
	TIME [epoch: 8.29 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08962001009855872		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.08962001009855872 | validation: 0.07320659582715337]
	TIME [epoch: 8.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08581486582561768		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.08581486582561768 | validation: 0.07260304742887713]
	TIME [epoch: 8.32 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08342401553932251		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.08342401553932251 | validation: 0.06793835119770045]
	TIME [epoch: 8.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08383919064735598		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.08383919064735598 | validation: 0.07822496248913444]
	TIME [epoch: 8.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08783136685427967		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.08783136685427967 | validation: 0.06736172623822126]
	TIME [epoch: 8.3 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08713744860991716		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.08713744860991716 | validation: 0.07970515790796888]
	TIME [epoch: 8.32 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09161887300534295		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.09161887300534295 | validation: 0.07315321459345683]
	TIME [epoch: 8.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08332195038029098		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.08332195038029098 | validation: 0.0714491048729518]
	TIME [epoch: 8.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08491206570394999		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.08491206570394999 | validation: 0.07421957380022778]
	TIME [epoch: 8.3 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08959558723510085		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.08959558723510085 | validation: 0.07279216529603993]
	TIME [epoch: 8.32 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08524253431177456		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.08524253431177456 | validation: 0.07069818851318413]
	TIME [epoch: 8.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08370934109190367		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.08370934109190367 | validation: 0.06805285868778287]
	TIME [epoch: 8.31 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08949235135309055		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.08949235135309055 | validation: 0.07303320938963366]
	TIME [epoch: 8.33 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08498877410820198		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.08498877410820198 | validation: 0.08379413983322459]
	TIME [epoch: 8.31 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08858354285471075		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.08858354285471075 | validation: 0.07863063709312765]
	TIME [epoch: 8.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08848951458676611		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.08848951458676611 | validation: 0.08413546734429433]
	TIME [epoch: 8.31 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08211564259773989		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.08211564259773989 | validation: 0.0831939122135993]
	TIME [epoch: 8.33 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08742056495552677		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.08742056495552677 | validation: 0.08071384510331911]
	TIME [epoch: 8.31 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09549823075299887		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.09549823075299887 | validation: 0.08713437951511882]
	TIME [epoch: 8.31 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08825700887992603		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.08825700887992603 | validation: 0.08437119610964398]
	TIME [epoch: 8.33 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08655348605681275		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.08655348605681275 | validation: 0.0736553344425293]
	TIME [epoch: 8.31 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08550363670799913		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.08550363670799913 | validation: 0.0689099709072696]
	TIME [epoch: 8.31 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0871020355678091		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.0871020355678091 | validation: 0.07706253300173152]
	TIME [epoch: 8.31 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08871742804793831		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.08871742804793831 | validation: 0.08296024065000124]
	TIME [epoch: 8.33 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08730789440054669		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.08730789440054669 | validation: 0.07117606857793005]
	TIME [epoch: 8.31 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0847611825030575		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.0847611825030575 | validation: 0.0696353900146538]
	TIME [epoch: 8.31 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08677065231675307		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.08677065231675307 | validation: 0.07365035008317379]
	TIME [epoch: 8.31 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08531240861188479		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.08531240861188479 | validation: 0.08242052352868035]
	TIME [epoch: 8.33 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08645076254344292		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.08645076254344292 | validation: 0.08131929921045364]
	TIME [epoch: 8.31 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08805389719311939		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.08805389719311939 | validation: 0.08973384990591461]
	TIME [epoch: 8.31 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08225945656581377		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.08225945656581377 | validation: 0.08349646991523296]
	TIME [epoch: 8.33 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08308601101541335		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.08308601101541335 | validation: 0.0820916136783699]
	TIME [epoch: 8.31 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08269812352642016		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.08269812352642016 | validation: 0.06846177408505352]
	TIME [epoch: 8.31 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08787118852562353		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.08787118852562353 | validation: 0.08513613753035232]
	TIME [epoch: 8.31 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08691255897685826		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.08691255897685826 | validation: 0.07921959264663438]
	TIME [epoch: 8.33 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08042820666597653		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.08042820666597653 | validation: 0.07718955253892454]
	TIME [epoch: 8.31 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0881922894861422		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.0881922894861422 | validation: 0.07828976916735]
	TIME [epoch: 8.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08557202208604406		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.08557202208604406 | validation: 0.07632070932614415]
	TIME [epoch: 8.32 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08775118835848165		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.08775118835848165 | validation: 0.07569872258636587]
	TIME [epoch: 8.33 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08885152800157456		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.08885152800157456 | validation: 0.07121622405575054]
	TIME [epoch: 8.31 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08655146510795127		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.08655146510795127 | validation: 0.06947994508785477]
	TIME [epoch: 8.31 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09282278083711515		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.09282278083711515 | validation: 0.07427725326892744]
	TIME [epoch: 8.33 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08946014808233574		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.08946014808233574 | validation: 0.059617790903404744]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r0_20240219_233648/states/model_tr_study203_1805.pth
	Model improved!!!
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08803873973705426		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.08803873973705426 | validation: 0.07834227968070426]
	TIME [epoch: 8.31 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08698859726742611		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.08698859726742611 | validation: 0.07587277501759682]
	TIME [epoch: 8.32 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0820975213750718		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.0820975213750718 | validation: 0.07237908597782475]
	TIME [epoch: 8.35 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0810278051609881		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.0810278051609881 | validation: 0.07094162578186235]
	TIME [epoch: 8.32 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0854423021708548		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.0854423021708548 | validation: 0.06574054014009936]
	TIME [epoch: 8.32 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08390174206106829		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.08390174206106829 | validation: 0.07839037569455276]
	TIME [epoch: 8.34 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08515772892175504		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.08515772892175504 | validation: 0.07201831661208637]
	TIME [epoch: 8.33 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08777314417707885		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.08777314417707885 | validation: 0.07901739422102817]
	TIME [epoch: 8.31 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08043450147630614		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.08043450147630614 | validation: 0.06992296967672115]
	TIME [epoch: 8.32 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08183055444696608		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.08183055444696608 | validation: 0.07928391283158917]
	TIME [epoch: 8.34 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08264413151331226		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.08264413151331226 | validation: 0.07527064735094152]
	TIME [epoch: 8.32 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08341994114484505		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.08341994114484505 | validation: 0.07200762423444731]
	TIME [epoch: 8.31 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08044568538393072		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.08044568538393072 | validation: 0.07428224715902629]
	TIME [epoch: 8.32 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08449164441343368		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.08449164441343368 | validation: 0.07109326465689891]
	TIME [epoch: 8.32 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08278076922885116		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.08278076922885116 | validation: 0.07825459592934948]
	TIME [epoch: 8.31 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0877149591210938		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.0877149591210938 | validation: 0.06796271896535268]
	TIME [epoch: 8.32 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08502715821212642		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.08502715821212642 | validation: 0.08464064969586062]
	TIME [epoch: 8.34 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08698642340506582		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.08698642340506582 | validation: 0.0834303652171344]
	TIME [epoch: 8.32 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08339189582043421		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.08339189582043421 | validation: 0.09532498699601819]
	TIME [epoch: 8.31 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09296301136136573		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.09296301136136573 | validation: 0.08411847881433195]
	TIME [epoch: 8.32 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0873662095185537		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.0873662095185537 | validation: 0.08394429615864893]
	TIME [epoch: 8.34 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08478160170244588		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.08478160170244588 | validation: 0.08944198348855179]
	TIME [epoch: 8.31 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0886479449937683		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.0886479449937683 | validation: 0.0933286476900228]
	TIME [epoch: 8.31 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0940204807140693		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.0940204807140693 | validation: 0.110748294703952]
	TIME [epoch: 8.33 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10143367801331922		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.10143367801331922 | validation: 0.10950126672161671]
	TIME [epoch: 8.32 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09785690513056686		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.09785690513056686 | validation: 0.11440631471582377]
	TIME [epoch: 8.31 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09407983843264024		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.09407983843264024 | validation: 0.0978123153050478]
	TIME [epoch: 8.31 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09430806012107079		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.09430806012107079 | validation: 0.09948040377354984]
	TIME [epoch: 8.33 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09521924782990467		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.09521924782990467 | validation: 0.09633503487508702]
	TIME [epoch: 8.31 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08738337593655746		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.08738337593655746 | validation: 0.08508398030047937]
	TIME [epoch: 8.31 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0854958036460047		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.0854958036460047 | validation: 0.08071524890961765]
	TIME [epoch: 8.32 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08686110904485156		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.08686110904485156 | validation: 0.0846527202127837]
	TIME [epoch: 8.34 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08366219449623549		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.08366219449623549 | validation: 0.08696872045173148]
	TIME [epoch: 8.32 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08173849687757168		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.08173849687757168 | validation: 0.07579217364918407]
	TIME [epoch: 8.32 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0837140582502445		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.0837140582502445 | validation: 0.07272123898386401]
	TIME [epoch: 8.32 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08957278866140353		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.08957278866140353 | validation: 0.07268601675783096]
	TIME [epoch: 8.31 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08222007243971252		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.08222007243971252 | validation: 0.07617907267733684]
	TIME [epoch: 8.31 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08441378857832685		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.08441378857832685 | validation: 0.07712381203402457]
	TIME [epoch: 8.31 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08001098296664062		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.08001098296664062 | validation: 0.07509143755080316]
	TIME [epoch: 8.33 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08289129154211819		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.08289129154211819 | validation: 0.07920330881270007]
	TIME [epoch: 8.32 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08103444380312738		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.08103444380312738 | validation: 0.0732746132683882]
	TIME [epoch: 8.33 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08661615553845367		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.08661615553845367 | validation: 0.07504724706296897]
	TIME [epoch: 8.32 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0830701145988307		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.0830701145988307 | validation: 0.07147477239963268]
	TIME [epoch: 8.34 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08407637741836613		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.08407637741836613 | validation: 0.07000524412545728]
	TIME [epoch: 8.32 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08843337381959045		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.08843337381959045 | validation: 0.07809261102386002]
	TIME [epoch: 8.31 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08891813116104694		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.08891813116104694 | validation: 0.08130793740522524]
	TIME [epoch: 8.34 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0880302566021319		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.0880302566021319 | validation: 0.06281802849572424]
	TIME [epoch: 8.32 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09157122188811886		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.09157122188811886 | validation: 0.07499072635036896]
	TIME [epoch: 8.32 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09018424316669603		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.09018424316669603 | validation: 0.07869564952219088]
	TIME [epoch: 8.32 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08513492383675993		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.08513492383675993 | validation: 0.07428035555055246]
	TIME [epoch: 8.33 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08677769603349297		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.08677769603349297 | validation: 0.06573927558676514]
	TIME [epoch: 8.31 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08724493351214643		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.08724493351214643 | validation: 0.07546373410533729]
	TIME [epoch: 8.31 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08856281634038535		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.08856281634038535 | validation: 0.07243149656771353]
	TIME [epoch: 8.32 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09062118309643707		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.09062118309643707 | validation: 0.07190275160705906]
	TIME [epoch: 8.33 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09104803156343766		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.09104803156343766 | validation: 0.07071535327791015]
	TIME [epoch: 8.31 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08676057610136749		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.08676057610136749 | validation: 0.08529489688132698]
	TIME [epoch: 8.32 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08932631457571306		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.08932631457571306 | validation: 0.07190396293217113]
	TIME [epoch: 8.34 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08577022262080818		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.08577022262080818 | validation: 0.07152076477734864]
	TIME [epoch: 8.33 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08675119699947556		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.08675119699947556 | validation: 0.06898190303715325]
	TIME [epoch: 8.32 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08532379212062483		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.08532379212062483 | validation: 0.08634427621811165]
	TIME [epoch: 8.32 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08598415414778236		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.08598415414778236 | validation: 0.08554071731271323]
	TIME [epoch: 8.33 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08032552558171377		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.08032552558171377 | validation: 0.07214856895633208]
	TIME [epoch: 8.31 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08882884972497418		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.08882884972497418 | validation: 0.0799132912247144]
	TIME [epoch: 8.32 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08531076155443498		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.08531076155443498 | validation: 0.07463951895287396]
	TIME [epoch: 8.33 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.086330010703947		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.086330010703947 | validation: 0.07112866512301086]
	TIME [epoch: 8.33 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08418590840442666		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.08418590840442666 | validation: 0.07604675453945825]
	TIME [epoch: 8.32 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08356529959208334		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.08356529959208334 | validation: 0.07717745259923679]
	TIME [epoch: 8.32 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07951129443791108		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.07951129443791108 | validation: 0.07543651393649035]
	TIME [epoch: 8.34 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0842607550695876		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.0842607550695876 | validation: 0.06842758370333563]
	TIME [epoch: 8.32 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08303067689611632		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.08303067689611632 | validation: 0.07913033462535111]
	TIME [epoch: 8.32 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08346410571108755		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.08346410571108755 | validation: 0.07703843732894372]
	TIME [epoch: 8.33 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08701856931953611		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.08701856931953611 | validation: 0.06969371736462901]
	TIME [epoch: 8.33 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08404916098459728		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.08404916098459728 | validation: 0.07961956375014981]
	TIME [epoch: 8.31 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08532184465673957		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.08532184465673957 | validation: 0.0662795935744453]
	TIME [epoch: 8.31 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08295498937017944		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.08295498937017944 | validation: 0.0828055105862652]
	TIME [epoch: 8.33 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08658064670011215		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.08658064670011215 | validation: 0.07199913243727538]
	TIME [epoch: 8.31 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08153112121552135		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.08153112121552135 | validation: 0.07638233694919022]
	TIME [epoch: 8.31 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08731994170518119		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.08731994170518119 | validation: 0.07429606838242661]
	TIME [epoch: 8.32 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0881795064006632		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.0881795064006632 | validation: 0.07978944707896887]
	TIME [epoch: 8.34 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08686516616887145		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.08686516616887145 | validation: 0.08108835217603384]
	TIME [epoch: 8.31 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08090977225491422		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.08090977225491422 | validation: 0.08125556404979914]
	TIME [epoch: 8.32 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08163817579436008		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.08163817579436008 | validation: 0.07223484333181786]
	TIME [epoch: 8.33 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08712923971385408		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.08712923971385408 | validation: 0.06918316813566139]
	TIME [epoch: 8.32 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08296630914172952		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.08296630914172952 | validation: 0.07265294545936303]
	TIME [epoch: 8.31 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08685937148068076		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.08685937148068076 | validation: 0.08471052912093818]
	TIME [epoch: 8.32 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08790078780587973		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.08790078780587973 | validation: 0.07572872506078587]
	TIME [epoch: 8.34 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08584394649022457		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.08584394649022457 | validation: 0.07248316142111602]
	TIME [epoch: 8.33 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08439572460126359		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.08439572460126359 | validation: 0.08326684586438246]
	TIME [epoch: 8.32 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08455164203261123		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.08455164203261123 | validation: 0.07550652020912954]
	TIME [epoch: 8.33 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08796258497697072		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.08796258497697072 | validation: 0.0805097940573417]
	TIME [epoch: 8.34 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08309492023863282		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.08309492023863282 | validation: 0.07129689807998812]
	TIME [epoch: 8.32 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08137159984470359		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.08137159984470359 | validation: 0.08876291986497573]
	TIME [epoch: 8.32 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08432311715482807		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.08432311715482807 | validation: 0.07868767514570883]
	TIME [epoch: 8.33 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08359216441735762		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.08359216441735762 | validation: 0.06979964805391328]
	TIME [epoch: 8.32 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08241711412697432		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.08241711412697432 | validation: 0.07714887690554556]
	TIME [epoch: 8.32 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08510671035888859		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.08510671035888859 | validation: 0.0759174079409751]
	TIME [epoch: 8.33 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08236054646534793		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.08236054646534793 | validation: 0.08367886548885792]
	TIME [epoch: 8.34 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08785862023182235		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.08785862023182235 | validation: 0.0749601425055424]
	TIME [epoch: 8.31 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0841504594216987		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.0841504594216987 | validation: 0.09596347607171551]
	TIME [epoch: 8.31 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08925010660994728		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.08925010660994728 | validation: 0.08343077282617022]
	TIME [epoch: 8.33 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0878775945709293		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.0878775945709293 | validation: 0.07575281650629284]
	TIME [epoch: 8.32 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08554394404979551		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.08554394404979551 | validation: 0.08183114470374403]
	TIME [epoch: 8.31 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08751224878599459		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.08751224878599459 | validation: 0.08656451867468071]
	TIME [epoch: 8.31 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08841190115957502		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.08841190115957502 | validation: 0.09010149301828066]
	TIME [epoch: 8.34 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08699869122495968		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.08699869122495968 | validation: 0.08854220255887507]
	TIME [epoch: 8.32 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08891335898129038		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.08891335898129038 | validation: 0.08640321247282096]
	TIME [epoch: 8.32 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08670508008280053		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.08670508008280053 | validation: 0.09855612359005567]
	TIME [epoch: 8.32 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08373521431455011		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.08373521431455011 | validation: 0.09333794362344922]
	TIME [epoch: 8.34 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08903524061740842		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.08903524061740842 | validation: 0.08040175926221506]
	TIME [epoch: 8.32 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08076212097864646		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.08076212097864646 | validation: 0.07714996340936803]
	TIME [epoch: 8.32 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08313906659765767		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.08313906659765767 | validation: 0.07373251689519926]
	TIME [epoch: 8.33 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08862522660998165		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.08862522660998165 | validation: 0.08469663237208039]
	TIME [epoch: 8.33 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08183470634396808		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.08183470634396808 | validation: 0.08482123561727152]
	TIME [epoch: 8.31 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07718130977070818		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.07718130977070818 | validation: 0.07751717582854213]
	TIME [epoch: 8.31 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08255235693821387		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.08255235693821387 | validation: 0.0880131264392858]
	TIME [epoch: 8.34 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08447513740506062		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.08447513740506062 | validation: 0.06404752640086196]
	TIME [epoch: 8.32 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08383266041541146		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.08383266041541146 | validation: 0.07864824467104517]
	TIME [epoch: 8.32 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08217145861621328		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.08217145861621328 | validation: 0.07697486072279032]
	TIME [epoch: 8.32 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07878755858923155		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.07878755858923155 | validation: 0.07320450266732309]
	TIME [epoch: 8.33 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0825785195032819		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.0825785195032819 | validation: 0.08545910021366238]
	TIME [epoch: 8.32 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08898518500856753		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.08898518500856753 | validation: 0.07996071260118465]
	TIME [epoch: 8.31 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08108094015619793		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.08108094015619793 | validation: 0.07785391816256002]
	TIME [epoch: 8.34 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08999793048369067		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.08999793048369067 | validation: 0.07449719800219572]
	TIME [epoch: 8.32 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08663063197099127		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.08663063197099127 | validation: 0.07081001637725161]
	TIME [epoch: 8.32 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08442780128392328		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.08442780128392328 | validation: 0.06983491008166726]
	TIME [epoch: 8.32 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08387515591495624		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.08387515591495624 | validation: 0.07406640303077058]
	TIME [epoch: 8.34 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08664965985398802		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.08664965985398802 | validation: 0.08630747643630454]
	TIME [epoch: 8.31 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08426201484487547		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.08426201484487547 | validation: 0.07458393347797987]
	TIME [epoch: 8.32 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08441328240825453		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.08441328240825453 | validation: 0.08055053060457981]
	TIME [epoch: 8.33 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0856027839479577		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.0856027839479577 | validation: 0.0742188859391103]
	TIME [epoch: 8.32 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08606683283035843		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.08606683283035843 | validation: 0.07158067041949573]
	TIME [epoch: 8.32 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0832837021414651		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.0832837021414651 | validation: 0.07316618280133827]
	TIME [epoch: 8.31 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08604830410523927		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.08604830410523927 | validation: 0.07524740283880132]
	TIME [epoch: 8.33 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08397527322669533		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.08397527322669533 | validation: 0.07743434157124973]
	TIME [epoch: 8.32 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0855808790981312		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.0855808790981312 | validation: 0.0836087386070768]
	TIME [epoch: 8.31 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08392830346626133		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.08392830346626133 | validation: 0.06953380275006846]
	TIME [epoch: 8.31 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08664459322099924		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.08664459322099924 | validation: 0.07408166345167069]
	TIME [epoch: 8.33 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08250922124436019		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.08250922124436019 | validation: 0.08494699714556786]
	TIME [epoch: 8.31 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08251325824804631		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.08251325824804631 | validation: 0.06968220603401709]
	TIME [epoch: 8.31 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08964845996662063		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.08964845996662063 | validation: 0.06618987685059827]
	TIME [epoch: 8.34 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08556508906515932		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.08556508906515932 | validation: 0.08568715163062407]
	TIME [epoch: 8.32 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08390056771182905		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.08390056771182905 | validation: 0.08067697123919851]
	TIME [epoch: 8.32 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08673750771454723		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.08673750771454723 | validation: 0.07692714707944917]
	TIME [epoch: 8.32 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08484487799120743		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.08484487799120743 | validation: 0.08309226096394004]
	TIME [epoch: 8.33 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08571120736774003		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.08571120736774003 | validation: 0.07999863842901753]
	TIME [epoch: 8.3 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08462776897977863		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.08462776897977863 | validation: 0.06853384509200641]
	TIME [epoch: 8.31 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0805974246052086		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.0805974246052086 | validation: 0.06946956297345966]
	TIME [epoch: 8.32 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08006023070922273		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.08006023070922273 | validation: 0.08219221208142338]
	TIME [epoch: 8.31 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08657252440543166		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.08657252440543166 | validation: 0.08211924100857966]
	TIME [epoch: 8.31 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08571642656604943		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.08571642656604943 | validation: 0.08268675109846743]
	TIME [epoch: 8.3 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0849917676792161		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.0849917676792161 | validation: 0.08913144343638196]
	TIME [epoch: 8.33 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08426889711549708		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.08426889711549708 | validation: 0.07845945215093902]
	TIME [epoch: 8.31 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08222054735021807		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.08222054735021807 | validation: 0.07887440526691836]
	TIME [epoch: 8.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0843487313678994		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.0843487313678994 | validation: 0.07944403959317539]
	TIME [epoch: 8.31 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08155752342380197		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.08155752342380197 | validation: 0.0751040727820735]
	TIME [epoch: 8.34 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08464039278959828		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.08464039278959828 | validation: 0.07022849716280341]
	TIME [epoch: 8.31 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08732724299263225		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.08732724299263225 | validation: 0.07768070626052256]
	TIME [epoch: 8.31 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08615146995638451		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.08615146995638451 | validation: 0.07297353546126975]
	TIME [epoch: 8.32 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08377649272397407		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.08377649272397407 | validation: 0.07548250195408161]
	TIME [epoch: 8.31 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08504735255428882		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.08504735255428882 | validation: 0.06748052301569844]
	TIME [epoch: 8.31 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08381070538895283		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.08381070538895283 | validation: 0.07084619094153752]
	TIME [epoch: 8.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0844323285497488		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.0844323285497488 | validation: 0.07039546250409348]
	TIME [epoch: 8.33 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08415116675317189		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.08415116675317189 | validation: 0.07650535541024997]
	TIME [epoch: 8.3 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08982201483946636		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.08982201483946636 | validation: 0.07316273356038538]
	TIME [epoch: 8.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09055553056325034		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.09055553056325034 | validation: 0.07856014863727112]
	TIME [epoch: 8.31 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08757267747555149		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.08757267747555149 | validation: 0.07349162539152834]
	TIME [epoch: 8.32 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0844844846794797		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.0844844846794797 | validation: 0.06744574591724278]
	TIME [epoch: 8.31 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08559781282340218		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.08559781282340218 | validation: 0.08018317296495439]
	TIME [epoch: 8.3 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08572140134125207		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.08572140134125207 | validation: 0.0769496314155689]
	TIME [epoch: 8.34 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08591479622090312		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.08591479622090312 | validation: 0.07718788237778318]
	TIME [epoch: 8.31 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08670146114769975		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.08670146114769975 | validation: 0.07548464000675174]
	TIME [epoch: 8.31 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08233162159228602		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.08233162159228602 | validation: 0.0790195619227983]
	TIME [epoch: 8.3 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08683627039754735		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.08683627039754735 | validation: 0.06711975973156092]
	TIME [epoch: 8.33 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.090858712893993		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.090858712893993 | validation: 0.08221058862137365]
	TIME [epoch: 8.31 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08656212453875048		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.08656212453875048 | validation: 0.0776597757139384]
	TIME [epoch: 8.3 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08869156404180896		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.08869156404180896 | validation: 0.07357868688022348]
	TIME [epoch: 8.32 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08784219085203965		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.08784219085203965 | validation: 0.07421814702327752]
	TIME [epoch: 8.31 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08998890331635605		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.08998890331635605 | validation: 0.06696513527167598]
	TIME [epoch: 8.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08878198690291922		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.08878198690291922 | validation: 0.08207397581492529]
	TIME [epoch: 8.3 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09119569377039335		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.09119569377039335 | validation: 0.0746516526964715]
	TIME [epoch: 8.33 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08683466855744824		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.08683466855744824 | validation: 0.06629476823888412]
	TIME [epoch: 8.31 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08944637179779429		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.08944637179779429 | validation: 0.06693834631377207]
	TIME [epoch: 8.31 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08781011409413285		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.08781011409413285 | validation: 0.07293570150111864]
	TIME [epoch: 8.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08895378534715273		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.08895378534715273 | validation: 0.06831902525106781]
	TIME [epoch: 8.33 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08908186054332883		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.08908186054332883 | validation: 0.07773890314501045]
	TIME [epoch: 8.31 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08987128346263802		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.08987128346263802 | validation: 0.07494891140003876]
	TIME [epoch: 8.3 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08689681071177083		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.08689681071177083 | validation: 0.06891621897658903]
	TIME [epoch: 8.32 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09183734765980837		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.09183734765980837 | validation: 0.08840286604182124]
	TIME [epoch: 8.32 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.088397999416376		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.088397999416376 | validation: 0.065073026834998]
	TIME [epoch: 8.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08746449014219077		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.08746449014219077 | validation: 0.07495024311142594]
	TIME [epoch: 8.31 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08783443740715433		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.08783443740715433 | validation: 0.06381196575311807]
	TIME [epoch: 8.33 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08152486497182457		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.08152486497182457 | validation: 0.0648169948701489]
	TIME [epoch: 8.31 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08444721973782293		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.08444721973782293 | validation: 0.06238951906342444]
	TIME [epoch: 8.31 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08422076151347961		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.08422076151347961 | validation: 0.06352283122685866]
	TIME [epoch: 8.31 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08462192383388449		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.08462192383388449 | validation: 0.06736976652743176]
	TIME [epoch: 8.33 sec]
Finished training in 16816.690 seconds.
