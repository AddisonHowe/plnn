Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r2', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2517749357

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.497289579864244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.497289579864244 | validation: 10.026692485265496]
	TIME [epoch: 79.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.468872131072022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.468872131072022 | validation: 9.745189036871903]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.498813636864508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.498813636864508 | validation: 8.515906269281881]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.808750229212603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.808750229212603 | validation: 8.486004543447326]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.284941774906541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.284941774906541 | validation: 8.123443323352591]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.394174615535027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.394174615535027 | validation: 7.862062949712509]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.895312729012734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.895312729012734 | validation: 7.689120786952016]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.812883301135848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.812883301135848 | validation: 7.251504266122196]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.379272775304545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.379272775304545 | validation: 7.881039942472167]
	TIME [epoch: 8.37 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.157991400388096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.157991400388096 | validation: 6.213102386964668]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.2745742720383575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2745742720383575 | validation: 6.0998738821635]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.06150793074867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.06150793074867 | validation: 4.651717233844029]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.625625718580783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.625625718580783 | validation: 5.372249041726635]
	TIME [epoch: 8.36 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.615429564136614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.615429564136614 | validation: 4.467608209343238]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.398086437089865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.398086437089865 | validation: 4.515527815223338]
	TIME [epoch: 8.33 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.391387087657926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.391387087657926 | validation: 4.879696039922186]
	TIME [epoch: 8.33 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.29707959697396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.29707959697396 | validation: 4.551793011188703]
	TIME [epoch: 8.34 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3561027497059275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3561027497059275 | validation: 5.552552673427693]
	TIME [epoch: 8.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.335698266103239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.335698266103239 | validation: 5.294670972839809]
	TIME [epoch: 8.32 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.190265803385577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.190265803385577 | validation: 5.374884123311746]
	TIME [epoch: 8.33 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.171698605890469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.171698605890469 | validation: 4.905551666804063]
	TIME [epoch: 8.33 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.080574434738969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.080574434738969 | validation: 4.599890502898754]
	TIME [epoch: 8.35 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.069969348316803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.069969348316803 | validation: 4.394417783710453]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.01480409050662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01480409050662 | validation: 4.605465598167747]
	TIME [epoch: 8.32 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9901089557078144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9901089557078144 | validation: 4.052454750823118]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.802387740808494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.802387740808494 | validation: 6.15708291193166]
	TIME [epoch: 8.35 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.267915741677286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.267915741677286 | validation: 3.896941147746002]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.833015681308441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.833015681308441 | validation: 3.944552089967977]
	TIME [epoch: 8.32 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8166899492366904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8166899492366904 | validation: 4.212909598436778]
	TIME [epoch: 8.32 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.781707519552603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.781707519552603 | validation: 4.831891687293449]
	TIME [epoch: 8.35 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.678877885077546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.678877885077546 | validation: 4.27281693715795]
	TIME [epoch: 8.32 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.619848592265547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.619848592265547 | validation: 4.512850037811193]
	TIME [epoch: 8.32 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.588866935859977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.588866935859977 | validation: 4.370128974269162]
	TIME [epoch: 8.33 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6094608641215493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6094608641215493 | validation: 4.247780821759393]
	TIME [epoch: 8.33 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5056643560508194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5056643560508194 | validation: 4.409180515214594]
	TIME [epoch: 8.34 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5021088951517543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5021088951517543 | validation: 3.9134247026869495]
	TIME [epoch: 8.32 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2821308339238504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2821308339238504 | validation: 3.4080293554158194]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.333569640535491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.333569640535491 | validation: 4.075959136719687]
	TIME [epoch: 8.32 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.375168610279423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.375168610279423 | validation: 4.077621953877011]
	TIME [epoch: 8.35 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.301652826052486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.301652826052486 | validation: 3.9607347272219173]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4793940269410664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4793940269410664 | validation: 3.5845036661199585]
	TIME [epoch: 8.32 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1811510761222457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1811510761222457 | validation: 4.269005155839662]
	TIME [epoch: 8.32 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1847004032241237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1847004032241237 | validation: 4.112090718348762]
	TIME [epoch: 8.35 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2580564778819947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2580564778819947 | validation: 3.7685621555705113]
	TIME [epoch: 8.33 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.153269640296438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.153269640296438 | validation: 3.8344388904172897]
	TIME [epoch: 8.33 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1662584245203336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1662584245203336 | validation: 4.139171906624076]
	TIME [epoch: 8.32 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1905999005967725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1905999005967725 | validation: 3.8999099111676268]
	TIME [epoch: 8.33 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0780337288202895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0780337288202895 | validation: 4.210580603252703]
	TIME [epoch: 8.34 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.107495248047136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.107495248047136 | validation: 3.7690149022238097]
	TIME [epoch: 8.32 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1595403150903087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1595403150903087 | validation: 3.899179888442613]
	TIME [epoch: 8.32 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071825514455871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.071825514455871 | validation: 3.6964371584850033]
	TIME [epoch: 8.32 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0419090020375976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0419090020375976 | validation: 3.9858571598421917]
	TIME [epoch: 8.34 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1047649834434017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1047649834434017 | validation: 3.756478210315554]
	TIME [epoch: 8.32 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1450096218451504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1450096218451504 | validation: 3.7394609582557212]
	TIME [epoch: 8.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.028120902868315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.028120902868315 | validation: 4.0967314912006225]
	TIME [epoch: 8.32 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0291383392965696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0291383392965696 | validation: 3.8448762594749626]
	TIME [epoch: 8.35 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0865390230299985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0865390230299985 | validation: 3.5357461097995793]
	TIME [epoch: 8.32 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.878737367287914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.878737367287914 | validation: 3.8745428083032887]
	TIME [epoch: 8.32 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.991062119413314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.991062119413314 | validation: 4.41258685823925]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0891490683253315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0891490683253315 | validation: 3.5420779536620506]
	TIME [epoch: 8.32 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.874365184120886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.874365184120886 | validation: 5.220765662887523]
	TIME [epoch: 8.35 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2011246223705285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2011246223705285 | validation: 4.132849455772636]
	TIME [epoch: 8.33 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9470632711390587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9470632711390587 | validation: 2.9625182409180892]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5974760023393695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5974760023393695 | validation: 2.6685252925375744]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4869620013690397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4869620013690397 | validation: 2.71481446738911]
	TIME [epoch: 8.34 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1193191260476674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1193191260476674 | validation: 2.0006813238037724]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8002596988907236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8002596988907236 | validation: 1.9064340722142075]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.976961188367656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.976961188367656 | validation: 1.9426939407584463]
	TIME [epoch: 8.32 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.853663347248343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.853663347248343 | validation: 1.8470728175937952]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7529737731333879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7529737731333879 | validation: 2.6618803616600024]
	TIME [epoch: 8.32 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3824819758565883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3824819758565883 | validation: 1.7167017640543798]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7830285432909547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7830285432909547 | validation: 3.107405705604468]
	TIME [epoch: 8.36 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.813382847564363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.813382847564363 | validation: 3.2410761277871676]
	TIME [epoch: 8.37 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9824803718759707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9824803718759707 | validation: 1.729180137174779]
	TIME [epoch: 8.35 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.821374829593323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.821374829593323 | validation: 2.2134002075087804]
	TIME [epoch: 8.36 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8415519765176374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8415519765176374 | validation: 1.7915835648784955]
	TIME [epoch: 8.35 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.496407411949766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.496407411949766 | validation: 2.3938404684386443]
	TIME [epoch: 8.36 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7184146793398025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7184146793398025 | validation: 2.677002080866821]
	TIME [epoch: 8.37 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7448333982581836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7448333982581836 | validation: 1.8733611347151726]
	TIME [epoch: 8.35 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6712788264188876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6712788264188876 | validation: 1.4679167940357347]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5898096462625226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5898096462625226 | validation: 1.8409334799318375]
	TIME [epoch: 8.35 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.797064309966268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.797064309966268 | validation: 1.799791157457361]
	TIME [epoch: 8.37 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6337719681261553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6337719681261553 | validation: 1.3967589584504747]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5848776994581386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5848776994581386 | validation: 2.4600686280171797]
	TIME [epoch: 8.35 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.609353029778657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.609353029778657 | validation: 1.6683764842636621]
	TIME [epoch: 8.35 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7043975502753295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7043975502753295 | validation: 2.3743917786608133]
	TIME [epoch: 8.37 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6474344262606369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6474344262606369 | validation: 2.2091331024961787]
	TIME [epoch: 8.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6219319122456277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6219319122456277 | validation: 1.903933988410584]
	TIME [epoch: 8.34 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6883273841736268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6883273841736268 | validation: 1.817442425855546]
	TIME [epoch: 8.35 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6768602357376454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6768602357376454 | validation: 2.6735593972921285]
	TIME [epoch: 8.36 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8429607280859592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8429607280859592 | validation: 2.0008734487629627]
	TIME [epoch: 8.36 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5008310856975875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5008310856975875 | validation: 2.067925428093764]
	TIME [epoch: 8.34 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.603391542597081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.603391542597081 | validation: 1.8739784529972874]
	TIME [epoch: 8.34 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6635230233227454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6635230233227454 | validation: 2.489842367971522]
	TIME [epoch: 8.35 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7081814123891348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7081814123891348 | validation: 1.3628511901455882]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6607558437056604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6607558437056604 | validation: 1.971706015708806]
	TIME [epoch: 8.35 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4481756281990052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4481756281990052 | validation: 2.3191616153419945]
	TIME [epoch: 8.34 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4763427954372939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4763427954372939 | validation: 1.2289944268376483]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.588129562839696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.588129562839696 | validation: 1.8824321480586654]
	TIME [epoch: 8.36 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5988979431672161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5988979431672161 | validation: 1.2981910143027529]
	TIME [epoch: 8.34 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3971002751393615		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 1.3971002751393615 | validation: 2.3574393285565973]
	TIME [epoch: 8.34 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.442789055587644		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 1.442789055587644 | validation: 1.2906466989144736]
	TIME [epoch: 8.33 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4866737479460177		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 1.4866737479460177 | validation: 1.3155698334788728]
	TIME [epoch: 8.36 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1864775203455773		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 1.1864775203455773 | validation: 1.2279258764706393]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.475562230502651		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 1.475562230502651 | validation: 1.4376526429245728]
	TIME [epoch: 8.34 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3740875441777176		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 1.3740875441777176 | validation: 1.680546996562588]
	TIME [epoch: 8.33 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.534550820999619		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 1.534550820999619 | validation: 2.0025380652349822]
	TIME [epoch: 8.34 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5285007480219637		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 1.5285007480219637 | validation: 1.9715507258761427]
	TIME [epoch: 8.36 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5514530197682306		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 1.5514530197682306 | validation: 2.197236514009247]
	TIME [epoch: 8.34 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6269727410498938		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 1.6269727410498938 | validation: 1.4236579825721425]
	TIME [epoch: 8.34 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5004306590158938		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 1.5004306590158938 | validation: 1.3271119052413647]
	TIME [epoch: 8.34 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.233767010938718		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 1.233767010938718 | validation: 1.2344224035555467]
	TIME [epoch: 8.36 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4757976208314825		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 1.4757976208314825 | validation: 2.0724997221997006]
	TIME [epoch: 8.34 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.369365344251052		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 1.369365344251052 | validation: 1.6852308964683131]
	TIME [epoch: 8.34 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3705067558085686		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 1.3705067558085686 | validation: 1.804251641305533]
	TIME [epoch: 8.33 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5090001171509948		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 1.5090001171509948 | validation: 1.4310810082070446]
	TIME [epoch: 8.36 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.426866475797905		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 1.426866475797905 | validation: 1.2839644693518961]
	TIME [epoch: 8.35 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4199580974536037		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 1.4199580974536037 | validation: 2.116414070629534]
	TIME [epoch: 8.34 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3669169053089363		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 1.3669169053089363 | validation: 1.7151473677268738]
	TIME [epoch: 8.34 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3186961726093667		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 1.3186961726093667 | validation: 2.5086146496780795]
	TIME [epoch: 8.34 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4995059993343518		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 1.4995059993343518 | validation: 2.7218418675938607]
	TIME [epoch: 8.36 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.434770643478192		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 1.434770643478192 | validation: 1.4885181987928846]
	TIME [epoch: 8.34 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2558225240097185		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 1.2558225240097185 | validation: 2.3031958126291396]
	TIME [epoch: 8.34 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.397870069541795		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 1.397870069541795 | validation: 1.618250582957836]
	TIME [epoch: 8.34 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1987536836107473		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 1.1987536836107473 | validation: 1.4402283639308933]
	TIME [epoch: 8.36 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.317826492212839		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 1.317826492212839 | validation: 1.0027742234860995]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.304009760488482		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 1.304009760488482 | validation: 2.504305306867459]
	TIME [epoch: 8.34 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.501986015692348		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 1.501986015692348 | validation: 1.3195351089555314]
	TIME [epoch: 8.33 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5367812484674601		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 1.5367812484674601 | validation: 1.7768011622143318]
	TIME [epoch: 8.37 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3428296703524008		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 1.3428296703524008 | validation: 1.8423854501828294]
	TIME [epoch: 8.34 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4558830185260954		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 1.4558830185260954 | validation: 2.0949059485529657]
	TIME [epoch: 8.33 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4801186067775092		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 1.4801186067775092 | validation: 1.5194176782785118]
	TIME [epoch: 8.33 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3760338423275125		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 1.3760338423275125 | validation: 1.6045784182631448]
	TIME [epoch: 8.34 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3857262690344274		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 1.3857262690344274 | validation: 1.2649904069603997]
	TIME [epoch: 8.36 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.419219396623164		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 1.419219396623164 | validation: 3.4201833241987907]
	TIME [epoch: 8.34 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.624133333835187		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 1.624133333835187 | validation: 1.0797632018591596]
	TIME [epoch: 8.33 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.317772435155676		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 1.317772435155676 | validation: 1.112340485019964]
	TIME [epoch: 8.34 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2128428121252317		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 1.2128428121252317 | validation: 0.9960914686863767]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.248465625302424		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 1.248465625302424 | validation: 1.5727075785409572]
	TIME [epoch: 8.35 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.393891318720237		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 1.393891318720237 | validation: 2.010726843210457]
	TIME [epoch: 8.34 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3393611904645737		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 1.3393611904645737 | validation: 1.327281733017239]
	TIME [epoch: 8.34 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3109868442111494		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 1.3109868442111494 | validation: 1.496086972953613]
	TIME [epoch: 8.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1674342352233673		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 1.1674342352233673 | validation: 1.3168821489799654]
	TIME [epoch: 8.34 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1393879847230974		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 1.1393879847230974 | validation: 1.4682910867059453]
	TIME [epoch: 8.34 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3196948324234392		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 1.3196948324234392 | validation: 1.2871463478826368]
	TIME [epoch: 8.33 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1628649406355744		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 1.1628649406355744 | validation: 2.0523447566889264]
	TIME [epoch: 8.34 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8735974853800434		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 1.8735974853800434 | validation: 1.5081203369010994]
	TIME [epoch: 8.37 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2745224233194778		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 1.2745224233194778 | validation: 1.497800687848548]
	TIME [epoch: 8.33 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.174988406416435		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 1.174988406416435 | validation: 1.207818105945134]
	TIME [epoch: 8.33 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2838871766394737		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 1.2838871766394737 | validation: 1.3303769165906334]
	TIME [epoch: 8.33 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1598536731428322		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 1.1598536731428322 | validation: 1.5553212532970462]
	TIME [epoch: 8.36 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1487184689733823		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 1.1487184689733823 | validation: 1.2398277564301399]
	TIME [epoch: 8.34 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2282736694234777		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 1.2282736694234777 | validation: 1.0832932806481523]
	TIME [epoch: 8.34 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3009848799301715		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 1.3009848799301715 | validation: 1.4412385818865663]
	TIME [epoch: 8.33 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2087970446802991		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 1.2087970446802991 | validation: 1.2656320356203214]
	TIME [epoch: 8.36 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9944227317637464		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 0.9944227317637464 | validation: 1.9934333797869337]
	TIME [epoch: 8.34 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.311939949279763		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 1.311939949279763 | validation: 1.825986981424812]
	TIME [epoch: 8.34 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2478326172227945		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 1.2478326172227945 | validation: 1.388600581519813]
	TIME [epoch: 8.34 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.215295793185548		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 1.215295793185548 | validation: 1.1557241574402872]
	TIME [epoch: 8.34 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1758617746829505		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 1.1758617746829505 | validation: 1.1635475334297765]
	TIME [epoch: 8.36 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.09238664299202		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 1.09238664299202 | validation: 1.3474602203189636]
	TIME [epoch: 8.34 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3119202063020272		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 1.3119202063020272 | validation: 1.379446927220887]
	TIME [epoch: 8.34 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.322555097524236		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 1.322555097524236 | validation: 1.0518363529583385]
	TIME [epoch: 8.33 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2636102046233042		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 1.2636102046233042 | validation: 1.591659950214304]
	TIME [epoch: 8.36 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1976929726518792		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 1.1976929726518792 | validation: 1.174882932076983]
	TIME [epoch: 8.34 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2289479352556816		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 1.2289479352556816 | validation: 1.0017035187214725]
	TIME [epoch: 8.33 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0595855291893561		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 1.0595855291893561 | validation: 1.8155190110767743]
	TIME [epoch: 8.33 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0654336495419332		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 1.0654336495419332 | validation: 1.4324378885673497]
	TIME [epoch: 8.36 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0558572911501571		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 1.0558572911501571 | validation: 1.5058558779639408]
	TIME [epoch: 8.34 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2519328230925437		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 1.2519328230925437 | validation: 0.8849319135137307]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2446735520543801		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 1.2446735520543801 | validation: 1.3568111831608642]
	TIME [epoch: 8.33 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2856807386016424		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 1.2856807386016424 | validation: 0.9176636970795893]
	TIME [epoch: 8.33 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0346011041581054		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 1.0346011041581054 | validation: 0.8620751607640738]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.000480065935458		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 1.000480065935458 | validation: 1.064204774162942]
	TIME [epoch: 8.33 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0808119652792558		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 1.0808119652792558 | validation: 1.3204722211122304]
	TIME [epoch: 8.33 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6288831484342066		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 1.6288831484342066 | validation: 0.8636790319721122]
	TIME [epoch: 8.33 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0139821843191614		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 1.0139821843191614 | validation: 0.9586821885848686]
	TIME [epoch: 8.35 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1578391334964553		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 1.1578391334964553 | validation: 1.6478392696276023]
	TIME [epoch: 8.33 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0268676088019815		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 1.0268676088019815 | validation: 1.8580646794399778]
	TIME [epoch: 8.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9790388930654977		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 0.9790388930654977 | validation: 1.6556552239182463]
	TIME [epoch: 8.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1513413949250169		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 1.1513413949250169 | validation: 1.2621668197374687]
	TIME [epoch: 8.34 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0790901456311563		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 1.0790901456311563 | validation: 1.0715216559182734]
	TIME [epoch: 8.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.077178411657675		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 1.077178411657675 | validation: 0.994649105620335]
	TIME [epoch: 8.33 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0504778240442427		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 1.0504778240442427 | validation: 1.358210467938358]
	TIME [epoch: 8.33 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1318320190116897		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 1.1318320190116897 | validation: 1.0083104771244058]
	TIME [epoch: 8.33 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0709262913134612		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 1.0709262913134612 | validation: 0.9334377239898006]
	TIME [epoch: 8.35 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1748249252179297		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 1.1748249252179297 | validation: 1.5480321810540374]
	TIME [epoch: 8.33 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0396107113680801		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 1.0396107113680801 | validation: 1.3949689245013621]
	TIME [epoch: 8.33 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.052271780479893		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 1.052271780479893 | validation: 1.6871089027197492]
	TIME [epoch: 8.32 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0333362404797708		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 1.0333362404797708 | validation: 1.2691019656503668]
	TIME [epoch: 8.34 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9649641742438868		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 0.9649641742438868 | validation: 1.1229625606138756]
	TIME [epoch: 8.32 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1925148595192998		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 1.1925148595192998 | validation: 0.8629010873651757]
	TIME [epoch: 8.32 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0222587648875483		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 1.0222587648875483 | validation: 1.0767095568982983]
	TIME [epoch: 8.32 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9133362856328417		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 0.9133362856328417 | validation: 0.802671731784238]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1990368346445088		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 1.1990368346445088 | validation: 1.3110672660695624]
	TIME [epoch: 8.33 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0620113567000034		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 1.0620113567000034 | validation: 1.135385050339858]
	TIME [epoch: 8.32 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.06558501737779		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 1.06558501737779 | validation: 1.5945341518947027]
	TIME [epoch: 8.32 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0827787925638244		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 1.0827787925638244 | validation: 1.2463220344383117]
	TIME [epoch: 8.32 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1012194804130762		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 1.1012194804130762 | validation: 1.2047376829283811]
	TIME [epoch: 8.34 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0371030262355156		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 2.0371030262355156 | validation: 2.278543744609922]
	TIME [epoch: 8.32 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1064730240609397		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 1.1064730240609397 | validation: 1.0877934868025543]
	TIME [epoch: 8.31 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0438172203179752		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 1.0438172203179752 | validation: 0.9668813725860258]
	TIME [epoch: 8.33 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9018600066357987		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 0.9018600066357987 | validation: 1.1934217665414653]
	TIME [epoch: 8.34 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0382376971365788		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 1.0382376971365788 | validation: 1.445656116346587]
	TIME [epoch: 8.32 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9718664708293261		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 0.9718664708293261 | validation: 1.2094307634815975]
	TIME [epoch: 8.32 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9316432808306659		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 0.9316432808306659 | validation: 1.4808064113071682]
	TIME [epoch: 8.32 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9833526368838814		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 0.9833526368838814 | validation: 1.1482670255636998]
	TIME [epoch: 8.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3661658028653778		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 1.3661658028653778 | validation: 1.3018467971489205]
	TIME [epoch: 8.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8610651474044253		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 0.8610651474044253 | validation: 1.4315628730950685]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9777589890999856		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 0.9777589890999856 | validation: 1.2871652151981352]
	TIME [epoch: 8.33 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9719111939832489		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 0.9719111939832489 | validation: 1.3420254400051124]
	TIME [epoch: 8.33 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578300030151643		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 0.9578300030151643 | validation: 1.24411993781499]
	TIME [epoch: 8.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9207921426654521		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 0.9207921426654521 | validation: 0.6796760207620147]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9504273007006283		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 0.9504273007006283 | validation: 0.8255789624718743]
	TIME [epoch: 8.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1160861379146094		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 1.1160861379146094 | validation: 1.1180600023265033]
	TIME [epoch: 8.34 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9225203956832914		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 0.9225203956832914 | validation: 1.1237584249901398]
	TIME [epoch: 8.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9177208197435134		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 0.9177208197435134 | validation: 0.9678376860882308]
	TIME [epoch: 8.33 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9735992610622628		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 0.9735992610622628 | validation: 1.6905258153015428]
	TIME [epoch: 8.32 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9410687104751269		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 0.9410687104751269 | validation: 1.1569409200103178]
	TIME [epoch: 8.32 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1157284468926287		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 1.1157284468926287 | validation: 1.2459008008398826]
	TIME [epoch: 8.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0113840088808617		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 1.0113840088808617 | validation: 0.6329082950286783]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9933280621844334		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 0.9933280621844334 | validation: 1.0528481324175691]
	TIME [epoch: 8.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8674328196108426		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 0.8674328196108426 | validation: 1.0183147891700222]
	TIME [epoch: 8.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7949283210613619		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 0.7949283210613619 | validation: 0.8118551548468537]
	TIME [epoch: 8.37 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9961361928530479		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 0.9961361928530479 | validation: 0.9102129036068511]
	TIME [epoch: 8.37 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8153445923008402		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 0.8153445923008402 | validation: 1.2434509933332243]
	TIME [epoch: 8.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0473361915506956		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 1.0473361915506956 | validation: 0.7665011564411843]
	TIME [epoch: 8.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.931210741607692		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 0.931210741607692 | validation: 1.401063360826646]
	TIME [epoch: 8.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8691486871217332		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 0.8691486871217332 | validation: 0.8012749805592254]
	TIME [epoch: 8.37 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1933879799106992		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 1.1933879799106992 | validation: 1.0954226715220792]
	TIME [epoch: 8.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8425256476920042		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 0.8425256476920042 | validation: 1.570503824028862]
	TIME [epoch: 8.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9721371384731559		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 0.9721371384731559 | validation: 1.1888439248968898]
	TIME [epoch: 8.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9386454906451167		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 0.9386454906451167 | validation: 1.364529186718838]
	TIME [epoch: 8.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8304997999559488		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 0.8304997999559488 | validation: 0.8745537948746891]
	TIME [epoch: 8.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8967466155676943		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 0.8967466155676943 | validation: 1.4144254904713494]
	TIME [epoch: 8.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8931341187571814		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 0.8931341187571814 | validation: 1.1474041000971087]
	TIME [epoch: 8.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9376804698769095		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 0.9376804698769095 | validation: 0.9670508315834452]
	TIME [epoch: 8.37 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8738391764548057		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 0.8738391764548057 | validation: 0.8420549163277145]
	TIME [epoch: 8.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8840356824923894		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 0.8840356824923894 | validation: 0.768010316778992]
	TIME [epoch: 8.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7651173677000453		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 0.7651173677000453 | validation: 0.618456902423697]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8765042611108832		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 0.8765042611108832 | validation: 1.5339396259287943]
	TIME [epoch: 8.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.023277747553418		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 1.023277747553418 | validation: 0.7413323516028574]
	TIME [epoch: 8.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529521334076823		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 0.9529521334076823 | validation: 1.243337181462812]
	TIME [epoch: 8.34 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0098096095926352		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 1.0098096095926352 | validation: 1.0639269825674684]
	TIME [epoch: 8.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6675821595441327		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 1.6675821595441327 | validation: 3.6424370054586337]
	TIME [epoch: 8.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1100363891928502		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 1.1100363891928502 | validation: 1.012594101403835]
	TIME [epoch: 8.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8054136231755054		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 0.8054136231755054 | validation: 0.7652368878333605]
	TIME [epoch: 8.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.267614931035845		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 1.267614931035845 | validation: 0.9072585609072905]
	TIME [epoch: 8.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7377024404645555		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 0.7377024404645555 | validation: 0.8268279632418418]
	TIME [epoch: 8.34 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8048031272989435		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 0.8048031272989435 | validation: 1.2690272342819116]
	TIME [epoch: 8.37 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9274200025508936		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 0.9274200025508936 | validation: 0.6958697322848872]
	TIME [epoch: 8.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.888900722547536		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 0.888900722547536 | validation: 1.121953903237964]
	TIME [epoch: 8.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.891955020305117		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 0.891955020305117 | validation: 0.7022019243545221]
	TIME [epoch: 8.34 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.807300266628374		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 0.807300266628374 | validation: 0.735481372431014]
	TIME [epoch: 8.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6870872628151888		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 0.6870872628151888 | validation: 1.6274448665552992]
	TIME [epoch: 8.37 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9367477849263961		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 0.9367477849263961 | validation: 0.7417800805153458]
	TIME [epoch: 8.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7748939723210211		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 0.7748939723210211 | validation: 0.8463122180596453]
	TIME [epoch: 8.34 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9088325504157989		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 0.9088325504157989 | validation: 0.7750061658381123]
	TIME [epoch: 8.34 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8691442080056305		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 0.8691442080056305 | validation: 0.7267559660149684]
	TIME [epoch: 8.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8742563212737252		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 0.8742563212737252 | validation: 1.1667354192167203]
	TIME [epoch: 8.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7758609714810574		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 0.7758609714810574 | validation: 1.3621369853304401]
	TIME [epoch: 8.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9932337897550921		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 0.9932337897550921 | validation: 0.6151939017731682]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.803256474613835		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 0.803256474613835 | validation: 0.6765925910095998]
	TIME [epoch: 8.37 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7855955250060971		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 0.7855955250060971 | validation: 0.8742939200689459]
	TIME [epoch: 8.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8645889960374225		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 0.8645889960374225 | validation: 0.7174427626310187]
	TIME [epoch: 8.34 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9146937013326504		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 0.9146937013326504 | validation: 0.5569302645390801]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.177052362990011		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 1.177052362990011 | validation: 1.2626580308703859]
	TIME [epoch: 8.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7445120987506388		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 0.7445120987506388 | validation: 0.7084590614331219]
	TIME [epoch: 8.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8375475751159278		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 0.8375475751159278 | validation: 0.6557971169869705]
	TIME [epoch: 8.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.812623286091777		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 0.812623286091777 | validation: 0.9317404404098897]
	TIME [epoch: 8.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7739899674276555		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 0.7739899674276555 | validation: 1.193852855643199]
	TIME [epoch: 8.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0283508260790242		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 1.0283508260790242 | validation: 0.5767201900587942]
	TIME [epoch: 8.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7551710291485975		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 0.7551710291485975 | validation: 0.69770213107975]
	TIME [epoch: 8.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612525588297988		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 0.9612525588297988 | validation: 0.5915119679552885]
	TIME [epoch: 8.34 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8882457796896587		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 0.8882457796896587 | validation: 0.6917042634200281]
	TIME [epoch: 8.34 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7356773547707327		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 0.7356773547707327 | validation: 1.0710822163587943]
	TIME [epoch: 8.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.821488480159138		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 0.821488480159138 | validation: 0.927091873696396]
	TIME [epoch: 8.34 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8063259646684602		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 0.8063259646684602 | validation: 0.6454389496590331]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7029369438398275		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 0.7029369438398275 | validation: 0.5999512435278097]
	TIME [epoch: 8.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8603017732578262		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 0.8603017732578262 | validation: 0.9027045809917618]
	TIME [epoch: 8.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8764981202546738		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 0.8764981202546738 | validation: 0.7633733580730904]
	TIME [epoch: 8.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7030829008993831		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 0.7030829008993831 | validation: 0.8581403403803349]
	TIME [epoch: 8.34 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7014508765996287		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 0.7014508765996287 | validation: 0.818753917457776]
	TIME [epoch: 8.34 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553328770865651		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 0.9553328770865651 | validation: 1.477247861985195]
	TIME [epoch: 8.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7315000941036842		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 0.7315000941036842 | validation: 0.8044906724434824]
	TIME [epoch: 8.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.943060431120499		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 0.943060431120499 | validation: 0.6427095589765406]
	TIME [epoch: 8.34 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7787452303452063		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 0.7787452303452063 | validation: 0.5281298005501631]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7187670133231993		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 0.7187670133231993 | validation: 0.554471260004388]
	TIME [epoch: 8.34 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8996343975535839		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 0.8996343975535839 | validation: 0.5513405214331807]
	TIME [epoch: 8.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.826200175781518		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 0.826200175781518 | validation: 1.4317506482853861]
	TIME [epoch: 8.33 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7251731229400351		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 0.7251731229400351 | validation: 0.5846346069917565]
	TIME [epoch: 8.33 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7746892791863423		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 0.7746892791863423 | validation: 0.7350919269079539]
	TIME [epoch: 8.33 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8845686566746889		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 0.8845686566746889 | validation: 1.2950972371879148]
	TIME [epoch: 8.33 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7382203301195649		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 0.7382203301195649 | validation: 0.5656214167397393]
	TIME [epoch: 8.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6685482182760858		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 0.6685482182760858 | validation: 1.2830473573450498]
	TIME [epoch: 8.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7781142532712251		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 0.7781142532712251 | validation: 0.49415234292372356]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7767689124782584		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 0.7767689124782584 | validation: 1.058575615258007]
	TIME [epoch: 8.34 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1681326716918468		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 1.1681326716918468 | validation: 0.6163275060852033]
	TIME [epoch: 8.45 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7870008125855341		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 0.7870008125855341 | validation: 1.0516068483550975]
	TIME [epoch: 8.34 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9219649233558563		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 0.9219649233558563 | validation: 1.0780747539048794]
	TIME [epoch: 8.34 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7043418230508892		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 0.7043418230508892 | validation: 0.8681772615992005]
	TIME [epoch: 8.33 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7530911128262832		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 0.7530911128262832 | validation: 0.9198954773865663]
	TIME [epoch: 8.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7491820828726194		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 0.7491820828726194 | validation: 1.4357110922179026]
	TIME [epoch: 8.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.193631572725374		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 1.193631572725374 | validation: 0.8369359715192034]
	TIME [epoch: 8.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7204108689822879		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 0.7204108689822879 | validation: 0.6976902948489656]
	TIME [epoch: 8.34 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6637208199394125		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 0.6637208199394125 | validation: 0.814210147393774]
	TIME [epoch: 8.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6713566022698293		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 0.6713566022698293 | validation: 0.7304612949177585]
	TIME [epoch: 8.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0063052937824255		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 1.0063052937824255 | validation: 1.4945759100152398]
	TIME [epoch: 8.33 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7491528970552526		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 0.7491528970552526 | validation: 0.579602989547519]
	TIME [epoch: 8.33 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8234736991812415		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 0.8234736991812415 | validation: 0.7831435766366026]
	TIME [epoch: 8.33 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6568929286524502		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 0.6568929286524502 | validation: 0.7530127879522903]
	TIME [epoch: 8.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7144854689074006		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 0.7144854689074006 | validation: 0.6479826516938408]
	TIME [epoch: 8.33 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6851107068599489		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 0.6851107068599489 | validation: 0.636847083920679]
	TIME [epoch: 8.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6770449874285908		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 0.6770449874285908 | validation: 0.5492090664267358]
	TIME [epoch: 8.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7364081946485004		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 0.7364081946485004 | validation: 0.7776568175551034]
	TIME [epoch: 8.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6947659666973005		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 0.6947659666973005 | validation: 0.4339284984201803]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6127269582409024		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 0.6127269582409024 | validation: 0.7053649566143927]
	TIME [epoch: 8.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8829438614921701		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 0.8829438614921701 | validation: 0.6220133771220153]
	TIME [epoch: 8.34 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6251724539501881		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 0.6251724539501881 | validation: 0.690657728901912]
	TIME [epoch: 8.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8028045932925242		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 0.8028045932925242 | validation: 0.6351587039309841]
	TIME [epoch: 8.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7840704311748736		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 0.7840704311748736 | validation: 0.7795476902787293]
	TIME [epoch: 8.34 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6306458029180392		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 0.6306458029180392 | validation: 0.6446323189850487]
	TIME [epoch: 8.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7391194790371143		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 0.7391194790371143 | validation: 0.6338229514888873]
	TIME [epoch: 8.33 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7506683383104599		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 0.7506683383104599 | validation: 0.7340271612636613]
	TIME [epoch: 8.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6647181215850014		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 0.6647181215850014 | validation: 0.9835835657037455]
	TIME [epoch: 8.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6280688358995754		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 0.6280688358995754 | validation: 1.220029355508745]
	TIME [epoch: 8.33 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6856154544205083		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 0.6856154544205083 | validation: 1.1733009952529239]
	TIME [epoch: 8.33 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6486985224734467		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 0.6486985224734467 | validation: 0.4308906094758869]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6670881689206628		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 0.6670881689206628 | validation: 0.564105327555224]
	TIME [epoch: 8.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7648562261811277		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 0.7648562261811277 | validation: 1.1228323490328858]
	TIME [epoch: 8.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.072009913610465		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 2.072009913610465 | validation: 1.185617634904166]
	TIME [epoch: 8.33 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6915484589917834		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 0.6915484589917834 | validation: 0.8006511898336269]
	TIME [epoch: 8.34 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6904337650048773		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 0.6904337650048773 | validation: 0.44438326951376517]
	TIME [epoch: 8.34 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6256357509873849		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 0.6256357509873849 | validation: 0.44023799814252457]
	TIME [epoch: 8.33 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7430851361665433		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 0.7430851361665433 | validation: 0.8942744519229555]
	TIME [epoch: 8.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.837413404626696		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 0.837413404626696 | validation: 0.864892919780317]
	TIME [epoch: 8.33 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.698684549095352		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 0.698684549095352 | validation: 0.8369030518300309]
	TIME [epoch: 8.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5631229189572491		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 0.5631229189572491 | validation: 0.5754376204737275]
	TIME [epoch: 8.33 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7704883896757105		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 0.7704883896757105 | validation: 0.7698541536393088]
	TIME [epoch: 8.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.554682526717307		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 0.554682526717307 | validation: 0.7067861969443898]
	TIME [epoch: 8.33 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6768105880421484		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 0.6768105880421484 | validation: 0.6368383763552432]
	TIME [epoch: 8.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6514480973579942		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 0.6514480973579942 | validation: 0.534944167684456]
	TIME [epoch: 8.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5692991334824509		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 0.5692991334824509 | validation: 0.45227584856470193]
	TIME [epoch: 8.32 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6722760164115356		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 0.6722760164115356 | validation: 0.718845973364207]
	TIME [epoch: 8.32 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5974405456367012		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 0.5974405456367012 | validation: 0.7683084395295836]
	TIME [epoch: 8.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7125511669236142		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 0.7125511669236142 | validation: 1.0474070301180105]
	TIME [epoch: 8.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6570386224740135		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 0.6570386224740135 | validation: 0.6885724171023371]
	TIME [epoch: 8.33 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6574905057759216		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 0.6574905057759216 | validation: 0.48425715675171443]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5847067738350522		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 0.5847067738350522 | validation: 0.6345077942277685]
	TIME [epoch: 8.32 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5869731416914957		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 0.5869731416914957 | validation: 0.6774345691287205]
	TIME [epoch: 8.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5761801841456695		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 0.5761801841456695 | validation: 0.9498522461364097]
	TIME [epoch: 8.33 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5549121772541188		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 0.5549121772541188 | validation: 0.6975963416007285]
	TIME [epoch: 8.33 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5436014424426641		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 0.5436014424426641 | validation: 0.44747454394321284]
	TIME [epoch: 8.32 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7300798507144203		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 0.7300798507144203 | validation: 0.6208330864542735]
	TIME [epoch: 8.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6972867571590059		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 0.6972867571590059 | validation: 0.688301634330267]
	TIME [epoch: 8.32 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6623064371690389		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 0.6623064371690389 | validation: 0.4348573661203887]
	TIME [epoch: 8.32 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8357529613580603		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 0.8357529613580603 | validation: 0.5259600467286192]
	TIME [epoch: 8.33 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5298934649006521		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 0.5298934649006521 | validation: 0.6375481389258464]
	TIME [epoch: 8.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5700835354356655		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 0.5700835354356655 | validation: 0.6855794480729965]
	TIME [epoch: 8.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6273324425382085		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 0.6273324425382085 | validation: 0.5865293015069941]
	TIME [epoch: 8.33 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7543566818032241		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 0.7543566818032241 | validation: 0.44864978083289797]
	TIME [epoch: 8.33 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6496960382978618		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 0.6496960382978618 | validation: 0.5914430512351909]
	TIME [epoch: 8.32 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5286506952753427		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 0.5286506952753427 | validation: 0.46284679950210245]
	TIME [epoch: 8.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5764798744557013		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 0.5764798744557013 | validation: 0.9303898497176066]
	TIME [epoch: 8.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8701378405506937		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 0.8701378405506937 | validation: 0.5658484020734871]
	TIME [epoch: 8.33 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5599432017596065		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 0.5599432017596065 | validation: 0.6389580365502732]
	TIME [epoch: 8.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6391767688829979		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 0.6391767688829979 | validation: 1.4728984405546526]
	TIME [epoch: 8.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6031033076596263		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 0.6031033076596263 | validation: 0.4174570937562966]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5918443552003432		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 0.5918443552003432 | validation: 0.4069142556172]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49460394141796		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 0.49460394141796 | validation: 0.6298480810559921]
	TIME [epoch: 8.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.554277550288088		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 0.554277550288088 | validation: 2.4672921270327786]
	TIME [epoch: 8.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7742462005515387		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 0.7742462005515387 | validation: 0.5016610172605271]
	TIME [epoch: 8.33 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6609087204337353		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 0.6609087204337353 | validation: 0.7049561361391834]
	TIME [epoch: 8.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.680889538860772		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 0.680889538860772 | validation: 0.8925954001986582]
	TIME [epoch: 8.32 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5392574673628221		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 0.5392574673628221 | validation: 0.9207636969045117]
	TIME [epoch: 8.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7283147488033286		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 0.7283147488033286 | validation: 0.612918933712431]
	TIME [epoch: 8.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5660582953555515		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 0.5660582953555515 | validation: 0.8786736282767554]
	TIME [epoch: 8.32 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6235635282646309		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 0.6235635282646309 | validation: 0.5844494850819963]
	TIME [epoch: 8.32 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8310209257273722		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 1.8310209257273722 | validation: 3.6734858074489023]
	TIME [epoch: 8.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3491880292988268		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 1.3491880292988268 | validation: 0.5210028889026909]
	TIME [epoch: 8.34 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7129078340002508		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 0.7129078340002508 | validation: 0.4804283471546863]
	TIME [epoch: 8.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5081424913003982		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 0.5081424913003982 | validation: 0.39210214494629814]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.832814816864607		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 0.832814816864607 | validation: 0.46821211051787526]
	TIME [epoch: 8.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5369004442426947		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 0.5369004442426947 | validation: 1.0010602936685922]
	TIME [epoch: 8.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5880632889010889		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 0.5880632889010889 | validation: 0.4784159568142773]
	TIME [epoch: 8.33 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6046264806231878		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 0.6046264806231878 | validation: 0.414474119585056]
	TIME [epoch: 8.33 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7295276020659195		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 0.7295276020659195 | validation: 1.4853031994580455]
	TIME [epoch: 8.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5800725423962316		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 0.5800725423962316 | validation: 0.874856089573515]
	TIME [epoch: 8.33 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5717307547956725		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 0.5717307547956725 | validation: 0.7895032838236208]
	TIME [epoch: 8.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.631381765721432		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 0.631381765721432 | validation: 0.671147697252048]
	TIME [epoch: 8.33 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5749597492329878		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 0.5749597492329878 | validation: 0.35929145247195116]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.60010420106568		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 0.60010420106568 | validation: 0.8518085891319611]
	TIME [epoch: 8.33 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5724403828181333		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 0.5724403828181333 | validation: 0.43274293499493205]
	TIME [epoch: 8.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5043184756003142		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 0.5043184756003142 | validation: 0.7546719326399378]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5766969338946242		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 0.5766969338946242 | validation: 0.9779563521171225]
	TIME [epoch: 8.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5697950192429294		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 0.5697950192429294 | validation: 0.41533171891535015]
	TIME [epoch: 8.33 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5989512365746725		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 0.5989512365746725 | validation: 1.5674043617204962]
	TIME [epoch: 8.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7582684774943157		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 0.7582684774943157 | validation: 0.9873674896351848]
	TIME [epoch: 8.33 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5759829625135116		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 0.5759829625135116 | validation: 0.43142117305377714]
	TIME [epoch: 8.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7364113746576689		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 0.7364113746576689 | validation: 0.44284095143172475]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7094588594377867		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 0.7094588594377867 | validation: 0.4754122661029077]
	TIME [epoch: 8.33 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7231523858442156		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 0.7231523858442156 | validation: 0.5930218259553821]
	TIME [epoch: 8.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.512079512639942		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 0.512079512639942 | validation: 1.1028820503429864]
	TIME [epoch: 8.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5517400529192849		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 0.5517400529192849 | validation: 0.4940356408242533]
	TIME [epoch: 8.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5700121266828668		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 0.5700121266828668 | validation: 0.4098922098506042]
	TIME [epoch: 8.33 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48180168418876085		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 0.48180168418876085 | validation: 0.9099898800014699]
	TIME [epoch: 8.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5551243531430669		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 0.5551243531430669 | validation: 3.8619542494435573]
	TIME [epoch: 8.34 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9485973922026425		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 0.9485973922026425 | validation: 0.7518909418125743]
	TIME [epoch: 8.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5226941556859821		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 0.5226941556859821 | validation: 0.5142182204429124]
	TIME [epoch: 8.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5149104252269125		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 0.5149104252269125 | validation: 0.5470473797690729]
	TIME [epoch: 8.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5453244520596533		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 0.5453244520596533 | validation: 0.41535126700308045]
	TIME [epoch: 8.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5522571313254054		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 0.5522571313254054 | validation: 0.5927714447325829]
	TIME [epoch: 8.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6225526011493328		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 0.6225526011493328 | validation: 0.5749011184404444]
	TIME [epoch: 8.32 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4869789415656511		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 0.4869789415656511 | validation: 0.5201005132023695]
	TIME [epoch: 8.33 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5155054795957976		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 0.5155054795957976 | validation: 0.5437219030520752]
	TIME [epoch: 8.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47806248260404305		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 0.47806248260404305 | validation: 0.3956198424785504]
	TIME [epoch: 8.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5046778360790543		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 0.5046778360790543 | validation: 0.393992490855936]
	TIME [epoch: 8.33 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5699893075974904		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 0.5699893075974904 | validation: 0.42145815155833444]
	TIME [epoch: 8.33 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.502391571275498		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 0.502391571275498 | validation: 0.3803933644539625]
	TIME [epoch: 8.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49952462468811937		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 0.49952462468811937 | validation: 0.7335883140481421]
	TIME [epoch: 8.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5697149096318687		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 0.5697149096318687 | validation: 1.9471501855282862]
	TIME [epoch: 8.32 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2852894349283859		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 1.2852894349283859 | validation: 0.3515573642034665]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6507152531281355		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 0.6507152531281355 | validation: 0.45580638962989733]
	TIME [epoch: 8.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5895772624026413		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 0.5895772624026413 | validation: 0.5456706708603267]
	TIME [epoch: 8.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5666087097466066		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 0.5666087097466066 | validation: 0.41154843809474384]
	TIME [epoch: 8.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5187514659967818		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 0.5187514659967818 | validation: 0.4771360290974261]
	TIME [epoch: 8.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5891878862802431		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 0.5891878862802431 | validation: 0.3653233194701416]
	TIME [epoch: 8.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5943086381998239		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 0.5943086381998239 | validation: 0.5016253875062111]
	TIME [epoch: 8.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5994069251759255		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 0.5994069251759255 | validation: 1.2139319047309387]
	TIME [epoch: 8.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6484159840390109		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 0.6484159840390109 | validation: 0.9627583225709291]
	TIME [epoch: 8.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7509746200473983		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 0.7509746200473983 | validation: 1.0754873037774733]
	TIME [epoch: 8.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5752359135769364		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 0.5752359135769364 | validation: 0.4757323424366232]
	TIME [epoch: 8.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5505367557457338		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 0.5505367557457338 | validation: 0.4973542835700338]
	TIME [epoch: 8.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5158981102087907		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 0.5158981102087907 | validation: 0.5106047147589814]
	TIME [epoch: 8.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5672532849934118		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 0.5672532849934118 | validation: 0.5109983433738327]
	TIME [epoch: 8.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49037571735205826		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 0.49037571735205826 | validation: 0.5375735874308218]
	TIME [epoch: 8.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4566679308794268		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 0.4566679308794268 | validation: 1.059424273029482]
	TIME [epoch: 8.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.58823724381203		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 0.58823724381203 | validation: 0.5937028402650791]
	TIME [epoch: 8.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5258203278667322		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 0.5258203278667322 | validation: 1.1875656055182984]
	TIME [epoch: 8.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48849068720005473		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 0.48849068720005473 | validation: 0.4884746635305204]
	TIME [epoch: 8.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6118208250486669		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 0.6118208250486669 | validation: 0.606103292933748]
	TIME [epoch: 8.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6021511373869486		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 0.6021511373869486 | validation: 0.7244002824780925]
	TIME [epoch: 8.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6579815405989471		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 0.6579815405989471 | validation: 0.44384806884314953]
	TIME [epoch: 8.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46425649883720316		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 0.46425649883720316 | validation: 0.3700728675418116]
	TIME [epoch: 8.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47530116963165164		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 0.47530116963165164 | validation: 0.4240850329469543]
	TIME [epoch: 8.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4716917201436779		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 0.4716917201436779 | validation: 0.38463632717362395]
	TIME [epoch: 8.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48023787089803555		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 0.48023787089803555 | validation: 0.41101881537175494]
	TIME [epoch: 8.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46873599324545256		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 0.46873599324545256 | validation: 0.6180136700423979]
	TIME [epoch: 8.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43782171608716086		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 0.43782171608716086 | validation: 0.5195418981574909]
	TIME [epoch: 8.38 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6294394174705632		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 0.6294394174705632 | validation: 0.3826415014120264]
	TIME [epoch: 8.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4807149064553936		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 0.4807149064553936 | validation: 0.5041602143140549]
	TIME [epoch: 8.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48158840588335405		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 0.48158840588335405 | validation: 0.5949817093965033]
	TIME [epoch: 8.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7799094617517937		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 0.7799094617517937 | validation: 0.5143754088515547]
	TIME [epoch: 8.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47551945537398266		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 0.47551945537398266 | validation: 0.6495398681487691]
	TIME [epoch: 8.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.496629420311913		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 0.496629420311913 | validation: 0.48061555158721897]
	TIME [epoch: 8.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.504642917373334		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 0.504642917373334 | validation: 0.4039557197361589]
	TIME [epoch: 8.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4513267124127511		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 0.4513267124127511 | validation: 0.5044714488728685]
	TIME [epoch: 8.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4397737258985711		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 0.4397737258985711 | validation: 0.4650491812748279]
	TIME [epoch: 8.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4224460323643597		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 0.4224460323643597 | validation: 0.5621062489819197]
	TIME [epoch: 8.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45772044154706154		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 0.45772044154706154 | validation: 0.7382765366624003]
	TIME [epoch: 8.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47954543782585957		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 0.47954543782585957 | validation: 0.6517165324622661]
	TIME [epoch: 8.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.50242130240312		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 0.50242130240312 | validation: 0.4496386334644471]
	TIME [epoch: 8.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5142954100134596		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 0.5142954100134596 | validation: 0.6259914666988502]
	TIME [epoch: 8.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4944463666972004		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 0.4944463666972004 | validation: 0.36630692692591416]
	TIME [epoch: 8.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5664153273772186		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 0.5664153273772186 | validation: 0.3161318050126471]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46857013318572294		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 0.46857013318572294 | validation: 0.7646202833889018]
	TIME [epoch: 8.37 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4476120256937574		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 0.4476120256937574 | validation: 0.468211369310269]
	TIME [epoch: 8.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4290347513746755		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 0.4290347513746755 | validation: 0.6566121443279841]
	TIME [epoch: 8.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6160966357698368		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 0.6160966357698368 | validation: 0.9805340862442076]
	TIME [epoch: 8.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5383834867177199		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 0.5383834867177199 | validation: 0.4168923819726095]
	TIME [epoch: 8.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4927832775532113		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 0.4927832775532113 | validation: 0.4487407779643862]
	TIME [epoch: 8.37 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.440519349130234		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 0.440519349130234 | validation: 0.38876588324512706]
	TIME [epoch: 8.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5832341386420506		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 0.5832341386420506 | validation: 0.8844366982877627]
	TIME [epoch: 8.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4619851424247682		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 0.4619851424247682 | validation: 0.5389815257500963]
	TIME [epoch: 8.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48372340570329014		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 0.48372340570329014 | validation: 0.4314823611392423]
	TIME [epoch: 8.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42834639924870527		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 0.42834639924870527 | validation: 0.8272582346002149]
	TIME [epoch: 8.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4863536660232894		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 0.4863536660232894 | validation: 0.5604952414359481]
	TIME [epoch: 8.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4464954309511027		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 0.4464954309511027 | validation: 0.5357679933510421]
	TIME [epoch: 8.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5788458118525903		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.5788458118525903 | validation: 0.40914211506862136]
	TIME [epoch: 8.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40989185240941295		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.40989185240941295 | validation: 0.4829483282256958]
	TIME [epoch: 8.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44175749819499444		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.44175749819499444 | validation: 0.825846412568587]
	TIME [epoch: 8.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43178037027517313		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.43178037027517313 | validation: 0.7684733474586481]
	TIME [epoch: 8.34 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5763374241939376		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.5763374241939376 | validation: 0.5684992711865549]
	TIME [epoch: 8.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44017751124996957		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.44017751124996957 | validation: 0.4079498386994318]
	TIME [epoch: 8.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5821667242920149		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.5821667242920149 | validation: 0.46211126748627107]
	TIME [epoch: 8.34 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.453982903536495		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.453982903536495 | validation: 0.34645026433744264]
	TIME [epoch: 8.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44961969833967713		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.44961969833967713 | validation: 0.4406572824103506]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5036538934323016		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.5036538934323016 | validation: 0.42897248916394415]
	TIME [epoch: 8.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6061962656904181		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.6061962656904181 | validation: 0.4506197419855539]
	TIME [epoch: 8.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4528872695922227		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.4528872695922227 | validation: 0.5493891763597272]
	TIME [epoch: 8.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6191060419062684		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.6191060419062684 | validation: 0.4423497394373096]
	TIME [epoch: 8.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6206789790089566		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.6206789790089566 | validation: 0.6382102766920721]
	TIME [epoch: 8.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4079651722449108		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.4079651722449108 | validation: 0.6408217787125848]
	TIME [epoch: 8.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7795156837036483		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.7795156837036483 | validation: 0.44642066939630576]
	TIME [epoch: 8.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41947043865609784		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.41947043865609784 | validation: 0.5310214353696476]
	TIME [epoch: 8.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6591357266349622		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.6591357266349622 | validation: 1.215919224805757]
	TIME [epoch: 8.34 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6381663316064377		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.6381663316064377 | validation: 0.35047523110810774]
	TIME [epoch: 8.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43203662448576863		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.43203662448576863 | validation: 1.009962567157516]
	TIME [epoch: 8.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4961756788323014		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.4961756788323014 | validation: 0.679637305497695]
	TIME [epoch: 8.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4952308648684678		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.4952308648684678 | validation: 0.5697220491139566]
	TIME [epoch: 8.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4825021298805572		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.4825021298805572 | validation: 0.5397415229719481]
	TIME [epoch: 8.37 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5736850457265661		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.5736850457265661 | validation: 0.8000285680565918]
	TIME [epoch: 8.35 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5149921459480701		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.5149921459480701 | validation: 0.5899880179703774]
	TIME [epoch: 8.35 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37513657097096076		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.37513657097096076 | validation: 0.37852615220421393]
	TIME [epoch: 8.35 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5234215681830401		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.5234215681830401 | validation: 0.7916754784649505]
	TIME [epoch: 8.36 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5519316242480372		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.5519316242480372 | validation: 0.7377605217873009]
	TIME [epoch: 8.36 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47540483164109376		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.47540483164109376 | validation: 0.4078844619291817]
	TIME [epoch: 8.34 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38069014440554905		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.38069014440554905 | validation: 0.3902661285801855]
	TIME [epoch: 8.34 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.411162296010849		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.411162296010849 | validation: 0.8009045159413379]
	TIME [epoch: 8.34 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5746897760017126		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.5746897760017126 | validation: 0.5585331674364514]
	TIME [epoch: 8.37 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4674134751538578		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.4674134751538578 | validation: 0.5223630375884458]
	TIME [epoch: 8.35 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6884720762396382		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.6884720762396382 | validation: 0.5712597061731368]
	TIME [epoch: 8.34 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46231610453431093		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.46231610453431093 | validation: 0.4206530597396577]
	TIME [epoch: 8.34 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44211589745226815		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.44211589745226815 | validation: 0.3165164251574296]
	TIME [epoch: 8.37 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37940403005126805		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.37940403005126805 | validation: 0.4099867551167248]
	TIME [epoch: 8.35 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5175476195706777		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.5175476195706777 | validation: 0.4612361020277048]
	TIME [epoch: 8.34 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4093373391578023		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.4093373391578023 | validation: 0.32611769983008443]
	TIME [epoch: 8.35 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36232048559156993		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.36232048559156993 | validation: 2.2908038039796166]
	TIME [epoch: 8.36 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5632099582510945		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.5632099582510945 | validation: 0.40082125030027854]
	TIME [epoch: 8.35 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6580887844454179		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.6580887844454179 | validation: 1.670571867178138]
	TIME [epoch: 8.35 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5279724682127794		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.5279724682127794 | validation: 0.45111787042047746]
	TIME [epoch: 8.34 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.391563263366649		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.391563263366649 | validation: 0.593479268334955]
	TIME [epoch: 8.35 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7452772438884109		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.7452772438884109 | validation: 0.28503275279018025]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3531648269838188		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.3531648269838188 | validation: 0.3647686882161334]
	TIME [epoch: 8.35 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4114422041233986		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.4114422041233986 | validation: 0.9232161582275511]
	TIME [epoch: 8.34 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.707633580478005		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.707633580478005 | validation: 0.627286728097094]
	TIME [epoch: 8.34 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4255583943040073		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.4255583943040073 | validation: 0.4469281862756411]
	TIME [epoch: 8.37 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3792710390540643		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.3792710390540643 | validation: 0.42251808627833826]
	TIME [epoch: 8.34 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48545624735147996		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.48545624735147996 | validation: 0.3514826468743745]
	TIME [epoch: 8.35 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4257248187148062		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.4257248187148062 | validation: 0.5501155328355598]
	TIME [epoch: 8.34 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.461050385466698		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.461050385466698 | validation: 0.5835186425488812]
	TIME [epoch: 8.36 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5210721855345264		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.5210721855345264 | validation: 0.4095699035389019]
	TIME [epoch: 8.35 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3657798556176811		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.3657798556176811 | validation: 0.6606479368315399]
	TIME [epoch: 8.34 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4447126157445405		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.4447126157445405 | validation: 0.3213240275087422]
	TIME [epoch: 8.34 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39540329954891235		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.39540329954891235 | validation: 0.31605487443034924]
	TIME [epoch: 8.34 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40360763414536444		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 0.40360763414536444 | validation: 0.4291777394052234]
	TIME [epoch: 8.37 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41530983913523245		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.41530983913523245 | validation: 0.4471792611024489]
	TIME [epoch: 8.34 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4171839322526435		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.4171839322526435 | validation: 0.46183832230829913]
	TIME [epoch: 8.34 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3918658811161726		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.3918658811161726 | validation: 0.41084483584592857]
	TIME [epoch: 8.34 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3832238272020813		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.3832238272020813 | validation: 0.40712007477897194]
	TIME [epoch: 8.36 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43948117746382553		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.43948117746382553 | validation: 0.36760840655086136]
	TIME [epoch: 8.35 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37552784170844083		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.37552784170844083 | validation: 0.4326189852506131]
	TIME [epoch: 8.34 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3805857513557861		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.3805857513557861 | validation: 0.6318292766096212]
	TIME [epoch: 8.34 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42345656842779444		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.42345656842779444 | validation: 0.3711928598834867]
	TIME [epoch: 8.36 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4827546250189368		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.4827546250189368 | validation: 0.49289223675343086]
	TIME [epoch: 8.35 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4410467754005598		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.4410467754005598 | validation: 0.35309988235833467]
	TIME [epoch: 8.34 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4336783968745562		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.4336783968745562 | validation: 0.6697168962797582]
	TIME [epoch: 8.34 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41314083920342765		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.41314083920342765 | validation: 0.6067713367555134]
	TIME [epoch: 8.34 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4518420868127563		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.4518420868127563 | validation: 0.3169375313667706]
	TIME [epoch: 8.36 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35908626073610905		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.35908626073610905 | validation: 0.3399495470693074]
	TIME [epoch: 8.34 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4387929287989466		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.4387929287989466 | validation: 0.3505422913754753]
	TIME [epoch: 8.34 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.423399474605484		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.423399474605484 | validation: 0.45959135233009385]
	TIME [epoch: 8.34 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4053287664770629		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.4053287664770629 | validation: 0.40390922374395594]
	TIME [epoch: 8.37 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5091762087919933		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.5091762087919933 | validation: 0.403072322942138]
	TIME [epoch: 8.34 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3731945464721378		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.3731945464721378 | validation: 0.2803801493431881]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39406131476018774		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.39406131476018774 | validation: 0.4120013065912842]
	TIME [epoch: 8.34 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40348452069564444		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.40348452069564444 | validation: 0.3228961263540204]
	TIME [epoch: 8.36 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37863262542710413		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.37863262542710413 | validation: 0.3843493592336565]
	TIME [epoch: 8.34 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36285289954999767		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.36285289954999767 | validation: 0.30688493414114215]
	TIME [epoch: 8.34 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4527293427428553		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.4527293427428553 | validation: 0.34744461443938035]
	TIME [epoch: 8.33 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3920176475060398		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.3920176475060398 | validation: 0.2836951267701649]
	TIME [epoch: 8.34 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3693111824978294		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.3693111824978294 | validation: 0.7742930335587921]
	TIME [epoch: 8.36 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4171462650476849		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.4171462650476849 | validation: 0.6417886459886003]
	TIME [epoch: 8.34 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4315873924094283		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.4315873924094283 | validation: 0.8416939849363944]
	TIME [epoch: 8.34 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3987556770841139		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.3987556770841139 | validation: 0.4208628965781751]
	TIME [epoch: 8.34 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6967459239760434		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.6967459239760434 | validation: 0.5536063468319269]
	TIME [epoch: 8.36 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36363486413668833		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.36363486413668833 | validation: 0.3603501474254578]
	TIME [epoch: 8.34 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36571240549998113		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.36571240549998113 | validation: 0.5193654579550989]
	TIME [epoch: 8.34 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3826801467837525		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.3826801467837525 | validation: 0.38408317817746807]
	TIME [epoch: 8.34 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3555674298794688		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.3555674298794688 | validation: 0.40313748936823834]
	TIME [epoch: 8.36 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3207153982316775		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.3207153982316775 | validation: 0.38423245406654316]
	TIME [epoch: 8.34 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4670920059098897		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.4670920059098897 | validation: 0.9307396206127171]
	TIME [epoch: 8.34 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5219756583701357		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.5219756583701357 | validation: 0.4982967520449485]
	TIME [epoch: 8.34 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4458223854944466		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.4458223854944466 | validation: 0.4538481908448837]
	TIME [epoch: 8.34 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3573709171881887		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.3573709171881887 | validation: 0.327934540822679]
	TIME [epoch: 8.36 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38513745140816386		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.38513745140816386 | validation: 0.367956282244049]
	TIME [epoch: 8.34 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3746721932169267		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.3746721932169267 | validation: 0.49632096199888043]
	TIME [epoch: 8.33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4144270563429096		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.4144270563429096 | validation: 0.36718415329630927]
	TIME [epoch: 8.33 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.349642102730944		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.349642102730944 | validation: 0.36114282369795336]
	TIME [epoch: 8.36 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42566695560577844		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.42566695560577844 | validation: 0.5531765001136266]
	TIME [epoch: 8.34 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37626800519614084		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.37626800519614084 | validation: 0.37676711795464757]
	TIME [epoch: 8.34 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32582407038974515		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.32582407038974515 | validation: 0.34037962688506446]
	TIME [epoch: 8.34 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3950370985493888		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.3950370985493888 | validation: 1.060965343353236]
	TIME [epoch: 8.36 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4480293267217491		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.4480293267217491 | validation: 0.3013303700570161]
	TIME [epoch: 8.34 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32713874373184354		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.32713874373184354 | validation: 0.4414432369744534]
	TIME [epoch: 8.34 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47342081892383997		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.47342081892383997 | validation: 0.4284398251117787]
	TIME [epoch: 8.33 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4382273310880893		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.4382273310880893 | validation: 0.4462492444977587]
	TIME [epoch: 8.34 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33920161482857625		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.33920161482857625 | validation: 0.8091856785184575]
	TIME [epoch: 8.36 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39327366632964894		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.39327366632964894 | validation: 0.4078551262221438]
	TIME [epoch: 8.34 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37746996846952274		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.37746996846952274 | validation: 0.3187683995743266]
	TIME [epoch: 8.34 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4006768841811156		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.4006768841811156 | validation: 0.5291262767371606]
	TIME [epoch: 8.34 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45481018300895804		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.45481018300895804 | validation: 0.4178812842014915]
	TIME [epoch: 8.36 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40291536484419604		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.40291536484419604 | validation: 0.33709990883776064]
	TIME [epoch: 8.34 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34395181947671705		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.34395181947671705 | validation: 0.32777832376437044]
	TIME [epoch: 8.34 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35193478400030825		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.35193478400030825 | validation: 0.7553679669470648]
	TIME [epoch: 8.34 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1684967323275857		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 1.1684967323275857 | validation: 0.46279477142479175]
	TIME [epoch: 8.36 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35054403237999254		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.35054403237999254 | validation: 0.6345278113036676]
	TIME [epoch: 8.35 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3569823083444913		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.3569823083444913 | validation: 0.29897326771520827]
	TIME [epoch: 8.34 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3947590384924752		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.3947590384924752 | validation: 0.8626214130915508]
	TIME [epoch: 8.33 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4057633798178718		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.4057633798178718 | validation: 0.5598637053038444]
	TIME [epoch: 8.34 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3856679090569014		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.3856679090569014 | validation: 0.29253137911956106]
	TIME [epoch: 8.36 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37736289212076646		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.37736289212076646 | validation: 0.39657120421338354]
	TIME [epoch: 8.34 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.338482937736983		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.338482937736983 | validation: 0.6758201394957148]
	TIME [epoch: 8.34 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39172976101602647		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.39172976101602647 | validation: 0.27759426052716457]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29181907899290893		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.29181907899290893 | validation: 0.692138045251234]
	TIME [epoch: 8.36 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3987846055499049		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.3987846055499049 | validation: 0.41931885520806134]
	TIME [epoch: 8.34 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3773040635645964		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.3773040635645964 | validation: 0.27773784333535945]
	TIME [epoch: 8.33 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3426454410571082		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.3426454410571082 | validation: 0.2521666837011292]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3877011177678548		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.3877011177678548 | validation: 0.36805288861407526]
	TIME [epoch: 8.36 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3997854209036723		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.3997854209036723 | validation: 0.31690976065424326]
	TIME [epoch: 8.34 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5862894450809214		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.5862894450809214 | validation: 0.34431282723251455]
	TIME [epoch: 8.33 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40962149035608275		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.40962149035608275 | validation: 0.5462768581961631]
	TIME [epoch: 8.33 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3812039833813172		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.3812039833813172 | validation: 0.3099547188651317]
	TIME [epoch: 8.34 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.330451350042326		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.330451350042326 | validation: 0.3370230901474762]
	TIME [epoch: 8.36 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31970625002874464		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.31970625002874464 | validation: 0.2975840900219413]
	TIME [epoch: 8.33 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37855448205204956		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.37855448205204956 | validation: 0.41720674512127237]
	TIME [epoch: 8.33 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35614404904304786		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.35614404904304786 | validation: 0.4046771403650248]
	TIME [epoch: 8.33 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3540106213800794		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.3540106213800794 | validation: 0.3398446502986615]
	TIME [epoch: 8.35 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3561415520451244		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.3561415520451244 | validation: 0.3546761406710499]
	TIME [epoch: 8.34 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3041111476344451		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.3041111476344451 | validation: 0.3715944317977995]
	TIME [epoch: 8.33 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32008651909629066		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.32008651909629066 | validation: 0.2861850467374211]
	TIME [epoch: 8.34 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3684943887930181		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.3684943887930181 | validation: 0.34904551881616586]
	TIME [epoch: 8.36 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33204314724649336		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.33204314724649336 | validation: 0.27172538689506265]
	TIME [epoch: 8.34 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3085350793993701		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.3085350793993701 | validation: 0.29499876031779193]
	TIME [epoch: 8.34 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3502509373153382		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.3502509373153382 | validation: 0.7634026823324396]
	TIME [epoch: 8.33 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3924681424189102		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.3924681424189102 | validation: 1.6717076391251842]
	TIME [epoch: 8.34 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5173516957474226		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.5173516957474226 | validation: 1.1055493526384355]
	TIME [epoch: 8.36 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.396143761905791		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.396143761905791 | validation: 0.6344819978506435]
	TIME [epoch: 8.34 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4024905099412221		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.4024905099412221 | validation: 0.38314050485068457]
	TIME [epoch: 8.34 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2861162806825908		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.2861162806825908 | validation: 0.4098995133962725]
	TIME [epoch: 8.33 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34353989775580274		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.34353989775580274 | validation: 0.42903897131668145]
	TIME [epoch: 8.36 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42284433411504735		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.42284433411504735 | validation: 0.25671375873094066]
	TIME [epoch: 8.34 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2978396810581435		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.2978396810581435 | validation: 0.5653436621231432]
	TIME [epoch: 8.34 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2816659889469232		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.2816659889469232 | validation: 0.3466649384033599]
	TIME [epoch: 8.33 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3083033241641152		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.3083033241641152 | validation: 0.3335634306596019]
	TIME [epoch: 8.36 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3332164420689644		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.3332164420689644 | validation: 0.32896903997787796]
	TIME [epoch: 8.34 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31761146288466097		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.31761146288466097 | validation: 0.515404473130899]
	TIME [epoch: 8.34 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.630918928599184		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.630918928599184 | validation: 0.3634054626672568]
	TIME [epoch: 8.34 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3183343990604536		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.3183343990604536 | validation: 0.3168400868568867]
	TIME [epoch: 8.34 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32389785918595404		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.32389785918595404 | validation: 0.3760156145555299]
	TIME [epoch: 8.36 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30927562518347734		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.30927562518347734 | validation: 0.5046600475965313]
	TIME [epoch: 8.33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3572656968781095		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.3572656968781095 | validation: 0.43306768204668267]
	TIME [epoch: 8.34 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34528290022064806		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.34528290022064806 | validation: 0.4432958094569966]
	TIME [epoch: 8.34 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33484811841244116		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.33484811841244116 | validation: 0.6205620124383453]
	TIME [epoch: 8.36 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3504076651413671		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.3504076651413671 | validation: 0.3378814295817551]
	TIME [epoch: 8.34 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3540514878682728		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.3540514878682728 | validation: 0.3015911022450317]
	TIME [epoch: 8.33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32996280906535985		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.32996280906535985 | validation: 0.36867684280166163]
	TIME [epoch: 8.34 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29994522619463704		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.29994522619463704 | validation: 0.40423197814099737]
	TIME [epoch: 8.36 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34674942592623853		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.34674942592623853 | validation: 0.4002153630993877]
	TIME [epoch: 8.34 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35590617410333913		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.35590617410333913 | validation: 0.31967993122637794]
	TIME [epoch: 8.34 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.302361714530893		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.302361714530893 | validation: 0.48380753646098934]
	TIME [epoch: 8.33 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3211540457895722		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.3211540457895722 | validation: 1.4792791801117113]
	TIME [epoch: 8.34 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4576580222059031		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.4576580222059031 | validation: 0.35700084979228647]
	TIME [epoch: 8.35 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37275249220619283		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.37275249220619283 | validation: 0.3454781550705479]
	TIME [epoch: 8.34 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4142464130142671		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.4142464130142671 | validation: 0.5752868388111915]
	TIME [epoch: 8.33 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31234713262111813		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.31234713262111813 | validation: 0.3130315666947513]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29992179147429654		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.29992179147429654 | validation: 0.2783390010419381]
	TIME [epoch: 8.36 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33861960115791667		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.33861960115791667 | validation: 0.2875812248494435]
	TIME [epoch: 8.34 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3228286576799816		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.3228286576799816 | validation: 0.3894540039078093]
	TIME [epoch: 8.33 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31406569565990117		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.31406569565990117 | validation: 0.3432585433354558]
	TIME [epoch: 8.33 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3537121391104258		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.3537121391104258 | validation: 0.2515206916970471]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30432221450242775		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.30432221450242775 | validation: 0.26788880542736193]
	TIME [epoch: 8.34 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3062493223731062		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.3062493223731062 | validation: 0.31879740322791644]
	TIME [epoch: 8.33 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28774653521327576		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.28774653521327576 | validation: 0.2974045202693374]
	TIME [epoch: 8.33 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2917777921428248		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.2917777921428248 | validation: 0.33097037532460316]
	TIME [epoch: 8.34 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3610495567482573		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.3610495567482573 | validation: 0.5055457889373816]
	TIME [epoch: 8.35 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35385878576064655		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.35385878576064655 | validation: 0.4456065993784947]
	TIME [epoch: 8.33 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27507074101735407		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.27507074101735407 | validation: 0.3269921708055651]
	TIME [epoch: 8.33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30391768821848975		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.30391768821848975 | validation: 0.3655123642626804]
	TIME [epoch: 8.33 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3015968302781351		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.3015968302781351 | validation: 0.28796829201812474]
	TIME [epoch: 8.36 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3069466245516673		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.3069466245516673 | validation: 0.3769669960920385]
	TIME [epoch: 8.34 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35015997795620923		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.35015997795620923 | validation: 0.5746206611557647]
	TIME [epoch: 8.34 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30773590498978826		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.30773590498978826 | validation: 0.2652962538572931]
	TIME [epoch: 8.33 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32277193498562073		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.32277193498562073 | validation: 0.25291788249430064]
	TIME [epoch: 8.36 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3010260723925072		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.3010260723925072 | validation: 0.5824496680302912]
	TIME [epoch: 8.33 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6309336308324826		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.6309336308324826 | validation: 0.41213140092626943]
	TIME [epoch: 8.33 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31148917348738847		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.31148917348738847 | validation: 0.23735533949846627]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3251281347612859		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.3251281347612859 | validation: 0.49349214231115207]
	TIME [epoch: 8.35 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31545753236259333		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.31545753236259333 | validation: 0.3427998158011058]
	TIME [epoch: 8.35 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3340042756693129		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.3340042756693129 | validation: 0.4093133574730915]
	TIME [epoch: 8.34 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3162484685144317		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.3162484685144317 | validation: 0.7847885700635905]
	TIME [epoch: 8.33 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3560622524817976		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.3560622524817976 | validation: 0.322012300401211]
	TIME [epoch: 8.34 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29821445836410365		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.29821445836410365 | validation: 0.3245144741862399]
	TIME [epoch: 8.36 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7948909890570072		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.7948909890570072 | validation: 0.34970449875517906]
	TIME [epoch: 8.33 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23052629121526982		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.23052629121526982 | validation: 0.34455141923694393]
	TIME [epoch: 8.34 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2683043920066762		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.2683043920066762 | validation: 0.446872483181902]
	TIME [epoch: 8.33 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2860998593616867		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.2860998593616867 | validation: 0.2727179070742489]
	TIME [epoch: 8.36 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2575543646863179		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.2575543646863179 | validation: 0.31192009954896216]
	TIME [epoch: 8.33 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25614281714189147		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.25614281714189147 | validation: 0.2572505184774572]
	TIME [epoch: 8.33 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4387439874873964		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.4387439874873964 | validation: 0.4897778359464542]
	TIME [epoch: 8.33 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29526047375222586		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.29526047375222586 | validation: 0.30303923856768655]
	TIME [epoch: 8.34 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2980315310081555		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.2980315310081555 | validation: 0.3814385552714252]
	TIME [epoch: 8.34 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3037801008852171		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.3037801008852171 | validation: 0.5562708672809847]
	TIME [epoch: 8.33 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2791173335594271		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.2791173335594271 | validation: 0.28491112709293076]
	TIME [epoch: 8.33 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29098843859306134		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.29098843859306134 | validation: 0.5817287729449692]
	TIME [epoch: 8.33 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36020024389176547		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.36020024389176547 | validation: 0.35109234906212805]
	TIME [epoch: 8.36 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37120366869562177		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.37120366869562177 | validation: 0.37290510882563754]
	TIME [epoch: 8.34 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3560513692155888		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.3560513692155888 | validation: 0.3385151970073361]
	TIME [epoch: 8.33 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34705610721235386		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.34705610721235386 | validation: 0.3049392034985324]
	TIME [epoch: 8.33 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32224393227744763		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.32224393227744763 | validation: 0.24103765766370663]
	TIME [epoch: 8.36 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4488144368938687		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.4488144368938687 | validation: 0.26767718329580037]
	TIME [epoch: 8.34 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26444562716284453		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.26444562716284453 | validation: 0.30317110616315823]
	TIME [epoch: 8.33 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26580134329905614		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.26580134329905614 | validation: 0.280235143289679]
	TIME [epoch: 8.33 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.292706861016759		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.292706861016759 | validation: 0.3608339525644447]
	TIME [epoch: 8.33 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3369658719835483		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.3369658719835483 | validation: 0.2673116673084486]
	TIME [epoch: 8.35 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3072978711275457		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.3072978711275457 | validation: 0.3411071607907787]
	TIME [epoch: 8.33 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28164859138291354		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.28164859138291354 | validation: 0.22407791940064434]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26742348267056437		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.26742348267056437 | validation: 0.2826577307939286]
	TIME [epoch: 8.33 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.276195497200595		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.276195497200595 | validation: 0.4027522024795803]
	TIME [epoch: 8.35 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3436392016354258		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.3436392016354258 | validation: 0.31039396948947895]
	TIME [epoch: 8.33 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2718199978211566		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.2718199978211566 | validation: 0.38778166936028247]
	TIME [epoch: 8.33 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3067969673823886		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.3067969673823886 | validation: 0.3005955662865427]
	TIME [epoch: 8.33 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2840552585756007		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.2840552585756007 | validation: 0.28831335473223557]
	TIME [epoch: 8.35 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28945753931859863		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.28945753931859863 | validation: 0.4018677828521119]
	TIME [epoch: 8.33 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2985729817381295		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.2985729817381295 | validation: 0.21484584781157315]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2981533618487158		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.2981533618487158 | validation: 0.45122368432575755]
	TIME [epoch: 8.33 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28555457823080055		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.28555457823080055 | validation: 0.3469725383123752]
	TIME [epoch: 8.34 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.293419736608913		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.293419736608913 | validation: 0.25833860022171173]
	TIME [epoch: 8.35 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24905523481497274		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.24905523481497274 | validation: 0.3396623019380931]
	TIME [epoch: 8.33 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2897580887619358		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.2897580887619358 | validation: 0.33176832402779377]
	TIME [epoch: 8.33 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25222747544738666		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.25222747544738666 | validation: 0.2505037893554409]
	TIME [epoch: 8.33 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2445144499901065		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.2445144499901065 | validation: 0.39064930635832384]
	TIME [epoch: 8.36 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29668631031631776		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.29668631031631776 | validation: 0.2736773688990604]
	TIME [epoch: 8.33 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24687419223836		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.24687419223836 | validation: 0.31219028709635266]
	TIME [epoch: 8.33 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3430749432739383		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.3430749432739383 | validation: 0.30093892072116185]
	TIME [epoch: 8.33 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.268896047549742		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.268896047549742 | validation: 0.2808944627425539]
	TIME [epoch: 8.35 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2566634180901673		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.2566634180901673 | validation: 0.2871327854506483]
	TIME [epoch: 8.34 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3510916987199242		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.3510916987199242 | validation: 0.2142819938737303]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27581990271335666		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.27581990271335666 | validation: 0.44899226667332226]
	TIME [epoch: 8.34 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39730576134795176		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.39730576134795176 | validation: 0.3986779978155278]
	TIME [epoch: 8.36 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3185933936625588		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.3185933936625588 | validation: 0.27926025359339324]
	TIME [epoch: 8.34 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2733656990803071		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.2733656990803071 | validation: 0.40336698171577745]
	TIME [epoch: 8.33 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3051965615877284		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.3051965615877284 | validation: 0.3228823937692592]
	TIME [epoch: 8.34 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28514267873519616		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.28514267873519616 | validation: 0.3397076615074551]
	TIME [epoch: 8.34 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2729600122186762		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.2729600122186762 | validation: 0.26990314518688285]
	TIME [epoch: 8.36 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23586789276814502		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.23586789276814502 | validation: 0.25885481805016985]
	TIME [epoch: 8.33 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.261388251162989		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.261388251162989 | validation: 0.32466488132275395]
	TIME [epoch: 8.34 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.266238334304717		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.266238334304717 | validation: 0.2385932254007051]
	TIME [epoch: 8.34 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2672000321788114		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.2672000321788114 | validation: 0.23860060087887436]
	TIME [epoch: 8.36 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2860693680736539		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.2860693680736539 | validation: 0.22127308161933312]
	TIME [epoch: 8.34 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28352651523335226		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.28352651523335226 | validation: 0.34575498916994435]
	TIME [epoch: 8.34 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2658013748575473		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.2658013748575473 | validation: 0.3291902522412965]
	TIME [epoch: 8.34 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24136426778304593		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.24136426778304593 | validation: 0.2744033736748118]
	TIME [epoch: 8.36 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2589634945216778		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.2589634945216778 | validation: 0.24819761836972923]
	TIME [epoch: 8.35 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28873445336365794		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.28873445336365794 | validation: 0.23821682786444556]
	TIME [epoch: 8.34 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22460710042292636		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.22460710042292636 | validation: 0.22098429939841524]
	TIME [epoch: 8.34 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26931668188611846		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.26931668188611846 | validation: 0.22474526896853111]
	TIME [epoch: 8.35 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2908199166629202		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.2908199166629202 | validation: 0.26719740761280636]
	TIME [epoch: 8.37 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30984443337199224		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.30984443337199224 | validation: 0.38082587188673334]
	TIME [epoch: 8.33 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30070459181388753		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.30070459181388753 | validation: 0.2274071779107169]
	TIME [epoch: 8.32 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25428583670673904		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.25428583670673904 | validation: 0.3204032733351189]
	TIME [epoch: 8.33 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2417922767130492		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.2417922767130492 | validation: 0.2880569222315382]
	TIME [epoch: 8.35 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22718024358643416		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.22718024358643416 | validation: 0.23951512456830532]
	TIME [epoch: 8.33 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2347249918329355		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.2347249918329355 | validation: 0.24538141889033055]
	TIME [epoch: 8.33 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24268906185470135		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.24268906185470135 | validation: 0.24452862836830058]
	TIME [epoch: 8.33 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2400628261739283		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.2400628261739283 | validation: 0.24710161805910075]
	TIME [epoch: 8.35 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25685119860793953		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.25685119860793953 | validation: 0.2562965218758487]
	TIME [epoch: 8.34 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2387418825890541		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.2387418825890541 | validation: 0.2560744919806252]
	TIME [epoch: 8.33 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3427681614287159		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.3427681614287159 | validation: 0.31811373095824375]
	TIME [epoch: 8.33 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2613869281303085		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.2613869281303085 | validation: 0.24781454164932137]
	TIME [epoch: 8.34 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27464272134962786		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.27464272134962786 | validation: 0.2854870371643853]
	TIME [epoch: 8.36 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24698608838001576		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.24698608838001576 | validation: 0.21932417716028008]
	TIME [epoch: 8.34 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2616495045810164		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.2616495045810164 | validation: 0.2828250522800633]
	TIME [epoch: 8.34 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3220114351172524		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.3220114351172524 | validation: 0.23913660746580678]
	TIME [epoch: 8.33 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31695613635152037		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.31695613635152037 | validation: 0.3767649280143516]
	TIME [epoch: 8.36 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2647144693113079		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.2647144693113079 | validation: 0.3620474841744583]
	TIME [epoch: 8.33 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35085578284784796		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.35085578284784796 | validation: 0.38199866765655677]
	TIME [epoch: 8.34 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2739660912215479		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.2739660912215479 | validation: 0.3319332402698062]
	TIME [epoch: 8.33 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2622452064058516		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.2622452064058516 | validation: 0.2930495505749485]
	TIME [epoch: 8.35 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2389220394418014		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.2389220394418014 | validation: 0.3171775933663783]
	TIME [epoch: 8.34 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2872716285958096		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.2872716285958096 | validation: 0.2584299011979251]
	TIME [epoch: 8.34 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.284615576820514		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.284615576820514 | validation: 0.25228361073786215]
	TIME [epoch: 8.33 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2225015674477726		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.2225015674477726 | validation: 0.27928729653028084]
	TIME [epoch: 8.33 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23314391275185634		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.23314391275185634 | validation: 0.26069039653993703]
	TIME [epoch: 8.35 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25374011371507843		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.25374011371507843 | validation: 0.3156333045341857]
	TIME [epoch: 8.34 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30342329334909995		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.30342329334909995 | validation: 0.3188162062726012]
	TIME [epoch: 8.33 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2667965790393595		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.2667965790393595 | validation: 0.2833723526529261]
	TIME [epoch: 8.32 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24025278708682704		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.24025278708682704 | validation: 0.2391973776257312]
	TIME [epoch: 8.35 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2445932430188547		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.2445932430188547 | validation: 0.22330146312882698]
	TIME [epoch: 8.33 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2558621802521094		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.2558621802521094 | validation: 0.23439300027035437]
	TIME [epoch: 8.33 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2717465728385754		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.2717465728385754 | validation: 0.3277823378012407]
	TIME [epoch: 8.32 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3790600705898126		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.3790600705898126 | validation: 0.6647891718050953]
	TIME [epoch: 8.35 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3292001143843776		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.3292001143843776 | validation: 0.487862241224006]
	TIME [epoch: 8.33 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2583051039206917		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.2583051039206917 | validation: 0.25256875593258044]
	TIME [epoch: 8.33 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2738477175577267		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.2738477175577267 | validation: 0.2624784704641602]
	TIME [epoch: 8.33 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23370582442284177		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.23370582442284177 | validation: 0.2329284608823352]
	TIME [epoch: 8.33 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24744108084654082		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.24744108084654082 | validation: 0.1962361555667561]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.277664523477462		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.277664523477462 | validation: 0.2383598888583196]
	TIME [epoch: 8.34 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22780902749459214		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.22780902749459214 | validation: 0.4532486347056349]
	TIME [epoch: 8.33 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2799941216346215		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.2799941216346215 | validation: 0.18970236397812673]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2094765114935		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.2094765114935 | validation: 0.23241926921109984]
	TIME [epoch: 8.38 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22473088645517408		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.22473088645517408 | validation: 0.2507581544333727]
	TIME [epoch: 8.36 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24903470024380137		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.24903470024380137 | validation: 0.3628089564985125]
	TIME [epoch: 8.35 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2616202497006298		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.2616202497006298 | validation: 0.2644224803726805]
	TIME [epoch: 8.35 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24768404478734748		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.24768404478734748 | validation: 0.2981911631479143]
	TIME [epoch: 8.37 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20729932730324982		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.20729932730324982 | validation: 0.2606412310416328]
	TIME [epoch: 8.35 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22152538480402537		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.22152538480402537 | validation: 0.31052891757752576]
	TIME [epoch: 8.35 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23439277036085482		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.23439277036085482 | validation: 0.2605661705969152]
	TIME [epoch: 8.35 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2206312281343788		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.2206312281343788 | validation: 0.3151162152304974]
	TIME [epoch: 8.36 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24042856645286825		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.24042856645286825 | validation: 0.35939153611154795]
	TIME [epoch: 8.37 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23998994467145912		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.23998994467145912 | validation: 0.24408580907617872]
	TIME [epoch: 8.35 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23682484913684249		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.23682484913684249 | validation: 0.23573979392185332]
	TIME [epoch: 8.35 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22178263907134124		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.22178263907134124 | validation: 0.29255935262982485]
	TIME [epoch: 8.35 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2540029382582626		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.2540029382582626 | validation: 0.22047588586218714]
	TIME [epoch: 8.37 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26401406574845065		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.26401406574845065 | validation: 0.3144573456810046]
	TIME [epoch: 8.35 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2607319378652817		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.2607319378652817 | validation: 0.21985338316420927]
	TIME [epoch: 8.35 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22527049046601225		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.22527049046601225 | validation: 0.5137799446841012]
	TIME [epoch: 8.35 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23234581756572373		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.23234581756572373 | validation: 0.29704733008582984]
	TIME [epoch: 8.38 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20941680156107037		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.20941680156107037 | validation: 0.2075601804012514]
	TIME [epoch: 8.35 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22136304052694405		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.22136304052694405 | validation: 0.22307127621846182]
	TIME [epoch: 8.35 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21161567947647156		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.21161567947647156 | validation: 0.22758059594368096]
	TIME [epoch: 8.35 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22362843480257505		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.22362843480257505 | validation: 0.21202224365418712]
	TIME [epoch: 8.36 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24774328128539774		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.24774328128539774 | validation: 0.19899927240119253]
	TIME [epoch: 8.36 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23043830510893484		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.23043830510893484 | validation: 0.2794538467857357]
	TIME [epoch: 8.35 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2269108688643267		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.2269108688643267 | validation: 0.28083526194659636]
	TIME [epoch: 8.35 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23605313192817218		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.23605313192817218 | validation: 0.18800004651018712]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22335522654641812		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.22335522654641812 | validation: 0.4465881846413491]
	TIME [epoch: 8.37 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2416456816513432		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.2416456816513432 | validation: 0.23190117858176146]
	TIME [epoch: 8.34 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22736398244992656		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.22736398244992656 | validation: 0.43042710022948677]
	TIME [epoch: 8.35 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23949675218137706		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.23949675218137706 | validation: 0.2499599953806914]
	TIME [epoch: 8.34 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2762850300973364		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.2762850300973364 | validation: 0.22039186949085096]
	TIME [epoch: 8.37 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2322496309253717		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.2322496309253717 | validation: 0.23066081221382478]
	TIME [epoch: 8.35 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2774470920339728		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.2774470920339728 | validation: 0.2822211509021163]
	TIME [epoch: 8.34 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23889980666513408		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.23889980666513408 | validation: 0.4195568105831842]
	TIME [epoch: 8.34 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28001430554991824		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.28001430554991824 | validation: 0.2332287487244441]
	TIME [epoch: 8.36 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24733555115666875		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.24733555115666875 | validation: 0.24483684860037208]
	TIME [epoch: 8.35 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2373784416727695		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.2373784416727695 | validation: 0.17886876851576444]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20522306143617514		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.20522306143617514 | validation: 0.25896904366257445]
	TIME [epoch: 8.35 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2766858643526241		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.2766858643526241 | validation: 0.24148348388790036]
	TIME [epoch: 8.34 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22912681082394495		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.22912681082394495 | validation: 0.22840881714409106]
	TIME [epoch: 8.37 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24650634119596618		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.24650634119596618 | validation: 0.213703261654007]
	TIME [epoch: 8.34 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24918882986900753		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.24918882986900753 | validation: 0.22051746962406804]
	TIME [epoch: 8.35 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2705700232817688		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.2705700232817688 | validation: 0.6788933136367477]
	TIME [epoch: 8.34 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29582284755733873		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.29582284755733873 | validation: 0.3294350924324396]
	TIME [epoch: 8.37 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21137075406077907		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.21137075406077907 | validation: 0.79998132813203]
	TIME [epoch: 8.35 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29925949057149204		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.29925949057149204 | validation: 0.19047175099261962]
	TIME [epoch: 8.34 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19854159671326171		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.19854159671326171 | validation: 0.21748169718525323]
	TIME [epoch: 8.34 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19188640552643602		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.19188640552643602 | validation: 0.22744959158166816]
	TIME [epoch: 8.37 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2195132498588571		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.2195132498588571 | validation: 0.20845736828218261]
	TIME [epoch: 8.34 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25276177237507824		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.25276177237507824 | validation: 0.23494332595494746]
	TIME [epoch: 8.35 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2405015842312021		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.2405015842312021 | validation: 0.2551967075130832]
	TIME [epoch: 8.34 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20913105816343824		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.20913105816343824 | validation: 0.2369862555072746]
	TIME [epoch: 8.35 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22721023770374388		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.22721023770374388 | validation: 0.23997444329364748]
	TIME [epoch: 8.37 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2576503091248811		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.2576503091248811 | validation: 0.21085325215754963]
	TIME [epoch: 8.34 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20493239599424812		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.20493239599424812 | validation: 0.30063290318599895]
	TIME [epoch: 8.34 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26245644016435726		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.26245644016435726 | validation: 0.20822969768559502]
	TIME [epoch: 8.33 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2370506542214023		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.2370506542214023 | validation: 0.20876905503019982]
	TIME [epoch: 8.36 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1979899772061547		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.1979899772061547 | validation: 0.22581450170433726]
	TIME [epoch: 8.34 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26371498111512476		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.26371498111512476 | validation: 0.3178466411494267]
	TIME [epoch: 8.34 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3502450924040429		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.3502450924040429 | validation: 0.2392310045940989]
	TIME [epoch: 8.34 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2173211826198163		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.2173211826198163 | validation: 0.2440355305466059]
	TIME [epoch: 8.36 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20889138937330984		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.20889138937330984 | validation: 0.22480342437925738]
	TIME [epoch: 8.34 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24240935498516208		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.24240935498516208 | validation: 0.2530609111502164]
	TIME [epoch: 8.33 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23397472927596213		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.23397472927596213 | validation: 0.22305183654433342]
	TIME [epoch: 8.33 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22344125046070967		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.22344125046070967 | validation: 0.29623811680470036]
	TIME [epoch: 8.34 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21192686029045937		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.21192686029045937 | validation: 0.31139567170208626]
	TIME [epoch: 8.36 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18367160099859206		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.18367160099859206 | validation: 0.22395244178747734]
	TIME [epoch: 8.33 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21547276506950178		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.21547276506950178 | validation: 0.3092727561364225]
	TIME [epoch: 8.33 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23953853055454039		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.23953853055454039 | validation: 0.23489419658909483]
	TIME [epoch: 8.33 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24580317602259388		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.24580317602259388 | validation: 0.22782718444904404]
	TIME [epoch: 8.36 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2123010682382498		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.2123010682382498 | validation: 0.22661355994329568]
	TIME [epoch: 8.34 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23215065516112046		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.23215065516112046 | validation: 0.19911032346288815]
	TIME [epoch: 8.33 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20152734069292078		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.20152734069292078 | validation: 0.1986805814322106]
	TIME [epoch: 8.33 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2139530069929167		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.2139530069929167 | validation: 0.21037586813029552]
	TIME [epoch: 8.36 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20943053365383482		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.20943053365383482 | validation: 0.19697686561636135]
	TIME [epoch: 8.33 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20673001150126585		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.20673001150126585 | validation: 0.24784462015815728]
	TIME [epoch: 8.34 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18848205228338027		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.18848205228338027 | validation: 0.2093806597195542]
	TIME [epoch: 8.33 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18067098117584893		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.18067098117584893 | validation: 0.6875920891138378]
	TIME [epoch: 8.34 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26235976367288216		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.26235976367288216 | validation: 0.2884027293778497]
	TIME [epoch: 8.35 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21602071692843405		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.21602071692843405 | validation: 0.21270664860860072]
	TIME [epoch: 8.33 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21004733842585538		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.21004733842585538 | validation: 0.2003935312972158]
	TIME [epoch: 8.34 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20977795902745716		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.20977795902745716 | validation: 0.1906819026320342]
	TIME [epoch: 8.33 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18352894905092712		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.18352894905092712 | validation: 0.18577238113915057]
	TIME [epoch: 8.36 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21839157662821446		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.21839157662821446 | validation: 0.2539812379727665]
	TIME [epoch: 8.33 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22662518053135966		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.22662518053135966 | validation: 0.2508203986527259]
	TIME [epoch: 8.33 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24034140743504753		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.24034140743504753 | validation: 0.1876197321828903]
	TIME [epoch: 8.33 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22371093407630852		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.22371093407630852 | validation: 0.18000963327614367]
	TIME [epoch: 8.35 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23380776266598174		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.23380776266598174 | validation: 0.2230242097831424]
	TIME [epoch: 8.33 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22171848188246063		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.22171848188246063 | validation: 0.2434587576123266]
	TIME [epoch: 8.33 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22127258590597093		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.22127258590597093 | validation: 0.27583223107141225]
	TIME [epoch: 8.33 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2267258915585891		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.2267258915585891 | validation: 0.25301706446989364]
	TIME [epoch: 8.33 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.226524047585731		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.226524047585731 | validation: 0.2389057636708536]
	TIME [epoch: 8.35 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2425534047041915		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.2425534047041915 | validation: 0.2229070525283774]
	TIME [epoch: 8.33 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2239559695764987		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.2239559695764987 | validation: 0.29138128599399393]
	TIME [epoch: 8.33 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.194159783989423		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.194159783989423 | validation: 0.22484035966345273]
	TIME [epoch: 8.33 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23123201153739453		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.23123201153739453 | validation: 0.2628056473498628]
	TIME [epoch: 8.35 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2421758051249125		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.2421758051249125 | validation: 0.2056230391215763]
	TIME [epoch: 8.34 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32917499087131363		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.32917499087131363 | validation: 0.216263865194548]
	TIME [epoch: 8.33 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24220529720518766		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.24220529720518766 | validation: 0.20380477216643522]
	TIME [epoch: 8.33 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2160363803499062		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.2160363803499062 | validation: 0.19910963789007835]
	TIME [epoch: 8.36 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21278731197225506		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.21278731197225506 | validation: 0.2627333562936976]
	TIME [epoch: 8.33 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23135996182959123		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.23135996182959123 | validation: 0.2175124273528901]
	TIME [epoch: 8.33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2096081734146193		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.2096081734146193 | validation: 0.22973665359160503]
	TIME [epoch: 8.33 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20906849084683246		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.20906849084683246 | validation: 0.19312503672043907]
	TIME [epoch: 8.34 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19465356214564902		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.19465356214564902 | validation: 0.3247800782644037]
	TIME [epoch: 8.35 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24125639998411413		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.24125639998411413 | validation: 0.2644478494028022]
	TIME [epoch: 8.34 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19447300555416316		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.19447300555416316 | validation: 0.42244778996925336]
	TIME [epoch: 8.33 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2141964856254329		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.2141964856254329 | validation: 0.2324412339560939]
	TIME [epoch: 8.33 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19825638508669924		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.19825638508669924 | validation: 0.17708574982664976]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_899.pth
	Model improved!!!
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20926213975079042		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.20926213975079042 | validation: 0.2121653330479345]
	TIME [epoch: 8.33 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2015796446717652		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.2015796446717652 | validation: 0.20689695801015107]
	TIME [epoch: 8.33 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22212354711315027		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.22212354711315027 | validation: 0.18460132753209363]
	TIME [epoch: 8.33 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1769663546464142		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.1769663546464142 | validation: 0.21886528494995938]
	TIME [epoch: 8.35 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21834369705475914		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.21834369705475914 | validation: 0.16954027059881124]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2006583963061182		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.2006583963061182 | validation: 0.24816919957283953]
	TIME [epoch: 8.34 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22050023469722319		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.22050023469722319 | validation: 0.2500491609735537]
	TIME [epoch: 8.33 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2319876493838104		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.2319876493838104 | validation: 0.19940625114712146]
	TIME [epoch: 8.33 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2025426985070919		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.2025426985070919 | validation: 0.21848376015977553]
	TIME [epoch: 8.35 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22625992365344957		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.22625992365344957 | validation: 0.30105941710282824]
	TIME [epoch: 8.33 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21749689334182495		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.21749689334182495 | validation: 0.194818628612479]
	TIME [epoch: 8.33 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2186424784350535		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.2186424784350535 | validation: 0.2569150616139092]
	TIME [epoch: 8.33 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21267878652095623		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.21267878652095623 | validation: 0.18332166453016585]
	TIME [epoch: 8.35 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19982670782514583		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.19982670782514583 | validation: 0.23525786509751068]
	TIME [epoch: 8.33 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21337766229503408		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.21337766229503408 | validation: 0.24441154890565042]
	TIME [epoch: 8.32 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17272392491736338		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.17272392491736338 | validation: 0.21363611294110058]
	TIME [epoch: 8.32 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20152420327027495		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.20152420327027495 | validation: 0.18079211780855642]
	TIME [epoch: 8.35 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2083257176438952		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.2083257176438952 | validation: 0.18583243786442838]
	TIME [epoch: 8.33 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20491517955369215		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.20491517955369215 | validation: 0.22632315264110942]
	TIME [epoch: 8.33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20286842870130833		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.20286842870130833 | validation: 0.2353334543031096]
	TIME [epoch: 8.33 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2004572662296728		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.2004572662296728 | validation: 0.23574955229148215]
	TIME [epoch: 8.33 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20681094277484674		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.20681094277484674 | validation: 0.2594894716987006]
	TIME [epoch: 8.35 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1908323483014835		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.1908323483014835 | validation: 0.1955002136230637]
	TIME [epoch: 8.33 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19729198245974475		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.19729198245974475 | validation: 0.17950723573306243]
	TIME [epoch: 8.33 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17486566038531035		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.17486566038531035 | validation: 0.21548833726203076]
	TIME [epoch: 8.33 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20517294205330044		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.20517294205330044 | validation: 0.21157950610215256]
	TIME [epoch: 8.35 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1700550393134872		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.1700550393134872 | validation: 0.17422451701089683]
	TIME [epoch: 8.33 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2206737350741526		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.2206737350741526 | validation: 0.24341592188625358]
	TIME [epoch: 8.33 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21548204016117706		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.21548204016117706 | validation: 0.2522875190528002]
	TIME [epoch: 8.32 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21701262153118614		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.21701262153118614 | validation: 0.2012610475816322]
	TIME [epoch: 8.35 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19465493832835506		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.19465493832835506 | validation: 0.19637264073458482]
	TIME [epoch: 8.33 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18940185382356006		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.18940185382356006 | validation: 0.163472826168075]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_931.pth
	Model improved!!!
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21419844125897253		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.21419844125897253 | validation: 0.17757350490940843]
	TIME [epoch: 8.32 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18440011464414222		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.18440011464414222 | validation: 0.20893031069635173]
	TIME [epoch: 8.32 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19381741892471524		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.19381741892471524 | validation: 0.20757941523179552]
	TIME [epoch: 8.34 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2069678487153189		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.2069678487153189 | validation: 0.25253443289749833]
	TIME [epoch: 8.32 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1826946945496681		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.1826946945496681 | validation: 0.19150766686513881]
	TIME [epoch: 8.32 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20637511652335272		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.20637511652335272 | validation: 0.2057005595350629]
	TIME [epoch: 8.32 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2280605970513217		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.2280605970513217 | validation: 0.171352716382537]
	TIME [epoch: 8.34 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19510675584606185		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.19510675584606185 | validation: 0.31909820401255506]
	TIME [epoch: 8.32 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19648894499176406		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.19648894499176406 | validation: 0.32591369343542387]
	TIME [epoch: 8.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1941022860045907		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.1941022860045907 | validation: 0.21290880828554665]
	TIME [epoch: 8.32 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21585375874125798		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.21585375874125798 | validation: 0.2503297239792334]
	TIME [epoch: 8.34 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17845027421686258		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.17845027421686258 | validation: 0.2652389469767884]
	TIME [epoch: 8.32 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16754907197329422		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.16754907197329422 | validation: 0.22138262215034366]
	TIME [epoch: 8.32 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19346674116090962		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.19346674116090962 | validation: 0.1825003809019577]
	TIME [epoch: 8.32 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18312812181207025		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.18312812181207025 | validation: 0.22810614136368956]
	TIME [epoch: 8.32 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2964189608513104		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.2964189608513104 | validation: 0.2768054487096595]
	TIME [epoch: 8.34 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2019141484298142		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.2019141484298142 | validation: 0.3086024140602398]
	TIME [epoch: 8.32 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19606727629945048		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.19606727629945048 | validation: 0.19461742445089952]
	TIME [epoch: 8.32 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21914955280894977		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.21914955280894977 | validation: 0.15962084500616008]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18217393760102796		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.18217393760102796 | validation: 0.1830084368836432]
	TIME [epoch: 8.35 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17806574004013556		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.17806574004013556 | validation: 0.18278928834130628]
	TIME [epoch: 8.32 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1799728972454051		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.1799728972454051 | validation: 0.1930276262290589]
	TIME [epoch: 8.32 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16713365186377557		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.16713365186377557 | validation: 0.17753898250460504]
	TIME [epoch: 8.32 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1810137667148599		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.1810137667148599 | validation: 0.23968258569933976]
	TIME [epoch: 8.34 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2425951951289628		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.2425951951289628 | validation: 0.24765867284537563]
	TIME [epoch: 8.32 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24216789112432546		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.24216789112432546 | validation: 0.27667627886006385]
	TIME [epoch: 8.32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19068400482803807		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.19068400482803807 | validation: 0.19633608036632638]
	TIME [epoch: 8.32 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22111871307560738		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.22111871307560738 | validation: 0.2039320533459527]
	TIME [epoch: 8.33 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18850440334070168		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.18850440334070168 | validation: 0.17630168801778678]
	TIME [epoch: 8.34 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19654891244352196		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.19654891244352196 | validation: 0.20445615292869523]
	TIME [epoch: 8.32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18774526845543452		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.18774526845543452 | validation: 0.215416212324334]
	TIME [epoch: 8.32 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2332249101352927		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.2332249101352927 | validation: 0.20661586868643367]
	TIME [epoch: 8.32 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26375596015832203		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.26375596015832203 | validation: 0.35361642537373816]
	TIME [epoch: 8.34 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23055497413274478		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.23055497413274478 | validation: 0.27801626241775246]
	TIME [epoch: 8.32 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19585446217840094		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.19585446217840094 | validation: 0.25429125709912714]
	TIME [epoch: 8.32 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16358729095516036		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.16358729095516036 | validation: 0.17756405053639945]
	TIME [epoch: 8.32 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16181408400764263		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.16181408400764263 | validation: 0.165442520148625]
	TIME [epoch: 8.35 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18515356836367264		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.18515356836367264 | validation: 0.19959079676861113]
	TIME [epoch: 8.32 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19227052227985403		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.19227052227985403 | validation: 0.2994333356117639]
	TIME [epoch: 8.32 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1830587232769105		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.1830587232769105 | validation: 0.20141239182618836]
	TIME [epoch: 8.32 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.201840358420101		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.201840358420101 | validation: 0.1537538297511452]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_972.pth
	Model improved!!!
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22806101065080772		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.22806101065080772 | validation: 0.1988494432635027]
	TIME [epoch: 8.34 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15662269573267412		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.15662269573267412 | validation: 0.17195751695856515]
	TIME [epoch: 8.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18924162062798972		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.18924162062798972 | validation: 0.23511919029717204]
	TIME [epoch: 8.32 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19008961361390309		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.19008961361390309 | validation: 0.16617216508248328]
	TIME [epoch: 8.32 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1774542447498106		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.1774542447498106 | validation: 0.16494512783379733]
	TIME [epoch: 8.34 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.183616564675843		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.183616564675843 | validation: 0.2090533855827512]
	TIME [epoch: 8.32 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2000825152422608		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.2000825152422608 | validation: 0.20163424733579574]
	TIME [epoch: 8.32 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20167801230015625		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.20167801230015625 | validation: 0.16897035317120762]
	TIME [epoch: 8.32 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20698762970819992		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.20698762970819992 | validation: 0.22193148028630905]
	TIME [epoch: 8.34 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17985366238879388		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.17985366238879388 | validation: 0.17407405395265738]
	TIME [epoch: 8.33 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18032586521026306		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.18032586521026306 | validation: 0.2224797640046174]
	TIME [epoch: 8.32 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1829401233436669		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.1829401233436669 | validation: 0.23821558941401516]
	TIME [epoch: 8.32 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17490347967389858		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.17490347967389858 | validation: 0.18720801723085456]
	TIME [epoch: 8.34 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1693631683022072		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.1693631683022072 | validation: 0.18447563993279759]
	TIME [epoch: 8.34 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17267613651209068		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.17267613651209068 | validation: 0.1896595396485152]
	TIME [epoch: 8.33 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17064772754469631		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.17064772754469631 | validation: 0.24333367004662504]
	TIME [epoch: 8.32 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1727525727554568		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.1727525727554568 | validation: 0.21436349726996748]
	TIME [epoch: 8.33 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20714407073816415		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.20714407073816415 | validation: 0.2854162675945555]
	TIME [epoch: 8.35 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2030169913359471		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.2030169913359471 | validation: 0.21474549942549412]
	TIME [epoch: 8.33 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17933872563163894		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.17933872563163894 | validation: 0.18372737049962495]
	TIME [epoch: 8.33 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17120167057114727		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.17120167057114727 | validation: 0.16701500860187082]
	TIME [epoch: 8.32 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17970304630516531		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.17970304630516531 | validation: 0.15006282323739478]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17164379659588772		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.17164379659588772 | validation: 0.27111514870579234]
	TIME [epoch: 8.33 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19026284579403321		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.19026284579403321 | validation: 0.20115907193537214]
	TIME [epoch: 8.33 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18578728294679991		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.18578728294679991 | validation: 0.19759089169931876]
	TIME [epoch: 8.32 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1836865500713834		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.1836865500713834 | validation: 0.1818347305566388]
	TIME [epoch: 8.34 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19372402002020728		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.19372402002020728 | validation: 0.2503229438538863]
	TIME [epoch: 8.33 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2097978060695635		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.2097978060695635 | validation: 0.22074736620748772]
	TIME [epoch: 8.32 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17347576480044108		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.17347576480044108 | validation: 0.1907096436507134]
	TIME [epoch: 8.33 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20752808348475904		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.20752808348475904 | validation: 0.7743545977610036]
	TIME [epoch: 8.33 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.491233760777427		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.491233760777427 | validation: 0.18005371953828053]
	TIME [epoch: 8.35 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18223622517492358		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.18223622517492358 | validation: 0.2484459292780839]
	TIME [epoch: 8.33 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1686928095568287		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.1686928095568287 | validation: 0.15753979128174256]
	TIME [epoch: 8.32 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15573514614632225		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.15573514614632225 | validation: 0.17111434381972657]
	TIME [epoch: 8.32 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18216683382614737		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.18216683382614737 | validation: 0.17420036112818904]
	TIME [epoch: 8.34 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1830203518160653		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.1830203518160653 | validation: 0.2048973061822853]
	TIME [epoch: 8.32 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2002999935208213		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.2002999935208213 | validation: 0.16631087446240955]
	TIME [epoch: 8.32 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17258196453329816		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.17258196453329816 | validation: 0.15248561381894393]
	TIME [epoch: 8.32 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1814592340027281		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.1814592340027281 | validation: 0.19487746929703584]
	TIME [epoch: 8.34 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17990830723332749		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.17990830723332749 | validation: 0.31678123151819715]
	TIME [epoch: 8.33 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16947421668439047		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.16947421668439047 | validation: 0.19086104844055976]
	TIME [epoch: 8.33 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16563734812717792		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.16563734812717792 | validation: 0.2592780424788105]
	TIME [epoch: 8.33 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20020024624117344		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.20020024624117344 | validation: 0.5041695002143955]
	TIME [epoch: 8.33 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2310347888932182		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.2310347888932182 | validation: 0.1955253372530611]
	TIME [epoch: 8.35 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1687916130626417		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.1687916130626417 | validation: 0.2507126400819767]
	TIME [epoch: 8.32 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2561350537874335		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.2561350537874335 | validation: 0.2575171759502094]
	TIME [epoch: 8.32 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2156172767281491		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.2156172767281491 | validation: 0.2215038295810044]
	TIME [epoch: 8.32 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1885813717964076		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.1885813717964076 | validation: 0.20283719752993112]
	TIME [epoch: 8.35 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1996158735776329		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.1996158735776329 | validation: 0.17332123917127332]
	TIME [epoch: 8.32 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15153258391620403		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.15153258391620403 | validation: 0.16190148501034612]
	TIME [epoch: 8.32 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16346549437324526		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.16346549437324526 | validation: 0.17675707195496373]
	TIME [epoch: 8.32 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19403635475386874		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.19403635475386874 | validation: 0.24060690776505922]
	TIME [epoch: 8.33 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1855339949999559		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.1855339949999559 | validation: 0.15069823892600853]
	TIME [epoch: 8.33 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15916345387529757		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.15916345387529757 | validation: 0.17072139015606136]
	TIME [epoch: 8.32 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17278244759434314		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.17278244759434314 | validation: 0.16813472081550201]
	TIME [epoch: 8.32 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15519156308723492		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.15519156308723492 | validation: 0.2101449258513859]
	TIME [epoch: 8.32 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16504912854286302		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.16504912854286302 | validation: 0.17055747384553738]
	TIME [epoch: 8.35 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1538672618454589		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.1538672618454589 | validation: 0.1622044803459315]
	TIME [epoch: 8.33 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15421613219003183		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.15421613219003183 | validation: 0.15136571292900566]
	TIME [epoch: 8.32 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18647729323587203		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.18647729323587203 | validation: 0.242688533266428]
	TIME [epoch: 8.32 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19889787670828557		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.19889787670828557 | validation: 0.22533698352958015]
	TIME [epoch: 8.35 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18900356790945566		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.18900356790945566 | validation: 0.2015380076612593]
	TIME [epoch: 8.33 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1688920905407712		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.1688920905407712 | validation: 0.22549788115108205]
	TIME [epoch: 8.32 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17470626657161925		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.17470626657161925 | validation: 0.16907434686417183]
	TIME [epoch: 8.33 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17167142781013942		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.17167142781013942 | validation: 0.2704776184757721]
	TIME [epoch: 8.34 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.206242208853279		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.206242208853279 | validation: 0.24417311969329691]
	TIME [epoch: 8.34 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1831008195345926		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.1831008195345926 | validation: 0.2226116666839063]
	TIME [epoch: 8.32 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18102504065579697		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.18102504065579697 | validation: 0.17673183122965264]
	TIME [epoch: 8.33 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18920471874033096		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.18920471874033096 | validation: 0.17197713942981968]
	TIME [epoch: 8.33 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17260856537723052		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.17260856537723052 | validation: 0.2239693316548485]
	TIME [epoch: 8.34 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1814433265893704		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.1814433265893704 | validation: 0.15179349615931392]
	TIME [epoch: 8.32 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1735543939484943		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.1735543939484943 | validation: 0.15156477650907738]
	TIME [epoch: 8.32 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1776992333317153		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.1776992333317153 | validation: 0.15150332565246324]
	TIME [epoch: 8.32 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16387027550446687		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.16387027550446687 | validation: 0.20364239993778088]
	TIME [epoch: 8.35 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16585479576671802		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.16585479576671802 | validation: 0.1884196160514783]
	TIME [epoch: 8.33 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16312021959711312		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.16312021959711312 | validation: 0.2135209055172677]
	TIME [epoch: 8.33 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17533352323098578		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.17533352323098578 | validation: 0.2362561220275786]
	TIME [epoch: 8.33 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23614138841218		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.23614138841218 | validation: 0.2955417738187894]
	TIME [epoch: 8.34 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16937999952049806		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.16937999952049806 | validation: 0.1891393633867759]
	TIME [epoch: 8.34 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15915047073341		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.15915047073341 | validation: 0.20501462091728517]
	TIME [epoch: 8.32 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15756112446574627		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.15756112446574627 | validation: 0.17703555230707535]
	TIME [epoch: 8.32 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17110373831795278		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.17110373831795278 | validation: 0.22754729051051303]
	TIME [epoch: 8.32 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15221361533537056		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.15221361533537056 | validation: 0.1484040052330614]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1055.pth
	Model improved!!!
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17434640020259193		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.17434640020259193 | validation: 0.16326365267716633]
	TIME [epoch: 8.33 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15901885552462086		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.15901885552462086 | validation: 0.19643473904813058]
	TIME [epoch: 8.33 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19309231019864836		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.19309231019864836 | validation: 0.18294806814679213]
	TIME [epoch: 8.32 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1422999254576691		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.1422999254576691 | validation: 0.15645514684292172]
	TIME [epoch: 8.35 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17149890086584785		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.17149890086584785 | validation: 0.37444355037369276]
	TIME [epoch: 8.33 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22963317495562166		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.22963317495562166 | validation: 0.1968028231888287]
	TIME [epoch: 8.33 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16477745182710926		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.16477745182710926 | validation: 0.16963887971295682]
	TIME [epoch: 8.32 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18027964964321652		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.18027964964321652 | validation: 0.20404786175802692]
	TIME [epoch: 8.34 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19813625964362774		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.19813625964362774 | validation: 0.17363121911624074]
	TIME [epoch: 8.33 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16305914120238568		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.16305914120238568 | validation: 0.26220622487466344]
	TIME [epoch: 8.32 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1552648475443979		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.1552648475443979 | validation: 0.19955321429971473]
	TIME [epoch: 8.32 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.167586144463376		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.167586144463376 | validation: 0.15979483291733296]
	TIME [epoch: 8.32 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16187092737787806		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.16187092737787806 | validation: 0.31078251459270145]
	TIME [epoch: 8.35 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16850552288448545		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.16850552288448545 | validation: 0.23408256836614072]
	TIME [epoch: 8.32 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17389715264489083		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.17389715264489083 | validation: 0.17217990677863992]
	TIME [epoch: 8.33 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16108069381426735		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.16108069381426735 | validation: 0.43066647128018737]
	TIME [epoch: 8.32 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2023024069874495		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.2023024069874495 | validation: 0.1637234547022902]
	TIME [epoch: 8.35 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15837272886582981		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.15837272886582981 | validation: 0.20531759833295277]
	TIME [epoch: 8.33 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.173276711262955		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.173276711262955 | validation: 0.1851830284832675]
	TIME [epoch: 8.33 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14836909815752364		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.14836909815752364 | validation: 0.16181650442477324]
	TIME [epoch: 8.32 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15669246350910443		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.15669246350910443 | validation: 0.17798227394021124]
	TIME [epoch: 8.34 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1512186870874961		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.1512186870874961 | validation: 0.1874468162731841]
	TIME [epoch: 8.34 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15442155845130512		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.15442155845130512 | validation: 0.1686871028183261]
	TIME [epoch: 8.32 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15775475392391122		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.15775475392391122 | validation: 0.16579101312668953]
	TIME [epoch: 8.32 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16793853689507063		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.16793853689507063 | validation: 0.23962572603450027]
	TIME [epoch: 8.32 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1620332312546375		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.1620332312546375 | validation: 0.1714917802480535]
	TIME [epoch: 8.34 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14607940425778498		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.14607940425778498 | validation: 0.17684025232053616]
	TIME [epoch: 8.32 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15032979519651035		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.15032979519651035 | validation: 0.1643021324871032]
	TIME [epoch: 8.32 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15824605584736914		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.15824605584736914 | validation: 0.17339422106502533]
	TIME [epoch: 8.32 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1843507064912182		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.1843507064912182 | validation: 0.21584509498032262]
	TIME [epoch: 8.34 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2041786895211181		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.2041786895211181 | validation: 0.1658430611959704]
	TIME [epoch: 8.33 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18791802940878702		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.18791802940878702 | validation: 0.1850772702056535]
	TIME [epoch: 8.32 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1743351397431563		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.1743351397431563 | validation: 0.17530866271779377]
	TIME [epoch: 8.32 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1749485139983406		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.1749485139983406 | validation: 0.19019668013760538]
	TIME [epoch: 8.33 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16006056263823237		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.16006056263823237 | validation: 0.15872872631856993]
	TIME [epoch: 8.34 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1552584838481565		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.1552584838481565 | validation: 0.16251189667109114]
	TIME [epoch: 8.32 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1568155447707225		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.1568155447707225 | validation: 0.18070347209681972]
	TIME [epoch: 8.32 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1499463838652954		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.1499463838652954 | validation: 0.20407059418593435]
	TIME [epoch: 8.33 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1410758396978349		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.1410758396978349 | validation: 0.17581962154608904]
	TIME [epoch: 8.34 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15975362287727884		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.15975362287727884 | validation: 0.2312671154766799]
	TIME [epoch: 8.32 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16559179245235192		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.16559179245235192 | validation: 0.15059130151065112]
	TIME [epoch: 8.32 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15758641644696525		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.15758641644696525 | validation: 0.16812794997659392]
	TIME [epoch: 8.33 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16475682879498865		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.16475682879498865 | validation: 0.26692809086293506]
	TIME [epoch: 8.35 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18346315115180828		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.18346315115180828 | validation: 0.182098715756016]
	TIME [epoch: 8.32 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15569523246795702		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.15569523246795702 | validation: 0.17353303073595455]
	TIME [epoch: 8.32 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16454029779759877		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.16454029779759877 | validation: 0.18233970816752731]
	TIME [epoch: 8.32 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17088592949302767		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.17088592949302767 | validation: 0.17137036729510402]
	TIME [epoch: 8.33 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15811841399013737		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.15811841399013737 | validation: 0.1861119651538105]
	TIME [epoch: 8.33 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14398301047782086		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.14398301047782086 | validation: 0.13781975745975303]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1104.pth
	Model improved!!!
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1478103610765168		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.1478103610765168 | validation: 0.15892722542722684]
	TIME [epoch: 8.32 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1454304535073107		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.1454304535073107 | validation: 0.1541148944861129]
	TIME [epoch: 8.32 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16072412504978884		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.16072412504978884 | validation: 0.1912415086198997]
	TIME [epoch: 8.34 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1417000317574582		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.1417000317574582 | validation: 0.18158790722179077]
	TIME [epoch: 8.32 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20277448917375634		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.20277448917375634 | validation: 0.18216972757822297]
	TIME [epoch: 8.32 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1625901855564181		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.1625901855564181 | validation: 0.2053436254147517]
	TIME [epoch: 8.32 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16705600790256656		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.16705600790256656 | validation: 0.17757986032791326]
	TIME [epoch: 8.34 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14634071383800387		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.14634071383800387 | validation: 0.16005002256264964]
	TIME [epoch: 8.32 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14311908014286984		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.14311908014286984 | validation: 0.15358046540239045]
	TIME [epoch: 8.32 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13973214777623041		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.13973214777623041 | validation: 0.17195819041654187]
	TIME [epoch: 8.32 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16498528364316611		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.16498528364316611 | validation: 0.2785052126053987]
	TIME [epoch: 8.33 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20491983670304736		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.20491983670304736 | validation: 0.21645471147734224]
	TIME [epoch: 8.33 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1563040486148319		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.1563040486148319 | validation: 0.16664520483201806]
	TIME [epoch: 8.32 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14923783522275358		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.14923783522275358 | validation: 0.16191959557844338]
	TIME [epoch: 8.32 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16985608107204558		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.16985608107204558 | validation: 0.19018190165368404]
	TIME [epoch: 8.32 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1592400914456802		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.1592400914456802 | validation: 0.14419737784102393]
	TIME [epoch: 8.34 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15389875663086702		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.15389875663086702 | validation: 0.1721425059395296]
	TIME [epoch: 8.32 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16702374892329408		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.16702374892329408 | validation: 0.157818244603937]
	TIME [epoch: 8.32 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17805455609267037		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.17805455609267037 | validation: 0.16400974582457234]
	TIME [epoch: 8.31 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1515944064424843		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.1515944064424843 | validation: 0.21319911655636958]
	TIME [epoch: 8.34 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18716454379573008		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.18716454379573008 | validation: 0.1795984466294649]
	TIME [epoch: 8.32 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19943951382831684		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.19943951382831684 | validation: 0.17815474686700428]
	TIME [epoch: 8.32 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16775532494920045		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.16775532494920045 | validation: 0.16740791952055373]
	TIME [epoch: 8.32 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1653621239431566		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.1653621239431566 | validation: 0.2309523908128468]
	TIME [epoch: 8.33 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15741772075410063		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.15741772075410063 | validation: 0.1640292366046831]
	TIME [epoch: 8.33 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16431542554509687		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.16431542554509687 | validation: 0.1816236233278764]
	TIME [epoch: 8.31 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1599814685471243		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.1599814685471243 | validation: 0.13683345236371852]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1131.pth
	Model improved!!!
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1365728533251128		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.1365728533251128 | validation: 0.14268159164968994]
	TIME [epoch: 8.32 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.161491092757849		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.161491092757849 | validation: 0.18987356513894105]
	TIME [epoch: 8.34 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15630030056886748		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.15630030056886748 | validation: 0.1938693271520275]
	TIME [epoch: 8.32 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1708072238715701		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.1708072238715701 | validation: 0.17397171810518683]
	TIME [epoch: 8.31 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1437046347093451		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.1437046347093451 | validation: 0.1487501455615537]
	TIME [epoch: 8.31 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1376675931708569		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.1376675931708569 | validation: 0.14165268179169616]
	TIME [epoch: 8.34 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15605419887641092		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.15605419887641092 | validation: 0.18822104468114642]
	TIME [epoch: 8.32 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1572263397760833		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.1572263397760833 | validation: 0.1743228057121517]
	TIME [epoch: 8.32 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17084093236266223		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.17084093236266223 | validation: 0.16216930418822836]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15689728627431931		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.15689728627431931 | validation: 0.15206228207552977]
	TIME [epoch: 8.33 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1677826296241633		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.1677826296241633 | validation: 0.1982140937743388]
	TIME [epoch: 8.33 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1675930091410483		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.1675930091410483 | validation: 0.14384754962140966]
	TIME [epoch: 8.32 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1739748745549447		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.1739748745549447 | validation: 0.1682058313953047]
	TIME [epoch: 8.32 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17920781869859942		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.17920781869859942 | validation: 0.17495735951916297]
	TIME [epoch: 8.32 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16196102002450702		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.16196102002450702 | validation: 0.16199566792397047]
	TIME [epoch: 8.34 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15389167476246784		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.15389167476246784 | validation: 0.16298211805544954]
	TIME [epoch: 8.32 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1444338485214935		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.1444338485214935 | validation: 0.1576423955906841]
	TIME [epoch: 8.32 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16360375635483276		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.16360375635483276 | validation: 0.1793615249771703]
	TIME [epoch: 8.32 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1453908361396727		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.1453908361396727 | validation: 0.14971162962042706]
	TIME [epoch: 8.34 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20394570491504033		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.20394570491504033 | validation: 0.16250657832470083]
	TIME [epoch: 8.32 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15688001373914148		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.15688001373914148 | validation: 0.15461211538375705]
	TIME [epoch: 8.32 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14579039344965902		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.14579039344965902 | validation: 0.18508710924257263]
	TIME [epoch: 8.32 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15155770011227637		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.15155770011227637 | validation: 0.1941966584406536]
	TIME [epoch: 8.33 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1421031004450915		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.1421031004450915 | validation: 0.1498579196701449]
	TIME [epoch: 8.33 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14704021498493544		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.14704021498493544 | validation: 0.1717787696481565]
	TIME [epoch: 8.32 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14287415871503822		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.14287415871503822 | validation: 0.14316197031298855]
	TIME [epoch: 8.31 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14144198665697744		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.14144198665697744 | validation: 0.1291146727630328]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16328375396104905		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.16328375396104905 | validation: 0.18132909842284492]
	TIME [epoch: 8.37 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14144702201980078		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.14144702201980078 | validation: 0.1803352648784726]
	TIME [epoch: 8.35 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16375507993827934		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.16375507993827934 | validation: 0.13637073876073524]
	TIME [epoch: 8.35 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18873443264272907		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.18873443264272907 | validation: 0.3127260506729692]
	TIME [epoch: 8.35 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15758088875459186		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.15758088875459186 | validation: 0.16710664514079512]
	TIME [epoch: 8.37 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14292741513916016		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.14292741513916016 | validation: 0.15066073720318734]
	TIME [epoch: 8.36 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15375241540697399		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.15375241540697399 | validation: 0.15646896791539922]
	TIME [epoch: 8.35 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14906545904811136		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.14906545904811136 | validation: 0.2063099944056464]
	TIME [epoch: 8.35 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15329042478377677		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.15329042478377677 | validation: 0.16183306090816793]
	TIME [epoch: 8.38 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15824762230798287		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.15824762230798287 | validation: 0.16670697124438427]
	TIME [epoch: 8.35 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1486221545636484		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.1486221545636484 | validation: 0.19798976473047625]
	TIME [epoch: 8.35 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15398980110945307		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.15398980110945307 | validation: 0.17588118244417894]
	TIME [epoch: 8.35 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1409897385603341		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.1409897385603341 | validation: 0.14646758638093071]
	TIME [epoch: 8.35 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13942184698520887		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.13942184698520887 | validation: 0.16020943823530354]
	TIME [epoch: 8.38 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13627466831517102		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.13627466831517102 | validation: 0.1582507023800348]
	TIME [epoch: 8.35 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14480125027901575		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.14480125027901575 | validation: 0.14368067998215905]
	TIME [epoch: 8.35 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12765633516648506		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.12765633516648506 | validation: 0.1531046905108286]
	TIME [epoch: 8.35 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1403531792629146		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.1403531792629146 | validation: 0.21215725433148203]
	TIME [epoch: 8.38 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19589728926583774		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.19589728926583774 | validation: 0.17089466143213777]
	TIME [epoch: 8.35 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15101869725955383		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.15101869725955383 | validation: 0.17769283870270713]
	TIME [epoch: 8.36 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16274270378906333		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.16274270378906333 | validation: 0.14815939050688082]
	TIME [epoch: 8.35 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13710671167581218		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.13710671167581218 | validation: 0.1471073388888215]
	TIME [epoch: 8.38 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15655411212756337		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.15655411212756337 | validation: 0.14931473278366608]
	TIME [epoch: 8.36 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14273508013120406		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.14273508013120406 | validation: 0.19729288321081234]
	TIME [epoch: 8.36 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14177927318492262		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.14177927318492262 | validation: 0.15152187282739157]
	TIME [epoch: 8.36 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12901562288770968		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.12901562288770968 | validation: 0.20223690968238328]
	TIME [epoch: 8.36 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14273620628654557		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.14273620628654557 | validation: 0.162581189761604]
	TIME [epoch: 8.38 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16344919859411927		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.16344919859411927 | validation: 0.20273452609021958]
	TIME [epoch: 8.35 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16189775834882064		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.16189775834882064 | validation: 0.20460399490224634]
	TIME [epoch: 8.35 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1692453113308363		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.1692453113308363 | validation: 0.2096706273230455]
	TIME [epoch: 8.35 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14235332962698202		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.14235332962698202 | validation: 0.16799646731560505]
	TIME [epoch: 8.37 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14385864338614493		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.14385864338614493 | validation: 0.16129065673483656]
	TIME [epoch: 8.35 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13819206153617095		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.13819206153617095 | validation: 0.14354325225334252]
	TIME [epoch: 8.34 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13581004421265883		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.13581004421265883 | validation: 0.1817637325527691]
	TIME [epoch: 8.34 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16769404703087662		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.16769404703087662 | validation: 0.2365534077566176]
	TIME [epoch: 8.37 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16041072266055584		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.16041072266055584 | validation: 0.15949835155104597]
	TIME [epoch: 8.34 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15297568836767844		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.15297568836767844 | validation: 0.1710056797952483]
	TIME [epoch: 8.35 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14480271236884162		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.14480271236884162 | validation: 0.17123286462436368]
	TIME [epoch: 8.34 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15388195026849735		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.15388195026849735 | validation: 0.13645870969201576]
	TIME [epoch: 8.35 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14253488435277303		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.14253488435277303 | validation: 0.12885803660193718]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1412035536905397		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.1412035536905397 | validation: 0.19819899786771272]
	TIME [epoch: 8.34 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14139884998748717		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.14139884998748717 | validation: 0.18522012415821787]
	TIME [epoch: 8.34 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1473126804007789		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.1473126804007789 | validation: 0.17188861896820665]
	TIME [epoch: 8.34 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14374888746208447		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.14374888746208447 | validation: 0.17592803819865943]
	TIME [epoch: 8.36 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1513443586783707		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.1513443586783707 | validation: 0.15719167586381633]
	TIME [epoch: 8.34 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13898220479535042		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.13898220479535042 | validation: 0.3469739635313117]
	TIME [epoch: 8.34 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17026815608313567		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.17026815608313567 | validation: 0.363180036548853]
	TIME [epoch: 8.34 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20275245164391892		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.20275245164391892 | validation: 0.1944197022623987]
	TIME [epoch: 8.36 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15268552357538784		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.15268552357538784 | validation: 0.17198503700191448]
	TIME [epoch: 8.34 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13765652143644141		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.13765652143644141 | validation: 0.18037848086647423]
	TIME [epoch: 8.34 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13932675423674892		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.13932675423674892 | validation: 0.2022051396559484]
	TIME [epoch: 8.34 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16818380243813466		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.16818380243813466 | validation: 0.16954306300306954]
	TIME [epoch: 8.34 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1354405631194584		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.1354405631194584 | validation: 0.1447038838127374]
	TIME [epoch: 8.36 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13330764216060675		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.13330764216060675 | validation: 0.14189606054276283]
	TIME [epoch: 8.34 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14000805844405398		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.14000805844405398 | validation: 0.15630290756615522]
	TIME [epoch: 8.34 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13999220541446286		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.13999220541446286 | validation: 0.14421190875904416]
	TIME [epoch: 8.34 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14239122667178625		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.14239122667178625 | validation: 0.16775861700786598]
	TIME [epoch: 8.36 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16401884379023135		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.16401884379023135 | validation: 0.15454386515719626]
	TIME [epoch: 8.34 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15542537086648262		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.15542537086648262 | validation: 0.1490811406223502]
	TIME [epoch: 8.34 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13084687050039517		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.13084687050039517 | validation: 0.17357432177733362]
	TIME [epoch: 8.34 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1462898667138008		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.1462898667138008 | validation: 0.17408032093475895]
	TIME [epoch: 8.36 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16032165091749967		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.16032165091749967 | validation: 0.19210026415432013]
	TIME [epoch: 8.34 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13662700655422363		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.13662700655422363 | validation: 0.12719305165761674]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1221.pth
	Model improved!!!
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14623765568000482		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.14623765568000482 | validation: 0.15019297441176518]
	TIME [epoch: 8.33 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14557391676142392		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.14557391676142392 | validation: 0.17433440966445474]
	TIME [epoch: 8.34 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14397191474910986		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.14397191474910986 | validation: 0.20374225032443552]
	TIME [epoch: 8.34 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16284035256053783		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.16284035256053783 | validation: 0.18432189771593904]
	TIME [epoch: 8.33 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16120317864863548		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.16120317864863548 | validation: 0.17240176103340538]
	TIME [epoch: 8.33 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13702104919584707		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.13702104919584707 | validation: 0.17611480677691344]
	TIME [epoch: 8.33 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15250933427961244		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.15250933427961244 | validation: 0.16172805177676033]
	TIME [epoch: 8.36 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1411528995155524		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.1411528995155524 | validation: 0.1560759691873376]
	TIME [epoch: 8.33 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12690633028291218		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.12690633028291218 | validation: 0.1411187380889165]
	TIME [epoch: 8.33 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16685796424551796		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.16685796424551796 | validation: 0.23535398019822812]
	TIME [epoch: 8.33 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18335503388371074		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.18335503388371074 | validation: 0.14989872313479796]
	TIME [epoch: 8.35 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13197748876874582		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.13197748876874582 | validation: 0.1328671343842999]
	TIME [epoch: 8.33 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13347448792985314		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.13347448792985314 | validation: 0.17415307531403457]
	TIME [epoch: 8.33 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14283480578831992		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.14283480578831992 | validation: 0.15128187041446525]
	TIME [epoch: 8.33 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13078413962301125		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.13078413962301125 | validation: 0.19244859209190013]
	TIME [epoch: 8.34 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15805027951867923		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.15805027951867923 | validation: 0.2019302117942935]
	TIME [epoch: 8.34 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14846553964603076		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.14846553964603076 | validation: 0.15656014122946538]
	TIME [epoch: 8.33 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348464848077497		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.1348464848077497 | validation: 0.14327889421353457]
	TIME [epoch: 8.33 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14283869811135297		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.14283869811135297 | validation: 0.15403643806590075]
	TIME [epoch: 8.33 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14899828909464224		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.14899828909464224 | validation: 0.1490126725494988]
	TIME [epoch: 8.35 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14783416117484866		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.14783416117484866 | validation: 0.1391190585210253]
	TIME [epoch: 8.33 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13107764668697833		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.13107764668697833 | validation: 0.17940046824406636]
	TIME [epoch: 8.33 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348990365533042		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.1348990365533042 | validation: 0.1959039319678784]
	TIME [epoch: 8.33 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1385254888361441		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.1385254888361441 | validation: 0.1616298792753172]
	TIME [epoch: 8.36 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14776816171943852		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.14776816171943852 | validation: 0.1397986390940634]
	TIME [epoch: 8.34 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13096180479841224		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.13096180479841224 | validation: 0.16299660420852552]
	TIME [epoch: 8.33 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14111067422068574		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.14111067422068574 | validation: 0.13914765777143273]
	TIME [epoch: 8.33 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14176216603711947		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.14176216603711947 | validation: 0.12470487441929844]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1249.pth
	Model improved!!!
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14021053039532894		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.14021053039532894 | validation: 0.16436295287607477]
	TIME [epoch: 8.34 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15437153314004332		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.15437153314004332 | validation: 0.16279517169850455]
	TIME [epoch: 8.33 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1488676651670231		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.1488676651670231 | validation: 0.159427420991282]
	TIME [epoch: 8.33 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12753701570356032		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.12753701570356032 | validation: 0.16491993575554814]
	TIME [epoch: 8.33 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1444994099189302		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.1444994099189302 | validation: 0.1457842812698578]
	TIME [epoch: 8.35 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1340373026601844		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.1340373026601844 | validation: 0.13686948156293993]
	TIME [epoch: 8.32 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1293448583773758		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.1293448583773758 | validation: 0.16727135624138814]
	TIME [epoch: 8.32 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14048668873170111		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.14048668873170111 | validation: 0.16008391257953097]
	TIME [epoch: 8.32 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13061254843836798		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.13061254843836798 | validation: 0.14212214887181018]
	TIME [epoch: 8.35 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1326400707290553		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.1326400707290553 | validation: 0.13883314010062603]
	TIME [epoch: 8.33 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13050653360335468		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.13050653360335468 | validation: 0.19969600892734635]
	TIME [epoch: 8.33 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13180918021315774		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.13180918021315774 | validation: 0.14275295513526087]
	TIME [epoch: 8.32 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13663392563737284		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.13663392563737284 | validation: 0.13444952998269813]
	TIME [epoch: 8.34 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13490283120794272		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.13490283120794272 | validation: 0.14845652797750614]
	TIME [epoch: 8.33 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13841862654683115		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.13841862654683115 | validation: 0.13904105870389144]
	TIME [epoch: 8.32 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14322647175963527		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.14322647175963527 | validation: 0.13984197170937757]
	TIME [epoch: 8.32 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13356456344357664		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.13356456344357664 | validation: 0.1369361913154452]
	TIME [epoch: 8.33 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12399122093745651		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.12399122093745651 | validation: 0.1374672567356432]
	TIME [epoch: 8.35 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13430663188146622		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.13430663188146622 | validation: 0.15725532085646948]
	TIME [epoch: 8.32 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13283882592718307		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.13283882592718307 | validation: 0.13560926758057795]
	TIME [epoch: 8.32 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1420900331180519		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.1420900331180519 | validation: 0.17878687461385756]
	TIME [epoch: 8.32 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13485049281978503		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.13485049281978503 | validation: 0.13602095536615255]
	TIME [epoch: 8.35 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12919912631812974		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.12919912631812974 | validation: 0.14424910733757187]
	TIME [epoch: 8.33 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13121241427404645		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.13121241427404645 | validation: 0.14371136725525407]
	TIME [epoch: 8.32 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12914873714238082		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.12914873714238082 | validation: 0.14254683982588612]
	TIME [epoch: 8.33 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13682591763174354		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.13682591763174354 | validation: 0.14494743302131305]
	TIME [epoch: 8.35 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14418038349921744		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.14418038349921744 | validation: 0.14542123706042528]
	TIME [epoch: 8.33 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.139966455664408		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.139966455664408 | validation: 0.17147570756309338]
	TIME [epoch: 8.32 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14210874101516052		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.14210874101516052 | validation: 0.14889993582840827]
	TIME [epoch: 8.32 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13876795062271446		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.13876795062271446 | validation: 0.15136308026094913]
	TIME [epoch: 8.33 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13538422990765744		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.13538422990765744 | validation: 0.1394030506149373]
	TIME [epoch: 8.35 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12799650552398414		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.12799650552398414 | validation: 0.14978168778072576]
	TIME [epoch: 8.33 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1307296850460707		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.1307296850460707 | validation: 0.14686168252098952]
	TIME [epoch: 8.32 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13567181476109957		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.13567181476109957 | validation: 0.13851062418524382]
	TIME [epoch: 8.32 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13291787528041027		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.13291787528041027 | validation: 0.15183848542265133]
	TIME [epoch: 8.34 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1254671738993597		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.1254671738993597 | validation: 0.13742499656137558]
	TIME [epoch: 8.33 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1267464973182968		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.1267464973182968 | validation: 0.1485759770318773]
	TIME [epoch: 8.32 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1392935872944913		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.1392935872944913 | validation: 0.19957089419045693]
	TIME [epoch: 8.32 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1393629624745364		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.1393629624745364 | validation: 0.15853409532286655]
	TIME [epoch: 8.34 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12781823468455528		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.12781823468455528 | validation: 0.12249405095919351]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1289.pth
	Model improved!!!
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1422262457199086		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.1422262457199086 | validation: 0.1307770204269845]
	TIME [epoch: 8.32 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13146761416723163		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.13146761416723163 | validation: 0.15374531811521414]
	TIME [epoch: 8.32 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12837945393044514		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.12837945393044514 | validation: 0.13117449215307767]
	TIME [epoch: 8.32 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12455470291934663		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.12455470291934663 | validation: 0.138669111309614]
	TIME [epoch: 8.34 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12556845059316146		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.12556845059316146 | validation: 0.12779333280635646]
	TIME [epoch: 8.32 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12602348559847848		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.12602348559847848 | validation: 0.15000740055918171]
	TIME [epoch: 8.33 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.129322479561316		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.129322479561316 | validation: 0.14364664856228831]
	TIME [epoch: 8.32 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14360503296728971		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.14360503296728971 | validation: 0.14696131870547324]
	TIME [epoch: 8.34 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13736510768494659		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.13736510768494659 | validation: 0.17178232168337326]
	TIME [epoch: 8.32 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348858011658354		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.1348858011658354 | validation: 0.16935229695049425]
	TIME [epoch: 8.32 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12725684975969226		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.12725684975969226 | validation: 0.15383439545492053]
	TIME [epoch: 8.32 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13506318640380063		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.13506318640380063 | validation: 0.14881398713158056]
	TIME [epoch: 8.34 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13035510672764353		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.13035510672764353 | validation: 0.13340123407277024]
	TIME [epoch: 8.32 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12317905600730858		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.12317905600730858 | validation: 0.12855748714760074]
	TIME [epoch: 8.33 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13025414911960037		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.13025414911960037 | validation: 0.15672970751872278]
	TIME [epoch: 8.32 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13396186984572406		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.13396186984572406 | validation: 0.13848810709782974]
	TIME [epoch: 8.32 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12702717488347584		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.12702717488347584 | validation: 0.13309168864491638]
	TIME [epoch: 8.34 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12956936288284757		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.12956936288284757 | validation: 0.1298194740998907]
	TIME [epoch: 8.32 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13331645618885238		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.13331645618885238 | validation: 0.1352158694318575]
	TIME [epoch: 8.33 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.138646016693319		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.138646016693319 | validation: 0.1489442706601588]
	TIME [epoch: 8.32 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13137116453871886		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.13137116453871886 | validation: 0.13565256585921584]
	TIME [epoch: 8.34 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13521701783960288		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.13521701783960288 | validation: 0.13967970619786113]
	TIME [epoch: 8.33 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12990547958736326		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.12990547958736326 | validation: 0.15491311407327676]
	TIME [epoch: 8.32 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12453910849874576		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.12453910849874576 | validation: 0.13738223619893636]
	TIME [epoch: 8.33 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13133111764752975		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.13133111764752975 | validation: 0.16486055878666933]
	TIME [epoch: 8.35 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15244543866645657		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.15244543866645657 | validation: 0.20030310325729023]
	TIME [epoch: 8.32 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14274758145549227		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.14274758145549227 | validation: 0.13298655409119275]
	TIME [epoch: 8.33 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12698314464887855		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.12698314464887855 | validation: 0.12760665048644426]
	TIME [epoch: 8.32 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13703266144802012		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.13703266144802012 | validation: 0.13380361152361983]
	TIME [epoch: 8.32 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13142959929544668		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.13142959929544668 | validation: 0.14178501035204033]
	TIME [epoch: 8.34 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13151340987674548		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.13151340987674548 | validation: 0.1489332399632239]
	TIME [epoch: 8.32 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12432308075110099		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.12432308075110099 | validation: 0.14498995789853905]
	TIME [epoch: 8.32 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13160095488154583		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.13160095488154583 | validation: 0.13965417205247602]
	TIME [epoch: 8.32 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13039293922664322		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.13039293922664322 | validation: 0.13545525642479503]
	TIME [epoch: 8.35 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12116978838614849		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.12116978838614849 | validation: 0.1516677926501645]
	TIME [epoch: 8.33 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13682465984176576		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.13682465984176576 | validation: 0.15088759420559067]
	TIME [epoch: 8.32 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12637316410896146		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.12637316410896146 | validation: 0.15744460768019156]
	TIME [epoch: 8.33 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12711628759602062		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.12711628759602062 | validation: 0.1382699101023552]
	TIME [epoch: 8.35 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12345922820505076		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.12345922820505076 | validation: 0.1498272869006309]
	TIME [epoch: 8.33 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13436412155408253		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.13436412155408253 | validation: 0.1586516405631641]
	TIME [epoch: 8.32 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12499649184092065		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.12499649184092065 | validation: 0.1392691373742678]
	TIME [epoch: 8.32 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13313143231323663		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.13313143231323663 | validation: 0.13471473432545414]
	TIME [epoch: 8.33 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12932752082902804		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.12932752082902804 | validation: 0.13552465950525983]
	TIME [epoch: 8.35 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1228605135049057		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.1228605135049057 | validation: 0.1320960629160201]
	TIME [epoch: 8.32 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1271115043344005		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.1271115043344005 | validation: 0.12207388032791]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1273673723292908		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.1273673723292908 | validation: 0.13939184790204018]
	TIME [epoch: 8.32 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1271418626393906		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.1271418626393906 | validation: 0.12010228483134566]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1336.pth
	Model improved!!!
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12643314778021156		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.12643314778021156 | validation: 0.13403714972825367]
	TIME [epoch: 8.32 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12912761891976754		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.12912761891976754 | validation: 0.12758477059913834]
	TIME [epoch: 8.31 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12068437283498146		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.12068437283498146 | validation: 0.12864024639878197]
	TIME [epoch: 8.32 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12916745143205582		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.12916745143205582 | validation: 0.1435778683809431]
	TIME [epoch: 8.34 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12920615686357292		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.12920615686357292 | validation: 0.13694385372616472]
	TIME [epoch: 8.32 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1303669015252838		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.1303669015252838 | validation: 0.15573240887870549]
	TIME [epoch: 8.32 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13837486538427765		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.13837486538427765 | validation: 0.13882217383963685]
	TIME [epoch: 8.32 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12408425566727758		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.12408425566727758 | validation: 0.13712213937958234]
	TIME [epoch: 8.32 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12650928274593815		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.12650928274593815 | validation: 0.13693193053612213]
	TIME [epoch: 8.34 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13373380340647364		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.13373380340647364 | validation: 0.14155538642137372]
	TIME [epoch: 8.32 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12622581913888178		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.12622581913888178 | validation: 0.1456236738375119]
	TIME [epoch: 8.32 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12584853177915645		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.12584853177915645 | validation: 0.13485647162446207]
	TIME [epoch: 8.31 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13604107024170217		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.13604107024170217 | validation: 0.1615279545189327]
	TIME [epoch: 8.34 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13436083495334641		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.13436083495334641 | validation: 0.1387208537277218]
	TIME [epoch: 8.32 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13365429799852538		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.13365429799852538 | validation: 0.1720035801095138]
	TIME [epoch: 8.32 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13220172309715766		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.13220172309715766 | validation: 0.1321690723438951]
	TIME [epoch: 8.31 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1245352771761999		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.1245352771761999 | validation: 0.1509654151706692]
	TIME [epoch: 8.34 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13960244335927724		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.13960244335927724 | validation: 0.1490276941127828]
	TIME [epoch: 8.32 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13317498705933178		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.13317498705933178 | validation: 0.1432546687532773]
	TIME [epoch: 8.31 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1300065316074411		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.1300065316074411 | validation: 0.14383689947954686]
	TIME [epoch: 8.32 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1380716261306948		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.1380716261306948 | validation: 0.20271114155448094]
	TIME [epoch: 8.31 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14048757600440415		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.14048757600440415 | validation: 0.1397757965259265]
	TIME [epoch: 8.33 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12345896847195115		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.12345896847195115 | validation: 0.14065591688153062]
	TIME [epoch: 8.31 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13526765233485497		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.13526765233485497 | validation: 0.13756818245247504]
	TIME [epoch: 8.31 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1489438216255399		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.1489438216255399 | validation: 0.15017576184913983]
	TIME [epoch: 8.31 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1294995205954267		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.1294995205954267 | validation: 0.1358783661189818]
	TIME [epoch: 8.33 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13830215476849186		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.13830215476849186 | validation: 0.13750113614146312]
	TIME [epoch: 8.32 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1274515615299206		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.1274515615299206 | validation: 0.13965976814589742]
	TIME [epoch: 8.31 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1307545972710128		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.1307545972710128 | validation: 0.1286269077778194]
	TIME [epoch: 8.32 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12864852970629573		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.12864852970629573 | validation: 0.13146339912385951]
	TIME [epoch: 8.34 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12280245429798402		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.12280245429798402 | validation: 0.1377639324606328]
	TIME [epoch: 8.32 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1255577985243461		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.1255577985243461 | validation: 0.12669981058193508]
	TIME [epoch: 8.32 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12183912165157011		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.12183912165157011 | validation: 0.12768266265350975]
	TIME [epoch: 8.32 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12519609871664247		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.12519609871664247 | validation: 0.13316331232223186]
	TIME [epoch: 8.32 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11783738539651474		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.11783738539651474 | validation: 0.1399283523853568]
	TIME [epoch: 8.34 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12468097693971034		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.12468097693971034 | validation: 0.13609646157292976]
	TIME [epoch: 8.32 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1399566612089307		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.1399566612089307 | validation: 0.14360736534260893]
	TIME [epoch: 8.31 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12584621144716704		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.12584621144716704 | validation: 0.12385974094102903]
	TIME [epoch: 8.32 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1171247574871999		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.1171247574871999 | validation: 0.13957066492490333]
	TIME [epoch: 8.33 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1412723890515361		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.1412723890515361 | validation: 0.17987285415235316]
	TIME [epoch: 8.32 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14829967201415153		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.14829967201415153 | validation: 0.17370097403363993]
	TIME [epoch: 8.32 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15598137500243398		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.15598137500243398 | validation: 0.13576542963628033]
	TIME [epoch: 8.31 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12607734931518316		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.12607734931518316 | validation: 0.13812802314554307]
	TIME [epoch: 8.34 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12619195035427225		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.12619195035427225 | validation: 0.12488025333994163]
	TIME [epoch: 8.32 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12197279450600011		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.12197279450600011 | validation: 0.14241819731820154]
	TIME [epoch: 8.32 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14411090040829652		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.14411090040829652 | validation: 0.1894249849015704]
	TIME [epoch: 8.32 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1407719832324924		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.1407719832324924 | validation: 0.1358815213406566]
	TIME [epoch: 8.32 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13193266630894887		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.13193266630894887 | validation: 0.15168372260295088]
	TIME [epoch: 8.34 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13088334987695793		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.13088334987695793 | validation: 0.13823364908485425]
	TIME [epoch: 8.31 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12701883435152084		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.12701883435152084 | validation: 0.13232947757101787]
	TIME [epoch: 8.32 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1337515521808708		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.1337515521808708 | validation: 0.14127519462282934]
	TIME [epoch: 8.31 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12756567226842505		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.12756567226842505 | validation: 0.14843515141830813]
	TIME [epoch: 8.34 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13070804967622335		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.13070804967622335 | validation: 0.1447762832064585]
	TIME [epoch: 8.32 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1261694259202605		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.1261694259202605 | validation: 0.1507087310473652]
	TIME [epoch: 8.32 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13486630321647824		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.13486630321647824 | validation: 0.154846615419483]
	TIME [epoch: 8.32 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15009182134671664		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.15009182134671664 | validation: 0.15274873029039837]
	TIME [epoch: 8.34 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1286918335757376		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.1286918335757376 | validation: 0.14251571159758183]
	TIME [epoch: 8.32 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1323382872331455		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.1323382872331455 | validation: 0.17372795724160384]
	TIME [epoch: 8.32 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14176547185263652		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.14176547185263652 | validation: 0.13069864042952634]
	TIME [epoch: 8.32 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13194969456751354		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.13194969456751354 | validation: 0.14159400150176166]
	TIME [epoch: 8.32 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1262761137966714		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.1262761137966714 | validation: 0.13635613886410547]
	TIME [epoch: 8.33 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12283475303233113		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.12283475303233113 | validation: 0.13789952653419268]
	TIME [epoch: 8.32 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1219660395306523		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.1219660395306523 | validation: 0.14104687237753244]
	TIME [epoch: 8.32 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12669022479560832		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.12669022479560832 | validation: 0.13667459246519123]
	TIME [epoch: 8.31 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12468338231163043		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.12468338231163043 | validation: 0.14632015766724152]
	TIME [epoch: 8.34 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12915730501300313		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.12915730501300313 | validation: 0.1358021823628616]
	TIME [epoch: 8.32 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12565440806812594		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.12565440806812594 | validation: 0.13359018624279373]
	TIME [epoch: 8.32 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1260433269620192		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.1260433269620192 | validation: 0.14136875172043617]
	TIME [epoch: 8.32 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12717021834390546		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.12717021834390546 | validation: 0.1157748674294977]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1405.pth
	Model improved!!!
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11802910469942836		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.11802910469942836 | validation: 0.13469737694892583]
	TIME [epoch: 8.33 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1382579926624082		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.1382579926624082 | validation: 0.136723356923115]
	TIME [epoch: 8.32 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12309148597477079		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.12309148597477079 | validation: 0.14046581945057085]
	TIME [epoch: 8.31 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12322660659092384		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.12322660659092384 | validation: 0.14468880191346026]
	TIME [epoch: 8.32 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14019746225370602		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.14019746225370602 | validation: 0.16531627320565184]
	TIME [epoch: 8.33 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13085861312949706		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.13085861312949706 | validation: 0.15124000686739272]
	TIME [epoch: 8.31 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1309804160356455		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.1309804160356455 | validation: 0.13515694320928706]
	TIME [epoch: 8.32 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1295955927635807		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.1295955927635807 | validation: 0.13517526904483945]
	TIME [epoch: 8.31 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12207003761344407		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.12207003761344407 | validation: 0.13270724795573663]
	TIME [epoch: 8.34 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12810293503732964		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.12810293503732964 | validation: 0.1418529715961712]
	TIME [epoch: 8.32 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11999898584218431		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.11999898584218431 | validation: 0.12200910197510825]
	TIME [epoch: 8.32 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12423988094493785		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.12423988094493785 | validation: 0.14236041890989182]
	TIME [epoch: 8.32 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12214845043581231		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.12214845043581231 | validation: 0.1403155578878793]
	TIME [epoch: 8.34 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1317004869985415		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.1317004869985415 | validation: 0.13510696991740773]
	TIME [epoch: 8.32 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12556430290506954		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.12556430290506954 | validation: 0.14630916874771815]
	TIME [epoch: 8.32 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12680801732594574		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.12680801732594574 | validation: 0.1505019585160247]
	TIME [epoch: 8.32 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12969183983138052		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.12969183983138052 | validation: 0.16413037926157276]
	TIME [epoch: 8.32 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14065712557228324		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.14065712557228324 | validation: 0.15224379816806244]
	TIME [epoch: 8.34 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12026797247614018		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.12026797247614018 | validation: 0.13282361270255788]
	TIME [epoch: 8.32 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1231625668613456		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.1231625668613456 | validation: 0.12851252145627806]
	TIME [epoch: 8.32 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12256022258221622		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.12256022258221622 | validation: 0.12507053931734538]
	TIME [epoch: 8.32 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11866897133828472		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.11866897133828472 | validation: 0.12767060671017827]
	TIME [epoch: 8.34 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1284526281936036		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.1284526281936036 | validation: 0.12624884230119243]
	TIME [epoch: 8.32 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12990762085187413		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.12990762085187413 | validation: 0.1354390841808396]
	TIME [epoch: 8.32 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1257131579211484		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.1257131579211484 | validation: 0.13440641922740534]
	TIME [epoch: 8.31 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1249599964771857		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.1249599964771857 | validation: 0.16136983136558314]
	TIME [epoch: 8.34 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13346773513466365		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.13346773513466365 | validation: 0.13963898613747455]
	TIME [epoch: 8.32 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12041723764411416		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.12041723764411416 | validation: 0.13152051005229023]
	TIME [epoch: 8.31 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1428707302697231		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.1428707302697231 | validation: 0.14748871839166866]
	TIME [epoch: 8.32 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1421919657716287		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.1421919657716287 | validation: 0.133966783487319]
	TIME [epoch: 8.32 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12694373134382295		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.12694373134382295 | validation: 0.15261404938160214]
	TIME [epoch: 8.34 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13004732861485754		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.13004732861485754 | validation: 0.1273337904863306]
	TIME [epoch: 8.32 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12859888438965675		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.12859888438965675 | validation: 0.1375852099752994]
	TIME [epoch: 8.32 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12023686742484818		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.12023686742484818 | validation: 0.13176421134359365]
	TIME [epoch: 8.32 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12840849172810437		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.12840849172810437 | validation: 0.1339651040325754]
	TIME [epoch: 8.34 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12460440320179929		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.12460440320179929 | validation: 0.1257512171429842]
	TIME [epoch: 8.32 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12318510421737885		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.12318510421737885 | validation: 0.1344565633865342]
	TIME [epoch: 8.31 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12906552019236261		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.12906552019236261 | validation: 0.13100073477948024]
	TIME [epoch: 8.32 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12253411923564952		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.12253411923564952 | validation: 0.13832568097392356]
	TIME [epoch: 8.33 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12083153144736695		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.12083153144736695 | validation: 0.14073121710984937]
	TIME [epoch: 8.31 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13065888123451636		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.13065888123451636 | validation: 0.15628582213509268]
	TIME [epoch: 8.32 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12794866743576777		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.12794866743576777 | validation: 0.13767698575165183]
	TIME [epoch: 8.31 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13230214699952636		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.13230214699952636 | validation: 0.1421017768950549]
	TIME [epoch: 8.32 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11846278060150853		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.11846278060150853 | validation: 0.14122512515895594]
	TIME [epoch: 8.33 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12391586819715279		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.12391586819715279 | validation: 0.13982058458495666]
	TIME [epoch: 8.31 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12484604372606459		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.12484604372606459 | validation: 0.12449458783682617]
	TIME [epoch: 8.31 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12039258880335432		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.12039258880335432 | validation: 0.13482624772032076]
	TIME [epoch: 8.31 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12709330434485328		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.12709330434485328 | validation: 0.13518986665287203]
	TIME [epoch: 8.34 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12486709195090658		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.12486709195090658 | validation: 0.1417701936167205]
	TIME [epoch: 8.31 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11768856166839166		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.11768856166839166 | validation: 0.1421652708969499]
	TIME [epoch: 8.31 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12180092504767163		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.12180092504767163 | validation: 0.12830258203056027]
	TIME [epoch: 8.31 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14301412910931566		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.14301412910931566 | validation: 0.14615965822321314]
	TIME [epoch: 8.34 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.139239672584552		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.139239672584552 | validation: 0.13722767142514375]
	TIME [epoch: 8.32 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12577564101748318		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.12577564101748318 | validation: 0.13392340229449032]
	TIME [epoch: 8.32 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12959290733120973		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.12959290733120973 | validation: 0.14524619734852756]
	TIME [epoch: 8.32 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1328768691796063		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.1328768691796063 | validation: 0.14315884695334663]
	TIME [epoch: 8.32 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1348887361102634		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.1348887361102634 | validation: 0.13862857173359394]
	TIME [epoch: 8.34 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13045597226108036		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.13045597226108036 | validation: 0.13205997813701004]
	TIME [epoch: 8.32 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13072391483155668		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.13072391483155668 | validation: 0.14321192199025706]
	TIME [epoch: 8.32 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1296356520975465		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.1296356520975465 | validation: 0.14636511614055062]
	TIME [epoch: 8.32 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1286906890899801		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.1286906890899801 | validation: 0.12733659029618538]
	TIME [epoch: 8.33 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13377328541739456		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.13377328541739456 | validation: 0.13545806792651616]
	TIME [epoch: 8.32 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12395870414936912		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.12395870414936912 | validation: 0.12562489778129482]
	TIME [epoch: 8.32 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12207470774173183		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.12207470774173183 | validation: 0.11210955828207066]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1469.pth
	Model improved!!!
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12069454459361965		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.12069454459361965 | validation: 0.13568418147708056]
	TIME [epoch: 8.34 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11797303899633323		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.11797303899633323 | validation: 0.13141723392870108]
	TIME [epoch: 8.32 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12254262464387171		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.12254262464387171 | validation: 0.1408425038075019]
	TIME [epoch: 8.32 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1237145425045173		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.1237145425045173 | validation: 0.14039850295331263]
	TIME [epoch: 8.32 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13622451316374956		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.13622451316374956 | validation: 0.1315884341469329]
	TIME [epoch: 8.33 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12487356130565852		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.12487356130565852 | validation: 0.12368866674385573]
	TIME [epoch: 8.34 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12125126007166878		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.12125126007166878 | validation: 0.1251832037677151]
	TIME [epoch: 8.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12547372215764313		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.12547372215764313 | validation: 0.12930259175023176]
	TIME [epoch: 8.32 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12051967784588007		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.12051967784588007 | validation: 0.12694251435984394]
	TIME [epoch: 8.32 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13057919355338013		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.13057919355338013 | validation: 0.12100214187250566]
	TIME [epoch: 8.34 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12099391274363658		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.12099391274363658 | validation: 0.15473359759791216]
	TIME [epoch: 8.32 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12664127517131912		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.12664127517131912 | validation: 0.13722122719504654]
	TIME [epoch: 8.31 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13012350046515614		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.13012350046515614 | validation: 0.12197231436589845]
	TIME [epoch: 8.32 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12406684418958303		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.12406684418958303 | validation: 0.1386628641321279]
	TIME [epoch: 8.33 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12812529233081987		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.12812529233081987 | validation: 0.1355838022128439]
	TIME [epoch: 8.32 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1320153499683549		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.1320153499683549 | validation: 0.16025212766698116]
	TIME [epoch: 8.32 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12666448246437859		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.12666448246437859 | validation: 0.1325178589809366]
	TIME [epoch: 8.32 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12552116257296214		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.12552116257296214 | validation: 0.13750578759217816]
	TIME [epoch: 8.32 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12461987406571513		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.12461987406571513 | validation: 0.12434516372664249]
	TIME [epoch: 8.34 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12376477319806203		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.12376477319806203 | validation: 0.13597863569945345]
	TIME [epoch: 8.32 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12145127358639995		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.12145127358639995 | validation: 0.13265290960665968]
	TIME [epoch: 8.31 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12784172667248186		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.12784172667248186 | validation: 0.1291320583279561]
	TIME [epoch: 8.32 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12376357786087673		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.12376357786087673 | validation: 0.11901148812273592]
	TIME [epoch: 8.34 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12909529839272543		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.12909529839272543 | validation: 0.1323036223688593]
	TIME [epoch: 8.32 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12520605467482937		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.12520605467482937 | validation: 0.1327520550987127]
	TIME [epoch: 8.32 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12038076543764673		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.12038076543764673 | validation: 0.1348496036187043]
	TIME [epoch: 8.32 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1248002541804524		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.1248002541804524 | validation: 0.11934964319984384]
	TIME [epoch: 8.34 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12584729832963656		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.12584729832963656 | validation: 0.143897838243355]
	TIME [epoch: 8.32 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1345332994115141		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.1345332994115141 | validation: 0.15417191766438115]
	TIME [epoch: 8.32 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14186157025511442		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.14186157025511442 | validation: 0.1818192713296049]
	TIME [epoch: 8.32 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12944562684665034		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.12944562684665034 | validation: 0.1362969429074303]
	TIME [epoch: 8.32 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12171362705515876		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.12171362705515876 | validation: 0.13236928545656906]
	TIME [epoch: 8.34 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12672780227646913		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.12672780227646913 | validation: 0.1337054449197915]
	TIME [epoch: 8.32 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1220247185968748		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.1220247185968748 | validation: 0.14001935151456205]
	TIME [epoch: 8.32 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12505522152810575		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.12505522152810575 | validation: 0.13151524460583622]
	TIME [epoch: 8.32 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1255304157132902		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.1255304157132902 | validation: 0.11981240619387581]
	TIME [epoch: 8.34 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11908273587405512		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.11908273587405512 | validation: 0.12959590104473082]
	TIME [epoch: 8.33 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11675731134164169		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.11675731134164169 | validation: 0.12850380423138832]
	TIME [epoch: 8.32 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12046192121583008		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.12046192121583008 | validation: 0.13148077616756215]
	TIME [epoch: 8.32 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1258176747546498		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.1258176747546498 | validation: 0.11795791893025107]
	TIME [epoch: 8.34 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12228906916317395		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.12228906916317395 | validation: 0.13391625173851268]
	TIME [epoch: 8.32 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12250123513867761		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.12250123513867761 | validation: 0.1339363888459807]
	TIME [epoch: 8.32 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12845775995583797		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.12845775995583797 | validation: 0.1260819078090259]
	TIME [epoch: 8.32 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1201015626886488		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.1201015626886488 | validation: 0.12336898532670268]
	TIME [epoch: 8.33 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12259350534526839		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.12259350534526839 | validation: 0.11908389785909056]
	TIME [epoch: 8.34 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12579590623191933		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.12579590623191933 | validation: 0.1335666044117294]
	TIME [epoch: 8.32 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1229855815164969		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.1229855815164969 | validation: 0.13657293324509387]
	TIME [epoch: 8.32 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12295718504066307		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.12295718504066307 | validation: 0.13013164262354407]
	TIME [epoch: 8.32 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12051606409096653		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.12051606409096653 | validation: 0.13129638017006495]
	TIME [epoch: 8.34 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12007924621780677		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.12007924621780677 | validation: 0.11913058890317249]
	TIME [epoch: 8.33 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12343122472423551		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.12343122472423551 | validation: 0.1332353081804168]
	TIME [epoch: 8.32 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12428101570986456		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.12428101570986456 | validation: 0.1333573699578526]
	TIME [epoch: 8.32 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12281851083062452		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.12281851083062452 | validation: 0.13327643219843124]
	TIME [epoch: 8.35 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13816357519348607		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.13816357519348607 | validation: 0.1631707384298757]
	TIME [epoch: 8.32 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12791260426540507		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.12791260426540507 | validation: 0.1365754431605083]
	TIME [epoch: 8.32 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1251583607124191		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.1251583607124191 | validation: 0.12058325734802544]
	TIME [epoch: 8.32 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11842591605314486		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.11842591605314486 | validation: 0.13648652146811488]
	TIME [epoch: 8.32 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13090043126792186		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.13090043126792186 | validation: 0.1421981682016374]
	TIME [epoch: 8.34 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13083974279770333		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.13083974279770333 | validation: 0.1428281563050262]
	TIME [epoch: 8.32 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13286150574488623		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.13286150574488623 | validation: 0.12868648776350916]
	TIME [epoch: 8.32 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14129552682083724		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.14129552682083724 | validation: 0.14579166069002947]
	TIME [epoch: 8.32 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13865025623469293		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.13865025623469293 | validation: 0.14887622580680304]
	TIME [epoch: 8.34 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12987560329348918		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.12987560329348918 | validation: 0.14709451828961725]
	TIME [epoch: 8.32 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12477231306340755		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.12477231306340755 | validation: 0.139635579747597]
	TIME [epoch: 8.32 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13019992946729264		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.13019992946729264 | validation: 0.14413804624521087]
	TIME [epoch: 8.32 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12734444243512527		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.12734444243512527 | validation: 0.13517249962792482]
	TIME [epoch: 8.34 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13584070335775694		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.13584070335775694 | validation: 0.1798511059799796]
	TIME [epoch: 8.32 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1744439730903436		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.1744439730903436 | validation: 0.14329681895949445]
	TIME [epoch: 8.32 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.147807069217208		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.147807069217208 | validation: 0.14231604447730387]
	TIME [epoch: 8.32 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1315290021701814		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.1315290021701814 | validation: 0.14267289975740688]
	TIME [epoch: 8.33 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1284273375716229		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.1284273375716229 | validation: 0.1268486336679402]
	TIME [epoch: 8.34 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1270283060815212		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.1270283060815212 | validation: 0.13268984818729976]
	TIME [epoch: 8.32 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12753757686354433		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.12753757686354433 | validation: 0.13483262441361663]
	TIME [epoch: 8.32 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12731775042170876		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.12731775042170876 | validation: 0.1326058023822524]
	TIME [epoch: 8.32 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1278502793866694		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.1278502793866694 | validation: 0.1275435025129749]
	TIME [epoch: 8.34 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12095314566263612		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.12095314566263612 | validation: 0.11924833182493724]
	TIME [epoch: 8.32 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13328289362998583		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.13328289362998583 | validation: 0.12618567426313027]
	TIME [epoch: 8.32 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1296067195537714		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.1296067195537714 | validation: 0.12390916507846061]
	TIME [epoch: 8.32 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12489385599117767		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.12489385599117767 | validation: 0.11825632066285632]
	TIME [epoch: 8.35 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1378301469810103		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.1378301469810103 | validation: 0.1286119083089816]
	TIME [epoch: 8.32 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12649552911874723		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.12649552911874723 | validation: 0.13430344565373187]
	TIME [epoch: 8.32 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12791116033739197		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.12791116033739197 | validation: 0.12513347530612706]
	TIME [epoch: 8.32 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12710481418254238		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.12710481418254238 | validation: 0.1364319707871277]
	TIME [epoch: 8.32 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12288959415504637		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.12288959415504637 | validation: 0.12687896476627303]
	TIME [epoch: 8.34 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13398019027938926		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.13398019027938926 | validation: 0.1503966572230429]
	TIME [epoch: 8.32 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12691650527883386		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.12691650527883386 | validation: 0.1270992597794247]
	TIME [epoch: 8.32 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11794075027806687		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.11794075027806687 | validation: 0.13320878913369616]
	TIME [epoch: 8.32 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12858063312836293		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.12858063312836293 | validation: 0.1419843559018183]
	TIME [epoch: 8.35 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1288512884598068		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.1288512884598068 | validation: 0.1272776970295122]
	TIME [epoch: 8.32 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13021857601370526		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.13021857601370526 | validation: 0.1307891629606891]
	TIME [epoch: 8.33 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12414513000870075		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.12414513000870075 | validation: 0.1296744147927739]
	TIME [epoch: 8.32 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1273058572347086		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.1273058572347086 | validation: 0.13503960960179795]
	TIME [epoch: 8.34 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13169100024692165		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.13169100024692165 | validation: 0.13852247423958247]
	TIME [epoch: 8.32 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1323627149992305		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.1323627149992305 | validation: 0.14038171627390367]
	TIME [epoch: 8.32 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.132788138045693		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.132788138045693 | validation: 0.1411129261831306]
	TIME [epoch: 8.32 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12385604474579695		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.12385604474579695 | validation: 0.13610709546632824]
	TIME [epoch: 8.32 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14016571557465854		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.14016571557465854 | validation: 0.1400265804071241]
	TIME [epoch: 8.34 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13210438681849435		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.13210438681849435 | validation: 0.1427664002458081]
	TIME [epoch: 8.32 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13001050573027118		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.13001050573027118 | validation: 0.13509960921699393]
	TIME [epoch: 8.32 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13683393134144117		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.13683393134144117 | validation: 0.12769370090013096]
	TIME [epoch: 8.32 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1306170295652909		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.1306170295652909 | validation: 0.13835047846840476]
	TIME [epoch: 8.34 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1282853953482289		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.1282853953482289 | validation: 0.12895708046868112]
	TIME [epoch: 8.32 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13420918338582394		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.13420918338582394 | validation: 0.15008780992111925]
	TIME [epoch: 8.32 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.127325153035344		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.127325153035344 | validation: 0.14020585215717665]
	TIME [epoch: 8.32 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1272911472106871		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.1272911472106871 | validation: 0.14007037352804386]
	TIME [epoch: 8.34 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.134507964200461		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.134507964200461 | validation: 0.13702731681964303]
	TIME [epoch: 8.32 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13137119515609533		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.13137119515609533 | validation: 0.1290359418875182]
	TIME [epoch: 8.32 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12577461065084522		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.12577461065084522 | validation: 0.14082037245906448]
	TIME [epoch: 8.32 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12287107870982876		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.12287107870982876 | validation: 0.13555830537720095]
	TIME [epoch: 8.33 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12358007910256508		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.12358007910256508 | validation: 0.1292002935699787]
	TIME [epoch: 8.35 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11732940693048008		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.11732940693048008 | validation: 0.13492045686568546]
	TIME [epoch: 8.32 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12807601888974177		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.12807601888974177 | validation: 0.13209853189660728]
	TIME [epoch: 8.32 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12290676775450078		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.12290676775450078 | validation: 0.1319799488734964]
	TIME [epoch: 8.32 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12198835815577577		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.12198835815577577 | validation: 0.12047312766320262]
	TIME [epoch: 8.34 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11931665573250197		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.11931665573250197 | validation: 0.13303719115098847]
	TIME [epoch: 8.33 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13111513372483086		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.13111513372483086 | validation: 0.12078570002008618]
	TIME [epoch: 8.32 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11733654162124744		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.11733654162124744 | validation: 0.14711284862232088]
	TIME [epoch: 8.32 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12112802021312027		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.12112802021312027 | validation: 0.11886336451150328]
	TIME [epoch: 8.35 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11998385657885739		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.11998385657885739 | validation: 0.11659022977955476]
	TIME [epoch: 8.33 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1253829961903915		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.1253829961903915 | validation: 0.13438198374734595]
	TIME [epoch: 8.33 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12156941896963455		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.12156941896963455 | validation: 0.13652732059475023]
	TIME [epoch: 8.32 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12572457707235069		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.12572457707235069 | validation: 0.12274803406705768]
	TIME [epoch: 8.33 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13820756158552325		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.13820756158552325 | validation: 0.12655750519387343]
	TIME [epoch: 8.34 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12065989239904348		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.12065989239904348 | validation: 0.12230279597338642]
	TIME [epoch: 8.32 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1219890063817867		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.1219890063817867 | validation: 0.12170547251646337]
	TIME [epoch: 8.32 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11599368048605889		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.11599368048605889 | validation: 0.12175408282454747]
	TIME [epoch: 8.32 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1217888305721014		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.1217888305721014 | validation: 0.11957314869629465]
	TIME [epoch: 8.34 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11698345893290549		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.11698345893290549 | validation: 0.11970182517451193]
	TIME [epoch: 8.33 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1264105543048546		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.1264105543048546 | validation: 0.1304694461737006]
	TIME [epoch: 8.32 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12049984927954202		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.12049984927954202 | validation: 0.1304350647521847]
	TIME [epoch: 8.33 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12060222231537807		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.12060222231537807 | validation: 0.1324970893795818]
	TIME [epoch: 8.35 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12489370472284671		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.12489370472284671 | validation: 0.13355249592315743]
	TIME [epoch: 8.32 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13375261993603546		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.13375261993603546 | validation: 0.1308149159633899]
	TIME [epoch: 8.32 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1160306003607686		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.1160306003607686 | validation: 0.14285202144702608]
	TIME [epoch: 8.32 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12425754531789188		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.12425754531789188 | validation: 0.1368004312821598]
	TIME [epoch: 8.33 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11830201977261086		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.11830201977261086 | validation: 0.12571723126582254]
	TIME [epoch: 8.34 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12060123524319499		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.12060123524319499 | validation: 0.12137367923043964]
	TIME [epoch: 8.32 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1193488751839444		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.1193488751839444 | validation: 0.13143277589783087]
	TIME [epoch: 8.33 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12480842052825931		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.12480842052825931 | validation: 0.12667513503710995]
	TIME [epoch: 8.32 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11714580314258409		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.11714580314258409 | validation: 0.123733142641421]
	TIME [epoch: 8.34 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11992867955601114		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.11992867955601114 | validation: 0.12525753860211122]
	TIME [epoch: 8.33 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11986254734355557		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.11986254734355557 | validation: 0.12455645972723198]
	TIME [epoch: 8.32 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11881085557322948		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.11881085557322948 | validation: 0.12425366191876813]
	TIME [epoch: 8.32 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11584327951961808		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.11584327951961808 | validation: 0.1246419285096077]
	TIME [epoch: 8.34 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11716522824207007		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.11716522824207007 | validation: 0.13263305426276037]
	TIME [epoch: 8.32 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12023250391917772		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.12023250391917772 | validation: 0.11805835032094647]
	TIME [epoch: 8.32 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11846961583635833		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.11846961583635833 | validation: 0.11941907181242414]
	TIME [epoch: 8.32 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11549106879542322		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.11549106879542322 | validation: 0.1227686760550753]
	TIME [epoch: 8.32 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12043826312566135		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.12043826312566135 | validation: 0.12625757930146606]
	TIME [epoch: 8.34 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1152471423437603		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.1152471423437603 | validation: 0.1293815574397853]
	TIME [epoch: 8.32 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1174902502717596		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.1174902502717596 | validation: 0.13038999154096714]
	TIME [epoch: 8.32 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11312478503853556		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.11312478503853556 | validation: 0.12898695039687286]
	TIME [epoch: 8.32 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12236235342524131		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.12236235342524131 | validation: 0.1300157404676967]
	TIME [epoch: 8.35 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11445758221797857		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.11445758221797857 | validation: 0.12145503169485478]
	TIME [epoch: 8.33 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13049123283935		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.13049123283935 | validation: 0.12113809532288408]
	TIME [epoch: 8.32 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11706559502597731		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.11706559502597731 | validation: 0.12410364582085198]
	TIME [epoch: 8.33 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11392627719937232		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.11392627719937232 | validation: 0.1294448910584972]
	TIME [epoch: 8.34 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11339916455520486		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.11339916455520486 | validation: 0.13164790396845083]
	TIME [epoch: 8.33 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11366193393122237		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.11366193393122237 | validation: 0.12422225039887494]
	TIME [epoch: 8.32 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11562116501913784		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.11562116501913784 | validation: 0.12405419278050461]
	TIME [epoch: 8.33 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1186601409440053		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.1186601409440053 | validation: 0.1317780397619433]
	TIME [epoch: 8.32 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1199240200324776		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.1199240200324776 | validation: 0.13311936370659713]
	TIME [epoch: 8.34 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12004851005851831		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.12004851005851831 | validation: 0.1290402919772445]
	TIME [epoch: 8.32 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11614797190718681		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.11614797190718681 | validation: 0.12867634101927147]
	TIME [epoch: 8.32 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11411944957593226		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.11411944957593226 | validation: 0.12294288529010189]
	TIME [epoch: 8.32 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12121405356368875		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.12121405356368875 | validation: 0.12790022196283507]
	TIME [epoch: 8.34 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1196927064493289		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.1196927064493289 | validation: 0.12119770033470294]
	TIME [epoch: 8.33 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11439245922283214		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.11439245922283214 | validation: 0.11677030094456103]
	TIME [epoch: 8.32 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1194856010925885		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.1194856010925885 | validation: 0.1145695193894547]
	TIME [epoch: 8.32 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11322550631512318		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.11322550631512318 | validation: 0.12233344300979776]
	TIME [epoch: 8.34 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11530103098626057		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.11530103098626057 | validation: 0.1258864848484298]
	TIME [epoch: 8.33 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1145513793645511		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.1145513793645511 | validation: 0.1192504494659923]
	TIME [epoch: 8.32 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11828934187096414		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.11828934187096414 | validation: 0.1309711046487921]
	TIME [epoch: 8.32 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12151603944110859		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.12151603944110859 | validation: 0.13437349161154014]
	TIME [epoch: 8.33 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12239800988530333		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.12239800988530333 | validation: 0.1255672770954274]
	TIME [epoch: 8.34 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11597045769082595		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.11597045769082595 | validation: 0.12395433863712366]
	TIME [epoch: 8.33 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1233992563665495		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.1233992563665495 | validation: 0.12281728100953604]
	TIME [epoch: 8.33 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11967107583967992		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.11967107583967992 | validation: 0.13388093769757572]
	TIME [epoch: 8.32 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11784334478364658		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.11784334478364658 | validation: 0.12948139919568025]
	TIME [epoch: 8.34 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11460781605054841		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.11460781605054841 | validation: 0.12269604661098893]
	TIME [epoch: 8.33 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1166437422006571		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.1166437422006571 | validation: 0.12242634769658803]
	TIME [epoch: 8.32 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11600402961174368		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.11600402961174368 | validation: 0.13243762036383863]
	TIME [epoch: 8.33 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11761589289712211		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.11761589289712211 | validation: 0.12865501646471056]
	TIME [epoch: 8.34 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11939828706452178		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.11939828706452178 | validation: 0.12999835567130497]
	TIME [epoch: 8.33 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11810553239752093		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.11810553239752093 | validation: 0.1299593801654927]
	TIME [epoch: 8.32 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11683487101825007		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.11683487101825007 | validation: 0.11842531195453997]
	TIME [epoch: 8.32 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11795295451878891		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.11795295451878891 | validation: 0.1216272795054688]
	TIME [epoch: 8.33 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11844402229343234		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.11844402229343234 | validation: 0.11918385367746559]
	TIME [epoch: 8.34 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12079013117550727		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.12079013117550727 | validation: 0.12923842907039879]
	TIME [epoch: 8.33 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12462899941205667		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.12462899941205667 | validation: 0.12640708225026964]
	TIME [epoch: 8.33 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1202545302822912		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.1202545302822912 | validation: 0.1276753710792186]
	TIME [epoch: 8.32 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11822342170635683		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.11822342170635683 | validation: 0.12826607707685042]
	TIME [epoch: 8.34 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11948379161433079		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.11948379161433079 | validation: 0.1228794720657495]
	TIME [epoch: 8.32 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1201152135604638		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.1201152135604638 | validation: 0.12227146122519253]
	TIME [epoch: 8.33 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11703077155600405		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.11703077155600405 | validation: 0.13373925348826704]
	TIME [epoch: 8.32 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12661376941305008		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.12661376941305008 | validation: 0.14066846068102917]
	TIME [epoch: 8.35 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1256416212531364		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.1256416212531364 | validation: 0.15609882467823954]
	TIME [epoch: 8.33 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12723487445715403		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.12723487445715403 | validation: 0.14571822495864367]
	TIME [epoch: 8.32 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12080959722906541		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.12080959722906541 | validation: 0.14329035792665706]
	TIME [epoch: 8.33 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12381184458188958		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.12381184458188958 | validation: 0.13485951952195996]
	TIME [epoch: 8.33 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12302972100117823		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.12302972100117823 | validation: 0.1355239577607117]
	TIME [epoch: 8.35 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11597148888476623		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.11597148888476623 | validation: 0.14654463397798012]
	TIME [epoch: 8.32 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1269849638926504		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.1269849638926504 | validation: 0.15163399678487524]
	TIME [epoch: 8.32 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1328843931828913		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.1328843931828913 | validation: 0.1628154775448633]
	TIME [epoch: 8.32 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13967702876514485		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.13967702876514485 | validation: 0.14277610547132702]
	TIME [epoch: 8.34 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12725958415884459		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.12725958415884459 | validation: 0.13113793421943376]
	TIME [epoch: 8.33 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12184698993767704		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.12184698993767704 | validation: 0.14711024176572923]
	TIME [epoch: 8.33 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12927832016980773		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.12927832016980773 | validation: 0.14346451890240336]
	TIME [epoch: 8.32 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13124157678682685		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.13124157678682685 | validation: 0.143896490878327]
	TIME [epoch: 8.34 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13243997125451237		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.13243997125451237 | validation: 0.14264237420427733]
	TIME [epoch: 8.32 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1219174880795603		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.1219174880795603 | validation: 0.1385350588407945]
	TIME [epoch: 8.32 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12544283466348377		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.12544283466348377 | validation: 0.1346980736893905]
	TIME [epoch: 8.32 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1276852072820362		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.1276852072820362 | validation: 0.13134887970144082]
	TIME [epoch: 8.32 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12270877933565807		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.12270877933565807 | validation: 0.14365782402283978]
	TIME [epoch: 8.34 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.119990193537089		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.119990193537089 | validation: 0.12533154812823605]
	TIME [epoch: 8.33 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11674183353420302		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.11674183353420302 | validation: 0.12679296338781726]
	TIME [epoch: 8.33 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12008361762680111		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.12008361762680111 | validation: 0.1251460543356983]
	TIME [epoch: 8.32 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11769192888374354		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.11769192888374354 | validation: 0.12544230991572586]
	TIME [epoch: 8.34 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12134343562761088		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.12134343562761088 | validation: 0.13081980145460387]
	TIME [epoch: 8.33 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12151202075268683		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.12151202075268683 | validation: 0.12848707257222786]
	TIME [epoch: 8.32 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12219585723949189		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.12219585723949189 | validation: 0.13692992890358954]
	TIME [epoch: 8.32 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12666536325828767		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.12666536325828767 | validation: 0.130802001770454]
	TIME [epoch: 8.34 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1224330268750411		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.1224330268750411 | validation: 0.12414932197255606]
	TIME [epoch: 8.33 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11826955350557511		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.11826955350557511 | validation: 0.1288547650136465]
	TIME [epoch: 8.33 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11708435605712922		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.11708435605712922 | validation: 0.13247979383456757]
	TIME [epoch: 8.32 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11772599529417598		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.11772599529417598 | validation: 0.1240207462307478]
	TIME [epoch: 8.33 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11882762992066395		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.11882762992066395 | validation: 0.121847669888801]
	TIME [epoch: 8.34 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11545892699743605		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.11545892699743605 | validation: 0.12346937735615587]
	TIME [epoch: 8.32 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12184778474358024		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.12184778474358024 | validation: 0.13502614614013425]
	TIME [epoch: 8.33 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11473136807673463		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.11473136807673463 | validation: 0.12901974089346968]
	TIME [epoch: 8.32 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1193382115707432		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.1193382115707432 | validation: 0.12565836856097115]
	TIME [epoch: 8.34 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11641891053546978		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.11641891053546978 | validation: 0.13298472576867076]
	TIME [epoch: 8.33 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12107229577196191		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.12107229577196191 | validation: 0.11825723324404606]
	TIME [epoch: 8.32 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12089758291685437		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.12089758291685437 | validation: 0.1213398875688383]
	TIME [epoch: 8.32 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11703745215532838		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.11703745215532838 | validation: 0.11902377079460189]
	TIME [epoch: 8.35 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11907137612995915		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.11907137612995915 | validation: 0.132417196898049]
	TIME [epoch: 8.32 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11733136449974482		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.11733136449974482 | validation: 0.12801660677951754]
	TIME [epoch: 8.33 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11816378575749728		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.11816378575749728 | validation: 0.1354319419315418]
	TIME [epoch: 8.32 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12379043911042471		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.12379043911042471 | validation: 0.12739243806950223]
	TIME [epoch: 8.33 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12127952174810988		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.12127952174810988 | validation: 0.11746256497881072]
	TIME [epoch: 8.34 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12133372769915471		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.12133372769915471 | validation: 0.13029905263424718]
	TIME [epoch: 8.32 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11801531013051983		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.11801531013051983 | validation: 0.13197514540994837]
	TIME [epoch: 8.32 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11995795715085118		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.11995795715085118 | validation: 0.12960325755268998]
	TIME [epoch: 8.32 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11737576611220257		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.11737576611220257 | validation: 0.1347165092418343]
	TIME [epoch: 8.34 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12115780553019657		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.12115780553019657 | validation: 0.11578323865019158]
	TIME [epoch: 8.32 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11839323620080233		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.11839323620080233 | validation: 0.11808251016431198]
	TIME [epoch: 8.32 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1202871362201754		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.1202871362201754 | validation: 0.11902615806681109]
	TIME [epoch: 8.32 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12017282171667046		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.12017282171667046 | validation: 0.12113259768867313]
	TIME [epoch: 8.34 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1161154462544911		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.1161154462544911 | validation: 0.126313473747965]
	TIME [epoch: 8.32 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12080438662794483		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.12080438662794483 | validation: 0.1206952848581535]
	TIME [epoch: 8.32 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1236514378066796		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.1236514378066796 | validation: 0.1323468251591553]
	TIME [epoch: 8.32 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11472501207750434		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.11472501207750434 | validation: 0.13251741337241538]
	TIME [epoch: 8.32 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11794707746071409		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.11794707746071409 | validation: 0.1138860628761782]
	TIME [epoch: 8.35 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11983722910408526		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.11983722910408526 | validation: 0.12682837142996628]
	TIME [epoch: 8.32 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1181337727899304		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.1181337727899304 | validation: 0.13129719407234533]
	TIME [epoch: 8.33 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11849512497860962		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.11849512497860962 | validation: 0.12222849139860734]
	TIME [epoch: 8.32 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11600729465478783		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.11600729465478783 | validation: 0.12320453115015025]
	TIME [epoch: 8.34 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11719477162811447		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.11719477162811447 | validation: 0.12660729582222466]
	TIME [epoch: 8.32 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11661079672282819		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.11661079672282819 | validation: 0.1223799448763121]
	TIME [epoch: 8.32 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11774385157120704		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.11774385157120704 | validation: 0.12539229974326463]
	TIME [epoch: 8.32 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11895610014498433		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.11895610014498433 | validation: 0.11747655991104997]
	TIME [epoch: 8.34 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11795048425196979		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.11795048425196979 | validation: 0.12251206757018385]
	TIME [epoch: 8.32 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1140804555070057		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.1140804555070057 | validation: 0.12574894833715852]
	TIME [epoch: 8.32 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11697477107197977		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.11697477107197977 | validation: 0.1332670350606756]
	TIME [epoch: 8.32 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11827984575263957		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.11827984575263957 | validation: 0.12291258595891787]
	TIME [epoch: 8.32 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11367369200783342		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.11367369200783342 | validation: 0.12178503224501769]
	TIME [epoch: 8.34 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11433484924025672		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.11433484924025672 | validation: 0.11674096066624884]
	TIME [epoch: 8.32 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11326837843982596		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.11326837843982596 | validation: 0.12453327782566978]
	TIME [epoch: 8.32 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11636302917411667		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.11636302917411667 | validation: 0.1251718880114934]
	TIME [epoch: 8.32 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11829874412282511		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.11829874412282511 | validation: 0.11117187047481455]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1739.pth
	Model improved!!!
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1215093762428725		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.1215093762428725 | validation: 0.1206327721682899]
	TIME [epoch: 8.31 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11825319395946035		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.11825319395946035 | validation: 0.1262821100294308]
	TIME [epoch: 8.32 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11274246851063227		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.11274246851063227 | validation: 0.12460456905213202]
	TIME [epoch: 8.31 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11703901988234548		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.11703901988234548 | validation: 0.11829742547726918]
	TIME [epoch: 8.34 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1176922723643649		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.1176922723643649 | validation: 0.12133346689964608]
	TIME [epoch: 8.32 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11885124971069441		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.11885124971069441 | validation: 0.11790650771507757]
	TIME [epoch: 8.32 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11944143676285526		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.11944143676285526 | validation: 0.12362085550397192]
	TIME [epoch: 8.32 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11810330407730607		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.11810330407730607 | validation: 0.1247153838841511]
	TIME [epoch: 8.32 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11604566931032126		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.11604566931032126 | validation: 0.1312437215922275]
	TIME [epoch: 8.33 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11690863940863008		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.11690863940863008 | validation: 0.12125031184608831]
	TIME [epoch: 8.31 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12375562655386185		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.12375562655386185 | validation: 0.12924518018998446]
	TIME [epoch: 8.32 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1184457596015934		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.1184457596015934 | validation: 0.11794959415529743]
	TIME [epoch: 8.32 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11722552812946416		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.11722552812946416 | validation: 0.13761849791106975]
	TIME [epoch: 8.34 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12218711423484815		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.12218711423484815 | validation: 0.11813583659874094]
	TIME [epoch: 8.32 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1167440163292913		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.1167440163292913 | validation: 0.12201811178705181]
	TIME [epoch: 8.32 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12435109584038914		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.12435109584038914 | validation: 0.13296523594967874]
	TIME [epoch: 8.32 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12007203976512484		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.12007203976512484 | validation: 0.1271843050966394]
	TIME [epoch: 8.34 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12342672651419206		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.12342672651419206 | validation: 0.1423748981112407]
	TIME [epoch: 8.31 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1219095128660433		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.1219095128660433 | validation: 0.11769662577710803]
	TIME [epoch: 8.32 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12439491658631552		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.12439491658631552 | validation: 0.12446674935362931]
	TIME [epoch: 8.32 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12456077011384632		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.12456077011384632 | validation: 0.134459793310367]
	TIME [epoch: 8.32 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11816372902040662		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.11816372902040662 | validation: 0.12850300069500412]
	TIME [epoch: 8.34 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12242730297501585		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.12242730297501585 | validation: 0.13021048236725205]
	TIME [epoch: 8.31 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11931326141319767		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.11931326141319767 | validation: 0.12659668892702441]
	TIME [epoch: 8.32 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11841212920148128		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.11841212920148128 | validation: 0.12298366187474703]
	TIME [epoch: 8.32 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11491523046282884		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.11491523046282884 | validation: 0.12134539843405265]
	TIME [epoch: 8.35 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11458819758279129		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.11458819758279129 | validation: 0.1304930118305507]
	TIME [epoch: 8.32 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11965061053630013		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.11965061053630013 | validation: 0.11793690033226388]
	TIME [epoch: 8.32 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11687223770743053		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.11687223770743053 | validation: 0.13238607164759553]
	TIME [epoch: 8.32 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11980959660957997		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.11980959660957997 | validation: 0.11613582252322906]
	TIME [epoch: 8.35 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11595075065822111		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.11595075065822111 | validation: 0.13219033966484084]
	TIME [epoch: 8.32 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12185345854900938		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.12185345854900938 | validation: 0.1221430383545963]
	TIME [epoch: 8.32 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11863627618106003		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.11863627618106003 | validation: 0.1362133767557297]
	TIME [epoch: 8.33 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1184836626024256		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.1184836626024256 | validation: 0.1222151057997759]
	TIME [epoch: 8.33 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1134506985595134		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.1134506985595134 | validation: 0.12679293578478487]
	TIME [epoch: 8.35 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11776111058966358		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.11776111058966358 | validation: 0.11853811880475279]
	TIME [epoch: 8.34 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11253001725156866		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.11253001725156866 | validation: 0.12307813282349356]
	TIME [epoch: 8.32 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11917678421666236		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.11917678421666236 | validation: 0.13269302506303698]
	TIME [epoch: 8.32 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12639366755025988		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.12639366755025988 | validation: 0.12337399992480393]
	TIME [epoch: 8.34 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11950652361135931		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.11950652361135931 | validation: 0.12186984469781406]
	TIME [epoch: 8.33 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11746765339723624		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.11746765339723624 | validation: 0.11969611748813538]
	TIME [epoch: 8.33 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11927012735746514		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.11927012735746514 | validation: 0.12432369931793744]
	TIME [epoch: 8.33 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.115982931552628		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.115982931552628 | validation: 0.12446298353440735]
	TIME [epoch: 8.34 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11857268783552663		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.11857268783552663 | validation: 0.12895981631612125]
	TIME [epoch: 8.33 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11699191971331716		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.11699191971331716 | validation: 0.11998578414833516]
	TIME [epoch: 8.33 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11600587319830805		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.11600587319830805 | validation: 0.11140617074679696]
	TIME [epoch: 8.33 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11751326446752024		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.11751326446752024 | validation: 0.12524786505287047]
	TIME [epoch: 8.33 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11667527897468655		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.11667527897468655 | validation: 0.11820701265504975]
	TIME [epoch: 8.34 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11281448809838195		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.11281448809838195 | validation: 0.13537316861536997]
	TIME [epoch: 8.32 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12104393997410608		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.12104393997410608 | validation: 0.1295390844438053]
	TIME [epoch: 8.33 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1172518047909139		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.1172518047909139 | validation: 0.1255739451315826]
	TIME [epoch: 8.32 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12042147287634161		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.12042147287634161 | validation: 0.12685318580652172]
	TIME [epoch: 8.34 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11644349649246546		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.11644349649246546 | validation: 0.12679345999305996]
	TIME [epoch: 8.33 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11912020392710039		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.11912020392710039 | validation: 0.12924591938825775]
	TIME [epoch: 8.32 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11640638308780274		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.11640638308780274 | validation: 0.1103480928229367]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1794.pth
	Model improved!!!
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11668817266656681		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.11668817266656681 | validation: 0.12900336044378397]
	TIME [epoch: 8.33 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11993481972757655		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.11993481972757655 | validation: 0.12219560763206883]
	TIME [epoch: 8.64 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11242757458792423		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.11242757458792423 | validation: 0.11640093009666251]
	TIME [epoch: 8.33 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11860216588243562		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.11860216588243562 | validation: 0.11836772229580522]
	TIME [epoch: 8.33 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1208686981346478		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.1208686981346478 | validation: 0.12705541010574434]
	TIME [epoch: 8.35 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11944660966673301		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.11944660966673301 | validation: 0.12689350053896614]
	TIME [epoch: 8.33 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11125682738125472		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.11125682738125472 | validation: 0.11905543791195289]
	TIME [epoch: 8.33 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1183194109599683		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.1183194109599683 | validation: 0.11227551206733236]
	TIME [epoch: 8.33 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11345165994238511		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.11345165994238511 | validation: 0.1168394645405204]
	TIME [epoch: 8.34 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11400417693428004		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.11400417693428004 | validation: 0.12673643390564682]
	TIME [epoch: 8.35 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11502716496664536		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.11502716496664536 | validation: 0.1190074285859363]
	TIME [epoch: 8.33 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11935261684579879		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.11935261684579879 | validation: 0.12119719573844326]
	TIME [epoch: 8.33 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11971511935854855		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.11971511935854855 | validation: 0.1197853127241087]
	TIME [epoch: 8.33 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11867795574328328		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.11867795574328328 | validation: 0.13333903031462474]
	TIME [epoch: 8.35 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1129258655550974		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.1129258655550974 | validation: 0.1243396250355288]
	TIME [epoch: 8.33 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11742969642940768		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.11742969642940768 | validation: 0.12286326953989467]
	TIME [epoch: 8.33 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11308033703288385		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.11308033703288385 | validation: 0.12316490540872599]
	TIME [epoch: 8.33 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11739025869487293		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.11739025869487293 | validation: 0.11999519900898399]
	TIME [epoch: 8.36 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11464834558737516		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.11464834558737516 | validation: 0.1317214826724836]
	TIME [epoch: 8.34 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11353809662450143		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.11353809662450143 | validation: 0.12971683241264945]
	TIME [epoch: 8.33 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11409032684070335		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.11409032684070335 | validation: 0.11643730834243105]
	TIME [epoch: 8.33 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11374795174798606		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.11374795174798606 | validation: 0.12302201017529474]
	TIME [epoch: 8.33 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11346883034883859		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.11346883034883859 | validation: 0.13272829077578532]
	TIME [epoch: 8.35 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12211698180939856		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.12211698180939856 | validation: 0.13322492809785574]
	TIME [epoch: 8.33 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11592039669192163		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.11592039669192163 | validation: 0.12138533628164719]
	TIME [epoch: 8.33 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11349656950087432		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.11349656950087432 | validation: 0.13412302188165537]
	TIME [epoch: 8.33 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11817876963441425		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.11817876963441425 | validation: 0.12546764917726996]
	TIME [epoch: 8.35 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12075501283635637		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.12075501283635637 | validation: 0.1273479524797851]
	TIME [epoch: 8.33 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11492418156225062		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.11492418156225062 | validation: 0.13066626540174428]
	TIME [epoch: 8.33 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11401901286104865		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.11401901286104865 | validation: 0.12928882315181953]
	TIME [epoch: 8.33 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11499328878608757		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.11499328878608757 | validation: 0.13001363366780325]
	TIME [epoch: 8.35 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12232194251731439		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.12232194251731439 | validation: 0.13227859126830466]
	TIME [epoch: 8.33 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11851589620124284		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.11851589620124284 | validation: 0.1300843523145901]
	TIME [epoch: 8.33 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1185460275077062		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.1185460275077062 | validation: 0.12000536057627384]
	TIME [epoch: 8.33 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11852658566989382		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.11852658566989382 | validation: 0.12639718952152107]
	TIME [epoch: 8.33 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11930806608787396		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.11930806608787396 | validation: 0.13016911262974434]
	TIME [epoch: 8.35 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12055612974191343		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.12055612974191343 | validation: 0.12889225683573005]
	TIME [epoch: 8.33 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11759495790628986		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.11759495790628986 | validation: 0.13675593727975682]
	TIME [epoch: 8.33 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12600445403144458		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.12600445403144458 | validation: 0.1439024943288451]
	TIME [epoch: 8.33 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1269746413418224		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.1269746413418224 | validation: 0.13965092323021788]
	TIME [epoch: 8.35 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12352567953193767		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.12352567953193767 | validation: 0.1414253927435411]
	TIME [epoch: 8.33 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1181390256560187		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.1181390256560187 | validation: 0.12453004127048312]
	TIME [epoch: 8.33 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11977548872174218		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.11977548872174218 | validation: 0.1321088558664361]
	TIME [epoch: 8.33 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1169524801839592		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.1169524801839592 | validation: 0.13409721391514257]
	TIME [epoch: 8.35 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11308770504374606		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.11308770504374606 | validation: 0.12036161398008999]
	TIME [epoch: 8.34 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11598486517399868		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.11598486517399868 | validation: 0.1271953481684743]
	TIME [epoch: 8.33 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11227940485688234		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.11227940485688234 | validation: 0.1183961670551128]
	TIME [epoch: 8.33 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11336536404808215		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.11336536404808215 | validation: 0.12566459987040177]
	TIME [epoch: 8.33 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11546364983930872		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.11546364983930872 | validation: 0.11953083835328135]
	TIME [epoch: 8.35 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11969360259336292		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.11969360259336292 | validation: 0.11408638368585937]
	TIME [epoch: 8.33 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12140422757314002		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.12140422757314002 | validation: 0.12156235461398421]
	TIME [epoch: 8.33 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11532805042370838		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.11532805042370838 | validation: 0.1212029196943051]
	TIME [epoch: 8.33 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11270128839734368		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.11270128839734368 | validation: 0.12112011540544201]
	TIME [epoch: 8.35 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11850953203700217		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.11850953203700217 | validation: 0.12810349615098027]
	TIME [epoch: 8.34 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1164305459173535		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.1164305459173535 | validation: 0.12585856396679063]
	TIME [epoch: 8.33 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11350451959114616		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.11350451959114616 | validation: 0.12436258770000451]
	TIME [epoch: 8.32 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1199281604999621		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.1199281604999621 | validation: 0.13199952615013189]
	TIME [epoch: 8.35 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11636364142168773		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.11636364142168773 | validation: 0.1289837898409818]
	TIME [epoch: 8.33 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1137283021995907		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.1137283021995907 | validation: 0.12192687873412512]
	TIME [epoch: 8.33 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11451394306975642		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.11451394306975642 | validation: 0.1231292679902909]
	TIME [epoch: 8.33 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12221449855797017		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.12221449855797017 | validation: 0.13455100430409778]
	TIME [epoch: 8.33 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12157583174329811		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.12157583174329811 | validation: 0.14655005526012968]
	TIME [epoch: 8.35 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12781980880534044		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.12781980880534044 | validation: 0.1253693740643523]
	TIME [epoch: 8.33 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11887815536175945		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.11887815536175945 | validation: 0.13011476741322991]
	TIME [epoch: 8.33 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12056270580588895		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.12056270580588895 | validation: 0.12502200330332533]
	TIME [epoch: 8.33 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12228488863488393		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.12228488863488393 | validation: 0.12885941879902135]
	TIME [epoch: 8.35 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11464888573643157		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.11464888573643157 | validation: 0.13086420269373622]
	TIME [epoch: 8.34 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1126438885760511		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.1126438885760511 | validation: 0.12608056060213818]
	TIME [epoch: 8.33 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11695922538139647		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.11695922538139647 | validation: 0.12183442844799403]
	TIME [epoch: 8.33 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11480991484461855		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.11480991484461855 | validation: 0.12506562987581835]
	TIME [epoch: 8.35 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11344086191008276		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.11344086191008276 | validation: 0.12805892604854655]
	TIME [epoch: 8.33 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11675161711000206		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.11675161711000206 | validation: 0.12776438779496907]
	TIME [epoch: 8.33 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11081116280682524		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.11081116280682524 | validation: 0.12406282691274689]
	TIME [epoch: 8.33 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11805380007475338		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.11805380007475338 | validation: 0.12604895954920298]
	TIME [epoch: 8.33 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1194287835836813		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.1194287835836813 | validation: 0.12279150092472836]
	TIME [epoch: 8.35 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11182777521919432		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.11182777521919432 | validation: 0.12857969713668416]
	TIME [epoch: 8.33 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11783174463822296		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.11783174463822296 | validation: 0.13037391477239402]
	TIME [epoch: 8.33 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11248473572564748		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.11248473572564748 | validation: 0.13345450308934792]
	TIME [epoch: 8.33 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11690201529031315		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.11690201529031315 | validation: 0.12352508676291221]
	TIME [epoch: 8.35 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11196913716967113		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.11196913716967113 | validation: 0.12328609852968567]
	TIME [epoch: 8.33 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11569023103611102		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.11569023103611102 | validation: 0.12934376505321865]
	TIME [epoch: 8.33 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11344850201662826		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.11344850201662826 | validation: 0.11766909322843021]
	TIME [epoch: 8.32 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11349745937954372		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.11349745937954372 | validation: 0.11564053956462164]
	TIME [epoch: 8.35 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1163987610897033		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.1163987610897033 | validation: 0.1280642302657312]
	TIME [epoch: 8.33 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11287806189258975		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.11287806189258975 | validation: 0.1279857904714502]
	TIME [epoch: 8.33 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11474011865746418		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.11474011865746418 | validation: 0.11492097268049409]
	TIME [epoch: 8.33 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11836522985851301		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.11836522985851301 | validation: 0.13260037350583584]
	TIME [epoch: 8.33 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11700191181864512		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.11700191181864512 | validation: 0.12125456095789713]
	TIME [epoch: 8.35 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11235636260018209		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.11235636260018209 | validation: 0.13404244945537058]
	TIME [epoch: 8.33 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11352238538567169		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.11352238538567169 | validation: 0.1210295730585518]
	TIME [epoch: 8.33 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11462509155597385		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.11462509155597385 | validation: 0.12081682041978957]
	TIME [epoch: 8.33 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11670762724974885		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.11670762724974885 | validation: 0.13918193778130253]
	TIME [epoch: 8.35 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1138230910834938		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.1138230910834938 | validation: 0.13084848544746236]
	TIME [epoch: 8.34 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11008560066540851		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.11008560066540851 | validation: 0.11590692458494309]
	TIME [epoch: 8.33 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11320233048594534		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.11320233048594534 | validation: 0.12500636751796634]
	TIME [epoch: 8.33 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11355114322328592		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.11355114322328592 | validation: 0.13009753948370195]
	TIME [epoch: 8.35 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11898530088251262		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.11898530088251262 | validation: 0.12633170257927523]
	TIME [epoch: 8.33 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11804744445320085		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.11804744445320085 | validation: 0.12621588467206646]
	TIME [epoch: 8.33 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11591133935585811		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.11591133935585811 | validation: 0.11215394590898334]
	TIME [epoch: 8.33 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11422514511406505		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.11422514511406505 | validation: 0.12034901113284933]
	TIME [epoch: 8.33 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11387171615295992		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.11387171615295992 | validation: 0.12412301527108982]
	TIME [epoch: 8.35 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11395845116431706		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.11395845116431706 | validation: 0.11470987730853804]
	TIME [epoch: 8.33 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11769762538452547		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.11769762538452547 | validation: 0.11514200079807727]
	TIME [epoch: 8.33 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11837888479396375		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.11837888479396375 | validation: 0.12095668715783955]
	TIME [epoch: 8.33 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11435197143782379		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.11435197143782379 | validation: 0.13704398027662562]
	TIME [epoch: 8.36 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11630337896818244		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.11630337896818244 | validation: 0.12561626908844192]
	TIME [epoch: 8.33 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11718983063415053		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.11718983063415053 | validation: 0.11391964475685212]
	TIME [epoch: 8.33 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11966701957476751		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.11966701957476751 | validation: 0.13939610626805823]
	TIME [epoch: 8.33 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11609221291988388		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.11609221291988388 | validation: 0.12943449741371574]
	TIME [epoch: 8.35 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11783940282204988		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.11783940282204988 | validation: 0.12782560707734697]
	TIME [epoch: 8.33 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11754980871144745		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.11754980871144745 | validation: 0.12901324528628966]
	TIME [epoch: 8.33 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11568710837412202		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.11568710837412202 | validation: 0.13632926658931302]
	TIME [epoch: 8.33 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11765183781746473		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.11765183781746473 | validation: 0.12416194214364024]
	TIME [epoch: 8.34 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11868515526636489		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.11868515526636489 | validation: 0.12965497213683347]
	TIME [epoch: 8.35 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11692933556023392		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.11692933556023392 | validation: 0.13030254980902936]
	TIME [epoch: 8.33 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11875928924684911		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.11875928924684911 | validation: 0.1250027019281566]
	TIME [epoch: 8.33 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11611294118357138		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.11611294118357138 | validation: 0.11900823699656038]
	TIME [epoch: 8.33 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11257269412531301		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.11257269412531301 | validation: 0.13167692128941783]
	TIME [epoch: 8.35 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11309234327599796		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.11309234327599796 | validation: 0.12354279586180517]
	TIME [epoch: 8.33 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11377166938614666		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.11377166938614666 | validation: 0.12959104642237354]
	TIME [epoch: 8.34 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11746165907723587		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.11746165907723587 | validation: 0.1309078932938766]
	TIME [epoch: 8.33 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11491977574978159		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.11491977574978159 | validation: 0.1368084413858137]
	TIME [epoch: 8.36 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11750540407281149		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.11750540407281149 | validation: 0.11827360851874844]
	TIME [epoch: 8.34 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11444652778179565		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.11444652778179565 | validation: 0.1348603713531211]
	TIME [epoch: 8.33 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11731380523868654		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.11731380523868654 | validation: 0.12025843700643359]
	TIME [epoch: 8.34 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11730340079346047		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.11730340079346047 | validation: 0.1243718496046966]
	TIME [epoch: 8.34 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11182954613155069		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.11182954613155069 | validation: 0.12267519377097011]
	TIME [epoch: 8.35 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11691252334148042		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.11691252334148042 | validation: 0.11967769509247665]
	TIME [epoch: 8.33 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.120826854999402		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.120826854999402 | validation: 0.11986064984279036]
	TIME [epoch: 8.33 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11372774581048406		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.11372774581048406 | validation: 0.12170643904394451]
	TIME [epoch: 8.33 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11162809753301924		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.11162809753301924 | validation: 0.11951551020878717]
	TIME [epoch: 8.35 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11429061818842678		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.11429061818842678 | validation: 0.1155267368406703]
	TIME [epoch: 8.33 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11895047161855259		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.11895047161855259 | validation: 0.11922123233853942]
	TIME [epoch: 8.33 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11320646091258109		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.11320646091258109 | validation: 0.1066622292690656]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r2_20240219_233648/states/model_tr_study203_1928.pth
	Model improved!!!
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1160244477141605		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.1160244477141605 | validation: 0.11828566318418346]
	TIME [epoch: 8.35 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1150072033624748		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.1150072033624748 | validation: 0.12049156242946443]
	TIME [epoch: 8.33 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1190092678138627		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.1190092678138627 | validation: 0.12601636956669335]
	TIME [epoch: 8.33 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.117819214300853		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.117819214300853 | validation: 0.12000466248464825]
	TIME [epoch: 8.33 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11246659453540282		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.11246659453540282 | validation: 0.13236004325499726]
	TIME [epoch: 8.34 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1215357200452651		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.1215357200452651 | validation: 0.12486139187337164]
	TIME [epoch: 8.34 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12091251903640983		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.12091251903640983 | validation: 0.1219687217457229]
	TIME [epoch: 8.33 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11811587210373231		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.11811587210373231 | validation: 0.12693076029506348]
	TIME [epoch: 8.32 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1145085419069454		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.1145085419069454 | validation: 0.11186176320777788]
	TIME [epoch: 8.32 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11452088374597676		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.11452088374597676 | validation: 0.12520898483189907]
	TIME [epoch: 8.34 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11470629701810267		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.11470629701810267 | validation: 0.12193789982717768]
	TIME [epoch: 8.33 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11559846543725516		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.11559846543725516 | validation: 0.11847147850851675]
	TIME [epoch: 8.33 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1162268167967869		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.1162268167967869 | validation: 0.12139845410693027]
	TIME [epoch: 8.33 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11254730430866795		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.11254730430866795 | validation: 0.11925534555551358]
	TIME [epoch: 8.35 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11143947238730723		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.11143947238730723 | validation: 0.12599279335883837]
	TIME [epoch: 8.33 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1114097950228077		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.1114097950228077 | validation: 0.11543635648445308]
	TIME [epoch: 8.33 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11410337925439631		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.11410337925439631 | validation: 0.11874408654976149]
	TIME [epoch: 8.32 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11339648886091805		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.11339648886091805 | validation: 0.11636174542205142]
	TIME [epoch: 8.34 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11364556146243773		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.11364556146243773 | validation: 0.12397455229675274]
	TIME [epoch: 8.33 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1119930672479064		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.1119930672479064 | validation: 0.1157781630260851]
	TIME [epoch: 8.33 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12157797708881561		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.12157797708881561 | validation: 0.12424910596594232]
	TIME [epoch: 8.33 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1158883123864719		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.1158883123864719 | validation: 0.12280250349420795]
	TIME [epoch: 8.33 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11754862808594006		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.11754862808594006 | validation: 0.10999706390518493]
	TIME [epoch: 8.34 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1187093537591992		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.1187093537591992 | validation: 0.11603042895561524]
	TIME [epoch: 8.32 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11354981912457562		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.11354981912457562 | validation: 0.11882738696270362]
	TIME [epoch: 8.33 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11309666181706576		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.11309666181706576 | validation: 0.11394577375566095]
	TIME [epoch: 8.32 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11821768833860913		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.11821768833860913 | validation: 0.13538878674814067]
	TIME [epoch: 8.35 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11540726565679474		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.11540726565679474 | validation: 0.1190123656234845]
	TIME [epoch: 8.33 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11495755038863069		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.11495755038863069 | validation: 0.12110066908430786]
	TIME [epoch: 8.32 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11364542032992664		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.11364542032992664 | validation: 0.125529121056543]
	TIME [epoch: 8.33 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11710198686258558		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.11710198686258558 | validation: 0.13442701926326]
	TIME [epoch: 8.34 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11365711743985873		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.11365711743985873 | validation: 0.12747996687621604]
	TIME [epoch: 8.33 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11286092442148261		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.11286092442148261 | validation: 0.12855575014267906]
	TIME [epoch: 8.32 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11487404145855513		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.11487404145855513 | validation: 0.11825176642217466]
	TIME [epoch: 8.33 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11643714870961519		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.11643714870961519 | validation: 0.12526785088476197]
	TIME [epoch: 8.33 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10949228293270737		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.10949228293270737 | validation: 0.12671719958433125]
	TIME [epoch: 8.35 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11253646201960739		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.11253646201960739 | validation: 0.13131685242571345]
	TIME [epoch: 8.33 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11460582485606045		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.11460582485606045 | validation: 0.11965623959951355]
	TIME [epoch: 8.33 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11342324944208208		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.11342324944208208 | validation: 0.11515997884924836]
	TIME [epoch: 8.32 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11504036023002735		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.11504036023002735 | validation: 0.11337762278155607]
	TIME [epoch: 8.34 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11643204314985542		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.11643204314985542 | validation: 0.12581886054600244]
	TIME [epoch: 8.32 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1198728864371286		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.1198728864371286 | validation: 0.13821921862383785]
	TIME [epoch: 8.32 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11503535447999491		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.11503535447999491 | validation: 0.12391893266158843]
	TIME [epoch: 8.32 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11747382544687315		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.11747382544687315 | validation: 0.1418647942400222]
	TIME [epoch: 8.34 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11637639618831024		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.11637639618831024 | validation: 0.12916080226211996]
	TIME [epoch: 8.33 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1125576030821771		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.1125576030821771 | validation: 0.12676791165428625]
	TIME [epoch: 8.32 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11949060282037043		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.11949060282037043 | validation: 0.1259019654025725]
	TIME [epoch: 8.32 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11647101443160042		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.11647101443160042 | validation: 0.1254549475954654]
	TIME [epoch: 8.32 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11321617800797994		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.11321617800797994 | validation: 0.12872912078411472]
	TIME [epoch: 8.34 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11544893534207053		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.11544893534207053 | validation: 0.114670315327156]
	TIME [epoch: 8.32 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11973062943929866		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.11973062943929866 | validation: 0.12584386691048965]
	TIME [epoch: 8.32 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1142844214797502		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.1142844214797502 | validation: 0.1275578347749801]
	TIME [epoch: 8.32 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11761056588310184		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.11761056588310184 | validation: 0.11289069723548706]
	TIME [epoch: 8.34 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11238162268510402		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.11238162268510402 | validation: 0.1259110537486322]
	TIME [epoch: 8.33 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11664689753721671		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.11664689753721671 | validation: 0.12128413709081062]
	TIME [epoch: 8.32 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11542951888928292		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.11542951888928292 | validation: 0.11307667859664602]
	TIME [epoch: 8.32 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11553138188363192		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.11553138188363192 | validation: 0.12280383626247898]
	TIME [epoch: 8.34 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11611818135416238		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.11611818135416238 | validation: 0.12588775244990297]
	TIME [epoch: 8.32 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11116135088733511		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.11116135088733511 | validation: 0.12788909171380264]
	TIME [epoch: 8.33 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11247957065175221		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.11247957065175221 | validation: 0.12478216260578939]
	TIME [epoch: 8.32 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11575427485833471		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.11575427485833471 | validation: 0.12483372494353975]
	TIME [epoch: 8.33 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11389610597852154		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.11389610597852154 | validation: 0.12326078622983691]
	TIME [epoch: 8.35 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11571022202237288		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.11571022202237288 | validation: 0.1201443820249615]
	TIME [epoch: 8.32 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11712764538013651		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.11712764538013651 | validation: 0.12261230167484963]
	TIME [epoch: 8.32 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11513419305868207		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.11513419305868207 | validation: 0.11579826927875772]
	TIME [epoch: 8.32 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11641940031709504		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.11641940031709504 | validation: 0.12568845779281912]
	TIME [epoch: 8.34 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11209532094113366		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.11209532094113366 | validation: 0.11356073177860074]
	TIME [epoch: 8.32 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11522610400522737		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.11522610400522737 | validation: 0.12116506925053486]
	TIME [epoch: 8.32 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11518262480068153		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.11518262480068153 | validation: 0.13058624851420741]
	TIME [epoch: 8.32 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11912633092823426		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.11912633092823426 | validation: 0.12159841222279619]
	TIME [epoch: 8.34 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1171343840372107		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.1171343840372107 | validation: 0.12436548339268223]
	TIME [epoch: 8.32 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11304396550368083		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.11304396550368083 | validation: 0.11791587281751395]
	TIME [epoch: 8.32 sec]
Finished training in 16836.200 seconds.
